[
  {
    "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical Dataset",
      "Breast Cancer Bone Metastasis",
      "Diagnosis",
      "Prognosis",
      "Sparse CT reconstruction",
      "CT",
      "X-ray",
      "Large language model",
      "AI for Science"
    ],
    "abstract": "Breast cancer bone metastasis (BCBM) affects women‚Äôs health globally, calling\n for the development of effective diagnosis and prognosis solutions. While deep\n learning has exhibited impressive capacities across various healthcare domains, its\n applicability in BCBM diseases is consistently hindered by the lack of an open,\n large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis\n (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical\n resource, which is derived from a well-accepted murine BCBM model. The unique\n advantage of BoneMet over existing human datasets is repeated sequential scans\n per subject over the entire disease development phases. The dataset consists of\n over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D\n CT scans, and detailed biological data (e.g., medical records and bone quantitative\n analysis), collected from more than five hundreds mice spanning from 2019 to\n 2024. Our BoneMet dataset is well-organized into six components, i.e., Rotation\nX-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further\n show that BoneMet can be readily adopted to build versatile, large-scale AI models\n for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term\n disease monitoring. Our preliminary results demonstrate that BoneMet has the\n potentials to jump-start the development and fine-tuning of AI-driven solutions\n prior to their applications to human patients. To facilitate its easy access and\n wide dissemination, we have created the BoneMet package, providing three APIs\n that enable researchers to (i) flexibly process and download the BoneMet data\n filtered by specific time frames; and (ii) develop and train large-scale AI models for\n precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a novel large-scale dataset (named BoneMet) of Breast tumor bone metastasis for disease diagnosis, prognosis, and treatment management. It consists of six components: Rotational X-Ray Imagery, Reconstructed CT Imagery, Segmented CT Imagery, Registered CT Imagery, Region of Interest CT Imagery, and Mice Medical Records. Besides, the author conducted a series of experiments on the BoneMet dataset by developing various deep learning models to exhibit its applicability and efficiency in managing BTBM disease. The enormous open-source dataset has significant implications for the development of new algorithms in this field beyond doubt.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n(1) The motivation of this study is clear and the background is well presented in the manuscript.\nThe main contribution of this work is to release a large-scale breast tumor bone metastasis dataset BoneMet, for the research community. Upon the subsets of the BoneMet, the researchers are allowed to develop some novel approaches to solve the task of breast tumor bone metastasis diagnosis and prognosis, and thus facilitating the automated analysis for breast tumor bone metastasis.\n(2) Apart from the information of the collected dataset, the authors also developed several deep learning models to validate the applicability and efficacy of BoneMet.\n(3) Dataset, code and tutorials are made available for free use.\n\n### Weaknesses\n\n(1) The technical contributions of this study are quite marginal, as most of the deep learning models are built upon existing methods.\n(2) Some details about the dataset preparation and experiments are missing.\n(3) Some settings and analyses in the experiment may not support the objectives.\n(4) The quality of the labels, particularly the pixel-wise annotation of the bone, might be questionable.\n\n### Questions\n\n1. Authors describe that Seg-CT and Recon-CT are independent. What is the difference between 3D CT scans in Recon-CT and Seg-CT? What is the meaning of the adjective 'segmented'?\n2. Line 341 \"Second, we observe a significant training-test accuracy gap of 20.4% with ViT (w/o STA), indicating a pronounced overfitting issue inherent to the ViT architecture.\"\nI think ViT, as a visual backbone network, is less overfitting compared to other structures such as CNN. In addition, is the decrease in accuracy necessarily overfitting?\n3. Section 3.1, the purpose and analysis of experiments is confusing. Why comparing two ViT models could demonstrate the applicability of the Rotation-X-Ray dataset to manage BTBM disease? What is the relation between the effectiveness of\nSTA and the applicability of the Rotation-X-Ray dataset? All subsections 3.2-4 have similar questions.\n4. The experiment did not report data partitioning.\n5. Table 2 reported metrics on the training and testing data. Is there any evaluation/dev data?\n6. The experiment reported many indicator values, but their actual clinical significance has not been evaluated. For example, what are the Precision/Recall/F1-Score/Accuracy of clinical experts on the BTBM diagnosis? Besides, the 3D-GAN, T-VAE, and ST-VAE methods achieve SSIM values of 0.767, 0.817, and 0.860. So, what role can they play in clinical practice?\n7. The authors used many existing softwares as the processing tools to generate the sub-datasets of BoneMet. Please provide the corresponding reference and specify whether it is free of charge or not. Also, please accurately explain in which case the datasets are processed by the Seg-API, Regist-API and RoI-API that were developed by the authors.\n8. Are Pre-Op and Post-Op refer to pre-operation and post-operation? Please give the full spelling.\n9. For RoI-CT, the tibiae ROI is assigned with different pixel values. As shown in Figure 4, for each CT example, is the processed CT image contains the values of 180, 240, 60 and 0 only? If yes, the original CT turns to a segmentation mask instead, and lots of information would be lost.\n10. In the whole preparation procedure, it seems that the manual annotation of the organs or tumors is not involved, except for the manual cropping of fibular by  CTAn¬Æ. How could we ensure the effectiveness of the existing software?\n11. In Table 1, the column \"Spatial Resolution\" might be accurately filled out as most of them indicate the organs. Please revise it.\n12. Some details about the BoneMet dataset are missing. For instance, the number of mice is not introduced. Besides, in Table 1, the images/records are classified into two categories, i.e., positive and negative. Is the label assigned at the slice level or animal level? Besides, we noticed that the numbers of Seg-CT are larger than those of Recon-CT, which makes it confusing, as the Seg-CT is obtained from Recon-CT.\n13. I also have concerns about the MiceMediRec dataset. It mainly consists of the medical record and quantitative analysis results of 3D CT images, simulation and mechanical testing. The authors list only a part of the features in Table S2 in the supplementary file. However, all feature should be provided. The off-the-shelf software, such as CTAn¬Æ, was used to generate the quantitative parameters, which took about half an hour for each case of animal. In Section 3.3, the multi-modal prognostic assessment model first leveraged the GAN to produce the CT scan of the future point and use the quantitative data in the MiceMediRec dataset as the ground truth label to validate the predicted reaction force values. However, the details about how to obtain the predicted reaction force values are not clear.\n14. In my opinion, multi-modal learning might refer to a way to integrate the information of multi-source data to build a model. However, the application in Section 3.3 just used CT data as model input. Please consider revising it.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe authors have obtained ethical approval from their institution, and thus, so the ethics review is not needed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents BoneMet, a novel large-scale dataset focused on breast tumor bone metastasis (BTBM) aimed at facilitating disease diagnosis, prognosis, and treatment management. The dataset comprises six components, including various types of CT imagery and medical records from mice. To demonstrate the dataset's applicability, the authors have conducted experiments utilizing multiple deep learning models. The creation of this open-source dataset holds significant promise for advancing research and algorithm development in the field.\n\n### Major Comments\n1. **Novelty and Depth**: The technical contributions are limited, as many deep learning models are based on existing methodologies rather than introducing new concepts.\n2. **Dataset Preparation and Experimentation**: Key details regarding dataset preparation and experimental setup are inadequate and require elaboration.\n3. **Relevance of Experiments**: Some experimental settings and analyses do not clearly align with the study‚Äôs objectives.\n4. **Label Quality Concerns**: The quality of pixel-wise annotations, particularly regarding bone labels, appears questionable.\n\n### Minor Comments\n- Clarify the differences between Seg-CT and Recon-CT. Define \"segmented\".\n- Address whether the observed accuracy gap with ViT indicates true overfitting.\n- Provide rationale for comparing two ViT models in terms of the dataset's applicability.\n- Data partitioning details were not mentioned.\n- Clarify the presence of evaluation data in the experimental metrics reported in Table 2.\n- Assess the clinical relevance of reported performance indicators compared to clinical expert benchmarks.\n- Provide references for software used in dataset processing, including details on cost and functionality.\n- Expand on the definitions of Pre-Op and Post-Op.\n- Confirm whether the pixel values in RoI-CT offer sufficient segmentation data without loss of information.\n- Justify the absence of manual annotation in the dataset preparation process.\n- Revise the \"Spatial Resolution\" in Table 1 for accuracy.\n- Include additional details on the BoneMet dataset, such as the number of mice involved and clarification of labeling at slice versus animal level.\n- Ensure comprehensive feature listings for the MiceMediRec dataset in supplementary materials.\n- Clarify the methodology behind predicting reaction force values using the GAN model.\n- Consider revisions to multi-modal learning definitions used in Section 3.3, as they appear limited.\n\n### Summary Paragraph\nIn evaluating the manuscript against TMI editorial criteria, the significance of the BoneMet dataset is evident, given its potential for impactful research in BTBM. The innovation is moderate, primarily due to the reliance on existing deep learning models. For evaluation, while the experimental design showcases various applications of the dataset, critical details are lacking, which affects reproducibility. Overall, the manuscript presents strengths in dataset contribution and accessibility while revealing weaknesses in methodological clarity and experimental validation.\n\n### Decision Recommendation\nMajor revision is recommended to address substantial concerns related to technical contributions and detail in the dataset and experimental framework.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *BoneMet*, a large-scale open-source dataset on breast tumor bone metastasis (BTBM), developed to support research in disease diagnosis, prognosis, and treatment management. The dataset comprises six subcomponents‚ÄîRotational X-Ray, Reconstructed CT, Segmented CT, Registered CT, Region of Interest CT, and Mice Medical Records‚Äîand is accompanied by code and tutorials. The authors further demonstrate the dataset‚Äôs applicability through multiple experimental deep learning models. Overall, the paper is clearly motivated and well organized, offering a valuable data resource, though the technical contributions and certain methodological details could be strengthened.\n\n**Major Comments**  \n1. **Technical Contribution:** The deep learning models mainly build upon existing architectures, resulting in limited methodological novelty.  \n2. **Dataset Clarity:** Key details regarding dataset preparation and processing are missing, including information on data partitioning, labeling strategy (slice vs. animal level), number of subjects, and label quality.  \n3. **Experimental Design:** The experimental setup and interpretation are sometimes unclear. The rationale linking specific model comparisons (e.g., ViT variants) to the stated objectives requires explanation; some analyses do not fully substantiate the conclusions.  \n4. **Annotation and Processing:** The pixel-wise label quality, especially for bone segmentation, is uncertain. It remains unclear how accurately automated tools perform without manual verification.  \n5. **Use of Software Tools:** The manuscript employs various existing tools (e.g., CTAn¬Æ, Seg-API, Regist-API, RoI-API) without fully specifying licensing, references, or usage contexts.  \n6. **Clinical Relevance:** Reported quantitative metrics (e.g., SSIM, F1, accuracy) lack comparison with clinical benchmarks, limiting interpretation of the dataset‚Äôs practical impact.  \n7. **MiceMediRec Module:** The structure, features, and ground-truth generation for this sub-dataset need clearer description‚Äîparticularly the process for obtaining predicted and actual force values.  \n8. **Terminology and Concept Use:** Terms such as ‚Äúmulti-modal learning,‚Äù ‚Äúsegmented CT,‚Äù and ‚ÄúPre-/Post-Op‚Äù require clearer definitions to avoid ambiguity.\n\n**Minor Comments**  \n- Clarify Table 1 entries, particularly ‚ÄúSpatial Resolution.‚Äù  \n- Confirm whether Seg-CT and Recon-CT images differ beyond segmentation masks.  \n- In Figure 4, specify pixel-value encoding for ROI-CT images.  \n- Review missing or incomplete references for software tools.  \n- Check typographical consistency and acronym definitions (e.g., STA).  \n\n**Summary Paragraph (TMI Editorial Criteria)**  \nThe dataset addresses a significant and underexplored clinical application, enhancing reproducibility and data accessibility for BTBM research. Its innovation lies primarily in dataset creation rather than modeling advances. While significance and potential impact are high, methodological rigor is reduced by insufficient experimental detail and unvalidated label accuracy. Evaluation is broad but lacks clinical contextualization, and reproducibility would benefit from more complete documentation of preprocessing and partitioning procedures.\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial clarification of dataset composition, labeling, experimental rationale, and clinical relevance is required before the paper can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces BoneMet, the first large-scale, open dataset specifically designed for BTBM research. This dataset offers over 50 terabytes of high-resolution, multi-modal data, including 2D X-rays, 3D CT scans, and comprehensive biological records collected from thousands of mice. Structured into six components, the dataset may be suitable for a range of AI tasks focused on BTBM diagnosis, prognosis, and treatment. The authors also conducted extensive experiments to demonstrate the usability of this dataset. The dataset, APIs for flexible data processing and retrieval, accompanying code, and tutorials are freely available. To some extent, this dataset makes a valuable contribution to this field.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.This paper presents a large-scale, open dataset for the diagnosis and prognosis of breast tumor bone metastasis (BTBM), with a total volume of over 50 terabytes, encompassing various modalities. To some extent, this dataset makes a valuable contribution to this field.\n2.The authors have developed a specialized toolkit for accessing and processing the dataset, facilitating its use for researchers in the field.\n3.The authors conducted detailed experiments that demonstrate the usability of this dataset for some tasks related to BTBM diagnosis and prognosis.\n\n### Weaknesses\n\n1.This dataset is derived from mice, and there are certain differences between mouse skeletons and human skeletons. Can models trained on this dataset be effectively transferred to the diagnosis and prognosis of human BTBM? If applicable, how is the performance? If not, what clinical value does this dataset or model hold?\n2.The authors do not provide sufficient details regarding the experimental setup, particularly how the data was partitioned. The experiments appear to be internal validations conducted solely within the dataset, with no external validation on other datasets. This raises concerns about the generalization ability of the models trained on this dataset. Therefore, it remains uncertain whether this dataset can be used to build versatile large-scale AI models or foundational models.\n\n### Questions\n\nrefer to the weaknesses.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents BoneMet, a pioneering large-scale open dataset developed for research in breast tumor bone metastasis (BTBM). With over 50 terabytes of high-resolution, multi-modal data, including 2D X-rays, 3D CT scans, and extensive biological records from thousands of mice, this dataset aims to support various AI tasks related to BTBM diagnosis, prognosis, and treatment. The authors have also provided APIs, code, and tutorials to facilitate data processing and accessibility. Overall, the contribution of this dataset to BTBM research is noteworthy.\n\n**Major Comments:**  \n1. While the dataset is comprehensive, it is derived from mice, raising concerns about the transferability of models trained on it to human BTBM diagnosis and prognosis. The manuscript lacks clarification on the performance of such models and their clinical value.\n2. The description of the experimental setup is inadequate, particularly concerning data partitioning. The experiments seem to be internal validations within the dataset without external validation, which casts doubt on the generalizability of the models developed.\n\n**Minor Comments:**  \n- Consider providing additional information on the differences between mouse and human skeletons to better contextualize the dataset‚Äôs application.\n- Clarify the specific tasks related to BTBM diagnosis and prognosis for which the dataset demonstrates usability.\n\n**Summary Paragraph:**  \nIn terms of significance, BoneMet represents a valuable resource for BTBM research, but concerns about its innovation stem from the lack of validation on human models. The evaluation of the dataset's applicability is limited due to insufficient experimental detail, particularly regarding generalizability. Reproducibility is also undermined by the absence of external validation, which is crucial for assessing the dataset's robustness in diverse applications. \n\n**Decision Recommendation:**  \nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *BoneMet*, a large-scale, open-access dataset specifically designed for research on breast tumor bone metastasis (BTBM). Comprising over 50 terabytes of high-resolution, multi-modal imaging and biological data from thousands of mice, the dataset includes 2D X-rays, 3D CT scans, and detailed biological records. Structured into six components, it aims to support various AI applications related to BTBM diagnosis, prognosis, and treatment. The authors provide accompanying code, APIs, and tutorials and perform experiments to illustrate dataset usability. Overall, the dataset represents a valuable resource, though its broader translational relevance requires further clarification.\n\n---\n\n**Major Comments**  \n1. **Translational relevance and generalizability:** Since BoneMet is derived entirely from mouse data, the anatomical and biological differences between mice and humans raise questions about the dataset‚Äôs applicability to human BTBM research. The authors should clarify whether the trained models can be effectively transferred to human data, present supporting performance evidence if available, or discuss the limitations in clinical translation.  \n2. **Experimental design and validation:** The paper lacks sufficient details about the experimental setup, particularly how data partitions were defined. The validation appears to be conducted only within the BoneMet dataset, with no external validation. This limits confidence in the generalizability of the results and leaves uncertain whether the dataset can support more robust or foundation-level AI models.\n\n---\n\n**Minor Comments**  \n- The manuscript would benefit from clearer presentation of the data split strategy and corresponding rationale.  \n- Certain sections could be reorganized to improve readability and coherence in describing dataset structure and usage.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, BoneMet contributes a large, comprehensive dataset for BTBM research, addressing a notable data gap. Its **innovation** lies in the dataset‚Äôs scale, multimodal integration, and accompanying tools. However, the **evaluation** is incomplete due to the lack of external benchmarking, and **reproducibility** would improve with more transparent details on data partitioning and experimental protocols. Overall, the work is promising but needs stronger validation and clarification before publication.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors integrated a publicly available large-scale Bone Metastasis (BoneMet) dataset with multimodal medical data, including 2D X-ray images, 3D CT scans, and comprehensive biological data. The primary objective of this database is to facilitate the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well-organized and clearly written.\n\nOur collected dataset is open, large-scale, and consists of multi-modal resources including 2D X-ray images, 3D CT scans, and medical records and quantitative analysis.\n\nThe author provides a tool on the Python Package Index (PyPI) for assisting researchers and practitioners.\n\nThe authors have conducted multiple benchmarks, such as BTBM Diagnosis to exemplify the utilisation of the collated dataset.\n\n### Weaknesses\n\nIn the abstract, the author states, ‚ÄúBreast tumor bone metastasis (BTBM) affects women‚Äôs health globally, necessitating the development of effective solutions for its diagnosis and prognosis.‚Äù However, the dataset collected was derived from mice rather than human patients.\n\nThe author should perform a comprehensive comparison between the BoneMet dataset and previously available datasets, including those derived from human subjects, to elucidate the differences and scales involved.\n\nAdditionally, the methods employed in each benchmark were limited. The author should justify the selection of the included methods over others.\n\n### Questions\n\nThis paper comprises two main components: the dataset and the benchmark. Both sections could benefit from additional details and comparisons.\n\nThe authors should evaluate and clarify how the collected data can directly benefit human health. For example, they could discuss the potential for training models on their dataset and subsequently transfer the obtained model to human datasets.\n\nFurthermore, the figures in the manuscript require better organization, as the text is currently too small for effective readability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a study that integrates the Bone Metastasis (BoneMet) dataset with multimodal medical data, including 2D X-ray images, 3D CT scans, and comprehensive biological information. The primary aim is to enhance the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors. Overall, the manuscript is well-organized and effectively communicates its objectives, yet some aspects require further clarification and justification.\n\n**Major Comments:**\n1. The authors claim that \"Breast tumor bone metastasis (BTBM) affects women‚Äôs health globally,\" but the dataset is derived from mice, not human patients. This discrepancy should be addressed.\n2. A comprehensive comparison of the BoneMet dataset with existing datasets derived from human subjects is necessary to highlight differences and implications for clinical relevance.\n3. The methods used in the benchmarks are limited. Justification for the selection of these specific methods over alternatives is needed.\n\n**Minor Comments:**\n- The dataset and its utility for human health could be better articulated. The authors should discuss the potential for training predictive models on this dataset and the ability to transfer results to human datasets.\n- Figures in the manuscript need improved organization, as the current text size inhibits readability.\n\n**Summary Paragraph:**  \nIn terms of the TMI editorial criteria, the study demonstrates significance through its focus on bone metastasis, though the innovation is compromised by the use of a murine dataset. Evaluation is addressed through benchmark comparisons, but the limited methods raise concerns about robustness. Reproducibility is supported by the open-source dataset and tools provided on the Python Package Index (PyPI), yet requires clearer connections to human health impacts. Overall, while the study has strengths, several key areas necessitate further attention.\n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the development of a large-scale, multimodal Bone Metastasis (BoneMet) dataset that integrates 2D X-ray images, 3D CT scans, and biological/medical data. The stated aim is to advance the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors. The paper is clearly written and well-organized, with an emphasis on facilitating further research through public data availability and benchmarking tools.\n\n**Major Comments**  \n1. **Data Source and Relevance:** Although the abstract emphasizes the global significance of breast tumor bone metastasis in humans, the dataset is derived from mice rather than human patients. The authors should explicitly acknowledge this limitation and clarify the translational relevance of the dataset for human studies.  \n2. **Comparative Analysis:** A comprehensive comparison with existing datasets‚Äîparticularly those based on human data‚Äîis needed to contextualize the BoneMet dataset in terms of scale, modality, and novelty.  \n3. **Benchmark Methodology:** The benchmarks provided (e.g., BTBM Diagnosis) are limited in methodological diversity. The rationale for selecting the specific baseline methods should be elaborated, and additional, more representative methods may strengthen the demonstration.  \n4. **Dataset Utility and Impact:** Further discussion is warranted on how models trained using the BoneMet dataset could transfer to or benefit human clinical research. This connection is important for demonstrating practical significance beyond preclinical contexts.\n\n**Minor Comments**  \n- Figures require improved organization and larger text to enhance legibility.  \n- Documentation or description of the PyPI package could be expanded to improve usability.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in providing an open, multimodal dataset for bone metastasis research and includes associated benchmarking and a Python package to aid reproducibility. Its innovation lies primarily in dataset integration rather than methodological advancement. However, the limited explanation of translational relevance, lack of comprehensive comparison with existing datasets, and narrow benchmarking reduce its perceived novelty and general impact. Reproducibility is reasonably supported through public release, but evaluation and contextualization need strengthening.\n\n**Decision Recommendation**  \n**Major Revision.** The study presents a valuable resource, but clearer justification of data relevance, expanded comparisons, and improved figure presentation are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase 1 ‚Äì Analytical & Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript clearly falls within the methodological scope of *IEEE Transactions on Medical Imaging (TMI)*.  \nAlthough it primarily introduces a dataset rather than a new algorithmic framework, it directly supports medical imaging methodology by enabling development and benchmarking of imaging reconstruction, segmentation, registration, and prognosis models. The inclusion of multiple imaging modalities (2D X-ray, 3D CT) and temporal alignment further strengthens its relevance for imaging physics, reconstruction, and AI-based image analysis.\n\n**2. Novelty & Contribution Level**  \n‚ö†Ô∏è Moderate. The work‚Äôs central novelty is the assembly and dissemination of an exceptionally large, multi-modal, longitudinal murine imaging dataset specialized for breast cancer bone metastasis (BCBM). While the technical foundation (image acquisition and FBP-based CT reconstruction) is standard, the dataset‚Äôs scale, longitudinal structure, systematic processing, and accompanying APIs for segmentation, registration, and RoI cropping provide methodological value. However, most methodological demonstrations (e.g., ViT diagnosis, generative prognosis, sparse-angle reconstruction) reuse established algorithms to illustrate utility rather than introduce fundamentally new imaging methodology.\n\n**3. Technical and Experimental Rigor**  \nHigh in data collection, moderate in validation. The paper meticulously documents imaging parameters, animal model design, sample numbers, temporal alignment, data organization, and preprocessing pipelines‚Äîindicating strong engineering and reproducibility standards. The demonstration studies show feasibility (diagnosis, prognosis, sparse reconstruction) but are limited in depth of experimental comparison and statistical evaluation. Metrics are provided, but details on error analysis, uncertainty, or generalization to human data are minimal.\n\n**4. Clarity and Presentation**  \nGenerally clear and well-structured; grammar and formatting are acceptable though inconsistent spacing, capitalization, and symbol rendering (e.g., ‚Äú0¬∞0.8¬∞1.6¬∞...‚Äù) and minor typographical errors recur. Figures are informative but sometimes heavily compressed. Section organization and dataset tables are comprehensive.\n\n**5. Ethical and Reproducibility Compliance**  \nStrong compliance. IACUC approval is explicitly stated with details on anesthesia, radiation dose, and humane animal treatment. Data is publicly released (Hugging Face, PyPI, GitHub), ensuring reproducibility and accessibility. Ethical data handling appears compliant with TMI standards.\n\n---\n\n## **Phase 2 ‚Äì IEEE TMI Review Report**\n\n### **1. Summary**\nThe authors present **BoneMet**, a large-scale, longitudinal, multi-modal murine dataset designed for research on breast cancer bone metastasis. The dataset contains over **67 TB** of high-resolution rotational X-rays and 3D CT scans from **500+ mice** over five years, complemented by biological and mechanical metadata. Data are organized into six components (Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec). The accompanying **BoneMet Python package** provides APIs for CT segmentation, registration, and region-of-interest cropping. Demonstration studies validate dataset usability in (i) X-ray/CT-based disease diagnosis, (ii) prognosis via 3D generative modeling and biomechanical simulation, and (iii) sparse-angle CT reconstruction using NeRF variants.\n\n### **2. Strengths**\n- **Significant dataset contribution:** First open, large-scale, longitudinal murine imaging database addressing a major gap in preclinical BCBM research.  \n- **Comprehensive documentation:** Detailed acquisition, reconstruction, labeling, and ethical compliance, promoting reproducibility.  \n- **Multi-modality & temporal richness:** Enables diverse imaging tasks including segmentation, registration, longitudinal progression modeling, and reconstruction.  \n- **Practical infrastructure:** Public dataset hosting, APIs, and tutorials lower access barriers for the imaging community.  \n- **Clinical and translational potential:** Provides a valuable foundation for deep-learning approaches in BCBM diagnosis and mechanical property prediction.\n\n### **3. Weaknesses**\n- **Limited methodological novelty:** The imaging pipeline primarily employs established methods (FBP, simple segmentation/registration), and the deep-learning models are off-the-shelf architectures used for illustration.  \n- **Insufficient methodological benchmarking:** Experimental sections demonstrate feasibility but lack comparison with alternative datasets or statistical robustness (error bars, uncertainty analysis).  \n- **Scope‚Äìfit risk:** As a dataset paper, the contribution may align better with *TMI‚Äôs Dataset and Benchmark Notes* if intended primarily as a resource rather than a methodological advance.  \n- **Presentation polish:** Occasional formatting, typographic inconsistencies, and figure readability issues.  \n- **Limited generalization evidence:** No demonstration on human data or cross-modality adaptation, leaving translational claims speculative.\n\n### **4. Major Comments**\n1. **Clarify methodological innovation:** Articulate explicitly how BoneMet advances medical imaging *methodology* rather than serving solely as a data resource (e.g., novel CT alignment pipeline, automated longitudinal registration protocols, or quantitative validation routines).  \n2. **Benchmarking and comparative analysis:** Provide systematic evaluations across algorithms, not just proof-of-concept. For instance, compare STA-ViT, CNN, and other temporal models statistically over multiple folds; quantify uncertainty across subjects.  \n3. **Dataset citation and maintenance:** Include DOIs, version control plans, and data access statements consistent with IEEE reproducibility guidelines.  \n4. **Broaden generalization discussion:** While murine data are preclinical, briefly assess representativeness and potential transfer to human imaging scenarios.  \n5. **Condense or restructure the manuscript:** Some content is descriptive (e.g., operational minutiae) and could move to supplementary material to emphasize scientific insights within main text.\n\n### **5. Minor Comments**\n- Correct spacing and capitalization in headings (e.g., ‚ÄúA N OPEN LARGE -SCALE MULTI -MODAL‚Ä¶‚Äù).  \n- Figures 1‚Äì4: improve font sizes and legends for readability.  \n- Provide consistent units (¬µm vs. micron, Gy vs. mGy).  \n- Ensure reference formatting conforms to IEEE style (e.g., numbered citations with bracketed numbers).  \n- Minor grammar issues (use ‚Äúdata are‚Äù instead of ‚Äúdata is‚Äù, etc.).  \n- Verify that ‚ÄúMMST-ViT,‚Äù ‚ÄúST-VAE,‚Äù and other abbreviations are defined upon first use.\n\n### **6. Overall Recommendation**\n**Recommendation:** üî∏ *Reject (Out of Scope / Low Novelty for TMI)* *or* consider resubmission as a **‚ÄúTMI Dataset or Resource Note‚Äù** if such track exists.  \n*Rationale:* Scientifically valuable and methodologically rigorous dataset, but limited novelty in imaging methodology relative to regular TMI research articles. The contribution is infrastructural rather than algorithmic.\n\nIf reoriented toward method development (e.g., automated registration toolkit or sparse-angle reconstruction algorithm with novel design and analytical validation), the paper could qualify for Major Revision consideration.\n\n### **7. Confidence Level:** 5/5  \nAssessment based on strong familiarity with medical imaging datasets, quantitative imaging, and deep-learning-based reconstruction methods.\n\n---\n\n**In summary:** *BoneMet* represents a high‚Äìimpact community resource with careful technical execution and strong potential to catalyze methodological advances. However, in its current form it is more a **dataset introduction paper** than a **methodological innovation manuscript**, placing it marginally below the typical novelty threshold for IEEE *Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces BoneMet, a large-scale, multi-modal dataset for breast cancer bone metastasis (BCBM) diagnosis and prognosis derived from murine models. The dataset comprises 67 terabytes of data from over 500 mice collected between 2019-2024, including 2D X-ray images, 3D CT scans, and medical records organized into six components: Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec (Section 2.1, Table 1). The authors demonstrate the dataset's applicability through experiments on BCBM diagnosis using Vision Transformers and CNNs, achieving accuracies ranging from 79.1% to 92.0% (Section 3.2-3.3), prognostic assessment of bone mechanical properties using generative models (Section 3.4), and sparse-angle CT reconstruction (Section 3.5). The dataset is publicly available through Hugging Face with accompanying APIs for image processing tasks.\n\n## Weaknesses\n\n‚Ä¢ **Limited experimental validation scope**: The evaluation demonstrates basic feasibility but lacks comprehensive benchmarking against established methods or clinical relevance assessment. The experiments primarily show that existing models can achieve reasonable performance on the dataset rather than demonstrating significant advances in BCBM diagnosis. Standard accuracy metrics (Table 3, Figures 5a-5d) do not establish clinical utility or compare against radiologist performance, limiting impact assessment.\n\n‚Ä¢ **Inadequate mathematical and technical rigor**: No mathematical formulations are provided for key processing steps, making reproducibility challenging. The segmentation threshold selection (75/255, Section 2.1, Page 4) lacks theoretical justification or validation studies. Registration methodology mentions mutual information maximization (Section A, Page 15) without presenting the optimization objective function or convergence criteria, hindering technical assessment and reproducibility.\n\n‚Ä¢ **Questionable generalizability to human clinical applications**: The murine model limitations are acknowledged but insufficiently addressed regarding translation potential. Significant physiological differences between mice and humans (disease progression timescales of weeks vs. years, Section G, Page 20) raise concerns about clinical relevance. The animal-level labeling approach (Section 3, Page 6) differs substantially from typical clinical diagnostic scenarios where lesion-level or time-specific predictions are needed.\n\n‚Ä¢ **Insufficient dataset quality assessment**: Missing critical analyses of data quality, annotation consistency, and potential biases. The 20.4% training-test accuracy gap with ViT (Table 3) suggests possible overfitting issues or data quality problems that remain unexplored. Inter-annotator agreement, segmentation accuracy validation, and systematic quality control measures are not reported (Section 2.1), raising concerns about dataset reliability.\n\n‚Ä¢ **Methodological limitations in experimental design**: The experimental setup lacks statistical rigor and proper controls. Cross-validation is mentioned (Section 3.1) but detailed statistical analysis, confidence intervals, or significance testing are absent from results (Tables 3-5). Comparison baselines are limited, and the choice of evaluation metrics may not reflect clinical decision-making requirements, reducing the validity of performance claims.\n\n## Suggestions for Improvement\n\n‚Ä¢ **Expand experimental validation comprehensively**: Conduct thorough benchmarking against state-of-the-art BCBM diagnosis methods and include radiologist performance comparisons. Evaluate clinical relevance through collaboration with medical professionals and assess diagnostic accuracy at clinically meaningful thresholds. Implement more diverse evaluation metrics including sensitivity/specificity analysis and receiver operating characteristic curves to better demonstrate clinical utility.\n\n‚Ä¢ **Enhance mathematical formalization and technical documentation**: Provide complete mathematical formulations for all image processing steps, including segmentation algorithms, registration optimization objectives, and reconstruction methods. Justify threshold selection through systematic validation studies and present convergence criteria for iterative procedures. Include detailed algorithmic descriptions and parameter sensitivity analyses to ensure reproducibility and technical soundness.\n\n‚Ä¢ **Address generalizability concerns systematically**: Conduct comprehensive analysis of murine-to-human translation challenges and provide validation strategies for clinical applications. Develop time-scale normalization approaches to bridge the temporal gap between mouse and human disease progression. Implement lesion-level labeling and evaluation protocols that better match clinical diagnostic scenarios and demonstrate translational potential through pilot human studies or existing human dataset comparisons.\n\n‚Ä¢ **Implement rigorous dataset quality assessment**: Establish comprehensive quality control protocols including inter-annotator agreement studies, systematic bias analysis, and data consistency validation. Address overfitting concerns through detailed data quality investigation and implement cross-dataset validation approaches. Develop standardized annotation guidelines and provide quantitative quality metrics for all dataset components to ensure reliability and scientific rigor.\n\n‚Ä¢ **Strengthen experimental methodology and statistical analysis**: Implement proper statistical testing frameworks with confidence intervals, significance tests, and multiple comparison corrections. Expand baseline comparisons to include more diverse and recent methods, and develop clinically-relevant evaluation protocols. Provide detailed cross-validation results, learning curve analyses, and failure case studies to better characterize model performance and limitations across different scenarios.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces the BoneMet dataset, a large-scale, multi-modal resource derived from a murine model of breast cancer bone metastasis (BCBM). The dataset encompasses over 67 terabytes of data, including 2D X-ray images, 3D CT scans, and detailed biological records, collected from more than 500 mice over five years. The authors argue that this dataset can significantly advance the development and evaluation of AI models for BCBM diagnosis and prognosis. The manuscript is well-structured and clearly articulates the significance of the dataset in addressing the limitations of existing human datasets.\n\n## Major Comments\n1. Novelty and Positioning: While the BoneMet dataset is novel and addresses the scarcity of large-scale, high-resolution, multi-modal datasets for BCBM, the manuscript could benefit from a more thorough comparison with existing datasets, particularly those that also include longitudinal imaging and detailed biological data. This would help clarify how BoneMet uniquely contributes to the field beyond its scale and resolution.\n\n2. Evaluation Design: The evaluation experiments are limited to a subset of deep learning models and do not extensively cover the breadth of potential applications mentioned. The manuscript should expand its experimental scope to include a wider range of deep learning architectures and tasks, such as sparse-angle CT reconstruction and generative modeling, to more comprehensively validate the dataset's utility.\n\n3. Comparisons: The manuscript compares the BoneMet dataset with existing datasets but lacks a detailed discussion of how it performs relative to these datasets in terms of specific metrics and tasks. Including more comparative analyses would strengthen the argument for the dataset's superiority and utility.\n\n4. Reproducibility: While the BoneMet package and APIs are described, the manuscript could benefit from a more detailed description of the preprocessing steps, hyperparameter settings, and data splitting strategies used in the experiments. Providing complete code and data processing pipelines would significantly enhance reproducibility.\n\n## Minor Comments\n1. Clarity in Figures: Some figures, such as Figure 3, are cluttered and could be improved by showing fewer representative slices with zoomed-in regions for better readability.\n   \n2. Notation Consistency: The notation for the forward operator is inconsistent in Section 2.1. Clarifying and standardizing the notation would improve the manuscript's clarity.\n\n3. Acronym Definitions: Several acronyms, such as \"R=4,\" are used without definition. Defining these acronyms would enhance the manuscript's accessibility.\n\n4. Typographical Errors: Minor typographical errors, such as \"k-spacce\" and \"undersampling maskes,\" should be corrected for better readability.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: the lack of large-scale, high-resolution, multi-modal datasets for BCBM diagnosis and prognosis. The BoneMet dataset is a substantial contribution to the field, offering high-resolution imaging data and detailed biological records from a murine model. The novelty and innovation of the dataset are clear, and the preliminary experimental results demonstrate its potential for advancing AI-driven solutions. However, the evaluation is somewhat limited in scope and could benefit from more comprehensive comparisons and a broader range of applications. The reproducibility of the dataset and experimental setup is generally good but could be further enhanced by providing more detailed methodological descriptions and complete codebases. Overall, the manuscript makes a strong case for the significance and potential impact of the BoneMet dataset, but the evaluation could be strengthened to fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis, broaden the evaluation scope to cover a wider range of deep learning tasks, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Fudong Lin",
      "Jason Jiang",
      "Liyun Wang",
      "Shubo Wang",
      "Tiankuo Chu",
      "Wiley Jia-Wei Gong",
      "Xu Yuan"
    ],
    "url": "pdfs/iclr.cc-2025-conference_c17c3d4a8e2210abad05684756a47d326d0f63e8.pdf",
    "remote_url": "https://openreview.net/pdf/c17c3d4a8e2210abad05684756a47d326d0f63e8.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "unsupervised anomaly detection",
      "medical images",
      "contrastive reverse distillation",
      "student-teacher"
    ],
    "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors propose a new method for anomaly detection in medical data setting using contrastive reverse distillation. The method uses knowledge distillation as it's foundation, implementing pre-trained teachers models along with a student model trained to either minimize or maximize its distance from the teachers in a feature-map space. They additionally utilize a scale adaptive mechanism to allow for better generalizations across different sizes of anomaly.\n\nTheir method's specific construction utilizes two teacher models with the same weights, one \"bad\" and one \"good\". The good model is given benign images while the bad model is given images with synthetic anomalies. The student model is then trained to maximize its cosine similarity with the per-layer features of the good teacher while minimizing the cosine similarity with the bad teacher. At evaluation time anomaly scores are computed by comparing the scaled, per-layer feature spaces of the good teacher against those of the student. Inputs with high levels of dissimilarity between the good model and student are given high anomaly scores.\n\nTheir evaluation against prior work is very comprehensive, comparing against a total of 18 other detectors. They out perform all other detectors on all three datasets (lung, brain, and skin anomalies) in terms of their main metrics (AUC, F1, and Accuracy). The authors further perform ablations with respect to their main contributions of Contrastive Reverse Distilation and Scale Adaptive Mechanisms.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n* Anomaly detection is somewhat outside my area of expertise, but the paper seems to frame itself well within the current body of literature. \n* The paper is well written and easy to follow. Figures are also informative and easy to understand.\n* The idea of contrastive reverse distillation seems novel (to the best of my knowledge) and intuitive. I'd like to see how future works can iterate upon this framework in other settings or with improved constructions.\n* Motivation is well established for the usage synthetic anomalies, and the choice of simplex noise is clever.\n* Empirical analysis is very comprehensive in terms of datasets, baselines, and ablations.\n* The proposed method out-performs all baselines in terms of AUC, F1, and Accuracy.\n* The proposed method scales well with model architecture size, which is a good sign that the method is sensible.\n\n### Weaknesses\n\nIn general, within my somewhat limited knowledge of this sub-field, I like this paper. In spite of that I think there are a few things the authors may want to consider to further strengthen their work.\n\nI think when discussing detection and AUC scores it is important to give them context within the given domain. See section B of [1] for some example discussion on this topic from a different field. I'd like the authors to consider the following questions:\n\n* Is it more important to maximize the true positive rate in spite of inducing some false positives? \n* Is it better to minimize false positives to increase confidence in the case of a positive prediction?\n* How do you anticipate doctors using these tools in their analysis?\n\nGiven this, it would also be nice if the authors could plot some of the ROCs between their method and some of the baselines (maybe including all of them is too visually messy, but at least the top 5). This way people can compare their method to the baselines at different levels of false positive rate.\n\nI think it would be good if the authors could include some extended discussion, either in the main body or the supplementary material, analyzing the more subtle features of their results. See the next section of my review \"Questions\" for some examples of what I think they can further analyze.\n\nLastly, the results of table 1 are good, but a bit overwhelming. I think it would help if the second best score in each metric is underlined and if the methods are visually grouped based on detector type (e.g. RD4AD, RD++, and ReContrast could be grouped together). Optionally it would be nice if all experiments were averaged over a few runs and given standard deviations or $\\pm$ scores, at least in the supplementary material. I understand if these experiments are too extensive to replicate too many times, though.\n\n [1] \"Membership Inference Attacks From First Principles\" https://arxiv.org/abs/2112.03570\n\n### Questions\n\nHere I include some questions for the authors to consider.\n\n* Why does ISIC seem like the hardest dataset for all the detectors? Why is Brain Tumor the easiest?\n* Why is SAM more impactful in the Brain Tumor dataset while it seems minimally impactful in the RSNA dataset in table 2? Is this related at all to the results of table 3 - where Gaussian noise performs similar to simplex noise on RSNA, but much worse on Brain Tumor?\n* Can the method achieve better generalization by using a mixture of synthetic noise types, perhaps 10% gaussian, 90% simplex? \n* Can we improve performance by including some real world data in the training set, or will that harm generalization?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel approach for anomaly detection in medical data through a technique termed contrastive reverse distillation. By employing knowledge distillation, the authors utilize pre-trained teacher models alongside a student model, optimizing its feature-map distances to improve anomaly detection capabilities. The study is well-articulated, demonstrating clarity in methodology and empirical results across various datasets.\n\n### Major Comments\n1. The novelty of the approach appears robust, particularly the concept of contrastive reverse distillation. However, it would be beneficial for the authors to provide context for their detection and AUC scores within the relevant domain.\n2. Consideration of the clinical implications is necessary. Questions to address include:\n   - Is maximizing the true positive rate more critical than managing false positives?\n   - How might clinicians utilize these tools in practice?\n3. The inclusion of ROC curves contrasting their method with the top five baseline detectors would facilitate comparative analysis.\n4. Extended discussions on the subtleties of the results should be incorporated in either the main text or supplementary materials.\n5. Table 1 results are somewhat overwhelming. Underlining the second-best scores per metric and grouping methods by detector type would enhance readability. Additionally, providing averages and standard deviations of experiments, if feasible, would improve the clarity of the findings.\n\n### Minor Comments\n- Figures are well-designed and informative, contributing positively to the overall readability of the manuscript.\n- There are some grammatical errors that need correction; a thorough proofreading is recommended.\n- The choice of simplex noise for generating synthetic anomalies is noted as a clever decision.\n\n### Summary Paragraph\nThis study is significant because it addresses a critical challenge in medical anomaly detection with an innovative method that shows strong empirical performance. While the evaluation against baselines is comprehensive and results indicate notable advancements in metrics like AUC, F1, and Accuracy, additional contextualization of results within the practical healthcare application would strengthen the manuscript. The reproducibility of the findings is commendable, yet further details on the methodology and potential variability in results would enhance the robustness of the claims.\n\n### Decision Recommendation\nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel framework for anomaly detection in medical imaging based on *contrastive reverse distillation*. The approach builds upon knowledge distillation, employing two teacher models‚Äîone ‚Äúgood‚Äù trained on benign images and one ‚Äúbad‚Äù using synthetic anomalies‚Äîand a student network optimized to align with the good teacher‚Äôs features while diverging from the bad one. A scale-adaptive mechanism further enhances generalization across anomaly sizes. The paper is clearly written, with informative figures and strong contextual integration within existing literature. Overall presentation and methodological soundness are satisfactory.  \n\n**Major Comments**  \n1. **Contextualization of Detection Metrics** ‚Äì The paper reports strong gains in AUC, F1, and accuracy over 18 competing methods on lung, brain, and skin datasets. However, it would benefit from contextual interpretation of these metrics within clinical practice. Discussion should address whether maximizing true positives at the cost of false positives is preferable, or whether minimizing false alarms is more critical for diagnostic reliability.  \n2. **ROC Curve Visualization** ‚Äì Including ROC curves for key baselines (e.g., top five methods) would help readers interpret the model‚Äôs relative performance across operating points and false positive rates.  \n3. **Deeper Analysis of Results** ‚Äì The results in Table 1, though impressive, are somewhat dense. Highlighting second-best results and grouping methods by detector type (e.g., RD4AD, RD++, ReContrast) would improve interpretability. Including standard deviations or repeated-run averages, even in supplementary materials, would strengthen empirical robustness.  \n4. **Further Discussion Opportunities** ‚Äì The authors could explore dataset-specific behaviors (e.g., ISIC being hardest, Brain Tumor easiest) and investigate why the Scale Adaptive Mechanism‚Äôs impact varies across datasets. Additional experiments on combining synthetic noise types or introducing limited real data might shed light on generalization properties.  \n\n**Minor Comments**  \n- Ensure consistent formatting of tables and figures.  \n- Clarify the role of simplex vs. Gaussian noise in the ablation tables.  \n- Consider placing extended analysis or supplemental discussions in the appendix.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in advancing anomaly detection techniques through a creative contrastive distillation setup, offering promising results across multiple modalities. Its innovation lies in the use of ‚Äúgood‚Äù and ‚Äúbad‚Äù teacher networks and the scale-adaptive mechanism. The evaluation is broad and convincing, yet interpretive context and reproducibility details (e.g., standard deviations, multiple runs) could be strengthened. Overall, the study is rigorous and well-presented, with opportunities for additional context and experimental transparency.  \n\n**Decision Recommendation:** **Minor Revision** ‚Äì The paper is solid and well-executed, needing only expanded discussion and improved result presentation for full clarity.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nTwo issues arise in reverse knowledge distillation for medical imaging, an inability to properly distinguish between different features and an inability to deal with different scales. This work proposes two methods to address these issues: a contrastive student-teacher learning approach that involves using both a ‚Äúgood‚Äù and a ‚Äúbad‚Äù teacher, as well as a scale adaptation mechanism. Results demonstrate that on almost every dataset the proposed approach is able to outperform existing techniques.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well written and the ideas are easy to follow. I appreciate how the authors identify the two problems facing medical imaging in this domain and then propose solutions. The new approach outperforms all other methods that the authors compare with in table 1.\n\n### Weaknesses\n\nIssue 1: This paper doesn‚Äôt have any formal proofs or theorems to guarantee its effectiveness, therefore the strengths of the paper must rely on the experiments and empirical results. For the datasets I will admit as a review I am not very familiar with them. For example, on the image dataset CIFAR-10, a few years ago a jump from 93% to 98% was considered significant. Today an increase from 99% to 99.9% on CIFAR-10 would not even be considered a major contribution. Likewise, for these datasets it is hard to determine whether a jump of 3.01% and 4.38% (as the authors show) is worthy of publication. Could the authors please clarify how much other papers pushed the accuracy forward in other works, as compared to the increase their method is offering?\n\nIssue 2: The authors say they will release their code upon publication, but I did not see any code given in the supplemental material. This might be an oversight on my part, but where is the code? I would like to see a good faith gesture of releasing an anonymous github for the code or at least a zip file of the code. Too many papers claim SOTA results and never release code. In this day and age it is simply unacceptable to continue this trend. \n\nIssue 3: The authors also don‚Äôt mention anything about the runtime of their algorithm. How does the student-teacher setup and training time compare to other existing methods in the field? While this is not a HUGE concern, I think it would be good to have some discussion and analysis of this. For example could you show the training time complexity of your algorithm and the next two best methods in the field? Or even an experimental runtime of the training in seconds of your algorithm and the next two best methods? Without this I feel we don‚Äôt have the complete story when comparing methods. \n\nIssue 4: Please see my minor comments about uncited claims in the papers. For acceptance I would like to see citations for all the claims you have made. \n\nIssue 5: I don‚Äôt like the terminology ‚Äúgood‚Äù and ‚Äúbad‚Äù teacher. It is very non-technical and hard to follow. Can you please change the terminology in your paper and name the teachers more appropriately? \n\nMinor Comments:\n\n=Line 36, ‚Äúanomalous samples is often prohibitively expensive and time-consuming‚Ä¶‚Äù I believe this is correct but I want to see a citation to backup this claim?\n\n=Line 45, ‚Äúthey tend to generalize too well, thereby risking the reconstruction of abnormal regions.‚Äù Citation for this claim? \n\n=Table 1 has many entries that are unsorted (aside from the bolded number indicating the best method). It would be better if this table was sorted in order of increasing AUC for the methods.\n\n=Line 167 ‚ÄúMedical images typically exhibit a power law distribution of frequencies, with lower frequency components dominating the image content.‚Äù Please give a citation for this bold claim?\n\n### Questions\n\nQ1: Please address each of my issues 1-5. If they are adequately addressed I would be willing to increase my score for the paper accordingly.\n\nEDIT: The reviewers have address my concerns adequately. I have update the score of my review accordingly.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript addresses challenges in reverse knowledge distillation for medical imaging, specifically the difficulties in distinguishing between different features and managing varying scales. The authors propose two methods: a contrastive student-teacher learning framework utilizing ‚Äúgood‚Äù and ‚Äúbad‚Äù teachers, as well as a scale adaptation mechanism. The results suggest that this approach outperforms existing techniques across almost all tested datasets, demonstrating promise and potential contributions to the field.\n\n### Major Comments\n1. **Lack of Formal Proofs**: The manuscript lacks formal proofs or theorems supporting the effectiveness of the proposed methods, relying heavily on empirical results. Clarity on the significance of improvements in accuracy, particularly regarding benchmarks like CIFAR-10, is necessary. Comparisons with advancements made in other works should be elucidated to assess the novelty of the presented results.\n  \n2. **Code Availability**: Although the authors mention releasing code upon publication, it is not provided in the supplemental material. The submission would benefit from a commitment to share code, ideally via a public repository like GitHub or through a zip file, to promote transparency and reproducibility in research.\n\n3. **Runtime Analysis**: There is no discussion regarding the runtime efficiency of the proposed algorithm. Comparative analysis of training time against existing methods would enhance the study‚Äôs rigor. Recommendations include showcasing training time complexity or providing experimental runtime data.\n\n4. **Citations for Claims**: Several claims in the manuscript are presented without citations. Adequate references should support all assertions to meet rigorous academic standards.\n\n5. **Terminology Issues**: The terms ‚Äúgood‚Äù and ‚Äúbad‚Äù teacher are non-technical and may lead to confusion. A more precise nomenclature should be adopted for clarity.\n\n### Minor Comments\n- Line 36: Please provide a citation for the claim regarding the expense and time consumption of handling anomalous samples.\n- Line 45: A citation is needed for the statement about generalization risks affecting reconstruction of abnormal regions.\n- Table 1: Many entries should be sorted in ascending order of AUC for clarity.\n- Line 167: A citation is required for the assertion about the power law distribution of frequencies in medical images.\n\n### Summary Paragraph\nThe paper presents an innovative approach to reverse knowledge distillation, contributing to the field of medical imaging. While the study demonstrates significant potential and shows improvement over existing methods, weaknesses include a lack of formal proofs and runtime analysis, absent documentation of code availability, and insufficient citations for several claims. These issues affect the reproducibility of the work, which is crucial given the technological context of the research.\n\n### Decision Recommendation\nMajor revision is recommended, contingent on addressing the outlined comments and concerns adequately.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses two key challenges in reverse knowledge distillation for medical imaging: distinguishing between heterogeneous features and handling variations in image scale. To overcome these issues, the authors propose a contrastive student‚Äìteacher framework employing both a ‚Äúgood‚Äù and a ‚Äúbad‚Äù teacher, along with a scale adaptation mechanism. Experimental results indicate consistent improvements over existing approaches across nearly all tested datasets. The paper is clearly written and easy to follow, and the proposed methodology appears thoughtfully motivated.\n\n---\n\n**Major Comments**  \n1. **Empirical significance of results:** The paper lacks theoretical justification or formal proofs of effectiveness, relying entirely on empirical evidence. The reported performance gains (3.01%‚Äì4.38%) may be modest depending on dataset difficulty. The authors should contextualize these improvements by comparing with prior work‚Äîhow does this magnitude of gain compare to historical progress on the same datasets?  \n2. **Code availability and reproducibility:** The manuscript states that code will be made public upon publication, but no implementation is currently available in the supplementary materials. A temporary anonymous code release or archive (e.g., GitHub or zip file) would demonstrate good faith and strengthen reproducibility.  \n3. **Computational and runtime analysis:** The paper does not discuss runtime or complexity. A comparison of training time or computational load between the proposed method and competing approaches is necessary to complete the evaluation.  \n4. **Citation completeness:** Several assertions lack citations. All major claims‚Äîparticularly those describing dataset properties or general trends in medical imaging‚Äîshould be appropriately referenced.  \n5. **Terminology choice:** The use of ‚Äúgood‚Äù and ‚Äúbad‚Äù teacher is imprecise and informal. More technical terminology should be adopted for clarity.\n\n---\n\n**Minor Comments**  \n- Line 36: Add citation supporting the claim about the cost of collecting anomalous samples.  \n- Line 45: Provide a reference for the statement on over-generalization leading to reconstruction of abnormalities.  \n- Table 1: Consider sorting entries by increasing AUC to improve readability.  \n- Line 167: Add citation for the statement regarding the power-law frequency distribution in medical images.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work introduces an interesting extension to teacher‚Äìstudent frameworks through contrastive guidance and scale adaptation, showing promising empirical results. While the method‚Äôs practical value seems reasonable, its broader significance remains uncertain without contextualized evaluation. The innovation is conceptually sound but limited by informal terminology and lack of theoretical grounding. Evaluation is largely empirical yet needs further runtime and comparative analysis. Reproducibility is currently weak due to missing code but can be easily improved with open sourcing.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The paper is well written and addresses a relevant problem, but key issues related to empirical context, code release, and completeness of analysis must be resolved before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a twofold approach: on one hand, data augmentation for unlabeled information is achieved by injecting noise, which then serves as the basis for contrastive learning. On the other hand, parameters are extracted at each layer to achieve multi-scale information fusion, enabling the model to better distinguish between positive and negative samples.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe methodology section of this paper is clearly written and well-presented.\n\n### Weaknesses\n\n1. In terms of innovation, this article presents two components that are commonly seen in other works. A straightforward suggestion would be for the authors to explore a more adaptable data augmentation method to address the issue raised in ‚ÄúQuestion 1.‚Äù\n\n2. Regarding the writing, the **Introduction** section of this article is somewhat confusing. Firstly, the paragraphing is suboptimal. In the second paragraph of the introduction, the author discusses the shortcomings of anomaly detection models based on generation. The third paragraph explains existing work aimed at improving these models, followed by an exploration of the development of self-supervised learning. Although self-supervised learning indeed enriches unsupervised learning within deep learning, the third paragraph would be clearer if it unified these two parts, focusing on how self-supervised learning specifically addresses the issues raised in the second paragraph. Secondly, and more importantly, an excessive portion of the introduction is dedicated to discussing related work, lacking sufficient focus on the contributions of this paper. Maybe you could merging the discussion of existing work and self-supervised learning, followed by a clear transition to the paper's contributions. Additionally, you could recommend what key points about the paper's contributions should be emphasized earlier in the introduction.\n\n### Questions\n\n1. In the noise generation process, the intensity of each position is sampled from a uniform distribution. However, in real-world medical imaging, anomalous regions in disease cases appear in specific locations; for example, abnormalities in a chest image typically do not appear outside body regions. For instance, you could suggest incorporating prior knowledge about typical anomaly locations in different types of medical images.\n\n2. Additionally, is each test sample measured at the image level or at the patch level? If it is at the image level, the positive-to-negative anomaly ratio in the dataset is relatively high, which does not quite align with the extremely imbalanced nature of anomaly detection tasks.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a dual approach that incorporates data augmentation for unlabeled information through noise injection, serving as a foundation for contrastive learning. Additionally, it extracts parameters from each layer to enable multi-scale information fusion, allowing the model to improve its discrimination between positive and negative samples. The methodology is clearly articulated; however, several aspects of clarity and innovation require attention.\n\n### Major Comments\n1. **Innovation**: The components introduced in this paper are largely reminiscent of existing work. To enhance novelty, the authors could consider developing a more flexible data augmentation method in response to the concerns highlighted in ‚ÄúQuestion 1.‚Äù\n\n2. **Writing Clarity**: The **Introduction** is confounding in structure. The second paragraph discusses the limitations of anomaly detection models predicated on generation, while the third paragraph shifts to existing efforts aimed at enhancing these models and self-supervised learning. It would benefit from a more cohesive narrative that integrates these concepts. Specifically, the transition to self-supervised learning's contributions should bridge the deficiencies noted in the second paragraph. Moreover, the introduction devotes excessive attention to related work without sufficiently emphasizing the article's unique contributions. Merging the discussions of existing work with self-supervised learning and clearly stating the paper's contributions earlier would improve clarity.\n\n### Minor Comments\n- Consider revising the paragraph structure in the **Introduction** for better flow.\n- Clarify which aspects of the paper's contributions should be prioritized in initial discussions.\n\n### Summary Paragraph\nIn relation to TMI editorial criteria, while the manuscript demonstrates sound methodology and presents a clear framework for data augmentation and contrastive learning, it falls short in innovation and clarity. The discussion of related works overshadows the manuscript‚Äôs contributions, compromising its significance. The review of noise generation processes raises valid concerns about real-world applicability in medical imaging, calling for further consideration of anomaly locations. Clarity in writing must be improved to enhance overall evaluation and reproducibility.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a twofold method for medical image anomaly detection. First, it introduces a noise-based data augmentation strategy for unlabeled data that supports contrastive learning. Second, it performs multi-scale information fusion by extracting parameters across layers, allowing better discrimination of positive and negative samples. While the methodology is generally clear and well-presented, the paper‚Äôs innovation and writing structure could be strengthened to improve overall clarity and impact.\n\n**Major Comments**  \n1. **Novelty and Innovation:** The proposed dual approach combines two components‚Äînoise-based contrastive learning and multi-scale feature fusion‚Äîthat are already common in the literature. The paper would benefit from exploring more adaptable or advanced data augmentation strategies to enhance originality and address limitations noted in ‚ÄúQuestion 1.‚Äù  \n2. **Clarity and Organization of the Introduction:** The Introduction lacks coherence and focus. Paragraph transitions do not clearly connect the identified challenges of generative anomaly detection with how self-supervised learning remedies these shortcomings. Large portions are devoted to related work, diluting emphasis on the paper‚Äôs own contributions. The authors should consider merging discussions on prior work and self-supervised learning, then explicitly highlighting their novel contributions earlier in the section.  \n3. **Evaluation Design:** Questions remain about the experimental setup. The noise intensity is sampled uniformly without accounting for the typical spatial priors of anomalies in medical images. Incorporating anatomical constraints or prior knowledge about expected anomaly locations could yield more realistic augmentation.  \n4. **Task Definition and Data Balance:** It is unclear whether evaluation is performed at the image or patch level. If conducted at the image level, the dataset‚Äôs positive-to-negative ratio may be too high, which could misrepresent the true imbalance characteristic of anomaly detection tasks.\n\n**Minor Comments**  \n- The methodology section is clearly presented and technically understandable.  \n- Paragraphing in the Introduction could be optimized for readability.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript addresses a relevant problem in unsupervised medical image anomaly detection. Its methods are conceptually sound but rely on relatively standard strategies, limiting their innovation. The experimental design would benefit from careful consideration of real-world data distributions and clearer articulation of contributions. Reproducibility appears feasible given the described methods, though more details on evaluation protocols would strengthen it.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚úÖ**  \n   The manuscript introduces a novel *scale-aware contrastive reverse distillation* framework for **unsupervised medical anomaly detection**. The innovation is methodological rather than clinical‚Äîit involves a new training paradigm combining contrastive learning and knowledge distillation with scale adaptation. This fits well within IEEE‚ÄØTMI‚Äôs scope in machine learning methodology and image analysis for medical imaging.\n\n2. **Novelty & Contribution Level ‚Äì Moderate to High**  \n   The work extends the reverse distillation concept with two methodological contributions: (1) a *contrastive student‚Äìteacher learning* mechanism that enhances representation discrimination through synthesized pseudo‚Äëabnormal data, and (2) a *scale‚Äëadaptation module* that dynamically weights multi‚Äëscale distillation losses. Prior art (RD4AD, RD++, ReContrast) did not combine these aspects. The novelty relative to RD++ and ReContrast appears incremental but non‚Äëtrivial‚Äîthe contrastive formulation and scale weighting address recognized shortcomings (feature indistinctness, scale sensitivity).  \n\n3. **Technical and Experimental Rigor ‚Äì High but with Some Gaps**  \n   Experiments are extensive across three imaging modalities (X‚Äëray, MRI, dermoscopy), comparing against over ten baselines with thorough ablations and backbone tests. The method appears technically sound and reproducible (code link provided). However, further clarification would aid rigor:  \n   - justification of simplex‚Äënoise realism relative to anatomical priors;  \n   - statistical significance of reported AUC improvements;  \n   - verification that no anomalous cases leak into training (dataset splits need confirmation).  \n\n4. **Clarity and Presentation ‚Äì Generally Clear**  \n   Writing is concise and logically structured.  \n   - Figures are informative (architecture, histograms, ROC curves) but could benefit from higher resolution and standardized color schemes for accessibility.  \n   - Some typographic and formatting inconsistencies remain (‚Äúf eature d iscriminability,‚Äù etc.).  \n\n5. **Ethical and Reproducibility Compliance ‚Äì Mostly Adequate**  \n   - All datasets are public; authors cite IRB/ethics sources appropriately.  \n   - Link to public code is provided.  \n   - No mention of anonymization within generated examples, but given datasets are de‚Äëidentified, risk is low.  \n   - Strong reproducibility practices (detailed algorithm pseudo‚Äëcode included).  \n\n---\n\n**Phase 2: IEEE‚ÄØTMI Review Report**\n\n### 1. Summary\nThe manuscript proposes a *Scale‚ÄëAware Contrastive Reverse Distillation (SACRD)* approach for unsupervised anomaly detection in medical images. Building on reverse distillation frameworks, the authors introduce (i) a contrastive student‚Äìteacher learning strategy that leverages a pair of ‚Äúclean‚Äù and ‚Äúnoisy‚Äù teachers to encourage discriminative representations, and (ii) a scale‚Äëadaptation module that dynamically weights loss contributions across feature scales. Synthetic anomalies are generated using *simplex noise* to produce structured perturbations. Evaluated on RSNA chest X‚Äërays, a brain‚Äëtumor MRI dataset, and the ISIC 2018 dermoscopy dataset, the proposed method achieves state‚Äëof‚Äëthe‚Äëart AUC, F1, and accuracy compared with 15+ recent baselines.\n\n### 2. Strengths\n- **Methodological innovation:** Combines contrastive and reverse‚Äëdistillation paradigms with scale weighting‚Äîan elegant enhancement addressing feature discriminability and scale variation.  \n- **Comprehensive experimentation:** Multiple datasets, ablation studies (contrastive loss, SAM module, noise type and intensity, backbone).  \n- **Strong results:** Reported AUC improvements of 3‚Äì4‚ÄØ% vs. best baselines and clear qualitative separation of normal vs. abnormal distributions.  \n- **Good reproducibility:** Public code, detailed architecture, and pseudo‚Äëcode.  \n- **Generalizability:** Demonstrated across modalities, reinforcing potential broad utility.  \n\n### 3. Weaknesses\n- **Incremental nature:** Builds directly upon RD4AD/RD++ with architectural tweaks rather than a fundamentally new learning principle; may be near TMI‚Äôs lower novelty bound.  \n- **Synthetic anomaly realism:** Simplex noise is a convenient proxy but lacks clinical plausibility; justification or qualitative examples of realism would strengthen claims.  \n- **Statistical validation:** No confidence intervals or significance analyses on AUC differences.  \n- **Limited discussion of failure cases:** Insight into missed anomalies or false‚Äëpositives is absent.  \n- **Figure/formatting issues:** Some blurry figures and inconsistent typography.  \n\n### 4. Major Comments\n1. **Novelty positioning:** Clearly delineate conceptual differences from RD++ and ReContrast in the Introduction and Discussion, ideally through a table or schematic emphasizing algorithmic and loss‚Äëfunction differences rather than implementation details.  \n2. **Statistical analysis:** Include significance testing (e.g., DeLong test) or bootstrapped confidence intervals for AUC to demonstrate robustness of reported improvements.  \n3. **Synthetic anomaly justification:** Provide qualitative comparison of simplex‚Äënoise anomalies versus realistic pathologies; discuss limitations regarding anatomical plausibility.  \n4. **Failure case analysis:** Present sample anomaly maps illustrating false detections or missed lesions to help readers understand boundary conditions of the model.  \n5. **Clinical interpretability:** Comment on how contrastive and scale‚Äëaware representations might translate to localizing pathology or assisting clinician trust.  \n\n### 5. Minor Comments\n- Correct text artifacts and spacing (e.g., ‚Äúf eature d iscriminability‚Äù).  \n- Improve figure legends and resolution.  \n- Standardize abbreviations (e.g., SAM vs. S.A.M.).  \n- Provide exact runtime environment in reproducibility statement (CUDA/cuDNN versions).  \n- Ensure that all referenced works have full bibliographic information (some abbreviations inconsistent).  \n\n### 6. Overall Recommendation  \n**Recommendation:** **Major Revision**  \n\n**Rationale:**  \nThe paper demonstrates strong technical execution and promising performance but remains a methodological refinement over existing reverse‚Äëdistillation models rather than a fundamentally novel concept. With clearer differentiation from prior art, additional statistical support, and expanded discussion of synthetic‚Äëanomaly validity, it could reach TMI‚Äôs publishable standard.\n\n### 7. Confidence Level  \n**4‚ÄØ/‚ÄØ5 ‚Äì High Confidence**  \nBased on familiarity with reverse‚Äëdistillation anomaly detection literature and medical imaging machine‚Äëlearning methods.\n\n---\n\n**Summary Judgment:**  \nA rigorous and well‚Äëexecuted study presenting an incremental yet meaningful enhancement to unsupervised anomaly detection. The work is within TMI scope and could be publishable after strengthened justification of novelty, statistical rigor, and interpretability.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes a scale-aware contrastive reverse distillation model for unsupervised medical anomaly detection. The method addresses two limitations of existing reverse distillation approaches: insufficient feature discriminability and inability to handle anomaly scale variations. The approach introduces a contrastive student-teacher learning framework with dual encoding pathways - a \"clean\" teacher processing normal data and a \"noisy\" teacher processing synthesized anomalies created using simplex noise (Section 2.2.2). A scale adaptation mechanism generates input-specific weights to handle multi-scale anomalies (Equation 3-4). The method is evaluated on three medical datasets (RSNA, Brain Tumor, ISIC) and achieves state-of-the-art performance, with AUC improvements of 3.01%, 4.38%, and 3.08% respectively over second-best methods (Table 1).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and unclear notation**\n  - Equation 2 presents a ratio-based loss without theoretical justification for why this specific formulation enhances discrimination compared to additive or weighted alternatives\n  - The reshaping operation mentioned in Section 2.2.1 for converting uk and vk to 1D representations lacks mathematical specification of dimensions and aggregation method\n  - The small epsilon value in the denominator (Equation 2) is introduced without analysis of its impact on gradient stability or convergence properties\n  - Scale descriptor normalization in Equation 3 uses softmax, but no justification is provided for why this constraint improves performance over other normalizations\n\n‚Ä¢ **Limited experimental rigor and insufficient baseline comparisons**\n  - Table 1 shows inconsistent performance patterns across methods without statistical significance testing or confidence intervals to validate claimed improvements\n  - Ablation study in Table 2 tests only two components, missing analysis of individual contributions of simplex noise type, noise intensity, and architectural choices\n  - The comparison with RD++ in Section 2.5 lacks experimental validation on the same datasets to substantiate claimed fundamental differences\n  - Figure 3 shows noise intensity analysis only on RSNA dataset, missing validation on other datasets where scale variations differ significantly\n\n‚Ä¢ **Methodological limitations in synthetic anomaly generation**\n  - Section 2.2.2 uses fixed hyperparameters (Œ≥ = 0.6, Œª = 0.2, six octaves) for simplex noise without dataset-specific optimization or sensitivity analysis\n  - The assumption that medical anomalies follow power law distributions lacks empirical validation or citation support in the manuscript\n  - Table 3 compares only Gaussian vs. simplex noise, missing comparison with other structured noise types or learnable anomaly synthesis methods\n  - Algorithm 1 shows random spatial sampling for noise placement without consideration of anatomically relevant regions in medical images\n\n‚Ä¢ **Incomplete evaluation and limited generalizability analysis**\n  - The three datasets represent limited medical imaging diversity, missing common modalities like CT, ultrasound, or histopathology mentioned in related work\n  - Table 4 backbone comparison lacks analysis of computational trade-offs or performance scaling with model complexity\n  - Figure 2 qualitative analysis shows only score distributions without spatial anomaly localization examples or failure case analysis\n  - Discussion in Section 3.4 acknowledges dataset-specific performance differences but provides no systematic analysis of when the method succeeds or fails\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical foundations and provide clearer notation**\n  - Provide theoretical analysis or empirical justification for the ratio-based loss formulation in Equation 2, comparing against alternative contrastive loss designs\n  - Specify exact mathematical operations for feature reshaping, including dimensional analysis and impact on spatial information preservation\n  - Conduct sensitivity analysis of epsilon values and their effect on training stability, providing guidance for hyperparameter selection\n  - Compare softmax normalization against alternative constraints for scale descriptors, with ablation studies showing the impact on multi-scale performance\n\n‚Ä¢ **Enhance experimental rigor with comprehensive validation**\n  - Include statistical significance testing with confidence intervals for all performance comparisons in Table 1, using appropriate multiple testing corrections\n  - Expand ablation studies to isolate individual component contributions, including noise type, architectural choices, and loss function variants\n  - Provide direct experimental comparison with RD++ on identical datasets and evaluation protocols to substantiate claimed improvements\n  - Extend noise intensity analysis (Figure 3) to all datasets, analyzing the relationship between optimal noise levels and dataset characteristics\n\n‚Ä¢ **Improve synthetic anomaly generation methodology**\n  - Develop dataset-adaptive hyperparameter selection for simplex noise generation, with systematic optimization across different medical imaging modalities\n  - Provide empirical validation of power law assumptions in medical images or explore alternative frequency distribution models\n  - Compare against a broader range of anomaly synthesis approaches, including learnable methods and anatomically-informed generation strategies\n  - Incorporate anatomical priors or attention mechanisms to guide noise placement in medically relevant regions as suggested in the conclusion\n\n‚Ä¢ **Broaden evaluation scope and provide deeper analysis**\n  - Include additional medical imaging modalities (CT, ultrasound, histopathology) to demonstrate broader applicability and identify domain-specific limitations\n  - Provide comprehensive computational analysis for different backbones, including inference time, memory usage, and performance scaling relationships\n  - Add spatial anomaly localization examples and systematic failure case analysis to complement the score distribution analysis in Figure 2\n  - Develop systematic characterization of method performance across different anomaly types, scales, and dataset characteristics to guide practical deployment",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces a scale-aware contrastive reverse distillation (CRD) model for unsupervised anomaly detection in medical imaging. The proposed approach addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. The authors augment a reverse distillation framework with a contrastive student-teacher learning method and a scale adaptation mechanism, which they claim enhances the model's ability to discriminate between normal and abnormal images. The manuscript evaluates the proposed method on three benchmark datasets (RSNA Pneumonia Detection, Brain Tumor MRI, and ISIC 2018), demonstrating state-of-the-art performance. The work is well-written, and the motivation for integrating contrastive learning and scale adaptation is clearly articulated.\n\n## Major Comments\n1. Novelty and Positioning: While the introduction of a scale adaptation mechanism and contrastive learning within the reverse distillation framework is innovative, the manuscript needs to more clearly distinguish itself from recent related works. Specifically, the authors should provide a more detailed comparison with RD++ (Tien et al., 2023) and other contemporary methods that also address scale variations and contrastive learning. The novelty of the proposed method is somewhat undermined by the lack of explicit differentiation from these related works.\n\n2. Evaluation Design: The experimental evaluation is conducted on three benchmark datasets, which is commendable. However, the datasets are relatively standard and may not fully capture the complexity of real-world clinical scenarios. Including additional datasets or real-world clinical data could strengthen the validation of the proposed method. Additionally, the authors should consider evaluating their method on more diverse medical imaging modalities beyond X-ray, MRI, and dermoscopy to demonstrate its generalizability.\n\n3. Comparisons: The comparisons include a wide array of methods, which is appreciated. However, the inclusion of some less-recent methods (e.g., early autoencoder-based approaches) may not provide a fair assessment of the proposed method's advancement. Including more recent state-of-the-art methods specifically designed for unsupervised anomaly detection in medical imaging would be beneficial.\n\n4. Reproducibility: The authors state that code will be made available, which is a positive step towards reproducibility. However, the manuscript lacks detailed descriptions of some key implementation details, such as specific hyperparameters, data preprocessing steps, and training protocols. Providing a more thorough description of these aspects would greatly enhance the reproducibility of the work.\n\n## Minor Comments\n1. Figures: Some figures (e.g., Figure 2) are cluttered and could benefit from clearer labeling and organization. Zooming in on specific regions or showing fewer representative slices would improve readability.\n\n2. Notation and Terminology: The notation introduced in Section 2.1 is inconsistent in places, particularly regarding the forward operator. Clarification and standardization of this notation would improve clarity.\n\n3. Acronyms: Several acronyms (e.g., \"AUC\", \"F1\") are used without initial definition, which could be confusing for readers who are not familiar with the terminology. Defining these acronyms when they are first introduced would be helpful.\n\n4. Typographical Issues: Minor typographical errors are present throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). Careful proofreading would address these issues.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in unsupervised anomaly detection in medical imaging, where labeled anomalous data are scarce. The proposed scale-aware contrastive reverse distillation model offers a technically interesting approach to enhance feature discriminability and handle scale variations. However, the novelty of the method is somewhat diminished by the lack of clear differentiation from related works. The evaluation, while comprehensive, could benefit from additional datasets and more recent comparative methods. The reproducibility of the approach is partially addressed, but more detailed implementation details are required. Overall, while the proposed method shows promise, the current evidence does not fully meet the rigorous standards expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis to include more recent and relevant methods, strengthen the validation across more diverse datasets and modalities, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chunlei Li",
      "Jingliang Hu",
      "Lichao Mou",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_8f2f06936cc0020aac0ce13e60997a27679bffa6.pdf",
    "remote_url": "https://openreview.net/pdf/8f2f06936cc0020aac0ce13e60997a27679bffa6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning General-purpose Biomedical Volume Representations using Randomized Synthesis",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "synthetic data",
      "representation learning",
      "medical image analysis",
      "image registration",
      "image segmentation"
    ],
    "abstract": "Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network's features can be used as robust representations of input images for downstream tasks and its weights provide a strong, _dataset-agnostic_ initialization for finetuning on new datasets. As a result, we set new standards across _both_ multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes training a backbone that generalizes across different datasets using synthetically generated dataset. The proposed pre-training strategy has 3 main steps: 1) Given a large datasets of 104 annotated organs, randomly sample anatomies, deform them and create a volume by ensembling these anatomies, 2) add noise and other augmentations to the volumes that are sampled in the previous step to simulate realistic looking synthetic medical images from labels, 3) train a U-Net with a contrastive objective by sampling two volumes that share the same 3D semantic layout but differ in appearance, treating corresponding features at different encoder levels as positives and all others as negatives. The pre-trained backbone is validated on two different tasks: 3D registration and 3D few-shot segmentation; using multiple datasets.  The results show the effectiveness of the proposed backbone in the experiments compared to existing methods.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- Foundational models are showing promising performance lately; however, we lack of a 3D model that work across different modalities in medical imaging. The paper proposes a solution to this important problem using domain randomisation and contrastive learning.\n- The paper contain experiments on multiple datasets both for registration and few shot segmentation, and the results demonstrate the potential of the method.\n- The idea of combining the ideas of domain randomisation and local contrastive learning to train a generic 3D backbone is quite interesting and, to my knowledge, is novel.\n\n### Weaknesses\n\n- One issue I see in the paper is the convoluted description of the data engine step, especially the part creating the label ensemble model in section 3. I understand that this step is mainly based on the domain randomisation idea proposed in the literature. However, it is not really clear to me the steps between 201-207, especially the parts multiplying the masks with a randomly deformed sphere and randomly encasing half of the foreground-masked volumes.\n\n- The images generated in the data engine step do not seem like as real medical images. Do they look like this because the deformation is too large? It is not clear why one would prefer training the model using such unrealistic images.\n\n- The paper does not discuss the recent foundational models that show better generalization performance on many medical image datasets [1]. The downstream task performance of the representations obtained from the proposed backbone should be compared with the those obtained by the representations of a foundational model (e.g. DinoV2 [2]). For example, [3] is a recent paper that uses DinoV2 features for registration; but the same applies for the segmentation experiments. One can use the DinoV2 features for segmentation.\n\n[1] Cekmeceli et al. \"Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?\"\n[2] Oquab et al. \"DINOv2: Learning Robust Visual Features without Supervision\"\n[3] Song et al. \"DINO-Reg: General Purpose Image Encoder for Training-Free Multi-modal Deformable Medical Image Registration\"\n\n### Questions\n\n1- What are the exact steps of the \"label ensemble model\" described in Section 3? Please write elaborate description of these steps.\n2- Why do the generated images not look like real medical images? Is it because the deformation is too large? Why such \"unrealistic\" looking images are preferred rather than more realistic ones obtained with smaller deformation?\n3- How does the quality of the representations obtained by the proposed backbone compares with SoTA foundational models such as DinoV2 or SAM2?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript proposes a novel pre-training strategy for a backbone model aimed at improving generalization across diverse datasets in medical imaging. The method involves generating synthetic medical images through a three-step process that includes anatomical sampling, noise addition, and training a U-Net with a contrastive objective. The effectiveness of the proposed approach is evaluated through experiments in 3D registration and 3D few-shot segmentation on multiple datasets, indicating promising results in comparison to existing methods.\n\n### Major Comments\n1. The description of the data engine, particularly the creation of the label ensemble model in Section 3, is convoluted and lacks clarity, especially the steps between lines 201-207 involving mask multiplication with randomly deformed spheres.\n2. The synthetic images generated appear unrealistic as medical images. Clarification is needed regarding why such images are preferred over more realistic representations that could be achieved through smaller deformations.\n3. The manuscript does not address recent advancements in foundational models that exhibit superior generalization on many medical imaging tasks. A comparative analysis of the proposed backbone's performance against established models like DinoV2 is crucial.\n\n### Minor Comments\n- The manuscript could enhance clarity by providing a more detailed explanation of the \"label ensemble model\" within Section 3.\n- Consider refining the language used to describe the image generation process to aid understanding.\n- Review the comparison between the proposed method and current foundational models for improved comprehensiveness.\n\n### Summary Paragraph\nIn terms of significance, the manuscript addresses a critical need for a robust 3D model across various modalities in medical imaging, demonstrating innovation through its combination of domain randomization and contrastive learning techniques. However, the clarity of the evaluation design is compromised by convoluted descriptions, particularly within the data engine section, and the unrealistic nature of synthetic images raises concerns regarding reproducibility. The omission of comparative benchmarks against leading foundational models also detracts from the evaluation's rigor. \n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a framework for training a general-purpose 3D backbone using synthetically generated datasets. The approach involves three main stages: assembling deformed anatomical volumes from 104 annotated organs, applying noise and augmentations to create realistic synthetic images, and training a U-Net backbone with a contrastive learning objective that pairs volumes sharing semantic layouts but differing in appearance. The pretrained backbone is evaluated on 3D registration and few-shot segmentation tasks across multiple datasets. The paper is clearly written overall, with an interesting concept that aims to improve cross-dataset generalization in medical imaging through domain randomization and contrastive pre-training.  \n\n**Major Comments**  \n1. **Clarity of Data Engine Description:**  \n   The procedure for constructing the ‚Äúlabel ensemble model‚Äù in Section‚ÄØ3 is difficult to follow, particularly lines‚ÄØ201‚Äì207 regarding mask multiplication with deformed spheres and partial foreground encasement. A more explicit, step-by-step explanation of this process is needed to ensure reproducibility.  \n2. **Realism of Synthetic Data:**  \n   The synthetic images appear unrealistic, possibly due to excessive deformation. The authors should clarify why such unrealistic appearances are acceptable or advantageous compared with more realistic simulations using milder transformations.  \n3. **Comparison with Foundational Models:**  \n   The paper lacks discussion and empirical comparison with recent foundation models that have shown strong generalization, such as DINOv2‚ÄØ[2] and its applications in medical imaging‚ÄØ[1,‚ÄØ3]. Including such baselines would better position the proposed backbone relative to current state-of-the-art approaches in representation learning.  \n\n**Minor Comments**  \n- Some descriptions in the data generation section could be streamlined for clarity.  \n- Ensure consistent terminology between ‚Äúlabel ensemble model‚Äù and ‚Äúdata engine.‚Äù  \n- Figures illustrating synthetic examples should include representative comparisons to real images for context.  \n\n**Summary (TMI Criteria)**  \nThe work addresses a significant problem‚Äîbuilding a generalizable 3D backbone for medical imaging‚Äîby combining domain randomization and contrastive learning. The approach is conceptually innovative but the methodology needs clearer exposition and stronger evaluation against contemporary foundational models. The experiments demonstrate potential but reproducibility and comparative evidence are limited. Overall, the paper is moderately sound, with contributions that could become significant once the issues above are addressed.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work proposes a pre-training approach for downstream tasks related to fine-grained volumetric medical data: image registration and semantic segmentation. The authors propose to learn appearance invariance and shapes of human anatomy through synthetic dense pixel volumes. In this process, 3D volumes are synthesized by randomly recomposing 3D anatomical shapes and assigning multiple sets of random pixel values, in together with synthetic noises and deformations. Pairs of synthetic volumes are used for multi-scale contrastive learning. The proposed approach demonstrates improved image registration and low-shot image segmentation results compared to some previous works. Detailed ablation studies on the pre-training configurations toward downstream performances are conducted.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe overall methodology is straightforward and easy to understand. It echoes with the classical computer vision concept of invariance learning in the deep neural network era (although learned through a data-driven approach).\n\nImproved image registration results on two public image registration benchmarks and image segmentation performance on six image segmentation datasets are shown, compared to those of some existing works. \n\nThe paper is well-written with sufficient clarity. The illustrations are self-explanatory. Readers will enjoy reading it.\n\n### Weaknesses\n\nTechnical novelty: The core idea behind the approach is to leverage data-driven approach to learn invariance to pixel values through paired synthetic shapes and different pixel values, and to learn semantic-independent shape representation through random geometric (real-world and pure synthetic) shapes ‚Äì both key ideas come from the well-established SynthMorph (random Ising shapes with random intensity + synthetic deformation for training registration networks. Hoffmann et al.) and SynthSeg (GMM-like pixel value model for repainting brain structures. Billot et al.) Despite leveraging more anatomical shapes beyond brain structures and applied to a contrastive framework, the essence remains unchanged.  \n\nMany medical decisions are made not only on shape but also on subtle textures, for example, differentiating subtypes of tumors/lesions ‚Äì toward which the proposed over-simplified appearance model by nature falls short. More sophisticated texture models need to be carefully studied beyond this manuscript.\n\nFor the same reason, high-level global semantic information such as relative locations between anatomical structures cannot be learned due to the nature of this approach. \n\nReal-world value: Given the increasing number of large-scale publicly accessible volumetric image datasets such as CT-RATE (Hamamci et al.), Totalsegmenter (Wasserthal et al.), and AbdomenAtlas (Li et al.), and the derived 3D foundation models, the real-world application of the proposed framework is unclear. Some of these large-scale public datasets come with fine-grain pixel-wise labels and associated radiological reports which provide additional supervision signals and text alignment potentials. The claimed generalization capability can be learned from multi-site large real-world datasets as well, through the intrinsic heterogeneity of big data and possibly through intense data augmentation.\n\n### Questions\n\nThe proposed workflow involves many hyper-parameters (Figure 12) controlling the properties of generated synthetic volumes -- what is the rule of thumb for choosing them?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a novel pre-training method aimed at enhancing downstream tasks in fine-grained volumetric medical data, specifically image registration and semantic segmentation. By leveraging synthetic dense pixel volumes generated through various means, including random recompositions of anatomical shapes and pixel value assignments, the authors demonstrate advancements in multi-scale contrastive learning. The results indicate notable improvements in image registration and low-shot image segmentation relative to prior studies.\n\n**Major Comments:**\n1. **Technical Novelty:** While the manuscript proposes a data-driven approach to learning appearance invariance from paired synthetic shapes, the foundational concepts echo previously established methods such as SynthMorph and SynthSeg. The contribution may not substantially diverge from these works, despite the broader range of anatomical shapes adopted.\n2. **Medical Relevance:** The proposed appearance model is seemingly oversimplified for medical applications that require finer performance, such as differentiating tumor subtypes based on subtle textural differences. More advanced texture modeling should be explored.\n3. **Global Contextual Learning:** The current framework does not facilitate learning of high-level semantic information, particularly concerning the relative placement of anatomical structures, which limits its applicability.\n4. **Real-World Application:** The practical significance of this approach is questionable given the availability of large-scale, publicly accessible volumetric datasets that already provide fine-grained labels and radiological insights, suggesting potential avenues for improved generalization that the current work does not address.\n\n**Minor Comments:**  \n- Figure 12 outlines multiple hyper-parameters for generating synthetic volumes; providing guidance on optimal selection would be beneficial.\n- The paper is generally well-written, with self-explanatory illustrations, contributing to clarity.\n\n**Summary Paragraph:**  \nIn summary, while the manuscript demonstrates statistical significance in enhancing image registration and segmentation through a novel pre-training approach, its innovation appears limited by foundational concepts established in prior works. The evaluation of the presented methodology against real-world applications and its generalization capability remains uncertain, necessitating further exploration of texture modeling and contextual information. However, clarity and organization are notable strengths, as indicated by the satisfactory presentation of content.\n\n**Decision Recommendation:**  \nRecommend major revision, with a focus on addressing the technical novelty critique and enhancing practical relevance through comprehensive analysis.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a pre-training framework designed for downstream medical imaging tasks, particularly volumetric image registration and semantic segmentation. The method aims to learn appearance invariance and anatomical shape representations by synthesizing dense 3D pixel volumes through random recombination of anatomical shapes and assignment of synthetic intensities, noise, and deformations. Pairs of synthetic volumes are used for multi-scale contrastive learning, and the approach shows improved registration and few-shot segmentation performance compared to selected prior works. The paper is clearly written, logically structured, and supported by illustrative figures and ablation studies that contribute to its readability and completeness.\n\n**Major Comments**  \n1. **Technical Novelty:** The core concept‚Äîlearning invariance to pixel-level appearance and shape variations through synthetic volume generation‚Äîclosely resembles established approaches such as SynthMorph and SynthSeg. While the present work extends these ideas to a contrastive learning framework and includes a wider set of anatomical structures, the fundamental innovation appears limited.  \n2. **Texture Representation:** Many clinical distinctions rely on fine textural cues (e.g., tumor subtype differentiation), which the simplified intensity model does not capture. The current formulation therefore lacks the capacity to model complex appearance variability essential for certain diagnostic applications.  \n3. **Global Semantics:** The approach‚Äôs synthetic recomposition process may prevent learning higher-level spatial relations between anatomical parts, restricting the potential to encode global semantic context.  \n4. **Practical Relevance:** With growing accessibility to large annotated 3D datasets (e.g., CT-RATE, Totalsegmenter, AbdomenAtlas) and foundation models trained thereon, the added real-world value of this synthetic-data-driven framework is uncertain. Comparable generalization could potentially be achieved through training on these heterogeneous real datasets with standard augmentation.  \n5. **Methodological Clarity:** The workflow involves many hyperparameters controlling the synthesis process (as in Figure 12). The manuscript would benefit from guidance or heuristics on how to select these parameters.\n\n**Minor Comments**  \n- Figures are clear and self-explanatory, though a concise table summarizing datasets and evaluation metrics would further improve readability.  \n- Ensure that all acronyms (e.g., GMM, SSIM) are defined at first use.\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important question in pre-training for volumetric medical imaging and presents a well-documented evaluation with consistent results. Its strengths lie in methodological clarity, extensive experiments, and connection to invariance learning principles. However, its degree of novelty is modest, the modeling of texture and global structural information is limited, and practical benefit relative to existing large-scale real-data training remains uncertain. Reproducibility appears achievable thanks to explicit ablations and parameter disclosure.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision** ‚Äì The paper demonstrates solid experimentation and clear presentation but requires stronger justification of novelty, practical impact, and methodological guidance before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nAuthors present a method to generate highly variable synthetic imaging data which is then used to pre-train a 3D network using contrastive learning. The data generation method consists of drawing samples from a semantically labelled repository of biomedical shape templates to randomly populate an empty 3D volume. The volume is then deformed. Empty space and organ 'envelopes' are also simulated. To simulate different modalities and protocols, random intensity transformation is applied to the deformed 3D volume to yield 2 images. Typical imaging artifacts such a sbias field and blurring are simulated through random augmentations.The two images are fed into a UNet, and contrastive pre-training is performed on features at each decoder layer. An anchor point is chosen in one of the images, and all voxels of that label in both images are considered positive, and everything else negative pairs. The network yields features that can be used to finetune on other modalities and tasks. Importantly, the representations are modality-agnostic and anatomy-agnostic.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n* The paper is very well written - it lays out the prior work and puts the contirbute in context.\n* The approach yields representations that are both modality-agnostic and task-agnostic while removing the need for dataset-specific and anatomy-specific pre-training.\n* Authors present results of several downstream tasks using their features including multi-modality registration image registration and few-shot segmentation on which their method outperform the others compared.\n* Authors perform ablation studies on the various components of their pipeilne.\n* The Authors present extensive visualization and quantitative results in their main text, and supplementary material. Algorithms and parameters are clearly presented too which allows for further scrutiny and improved reproducability.\n* Authors are aware of the limitations of their approach and include these in the paper.\n\n### Weaknesses\n\n* The segmentation task performed using the Authors features may yield better results than the other methods that are compared, however the result still misses significant portions of the anatomical regions they aim to segment. The features require further adjustment and extensive fine-tuning to be useful in diagnosis and treatment.\n\n### Questions\n\n* Authors compare their randomized shape template-based synthetic data engine to one that uses data with no biomedical priors and one using brain regions. Can Authors elaborate more on the intuiton for why their randomly deformed template shapes are so effective? Is there some point at which the extent of the deformation causes the representations to be less useful?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a method for generating highly variable synthetic imaging data, which is employed to pre-train a 3D network using contrastive learning. The data generation involves sampling from a semantically labeled repository of biomedical shape templates to populate an empty 3D volume, which is then deformed. This process integrates random intensity transformations and simulates typical imaging artifacts, ultimately creating two images that serve as input to a UNet model for contrastive pre-training. The results indicate that the generated synthetic representations are modality-agnostic and anatomy-agnostic, enhancing their applicability across multiple tasks.\n\n### Major Comments\n1. The segmentation task using the authors' features does yield improved results compared to other methods; however, significant anatomical regions remain under-segmented. Further refinement and extensive fine-tuning of the features are needed for effective diagnostic and treatment applications.\n2. The comparison between the authors' randomized shape template approach and alternatives lacking biomedical priors or using brain-region data needs further elaboration. The authors should clarify the rationale behind the effectiveness of their randomly deformed template shapes and discuss potential limits in deformation extent that might impact representation utility.\n\n### Minor Comments\n- The writing is clear and well-structured, effectively contextualizing the contributions within prior work.\n- Extensive visualizations and quantitative results are provided in both the main text and supplementary material, enhancing reproducibility.\n- Algorithms and parameters are presented clearly, allowing for further scrutiny.\n\n### Summary Paragraph\nThis manuscript addresses significant issues in synthetic imaging data generation and offers an innovative approach to pre-training models for biomedical applications. The evaluation shows promise, with features being modality-agnostic and applicable to various tasks. However, the reproducibility is somewhat hindered by the current limitations in the segmentation performance. The concerns raised regarding the clarity of comparisons made in the study suggest areas for refinement.\n\n### Decision Recommendation\nMinor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a framework for generating highly variable synthetic 3D biomedical images for contrastive pretraining of neural networks. By sampling from a semantically labeled shape repository, populating 3D volumes, applying random deformations, and simulating imaging effects (intensity changes, bias fields, and blur), the authors create realistic variations across modalities. These synthetic image pairs are used to train a UNet via voxel-level contrastive learning to obtain representations that are both modality- and anatomy-agnostic. The manuscript is clearly written, well contextualized within related work, and supported by detailed methodological and experimental descriptions.\n\n**Major Comments**  \n1. **Performance Limitations:** Although the proposed features outperform comparable methods in multi-modality registration and few-shot segmentation, the segmentation results still omit substantial portions of target anatomy. This suggests that the learned representations may require further refinement or fine-tuning for clinical utility.  \n2. **Effectiveness of Synthetic Deformations:** The comparison between random shape-template generation and other approaches is interesting, but the rationale for why random deformations enhance representation learning could be explained more thoroughly. It would be valuable to discuss whether excessive deformation could reduce representational utility.  \n3. **Evaluation Scope:** While comprehensive ablations and downstream results are shown, further discussion of how these results generalize across unseen modalities or clinical tasks would strengthen reproducibility and external validity claims.\n\n**Minor Comments**  \n- The manuscript‚Äôs writing and structure are strong, with clear algorithmic details and reproducible parameter settings.  \n- Typographical errors (e.g., ‚Äúcontirbute,‚Äù ‚Äúpipeilne,‚Äù ‚Äúsbias‚Äù) should be corrected for presentation polish.  \n- Ensure consistent terminology for tasks (e.g., ‚Äúmulti-modality registration‚Äù vs. ‚Äúimage registration‚Äù) and clear definition of acronyms on first use.\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in that it provides a general pretraining approach that eliminates the need for anatomy- or modality-specific datasets. Its **innovation** lies in the synthetic biomedical shape-based data engine and contrastive volumetric feature learning. The **evaluation** is thorough, including multiple downstream tasks and ablations, though some results indicate that fine-tuning remains necessary. The study‚Äôs **reproducibility** is high due to clear presentation of methods and parameters. Overall, a strong, well-presented contribution with room for improvement in practical performance and theoretical justification.\n\n**Decision Recommendation**  \n**Recommendation: Minor Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1: Analytical and‚ÄØScope‚ÄØAudit**\n\n1. **Scope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI‚ÄØ‚úÖ**  \n   The manuscript presents a methodological contribution directly aligned with TMI‚Äôs remit. It proposes a new framework for volumetric biomedical representation learning that uses a synthetic‚Äêdata engine and contrastive pretraining to produce general‚Äëpurpose 3‚ÄëD features applicable to registration and segmentation. The emphasis is on algorithmic design, theory, and validation‚Äînot on a single clinical application.\n\n2. **Novelty‚ÄØ&‚ÄØContribution‚ÄØLevel‚ÄØ‚úÖ**  \n   The work introduces a distinctive combination of (a)‚ÄØdomain‚Äërandomized volumetric label synthesis informed by biomedical priors, and‚ÄØ(b)‚ÄØa voxel‚Äëwise multi‚Äëpositive contrastive objective that leverages perfect label supervision from those synthetic maps. The claim that the model generalizes across modalities and tasks without seeing real data is novel relative to current 3‚ÄëD medical‚Äëfoundation models, which are mostly trained on pooled real datasets.\n\n3. **Technical‚ÄØand‚ÄØExperimental‚ÄØRigor‚ÄØ‚úÖ/‚ö†Ô∏è**  \n   Implementation details are extensive (data‚Äëgeneration pseudocode, UNet configuration, ablation tables). Experiments span multiple benchmarks and tasks with quantified uncertainty. However, (i)‚ÄØstatistical significance tests beyond Dice/SDs are not shown, (ii)‚ÄØgeneration pipeline realism or possible bias is not formally analyzed, and (iii)‚ÄØresults rely primarily on two downstream tasks. Overall rigor is high but could be strengthened by clearer validation of the synthetic‚Äëto‚Äëreal domain gap.\n\n4. **Clarity‚ÄØand‚ÄØPresentation‚ÄØ‚úÖ**  \n   The paper is well‚Äëstructured, grammatically sound, and rich in figures. Given length and appendix volume, a condensed graphical overview of the pipeline and main results would improve readability.\n\n5. **Ethical‚ÄØ&‚ÄØReproducibility‚ÄØCompliance‚ÄØ‚úÖ**  \n   All used public datasets are cited with appropriate licenses; synthetic generation avoids patient data. The authors state code and pretrained weights will be released. The in‚Äëhouse dataset is anonymized and IRB‚Äëapproved.\n\n---\n\n**Phase‚ÄØ2:‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n### 1.‚ÄØSummary\nThe authors propose **a synthetic‚Äëdata‚Äìdriven framework for general‚Äëpurpose 3‚ÄëD biomedical representation learning**.  \nA ‚Äúdata engine‚Äô‚Äô first composes label ensembles from randomly deformed anatomical templates and assigns diverse textures via randomized Gaussian‚Äëmixture intensity models and physics‚Äëbased augmentations.  \nA 3‚ÄëD‚ÄØUNet is then pretrained using a supervised contrastive loss at voxel level so that features from the same label remain close despite large appearance variation.  \nWithout real‚Äëimage pretraining, the resulting model improves over state‚Äëof‚Äëthe‚Äëart methods on **unsupervised multimodality registration** and **few‚Äëshot segmentation** across MRI and‚ÄØCT datasets.\n\n### 2.‚ÄØStrengths\n- Addresses a long‚Äëstanding limitation of 3‚ÄëD foundation models‚Äîthe paucity of large, diverse volumes.  \n- Elegant integration of domain randomization and contrastive learning; full algorithmic transparency.  \n- Demonstrates genuine multi‚Äëtask generalization (registration‚ÄØ+‚ÄØsegmentation).  \n- Extensive ablation and comparison to leading baselines (SynthMorph,‚ÄØuniGradICON,‚ÄØSMIT,‚ÄØPrimGeoSeg).  \n- Fully reproducible with planned code release, aligning with TMI‚Äôs open‚Äëscience priorities.\n\n### 3.‚ÄØWeaknesses\n- The ‚Äúsynthetic‚Äëto‚Äëreal‚Äô‚Äô transfer remains empirically, not theoretically, justified; lacks quantitative diversity metrics or domain‚Äëgap analysis.  \n- Validation scope limited to registration‚ÄØ/‚ÄØsegmentation; testing on additional volumetric tasks (e.g., detection, quantitative mapping) would strengthen generality claims.  \n- Some performance gains (few‚Äëshot segmentation) are modest and may depend on heavy fine‚Äëtuning.  \n- Discussion of computational cost and training efficiency is minimal.  \n- Presentation could be condensed to improve accessibility.\n\n### 4.‚ÄØMajor‚ÄØComments\n1. **Quantify Synthetic Data Diversity:** Provide statistical metrics (e.g., intensity histograms, structural similarity) comparing synthetic and real datasets to substantiate ‚Äúbroad variability.‚Äô‚Äô  \n2. **Generalization Analysis:** Include cross‚Äëmodality transfer or out‚Äëof‚Äëdistribution validation beyond MRI/CT (e.g., ultrasound, microscopy) within the main paper rather than appendix only.  \n3. **Ablation of Contrastive Objective:** Expand discussion of why multi‚Äëpositive supervision outperforms self‚Äësupervised or reconstructive losses; possibly include qualitative embedding visualizations or clustering metrics.  \n4. **Computation‚ÄØ&‚ÄØThroughput:** Report pretraining time, GPU resources, and scalability relative to standard self‚Äësupervised pipelines.  \n5. **Limitations‚ÄØ&‚ÄØClinical‚ÄØOutlook:** Strengthen the discussion regarding tasks where intensity invariance may harm performance and how those could be mitigated.\n\n### 5.‚ÄØMinor‚ÄØComments\n- Improve figure captions for readability; some multi‚Äëpanel results lack color‚Äëmap legends.  \n- Ensure consistent notation between equations and algorithm listings.  \n- Typographical cleanup: occasional spacing errors (‚Äúbiomed‚Äë ical‚Äô‚Äô).  \n- Consider moving lengthy dataset details to supplementary material with summarized statistics in the main text.\n\n### 6.‚ÄØOverall‚ÄØRecommendation  \n**Major‚ÄØRevision** ‚Äì The paper offers clear methodological innovation and strong preliminary evidence, meeting TMI‚Äôs scope and novelty bar.  However, before acceptance the authors should (i)‚ÄØstrengthen quantitative validation of the synthetic‚Äëto‚Äëreal generalization,‚ÄØ(ii)‚ÄØbroaden evaluation beyond two downstream tasks, and‚ÄØ(iii)‚ÄØclarify computational and methodological limits.\n\n### 7.‚ÄØConfidence‚ÄØLevel  \n**4‚ÄØ/‚ÄØ5** ‚Äì High confidence in assessing technical merit and alignment with TMI standards; moderate uncertainty regarding potential unseen limitations of large‚Äëscale synthetic pretraining.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses the challenge of limited generalization in volumetric biomedical foundation models due to small and domain-specific 3D datasets. The authors propose a representation learning framework combining: (1) a synthetic data engine that generates highly variable training samples using randomized compositions of biomedical shape templates from TotalSegmentator (Section 3, Figure 2), and (2) a contrastive learning method that pretrains a 3D UNet to be stable against nuisance imaging variation while preserving semantic consistency (Equation 1, Figure 3). The method trains exclusively on synthetic data without using any real biomedical images. Experiments demonstrate state-of-the-art performance on unsupervised multimodality registration across L2RAb and MM-WHS datasets (Figure 5, Table 2) and competitive few-shot segmentation results across six diverse datasets (Table 1), establishing the first 3D biomedical foundation model capable of both registration and segmentation tasks.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation lacks precision and completeness**\n  - The contrastive loss definition in Equation 1 uses ambiguous notation where spatial indices are pooled from both volumes (I = {1,...,2HWD}) but the relationship between indices across V1 and V2 is unclear, making reproducibility difficult\n  - The positive set P(i) definition assumes label correspondence across volumes but doesn't specify how spatial alignment is maintained during geometric augmentations, creating potential training inconsistencies\n  - Temperature parameter œÑ = 0.33 deviates significantly from standard contrastive learning values (typically 0.07-0.1) but lacks theoretical justification or sensitivity analysis in the main text\n\n‚Ä¢ **Experimental evaluation has significant methodological limitations**\n  - Registration baselines exclude recent deep learning methods like VoxelMorph variants that could provide stronger comparisons, limiting the scope of performance claims (Section 4.1)\n  - Few-shot segmentation uses only 1-3 training volumes per dataset (Table 1), which may not reflect realistic clinical deployment scenarios where more data is typically available\n  - Cross-validation or multiple random splits are not reported, raising questions about statistical significance and generalizability of the Dice improvements shown in Table 1\n  - The MM-WHS dataset required substantial preprocessing including custom spine annotations (Appendix B.4.4), potentially introducing evaluation bias\n\n‚Ä¢ **Limited technical novelty and insufficient analysis of design choices**\n  - The synthetic data engine primarily combines existing techniques (Dead Leaves model, domain randomization) with biomedical templates, offering incremental rather than fundamental innovation (Section 2, Page 3)\n  - Ablation studies in Table 3 show relatively small performance differences between design choices, questioning the necessity of the proposed complexity\n  - The choice of TotalSegmentator templates introduces potential bias toward CT-based anatomical structures, but impact on MRI-heavy datasets is not thoroughly analyzed\n  - Feature visualization in Figure 1 lacks quantitative stability metrics, relying only on qualitative assessment of cross-modal consistency\n\n‚Ä¢ **Insufficient baseline comparisons and evaluation scope**\n  - Comparison with 2D foundation models like DINOv2 is relegated to appendix (Table 8) despite their widespread adoption in medical imaging, understating competitive landscape\n  - Large-scale medical foundation models comparison (SuPreM in Table 9) shows the proposed method's advantages are modest, questioning practical significance\n  - Registration experiments lack comparison with learning-based multimodal registration methods that could leverage similar synthetic training strategies\n  - The multitask evaluation in Table 2 tests only one additional task (registration), insufficient to claim general-purpose capabilities\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and strengthen mathematical formulations**\n  - Provide explicit spatial correspondence mechanism between V1 and V2 indices in Equation 1, including how geometric augmentations preserve label alignment during training\n  - Add theoretical analysis or empirical justification for the œÑ = 0.33 choice, including comparison with standard contrastive learning temperatures\n  - Include algorithmic pseudocode for the complete contrastive training procedure to ensure reproducibility\n\n‚Ä¢ **Enhance experimental rigor and scope**\n  - Include recent deep learning registration baselines (e.g., VoxelMorph variants, TransMorph) to provide comprehensive performance comparison\n  - Expand few-shot evaluation to include 5-10 training volumes scenarios that better reflect clinical reality\n  - Report confidence intervals and statistical significance tests across multiple random dataset splits for all major results\n  - Validate registration performance on standard datasets without custom preprocessing to ensure fair comparison\n\n‚Ä¢ **Strengthen technical contributions and analysis**\n  - Conduct systematic ablation study comparing synthetic vs. real label sources with quantitative stability metrics beyond visual inspection\n  - Analyze the impact of TotalSegmentator's CT bias on different imaging modalities with dedicated experiments\n  - Provide theoretical framework connecting synthetic data diversity to downstream generalization performance\n  - Include quantitative feature stability analysis (e.g., mutual information, correlation) to supplement Figure 1 visualizations\n\n‚Ä¢ **Expand baseline comparisons and evaluation comprehensiveness**\n  - Move DINOv2 and large-scale foundation model comparisons to main text with detailed analysis of when synthetic pretraining outperforms real data approaches\n  - Include learning-based multimodal registration methods trained on synthetic data for direct comparison\n  - Evaluate on additional tasks (e.g., classification, detection) to substantiate general-purpose claims beyond registration and segmentation\n  - Provide computational cost analysis comparing synthetic pretraining with real data collection and annotation efforts",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Adrian V Dalca",
      "Benjamin Billot",
      "Clinton Wang",
      "Ellen Grant",
      "Mengwei Ren",
      "Neel Dey",
      "Polina Golland",
      "Hallee E. Wong"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2d77a9ccf261321c494f73f45590632c7fc2bce6.pdf",
    "remote_url": "https://openreview.net/pdf/2d77a9ccf261321c494f73f45590632c7fc2bce6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Time-to-Event Pretraining for 3D Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Multimodal learning",
      "medical imaging",
      "Electronic Health Records"
    ],
    "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell‚Äôs C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\n*Edit: Score increased from 6 to 8 during discussion period.*\n\nThis paper presents a self-supervised learning (SSL) method for 3D medical imaging data that leverages electronic health records (EHR) to provide extra sources of supervision via time-to-event modeling. The proposed method, future-guided pretraining, performs time-to-event (TTE) survival modeling of various medical events in the longitudinal EHR associated with each 3D scan. The authors show that future-guided pretraining consistently improves downstream TTE modeling and prognostic classification tasks ‚Äì also improving data efficiency ‚Äì without degrading standard diagnostic classification performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The presentation quality is very high. Care has been taken to logically organize the paper, clearly articulate key points, and straightforwardly present results with concise figures and tables.\n- The core idea is creative, making use of the wealth of longitudinal EHR data associated with each 3D volume for pretraining.\n- Discussion or related work and background is particularly strong.\n- Experiments are sufficiently thorough and easy to interpret ‚Äì results are convincing.\n\n### Weaknesses\n\n- The actual description of the TTE pretraining approach is brief (lines 184-191) and somewhat unclear. I would advise the authors to flesh out this section. See specific questions below.\n- A description or list of the 8,192 EHR pretraining tasks is never provided. I‚Äôm aware there may not be a convenient place to list this many items, but a general description of categories of events or a few illustrative examples would be helpful. Without this information, it‚Äôs impossible to assess whether, e.g., one the TTE pretraining tasks is *also* used as a downstream TTE modeling task. In this case, there may be concerns of ‚Äúlabel leakage‚Äù.\n\nI‚Äôm happy to increase my score once these issues are addressed ‚Äì this is an otherwise strong submission.\n\n### Questions\n\n- What exactly does it mean that Steinberg et al.‚Äôs method was used to ‚Äú[sample tasks to maximize entropy given the frequency distribution of medical codes populating the DAG‚Äù? I feel that a basic plain-language description of the motivation for this procedure is needed first: why is this method being applied at all? Are there way more than 8k events and the goal is to settle on a subset of 8k ‚Äúmeaningful‚Äù/common ones for pretraining? I don‚Äôt understand the motivation.\n- Unless I am misunderstanding, this is the only description of the TTE pretraining procedure and labels used: ‚ÄúWe define our TTE task labels by predicting the time until the next occurrence of a medical code.‚Äù The previous Section 3 described deep survival modeling in the abstract, so I expected Section 4 to more concretely describe how TTE pretraining works. Is this a ‚Äúcompeting risks‚Äù approach, where multiple events are being modeled simultaneously (in ‚Äúmulti-label‚Äù fashion)?\n- What are the 8,192 EHR tasks/events? I‚Äôm aware it would be cumbersome or impossible to list and define them all, but any reasonable attempt to convey information about them would be useful. What kinds of ‚Äúevents‚Äù are they? What are some examples?\n- Related to the above point, are the downstream labels *also* present in the set of TTE pretraining tasks? If so, isn‚Äôt there concern of ‚Äúlabel leakage‚Äù, where the model has been pretrained on label information present in the downstream training dataset? Please clarify this.\n\n**Minor comments/questions:**\n- Line 13: Maybe ‚Äúbuild‚Äù instead of ‚Äúcapture‚Äù since you use this word in the next sentence.\n- In-text citation style seems off ‚Äì should be parenthetical (\\pcite{}) in most cases when used at end of sentence/clause: ‚ÄúSox et al. (2024)‚Äù -> ‚Äú(Sox et al., 2024)‚Äù\n- Change ‚Äúe.g.‚Äù -> ‚Äúe.g.,‚Äù throughout\n- Would include more recent references [1,2] when discussing deep prognosis models on longitudinal medical imaging (first paragraph of Section 2)\n- ‚Äúi.e. 8192‚Äù -> ‚Äúi.e., 8.192‚Äù\n- ‚ÄúOur approach improves training data efficiency, increasing training labels by an average of 3x over labels assigned to patients based on their current EHR visit.‚Äù This is a bit unusual to highlight as a main contribution ‚Äì I don‚Äôt think readers will understand what ‚Äúincreasing training labels‚Äù means without having read the entire paper (nor why this impact data efficiency). Perhaps clarify language here to indicate that your approach provides 3x as many sources of supervision during SSL + that this is what provides data efficiency benefits.\n- ‚ÄúPretraining task labels as assigned per-CT scan and vary in density based on pretraining approach, see Figure 2.‚Äù Perhaps ‚Äúas assigned‚Äù is meant to be ‚Äúare assigned‚Äù? Also change ‚Äú, see Figure 2‚Äù -> ‚Äú(Figure 2)‚Äù.\n- Be consistent with ‚Äúc-statistic‚Äù vs. ‚ÄúC-statistic‚Äù\n\n**References**\n[1] Holste, Gregory, et al. \"Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling.\" NPJ Digital Medicine 7.1 (2024): 216.\n[2] Sriram, Anuroop, et al. \"Covid-19 prognosis via self-supervised representation learning and multi-image prediction.\" arXiv preprint arXiv:2101.04909 (2021).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces a novel self-supervised learning (SSL) method specifically designed for 3D medical imaging data, which integrates electronic health records (EHR) to enhance supervision through time-to-event (TTE) modeling. The proposed future-guided pretraining approach aims to model TTE survival of various medical events associated with longitudinal EHR data linked to each 3D scan. The authors demonstrate that this methodology consistently enhances downstream TTE modeling and prognostic classification tasks, improving data efficiency without compromising standard diagnostic classification performance.\n\n**Major Comments:**\n1. The description of the TTE pretraining approach is notably brief (lines 184-191) and lacks clarity; further elaboration in this section is recommended.\n2. A detailed description or a list of the 8,192 EHR pretraining tasks is absent. While exhaustive detailing may not be feasible, providing categories or illustrative examples would help assess the relevance of the tasks. This is crucial to understanding potential \"label leakage\" concerns.\n3. The method utilizing Steinberg et al.'s approach to \"[sample tasks to maximize entropy given the frequency distribution of medical codes populating the DAG]\" requires clearer explanation and motivation. \n4. The description of TTE task labels, specifically ‚Äúpredicting the time until the next occurrence of a medical code,‚Äù lacks concrete exposition; clarification on whether this employs a ‚Äúcompeting risks‚Äù approach is needed.\n5. Further insights into the nature and examples of the 8,192 EHR tasks/events would be beneficial.\n6. Clarification is needed regarding whether downstream labels overlap with TTE pretraining tasks to address potential \"label leakage.\"\n\n**Minor Comments:**\n- \"Capture\" in line 13 could be replaced with \"build.\"\n- In-text citations should generally be formatted parenthetically: change ‚ÄúSox et al. (2024)‚Äù to ‚Äú(Sox et al., 2024).‚Äù\n- Modify \"e.g.\" to \"e.g.,\" consistently throughout the manuscript.\n- Recent references [1,2] should be included within the discussion of deep prognosis models in Section 2's first paragraph.\n- Correct ‚Äúi.e. 8192‚Äù to ‚Äúi.e., 8,192.‚Äù\n- Refine the phrase about increasing training labels for clarity; specify that the approach increases supervision sources by an average of 3x.\n- The sentence about assignment per-CT scan needs review for grammatical accuracy.\n- Ensure consistency in terminology regarding ‚Äúc-statistic‚Äù and ‚ÄúC-statistic.‚Äù\n\n**Summary Paragraph:**  \nThis study presents a significant innovation in SSL for 3D medical imaging, leveraging EHR data to improve TTE modeling. The manuscript is well-organized and the experiments are convincingly thorough, aligning well with TMI's editorial criteria. However, the clarity of key methodological sections and potential ambiguities regarding task overlap present notable concerns that must be addressed to enhance reproducibility and evaluation. The contributions, while substantial, are somewhat diluted by these weaknesses.\n\n**Decision Recommendation:**  \nMinor revision required to address the highlighted issues and improve clarity before acceptance.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a self‚Äësupervised learning (SSL) framework for 3D medical imaging that integrates longitudinal electronic health records (EHRs) through time‚Äëto‚Äëevent (TTE) modeling. The approach, termed *future‚Äëguided pretraining*, leverages survival modeling of EHR‚Äëderived medical events to enhance representation learning and improve downstream prognostic and classification tasks. The paper is well structured and clearly written, with logical organization, informative figures and tables, and a thorough discussion of related work. Overall presentation quality is high, and the contribution‚Äîlinking EHR‚Äëbased temporal information with SSL for 3D imaging‚Äîis both creative and promising.\n\n---\n\n**Major Comments**  \n1. **Method Description Needs Expansion:** The explanation of the TTE pretraining method (lines 184‚Äì191) is overly brief and difficult to follow. Further detail is needed on how TTE modeling is implemented in pretraining and how it differs from standard survival analysis frameworks.  \n2. **Specification of Pretraining Tasks:** The manuscript mentions 8,192 EHR‚Äëbased pretraining tasks but never describes their nature or provides examples. Summarizing the types of medical events, or illustrating several representative categories, would help assess the validity and relevance of the pretraining signals.  \n3. **Clarification of Task Sampling Procedure:** The sentence referencing Steinberg‚ÄØet‚ÄØal.‚Äôs method for ‚Äúsampling tasks to maximize entropy‚Äù is obscure. A plain‚Äëlanguage explanation of why this approach is used and whether it selects a representative or reduced subset of medical codes is necessary.  \n4. **Potential for Label Leakage:** The review raises an important question regarding overlap between pretraining and downstream TTE tasks. If identical events appear in both, there may be inadvertent label leakage. The authors should explicitly clarify whether task sets are disjoint.  \n\n---\n\n**Minor Comments**  \n- Use ‚Äúbuild‚Äù instead of ‚Äúcapture‚Äù in line‚ÄØ13 for fluency.  \n- Adjust citation style to parenthetical form (e.g., ‚Äú(Sox‚ÄØet‚ÄØal.,‚ÄØ2024)‚Äù).  \n- Add comma after ‚Äúe.g.‚Äù and ‚Äúi.e.‚Äù consistently.  \n- Include recent references such as Holste‚ÄØet‚ÄØal.‚ÄØ(2024) and Sriram‚ÄØet‚ÄØal.‚ÄØ(2021) when discussing prognosis modeling.  \n- Clarify expression ‚Äúincreasing training labels by an average of‚ÄØ3√ó‚Äù to indicate increased supervisory signals during SSL training.  \n- Rephrase ‚ÄúPretraining task labels as assigned‚Ä¶‚Äù ‚Üí ‚Äúare assigned‚Äù and move ‚Äúsee Figure‚ÄØ2‚Äù into parentheses.  \n- Maintain consistency in ‚Äúc‚Äëstatistic‚Äù capitalization.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in exploring an innovative intersection between EHR‚Äëderived temporal data and 3D image SSL, potentially benefiting data efficiency and medical prognostic modeling. The novelty and evaluation are convincing, supported by clear experiments and strong discussion. However, reproducibility would be improved by a fuller description of pretraining tasks and clearer methodology for task sampling and TTE formulation. Addressing these clarifications would remove remaining ambiguities and strengthen the technical soundness.  \n\n---\n\n**Decision Recommendation:** **Minor Revision (score‚ÄØ=‚ÄØ8, confidence‚ÄØ=‚ÄØ4)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors proposed to utilize the time-to-event information in EHR that paired with the imaging data as a form of supervision for the pre-training purpose. A public dataset with both 3D images and EHR notes is employed for the pre-training and downstream applications. Another dataset without the time events is also used for the evaluation of model adaptation. The manuscript is easy to follow. However, it also suffers from several critical flaws, which are detailed below.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- Propose utilizing the time events as pre-training tasks specially designed for prognosis tasks in downstream applications. \n- The manuscript is overall easy to follow\n\n### Weaknesses\n\n- The proposed method is limited in generalization since it will require longitudinal time-to-event EHR data as the supervision for the pre-training. In comparison to the common self-supervised pre-training, the proposed methods are harder to scale up.\n\n- There is no comparison evaluation between the proposed method and prior methods in model pre-training. Only the results of the proposed method with different model architectures are reported. It will be difficult to appreciate the benefits of the proposed method.\n\n- The selected model architecture also raises questions since there are many popular model networks in medical imaging, e.g., 3D-UNet, ViT, etc. It will be helpful to see their performance compared to the vanilla ResNet. \n\n- Baselines without the pre-training process should also be reported.\n\n- The current setting utilizes public data for both pre-training and downstream applications. Having a separate evaluation dataset of a prognosis task will be helpful. \n\n- The proposed method is limited in technical innovation, though utilizing the time-to-event data as a form of supervision is relatively new in the pre-training. Mostly existing techniques are adopted for the pre-training.\n\n### Questions\n\nSee above\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a strategy for leveraging time-to-event information from electronic health records (EHR) combined with imaging data for pre-training purposes in prognostic tasks. Utilizing a public dataset that includes both 3D images and EHR notes, the authors aim to enhance downstream applications. While the manuscript is generally clear and accessible, it contains several significant shortcomings that impact its overall quality.\n\n**Major Comments:**  \n1. The generalization of the proposed method is limited, as it relies heavily on longitudinal time-to-event EHR data for pre-training supervision. This reliance makes scalability challenging compared to standard self-supervised pre-training methods.\n2. The manuscript lacks comparative evaluations of the proposed method against prior methods in model pre-training. Only results for the proposed method using different architectures are presented, making it difficult to assess its advantages.\n3. The choice of model architecture warrants further scrutiny, as many well-established networks, such as 3D-UNet and ViT, are not considered. A comparison of these models with the chosen ResNet would provide insight into performance differences.\n4. Baseline results without pre-training are not reported, which limits the manuscript's rigor.\n5. The use of publicly available data for both pre-training and downstream applications necessitates an additional separate evaluation dataset specifically for prognosis tasks to bolster findings.\n6. Although the concept of using time-to-event data for supervision is relatively novel, the overall technical innovation appears limited, relying primarily on existing techniques for pre-training.\n\n**Minor Comments:**  \n- The manuscript is well-structured, but minor revisions for clarity could enhance understanding.\n- Specific figures and results should include clearer labels and explanations for better comprehension.\n- Attention to typographical errors and proper notations would improve presentation.\n\n**Summary Paragraph:**  \nIn reviewing the manuscript against the TMI editorial criteria, the strengths lie in its innovative approach to integrating time-to-event data with pre-training tasks, enhancing significance in prognostic applications. However, weaknesses include limited innovation, the lack of comprehensive evaluations, and reliance on specific dataset requirements that hinder reproducibility. Addressing these issues would significantly improve the work's overall contribution.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a pre-training strategy that leverages time-to-event information from electronic health records (EHR) paired with 3D imaging data as supervisory signals. A public dataset containing both imaging and EHR data is used for pre-training and downstream analyses, while a separate dataset lacking time-event data is employed for adaptation evaluation. The paper is written clearly and is easy to follow. However, despite a promising motivation, the work has notable methodological and experimental limitations that reduce its technical and scientific impact.\n\n**Major Comments**  \n1. **Generalization and scalability:** The reliance on longitudinal time-to-event EHR data limits the generalization and scalability of the approach compared to standard self-supervised pre-training strategies that can utilize larger unlabeled datasets.  \n2. **Lack of comparative evaluation:** The manuscript provides no comparisons between the proposed pre-training approach and established pre-training methods. Results are restricted to varying model architectures of the proposed method, making it difficult to assess relative benefit.  \n3. **Choice of architecture:** The study employs a basic ResNet model, but more widely used imaging architectures (e.g., 3D U-Net, Vision Transformer) are not explored. Inclusion of these would help verify that improvements arise from the proposed strategy rather than architectural choice.  \n4. **Missing baseline:** Experiments omit a baseline without pre-training, which would clarify the added value of the proposed stage.  \n5. **Evaluation design:** Both pre-training and downstream experiments are conducted on public data from similar sources. Using an independent dataset for downstream prognosis evaluation would help establish external validity.  \n6. **Limited technical innovation:** Although the use of time-to-event data for supervision is somewhat novel, the overall approach mainly reuses existing techniques without substantial algorithmic innovation.\n\n**Minor Comments**  \n- The manuscript is well structured and readable.  \n- Figures and experimental settings are generally clear.  \n- No ethical issues are identified.\n\n**Summary Paragraph**  \nRegarding significance, the idea of integrating survival information into pre-training is interesting but constrained by dataset requirements. In terms of innovation, the method repurposes existing frameworks with modest novelty. The evaluation is incomplete due to missing comparisons and baselines, limiting assessment of performance gains. Reproducibility is reasonable given the use of public data, though broader validation is needed to confirm generalization.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a future-guided pretraining approach using time-to-event supervision to enhance the prognostic capabilities of 3D medical imaging models. By incorporating longitudinal EHR data into the pretraining process and predicting time-until-event, the model outperforms traditional methods across multiple standard tasks, as demonstrated by thorough experiments.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. Innovative Approach: The method creatively leverages EHR data following a medical scan to assist model pretraining, demonstrating better performance compared to imaging-only pretraining.\n2. Comprehensive Evaluation: Extensive comparisons across multiple tasks validate the robustness and efficiency of the TTE-based approach across different architectures.\n\n### Weaknesses\n\n1. Dependence on Large EHR Datasets: This approach relies on extensive, high-quality EHR data, which many medical datasets do not include.\n2. Limited Modality Scope: Tested only on CT images; broader modality testing could validate versatility across imaging types.\n3. Interpretability: The TTE pretraining‚Äôs impact on specific pixel-level biomarkers is less clear; additional analysis on feature attribution could help.\n\n### Questions\n\n1. Why start from 3D image scans instead of 2D medical images? Is this due to the dataset choice, or has similar work already been done on 2D data?\n2. How does the choice of time segmentation for EHR data affect model results during pretraining? Specifically, my understanding is that the model predicts the probability of a patient experiencing a certain event at intervals like 1, 2, or 3 years post-scan. How does the granularity of these time segments impact the performance of the pretrained encoder?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript proposes a future-guided pretraining methodology that utilizes time-to-event (TTE) supervision to improve the prognostic capabilities of 3D medical imaging models. By integrating longitudinal electronic health record (EHR) data into the pretraining framework and predicting time-until-event outcomes, the study demonstrates that the new model consistently outperforms traditional imaging-only approaches across various standard tasks, supported by thorough experimental validation.\n\n### Major Comments\n1. **Innovative Approach:** The methodology creatively utilizes EHR data collected post-medical scan to enhance model pretraining, yielding superior performance over conventional methods that rely solely on imaging.\n2. **Comprehensive Evaluation:** The research includes extensive task comparisons, which affirm the robustness and effectiveness of the TTE-based approach across different architectures.\n3. **Dependence on Large EHR Datasets:** The effectiveness of the proposed approach is heavily reliant on the availability of extensive and high-quality EHR data, which may not be accessible in numerous medical datasets.\n4. **Limited Modality Scope:** The study exclusively tests the approach on CT images; examining a broader range of modalities could further establish its versatility across different imaging types.\n5. **Interpretability Issues:** The effects of TTE pretraining on specific pixel-level biomarkers remain ambiguous; a deeper analysis surrounding feature attribution is recommended.\n\n### Minor Comments\n- Clarification needed on the rationale for starting with 3D scans instead of 2D medical images; is this related to dataset selection, or has prior research addressed 2D data?\n- Inquiry about how the segmentation of time intervals for EHR data influences the model‚Äôs pretraining outcomes; specifically, what effect does the granularity of these intervals have on the performance of the pretrained encoder?\n\n### Summary Paragraph\nThe manuscript presents significant contributions by introducing an innovative TTE-based pretraining approach that enhances prognostic capabilities using 3D medical imaging in conjunction with EHR data. While the evaluation is comprehensive and robust, the reliance on large EHR datasets and the limited scope of tested modalities present notable challenges. Additionally, interpretability remains a concern, indicating areas for improvement. Overall, the study shows strong potential in terms of innovation and significance, though its evaluation and reproducibility may be hindered by external data dependencies and lack of broad modality validation.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a future-guided pretraining framework that integrates time-to-event (TTE) supervision with longitudinal electronic health record (EHR) data to enhance prognostic modeling from 3D medical images. The model aims to improve representation learning by predicting time-until-outcome following imaging acquisition. Experimental results show consistent performance gains over conventional imaging-only pretraining methods across multiple benchmark tasks. The paper is clearly written and technically sound, though some methodological dependencies and interpretability aspects warrant further clarification.\n\n**Major Comments**  \n1. **Dependence on Large EHR Datasets:** The proposed approach requires substantial, well-curated EHR collections synchronized with imaging data. Such resources are not widely available, limiting the method‚Äôs applicability to institutions with large integrated datasets.  \n2. **Limited Modality Evaluation:** Experiments are restricted to CT imaging, leaving open the question of whether the approach generalizes to other modalities (e.g., MRI, X-ray). Demonstrating cross-modality performance would strengthen the paper‚Äôs claims of generality.  \n3. **Interpretability and Feature Attribution:** The link between TTE-based pretraining and specific imaging biomarkers remains insufficiently explained. More analysis on which spatial features contribute to improved prognostic performance would improve transparency and trustworthiness.  \n4. **Impact of EHR Time Segmentation:** Further explanation is needed regarding how the temporal granularity of EHR-based time segments (e.g., 1-, 2-, or 3-year intervals) influences model performance. This design choice may substantially affect the learned representations.\n\n**Minor Comments**  \n- Clarify the rationale for focusing on 3D imaging rather than 2D modalities‚Äîwhether due to dataset availability or prior work in 2D settings.  \n- Ensure consistent terminology when referring to time-to-event intervals and survival probabilities for clarity.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study proposes an innovative fusion of imaging and longitudinal EHR data for representation learning, addressing an important problem in predictive medical imaging. It shows promising improvements (significance and innovation) supported by thorough experiments (evaluation). However, reproducibility may be constrained by data availability and incomplete methodological detail concerning time segmentation and feature interpretation.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The work is innovative and well-executed but requires additional justification on data requirements, temporal design choices, and modality generalization before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**IEEE TMI Peer Review Report**\n\n---\n\n### Phase 1: Analytical and Scope Audit\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a *methodological innovation* in medical image modeling ‚Äî a new *time‚Äëto‚Äëevent (TTE) pretraining* framework for 3‚ÄëD imaging encoders that integrates longitudinal EHR‚Äëderived temporal supervision. This lies firmly within the scope of IEEE *Transactions on Medical Imaging*, which emphasizes new imaging algorithms, reconstruction and modeling techniques, and machine‚Äëlearning methods grounded in imaging science rather than purely clinical application.\n\n**2. Novelty & Contribution Level**  \nThe study introduces the first large‚Äëscale *TTE pretraining paradigm for 3‚ÄëD medical images*. It bridges survival analysis and large‚Äëscale representation learning, moving beyond per‚Äëexam or static self‚Äësupervised schemes. The idea of using TTE distributions as supervision signals in pretraining is distinctly new for imaging foundation models. The contribution level is high, provided the implementation and validation are rigorous.\n\n**3. Technical and Experimental Rigor**  \nThe technical formulation of the piecewise exponential neural network and the PEANN‚Äëbased loss is mathematically consistent; the authors also present ablations, metrics (AUROC, Harrell‚Äôs C‚Äëindex, Brier score), and comparisons to strong baselines. However, parts of the methodology‚Äîsuch as details of task sampling from 8,192 events, data harmonization, and how censorship handling interacts with multitask pretraining‚Äîshould be explained more precisely. Experiments are extensive, use publicly available datasets (INSPECT and RSPECT), and include statistical confidence intervals. Overall rigor is good, though the theoretical justification for why TTE pretraining improves representation quality could be expanded.\n\n**4. Clarity and Presentation**  \nThe paper is clearly structured with detailed tables, appendices, and well‚Äëcommented figures. The terminology (‚Äúmissing context problem,‚Äù ‚Äúpiecewise exponential neural network‚Äù) is appropriate for the field. Minor language polishing could improve readability; currently, the manuscript is densely written.\n\n**5. Ethical and Reproducibility Compliance**  \n‚úÖ The authors use publicly released, de‚Äëidentified datasets, describe compliance with HIPAA requirements, and provide open code and checkpoints. Ethical considerations and subgroup bias analyses are included, aligning with reproducibility and fairness expectations of *TMI*.\n\n---\n\n### Phase 2: IEEE TMI Review Report\n\n#### 1. Summary\nThis paper proposes **time‚Äëto‚Äëevent (TTE) pretraining** for 3‚ÄëD medical image foundation models. The method links each imaging study to long‚Äëterm EHR outcomes to learn representations predictive not only of current disease but of *future events*. Using a **piecewise exponential neural network (PEANN)** as the survival objective, 8,192 EHR‚Äëderived TTE tasks are generated across ~19k CT exams. The pretrained encoder is evaluated by linear and Cox‚Äêbased probes on eight outcome‚Äëprediction tasks and eight diagnostic tasks. Results show mean gains of roughly 20‚Äì30‚ÄØ% in AUROC and Harrell‚Äôs‚ÄØC‚Äëindex on prognostic prediction without loss of diagnostic accuracy.\n\n#### 2. Strengths\n- **Methodological novelty**: introduces TTE modeling as a large‚Äëscale pretraining signal for 3‚ÄëD imaging.  \n- **Technical soundness**: clearly formulated PEANN loss and comprehensive experimental design.  \n- **Empirically broad**: multi‚Äëarchitecture validation (SwinUNETR, DenseNet, ResNet), multiple performance metrics, and subgroup fairness checks.  \n- **Reproducibility and openness**: code, model weights, and dataset identifiers are supplied.  \n- **Clinical relevance**: connects imaging features to long‚Äëterm outcomes, addressing a key gap in prognostic imaging.\n\n#### 3. Weaknesses\n- Limited theoretical explanation of *why* survival‚Äëbased supervision yields more transferable representations beyond increased label density.  \n- EHR‚Äìimaging alignment and task‚Äësampling procedure depend on several heuristic design choices; robustness to those choices is not explored.  \n- Evaluation restricted to chest CT; generalizability to other modalities, body regions, and tasks remains untested.  \n- Compute cost is extremely high (hundreds of GPU‚Äëhours), potentially limiting reproducibility for typical labs.  \n- Writing is occasionally repetitive; the message could be clearer with condensation of tables/figures.\n\n#### 4. Major Comments\n1. **Clarify the pretraining objective integration:** Expand Section‚ÄØ4 describing how multiple TTE tasks are optimized jointly ‚Äî whether losses are summed, weighted, or sampled per batch. Include sensitivity analysis for the number of time bins and task selection method (entropy‚Äëranked vertex cover).  \n2. **Provide deeper theoretical motivation:** Connect the survival likelihood to representation learning theory‚Äîwhy should time‚Äëdependent hazard supervision improve prognostic encoding?  \n3. **Generalization assessment:** Consider at least one non‚Äëthoracic dataset or 2‚ÄëD modality to test transferability beyond chest CT.  \n4. **Compute and scalability discussion:** Include a quantitative comparison of performance versus compute cost or label density to highlight efficiency gains.  \n5. **Bias and censoring:** Elaborate on how competing risks and varying censoring distributions affect training stability; include discussion of possible biases introduced by EHR‚Äëderived event times.\n\n#### 5. Minor Comments\n- Proofread for minor grammar inconsistencies (‚Äúpretrianing,‚Äù ‚Äúinspsect‚Äù).  \n- Some figures (e.g., Grad‚ÄëCAM montages, Appendix‚ÄØQ) could be resized or compressed; the main text may summarize key examples instead of showing all.  \n- Consolidate identical numeric results presented across tables/appendices; highlight essential findings in the primary text.  \n- Add consistent notation for survival time variables; some equations reuse symbols (T,‚ÄØÀúT) inconsistently.  \n- Cite recent TMI relevant works on self‚Äësupervised risk prediction if available.\n\n#### 6. Overall Recommendation  \n**Major Revision** ‚Äì The study is promising and methodologically innovative, clearly within TMI‚Äôs scope, but requires stronger justification, clearer methodological exposition, and possibly an additional demonstration of generalizability. With these revisions, it could reach TMI‚Äôs publication standard.\n\n#### 7. Confidence Level  \n**4/5** ‚Äì High confidence in assessment; the reviewer has expertise in medical imaging machine learning and time‚Äëto‚Äëevent modeling.\n\n---\n\n**Summary Decision:**  \nThe paper offers *substantive methodological advancement* with strong empirical evidence and reproducibility. Addressing the above major comments‚Äîparticularly on theoretical motivation, cross‚Äëmodality robustness, and detailed implementation clarity‚Äîwould likely make it a strong candidate for publication in *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces time-to-event (TTE) pretraining for 3D medical imaging models to address the \"missing context problem\" in current self-supervised learning approaches. The authors argue that existing methods fail to learn prognostic biomarkers because they rely only on imaging data and concurrent text descriptions, lacking temporal information about disease progression. Their method leverages longitudinal electronic health records (EHRs) to create 8,192 TTE pretraining tasks using piecewise exponential neural networks (PEANN). The approach is evaluated on 18,945 CT scans from the INSPECT dataset and tested on three architectures (SwinUNETR, DenseNet-121, ResNet-152). Results show average improvements of 23.7% in AUROC and 29.4% in Harrell's C-index across 8 benchmark tasks for prognostic prediction, while maintaining diagnostic classification performance on the RSPECT dataset.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and notation clarity issues**\n  - Equation 1 uses inconsistent notation where the survival function Si(t) depends on both patient i and time piece p, but the relationship between image-specific representations Mip and the hazard calculation is unclear (Section 3, Equation 1)\n  - The derivation of the loss function references Appendix M but the main text lacks sufficient mathematical detail to verify the PEANN implementation independently (Section 3, page 5)\n  - The piecewise exponential model assumes constant hazard rates within intervals, but the paper doesn't adequately justify this assumption for medical events or discuss potential violations (Section 3, Equation 1)\n\n‚Ä¢ **Limited experimental scope and generalizability concerns**\n  - The evaluation is restricted to chest CT scans from a single institution (INSPECT dataset), limiting generalizability across different imaging modalities, anatomical regions, and healthcare systems (Table 1, Section 5)\n  - Only three architectures are tested, with two (DenseNet, ResNet) using 2D weight inflation rather than native 3D pretraining, potentially confounding the comparison (Section 5, page 7)\n  - The baseline comparisons lack strong SSL methods specifically designed for 3D medical imaging, making it difficult to assess the true benefit of TTE supervision over state-of-the-art alternatives (Section 5, page 7)\n\n‚Ä¢ **Methodological limitations in task selection and evaluation design**\n  - The entropy-ranked vertex cover approach for selecting 8,192 pretraining tasks from 4.3 million candidates lacks sufficient detail and validation of task relevance (Section 4, page 5)\n  - The frozen encoder evaluation strategy may not reflect real-world deployment scenarios where full fine-tuning is often preferred (Section 4, page 6)\n  - Statistical significance testing uses bootstrap confidence intervals but doesn't adequately address multiple comparisons across numerous tasks and metrics (Section 5, Tables 2-4)\n\n‚Ä¢ **Insufficient analysis of temporal assumptions and competing risks**\n  - The method treats patient death as the only competing risk and censors at death, but medical events often have complex competing risk structures that could bias results (Section 4, page 6)\n  - The choice of uniform time bins for the piecewise exponential model lacks justification and sensitivity analysis across different temporal discretizations (Section 5, page 6)\n  - No evidence found in the manuscript for handling time-varying covariates or validating the proportional hazards assumption underlying the evaluation metrics\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation consistency**\n  - Provide complete mathematical derivations in the main text rather than relegating them to appendices, ensuring all notation is clearly defined and consistently used throughout\n  - Include formal justification for the piecewise exponential assumption with empirical validation on the medical event data\n  - Add sensitivity analyses examining how different choices of time intervals and hazard rate assumptions affect model performance\n\n‚Ä¢ **Expand experimental validation scope**\n  - Evaluate the approach on multiple datasets from different institutions and imaging modalities to demonstrate broader applicability beyond chest CT scans\n  - Include comparisons with recent 3D SSL methods specifically designed for medical imaging rather than relying primarily on 2D weight inflation baselines\n  - Conduct experiments using native 3D pretrained models for all architectures to ensure fair comparisons\n\n‚Ä¢ **Strengthen methodological foundations and evaluation protocols**\n  - Provide detailed algorithmic descriptions and validation studies for the entropy-ranked vertex cover task selection method, including ablation studies on different numbers of selected tasks\n  - Compare frozen encoder results with full fine-tuning approaches to better understand the trade-offs and practical deployment considerations\n  - Implement proper multiple comparison corrections and provide more comprehensive statistical analysis addressing the numerous hypothesis tests conducted\n\n‚Ä¢ **Address temporal modeling limitations comprehensively**\n  - Extend the competing risk analysis beyond death censoring to include other relevant competing events and validate these assumptions empirically\n  - Conduct systematic sensitivity analyses across different temporal discretization schemes and validate the impact on downstream performance\n  - Incorporate methods for handling time-varying covariates and provide empirical validation of key survival analysis assumptions using the available longitudinal data",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Alejandro Lozano",
      "Curtis Langlotz",
      "Ethan Steinberg",
      "Jason Alan Fries",
      "Jeya Maria Jose Valanarasu",
      "Louis Blankemeier",
      "Nigam Shah",
      "Akshay S Chaudhari",
      "Zepeng Frazier Huo"
    ],
    "url": "pdfs/iclr.cc-2025-conference_ab67303155dc14cfbb1febaf43b5bf38ae9bd5b0.pdf",
    "remote_url": "https://openreview.net/pdf/ab67303155dc14cfbb1febaf43b5bf38ae9bd5b0.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "Multimodal Large Language Model",
      "Biomedicine",
      "Region-Text"
    ],
    "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. \nMost current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence.\nTo mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans.\nTo achieve it, we first formulate \\textbf{Region-Centric tasks} and construct a \\textbf{large-scale dataset, MedRegInstruct,} to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a \\textbf{Region-Aware medical MLLM, MedRegA}, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. The codes and model will be made publicly available.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a new large-scale, multi-site, medical visual-question answering (VQA) dataset, called MedRegInstruct, with fine-grained region-specific annotation and multiple corresponding region-centric tasks. The proposed dataset contains multiple region-centric evaluation tasks including, region-to-text generation, text-to-region detection, and grounded medical report generation. The paper further proposed a new region-aware multi-modal large language model (MLLM), called, MedRegA, pre-trained with the proposed dataset and regional chain of thought (CoT). The proposed method outperforms multiple existing SOTA medical VQA models on the regional-centric evaluation.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n1. The proposed new region-centric medical dataset MedRegInstruct addressed the need for fine-grained, region-level, training and evaluation datasets in the medical domain. It is novel to the field and will help the development of the field as provides a more fine-grained training and evaluation annotation. It is proven that the method pre-trained with the new dataset can demonstrate more robust performance in the targeted region-specific tasks and also other related medical tasks. \n2. The paper has provided enough details about the data curation and evaluation, including the process of question-answering pair generation from medical data with region annotation, a prompt for each downstream evaluation, and detailed settings for each task. The author also seems to claim they will release the code and pre-trained model later.\n3. The paper is overall well written with nice figures and a clean presentation. It is easy to follow even if there are many details in it.\n\n### Weaknesses\n\n1. While the proposed dataset is novel and important to the field, the corresponding MedRegA model is not that novel, though I hate to say that, in comparison. The overall model design is basically the same as the base InternVL model and the major change is the training scheme and prompt formulation with the new dataset, which can be extended to other medical VQL models as well. While the author further proposed single-step regional CoT in the paper, it is still relatively straightforward. **Yet**, this is not a major drawback as the performance of this model demonstrates the superiority of training with the proposed dataset.\n2. Similarly, it seems that the baselines used in the evaluation are not all fine-tuned with the proposed dataset, which enables the model to understand regional information. Comparing the model trained on the proposed dataset with baselines that are not trained on this dataset is not very fair, to some extent. It would be interesting to see how well each baseline would improve if they were fine-tuned on the proposed dataset, even if just in a few-shot manner.\n3. Another concern about this paper is the quality of the data. Most of the regional bounding box-text pairs in the dataset were taken from the existing dataset and generated from an automated rule-based system and an LLM. The evaluation of the data quality is missing in this paper, which is very critical as the main purpose is **fine-grained regional evaluation,** where the quality of the bounding box and the correctness of the corresponding text description are very important. It would be better if some sort of data quality check could be done before releasing it to the public.\n\n### Questions\n\nOverall, this is a good paper with solid contributions and it has a significant meaning to the field. Still, the reviewer has a few questions here.\n\n1. How is the English version of the text report/caption generated for the in-house Chinese data? What kind of translation was used here?\n2. According to the ‚ÄúStructure Detection‚Äù section on page 5, the structure bounding box in the dataset was generated with a fine-tuned MLLM on the existing Region-Text dataset given the corresponding organ prompt. The reviewer wonders if any kind of evaluation was done on this fine-tuned MLLM, how accurate is the extract bounding box? And why not train a more straightforward detection model for this purpose? Considering that the specific detection model usually performs better than a general MLLM.\n3. Also, as mentioned above, the reviewer wonders how much will the other baseline medical MLLMs improve if they were trained with the proposed dataset. The comparison between InternVL and MedRegA in the paper is not very fair as the InternVL was only trained with natural image-text pairs. The reviewer understands it may take tons of extra time/money to conduct such an evaluation, but it would still be an interesting question to explore.\n4. According to the abstract, the code and model will be released to the public, but the reviewer wonders if the data will be released as well. This is pretty critical as the dataset is the major contribution of the paper. The answer to this question will influence the reviewer's final score for the paper.\n\n### Flag For Ethics Review\n\n- Yes, Privacy, security and safety\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe paper proposed a new medical visual question-answering dataset with in-house medical imaging data from a Chinese hospital. Though the author has an ethics statement section in the paper, further ethics evaluation may be needed to ensure there is no critical ethical issue.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript introduces a new large-scale, multi-site medical visual-question answering (VQA) dataset called MedRegInstruct, which features fine-grained region-specific annotations and several region-centric tasks, including region-to-text generation, text-to-region detection, and grounded medical report generation. Additionally, the authors present a region-aware multi-modal large language model (MLLM) named MedRegA, which is pre-trained on the MedRegInstruct dataset and a regional chain of thought (CoT). The proposed method shows superior performance compared to existing state-of-the-art medical VQA models in region-centric evaluations.\n\n### Major Comments\n1. **Model Novelty:** While the MedRegInstruct dataset is a crucial advancement for the field, the MedRegA model lacks novelty as its design closely resembles the base InternVL model, primarily differing in training scheme and prompt formulation. The introduction of a single-step regional CoT, though an improvement, remains straightforward.\n2. **Baseline Evaluation:** The evaluation comparisons may not be entirely fair, as not all baselines were fine-tuned on the proposed dataset that enables regional information comprehension. Investigating their performance after fine-tuning, even in a few-shot manner, would be valuable.\n3. **Data Quality Concerns:** A significant concern arises regarding data quality since the regional bounding box-text pairs were sourced from existing datasets and generated via an automated rule-based system and an LLM. A robust data quality evaluation is necessary before the dataset release, particularly given the emphasis on fine-grained regional evaluation.\n\n### Minor Comments\n- The paper is well-written with effective figures and clear presentation.\n- Clarification on the translation process used for the English captions generated from the in-house Chinese data would enhance understanding.\n\n### Summary Paragraph\nThe manuscript holds substantial significance with the introduction of the MedRegInstruct dataset, which innovatively focuses on region-specific medical VQA tasks. However, concerns exist regarding the novelty of the MedRegA model and the fairness of the evaluation against baseline models not trained on this dataset. Additionally, the lack of a data quality assessment poses a potential risk to the reproducibility of results derived from the dataset. Therefore, while the contributions are valuable, addressing these weaknesses is essential for the work's impact.\n\n### Decision Recommendation\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegInstruct*, a large-scale, multi-institutional medical visual question answering (VQA) dataset with fine-grained region-level annotations and multiple related region-centric tasks, including region-to-text generation, text-to-region detection, and grounded report generation. It also presents *MedRegA*, a region-aware multimodal large language model (MLLM) pre-trained on this dataset using a regional chain-of-thought mechanism. The paper is clearly written, well-structured, and easy to follow, with informative figures and a clean presentation. Overall, the work significantly contributes to the expansion of fine-grained, region-level medical VQA data and models.\n\n**Major Comments**  \n1. **Novelty of the Model:** While the dataset contribution is strong, the novelty of the proposed MedRegA model is limited. Its architecture largely follows the existing InternVL model, with the principal difference being the training scheme and regional prompts. The single-step regional chain-of-thought extension is conceptually straightforward. Although these modifications demonstrate improved performance, methodological innovation is modest.  \n2. **Fairness of Comparisons:** The evaluation may not be fully fair, as baselines were not fine-tuned on the proposed dataset. Since MedRegA benefits from this dataset‚Äôs regional annotations, the comparison potentially favors the new model. Additional experiments fine-tuning other baselines‚Äîeven in a few-shot setting‚Äîwould provide fairer comparisons and better contextualize gains.  \n3. **Data Quality Assessment:** The dataset combines bounding box‚Äìtext pairs extracted from existing datasets and those produced via automated rule-based and LLM-based systems. However, no quantitative or qualitative evaluation of data quality is provided. Given the paper‚Äôs emphasis on fine-grained regional tasks, verifying annotation quality (bounding box accuracy and text correctness) is essential.  \n4. **Dataset Release and Translation Details:** Clarification is needed on whether the dataset will be publicly released, as this is central to the paper‚Äôs impact. Additional details are also requested on how English captions were generated from Chinese data (e.g., translation methods).  \n5. **Region Detection Method:** The ‚ÄúStructure Detection‚Äù step reportedly uses a fine-tuned MLLM rather than a dedicated detection model. An evaluation of this step‚Äôs accuracy or a rationale for not using a specialized detector would strengthen the technical soundness.\n\n**Minor Comments**  \n- Please clarify if ethical approval and patient data anonymization procedures are confirmed for the in-house medical data.  \n- A few notational clarifications and implementation details (e.g., prompt formats) would improve reproducibility.  \n- Ensure the paper explicitly states future release plans for data, code, and models.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \n*Significance:* The proposed dataset fills an important gap in region-level medical VQA, enabling fine-grained reasoning.  \n*Innovation:* The dataset itself is the primary innovation; the model design is comparatively incremental.  \n*Evaluation:* Experimental results show strong performance, though fairness concerns remain regarding non-fine-tuned baselines and lack of data quality assessment.  \n*Reproducibility:* The methodology and experiments are described in detail, and prospective data/code release would enhance reproducibility. Ethical considerations regarding medical data warrant additional verification.\n\n**Decision Recommendation:** **Major Revision** ‚Äì Strong contribution through dataset introduction, but concerns about model novelty, fairness of evaluation, dataset quality validation, and the transparency of data release need to be addressed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this work, the authors tackle the current problem in MLLMs (multimodal large language models) of how they don't necessarily \"focus\" on particular regions, but rather take the entire image as context and then have to solve a variety of downstream tasks. To address this problem they offer two main sets of contributions. The first, is the formulation of several region specific tasks, namely region-to-text identification, text-to-region detection, and grounded report generation, which they combine into a dataset called MedRegInstruct. The second is the training strategy and modeling paradigm for a MLLM they propose, which they refer to as MedRegA. MedRegA is a bilingual, as it was trained on both English and Chinese paired datapoints, model that they apply to this larger dataset they prepare in comparison to currently available medical MLLMs.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n‚Ä¢ The paper really introduces another method of interaction, in the bounding boxes, that can be quite useful as MedRegA is able to reason about specific regions. This is an immensely useful tool for narrowing of problems as opposed to current trained MLLMs have to do.\n\n‚Ä¢ The proposed training strategy for learning how to incorporate the bounding boxes is simple and scalable, seems like it could be generally applied to other strategies that want to apply bounding boxes.\n\n‚Ä¢ Regional CoT makes a lot of sense intuitively and provides a nice bias for solving region specific problems.\n\n‚Ä¢ MedRegA has very strong performance on the chosen set of benchmarks and metrics in comparison to existing state of the art methods.\n\n### Weaknesses\n\n‚Ä¢ The section of the report grounding dataset is quite confusing as to how its constructed, and how much it relies on the models it uses for automation for providing correct labels/groundtruth. For example, is InternLM perfect at sub-select different parts of the reports? \n\n‚Ä¢ The paper claims to work across a broad set of modalities, but when considering the grounding dataset it seems that it only considers, as other MLLMs do, Chest Xrays?\n\n‚Ä¢ It's unclear if the comparison to existing baselines are exactly fair as the trained MedRegA has access to more data. In fact, compared to the baseline of InternLM, it should be strictly better because MedRegA is trained with InternLM as an initialization. While it's true that the baselines don't provide a mechanism for highlighting particular regions, they still should be adapted to the new data that the authors are evaluating on.\n\n### Questions\n\n‚Ä¢ What was the filtering criterion for reducing SA-Med2D-20M?\n\n‚Ä¢ Is there a noticeable performance difference between English and Chinese tasks? Is there paired data that this can be evaluated on? ‚Ä¢ ‚Ä¢ ‚Ä¢ \n\n‚Ä¢ Overall, it is unclear from the paper why exactly these two particular languages are used, other than being able to source data for both, rather than just arbitrary languages with medical report data.\n\n‚Ä¢ Is the MedRegInstruct dataset planning to be release?\nFor the Region-Text dataset, why is the case that there needs to be this split into half for region-to-text and half for text-to-region? Can't every datapoint server the purpose of being either during training?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript addresses the challenge faced by multimodal large language models (MLLMs) regarding their lack of focus on specific regions within images, which limits their effectiveness in performing certain downstream tasks. The authors contribute a solution through the development of region-specific tasks, encapsulating these in a dataset called MedRegInstruct. Additionally, they propose a new MLLM model, MedRegA, trained on both English and Chinese data, which they evaluate against existing medical MLLMs.\n\n### Major Comments\n1. The formulation of tasks such as region-to-text identification, text-to-region detection, and grounded report generation is innovative and potentially enhances the functionality of MLLMs in medical contexts.\n2. However, the construction of the report grounding dataset lacks clarity, particularly regarding its reliance on automation for accurate labeling, raising questions about the effectiveness of the model used for labeling.\n3. The claim of multi-modality in the manuscript is questionable, as the grounding dataset appears limited to chest X-rays, similar to other MLLMs.\n4. The fairness of baseline comparisons is uncertain; MedRegA seems to have advantages due to its training data access, which raises concerns about the validity of performance claims relative to existing models like InternLM.\n\n### Minor Comments\n- Clarification is needed regarding the filtering criteria for SA-Med2D-20M.\n- Is there a notable performance difference between the tasks conducted in English and Chinese, and is there paired data to reflect this?\n- The rationale behind selecting English and Chinese for data sourcing is not adequately justified.\n- Clarification on whether the MedRegInstruct dataset will be released is necessary.\n- The decision to split the dataset into half for region-to-text and half for text-to-region tasks warrants further explanation; can‚Äôt each data point serve both purposes in training?\n\n### Summary Paragraph\nIn terms of significance, the manuscript presents a novel approach to incorporating bounding boxes in MLLMs, which enhances the focus on specific regions and offers a scalable training strategy applicable to other efforts. While its innovation lies in the proposed tasks and modeling approach, concerns remain regarding the clarity of the dataset construction and the fairness of comparative evaluations. The reproducibility of findings is also questioned given the chosen modalities and the lack of detailed performance analysis across languages. Overall, the manuscript shows promise but requires critical improvements in clarity and methodological robustness.\n\n### Decision Recommendation\nMajor revision is recommended to address the outlined concerns and enhance the overall quality and transparency of the research.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegA*, a bilingual multimodal large language model (MLLM) designed to improve region-level reasoning in medical imaging. To address the limitation of current MLLMs that process entire images globally, the authors propose region-specific tasks‚Äîregion-to-text identification, text-to-region detection, and grounded report generation‚Äîcompiled into a new dataset named *MedRegInstruct*. They further describe a training paradigm integrating bounding box information and a ‚ÄúRegional Chain of Thought‚Äù (CoT) mechanism. Overall, the paper is clearly written and presents a technically sound approach with strong empirical results on several benchmarks.\n\n**Major Comments**  \n1. **Dataset Construction Clarity:** The description of the report grounding dataset is confusing. Details about its construction, the extent of automation, and the accuracy of models (e.g., InternLM) used to generate labels are not sufficiently explained.  \n2. **Scope of Modalities:** Although the paper claims multimodal generality, it appears that only chest X-rays are considered for grounding. The extent of modality coverage should be clarified.  \n3. **Fairness of Comparisons:** The fairness of comparisons with baselines is uncertain. MedRegA benefits from more data and is initialized from InternLM, which may bias results. It is unclear whether baseline models were adapted to the same data for a fair evaluation.  \n4. **Language Choices:** The motivation for selecting English and Chinese as training languages requires elaboration beyond data availability. Clarify whether paired bilingual datasets are used and if performance varies across languages.  \n5. **Dataset Release and Design Decisions:** Specify whether *MedRegInstruct* will be publicly released. Additionally, explain the rationale for dividing region-text pairs into separate region-to-text and text-to-region subsets instead of allowing bidirectional training from all examples.\n\n**Minor Comments**  \n- Provide more detail on the filtering criterion used to reduce the SA-Med2D-20M dataset.  \n- Minor editorial suggestions include improving clarity in the dataset section and explicitly stating figure details and dataset statistics.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript is significant for introducing a novel way to incorporate spatial localization into MLLMs through bounding box‚Äìbased interaction and Regional CoT reasoning. Its innovation lies in extending multimodal comprehension to region-focused learning with a straightforward, scalable training scheme. Evaluation results are strong, though the data and initialization advantages limit fairness in baseline comparisons. Reproducibility is promising given the model and dataset structure, but public release details remain uncertain.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nWhile the contribution is innovative and performance promising, issues regarding dataset construction, language rationale, and fairness of comparisons must be more clearly addressed before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work introduces a novel region-centric task and presents a large-scale Chinese medical dataset named MedRegInstruct. To address this new task, the authors developed a bilingual multimodal large language model, MedRegA, which outperforms baseline methods across most evaluation metrics.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.\tThe paper is well-organized.\n2.\tThis work introduces a novel task alongside a large-scale medical dataset.\n3.\tThe authors propose a new model that significantly outperforms other models across the majority of datasets and evaluation metrics, particularly for the newly proposed task.\n\n### Weaknesses\n\n1.\tThe article‚Äôs description of the model structure in the main text is incomplete, relying heavily on Figure 4 to convey the model‚Äôs architecture. For example, the structure of the encoder and tokenizer remains ambiguous. The authors should clarify which components utilize existing models, which are fine-tuned, and which are trained from scratch.\n2.\tThe fairness of the model comparison is questionable. In Table 1, only a subset of models has been fine-tuned based on the test dataset, leaving others unmodified. Is this due to an inability to fine-tune certain models, or for another reason? The authors should provide detailed criteria for choosing to fine-tune specific models and address the fairness of this experimental design.\n3.\tOn the newly proposed task, the authors‚Äô model demonstrates a substantial performance advantage. They mention that existing open-source models cannot accurately capture coordinates, but it would be insightful to know if fine-tuning on the authors‚Äô proposed dataset could improve this. If fine-tuning other models would yield better results, the authors should do so for a more persuasive comparison. Conversely, if fine-tuning has no impact, the authors should explain the model‚Äôs unique advancements to provide deeper insights.\n4.\tThe role of the CoT component is somewhat unclear, with insufficient ablation experiments to support its effectiveness. The authors should conduct a more specific ablation study to illustrate the impact of CoT rather than merely reporting improved results with its inclusion.\n5.\tWhile the authors claim that the proposed model can perform diagnostic tasks, there is no clear evaluation or user study provided. A brief evaluation or explanation of the diagnostic results would add clarity.\n6.\tSeveral prior studies focus on medical image segmentation, which aligns well with the proposed region-centric task. Have the authors considered using such methods as feature extractors, backbones, or even as comparative models?\n7.\tThe experimental results on report generation are insufficient, as the authors rely solely on descriptive analysis without quantitative evaluation or user studies. Similar to point 5, if the authors wish to validate effectiveness without explicit evaluation metrics, a user study would provide valuable support.\n\n### Questions\n\n1.\tPlease explain why certain models were fine-tuned in the comparison while others were not.\n2.\tThe authors state that adding CoT improves model performance; however, no comprehensive ablation experiments were conducted to confirm this.\n3.\tHave the authors considered comparisons using other large models as the backbone?\n4.\tDo the authors plan to open-source this dataset?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a novel region-centric task along with the introduction of a large-scale Chinese medical dataset named MedRegInstruct. The authors have developed a bilingual multimodal large language model, MedRegA, which reportedly outperforms baseline methods across most evaluation metrics. The paper is well-organized and contributes to the existing body of knowledge in medical AI by defining a new task and dataset.\n\n**Major Comments:**\n1. The description of the model structure in the main text is incomplete; it relies heavily on Figure 4, and the architecture specifics, such as the encoder and tokenizer details, are unclear. The authors should clarify which components utilize existing models versus those that are fine-tuned or trained from scratch.\n2. The fairness of the model comparison is questionable; in Table 1, only some models were fine-tuned with the test dataset, while others were left unmodified. It is unclear whether this decision was due to limitations in fine-tuning certain models or other reasons. The authors should detail criteria for model selection and address the experimental fairness.\n3. The authors mention that existing models struggle with coordinate capture; additional analysis on whether fine-tuning could enhance performance is needed. If other models might yield better outcomes with fine-tuning, such comparisons should be made.\n4. The role of the CoT component is not sufficiently explained, and there is a lack of supporting ablation studies to validate its effectiveness. The authors should conduct a specific ablation study to illustrate its impact.\n5. The manuscript lacks clear evaluations or user studies to support the claim of the proposed model's diagnostic task performance; a brief evaluation should be provided to enhance clarity.\n6. The authors should consider using prior studies on medical image segmentation as feature extractors or comparisons, given their relevance to the region-centric task.\n7. The experimental results on report generation are lacking, relying only on descriptive analysis. A quantitative evaluation or user studies would bolster the assessment of effectiveness.\n\n**Minor Comments:**\n- Clarify the experimental design and structure throughout the manuscript for improved understanding.\n- Address typos or inconsistencies in figure labeling.\n- Enhance definitions of acronyms and ensure clarity in technical terms used.\n\n**Summary Paragraph:**\nIn summary, this manuscript presents a significant contribution to the field by introducing a new task and dataset, demonstrating innovation through the development of MedRegA. However, there are concerns regarding evaluation design, particularly in terms of fairness in model comparisons and a lack of detailed evaluations. While the results show promise, reproducibility may be affected by the insufficient clarity around model structure and comparative evaluations. Overall, the manuscript requires further refinement to ensure robust and reproducible findings.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedRegA*, a bilingual multimodal large language model developed to address a newly introduced region-centric task in medical imaging. It also contributes a large-scale Chinese medical dataset, *MedRegInstruct*, as a resource for this task. The paper is well-structured and demonstrates that the proposed model outperforms baseline methods across most evaluation metrics. Overall, the work offers a novel problem formulation and dataset with solid experimental results, though several methodological and evaluation aspects require clarification.\n\n---\n\n**Major Comments**  \n1. **Model Architecture Clarity:** The description of *MedRegA*‚Äôs architecture is incomplete. Essential details such as the encoder and tokenizer structure are only implied through Figure 4. The authors should clearly specify which components are inherited from existing models, which are fine-tuned, and which are trained from scratch.  \n2. **Fairness of Model Comparison:** In Table 1, only some comparative models are fine-tuned on the test dataset, raising questions about fairness. The rationale for this selective fine-tuning should be provided, including whether technical limitations prevented tuning some models.  \n3. **Evaluation on the Proposed Task:** The paper attributes much of the performance advantage to the model‚Äôs ability to capture coordinates. It would be helpful to test whether fine-tuning existing open-source models on *MedRegInstruct* improves their performance; otherwise, a clearer explanation of what uniquely enables *MedRegA*‚Äôs success is needed.  \n4. **CoT Component Analysis:** The contribution of the Chain-of-Thought (CoT) module is unclear due to limited ablation analysis. More targeted ablation studies are needed to demonstrate its specific impact.  \n5. **Diagnostic Capability Claims:** The claim that *MedRegA* can perform diagnostic tasks is not substantiated by quantitative or user-based evaluations. A brief diagnostic evaluation or qualitative analysis would enhance credibility.  \n6. **Comparison with Segmentation-Based Methods:** Since medical segmentation aligns with the region-centric concept, the authors should discuss or test segmentation methods as feature extractors, backbones, or baselines.  \n7. **Report Generation Evaluation:** Results for report generation rely solely on descriptive analysis. Quantitative or user study‚Äìbased evaluation would strengthen the evidence.\n\n---\n\n**Minor Comments**  \n- Clarify terminology and acronyms (e.g., CoT).  \n- Ensure model figures and tables are self-contained and interpretable.  \n- Minor improvements in clarity would aid reproducibility.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in defining a new region-centric vision-language task and providing an accompanying large dataset. Its **innovation** lies in the bilingual, multimodal model that integrates coordinate reasoning. However, the manuscript‚Äôs **evaluation** requires stronger baselines, clearer fine-tuning protocols, and more comprehensive ablation studies. **Reproducibility** remains limited by incomplete methodological transparency and lack of opened code or dataset (pending). Addressing these issues would greatly strengthen the contribution.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase 1 ‚Äì Analytical and Scope Audit**\n\n### **1. Scope & Relevance to TMI**\n‚úÖ **Within scope.**  \nThis paper presents a **novel methodological contribution** centered on enhancing medical multimodal large language models (MLLMs) with *region-centric understanding*. The work integrates spatial localization (region-to-text, text-to-region, grounded report generation) into a large bilingual medical foundation model. The emphasis on interpretability, spatial grounding, structured dataset design, and model formulation positions it squarely within **medical imaging methodology** ‚Äî particularly image understanding and vision-language modeling. Suitable for IEEE *TMI*.\n\n### **2. Novelty & Contribution Level**\nThe introduction of a **region-aware medical MLLM** and the formulation of **region-centric tasks** (MedRegInstruct dataset, Region-CoT mechanism, and Region-Aligned evaluation scheme) represent a step beyond prior ‚Äúglobal‚Äù or holistic MLLMs such as Med-Flamingo, RadFM, and MedDr.  \nKey conceptual advances:\n- Structured modeling of image-region-text alignment;\n- Semi-automated curation pipeline integrating bounding-box annotations into bilingual reports;\n- Introduction of interpretable, region-aware grounding in multimodal tasks.  \nThe novelty is incremental relative to major foundation-model efforts but methodologically strong and more interpretable ‚Äî at or just above *TMI‚Äôs novelty threshold.*\n\n### **3. Technical and Experimental Rigor**\n- **Dataset construction:** detailed description, clear provenance (public + institutional data), semi-automatic annotation, and expert validation (‚âà93% textual accuracy, 72% bounding-box accuracy).  \n- **Modeling:** builds upon InternVL foundation; clearly stated two-stage training (alignment ‚Üí instruction tuning); includes mathematical formulations of tasks and explicit representation (<box> tags).  \n- **Evaluation:** extensive benchmarking across >15 datasets, region-specific metrics, and comparisons with six baselines.  \n- Potential gaps: limited discussion of generalization to unseen modalities, ablation of dataset quality, and clinical verification of generated regions.\n\n### **4. Clarity and Presentation**\n- Generally **well written and organized**, with coherent narrative from motivation through experiments.\n- Minor grammatical issues, overuse of long sentences, and inconsistent figure captions (e.g., Figure 5/6 labeling).\n- Results tables are comprehensive but dense ‚Äî clarity could improve via summarizing key metrics.\n\n### **5. Ethical and Reproducibility Compliance**\n- Proper anonymization, IRB-like statement provided, clear delineation of public vs. private data.\n- Ethical practices well documented.\n- Reproducibility: model and code availability implied but not confirmed; specifying release plan for MedRegInstruct (public vs. derived subsets) would be helpful.\n\n---\n\n## **Phase 2 ‚Äì IEEE TMI Review Report**\n\n### **1. Summary**\nThe manuscript introduces **MedRegA**, a bilingual multimodal large language model for medical imaging that explicitly models **region-level understanding**. The authors define three region-centric tasks ‚Äî region-to-text identification, text-to-region detection, and grounded report generation ‚Äî and create the **MedRegInstruct** dataset (‚âà790K samples). Leveraging a hybrid of public and in-house clinical data, the model integrates bounding-box‚Äìbased regional encoding, a *Regional Chain-of-Thought* reasoning strategy, and a *Region-Aligned evaluation framework*. Extensive experiments demonstrate superior performance over prior medical MLLMs (MedDr, RadFM, BiomedGPT, etc.) across vision-language and classification tasks, with enhanced interpretability via spatial grounding.\n\n### **2. Strengths**\n- **Methodological novelty:** clear transition from global to region-aware MLLMs in medicine.\n- **Dataset innovation:** semi-automatic bilingual dataset that supports grounding and interpretability.\n- **Comprehensive evaluation:** large experimental suite across multiple modalities and languages.\n- **Clinical interpretability:** bounding-box reasoning improves trust and transparency.\n- **Ethical & bilingual scope:** multilingual component broadens utility for global medical datasets.\n\n### **3. Weaknesses**\n- **Novelty breadth:** approach builds atop existing LLM backbones; novelty lies in dataset and task definition rather than underlying architecture.\n- **Evaluation limitations:** limited human expert validation of clinical correctness and no reader study.\n- **Annotation accuracy:** automated bounding-box localization (72%) may limit reliability for fine-grained tasks.\n- **Reproducibility:** unclear public release of weights or dataset subsets; details on computing resources could be expanded.\n- **Writing:** some redundancy; figures occasionally crowded.\n\n### **4. Major Comments**\n1. **Clarify methodological novelty** relative to prior grounded-reporting models (e.g., Maira‚Äë2, AutoRG‚ÄëBrain). Explicit comparison or ablation without region-centric training would strengthen claims.\n2. **Validation of regional accuracy**‚ÄîConsider radiologist-level qualitative evaluation of MedRegA‚Äôs localized predictions to substantiate interpretability and clinical realism.\n3. **Dataset transparency**‚ÄîProvide explicit plan for releasing de‚Äëidentified subsets and annotation tools, consistent with IEEE TMI reproducibility.\n4. **Generalization analysis**‚ÄîInclude experiments on unseen modalities or pathologies to assess model robustness and domain shift sensitivity.\n5. **Ablation and explainability**‚ÄîQuantify how each component (MedRegInstruct, Regional¬†CoT) contributes to improvements on VQA and report generation.\n\n### **5. Minor Comments**\n- Improve caption clarity in Figures¬†2,¬†4,¬†6.\n- Correct minor typos (e.g., ‚Äúr egions t hey‚Äù, ‚Äúinterpertability‚Äù).\n- Provide parameter counts and inference speed.\n- Consider compressing large tables or moving secondary results to supplementary material.\n- Clarify whether ‚ÄúRegion-Aligned Accuracy‚Äù ‚â•0.5 denotes threshold or normalized metric.\n\n### **6. Overall Recommendation**\n**Recommendation:** ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ¬†**Major Revision**  \nWhile solid and relevant, the paper‚Äôs publication in *IEEE Transactions on Medical Imaging* would benefit from strengthened empirical validation (clinical/qualitative) and refinement of the novelty claims. The methodological framework is promising and aligns with *TMI‚Äôs* emphasis on interpretable medical imaging AI.\n\n### **7. Confidence Level**\n**4 / 5** ‚Äì High confidence based on manuscript clarity, literature familiarity, and reproducible details.\n\n---\n\n**Summary Judgment:**  \nAn ambitious and well‚Äëstructured paper introducing a region‚Äëaware paradigm for bilingual medical MLLMs. Methodologically interesting and well executed, yet requiring stronger experimental justification of clinical interpretability and fuller comparative discussion before meeting *IEEE‚ÄØTMI* acceptance standards.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MedRegA, a bilingual multimodal large language model designed to address region-centric tasks in medical imaging. The authors identify that existing medical MLLMs are region-agnostic, treating entire images holistically without focusing on specific anatomical regions. To address this limitation, they propose three region-centric tasks: Region-to-Text Identification, Text-to-Region Detection, and Grounded Report Generation. The authors construct MedRegInstruct, a large-scale dataset containing 550K region-text triplets and 240K grounded reports from both public datasets and 25K clinical scans from Sun Yat-Sen Memorial Hospital. MedRegA is built upon InternVL 1.2 and trained using alignment training followed by instruction tuning. Experimental results show improvements over existing methods across visual question answering, report generation, and medical image classification tasks, with the model achieving bilingual capabilities in both English and Chinese.\n\n## Weaknesses\n\n‚Ä¢ **Dataset Construction and Validation Concerns**\n  - The automatic labeling system for structure detection achieves only 72% accuracy compared to human annotations (Page 5), which may introduce systematic errors that propagate through training and evaluation\n  - Human validation is limited to only 50 samples with 2 experts (Page 5), providing insufficient statistical power to assess dataset quality across the claimed 790K total samples\n  - The rule-based strategy for organ summarization and LLM-based report segmentation lacks detailed validation metrics beyond sentence-level accuracy of 93.33% (Page 5)\n\n‚Ä¢ **Mathematical Formulation and Technical Specification Issues**\n  - Bounding box coordinates are normalized to integers within [0, 1000) (Page 6) but the paper lacks explanation of why this specific range was chosen or how it affects model performance\n  - The Region-Aligned evaluation framework introduces multiple complex metrics (Figure 6, Algorithm 1) but provides no theoretical justification or convergence analysis for the Hungarian Matching algorithm application\n  - Loss function specification is vague, stating only \"language model loss is applied\" (Page 6) without mathematical formulation or discussion of how region-specific losses are weighted\n\n‚Ä¢ **Experimental Design and Evaluation Limitations**\n  - Baseline comparisons are problematic as most existing models cannot perform region-centric tasks (Table 1 shows \"‚úó\" for most baselines), making performance comparisons potentially misleading\n  - The Regional CoT evaluation (Table 14, Figure 19) lacks proper ablation studies to isolate the contribution of regional information versus improved prompting strategies\n  - Test set construction methodology is unclear, particularly for the bilingual evaluation where translation quality could significantly impact results (Tables 8, 10)\n\n‚Ä¢ **Reproducibility and Implementation Concerns**\n  - Training details are relegated to appendix with insufficient hyperparameter specification in the main paper (Page 6 references \"Appendix B\" for critical implementation details)\n  - The semi-automatic pipeline for dataset construction involves multiple complex steps (Figure 3) but lacks sufficient detail for reproduction, particularly regarding the \"Doctor Finetuned MLLM\" component\n  - Clinical data collection protocol and ethical approval details are mentioned only briefly in ethics statement (Page 11) without adequate description of data preprocessing and anonymization procedures\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen Dataset Validation and Quality Assessment**\n  - Expand human validation to at least 500 samples across different modalities and anatomical regions to provide statistically meaningful quality assessment\n  - Provide detailed inter-annotator agreement scores (e.g., Cohen's kappa) for both bounding box accuracy and report segmentation quality\n  - Include analysis of how the 72% structure detection accuracy affects downstream model performance and implement error correction mechanisms\n\n‚Ä¢ **Enhance Mathematical Rigor and Technical Clarity**\n  - Provide mathematical justification for the [0, 1000) normalization range choice and include ablation studies comparing different coordinate encoding schemes\n  - Formalize the loss function with explicit mathematical notation, including how region-specific and text-specific losses are combined and weighted\n  - Add theoretical analysis of the Hungarian Matching algorithm's computational complexity and convergence properties within the evaluation framework\n\n‚Ä¢ **Improve Experimental Design and Analysis**\n  - Develop region-capable versions of baseline models through fine-tuning on region-centric tasks to enable fair comparison, or clearly acknowledge this limitation in the evaluation\n  - Conduct comprehensive ablation studies separating the effects of regional information, improved prompting, and bilingual training\n  - Implement cross-validation procedures for test set construction and provide confidence intervals for all reported metrics\n\n‚Ä¢ **Enhance Reproducibility and Documentation**\n  - Move critical implementation details from appendix to main paper, including complete hyperparameter specifications, training procedures, and convergence criteria\n  - Provide detailed algorithmic descriptions and code snippets for the semi-automatic dataset construction pipeline\n  - Include comprehensive supplementary materials with dataset statistics, preprocessing steps, and detailed experimental protocols to ensure full reproducibility",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Haonan Wang",
      "Honglong Yang",
      "Jiaji Mao",
      "Jun Shen",
      "Lehan Wang",
      "Xiaomeng Li",
      "Zehong Yang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2b1c7dbf52c032b412c014b02b84f27ac218e3ee.pdf",
    "remote_url": "https://openreview.net/pdf/2b1c7dbf52c032b412c014b02b84f27ac218e3ee.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical Foundation Model",
      "Multimodal Dataset",
      "Vision-Language Pretraining."
    ],
    "abstract": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with multigranular annotations for more than 65 diseases. These multigranular annotations encompass both global information, such as modality and organ detection, and local information like ROI analysis, lesion texture, and region-wise correlations. Unlike the existing multimodal datasets, which are limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations in the form of image-ROI-description triplets without the need for any paired text descriptions. Specifically, data from over 30 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M, achieving state-of-the-art performance on VQA-RAD, SLAKE, and PathVQA, surpassing representative SOTA multimodal large language models. Furthermore, MedTrinity-25M can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. We will make our dataset available. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces MedTrinity-25M, a comprehensive and large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with detailed annotations for more than 65 diseases. The dataset provides the most enriched annotations for various multimodal tasks, such as captioning, classification, and segmentation, and supports the pre-training of advanced multimodal medical AI models, achieving state-of-the-art performance.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe paper introduces MedTrinity-25M, the largest multimodal medical dataset to date, featuring multigranular annotations and containing over 25 million triplets (image-ROI-description). The development of an automated data annotation pipeline significantly scales up medical image-text data.\n2.\tWith the support of MedTrinity-25M, the pretrained CLIP and LLaVA models demonstrate better performance compared to previous methods.\n3.\tThe release of this dataset contributes to the medical AI community, providing researchers and practitioners with a valuable resource for advancing multimodal tasks and improving healthcare applications.\n\n### Weaknesses\n\n1.\tThe quality of the generated image-text data may not be sufficiently high. It is advisable to review the questions associated with this data. In the image-captioning constructed from MedTrinity-25M, we found that many basic imaging modalities were incorrectly identified. For instance, in the CT-RATE data used as the source, over 60,000 images were misidentified as MRI, more than 90,000 as X-ray, and even a small number were identified as endoscopy. Below, we provide some captions to illustrate this phenomenon present in MedTrinity-25M.\n‚Äú‚Äù‚Äù\n{\"image_path\":\"seg_train_126_a_2-right lung.nii-34.jpg\",\"id\":\"b573995e-2d11-11ef-bbea-f02f74942466\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the thoracic region, showing the heart, part of the lung, and the spine. The heart is centrally located with the lung on either side and the spine running vertically in the background. The region of interest, located in the left-center at the bottom of the image, appears to have a different texture and intensity compared to the surrounding tissue, suggesting a possible abnormality. This region's relative position is towards the lower left side of the image, adjacent to the lower part of the lung. The content within this region may indicate a disease process, which could be related to or affecting the adjacent lung tissue. Given the proximity to the lung, it is possible that the abnormality could be influencing or being influenced by the pulmonary structures.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_126_a_2-right lung.nii-11.jpg\",\"id\":\"b57345c6-2d11-11ef-a899-f02f74942466\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the thoracic region, showing the heart, part of the lung, and the spine. The heart is centrally located with the lung on either side and the spine visible posteriorly. The region of interest, located in the lower-middle left-center of the image, shows an area with altered signal intensity, which is indicative of a pathological condition. This area is situated within the lung tissue and is characterized by a texture and signal intensity that differ from the surrounding healthy lung tissue, suggesting the presence of a disease process such as consolidation, infection, or a mass.\\n\\nThe region of interest may be related to the surrounding lung tissue in that it could represent a localized disease process affecting the lung, potentially leading to or resulting from changes in the adjacent lung parenchyma. Given the nature of MRI imaging and the appearance of the region, it is possible that this area could be associated with a process such as inflammation, demyelination, or a neoplastic growth, which may have implications for the function of the adjacent lung tissue.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_14684_a_1-covid-19 infection.nii-76.jpg\",\"id\":\"a77ee972-2d78-11ef-bdc4-f02f74942576\",\"caption\":\"The image is a radiographic scan, likely a chest X-ray, showing the thoracic region with the trachea and main bronchi appearing open, indicating no occlusive pathology. The lungs exhibit minimal emphysematous changes, and there are pleuroparenchymal sequelae changes at the apex of both lungs. Additionally, there are linear atelectasis in the middle lobe of the right lung and the lingular segment of the left lung upper lobe. The heart contour and size are normal, and there are no pleural or pericardial effusions. The mediastinal structures are not optimally evaluated due to the absence of contrast material, and the mediastinal main vascular structures are of normal width. No enlarged lymph nodes or pathological wall thickness increase are observed in the esophagus. The thoracic vertebral corpus shows normal heights, alignment, and densities, with osteophytes at the vertebral corpus corners, and the neural foramina are open.\\n\\nThe region of interest, located centrally and in the upper-middle area of the image, occupying approximately 0.3% of the area, corresponds to the lung apices where pleuroparenchymal sequelae changes are noted. These changes are characterized by alterations in lung parenchyma texture, which may appear as irregularities or areas of increased density compared to the surrounding lung tissue.\\n\\nThe pleuroparenchymal sequelae changes in the lung apices may be related to the emphysematous changes in the lungs, as both conditions can result from chronic inflammatory processes, suggesting a possible pathophysiological connection between the two findings.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_9381_a_2-covid-19 infection.nii-145.jpg\",\"id\":\"9880af92-2c1e-11ef-b9dd-f02f74942466\",\"caption\":\"The image is a chest X-ray showing a cross-sectional view of the thoracic cavity, including the lungs, heart, and part of the spine. A region of interest is located at the periphery of the thoracic cavity, likely within the lung tissue, which appears as a darker area compared to the surrounding lung parenchyma. The region of interest, which is abnormal, shows an area of increased opacity that suggests the presence of a pathological condition, possibly a mass or lesion within the lung tissue. This abnormal area is indicative of a disease process, which could be related to the surrounding lung tissue either as a primary pathology or as a secondary effect of a systemic condition affecting the lung. The abnormality's proximity to other structures within the thoracic cavity, such as the pleura or lung tissue could imply a relationship where the disease process in the region of interest may have originated from or is affecting adjacent areas, potentially impacting nearby structures due to its location within the thoracic cavity.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_7973_a_1-right lung.nii-84.jpg\",\"id\":\"88af61aa-2d11-11ef-959e-f02f74942466\",\"caption\":\"The image is a sagittal section of an endoscopic view of the thoracic region, showing the spine, part of the lung, and the surrounding thoracic structures. A region of interest is located at the lower-middle part of the image, horizontally left-center, occupying approximately 0.6% of the area. The region of interest, located in the lower-middle left-center of the image, displays an abnormality in the lung tissue, which appears to be a small, localized area with a different texture and possibly altered density compared to the surrounding lung parenchyma, suggesting a pathological change. This abnormality could be related to the adjacent lung tissue, potentially indicating a localized disease process or lesion that may be affecting or resulting from the surrounding lung tissue, given the proximity and the nature of the disease knowledge provided.\",\"source\":\"ct_rate\"}\n‚Äú‚Äù‚Äù\nMoreover, there are also issues with modality misidentification in the image-captioning derived from the quilt-1m dataset. For example, pathology images were misidentified as X-ray and MRI, as follow, \n‚Äú‚Äù‚Äù\n{\"image_path\":\"G-tdJ0oZxJ4_image_e74e3372-a40e-40e3-ac06-e3d53eeab845.jpg\",\"id\":\"f5ce616b-89ca-41f6-b820-b480bb3327af\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the lung, showing a cross-sectional view of the thoracic cavity. A region of interest is located at the top of the image, which appears to be in the upper part of the lung field, likely within the upper lobe, given the position of the lung's anatomy. The region of interest, which is abnormal, exhibits an unusual appearance compared to the surrounding lung tissue, possibly indicating a neoplastic process. This abnormality is characterized by a difference in texture and density, which may suggest a pathological change, such as a mass or lesion within the lung tissue. The affected area's relationship to the rest of the lung tissue could imply a localized pathological process that may be the primary site of the disease, potentially impacting or being impacted by adjacent lung structures due to its proximity to other regions, although the exact relationship depends on the nature of the pathology and its progression.\",\"source\":\"quilt_1m\"}\n{\"image_path\":\"895313503048712192_0.jpg\",\"id\":\"0b53a8c8-5661-449a-b39a-f92887fceb86\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the brain, showing a cross-sectional view that includes brain tissue with various shades of gray indicating different structures and densities. A region of interest is located at left-center part of the image, which appears to be in the cerebral hemisphere, likely within the white matter of the brain. The region of interest exhibits an abnormality that differs in texture and possibly size from the surrounding brain tissue, suggesting the presence of a pathological change. This abnormal area could be related to the surrounding brain structures, potentially affecting or being affected by them due to its proximity and the nature of brain tissue, which may indicate a pathological process such as a tumor, edema, or other brain abnormalities. The abnormality's characteristics, such as altered signal intensity, could be indicative of the disease process affecting the brain tissue's function or structure.\",\"source\":\"quilt_1m\"} \n{\"image_path\":\"962043872649011205_0.jpg\",\"id\":\"f94e8ba4-7ddf-450e-9616-d4681e9dcf02\",\"caption\":\"The image is a chest X-ray image of a 15-year-old boy's hand, showing the left side of the image is a chest X-ray showing the hand's anatomy, including the bones of the hand, with the focus on the epiphysis and possibly the bones of the hand's proximal and distal parts such as the clavicles, ribs, clavicles, and parts of the spine, which are essential for various activities like running, throwing, and punching, along with the presence of the epiphyseal plate and the development of the trapezius, which are crucial for a 15-year-old boy's hand, suggesting a focus on the skeletal structure and function of the hand, including the development of the hand's bones such as the clavicles and the development of the wrist bones, which are crucial for various activities like walking, running, and running, as well as the development of the wrist bones of the hand, which are essential for a growing hand and are indicative of a developing hand and are crucial for a growing hand and are typically found in activities such as playing with play and sports, suggesting a comprehensive evaluation of the hand's anatomy and function.\",\"source\":\"quilt_1m\"}\n{\"image_path\":\"1301506707483439105_1.jpg\",\"id\":\"7b0161c9-1451-4f27-9278-aff87726bacb\",\"caption\":\"The image is a lateral chest X-ray image of a 15-year-old boy's hand, showcasing the epiphysis of the wrist bones, which are the bones of the hand, including the radius of the wrist bones, clavicles and the bones of the hand's growth plate are visible, such as the clavicles, ribs, and vertebrae are the primary focus of the image is a close-up view of a lateral chest X-ray image, showcasing the epiphysis of the hand's anatomy, which are essential for various stages of development and growth patterns. The image is a detailed X-ray of the hand's anatomy, displaying the epiphyseal growth plate and development of the wrist bones of a 15-year-old boy's hand, with the bones of the epiphyseal plate visible in the image are the primary organ of interest, which includes the epiphyseal plate and epiphyseal plate, as well as the epiphyseal plate's development is crucial for diagnosing the stage of developmental stage of the wrist bones, typically found in the wrist bones of a 15-year-old boy's hand, which is crucial for understanding the zone of developmental development, and the image is likely to be a lateral chest X-ray image showing the epiphyseal plate's development is crucial for diagnosing the stage of developmental stage of the wrist, with the epiphyseal plate being the primary focus of the image.\",\"source\":\"quilt_1m\"}.\n2.\tThe experiments lack comprehensiveness. The comparison of multimodal large models is limited to VQA-RAD, SLAKE, and PathVQA. A broader range of specialized benchmarks, such as the health subset of MMMU, could provide a more robust comparison of the multimodal large models‚Äô performance.\n\n### Questions\n\n1.\tThe data construction pipeline consists of two main steps: first, generating 200,000 multigranular textual descriptions via GPT-4V, and then fine-tuning LLaVA with this dataset. In my opinion, the performance of the trained model LLaVA-MedCap is significantly influenced by the quality of the data generated by GPT-4V. However, in Table 3, GPT-4V's performance falls far short of LLaVA-Tri. Why did you choose to use GPT-4V to generate the supervised fine-tuning data for the fine-tuning process? In my recent review of the publicly released MedTrinity-25M data, I found that many of the generated data instances were of average quality, with even simple modalities being incorrect.\n2.\tIn Section 4.1, it is stated that ‚ÄúThe model is fine-tuned for three epochs on each VQA dataset and evaluated accordingly.‚Äù However, this evaluation setup is not fair when comparing with the methods presented in the table. Some results in Table 3 appear to be directly extracted from llava-med‚Äôs table, but these methods were not fine-tuned on the training set of the VQA benchmark. In contrast, the accuracy of the chosen llava-med method was achieved after fine-tuning for 15 epochs on the corresponding VQA benchmark training set, making this comparison inappropriate.\nCurrently, the evaluation paradigm for multimodal large models (MLLM) typically involves assessing the model directly on multiple benchmarks after two stages of pretraining and supervised fine-tuning. Therefore, I suggested distinguishing the results presented: there should be a comparison of the results not fine-tuned on the corresponding training set against the models that were not fine-tuned. Additionally, when comparing with methods fine-tuned on the corresponding training set, the number of fine-tuning epochs (including for the comparison methods) should be clearly indicated. If possible, please also conduct an ablation study regarding the number of fine-tuning epochs.\n3.\tIn Table 3, comparing CLIP-like models with LLaVA, a multimodal large model, seems inappropriate. It would be better to categorize them into two groups: one for CLIP models and another for MLLM.\n4.\tThe evaluation details in Table 3&4 are not clearly explained. What do \"open\" and \"close\" specifically refer to?  Please add a brief explanation of the \"open\" and \"close\" terms in the table caption or in the text describing the results.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n- Yes, Other reasons (please specify below)\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents MedTrinity-25M, a large-scale multimodal dataset for medicine, comprising over 25 million annotated images across 10 modalities, associated with more than 65 diseases. This dataset aims to enhance multimodal tasks such as captioning, classification, and segmentation while supporting the development of advanced multimodal medical AI models. Overall, the clarity and quality of the manuscript are moderate.\n\n### Major Comments\n1. **Novelty**: The contribution of MedTrinity-25M as the largest multimodal medical dataset is notable; however, concerns arise regarding the quality of the generated image-text data. Misidentifications of modalities are prevalent, e.g., CT images incorrectly labeled as MRI or X-ray, potentially impacting data utility.\n2. **Evaluation Design**: The experiments lack sufficient comprehensiveness. The comparison is limited to three benchmarks (VQA-RAD, SLAKE, and PathVQA). A wider range of specialized evaluations, such as the health subset of MMMU, could offer a more robust performance assessment.\n3. **Reproducibility**: The description of the data construction pipeline and the influence of GPT-4V raises questions. There are discrepancies in the performance and quality of generated data indicated in Table 3, necessitating a clearer rationale for the choice of GPT-4V.\n4. **Evaluation Methodology**: The evaluation set-up appears unfair when comparing with others in Table 3, as fine-tuning duration varies. A clearer distinction between results from models fine-tuned on training sets versus those not fine-tuned is essential for transparency.\n\n### Minor Comments\n1. Add clarification on the terms \"open\" and \"close\" in the evaluations presented in Tables 3 and 4.\n2. There are multiple instances of modality misidentification in data derived from the quilt-1m dataset that need addressing for accuracy.\n\n### Summary Paragraph\nThe MedTrinity-25M dataset signifies a substantial contribution to the field of medical AI through its comprehensive annotations and sizable imagery. However, there are notable weaknesses regarding data quality and evaluation comprehensiveness. The manuscript should address these weaknesses by improving data accuracy and validating model performance against a broader set of benchmarks. In terms of significance and innovation, the dataset holds promise, but the evaluation and reproducibility aspects require further attention to meet the necessary standards.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical dataset encompassing more than 25 million image‚ÄëROI‚Äëdescription triplets across ten imaging modalities and annotated for over 65 diseases. It aims to provide comprehensive resources for multimodal tasks such as captioning, classification, and segmentation, facilitating the pretraining of advanced AI models and achieving strong performance on standard benchmarks. Overall, the work is ambitious and of potential value to the medical AI community, though aspects of data quality, evaluation fairness, and experimental comprehensiveness warrant further improvement.  \n\n**Major Comments**  \n1. **Data Quality and Modality Misidentification:** A primary concern involves the accuracy of the automatically generated image‚Äëtext annotations. Numerous instances of modality misclassification were observed‚Äîfor example, CT images labeled as MRI, X‚Äëray, or endoscopy, and pathology images wrongly identified as radiographic studies. These errors suggest systemic issues in the caption generation process and question the reliability of the dataset for downstream modeling.  \n2. **Limited Benchmark Coverage:** The evaluation covers only a few VQA datasets (VQA‚ÄëRAD, SLAKE, and PathVQA). Broader benchmarking, such as inclusion of the MMMU health subset, would yield a more comprehensive assessment of model generalization.  \n3. **Choice of Supervision Source:** The dataset construction pipeline relies on GPT‚Äë4V to produce 200,000 textual descriptions later used to fine‚Äëtune LLaVA‚ÄëMedCap. Given that GPT‚Äë4V performs relatively poorly in Table‚ÄØ3 and that generated captions contain basic modality errors, justification for using GPT‚Äë4V as the data source is needed.  \n4. **Unfair Evaluation Protocols:** Comparison fairness in Table‚ÄØ3 is problematic because competing methods were not equally fine‚Äëtuned. The authors fine‚Äëtuned LLaVA for three epochs, whereas other results were obtained with fifteen. Clearer differentiation between pre‚Äëtrained‚Äëonly and fine‚Äëtuned results, and inclusion of ablation on fine‚Äëtuning epochs, are recommended.  \n5. **Category Inconsistency:** CLIP‚Äëlike models and multimodal large language models (MLLMs) are compared directly in Tables‚ÄØ3‚Äì4. These categories should be evaluated separately for clearer interpretation.  \n6. **Ambiguity in ‚ÄúOpen‚Äù vs ‚ÄúClose‚Äù Evaluation:** The meaning of these terms in Tables‚ÄØ3‚Äì4 is not defined and should be clarified in captions or the text.  \n\n**Minor Comments**  \n- Define all acronyms upon first use and maintain consistent table formatting.  \n- Ensure that tables explicitly describe evaluation settings, including the ‚Äúopen/close‚Äù terminology.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe dataset has high potential significance as one of the largest multimodal medical resources and is innovative in scalability and automated annotation. However, questions about label fidelity and modality correctness reduce confidence in reproducibility and reliability. The evaluation demonstrates promising results but is incomplete and lacks fairness in comparison. Addressing data validation, expanding benchmarks, and standardizing evaluation protocols would notably strengthen significance and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The dataset contribution is valuable, but substantial revisions are needed to ensure data quality, fair evaluation, and accurate reporting before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million images across 10 imaging modalities and covering more than 65 diseases. The dataset features multigranular annotations that include both global information and local information. The authors develop an automated pipeline that generates image-ROI-description triplets without the need for paired textual descriptions. This pipeline utilizes domain-specific expert models to identify ROIs and prompts multimodal large language models (MLLMs) to produce detailed annotations through retrieval-augmented generation (RAG).\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- Dataset Comprehensiveness: The dataset is exceptionally extensive, encompassing a wide array of imaging modalities, diseases, and anatomical structures. This diversity significantly enhances its comprehensiveness and utility for various medical AI applications.\n- Construction Pipeline through Advanced Models: The utilization of domain-specific expert models and multimodal large language models (MLLMs) for annotation substantially enriches the dataset. This approach adds multigranular details that improve the quality and depth of the annotations, making the dataset more valuable for training sophisticated models.\n\n### Weaknesses\n\n- Limited Applicability for Visual Data without ROIs: The proposed pipeline relies heavily on regions of interest (ROIs) for constructing multimodal data pairs. For visual data that lack explicit ROI annotations, it is unclear how the pipeline can be effectively applied. This limitation may restrict the dataset's usability across the full spectrum of medical images, particularly those where ROI determination is challenging or subjective.\n- Impact of Multigranular Information Not Fully Explored: While Tables 3 and 4 validate the effectiveness of the benchmark in general terms, the paper does not specifically assess how the inclusion of multigranular information, such as ROIs, influences the performance of medical MLLMs. Given that ROI is a pivotal component of the dataset, a focused evaluation comparing models trained with and without multigranular information would strengthen the claims about its benefits.\n- Insufficient Discussion of Related Benchmarks: The paper lacks a thorough discussion of existing med-MLLM benchmarks that involve multigranular information. Notably, works like \"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI\" and \"A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models\" are not cited or compared against. Including a comparative analysis would provide context and clarify how MedTrinity-25M advances the field relative to existing resources.\n- Lack of Evaluation on Multigranular-Specific Tasks: The dataset is primarily used for training and is evaluated only on VQA-RAD, SLAKE, and PathVQA. These evaluations may not fully capture the effectiveness of the dataset's multigranular annotations since they do not specifically target tasks designed to leverage such detailed information. This omission raises questions about the practical benefits of the multigranular data provided.\n- Challenges with Unique Medical Image Descriptions in RAG: The Retrieval-Augmented Generation (RAG) technique used for annotation relies on a general medical knowledge base. However, medical images often have unique features and presentations, even within the same disease category. For example, Atelectasis can manifest differently in chest X-rays depending on the affected lobe. The paper does not address how the RAG system accounts for these unique variations when the initial visual datasets typically offer only generic classification labels. This could impact the accuracy and specificity of the generated annotations.\n\n### Questions\n\n- Handling of Visual Data Without ROIs: For medical images that lack explicit ROI annotations, how does your pipeline construct the corresponding multimodal data pairs? Is there a mechanism to generate or infer ROIs in such cases, or is the pipeline limited to images where ROIs are predefined?\n- Assessing the Impact of Multigranular Information on Med-MLLMs: Considering that ROIs and multigranular annotations are central to your dataset, have you conducted experiments to evaluate how these features specifically affect the performance of medical MLLMs? Can you provide insights or results that demonstrate the advantages of including multigranular information in model training?\n- Comparison with Existing Multigranular Benchmarks: Could you elaborate on how MedTrinity-25M compares with existing benchmarks that involve multigranular information? What distinguishes your dataset from these, and how does it contribute uniquely to the advancement of general medical AI?\n- Evaluation on Multigranular-Specific Tasks: Given that the dataset is evaluated only on VQA-RAD, SLAKE, and PathVQA, which may not fully utilize multigranular annotations, do you have plans to test your dataset on tasks specifically designed for multigranular information? How can you demonstrate the effectiveness of your dataset's detailed annotations in improving model performance on such tasks?\n- Addressing Unique Medical Image Presentations in RAG: Medical images often present unique and variable features even when categorized under the same disease label. How does your RAG approach handle the specificity and variability of individual medical image descriptions? For instance, with conditions like Atelectasis that can manifest differently in imaging, how does the system ensure that the generated annotations accurately reflect these variations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents MedTrinity-25M, a comprehensive multimodal medical dataset containing over 25 million images across 10 imaging modalities and encompassing more than 65 diseases. It features multigranular annotations that provide both global and local information. The authors introduce an innovative automated pipeline that generates image-region-of-interest (ROI)-description triplets without requiring paired textual descriptions, leveraging domain-specific expert models and multimodal large language models (MLLMs) in a retrieval-augmented generation (RAG) framework.\n\n**Major Comments:**\n1. **Limited Applicability for Visual Data without ROIs:** The pipeline's reliance on ROIs raises questions about its applicability to images without explicit annotations. This limitation could hinder the dataset's effectiveness for a broader range of medical images.\n2. **Impact of Multigranular Information Not Fully Explored:** The paper does not evaluate how multigranular annotations affect MLLM performance. A focused study comparing models trained with and without these details would bolster the claims about their benefits.\n3. **Insufficient Discussion of Related Benchmarks:** The absence of a comparison with existing multigranular med-MLLM benchmarks, such as \"GMAI-MMBench\" or \"A Spectrum Evaluation Benchmark,\" limits the contextual understanding of MedTrinity-25M's advancements.\n4. **Lack of Evaluation on Multigranular-Specific Tasks:** The dataset's evaluation is centered on general tasks (VQA-RAD, SLAKE, PathVQA) that may not fully leverage the detailed annotations provided by the dataset.\n5. **Challenges with Unique Medical Image Descriptions in RAG:** The RAG technique does not address variations in medical image representations adequately, which may compromise the specificity of generated annotations for conditions such as Atelectasis.\n\n**Minor Comments:**\n- The overall clarity of the manuscript could be enhanced, as noted by the soundness rating. \n- Consider adding definitions for technical acronyms early in the document for improved accessibility.\n- Review figures for clarity and ensure that all relevant details are adequately labeled.\n- Minor typographical errors were observed and should be corrected.\n\n**Summary Paragraph:**  \nOverall, MedTrinity-25M demonstrates significant potential in terms of dataset comprehensiveness and the innovative construction of annotation pipelines. However, its significance is somewhat tempered by limitations related to the applicability of the proposed pipeline, insufficient exploration of critical performance factors, and a lack of discussion concerning existing benchmarks. While the dataset's intended contributions are clear, its reproducibility and evaluation in tasks specifically suited for multigranular data need further development.\n\n**Decision Recommendation:**  \nMajor revision is recommended to address the outlined weaknesses and enhance the manuscript's contributions to the field.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical dataset containing over 25‚ÄØmillion images across 10 imaging modalities and more than 65 diseases. The dataset is annotated at multiple granularities using an automated pipeline that generates image‚ÄìROI‚Äìdescription triplets without requiring pre‚Äëexisting paired texts. The pipeline integrates domain‚Äëspecific expert models for region‚Äëof‚Äëinterest (ROI) detection and employs multimodal large language models (MLLMs) via a retrieval‚Äëaugmented generation (RAG) framework for detailed textual annotation. Overall, the paper is clearly written and technically sound, though several methodological and comparative aspects require further clarification and experimental validation.  \n\n**Major Comments**  \n1. **Dependence on ROI Annotations:** The proposed pipeline presumes the availability of explicit ROI labels. Its applicability to images lacking such regions remains unclear, potentially limiting dataset generality across modalities or conditions where ROI identification is subjective or unavailable.  \n2. **Lack of Analysis on Multigranular Benefits:** While the benchmark evaluations in Tables‚ÄØ3‚Äì4 demonstrate overall dataset utility, they do not isolate the contribution of multigranular information. Experiments comparing models trained with and without ROI‚Äëlevel annotations would substantiate claims regarding their added value.  \n3. **Insufficient Comparison with Related Benchmarks:** The manuscript omits discussion of similar multigranular medical benchmarks such as *GMAI‚ÄëMMBench* and *Spectrum Evaluation Benchmark*. Including such comparisons would contextualize the novelty and advancement of MedTrinity‚Äë25M.  \n4. **Evaluation Tasks Not Tailored to Multigranular Data:** The dataset is primarily validated through VQA‚ÄëRAD, SLAKE, and PathVQA, which do not explicitly exploit ROI‚Äëbased annotations. Testing on multigranular‚Äëspecific or segmentation‚Äëaware tasks would better demonstrate its strengths.  \n5. **Handling of Case‚ÄëSpecific Variability in RAG:** The RAG procedure relies on a general medical knowledge base without clearly accounting for inter‚Äëcase variability in image manifestations (e.g., heterogeneous presentations of atelectasis). Further explanation of how the system ensures annotation specificity is needed.  \n\n**Minor Comments**  \n- Clarify how ROI inference operates when non‚Äëannotated images are processed.  \n- Discuss distinctions between this dataset and existing multigranular benchmarks more explicitly.  \n- Ensure consistent terminology for ‚Äúmultigranular‚Äù versus ‚Äúmulti‚Äëlevel‚Äù annotations.  \n\n**Summary Paragraph (TMI Editorial Criteria)**  \n*Significance:* MedTrinity‚Äë25M represents a substantial contribution due to its unprecedented scale and multimodal diversity.  \n*Innovation:* The automated ROI‚Äëbased pipeline and integration of expert models with MLLMs are innovative but require clearer validation of their advantages.  \n*Evaluation:* Current experiments support general usefulness but insufficiently address the specific value of multigranular annotation or comparison to peers.  \n*Reproducibility:* The workflow appears well described; however, additional implementation details on ROI generation and RAG customization would enhance transparency.  \n\n**Decision Recommendation**  \n**Major Revision.** The dataset‚Äôs scope is impressive, but stronger empirical evaluation of multigranular benefits, explicit ROI handling, and comparisons with existing benchmarks are required for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper presents MedTrinity-25M, a large-scale multimodal medical dataset with 25 million images across 10 modalities and detailed annotations for over 65 diseases. Enhanced by automated multigranular annotations, it supports tasks like image captioning and report generation. The LLaVA-Tri model, pre-trained on MedTrinity-25M, achieved state-of-the-art results on multiple medical VQA benchmarks.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The dataset uses an automated process with medical knowledge retrieval and domain-specific models, greatly reducing manual labeling needs.\n2. With over 25 million image-ROI-description triplets, the dataset supports classification, segmentation, report generation, and spans multiple medical domains.\n3. The LLaVA-Tri model, pre-trained on MedTrinity-25M, performs exceptionally well across multiple benchmarks, showcasing the dataset‚Äôs potential to enhance medical AI applications.\n\n### Weaknesses\n\n1. Although the labeling process is automated, its reliance on domain-specific models may limit scalability when handling new modalities or emerging diseases.\n2. The accuracy of automated labeling may fall short of expert-labeled datasets, potentially impacting performance in critical medical applications. How can a high standard of automated labeling be ensured?\n3. With data from over 30 sources, potential biases in image quality, demographics, or disease distribution call for a deeper integration analysis.\n4. Performance depends on external medical knowledge bases, risking inconsistency if not updated, and the LLaVA-Tri comparison may be biased due to overlap with MedTrinity-25M's data sources.\n\n### Questions\n\n1. Given the reliance on automated processes and external knowledge sources, how is labeling consistency ensured in the data generation process? Additionally, has it been validated by human experts?\n2. Has a comparison with expert-labeled datasets been considered to further quantify the quality of automated labeling?\n3. How does the dataset address potential biases in source data? For example, is there a mechanism to prevent overrepresentation of certain demographics?\n4. What is the expandability of the labeling process for new, unrepresented modalities or diseases? Does it offer strong scalability?\n5. Although LLaVA-Tri achieved good performance on RAD, PathVQA, and SLAKE, how do the authors ensure that MedTrinity-25M does not contain data from these datasets? If these datasets were included, the model may have seen the questions and answers during training, leading to artificially high accuracy.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents MedTrinity-25M, a comprehensive multimodal medical dataset composed of 25 million images across 10 modalities, with detailed annotations for over 65 diseases. This dataset is enhanced by automated multigranular annotations, facilitating various tasks including image captioning and report generation. Additionally, the LLaVA-Tri model, pre-trained on this dataset, demonstrates state-of-the-art performance on multiple medical visual question answering (VQA) benchmarks.\n\n### Major Comments\n1. The dataset benefits from an automated labeling process utilizing medical knowledge retrieval and domain-specific models, which significantly decreases the need for manual labeling.\n2. With the extensive volume of over 25 million image-ROI-description triplets, the dataset supports classification, segmentation, and report generation across multiple medical domains.\n3. The LLaVA-Tri model, trained on MedTrinity-25M, shows exceptional performance across various benchmarks, highlighting the dataset's potential for advancing medical AI applications.\n4. There are concerns regarding the scalability of the automated labeling process, particularly its dependence on domain-specific models when integrating new modalities or addressing emerging diseases.\n5. The accuracy of automated labeling may not match that of expert-labeled datasets, raising questions about the consistency and rigor of labeling processes and their implications on critical medical applications.\n6. The dataset's compilation from over 30 sources raises potential biases in image quality, demographics, or disease distribution, which warrants a detailed integration analysis.\n7. Performance is contingent upon external medical knowledge bases, leading to risks of inconsistency if these bases are not regularly updated. Furthermore, any overlap with the MedTrinity-25M data sources could lead to biased comparisons in LLaVA-Tri's performance.\n\n### Minor Comments\n- Clarification is needed regarding the validation of the automated labeling process by human experts.\n- A comparison with expert-labeled datasets should be considered to assess the quality of automated labeling more accurately.\n- The dataset should address how it mitigates potential biases from source data, especially concerning demographic representation.\n- Additional information regarding the scalability of the labeling process for new, unrepresented modalities or diseases would be valuable.\n- Authors should explicitly confirm that MedTrinity-25M does not overlap with RAD, PathVQA, and SLAKE datasets to avoid artificially high accuracy metrics.\n\n### Summary Paragraph\nIn summary, the manuscript introduces a significant contribution to the field of medical AI through the establishment of MedTrinity-25M, characterized by its large-scale, multimodal dataset that incorporates automated annotations. While the dataset demonstrates innovation and has the potential for high significance in medical applications, concerns arise regarding evaluation consistency, the reproducibility of results, and the potential biases present in the data. Addressing these weaknesses is crucial for enhancing the reliability of the dataset and its associated model.\n\n### Decision Recommendation\nMajor revision required.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MedTrinity-25M**, a large-scale multimodal medical dataset comprising approximately 25 million images across 10 imaging modalities, annotated for over 65 diseases. Through automated multigranular annotation integrating domain-specific models and medical knowledge retrieval, the dataset supports diverse tasks such as classification, segmentation, image captioning, and report generation. A companion model, **LLaVA-Tri**, pre-trained on MedTrinity-25M, demonstrates promising results on multiple medical visual question answering (VQA) benchmarks. The manuscript is clearly presented and technically sound, though aspects of scalability and labeling quality merit closer consideration.\n\n---\n\n**Major Comments**  \n1. **Scalability and Generalization:** The automated labeling framework relies heavily on domain-specific models and knowledge retrieval. This dependence may hinder scalability to new imaging modalities or emerging diseases. The authors should clarify how the current pipeline can accommodate such future extensions.  \n2. **Labeling Accuracy and Validation:** While automation reduces manual effort, the annotation accuracy may not match that of expert-labeled datasets. Quantitative and qualitative validation against human experts would strengthen confidence in labeling reliability.  \n3. **Bias and Data Integration:** The dataset integrates data from over 30 sources, which may introduce heterogeneity or demographic imbalance. More analysis is needed on potential source biases and their implications for downstream model performance.  \n4. **External Knowledge Dependence:** LLaVA-Tri‚Äôs performance depends on external medical knowledge bases that require regular updates. The manuscript should discuss how such dependencies are managed to ensure consistent model behavior.  \n5. **Benchmark Overlap:** Clarification is requested on whether data overlap exists between MedTrinity-25M and evaluation datasets (e.g., RAD, PathVQA, SLAKE), as such overlap could inflate performance metrics.\n\n---\n\n**Minor Comments**  \n- The manuscript would benefit from explicit mention of any manual checks conducted to validate automated labels.  \n- Ensure clear descriptions of all acronyms upon first use.  \n- Minor editorial adjustments could improve readability and alignment across tables and figures.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in providing a comprehensive multimodal medical dataset that could support a wide range of AI tasks. Its **innovation** lies in large-scale automated annotation integrating medical knowledge retrieval. The **evaluation** demonstrates strong benchmark results but would be more convincing with additional external validation and bias assessment. **Reproducibility** is supported by automation, yet further detail on data sources, annotation workflows, and quality assurance would enhance transparency.\n\n---\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The dataset and model are promising, but the manuscript requires deeper analysis of labeling quality, data bias, scalability, and validation to fully support its claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript proposes a methodological framework for automated, large‚Äêscale generation of multimodal medical datasets. Though the main product is a dataset (MedTrinity‚Äë25M), the work also introduces a structured pipeline‚Äîintegrating expert segmentation models, retrieval‚Äëaugmented generation, and multimodal large language models‚Äîto synthesize ‚Äúimage‚ÄìROI‚Äìdescription‚Äù triplets. This development directly concerns imaging data curation and quantitative annotation methodology, falling within TMI‚Äôs methodological scope.\n\n2. **Novelty & Contribution Level**  \nThe pipeline departs from existing datasets (e.g., MIMIC‚ÄëCXR, PMC‚ÄëOA, Quilt‚Äë1M) by removing the need for expert text pairing. Generative annotation of multigranular information (global + local + regionwise correlations) constitutes a fresh methodological idea. While some components rely on existing tools (e.g., pretrained LLMs, segmentation networks), their orchestration for scalable medical data generation and evaluation offers an original contribution. The empirical section on re‚Äëtraining LLaVA provides proof‚Äëof‚Äëconcept rather than foundational modeling advance, so the novelty centers on dataset construction and validation.\n\n3. **Technical and Experimental Rigor**  \nThe description is detailed: data sources (30+, 25‚ÄØM images), clear pipeline steps (metadata integration, ROI localization, knowledge retrieval), and both expert and LLM evaluations are provided. However, reproducibility would benefit from explicit release of processing code, model prompts, and quality control statistics per modality. The medical knowledge retrieval and privacy safeguards need slightly more elaboration (e.g., IRB considerations for reused clinical data).\n\n4. **Clarity and Presentation**  \nThe paper is mostly clear though dense. Occasional grammar and formatting issues (e.g., duplicated spaces, capitalization) should be corrected. Figures effectively illustrate qualitative comparisons. The flow between Sections‚ÄØ3.2.1‚Äì3.2.2 could be tightened for readability.\n\n5. **Ethical and Reproducibility Compliance**  \nSources appear to derive from publicly available datasets, and data are said to be de‚Äëidentified. The manuscript asserts public release but should include a clear statement on license compliance and absence of protected health information. Code and generation prompts are outlined but not yet openly documented‚Äîimportant for transparency and validation.\n\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThe paper presents *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical image dataset (‚âà25‚ÄØM samples across‚ÄØ10‚ÄØmodalities). It introduces an automated pipeline that combines metadata integration, ROI localization (via ensemble of domain‚Äëspecific models), and retrieval‚Äëaugmented generation with medically tuned LLMs to produce structured ‚Äúimage‚ÄìROI‚Äìdescription‚Äù triplets. Using this dataset, the authors pre‚Äëtrain an enhanced multimodal model (*LLaVA‚ÄëTri*) that demonstrates improved accuracy on standard medical VQA benchmarks.\n\n**2. Strengths**\n- First systematic framework to scale medical multimodal data generation without human text pairing.  \n- Detailed integration of vision and language modules, enabling multigranular annotations (global/local/relational).  \n- Extensive comparative evaluation against existing datasets and validation using both expert and LLM scoring.  \n- Demonstrated utility through downstream performance gains on multiple benchmarks.  \n\n**3. Weaknesses**\n- Limited methodological novelty in model architecture (fine‚Äëtuning existing components).  \n- Ethical and data‚Äëuse clarifications are brief; legal compliance for redistributed data is not fully specified.  \n- Evaluation relies partly on automatic scoring by GPT‚Äë4V, which may introduce bias; more human validation would strengthen claims.  \n- Quantitative quality metrics (e.g., inter‚Äërater consistency, uncertainty) are limited.  \n- Some figures and tables could be condensed or moved to supplementary to improve readability.\n\n**4. Major Comments**\n1. **Reproducibility and Transparency** ‚Äì Please provide all code, prompts, and configuration for the data‚Äëgeneration pipeline to ensure evaluators can verify annotation quality.  \n2. **Quality Control & Validation** ‚Äì Expand human evaluation beyond‚ÄØ200‚ÄØsamples, report inter‚Äëobserver agreement, and discuss potential propagation of LLM hallucination or bias.  \n3. **Ethical/Data Licensing Statement** ‚Äì Explicitly describe data governance (licenses of reused datasets, patient consent status, IRB considerations).  \n4. **Scientific Positioning** ‚Äì Clarify the methodological core: is the novelty primarily the scalable triplet‚Äëgeneration algorithm or the resultant dataset? Explicit theoretical or algorithmic framing would help position it within imaging methodology rather than data release.  \n5. **Clinical Relevance** ‚Äì Discuss how multigranular annotations concretely benefit clinical imaging tasks (e.g., lesion quantification, multimodal fusion).  \n\n**5. Minor Comments**\n- Correct typographical inconsistencies (e.g., ‚ÄúLLaV‚ÄØA‚ÄëTri by pretraining LLaV‚ÄØA‚Äù ‚Üí ‚ÄúLLaVA‚ÄëTri, pretrained on‚Ä¶‚Äù).  \n- Ensure references are consistent in style (some duplicates and capitalization issues).  \n- Improve figure legends for self‚Äëcontainment, particularly Figs.‚ÄØ3‚Äì6.  \n- Include dataset sampling statistics by disease and modality in the main text or supplement for quick reference.  \n\n**6. Overall Recommendation**  \n**Major Revision** ‚Äî The work has high potential impact as an infrastructure and methodology paper but requires stronger demonstration of data quality assurance, clearer ethical governance, and more explicit methodological framing to reach IEEE‚ÄØTMI standards.\n\n**7. Confidence Level:**‚ÄØ4‚ÄØ/‚ÄØ5  \n\n---\n\n**Guidance Summary**  \nTo reach TMI publishability, the authors should:  \n- Strengthen reproducibility and ethics documentation,  \n- Substantiate annotation validity with larger expert review,  \n- Emphasize methodological advances (automation framework, retrieval‚Äìgeneration coupling) rather than purely dataset scale.  \nWith these refinements, the paper could represent a valuable contribution to medical imaging data methodology.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MedTrinity-25M, a large-scale multimodal medical dataset containing over 25 million image-ROI-description triplets across 10 modalities and 65+ diseases. The authors develop an automated pipeline that generates multigranular annotations without requiring paired text descriptions, using metadata integration, ROI localization via expert models, medical knowledge retrieval, and multimodal large language models (MLLMs) for text generation. The resulting dataset provides structured descriptions covering five attributes: modality, organ detection, ROI analysis, lesion texture, and region-wise correlations. To demonstrate effectiveness, they propose LLaVA-Tri, achieving state-of-the-art performance on VQA-RAD (81.6%), SLAKE (87.8%), and PathVQA (82.8%) benchmarks. The dataset aims to support various medical AI tasks including captioning, report generation, classification, and segmentation.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical and Technical Formulation Gaps**\n  - The paper lacks mathematical formalization of the ROI localization process and coordinate system transformations mentioned in Section 3.2.1, making reproducibility difficult\n  - No quantitative metrics are provided for the \"left-right reversal in image representation\" for X-ray and MRI modalities (Section 3.2.1, Figure 6)\n  - The vector similarity search methodology using Med-CPT encoder and Faiss indexing lacks technical specifications such as distance metrics, dimensionality, and retrieval thresholds (Section 3.2.1)\n\n‚Ä¢ **Insufficient Evaluation Methodology**\n  - The quality evaluation relies on only 200 samples from a 25M dataset (Table 2), representing 0.0008% coverage, which may not be statistically representative\n  - Expert evaluation methodology lacks details about evaluator qualifications, inter-rater agreement, or evaluation protocols (Section 3.3)\n  - No comparison with human-generated descriptions or established medical text quality metrics beyond word count (Figure 8d)\n\n‚Ä¢ **Dataset Construction Pipeline Reliability**\n  - Heavy dependence on metadata extraction from diverse sources (30+ datasets) without validation of metadata accuracy or consistency across sources (Section 3.2.1, Table 5)\n  - The automated ROI generation using different expert models for different modalities (Table 6) lacks cross-validation or quality assessment\n  - LLaVA-Medcap training details are relegated to appendix, yet this model generates all 25M descriptions, raising concerns about potential systematic biases\n\n‚Ä¢ **Limited Experimental Validation**\n  - Evaluation restricted to three VQA benchmarks without testing on other medical tasks like classification, segmentation, or report generation despite claims of broad applicability (Abstract, Section 5)\n  - Ablation studies focus only on presence/absence of MedTrinity-25M pretraining without analyzing individual pipeline components' contributions\n  - No analysis of how multigranular annotations specifically contribute to performance improvements compared to simpler annotation schemes\n\n‚Ä¢ **Scalability and Reproducibility Concerns**\n  - The pipeline's computational requirements, processing time, and resource costs for generating 25M descriptions are not reported, limiting practical adoption assessment\n  - No direct evidence found in the manuscript regarding the availability timeline or access procedures for the dataset despite public availability claims\n  - Quality control mechanisms for detecting and correcting errors in automated annotation generation are not described\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance Mathematical and Technical Documentation**\n  - Provide formal mathematical definitions for ROI coordinate transformations and localization algorithms, including pseudocode for the pipeline components\n  - Specify technical parameters for vector similarity search including distance metrics, embedding dimensions, and retrieval confidence thresholds\n  - Include detailed algorithmic descriptions of the metadata integration process with validation procedures\n\n‚Ä¢ **Strengthen Evaluation Framework**\n  - Expand quality evaluation to at least 1000-2000 samples (0.004-0.008% of dataset) with stratified sampling across modalities and disease types\n  - Implement comprehensive inter-rater agreement studies with multiple medical experts and provide detailed evaluator qualification criteria\n  - Compare generated descriptions against human-written references using established medical text quality metrics such as medical concept coverage and clinical accuracy\n\n‚Ä¢ **Improve Pipeline Validation and Transparency**\n  - Implement cross-validation procedures for metadata accuracy across the 30+ source datasets with systematic quality checks\n  - Provide comprehensive quality assessment of ROI generation across different expert models including precision/recall metrics\n  - Move LLaVA-Medcap training details to main paper and include systematic bias analysis of generated descriptions\n\n‚Ä¢ **Expand Experimental Scope**\n  - Evaluate MedTrinity-25M on medical classification, segmentation, and report generation tasks beyond VQA to validate broad applicability claims\n  - Conduct detailed ablation studies analyzing individual pipeline components (metadata integration, ROI localization, knowledge retrieval) contributions\n  - Compare multigranular annotations against simpler annotation schemes to demonstrate specific value of the proposed approach\n\n‚Ä¢ **Address Practical Implementation Issues**\n  - Report computational requirements, processing times, and estimated costs for dataset generation to enable practical adoption assessment\n  - Provide concrete dataset access procedures, file formats, and expected availability timeline with reproducibility guidelines\n  - Implement and describe quality control mechanisms for error detection and correction in the automated annotation pipeline",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million image-ROI-description triplets across ten imaging modalities and more than 65 diseases. The dataset is notable for its automated pipeline, which generates multigranular annotations without requiring paired text descriptions. The authors argue that MedTrinity-25M can significantly advance multimodal medical AI models, as evidenced by the superior performance of LLaV A-Tri on three external VQA benchmarks. The paper is well-written, and the motivation for automating the generation of rich annotations is compelling.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: The automation of multigranular annotation generation is a significant contribution. However, the manuscript could better situate its contributions relative to existing automated annotation pipelines and multimodal datasets. The authors should clarify how their approach differs from and improves upon recent advancements in automated medical image annotation.\n\n2. Evaluation Design: The evaluation is primarily based on the performance of LLaV A-Tri on three VQA benchmarks. While these benchmarks are relevant, the manuscript could benefit from additional validation on other types of medical tasks, such as segmentation or classification, to better demonstrate the versatility of MedTrinity-25M. Additionally, the inclusion of a comparison with other datasets in terms of performance and annotation richness would strengthen the argument for the dataset's superiority.\n\n3. Comparisons: The manuscript should include comparisons with recent datasets and models that have achieved state-of-the-art performance in similar tasks. The omission of relevant baselines, such as those from recent papers on multimodal medical AI, undermines the robustness of the experimental section. Including these comparisons would provide a clearer picture of how MedTrinity-25M stands out.\n\n4. Reproducibility: While the authors state that the dataset is publicly available, the manuscript lacks detailed instructions on how to reproduce the pipeline for generating annotations. Providing a clear, step-by-step guide for replicating the dataset creation process would enhance the reproducibility and utility of MedTrinity-25M.\n\n###\n\n## Minor Comments\n1. Figure 1: The comparison between MedTrinity-25M and other datasets could be more visually intuitive. Consider adding a table or a diagram that highlights the differences in annotation types and richness.\n2. Section 3.1: The description of the data triplet could benefit from a clearer breakdown of the components (image, ROI, and description) and their roles.\n3. Appendix B: The quantitative comparison between GPT-4V and LLaV A-Medcap could be supplemented with more qualitative examples to illustrate the differences in the generated descriptions.\n4. Typographical Issues: There are a few typographical errors throughout the manuscript, such as \"interleave\" instead of \"interleaved\" and inconsistent spacing in tables.\n\n###\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical imaging by introducing MedTrinity-25M, a large-scale multimodal dataset with rich annotations. The dataset's automated pipeline for generating multigranular annotations represents a meaningful innovation. However, the evaluation is somewhat limited, focusing primarily on VQA benchmarks, and could benefit from additional validation on other tasks. The manuscript also needs to better contextualize its contributions relative to existing work. The reproducibility of the dataset creation process is another area that requires more detailed documentation. Overall, while the idea is promising, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis, strengthen validation across different medical tasks, and provide detailed methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Ce Zhou",
      "James Zou",
      "Juncheng Wu",
      "Lang Gao",
      "Lei Xing",
      "Sheng Liu",
      "Xianhang Li",
      "Yunfei Xie",
      "Yuyin Zhou",
      "Cihang Xie",
      "Hong-Yu Zhou"
    ],
    "url": "pdfs/iclr.cc-2025-conference_d35cf31b42d0db969be38ced93f86fe655f141fa.pdf",
    "remote_url": "https://openreview.net/pdf/d35cf31b42d0db969be38ced93f86fe655f141fa.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Human-aligned models",
      "robust neural networks",
      "visual perception",
      "perceptual learning",
      "medical machine learning"
    ],
    "abstract": "The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents L-WISE, a framework leveraging adversarially robust ANNs to estimate image difficulty and apply nuanced perturbations that facilitate human learning in visual categorization tasks. By selecting challenging images and amplifying category-specific features, L-WISE improves human categorization accuracy by 33-72% and reduces training time by 20-23% across both general and clinical domains (e.g., dermoscopy and histology). The authors also discuss ethical implications, emphasizing the benefits of enhanced medical training and cautioning against potential biases that may arise from reliance on model-derived guidance.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The application of Robustified ANNs for improving human visual performance on image categorization seems like an interesting avenue. \n2. L-Wise empirically demonstrate gains in categorization accuracy and training efficiency. \n3. The paper addresses ethics concerns. Since the work mentions use of clinical data ethics discussion is of critical importance.\n\n### Weaknesses\n\n1. The paper focused on the performance of ventral stream. But we know that the human visual stream has a dorsal stream (where) that locates an object and the ventral stream (what) stream. And the interplay of these two streams forms the basis of human visual system. In this work the authors mainly focused on the ventral stream. From only quantified data, we can see the gains but it is very hard to trace this back to the nuanced perturbations the ANN produces. Hence, the suggestion is to use human gaze. The human gaze will precisely pin-point the \"where\" aspect and then will truly help us understand if at all the model perturbations are helping improve human performance. Can the authors please explain this? \n2. A robust DNN actually has worse performance on nominal data points. Data points that have not been corrupted adversarially. What was the motivation of the authors to select such a model for their experiments? \n3. The perturbations -  The perturbations if I am not mistaken are very subtle ones. For fine grained classifications, humans do follow curriculum learning but learning structures gradually from simpler to harder concepts. No experiments have shown this. It would be great if the authors can provide some empirical results/ explanation that can explain how will their method occur when you focus on structural cues rather than model perturbations that will benefit fine grained categorization. \n4. I am providing some citations related to Gaze and dual stream hypothesis that can help authors clarify my concerns \n\na. A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains, NeurIPS 2023.\nb. Literature related to papers accepted in NeurIPS Gaze Meets ML workshop. That workshop accepted papers will provide intuition of how human gaze can be used in coherence with DL models.\n\n### Questions\n\n1. For model perturbations, can the authors please provide heat maps or any qualitative results that will help us track the ANN perturbations to human visual learning? \n2. Is there a way to show if the study scales/generalizes to other ANNs as well other than Adversarially trained ones? Since human subjects have been used here, I am not sure how feasible experiments will be. This can be a general neural network or a network trained by the CutMix [a] loss that provides robustness benefits as well. \n3. I feel generating perturbations based of dual stream networks and then using human gaze to track these will be a much stronger claim to the work. Can the authors please address this question? \n4. What about adversarially trained transformers? The attention maps are different from CNN feature maps. How will the study be applicable for perturbations based of a transformer backbone? \n5. Please also address concerns raised in the weakness sections. \n\na. CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019,\n\n### Flag For Ethics Review\n\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 6\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript introduces L-WISE, a framework that utilizes adversarially robust artificial neural networks (ANNs) to evaluate image difficulty and implement targeted perturbations, thereby enhancing human performance in visual categorization tasks. The authors report significant improvements in categorization accuracy (by 33-72%) and reductions in training time (by 20-23%) across general and clinical applications, including dermoscopy and histology. Additionally, the manuscript addresses pertinent ethical considerations linked to advancing medical training while acknowledging possible biases related to model-derived guidance.\n\n### Major Comments\n1. The study primarily investigates the performance of the ventral visual stream, neglecting the dorsal stream's role in object localization. Understanding the interplay between these streams is essential. The authors should incorporate human gaze metrics to elucidate the effects of ANN perturbations on human performance.\n2. The authors' choice of a robust deep neural network (DNN) warrants clarification, as such models may perform poorly on unperturbed nominal data. What was the rationale for selecting this model for experiments?\n3. The subtlety of the applied perturbations is intriguing, yet the paper lacks empirical evidence demonstrating how these perturbations align with the principles of curriculum learning, which advocates for gradually progressing from simpler to more complex concepts. Additional empirical results supporting the methodology would strengthen the argument.\n4. Relevant citations concerning gaze and the dual stream hypothesis are suggested to clarify concerns:\n   - \"A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains,\" NeurIPS 2023.\n   - Works from the NeurIPS Gaze Meets ML workshop that provide insights into integrating human gaze with deep learning models.\n\n### Minor Comments\n- Consider providing qualitative results, such as heat maps, to illustrate how ANN perturbations correlate with human visual learning.\n- Discuss the generalizability of the findings to other neural network architectures beyond adversarially trained ones. Given the involvement of human subjects, the feasibility of such experiments may be limited.\n- It may be beneficial to explore perturbations based on dual stream networks and use human gaze tracking to support the framework's claims.\n- Address how adversarially trained transformers could yield different attention maps compared to CNNs and the implications for perturbation applications.\n\n### Summary Paragraph\nThe study's significance lies in its innovative approach to enhancing human visual categorization through adversarially robust ANNs. However, the reliance on the ventral stream and the lack of empirical support for the perturbation method raise concerns regarding reproducibility and the broader applicability of findings. Ethical implications are appropriately acknowledged, emphasizing the need for responsible research practices in human subject studies.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **L-WISE**, a framework that employs adversarially robust artificial neural networks (ANNs) to estimate image difficulty and apply targeted perturbations intended to enhance human learning in visual categorization tasks. By emphasizing challenging images and amplifying category-specific features, the method reportedly improves human categorization accuracy (33‚Äì72%) and reduces training time (20‚Äì23%) across general and clinical domains such as dermoscopy and histology. The authors also include a thoughtful discussion on ethical considerations in clinical training contexts. Overall, the paper tackles an interesting problem, though several methodological and conceptual aspects require clarification.\n\n---\n\n**Major Comments**  \n1. **Biological plausibility and dual-stream modeling:** The study focuses on the ventral (object-identification) pathway of human vision, while neglecting the dorsal (spatial-localization) stream. Since the interplay between these streams underlies human visual processing, the exclusive focus on the ventral stream limits interpretability. The reviewer suggests incorporating or discussing human gaze data to connect model perturbations with spatial attention, thereby better explaining how perturbations enhance human performance.  \n2. **Choice of adversarially trained models:** Robust DNNs are known to underperform on nominal (non-adversarial) data. The rationale for selecting such models for the presented experiments should be explicitly justified.  \n3. **Perturbation design and curriculum learning:** The perturbations appear subtle. In fine-grained categorization, humans often benefit from structured or curriculum-based learning that progresses from simple to complex concepts. It would strengthen the study to include experiments or explanations exploring how L-WISE compares or aligns with such structural learning approaches.  \n4. **Generalization and alternative architectures:** Clarification is needed on whether the method generalizes beyond adversarially trained ANNs, for example to standard networks or those trained with alternative robustness methods (e.g., CutMix). Additionally, implications for transformer-based architectures with attention maps distinct from CNN feature maps should be addressed.  \n5. **Qualitative visualization:** Please provide qualitative evidence, such as heat maps, illustrating how perturbations relate to human learning effects.  \n6. **Ethics and gaze-related literature:** The reviewer recommends consulting dual-stream and gaze-related literature (e.g., NeurIPS 2023 ‚ÄúDual-Stream Neural Network‚Äù and the ‚ÄúGaze Meets ML‚Äù workshop) for theoretical and empirical grounding.\n\n---\n\n**Minor Comments**  \n- Minor editorial and clarity improvements are suggested to improve presentation, such as brief explanations of technical terms and consistent use of acronyms.  \n- Ethical discussion is appreciated and appropriately placed; ensure data-handling descriptions meet responsible research standards.\n\n---\n\n**Summary (TMI Criteria)**  \nThe paper addresses a **significant** question‚Äîimproving human learning through model-guided visual perturbations‚Äîbut its **innovation** is somewhat constrained by limited theoretical grounding in the full visual processing model. The **evaluation** demonstrates quantitative benefits but lacks qualitative linkage to cognitive mechanisms like gaze or spatial attention. **Reproducibility** is moderately supported but would benefit from added experimental detail and visualizations. Strengths include empirical gains and ethical awareness; weaknesses concern the neurobiological interpretation and model selection rationale.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a method to enhance human visual learning by designing a model-based selection and enhancement algorithm to improve classification accuracy during testing. First, the authors select images to present to novice learners based on a model‚Äôs estimated recognition difficulty for each image. Next, they apply image perturbations intended to aid recognition for novice learners. The authors conduct experiment on the three benchmark datasets, including the natural image and the clinical image to verify the effectiveness of their proposal.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper is well-motivated, and a decent amount of technical details are given.\n2. The idea of improving the categorization performance of the novice learner by leveraging the capacity of the robustified artificial neural network is both interesting and practical.\n3. The reported improvement in novice learners' performance is notable, with gains in both test accuracy and reduced training time.\n\n### Weaknesses\n\n1. The establishment of the empirical observations is somewhat unconvincing. Do these observations hold in more complex classification tasks or when applied to medical imaging?\n2. The related work section lacks discussion of both the machine teaching and human-machine vision alignment methods.\n3. The size of the particants is somewhat small. \n4. The perception of enhanced images may be altered due to perturbations.\n\n### Questions\n\n1. The empirical observations are derived from a 16-way animal categorization task on natural images, which seems somewhat simplistic. It would be valuable to examine how these observations hold up in more complex categorization tasks or with different types of images, especially medical images. Given the typically limited availability of medical images, the proposed method could have promising applications in the medical imaging field.\n\n2. Beyond the empirical observations, is there any physiological insight or analysis on why the proposed model-based selection and enhancement method could improve novice learners‚Äô performance in categorization tasks?\n\n3. The authors do not discuss the related machine teaching literature. An in-depth comparison with machine teaching methods, particularly with \"Teaching Categories to Human Learners with Visual Explanations\" (CVPR 2018), would be valuable. This work similarly considers image difficulty; an introduction and comparison with it are beneficial.\n\n4. The authors should also discuss the connection to human-machine vision alignment methods, such as \"Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans\" (NeurIPS 2022).\n\n5. The sample size of participants is relatively small, and expanding the participant pool is recommended to enhance the reliability of the results; also, recruiting participants from diverse backgrounds would improve the generalizability of the findings. If expanding the participant pool is impractical due to time or budget constraints, performing a power analysis or discussing effect sizes could help strengthen the reliability of the analysis.\n\n6. When using perturbations to enhance images, how does the method ensure that essential image details remain unchanged, particularly when using a large œµ (e.g., 20)? This concern is especially pertinent for medical images, where even slight pixel changes may alter critical information. I recommend involving medical experts to review the enhanced images or using quantitative similarity measures (such as SSIM or FSIM) to verify that essential details are preserved.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a novel approach to enhancing human visual learning through a model-based selection and enhancement algorithm aimed at improving classification accuracy for novice learners. The authors detail the selection of images based on estimated recognition difficulty, followed by the application of image perturbations to facilitate recognition. Experiments are conducted on three benchmark datasets, including natural and clinical images, to validate the proposed method's effectiveness.\n\n### Major Comments\n1. The empirical observations reported are somewhat unconvincing. Further examination is needed to determine if these findings hold true in more complex classification tasks or within medical imaging contexts.\n2. The related work section is lacking in discussions surrounding both machine teaching literature and human-machine vision alignment methods, which would enrich the manuscript.\n3. The sample size of participants in the study is relatively small, potentially limiting the generalizability of the findings.\n4. There are concerns that image perturbations could alter the perception of enhanced images.\n\n### Minor Comments\n- The simplistic nature of the 16-way animal categorization task raises questions about the robustness of findings in more complex scenarios.\n- The authors should explore physiological insights that could explain improvements in novice learner performance.\n- An introduction and comparison with machine teaching methodologies, particularly \"Teaching Categories to Human Learners with Visual Explanations\" (CVPR 2018), would strengthen the discussion.\n- Connections to the study \"Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans\" (NeurIPS 2022) should be explored.\n- There should be an emphasis on the need for a larger and more diverse participant pool to enhance the reliability and generalizability of results; if impractical, the authors could conduct a power analysis or discuss effect sizes.\n- It is critical to ensure that essential details in images remain unchanged while using perturbations, especially for medical images. Consulting medical experts or utilizing quantitative similarity measures (such as SSIM or FSIM) is recommended.\n\n### Summary Paragraph\nIn conclusion, the manuscript contributes to the field of visual learning by proposing an innovative method that harnesses advanced neural network techniques. The work is significant in its potential application to medical imaging; however, concerns regarding empirical validity, literature engagement, and participant diversity may affect its evaluation and reproducibility. The strengths include technical motivation and notable performance improvements reported, while weaknesses relate to the simplicity of the empirical observations and sample size. Addressing these issues could enhance the overall impact and reliability of the study.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a method to enhance human visual learning through a model-based selection and image enhancement algorithm designed to improve classification accuracy in testing. The approach selects images for novice learners based on a model‚Äôs predicted recognition difficulty and applies targeted perturbations to aid recognition. Experiments on three benchmark datasets, including both natural and clinical images, evaluate the method‚Äôs effectiveness. The paper is well-motivated and clear in its presentation, though some claims require stronger empirical or theoretical support.\n\n---\n\n**Major Comments**  \n1. **Empirical Validity:** The presented empirical findings, primarily derived from a 16-way animal categorization task, appear limited in scope. It is unclear whether they generalize to more complex classification problems or to medical imaging tasks, where data distribution and perceptual challenges differ substantially.  \n2. **Related Work Coverage:** The related work section lacks discussion of relevant research on *machine teaching* (e.g., ‚ÄúTeaching Categories to Human Learners with Visual Explanations,‚Äù CVPR 2018) and *human‚Äìmachine vision alignment* (e.g., ‚ÄúHarmonizing the Object Recognition Strategies of Deep Neural Networks with Humans,‚Äù NeurIPS 2022). These comparisons are essential to contextualize and position the contribution.  \n3. **Participant Sample Size:** The participant pool is relatively small, limiting the reliability and generalizability of the results. Expanding the sample or conducting a power/effect size analysis would help validate the findings.  \n4. **Perturbation Effects:** The enhancement process may unintentionally distort perceptual content, especially when large perturbation magnitudes (e.g., œµ = 20) are used. Verification by domain experts or quantitative similarity metrics (SSIM, FSIM) is recommended.  \n5. **Physiological Interpretation:** The paper would benefit from discussion of possible cognitive or perceptual mechanisms explaining why the proposed method improves novice performance.\n\n---\n\n**Minor Comments**  \n- Some sections could better distinguish between natural image and medical image experiments.  \n- Minor typographical and grammatical corrections are needed.  \n- A clearer description of participant demographics would improve transparency.\n\n---\n\n**Summary (TMI Editorial Criteria)**  \n- **Significance:** The problem of improving human visual categorization is meaningful, particularly for educational and clinical applications.  \n- **Innovation:** Combining model-driven selection with perturbation-based enhancement is novel, though related literature should be more deeply examined.  \n- **Evaluation:** Promising but limited‚Äîbroader datasets, larger participant groups, and stronger statistical assessment are needed.  \n- **Reproducibility:** The methodology is described in reasonable technical detail, but reproducibility could be improved with additional quantitative validation and explicit data processing steps.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe study presents an interesting and potentially impactful idea but requires broader experimental validation, deeper comparison with prior work, and clearer justification of empirical findings.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes a novel approach to augment human learning in image categorization tasks. By leveraging robustified ANNs, the study introduces model-guided image selection and enhancement strategies that increase human test-time categorization accuracy by up to 72% and reduce training duration by around 20-23%. L-WISE employs selecting images based on predicted difficulty levels and enhancing images with pixel perturbations.The proposed approach is tested on natural images, dermoscopy, and histology images. The results demonstrates efficacy of L-WISE in aiding novice learners in fine-grained categorization tasks. This research represents one of the first applications of ANNs in optimizing human visual learning in clinically relevant domains.\n\n### Soundness: 1\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- Presents an innovative use of robustified ANNs to predict task difficulty and enhance images, aiding human perception and learning.\n- Shows broad applicability by successfully testing across diverse domains, such as natural image classification, dermoscopy, and histology.\n- Achieves practical efficiency by reducing training time and improving test-time accuracy, beneficial for fields requiring rapid, accurate human image categorization training.\n\n### Weaknesses\n\n- Lacks a dedicated related work section, which would help contextualize the research.\n- Both low and high logits from ANNs show significant variation in human accuracy, making predictions less reliable in certain logit intervals.\n- Uses only the ResNet-50 architecture, limiting generalization; further testing with models like vision transformers (ViT) is needed to support broader conclusions.\n- Image enhancement may introduce biases, potentially improving accuracy only for certain major classes; additional metrics like precision and recall per class, rather than just mean accuracy, should be reported to provide a clearer assessment.\n\n### Questions\n\n1. How to choose $\\epsilon$ for different tasks and image domains?\n2. What criteria determine if a model is \"robustified\" enough for use? Have you considered specific metrics to evaluate the robustness of guide models, and how do these metrics correlate with human learning outcomes?\n3. Did you collect qualitative feedbacks from participants? Did the new curriculum and enhanced images increase mental stress of human learners? Additional learning costs beyond training time should be considered, such as cognitive load and emotional well-being.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe use of ANN-guided models in human learning may introduce unintended biases, potentially affecting participants' learning outcomes in ways that favor certain demographic groups over others. For instance, if the training of ANNs inadvertently enhances accuracy for only a subset of the population (such as specific genders, ages, or races), it could lead to biased learning outcomes. Such biases could potentially skew related job opportunities, ultimately reinforcing inequities. Ensuring that ANNs benefit all demographic groups equitably requires further investigation and ongoing evaluation to mitigate these risks.\n\nThe above concern is addressed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript presents a novel approach aimed at augmenting human learning in image categorization tasks through the use of robustified artificial neural networks (ANNs). The study introduces model-guided image selection and enhancement strategies that significantly improve the accuracy of human categorization by up to 72% while decreasing training time by approximately 20-23%. This research explores the effectiveness of the proposed L-WISE methodology across various domains, including natural images, dermoscopy, and histology, marking an important contribution to the application of ANNs in enhancing human visual learning in clinically relevant areas.\n\n**Major Comments:**\n1. The manuscript lacks a dedicated related work section, which would provide necessary context for the research.\n2. Significant variability in human accuracy for both low and high logits from the ANNs raises concerns about the reliability of predictions within certain logit ranges.\n3. The use of only the ResNet-50 architecture limits generalizability; further testing with alternative architectures, such as vision transformers (ViT), is recommended for broader applicability.\n4. Image enhancement techniques may introduce biases that could skew accuracy improvements according to specific major classes; reporting additional metrics like precision and recall per class is suggested for a more comprehensive evaluation.\n\n**Minor Comments:**\n- Consider providing clarity on how to select the parameter $\\epsilon$ for different tasks and image domains.\n- Define criteria for determining the robustness of models and suggest metrics that could evaluate this while correlating with human learning outcomes.\n- Inquiry into whether qualitative feedback from participants was gathered regarding the impact of the enhanced images and curriculum on cognitive load and emotional well-being.\n\n**Summary Paragraph:**\nIn terms of significance, the study introduces innovative methodologies that potentially advance human learning processes in image categorization. However, its generalizability is limited by the architecture choice and lack of related work discussion. The evaluation of the approach demonstrates notable efficiency in training and accuracy; yet, concerns about biases in image enhancement and the need for comprehensive assessment metrics highlight areas requiring improvement. Reproducibility aspects could benefit from broader testing and transparent methodological choices.\n\n**Decision Recommendation:**\nMajor revision is recommended to address the outlined weaknesses and substantiate findings further.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *L-WISE*, a framework that leverages robustified artificial neural networks (ANNs) to enhance human learning in image categorization tasks. The method employs model-guided image selection based on predicted difficulty levels and pixel-level image enhancement to improve novice learners‚Äô accuracy and efficiency. The approach is evaluated across natural image, dermoscopy, and histology datasets, showing improvements of up to 72% in test-time accuracy and a 20‚Äì23% reduction in training time. Overall, the study is clearly motivated and presents one of the first attempts to optimize human visual learning in clinically relevant settings using machine-guided augmentation.\n\n**Major Comments**  \n1. The manuscript lacks a distinct *related work* section. Integrating this would help situate the study within existing literature on human‚Äìmachine collaborative learning and visual training augmentation.  \n2. The reliability of ANN-predicted logits varies: both low and high confidence predictions show fluctuations in corresponding human accuracy, which limits the stability of the model‚Äôs difficulty predictions.  \n3. The experiments rely solely on the ResNet-50 architecture. Additional validation using diverse backbones, such as Vision Transformers, would strengthen claims of generality.  \n4. The potential for bias in the image-enhancement process requires attention. Enhancements may improve accuracy predominantly for major classes. Reporting class-wise metrics (e.g., precision and recall) instead of only mean accuracy would allow for a more balanced assessment.  \n5. Further clarification is needed regarding (a) how the perturbation parameter \\(\\epsilon\\) is selected across domains, (b) what robustness metrics define a \"robustified\" model, and (c) whether ANN-guided training affects cognitive load or emotional stress in human participants‚Äîfactors that influence practical deployment.\n\n**Minor Comments**  \n- Improve figure and table captions for clarity.  \n- Define abbreviations (e.g., ANN) at first appearance for consistency.  \n- Minor grammatical issues should be corrected.\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is significant in proposing a machine-assisted framework for human visual learning with potential relevance to clinical training. Its innovation lies in combining robustified ANNs with adaptive image selection and enhancement. The evaluation is promising but limited by the single-network setup and lack of detailed per-class metrics. Reproducibility could be improved by elaborating the selection of \\(\\epsilon\\), the robustness metrics used, and participant feedback protocols. Overall, the study demonstrates meaningful potential but would benefit from methodological and comparative expansion.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚ö†Ô∏è Borderline**  \n   The manuscript, *L‚ÄëWISE: Boosting Human Visual Category Learning through Model‚ÄëBased Image Selection and Enhancement*, explores how adversarially robust neural networks can predict human image recognition difficulty and generate subtle ‚Äúenhancements‚Äù that facilitate visual category learning for humans.  \n   Although the paper includes experiments in dermoscopy and histology, the primary methodological innovation lies in **human learning augmentation** and **human‚ÄëAI perceptual alignment**, rather than in imaging physics, reconstruction, or quantitative medical image analysis per se. These goals fall closer to human‚Äìmachine interaction or cognitive vision than to core TMI topics (e.g., image formation, quantitative biomarkers, or clinical imaging pipelines). The contribution is therefore *marginally relevant* to TMI‚Äôs methodological orientation.\n\n2. **Novelty & Contribution Level ‚Äì Moderate**  \n   The concept of using robustified ANNs to both estimate human recognition difficulty and generate category‚Äëspecific perturbations to aid learning is novel in the context of perceptual augmentation. The combination of difficulty‚Äëweighted image selection and enhancement (‚ÄúL‚ÄëWISE‚Äù) is an original integration. However, within medical imaging, the methodological novelty is indirect: the work shows utility of an existing robust adversarial training framework rather than introducing a new imaging algorithm.  \n\n3. **Technical and Experimental Rigor ‚Äì Generally strong, with caveats**  \n   ‚Ä¢ Clear explanation of model training, datasets (HAM10000, MHIST, moth data), and subject recruitment.  \n   ‚Ä¢ Multiple ablation and cross‚Äëdomain validations reinforce internal consistency.  \n   ‚Ä¢ The human‚Äësubject experiments are well powered for behavioral analysis, not for clinical validation.  \n   ‚Ä¢ No quantitative evaluation against radiologist performance or diagnostic endpoints‚Äîhence limited medical imaging rigor.  \n   ‚Ä¢ Statistical treatment (œá¬≤ tests, bootstrapped CIs, logistic regressions) is appropriate.\n\n4. **Clarity and Presentation ‚Äì Excellent**  \n   Manuscript is well structured, well illustrated, and grammatically polished. Figures are clear and annotated; methods and appendices provide reproducibility details.\n\n5. **Ethical and Reproducibility Compliance ‚Äì Adequate**  \n   IRB approval, informed consent, fair participant compensation, and code availability are indicated. The authors discuss dataset biases (e.g., skin‚Äëtone imbalance in HAM10000) responsibly.\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \n   The authors propose *Logit‚ÄëWeighted Image Selection and Enhancement* (L‚ÄëWISE), a model‚Äëbased method to accelerate and improve human visual category learning. Using adversarially trained (‚Äúrobustified‚Äù) neural networks, they (a) predict each image‚Äôs recognition difficulty for humans from the ground‚Äëtruth logit output, and (b) apply pixel‚Äëbounded perturbations that highlight class‚Äëspecific features. These signals define an adaptive curriculum of progressively harder images and fading enhancement strength. Human participants trained with L‚ÄëWISE attained 33‚Äì72‚ÄØ% higher test‚Äëphase accuracy and ~20‚ÄØ% shorter training duration across natural, dermoscopy, and histology images.\n\n2. **Strengths**  \n   ‚Ä¢ Creative cross‚Äëdisciplinary idea linking adversarial robustness to human perceptual learning.  \n   ‚Ä¢ Extensive behavioral validation with >500 participants.  \n   ‚Ä¢ Strong statistical analysis and reproducibility documentation.  \n   ‚Ä¢ Ethical and bias awareness sections are exemplary.  \n   ‚Ä¢ Potential relevance for educational tools in pathology and dermatology training.\n\n3. **Weaknesses**  \n   ‚Ä¢ The methodological innovation pertains to **human learning augmentation**, not directly to **medical imaging methodology** as defined by TMI.  \n   ‚Ä¢ The medical tasks used are illustrative small‚Äëscale classification problems; no evidence of translation to clinical diagnostic improvement.  \n   ‚Ä¢ Enhancement mechanism is largely heuristic‚Äîno formal modeling of perceptual or imaging physics.  \n   ‚Ä¢ The perturbations may distort image realism; risk of misleading visual cues is insufficiently quantified.  \n   ‚Ä¢ Sparse comparison with existing curriculum‚Äëlearning or active‚Äëlearning frameworks in medical imaging.\n\n4. **Major Comments**  \n   1. **Clarify TMI relevance.** The paper should explicitly discuss how L‚ÄëWISE contributes to *medical imaging science* beyond cognitive training‚Äîfor instance, could similar methodology optimize annotation, reader training, or quality control in radiology workflows?  \n   2. **Quantify perturbation realism.** Provide perceptual or image‚Äëquality metrics (e.g., SSIM, color fidelity) on medical data to ensure that enhancements do not produce artificial or non‚Äëphysiologic appearances.  \n   3. **Ablation within clinical tasks.** The ablation study is mainly on natural or dermoscopy data; it would strengthen the translational argument to repeat at least one ablation in histology.  \n   4. **Statistical generalization.** Randomization and participant pooling across experiments should be formalized‚Äîreport per‚Äësubject variance and number of images per condition.  \n   5. **Potential biases.** Include quantitative assessment of whether perturbations exacerbate known dataset biases (e.g., skin tone).  \n\n5. **Minor Comments**  \n   ‚Ä¢ Correct occasional typographical issues (‚Äúrobustu‚Äëfied‚Äù, ‚Äúprecariousness‚Äù).  \n   ‚Ä¢ Provide actual number of images per class and dataset partition in main text rather than only in appendix.  \n   ‚Ä¢ Clarify computational cost for generating perturbations and whether code supports GPU inference.  \n   ‚Ä¢ Suggest renaming sections on ‚Äúhistology‚Äîmalignant vs‚ÄØbenign‚Äù to specify true class labels (hyperplastic polyp vs‚ÄØsessile serrated adenoma).  \n   ‚Ä¢ Ensure all figures use consistent font sizes and color legends.\n\n6. **Overall Recommendation** ‚Äì **Reject (Out‚ÄØof‚ÄØScope / Low‚ÄØNovelty for TMI)**  \n   The study is methodologically careful and creative, but its primary focus‚Äîaugmenting human visual learning via robustified networks‚Äîis outside the central scope of *IEEE‚ÄØTransactions on Medical Imaging*, which prioritizes new imaging or analytic methodologies rather than cognitive‚Äëtraining paradigms. A venue emphasizing human‚ÄìAI interaction, neuro‚Äëinspired vision, or medical education technology (e.g., *IEEE‚ÄØTransactions on Cognitive and Developmental Systems* or *Frontiers in Digital Health*) would be a better fit.\n\n7. **Confidence Level:**‚ÄØ4‚ÄØ/‚ÄØ5  \n   High confidence in assessment of scientific quality and scope alignment; moderate uncertainty about editorial policy boundaries.\n\n---\n\n**Summary Judgment:**  \nInnovative and well‚Äëexecuted behavioral research with interesting implications for visual training, but its contributions lie more in human‚Äìmachine perceptual alignment than in the methodological advancement of medical imaging. Recommend rejection for TMI due to limited scope fit, while encouraging submission to a journal focused on human‚ÄìAI perception or educational applications.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents L-WISE (Logit-Weighted Image Selection and Enhancement), a method that uses robustified artificial neural networks to augment human visual category learning. The authors establish that ground truth logit values from adversarially-trained models can predict human image recognition difficulty (Figure 1A) and that perturbations maximizing these logits enhance human categorization accuracy (Figure 1B). L-WISE combines two strategies: (1) selecting training images based on model-estimated difficulty with gradually increasing complexity, and (2) applying image enhancements that decrease over time (Figure 2). The method was tested across three domains: moth species classification, dermoscopy lesion classification, and histology tissue classification. Results show 33-72% relative accuracy gains above chance on unmodified test images compared to controls, while reducing training time by 20-23% (Figure 4A). Ablation studies indicate both image selection and enhancement contribute to performance improvements (Table 1).\n\n## Weaknesses\n\n‚Ä¢ **Limited Mathematical Foundation for Enhancement Optimization** \n  - Equation 1 lacks theoretical justification for the choice of Œ± parameter and its relationship to enhancement effectiveness, with Œ± set to different values (0 for ImageNet, 1 for fine-grained tasks) without principled reasoning\n  - The projection operation in Equation 2 uses a simple L2 constraint but no analysis is provided on why this norm is optimal for human perception compared to other constraint formulations\n  - The step size Œ∑ and number of steps formula (ceil(2œµ) for ResNet-50, ceil(4œµ) for XCiT) appear ad-hoc without convergence analysis or theoretical grounding (Page 10, Appendix S1)\n\n‚Ä¢ **Insufficient Statistical Power and Multiple Comparisons Control**\n  - Sample sizes of ~30 participants per condition (Page 10) may be inadequate for detecting meaningful effect sizes, particularly for class-specific analyses where the authors acknowledge being \"statistically underpowered\" (Page 26)\n  - No correction for multiple comparisons is applied across the numerous statistical tests performed, including comparisons between conditions in Table 1 and across different image domains\n  - The logistic regression analysis in Figure 1A pools data from different experimental conditions without accounting for potential confounding factors or participant-level random effects\n\n‚Ä¢ **Questionable Generalizability of Enhancement Schedules**\n  - The enhancement schedules (exponential tapering from Œµ=8, difficulty selection increasing by 0.15 per block) were not optimized systematically and appear to be based on limited pilot testing (Page 10, Figure S12H-M)\n  - No evidence provided that the fixed schedules work across different participant populations, learning rates, or task complexities beyond the three tested domains\n  - The authors acknowledge not conducting \"exhaustive search for optimal curriculum design strategies\" (Page 9) but provide no framework for how practitioners could adapt these schedules to new domains\n\n‚Ä¢ **Methodological Concerns in Human Experiments**\n  - The 17ms presentation time for ImageNet animal classification (Page 10) is extremely brief and may not reflect natural viewing conditions, potentially limiting ecological validity\n  - Assignment of Greek mythology aliases to categories (Page 10) introduces an additional layer of abstraction that may not generalize to real-world learning scenarios where meaningful category names are available\n  - Dropout analysis (Appendix S9) suggests systematic differences between groups that could indicate selection bias, with control participants withdrawing at significantly higher rates\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen Mathematical Framework for Enhancement**\n  - Provide theoretical analysis or empirical justification for the choice of Œ± parameter in Equation 1, including systematic evaluation across different Œ± values to determine optimal weighting\n  - Compare L2 projection constraints against other norms (L‚àû, L1) and provide principled guidelines for selecting constraint types based on image domain characteristics\n  - Develop convergence criteria and theoretical bounds for the optimization procedure in Equation 2, replacing ad-hoc step size formulas with principled optimization schedules\n\n‚Ä¢ **Improve Statistical Rigor and Power Analysis**\n  - Conduct proper power analysis to determine adequate sample sizes for detecting meaningful effects, particularly for class-specific comparisons mentioned in Table S1\n  - Apply appropriate multiple comparisons corrections (Bonferroni, FDR) across all statistical tests and clearly report adjusted p-values\n  - Implement mixed-effects models that account for participant-level variability and potential confounding factors in the pooled analyses shown in Figure 1A\n\n‚Ä¢ **Develop Systematic Schedule Optimization Framework**\n  - Implement systematic hyperparameter optimization (grid search, Bayesian optimization) for enhancement and difficulty selection schedules across different domains\n  - Provide adaptive scheduling algorithms that adjust enhancement and difficulty parameters based on individual learner performance during training\n  - Establish guidelines or decision trees for practitioners to customize schedules based on domain characteristics, task complexity, and target population\n\n‚Ä¢ **Enhance Experimental Methodology and Controls**\n  - Test multiple presentation durations for image classification tasks to establish the robustness of findings across realistic viewing conditions\n  - Include conditions with meaningful category names alongside Greek aliases to assess the impact of semantic knowledge on learning outcomes\n  - Implement balanced randomization procedures and intention-to-treat analyses to address potential selection bias indicated by differential dropout rates across conditions",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Gabriel Kreiman",
      "Guy Gaziv",
      "James J. DiCarlo",
      "Morgan Bruce Talbot"
    ],
    "url": "pdfs/iclr.cc-2025-conference_f92d03707a21b34104857495b389885b58b8ecc6.pdf",
    "remote_url": "https://openreview.net/pdf/f92d03707a21b34104857495b389885b58b8ecc6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Diffusion based models",
      "Self-supervised MRI denoising"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a ``microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks. Code is available at https://github.com/FouierL/Di-Fusion.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a new self-supervised learning-based denoising method for diffusion MRI (dMRI). The proposed method leveraged the diffusion modeling concept, but instead of training a diffusion model with ‚Äúclean images‚Äù as x_0 and noise as x_T, it utilized two diffusion weighted images (DWIs) with different diffusion encodings at both ends of a ‚Äúdiffusion-like‚Äù process. A denoising network was trained by predicting one DWI using a linear combination of two DWIs and an added noise term. The linear combination coefficients are time-dependent and determined via a scheduling strategy similar to training a diffusion model. The network was then used for a conditional sampling step for generating the final denoised images. The idea to utilize images acquired with different diffusion encodings to denoise one of them is interesting and the training strategy is an interesting approach to leverage the diffusion modeling concept, especially with training only latter diffusion steps to reduce hallucinations. However, several key assumptions made are questionable and the overall methodology and presentation lacks clarity. Evaluation using only dMRI signal model goodness of fit is limited and can be biased. There are a few overstatements that can mislead the readers. Detailed comments can be found below.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nA diffusion-like modeling that learns the relationship between two DWI volumes with different diffusion encodings to denoise one or each other.\n\nTraining only later step diffusion to avoid hallucination\n\nA fusion strategy that exploits linear combination of two DWIs with different contrasts with time-dependent coefficients and iterative refinement.\n\nExtensive evaluations using both simulations that exactly followed the assumptions for the proposed methodology and practical magnitude DWI data.\n\n### Weaknesses\n\nThere are statements that can be misleading in the context of MR physics (aka domain knowledge). For example, \"the noise predominantly originates from physical interferences (Fadnavis et al., 2020a)\". This statement about physical interferences is  both vague and inaccurate. This work is dealing with thermal noise or noise resulting from thermal noise in the measurements, which is not really physical interferences depending on how ones interpret them. Another example, \"Different clinical applications require varying numbers of diffusion vectors and acquisition strategies, which makes modeling the noise distribution and further implementing denoising techniques challenging\". Acquiring DWIs with varying numbers of diffusion vectors had nothing to do with the difficulty of  modeling noise distribution.\n\nMany key assumptions for the proposed method was built on do not hold which made the theoretical/mathematical foundations questionable, e.g.,\na) It seems that the authors assumed DWIs acquired with different diffusion encodings had the same underlying ‚Äúclean‚Äù image and were corrupted by independent noise. This is inaccurate. In fact, two DWIs can have rather different contrasts due to the diffusion encoding effects, e.g., different diffusion encoding directions. More specifically, x and x‚Äô cannot be simply modeled as the same y plus different noise. What are the implications of this assumption not met?\n\nb) Line 111: The authors claimed that that the proposed method does not require an explicit noise model. This is an overstatement. The J-invariance assumption, which formed the basis of the training objective in Eq. (9) implicitly requires that the noise distribution be zero-means and conditionally independent. Furthermore, additive noise model was assumed, x = y + n1 (Line 200). In dMRI, the magnitude images with higher b-values (stronger diffusion weightings) can have lower SNR for which additive noise may not hold. These need to be clarified.\n\n-  Overall, the presentation lacks clarity and there seem to be some concerning inaccuracies.\na) The linear combination relationship claimed in Section 3.1 does not seem accurate. I checked the derivation. Eq. 31 is correct which is known (so this is not a contribution of the authors), but I'm not sure about going from Eq. 31 to 32 as F_theta predicts x_0, but they are not equal, and there is also an additional term of sigma_t^2*z. Therefore, I don't think it's a correct statement to say x_(t-1) is a linear interpolation between x_out and x_t. But is this really needed for the proposed method? I really don‚Äôt see a connection between what‚Äôs argued theoretically and what‚Äôs actually being implemented.\n\nb) There are a few other inaccurate mathematical statements and notations which are confusing. For example, Eq. 7, the left side has q(x1:T |xt*) which is a joint distribution for x1 to xT, and the right side is a Gaussian distribution for xt. \nOn Line 160: {xt}1:T was described as‚Äùobtained from the reverse process.‚Äù However, in\nFigure 1 and on the right side of Equation (7) on Line 186, it appears that xt is a corrupted version of xt*.  This interpretation, along with the notation in the Fig. 1, implies that {xt}1:T would represent a forward process. It appears to this reviewer the authors had not been using a consistent definition of forward and reverse diffusion which made the overall description rather confusing. These are just examples of inconsistencies found.\n\nc) According to the J-invariance property, the noise should ideally have zero mean\nand be conditionally independent of the target output. This requirement is necessary to ensure that the expected loss for self-supervised training asymptotical approaching the supervised loss. However, the input to F(.) in Eq. (9) includes xt*, which is a linear combination of x and x‚Äô (Eq. (6)). Given that x serves as the supervision signal for the loss, this implies a correlation between the input x‚àót and the target x, which would violate the conditional independence requirement for J-invariance.\n\n### Questions\n\nIn Eq. (5) on Line 155, the authors highlighted a specific term as the ‚Äùmajor difference‚Äù between xt‚àí1 and x^bar_t‚àí1. Could the authors clarify why this particular term is considered the primary source of difference? Furthermore, can the authors elaborate on the underlying reason(s) for the ‚Äúdrift‚Äù in the model and how it emerges during the reverse diffusion process?\n\nAccording to the definition of the Fusion process in Eq. (6)  and the ‚Äúforward process‚Äù in Eq. (7), it appears that the starting point for the forward process changes based on t, as x_t* is dependent on t. This dependence implies that the Fusion process dynamically adjusts the starting point of the forward process at each step, which is unconventional compared to typical diffusion models. Could the authors clarify the rationale behind this design?\n\nOther more recent self-supervised denoising methods should be compared, if not for all, e.g., Noise2Score and Recorrupted2Recorrupted etc.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript introduces a novel self-supervised learning-based denoising method tailored for diffusion MRI (dMRI). By utilizing two diffusion weighted images (DWIs) with varying diffusion encodings, the authors propose a method that predicts one DWI from a linear combination of the two, incorporating noise. While the concept of leveraging different diffusion encodings is intriguing and offers a fresh perspective on denoising, several key assumptions in the methodology raise questions about its clarity and validity.\n\n### Major Comments\n1. **Questionable Assumptions**: The theoretical foundations seem flawed, as the authors assume that DWIs obtained with different encodings share the same underlying ‚Äúclean‚Äù image and are affected by independent noise, which is inaccurate. Such an assumption leads to potential inaccuracies in the modeling.\n   \n2. **Clarity of Noise Model**: The claim that the method doesn't require an explicit noise model is overstated; the J-invariance assumption requires specific characteristics of the noise distribution that may not hold true in dMRI applications, particularly regarding additive noise constraints.\n   \n3. **Inconsistencies in Presentation**: Mathematical derivations, particularly from Eq. 31 to Eq. 32, lack foundational accuracy and clarity. The use of inconsistent definitions regarding forward and reverse diffusion processes creates confusion in the overall description.\n\n4. **Misleading Statements**: Certain statements regarding MR physics appear vague or inaccurate, potentially misguiding readers on the nature of noise in dMRI.\n\n### Minor Comments\n- The notation used in Eq. 7 is inconsistent; the joint distribution vs. Gaussian distribution needs clarification.\n- Descriptions in Figure 1 and Equation (7) need consistency regarding the definition of the diffusion processes.\n- Typographical and mathematical inaccuracies are present throughout, particularly regarding statistical definitions and conditions.\n\n### Summary Paragraph\nThe manuscript presents significant innovation through its unique use of DWIs for denoising in dMRI. However, the soundness of its methods is compromised by questionable assumptions and inconsistencies that undermine clarity. While the contribution is noteworthy, the evaluation is limited and may lead to biased interpretations. Overall, the manuscript exhibits potential but requires substantial refinement to meet the editorial criteria of significance, innovation, evaluation, and reproducibility.\n\n### Decision Recommendation\nMajor revision is recommended to address the critical issues raised regarding assumptions, clarity, and presentation of the methodology.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a self‚Äësupervised denoising framework for diffusion MRI (dMRI) that adapts the concept of diffusion modeling. Instead of training a standard diffusion model between clean and noisy data, the method uses two diffusion‚Äëweighted images (DWIs) with different diffusion encodings at the ends of a ‚Äúdiffusion‚Äëlike‚Äù process. A neural network is trained to predict one DWI from a time‚Äëdependent linear combination of the two DWIs with added noise, followed by a conditional sampling step for denoising. The idea of leveraging multiple diffusion encodings for denoising is interesting; however, the paper suffers from unclear formulation, questionable assumptions, and limited and potentially biased evaluation. Several statements are scientifically inaccurate or misleading, and the overall presentation lacks precision.  \n\n**Major Comments**  \n1. **Questionable Assumptions:** The framework assumes that DWIs acquired with different diffusion encodings share the same underlying ‚Äúclean‚Äù image with independent noise, which is physically inaccurate as DWIs can exhibit substantially different contrasts depending on encoding direction. The implications of this violated assumption are not addressed.  \n2. **Implicit Noise Model Dependence:** Although the authors claim the method does not require a noise model, the formulation inherently assumes additive, zero‚Äëmean, and independent noise through the J‚Äëinvariance property. These assumptions may not hold for magnitude dMRI data with low SNR and should be clarified.  \n3. **Mathematical Inconsistencies:** Several equations (e.g., Eqs. 7, 31‚Äì32) and notations appear inconsistent or incorrect. The derivation of the linear interpolation relationship and the definitions of forward vs. reverse diffusion are unclear and inconsistent between text and figures.  \n4. **Violation of J‚ÄëInvariance Conditions:** The inclusion of \\(x_t^*\\), a linear combination of x and x‚Ä≤, as input while using x as supervision introduces correlation that violates the conditional independence requirement central to J‚Äëinvariance.  \n5. **Evaluation Limitations:** Performance is assessed only via dMRI signal model goodness‚Äëof‚Äëfit, which can be biased and insufficient to support general claims. Comparisons with more recent self‚Äësupervised denoising methods (e.g., Noise2Score, Recorrupted2Recorrupted) are missing.  \n\n**Minor Comments**  \n- Some MR physics statements are inaccurate or misleading (e.g., mischaracterizing noise origins as ‚Äúphysical interferences‚Äù).  \n- Theoretical derivations and notation (particularly in Sections 3.1 and 3.2) need clearer, consistent explanations.  \n- Figures and equations should align in terms of symbol definitions.  \n- Typographical and clarity issues at specific line references (e.g., Lines 111, 155, 160, 186) should be corrected.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper addresses an important problem in dMRI denoising and offers an innovative adaptation of diffusion‚Äëmodel training concepts in a self‚Äësupervised context. However, the approach rests on debatable assumptions about DWI relationships and noise properties, undermining its theoretical soundness. Presentation and derivations contain inconsistencies that hinder reproducibility. Evaluation is limited and does not convincingly demonstrate practical benefits or generalizability. Overall, while the idea is creative, the manuscript requires substantial revision for soundness and clarity before it can meet standards of significance, innovation, evaluation, and reproducibility.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces Di-Fusion, a fully self-supervised diffusion MRI (dMRI) denoising method designed to enhance the signal-to-noise ratio (SNR) of MRI data without requiring clean reference data. The authors leverage novel late diffusion steps and an adaptive sampling process to create a single-stage framework that operates without an explicit noise model. Di-Fusion demonstrates superior performance over state-of-the-art denoising methods in tasks such as microstructure modeling and tractography. The method‚Äôs efficacy is validated through extensive quantitative and qualitative evaluations on real and simulated data.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- **Flexibility with data and noise models**: Instead of relying on explicit noise models or clean training data, the method relies on an N2N training strategy and pixel shuffling to reorganize the noise, providing strong generalization potential across different noise distributions. This suggests that the method has the potential to be applied to a wider range of denoising scenarios, such as cryo-EM.\n- Compared to the current state-of-the-art method, DDM^2, this approach demonstrates comprehensive improvements. Not only does it outperform in terms of performance, but it is also simpler to implement. Notably, this method does not require additional denoiser training, significantly enhancing its practical usability.\n- As a study on dMRI denoising, this paper conducts thorough and comprehensive experiments, including extensive comparisons and analyses on downstream task performance. This renders the work methodologically and experimentally well-rounded.\n\n### Weaknesses\n\nPlease refer to the **Questions** section for details.\n\n### Questions\n\n- To my understanding, the primary goal of dMRI denoising is to reduce the number of gradients required during acquisition, thus accelerating DWI scanning. In downstream tasks based on DTI, the authors compare DTI metrics computed from noisy images with those from denoised images. Why did the authors not use more DWI data to compute a clean DTI metric as a reference for comparison?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents Di-Fusion, a fully self-supervised diffusion MRI (dMRI) denoising method that aims to improve the signal-to-noise ratio (SNR) of MRI data without the necessity of clean reference data. By utilizing innovative late diffusion steps and an adaptive sampling process, the authors propose a single-stage framework that operates without an explicit noise model. Di-Fusion shows enhanced performance compared to state-of-the-art denoising methods specifically in microstructure modeling and tractography, with extensive quantitative and qualitative evaluations substantiating its effectiveness on both real and simulated datasets.\n\n### Major Comments\n1. **Flexibility**: The method does not depend on explicit noise models or clean training data, employing an N2N training strategy and pixel shuffling to reorganize noise. This enhances its generalization potential across various noise distributions and suggests possible applications in diverse denoising scenarios, including cryo-EM.\n2. **Comparative Performance**: Di-Fusion demonstrates significant improvements over the current leading method, DDM¬≤. It not only achieves superior performance but is also easier to implement, as it does not necessitate additional denoiser training, thereby improving practical usability.\n3. **Methodological Rigor**: The study conducts thorough experiments, offering extensive comparisons and analyses regarding downstream task performance, contributing to the overall robustness of the research.\n\n### Minor Comments\n- Clarification on the choice of DTI metrics for comparison would be beneficial.\n- Some figures can be more informative with enhanced labeling or descriptions.\n\n### Summary Paragraph\nIn summary, the manuscript presents a significant advancement in dMRI denoising through the Di-Fusion method. The innovation lies in its self-supervised approach and flexibility with data and noise models, fulfilling the importance criteria of the study. While the evaluations substantiate its effectiveness, additional clarity in experimental design‚Äîparticularly regarding cleanliness in DTI reference metrics‚Äîwould strengthen the reproducibility and evaluation aspects. Overall, Di-Fusion exhibits a successful combination of significance and innovation, with minor concerns over methodological transparency.\n\n### Decision Recommendation\nMajor revision is recommended to address the methodological concerns and enhance clarity in the comparisons made within the study.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Di-Fusion*, a fully self-supervised diffusion MRI (dMRI) denoising framework that enhances signal-to-noise ratio without requiring clean reference data or explicit noise models. The approach integrates late diffusion steps with an adaptive sampling process to create a single-stage, self-supervised learning strategy. The paper reports notable improvements over existing state-of-the-art denoising techniques, particularly in microstructure modeling and tractography. The study is overall clear, methodologically sound, and supported by comprehensive quantitative and qualitative experiments on both real and simulated data.  \n\n**Major Comments**  \n1. **Scope and Clarity of Evaluation:** Although the work demonstrates strong performance relative to current methods (e.g., DDM¬≤), it would be useful to better connect the denoising objective with the central application goal in dMRI‚Äîreducing the number of diffusion gradients to enable faster acquisitions.  \n2. **Reference Comparisons:** In downstream DTI analyses, the comparison between denoised and noisy data is informative. However, it remains unclear why the authors did not use additional DWI data to compute a cleaner DTI reference metric. Clarifying or justifying this choice would strengthen the evaluation and make performance claims more convincing.  \n\n**Minor Comments**  \n- The presentation and writing are generally clear, but it may help to make the relationship between N2N training, pixel shuffling, and adaptive sampling more explicit for non-specialist readers.  \n- Ensure that acronyms (e.g., DTI, DWI) are defined at first use for clarity.  \n- No ethical concerns are identified.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is **significant** in advancing self-supervised dMRI denoising by removing dependence on clean reference data. Its **innovation** lies in combining late-stage diffusion modeling with adaptive sampling in a single-stage framework. The **evaluation** is extensive and convincingly demonstrates improved performance over prior methods, though further clarification of reference comparisons would enhance credibility. In terms of **reproducibility**, the method is straightforward to implement and does not require separate denoiser training, supporting practical applicability.  \n\n**Decision Recommendation**  \n**Recommendation: Accept (Rating: 8)** ‚Äî The paper presents a solid and well-executed contribution, with minor revisions suggested for improved clarity in the evaluation rationale.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes a method for denoising diffusion MRI data sets.\n\nThis is a well-studied problem with many solutions in the literature. It is an important problem, as diffusion MRI is widely used for neuroscience and for clinical medicine. Recent years have seen a trend towards using self-supervised approaches to characterise the noise distribution and separate noise from the underlying signal.  This submission falls very much in this category, but proposes a different algorithm to those that are popular in the literature.\n\nExperiments compare against five baselines and results appear competitive with other methods, sometimes surpassing them.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe algorithm appears novel, although I found it hard to tell from the literature review how novel it is - whether it takes ideas from other areas and repurposes them for this problem, or if this is an algorithm specifically designed for diffusion MRI.\n\nThe problem is an important one with widespread application.\n\nResults appear competitive on a few example images shown in the figures.\n\n### Weaknesses\n\nThe baselines chosen do not include the most widely used denoising methods.  A clear omission is the random-matrix theory approaches proposed by Veraart et al in a series of very highly cited papers starting with Neuroimage 2016.\n\nThe only quantitative results use simulations, which seem likely to be skewed towards to capabilities of the proposed algorithm.\n\nThe qualitative results on actual human data are questionable as to whether they show improvement over baselines.  Even if they do, these are single cherry-picked examples and it is not clear whether these are advantages that manifest over large collections of images/scenarios.\n\n### Questions\n\nCorresponding to weaknesses listed above.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel method aimed at denoising diffusion MRI data sets, an area of significant importance in neuroscience and clinical medicine. Given the proliferation of solutions in existing literature, the authors contribute a different algorithm that aligns with recent trends of using self-supervised approaches for noise characterization. The overall clarity of the manuscript could be improved, particularly within the literature review context.\n\n**Major Comments:**  \n1. **Novelty:**\n   - The algorithm appears novel; however, clarity regarding its uniqueness in relation to existing methods is lacking. The literature review should better articulate whether it innovatively repurposes concepts from other domains or is specifically tailored for diffusion MRI.\n   \n2. **Evaluation Design:**\n   - The chosen baselines for comparisons do not encompass the most widely adopted denoising methods. A notable omission includes the random-matrix theory approaches from Veraart et al., whose papers are highly cited in this context.\n   \n3. **Results:**\n   - The quantitative validation relies solely on simulations, which may skew the perception of the proposed algorithm's effectiveness. The qualitative results derived from human data raise questions about demonstrated improvements over baseline methods. The reliance on cherry-picked examples does not convincingly support generalizability across diverse scenarios.\n\n**Minor Comments:**  \n- Figures showcasing results should be accompanied by clearer labels and captions for improved comprehension.  \n- Additional typographical errors should be corrected for better presentation clarity.  \n- Acronyms and notation should be explicitly defined upon first use for reader clarity.\n\n**Summary Paragraph:**  \nIn terms of significance, the problem addressed by the manuscript holds considerable relevance. While the innovation aspect is demonstrated through the proposed algorithm, its true novelty remains ambiguous. The evaluation lacks comprehensive comparisons with leading methodologies, which impacts reproducibility as well. The majority of results stem from simulations and limited qualitative examples on actual human data, raising concerns about their applicability to real-world contexts.\n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a new method for denoising diffusion MRI (dMRI) data. Denoising is a crucial step in dMRI analysis, with many techniques previously developed, including recent self-supervised approaches that model noise and signal separation. The authors position their work within this emerging category but claim a distinct algorithmic design. Overall, the paper addresses an important and widely applicable problem. The presentation is somewhat unclear in explaining the degree of novelty and in contextualizing the method relative to existing literature, but experimental results indicate competitive performance.  \n\n**Major Comments**  \n1. **Literature Context and Novelty** ‚Äì The review of prior work does not make it clear how the proposed algorithm differs from or extends established approaches. It is uncertain whether the method repurposes concepts from other domains or is specifically tailored for diffusion MRI.  \n2. **Baseline Selection** ‚Äì The comparative study omits key baselines, particularly the random-matrix theory methods by Veraart *et al.* (Neuroimage, 2016 and subsequent works). These are highly cited and represent a major benchmark in the field.  \n3. **Evaluation Design** ‚Äì Quantitative evaluations rely solely on simulations, which may favor the proposed algorithm. Real data experiments are limited to qualitative inspection of a few human examples, making it difficult to assess generalizability.  \n4. **Result Interpretation** ‚Äì Visual results on real data are ambiguous, and given the small and potentially cherry-picked examples, it is unclear whether improvements over competing methods are consistent across broader datasets.  \n\n**Minor Comments**  \n- Clarify the description of the algorithm‚Äôs originality and draw clearer links to related methods.  \n- Improve presentation and figure clarity to allow more straightforward comparison of results.  \n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work targets an important denoising problem with broad relevance in neuroscience and clinical imaging. While the proposed approach might offer innovation, its novelty relative to prior literature is not sufficiently demonstrated. Evaluation lacks breadth, relying mainly on simulations and limited qualitative evidence, which restricts assessment of robustness and reproducibility. The paper‚Äôs significance would be strengthened by inclusion of standard baselines and more comprehensive real-data validation.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial improvements are needed in comparative evaluation, validation on real data, and clarification of novelty before the manuscript can be fully assessed for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚úÖ**  \n   The submission introduces a novel self‚Äësupervised denoising framework (**Di‚ÄëFusion**) for diffusion MRI based on modifications to denoising diffusion probabilistic models (DDPMs). The paper focuses on imaging methodology: forward/reverse diffusion processes, statistical self‚Äësupervision, and controllable inference ‚Äî directly aligned with TMI‚Äôs scope in image reconstruction and quantitative image analysis.\n\n2. **Novelty & Contribution Level ‚Äì ‚úÖ**  \n   The authors propose several conceptually new elements:  \n   * ‚ÄúFusion process‚Äù to reduce sampling drift by aligning forward trajectories.  \n   * ‚ÄúDi‚Äëprocess‚Äù to empirically model real‚Äëworld noise without learned priors.  \n   * Restriction to latter diffusion steps to improve stability.  \n   * ‚ÄúRun‚ÄìWalk accelerated sampling‚Äù with adaptive termination for controllable denoising.  \n   This combination goes beyond incremental application and constitutes methodological innovation compared with prior self‚Äësupervised diffusion MRI denoising (e.g., DDM2, Patch2Self).\n\n3. **Technical & Experimental Rigor ‚Äì High**  \n   Mathematical derivations are detailed, describing both forward and reverse processes. Experiments span multiple real dMRI datasets and simulated MRI data, with ablation, assessment on downstream tractography and microstructure modeling, and quantitative SNR/CNR, PSNR, SSIM analyses. Reproducibility is covered (code link, hyperparameters, open datasets). However, more quantitative uncertainty analysis and statistical testing would strengthen claims.\n\n4. **Clarity & Presentation ‚Äì Moderate‚ÄëHigh**  \n   The manuscript is extensive and mathematically thorough. Figures aid intuition but text could be condensed; notation density may challenge general readers. English usage is good with minor grammatical inconsistencies. Structuring is consistent with IEEE style.\n\n5. **Ethical & Reproducibility Compliance ‚Äì ‚úÖ**  \n   All data are publicly available; privacy not an issue. The authors release source code and describe training details, implying strong reproducibility.\n\n---\n\n**Phase 2: IEEE‚ÄØTMI Review Report**\n\n### 1. Summary\nThe paper proposes **Di‚ÄëFusion**, a fully self‚Äësupervised denoising framework for diffusion MRI. It integrates self‚Äësupervised statistical denoising theory (Noise2Self) with diffusion probabilistic models, introducing a *Fusion process* to avoid sampling drift and a *Di‚Äëprocess* to estimate data‚Äëdependent noise. Training focuses on latter diffusion steps for stable learning, while inference applies an adaptive ‚ÄúRun‚ÄìWalk‚Äù sampling and adjustable stopping criterion to control denoising strength. Tested on multiple public datasets and simulations, Di‚ÄëFusion yields higher SNR/CNR, improved tractography coherence, and better microstructure fitting (CSD/DTI R¬≤) than existing denoisers (Patch2Self, DDM2, etc.).\n\n### 2. Strengths\n- **Methodological novelty:** Combines self‚Äësupervision, adaptive diffusion sampling, and real‚Äënoise modeling in a unified single‚Äëstage framework.  \n- **Technical rigor:** Mathematical derivations, algorithmic clarity, and ablation studies identify each component‚Äôs role.  \n- **Robust validation:** Evaluations across in‚Äëvivo and simulated data; clinical impact examined through tractography and DTI metrics.  \n- **Open‚Äësource release** enhances transparency and reproducibility.\n\n### 3. Weaknesses\n- The theoretical justification of stability improvements (e.g., training only latter steps) is largely empirical.  \n- Quantitative uncertainty or statistical significance of performance gains is missing.  \n- Heavy notation and supplementary length reduce accessibility.  \n- Practical runtime issues remain: diffusion models are computationally intensive, and inference time, though mitigated, may limit clinical use.\n\n### 4. Major Comments\n1. **Quantitative support for stability claims:**  \n   Provide statistical variance or confidence intervals across multiple subjects/runs to confirm stability improvements from latter‚Äëstep training.\n2. **Complexity and runtime:**  \n   Report detailed training/inference time compared to DDM2 and classical methods; discuss feasibility for clinical pipelines.\n3. **Ablation depth:**  \n   Current ablations are qualitative; include numerical metrics (PSNR/SSIM/SNR) for each component removal.\n4. **Notation clarity:**  \n   Streamline notation in Sections‚ÄØ3.1‚Äì3.3 to enhance readability; summarize equations in a table or schematic.\n5. **Clinical translation:**  \n   Elaborate on how parameter tuning (e.g., CSNR threshold) could be standardized for routine deployment.\n\n### 5. Minor Comments\n- Fix minor typos (‚ÄúDi-‚Äù sometimes written inconsistently, spacing before figures).  \n- When referencing appendices, denote key equations in main text more clearly.  \n- Improve figure captions (especially Fig.‚ÄØ1) for self‚Äëcontained explanation.  \n- Confirm that all datasets cite proper usage licenses and IRB waivers.\n\n### 6. Overall Recommendation\n**Major Revision** ‚Äì Strong potential for publication in *IEEE‚ÄØTMI*. The paper introduces a meaningful methodological advance in self‚Äësupervised diffusion MRI denoising but would benefit from clearer theoretical justification, additional quantitative comparisons, and improved presentation before acceptance.\n\n### 7. Confidence Level\n**4 / 5** ‚Äì High confidence in technical assessment; moderate confidence in clinical utility judgment awaiting clarified experiments.\n\n---\n\n**Reviewer summary judgment:**  \nThe work offers a significant step toward self‚Äësupervised diffusion MRI denoising within the diffusion‚Äëmodel framework. With enhanced quantitative validation and streamlined presentation, it would be a valuable contribution to *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces Di-Fusion, a self-supervised denoising method for diffusion MRI (dMRI) that leverages diffusion probabilistic models without requiring clean training data. The method addresses three key challenges: (1) a \"Fusion process\" that creates linear interpolations between noisy volumes to avoid drift during sampling, (2) a \"Di-\" process that characterizes real-world noise using differences between independent noisy measurements, and (3) training only the latter diffusion steps (Tc ‚â§ T) to reduce generative diversity and improve stability. The approach enables adaptive sampling termination based on a controllable SNR threshold (CSNR). Experiments on three dMRI datasets (Stanford HARDI, Sherbrooke 3-Shell, PPMI) and simulated data demonstrate state-of-the-art performance in downstream tasks including tractography, microstructure modeling, and DTI fitting compared to methods like DDM2, Patch2Self, and traditional approaches.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Equation (5) introduces notation ¬Øxt‚àí1 without clear definition until much later in the text (Section 3.1), creating confusion about the relationship between different trajectory sets {xt} and {¬Øxt}\n  - The \"mess(¬∑)\" operation in Equation (8) is defined only as \"spatial shuffling\" without mathematical rigor, yet it's critical for the noise characterization claim\n  - Appendix C.1 derivations contain algebraic steps that appear to conflate different probability distributions without proper justification for substituting x‚àót in place of x‚Ä≤\n\n‚Ä¢ **Insufficient theoretical justification for key design choices**\n  - The claim that training only latter diffusion steps (Tc = 300) reduces \"diverse generative capabilities\" lacks theoretical grounding beyond intuitive explanations (Section 3.2)\n  - No formal analysis of why the Fusion process mathematically prevents drift, despite this being a central contribution (Section 3.1, Figure 1a)\n  - The variance preservation claim for Œæx‚àíx‚Ä≤ in Appendix C.2 assumes independence that may not hold in practice for dMRI noise\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - Competing methods use different architectures while claiming \"fair comparison\" - only DDM2, Nr2N, and DIP adopt Di-Fusion's U-Net architecture (Appendix D.1)\n  - The primary evaluation metric (R¬≤ for microstructure fitting, Table S2) shows marginal improvements (0.967 vs 0.959 for Nr2N in CSD-CC) that may not be clinically significant\n  - Simulated experiments (Table 1) show high standard deviations for DDM2 and Nr2N but not for Di-Fusion, suggesting potential implementation or hyperparameter optimization bias\n\n‚Ä¢ **Limited scope and generalizability concerns**\n  - Method requires specific dMRI acquisition with adjacent volume pairs (j and j-1), which may not be available in many clinical protocols\n  - Noise model assumptions explicitly target only additive Gaussian thermal noise, excluding physiological artifacts common in real dMRI (Section 5, limitations)\n  - Computational overhead of diffusion model sampling (5+ hours training, Section D.1) limits practical clinical adoption compared to methods like Patch2Self\n\n‚Ä¢ **Presentation and reproducibility issues**\n  - Critical implementation details scattered across appendices make reproduction difficult (noise schedule in D.1, hyperparameters in multiple sections)\n  - Figure quality issues with small text and overlapping elements in Figure 2 and supplementary figures\n  - Inconsistent notation switching between FŒ∏ and F without subscripts throughout equations\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and presentation clarity**\n  - Provide formal mathematical definition of the \"mess(¬∑)\" operation with properties and theoretical justification for variance preservation\n  - Reorganize Section 3.1 to define all trajectory notations (xt, ¬Øxt, x‚àót) upfront before presenting equations\n  - Include rigorous proofs for drift prevention claims and theoretical analysis of why latter-step training reduces generative capacity\n\n‚Ä¢ **Strengthen theoretical foundations with formal analysis**\n  - Develop theoretical framework explaining why Tc = 300 is optimal, including analysis of training stability vs. generative capacity trade-offs\n  - Provide mathematical proof that the Fusion process prevents drift under stated assumptions\n  - Analyze conditions under which independence assumptions for noise hold in real dMRI scenarios\n\n‚Ä¢ **Improve experimental rigor and fairness**\n  - Implement all competing methods with identical U-Net architectures and report hyperparameter optimization procedures for fair comparison\n  - Include statistical significance testing for R¬≤ improvements and establish clinical significance thresholds\n  - Provide error bars and multiple runs for all methods in simulated experiments, not just selected ones\n\n‚Ä¢ **Expand applicability and address limitations**\n  - Evaluate performance on datasets without adjacent volume pairs by developing alternative pairing strategies\n  - Extend noise model to handle physiological artifacts beyond additive Gaussian noise\n  - Compare computational efficiency more comprehensively, including memory requirements and inference time comparisons with all baselines\n\n‚Ä¢ **Improve presentation and ensure reproducibility**\n  - Consolidate all implementation details into main paper or single appendix section with complete hyperparameter tables\n  - Enhance figure quality with larger fonts and clearer layouts, particularly Figure 2 and supplementary visualizations\n  - Standardize notation throughout and provide a symbols table for complex mathematical expressions",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chenxu Wu",
      "Qingpeng Kong",
      "S Kevin Zhou",
      "Zihang Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_6bd7c307edad6542cdf2945234d9af92905ebf8f.pdf",
    "remote_url": "https://openreview.net/pdf/6bd7c307edad6542cdf2945234d9af92905ebf8f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Large Language Models",
      "Large Vision Models",
      "Semantic Evaluation",
      "Computational Pathology",
      "Medical Imaging"
    ],
    "abstract": "Do Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts? Currently, only biased, qualitative approaches with limited, small-scale expert evaluations are available to answer this question. In this study, we propose the Boltzmann Semantic Score (BSS), a novel method inspired by state space modeling, to evaluate the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. Through extensive experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, we first establish a baseline of LLMs' performance in digital pathology and show that LLMs' encoding can be linked to patient outcomes. Then, we compared seven LVMs with BSS and showed that LVMs suffer from poor semantic capability when compared with encoded expert knowledge from pathology reports.\nWe also found statistically significant correlations between BSS (as a measure of structural similarity) and performance in two downstream tasks: information retrieval and survival prediction tasks. Our study also investigates the consensus among LLMs in evaluating LVMs using BSS, indicating that LLMs generally reach substantial consensus in rating LVMs, with some variation dependant on the cancer type. We believe the BSS metric proposed here holds significant potential for application in other domains with similar contexts. Data and code can be found in \\footnotesize \\url{ https://github.com/AIMLab-UBC/Boltzmann}",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a novel semantic metric called Boltzmann Semantic Score (BSS), which is inspired by state space modeling, to evaluate the semantic capability of large vision models (LVMs) in medical image processing. The authors demonstrate the effectiveness of this metric through experiments, revealing that LVMs exhibit low semantic capabilities. Additionally, BSS shows a strong correlation with the performance of LVMs on two clinical tasks: information retrieval and survival prediction.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.The paper is well-structured and clearly presented, which significantly improves its readability.\n2.The introduction of the Boltzmann Semantic Score (BSS) is an innovative approach inspired by state space modeling, providing a fresh perspective on evaluating the semantic capabilities of LVMs in medical image processing.\n3.The experiments demonstrate significant correlations between BSS and performance on the clinical tasks of information retrieval and survival prediction. Additionally, the experiments show LLMs' capabilities in these two key tasks and provide a quantitative comparison of LLM consistency. This consistency further supports BSS as an effective metric for evaluating the semantic capabilities of LVMs.\n\n### Weaknesses\n\n1.The computational complexity of BSS may be high in practical applications, particularly when applied to large-scale datasets.\n2.While the experiments show strong performance of BSS in the information retrieval task, its correlation with survival prediction is weaker. This may indicate that BSS lacks robustness across different types of tasks, especially in more complex medical applications. Therefore, its effectiveness as a general semantic metric remains to be further validated.\n3.The experiments focus on the tasks of information retrieval and survival prediction, but these tasks may differ in nature from other potential tasks. The consistency of LLMs and the effectiveness of BSS in other semantic tasks require further experimental validation across a broader range of tasks.\n4.The paper focuses on evaluating the semantic capabilities of existing LVMs, but it lacks concrete suggestions on how to improve their semantic performance. Although the limitations of LVMs are highlighted, there is little discussion on how to optimize or modify their architectures to overcome these shortcomings.\n\n### Questions\n\n1.Could the authors suggest ways to optimize BSS for large-scale datasets, or clarify if any tests on smaller subsets were conducted for comparative analysis?\n2. Since BSS performs better on information retrieval than survival prediction, could the authors elaborate on the reasons for this difference? Is there evidence BSS might generalize to other medical tasks?\n3. The paper notes limitations in LVMs' semantic capabilities. Do the authors have ideas on potential architectural or training adjustments that might address these limitations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces a novel semantic metric known as the Boltzmann Semantic Score (BSS), which is derived from state space modeling, aimed at evaluating the semantic capabilities of large vision models (LVMs) in medical image processing. The authors claim that BSS effectively highlights the limited semantic capabilities of LVMs and demonstrates a strong correlation with performance in clinical tasks, such as information retrieval and survival prediction.\n\n**Major Comments:**  \n1. The computational complexity associated with BSS may pose challenges in practical applications, particularly with large-scale datasets.  \n2. Although BSS shows strong correlation with information retrieval, its weaker correlation with survival prediction raises questions about its robustness across diverse tasks, suggesting a need for further validation.  \n3. The experimental focus is limited to information retrieval and survival prediction, potentially overlooking the performance of BSS in other semantic tasks. Comprehensive validation is essential to affirm BSS's effectiveness across a wider array of tasks.  \n4. The manuscript effectively identifies limitations in LVMs but does not offer specific strategies or recommendations for improving their semantic performance or modifying architectures to address these concerns.\n\n**Minor Comments:**  \n- The paper is well-structured and maintains clarity, enhancing overall readability.  \n- Further elaboration on the performance differences between information retrieval and survival prediction is needed, including potential generalizability of BSS to other medical tasks.  \n- Suggestions for optimizing BSS for large datasets or insights from tests on smaller subsets would improve the manuscript.  \n\n**Summary Paragraph:**  \nIn terms of significance, the introduction of the BSS presents innovative contributions to evaluating LVMs; however, methodological limitations reduce its robust application, particularly related to reproducibility and evaluation criteria. The experiments confirm some strengths of BSS in specific tasks, but weaknesses are evident in its correlation with complex medical tasks and the lack of practical guidance for improving LVMs' capabilities. Thus, while the study holds promise, further validation and expansion are crucial for comprehensive application.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThis manuscript introduces the *Boltzmann Semantic Score (BSS)*, a novel semantic metric inspired by state space modeling, designed to evaluate the semantic capabilities of large vision models (LVMs) in medical image processing. The study demonstrates BSS‚Äôs utility through experiments that reveal generally low semantic capability in current LVMs and show correlations between BSS and LVM performance on two downstream clinical tasks‚Äîinformation retrieval and survival prediction. Overall, the paper is clearly written and well organized, effectively communicating both the motivation and experimental findings.\n\n---\n\n**Major Comments**  \n1. **Computational Complexity:** The BSS metric may entail considerable computational cost, particularly for large-scale datasets. Discussion of scalability or algorithmic efficiency would improve practical applicability.  \n2. **Task Robustness:** While BSS shows a strong correlation with information retrieval outcomes, its weaker association with survival prediction suggests limited robustness across diverse clinical tasks. The generalizability of BSS beyond these specific settings should be further validated.  \n3. **Breadth of Experimental Evaluation:** The experiments focus on two tasks that may differ substantially from other potential medical imaging applications. Broader evaluation across additional tasks would strengthen evidence for BSS as a general semantic metric.  \n4. **Implications for LVM Development:** Although the paper identifies limitations in LVMs‚Äô semantic capability, it provides little guidance on how model architectures or training might be improved to enhance semantic understanding. Discussion of possible optimization directions would add value.  \n\n---\n\n**Minor Comments**  \n- Clarify whether BSS was tested on subsets of large datasets to evaluate scalability.  \n- Elaborate on why BSS correlates differently across the two tasks.  \n- Ensure consistent terminology when referencing large vision models versus language‚Äìvision models.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript is significant in proposing an innovative metric (BSS) for assessing semantic understanding in LVMs, a topic of increasing importance in medical imaging AI. The methodological novelty is notable, yet the evaluation is limited in scope, and robustness across tasks remains uncertain. While experimental procedures appear reproducible, additional details on computational efficiency and cross-task validation would strengthen confidence in the approach.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes the Boltzmann Semantic Score (BSS) as a novel metric to evaluate the semantic performance of latent visual models (LVMs) by leveraging large language models (LLMs). The idea behind using BSS is to quantify how well the visual representations align with text expert annotations. The authors show that BSS could be used as a measure of semantic similarity for LVMs. This paper include applications to pathology reports and whole slide images from  The Cancer Genome Atlas (TCGA), a large publicly available cancer genome dataset. Evaluation on various tasks such as information retrieval and survival prediction is included. This paper suggests high correlations for certain cancer between BSS and performance in both survival prediction and information retrieval.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n‚Ä¢\tBoltzmann Semantic Score is a novel approach to evaluate the semantic perspective of LVMs.\n‚Ä¢\tThe work leverages a large dataset (TCGA), and experiments are performed on several benchmarks.\n‚Ä¢\tThe work provides interesting insights on the model performance using BSS based on observed results.\n‚Ä¢\tHigh correlation between BSS and two downstream tasks i.e information retrieval and survival prediction, highlighting the significance of the results.\n‚Ä¢\tInteresting experiments on clinical tasks showing correspondence between LLMs and patient survival.\n\n### Weaknesses\n\n‚Ä¢\tThe mathematics for the explanation of the Boltzmann Score and its application is rather heavy. A more concise and clearer explanation would enable to understand better the intuition behind the usefulness of BSS as an evaluation metric for LVMs.\n‚Ä¢\tThe authors could develop more on the clinical implication and real-word use of BSS in decision-making.\n‚Ä¢\tSome insights are provided to explain the differences between LVMs and LLMs performance, but the paper could investigate more thoroughly those differences and inherent variations.\n‚Ä¢\tA discussion of the limitations of this work in terms of generalization under different contexts is lacking.\n\n### Questions\n\n‚Ä¢\tDid you visualize the semantic similarity and qualitatively assess the use of BSS as evaluation metric?\n‚Ä¢\tHow reliable is Boltzmann Semantic Score ? \n‚Ä¢\tWhat preprocessing was applied to the medical reports?\n‚Ä¢\tCould you explain the differences observed in Table 3 a) for the two-sided Pearon's Correlation Test? \n‚Ä¢\tWhat is the effect of bias originating from the datasets ?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript proposes the Boltzmann Semantic Score (BSS) as an innovative metric for evaluating the semantic performance of latent visual models (LVMs) using large language models (LLMs). It aims to quantify the alignment of visual representations with expert text annotations, demonstrating potential applications in pathology reports and whole slide images from The Cancer Genome Atlas (TCGA). The authors present evaluations across various tasks such as information retrieval and survival prediction, indicating high correlations between BSS and performance outcomes.\n\n**Major Comments:**  \n1. The mathematics surrounding the Boltzmann Score requires simplification for better comprehension, enabling clearer understanding of its utility as an evaluation metric for LVMs.\n2. Further exploration of the clinical implications and real-world applications of BSS in decision-making is necessary.\n3. The discussion on performance discrepancies between LVMs and LLMs could be more robust, investigating inherent variations in detail.\n4. The paper lacks a discussion on the limitations related to generalization across different contexts.\n\n**Minor Comments:**  \n- Some figures could be presented more clearly for improved clarity.  \n- The notation used in the mathematical explanations needs standardization.  \n- Typos in the text should be addressed for better readability.  \n\n**Summary Paragraph:**  \nIn terms of significance, the introduction of the Boltzmann Semantic Score marks a noteworthy contribution to the evaluation of LVMs. The innovative approach, backed by extensive dataset utilization and experimental evaluations, offers significant insights into model performance. However, the paper could benefit from more thorough discussions on clinical implications and an exploration of the limitations of the approach. The reproducibility of results, while suggested through high correlation findings, needs further confirmation through additional datasets or different contexts.\n\n**Decision Recommendation:**  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Boltzmann Semantic Score (BSS)*, a novel metric designed to assess the semantic performance of latent visual models (LVMs) by leveraging large language models (LLMs). The approach quantifies the alignment between learned visual representations and textual expert annotations, aiming to evaluate semantic similarity in a principled manner. Experiments use The Cancer Genome Atlas (TCGA) dataset, applying BSS to tasks such as information retrieval and survival prediction. The paper reports strong correlations between BSS and task performance, suggesting the potential of BSS as a surrogate measure for semantic evaluation. Overall, the manuscript presents an intriguing idea, though some aspects of the presentation and theoretical exposition require clarification.  \n\n**Major Comments**  \n1. The mathematical formulation of the Boltzmann Semantic Score is overly detailed and difficult to interpret. A more concise and intuitive explanation would improve accessibility and clarify how BSS captures semantic alignment.  \n2. The discussion of clinical implications is limited. The manuscript would benefit from elaborating how BSS could be integrated into medical decision-making or interpretable clinical workflows.  \n3. The differences in performance across various LVMs and LLMs are mentioned but not thoroughly analyzed. A deeper investigation of these variations would strengthen the conclusions.  \n4. The paper lacks a clear discussion of the limitations, particularly regarding generalization across datasets or disease domains.  \n5. More detail on preprocessing of medical reports and bias mitigation would enhance reproducibility and transparency.  \n\n**Minor Comments**  \n- Clarify the terminology and ensure consistent presentation of mathematical notation.  \n- Improve readability by simplifying dense mathematical sections.  \n- Provide clearer explanations for statistical tests (e.g., the two-sided Pearson correlation in Table 3).  \n- Consider visualizations of semantic similarity to qualitatively demonstrate BSS behavior.  \n- Minor grammatical and formatting issues should be addressed.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe work is *significant* in proposing a new evaluation metric linking visual-linguistic alignment to downstream medical tasks. Its *innovation* lies in integrating LLM-driven semantic assessment with LVM evaluation. However, *evaluation* could be strengthened by broader validation, clearer mathematical exposition, and better clinical contextualization. In terms of *reproducibility*, dataset details are adequate, but preprocessing steps and bias control require elaboration. Overall, the concept is promising but the manuscript needs refinement for clarity and depth.  \n\n**Decision Recommendation**  \n**Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a semantic metric, BBS, to evaluate LVMs from a medically semantic perspective.\n\nThe paper also leverages LLMs and a large and collective database of medical reports across more than 30 cancer types that represent more than 9,500 patients and it also establishes a baseline of LLMs' performance in two large-scale digital pathology tasks.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n* Originality: Noval evaluation metric which evaluates the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. \n\n* Quality: Extensive experiments including experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, comparison of seven LVMs with BSS, and two correlation analyses between BSS and performance in two downstream tasks.\n\n* Clarity: Well-painted figures and clear formulas.\n\n* Significance: Well-designed metric is important for the community, especially for the evaluation of latent embedding space.\n\n### Weaknesses\n\nI cannot find significant Weaknesses in this paper.\n\n### Questions\n\nWould it be possible to make the BSS a training loss to guide and supervise vision encoder embeddings to align with the strong LLM embeddings? Will BSS have additional advantages over contrastive learning loss, such as smaller batch size requirements?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces a novel semantic metric, BBS, aimed at evaluating latent variable models (LVMs) from a medical semantic perspective. The work utilizes large language models (LLMs) alongside an extensive database of medical reports covering over 30 cancer types and representing more than 9,500 patients. Additionally, it establishes a baseline for LLMs' performance in two significant digital pathology tasks.\n\n**Major Comments:**  \n1. **Originality:** The proposed BBS metric is innovative, as it evaluates the encoding space of LVMs derived from medical images using the encoding space of LLMs from medical reports.\n2. **Quality of Experiments:** The manuscript presents comprehensive experiments, including an analysis of 32 datasets from The Cancer Genome Atlas, comparisons of seven LVMs with BSS, and two correlation analyses linking BSS to performance in downstream tasks.\n3. **Significance of the Metric:** The well-structured BBS metric is crucial for the research community, particularly in assessing latent embedding spaces in medical contexts.\n\n**Minor Comments:**  \n- The figures are well-illustrated, and the formulas are clearly presented, contributing to the overall clarity of the manuscript.  \n- The paper contains no discernible significant weaknesses.\n\n**Summary Paragraph:**  \nThis review indicates that the manuscript demonstrates high significance and originality through its novel evaluation metric and well-designed experiments. While there are no significant weaknesses noted, the integration of BBS as a training loss for guiding vision encoder embeddings could be explored further. This inquiry may elucidate whether BBS offers advantages over existing contrastive learning losses, particularly concerning batch size requirements. Overall, the evaluation and experiments offer a strong foundation that aligns well with the editorial criteria of significance, innovation, evaluation, and reproducibility.\n\n**Decision Recommendation:**  \nMinor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel semantic metric, BBS, designed to evaluate latent vision models (LVMs) from a medically semantic perspective. The approach leverages large language models (LLMs) and a comprehensive database of medical reports covering more than 30 cancer types and 9,500 patients. The study provides a baseline of LLM performance across two large-scale digital pathology tasks, with generally clear presentation and comprehensive experimental support.  \n\n**Major Comments**  \n1. The work presents substantial originality by proposing an evaluation metric (BBS) that relates visual embeddings of LVMs to semantic embeddings derived from medical reports via LLMs. The idea of measuring semantic alignment between modalities is innovative and relevant to the medical imaging community.  \n2. The evaluation is extensive, including experiments on 32 datasets from The Cancer Genome Atlas, the use of five state-of-the-art LLMs, comparison of seven LVMs with BBS, and correlation analyses between BBS scores and downstream task performance. This supports the soundness and generalizability of the method.  \n3. The paper is clearly written and visually well presented, with well-designed figures and formulas that clarify the proposed method.  \n4. While the reviewer does not identify significant weaknesses, they raise an interesting question regarding the potential of using BBS not only as an evaluation metric but also as a training loss to guide vision encoder embeddings. The reviewer further inquires whether such usage might offer advantages over contrastive learning losses, such as requiring smaller batch sizes.  \n\n**Minor Comments**  \n- The presentation could be further refined to enhance readability; currently rated moderate (Presentation: 3).  \n- No ethical concerns are identified by the reviewer.  \n\n**Summary Paragraph**  \nOverall, the paper is strong in significance and innovation, introducing an important metric that could broaden the community‚Äôs ability to assess semantic quality in vision-language systems. Evaluation is comprehensive and reproducible across a large and well-documented dataset. The main limitation lies in the absence of exploration of BBS as a training component, which could strengthen its practical impact. Clarity and presentation are satisfactory but could still be improved.  \n\n**Decision Recommendation**  \n**Recommendation: Accept (Rating: 8)** ‚Äî The paper is judged to be original, methodologically sound, and significant, with no major weaknesses identified.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI:**  \n‚úÖ The manuscript sits squarely within the methodological scope of IEEE TMI. It introduces a new metric‚Äî**the Boltzmann Semantic Score (BSS)**‚Äîfor evaluating large vision models (LVMs) by leveraging the embedding spaces of large language models (LLMs) applied to paired image‚Äìtext data from medical pathology. The paper focuses on foundational mathematical formulation, algorithmic definition, and quantitative validation using large-scale medical datasets (TCGA), rather than an application case study. The contribution is methodological and generalizable to medical imaging domains.\n\n2. **Novelty & Contribution Level:**  \nModerate to high novelty. BSS introduces a formal probabilistic framework (based on Boltzmann factors and state-space analogy) for quantifying cross-modal semantic alignment between medical image and text embeddings. Unlike prior comparative studies of LVM‚ÄìLLM correlations, this work defines a differentiable and theoretically motivated metric. The notion of ‚Äúsemantic similarity‚Äù between modalities grounded in energy-based modeling is new within medical imaging evaluation. The theoretical innovation is meaningful, though empirical validation still relies on standard dataset/architecture comparisons.\n\n3. **Technical and Experimental Rigor:**  \nThe manuscript is strong in theoretical exposition, with clear equations and algorithmic outlines. Experimental design is extensive: five LLMs, seven LVMs, 32 TCGA cohorts, and two downstream tasks (retrieval and survival). The statistical methods (Pearson correlation tests, cross-validation, standard retrieval and survival evaluation metrics) are appropriate. Reproducibility is aided by public code and detailed appendices.  \nPotential gaps:  \n- The description of hyperparameters and data preprocessing, while extensive in the appendix, could still be summarized in the main text for clarity.  \n- Limited ablation on the sensitivity of BSS hyperparameters (e.g., *k* selection, influence of LLM choice).  \n- The baseline comparison to existing semantic or alignment metrics (e.g., CCA-based measures, mutual information) is missing.  \n\n4. **Clarity and Presentation:**  \nVery well organized and clearly written for a technically complex topic. Figures are numerous and generally informative. The mathematical narrative (from intuitive derivation to formal algorithm) is sound, though slightly dense in mid-section 3. Minor language and formatting inconsistencies (e.g., spacing, figure callouts). Overall readability is high.\n\n5. **Ethical and Reproducibility Compliance:**  \nDataset usage (public TCGA), de-identification, and preprocessing are transparently described. Code availability is promised. Ethical statements about patient privacy and bias are explicitly included in the appendix. The study meets TMI reproducibility and compliance expectations.\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \nThe authors propose the **Boltzmann Semantic Score (BSS)**, a theoretically derived cross-modal metric to evaluate the semantic alignment between visual features from large vision models (LVMs) and textual encodings from large language models (LLMs) in medical imaging. Using TCGA pathology images and reports, they validate BSS as a measure of semantic correspondence that correlates with downstream task performance (retrieval, survival prediction). The study argues that BSS provides a quantitative, scalable alternative to expert-based qualitative evaluations.\n\n2. **Strengths**  \n- Clear methodological innovation linking statistical mechanics with multimodal embedding evaluation.  \n- Large-scale empirical validation across dozens of cancer datasets.  \n- Transparent code/data availability and detailed experiments.  \n- Conceptually bridges LLM-based reasoning with visual representation quality assessment in medicine.  \n- Potential extensibility beyond pathology to other imaging‚Äìtext modalities.  \n\n3. **Weaknesses**  \n- Limited comparative analysis with existing quantitative alignment metrics (e.g., cosine similarity baselines, cross-modal contrastive losses).  \n- The connection between BSS magnitude and specific model behaviors is descriptive rather than mechanistically analyzed.  \n- Writing occasionally conflates ‚Äúsemantic‚Äù with ‚Äúclinical‚Äù relevance‚Äîclarify interpretability versus outcome prediction.  \n- Some figures (e.g., dense multipanel charts) could be simplified for readability.  \n- Statistical significance is shown for correlations but effect size interpretation is modest.\n\n4. **Major Comments**  \n- **Contextual Comparison:** Include comparison or discussion against established metrics for modality alignment (e.g., CCA, linear alignment, information-theoretic similarity), situating BSS within quantitative literature.  \n- **Sensitivity and Robustness:** Provide a brief ablation or analytic justification of the choice of *k* in neighborhood graphs and of the Boltzmann formulation constants.  \n- **Interpretability / Usability:** Discuss what a specific BSS value implies operationally‚Äîhow practitioners might use it to compare or tune LVMs.  \n- **Generalization / Future Work:** Summarize in main text (not only appendix) how BSS could inform model training (e.g., as a loss), since this is important for TMI‚Äôs methodological emphasis.  \n\n5. **Minor Comments**  \n- Correct minor typographical spacing (‚ÄúLVMs,‚Äù ‚ÄúBoltzmann‚Äôs,‚Äù etc.).  \n- Improve axis labeling readability in Figs.‚ÄØ5‚Äì8.  \n- Condense the introduction by merging some background on foundation models.  \n- Ensure consistent citation formatting (some ‚Äúet‚ÄØal.‚Äù listings inconsistent).  \n\n6. **Overall Recommendation**  \n**Major Revision.**  \nThe paper introduces an interesting and potentially impactful metric, with sound theory and strong experiments. However, to meet TMI‚Äôs threshold for methodological completeness, the authors should (a) include direct comparison with alternative semantic similarity metrics, (b) perform a limited ablation to confirm robustness, and (c) clarify interpretability and practical relevance. Addressing these points would elevate the contribution from exploratory to mature methodological advancement.\n\n7. **Confidence Level:**‚ÄØ**5‚ÄØ/‚ÄØ5** ‚Äî reviewer is confident in evaluation given background in imaging‚ÄìAI methodology.\n\n---\n\n**Summary Statement for Editors:**  \nThe submission offers a novel, theoretically grounded metric that could become a useful tool for assessing and aligning multimodal medical foundation models. It is methodologically innovative, scientifically rigorous, and well executed. With additional comparative and robustness analyses, it is likely to reach the publication standard of IEEE‚ÄØTMI.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces the Boltzmann Semantic Score (BSS), a novel metric to evaluate Large Vision Models (LVMs) using Large Language Models (LLMs) as semantic references. The method addresses the question of whether LVMs extract medically relevant features similar to human experts by comparing structural similarity between LVM and LLM embedding spaces. The authors first establish LLM baselines on 32 TCGA datasets using five state-of-the-art LLMs for information retrieval and survival prediction tasks. They then propose BSS, inspired by statistical mechanics and Boltzmann distributions, to measure semantic alignment between vision and text encodings. The method constructs k-nearest neighbor graphs in both modalities and computes semantic scores based on matching and non-matching states. Experiments on seven LVMs show generally low BSS values, indicating poor semantic capability. The authors demonstrate statistically significant correlations between BSS and downstream task performance, and show substantial consensus among LLMs in ranking LVMs using Cohen's kappa scores.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - The transition from Boltzmann factor in Equation 2 to the constant ‚àöd1 replacing kT lacks proper justification (Section 3.3, Page 5), making the physical analogy questionable\n  - The definition of sets A and D in Equations 4-5 (Page 5) contains unclear notation where Li = Lj relationships are not precisely defined\n  - Algorithm 1 (Page 19) lacks mathematical rigor in describing how \"corresponding nodes\" are identified in the observer graph\n  - The normalization in Equation 6 (Page 5) assumes non-zero denominators but provides no handling for edge cases\n\n‚Ä¢ **Experimental design and validation limitations**\n  - BSS correlations with downstream tasks show high variability across datasets (Table 3a-3b, Page 9) with some non-significant results, questioning the metric's reliability\n  - The choice of k=5 neighbors (Figures 5-6, Pages 8-9) appears arbitrary with insufficient ablation studies on this critical hyperparameter\n  - Survival prediction experiments use only 8 cancer types (Table 2, Page 8) while information retrieval uses all 32, creating inconsistent evaluation scope\n  - The supervised pooling methodology using AbMIL (Section J.2, Page 19) introduces task-specific bias that may confound BSS measurements\n\n‚Ä¢ **Methodological assumptions and theoretical foundation concerns**\n  - The fundamental assumption that LLM representations serve as ground truth for medical semantics lacks empirical validation beyond retrieval tasks (Section 3.1, Page 3)\n  - The physical analogy to statistical mechanics appears forced, with energy differences defined as L2 distances without theoretical justification (Equation 2, Page 5)\n  - The method assumes semantic similarity should correspond to spatial proximity in embedding spaces, which may not hold across different model architectures\n  - No comparison with existing semantic evaluation metrics or human expert validation is provided to establish BSS validity\n\n‚Ä¢ **Reproducibility and generalization challenges**\n  - Critical implementation details for graph construction and nearest neighbor computation are missing from the main text\n  - The consensus analysis (Figure 7, Page 9) shows moderate agreement for some LLM pairs, suggesting potential inconsistencies in BSS computation\n  - Dataset-specific performance variations (Figures 5-6, Pages 8-9) raise questions about BSS generalizability across different medical domains\n  - The method's dependence on specific LLM architectures may limit its applicability as foundation models evolve\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical rigor and theoretical foundation**\n  - Provide rigorous justification for the choice of ‚àöd1 as the temperature equivalent, including sensitivity analysis across different dimensionalities\n  - Clarify the mathematical definition of matching states in sets A and D with explicit algorithmic descriptions\n  - Add formal complexity analysis and convergence properties of the BSS computation\n  - Include theoretical analysis of BSS behavior under different embedding space properties\n\n‚Ä¢ **Enhance experimental validation and consistency**\n  - Conduct comprehensive ablation studies on the k parameter across multiple values and datasets to justify the choice of k=5\n  - Expand survival prediction experiments to all 32 cancer types for consistent comparison with retrieval tasks\n  - Include comparison with existing semantic similarity metrics and human expert evaluations as validation benchmarks\n  - Separate analysis of supervised vs unsupervised pooling effects on BSS to isolate methodological confounds\n\n‚Ä¢ **Validate core methodological assumptions**\n  - Provide empirical evidence that LLM representations truly capture medical semantics beyond simple keyword matching\n  - Compare BSS with direct human expert assessments of LVM semantic quality on a subset of cases\n  - Investigate the relationship between embedding space geometry and semantic meaning across different model architectures\n  - Include analysis of failure cases where BSS may not reflect true semantic alignment\n\n‚Ä¢ **Improve reproducibility and broader applicability**\n  - Provide complete algorithmic specifications and open-source implementation for all BSS computation steps\n  - Address the consensus variability issue by investigating sources of disagreement between different LLMs\n  - Test BSS stability across different medical domains and imaging modalities beyond pathology\n  - Develop guidelines for selecting appropriate reference LLMs and handling cases where LLM consensus is low",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Ali Bashashati",
      "Ali Khajegili Mirabadi",
      "Hossein Farahani",
      "Katherine Rich"
    ],
    "url": "pdfs/iclr.cc-2025-conference_4a62e78f3be27e86a9498d083dc2dcb4d2136f2f.pdf",
    "remote_url": "https://openreview.net/pdf/4a62e78f3be27e86a9498d083dc2dcb4d2136f2f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Vision-language model",
      "fine-grained alignment",
      "large-scale pre-training",
      "CT image"
    ],
    "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.",
    "decision": "Accept (Spotlight)",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes a fine-grained vision-language model (fVLM) to improve CT image interpretation by associating specific anatomical regions in CT scans with corresponding descriptions in radiology reports, addressing limitations of global image-report contrastive methods. It introduces a large dataset, MedVL-CT69K, encompassing 272,124 CT scans and achieves state-of-the-art performance. Key contributions include anatomy-level contrastive learning and a method to handle false negatives.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. Fine-Grained Anatomy-Level Alignment: The paper introduces a novel, fine-grained vision-language model (fVLM) that aligns anatomical regions in CT images with corresponding report descriptions.\n2. Large and Comprehensive Dataset: Experiments were conducted on a range of datasets including the largest CT dataset to date (MedVL-CT69K)\n3. Effectively tackling shortcomings of their method by introducing the dual false-negative reduction approach.\n\n### Weaknesses\n\n1. Experiments are incomplete: Table 1 doesn't include a performance evaluation of the methods from Table 3, namely CT-CLIP, CT-VocabFine, CT-LiPro. Table 3 doesn't include performance evaluation of the methods from Table 1. In both cases it's not argued why those experiments were not conducted. For Table 1, and Table 2 it's also unclear how the 2D approaches were adapted for 3D and how their evaluation was then done. Additionally, the evaluation metrics in Table 1 and Table 3 differ. Why? Furthermore, the performance comparison in Table 2 should also include comparisons to other CT-CLIP approaches, not only natural image counterparts (he it's again not clear how the 2D models were adapted to 3D).  \n2. Report generation uses only a single NLU metric. Other NLU metrics such as ROUGE would be interesting. Furthermore, using the GREEN metric[1] or RadFact[2] for evaluation would allow assessing clinical relevance.\n3. T-SNE visualization: An (additional) comparison of the embeddings to CT-CLIP would be be interesting to see if they have the same clustering behavior. \n4. Ablations study on masking missing: The masking approach is not well investigate and no ablation studies are done. One intuitive baseline would be the anatomical cropping of the image. What's the advantage of the masking approach? \n5. The authors approach only works for CT scans, since it relies on anatomy segmentations from TotalSegmentator. Furthermore, only on the classes which are available in TotalSegmentator.\n\n[1] Ostmeier, S., Xu, J., Chen, Z., Varma, M., Blankemeier, L., Bluethgen, C., ... & Delbrouck, J. B. (2024). GREEN: Generative Radiology Report Evaluation and Error Notation. arXiv preprint arXiv:2405.03595.\n\n[2] Bannur, S., Bouzid, K., Castro, D. C., Schwaighofer, A., Bond-Taylor, S., Ilse, M., ... & Hyland, S. L. (2024). MAIRA-2: Grounded Radiology Report Generation. arXiv preprint arXiv:2406.04449.\n\n### Questions\n\n1. What's the authors reproducibility statement? Will the dataset, code and weights be released? Especially publishing the dataset would be of great value for the community.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a fine-grained vision-language model (fVLM) aimed at enhancing CT image interpretation by associating specific anatomical regions in CT scans with corresponding descriptions in radiology reports. This approach seeks to overcome the limitations of existing global image-report contrastive methods. Notably, the work utilizes a substantial dataset, MedVL-CT69K, which includes 272,124 CT scans, achieving state-of-the-art performance. The main contributions consist of anatomy-level contrastive learning and a strategy to address false negatives.\n\n**Major Comments:**\n1. **Incomplete Experiments:** The performance evaluation in Table 1 omits results for methods listed in Table 3, particularly CT-CLIP, CT-VocabFine, and CT-LiPro. Conversely, Table 3 fails to report on the methods in Table 1. The rationale for these omissions is unclear. Additionally, the adaptations of 2D approaches for 3D evaluation are not well-explained. There are inconsistencies in evaluation metrics used in Tables 1 and 3.\n2. **Report Generation Metrics:** The evaluation relies solely on one NLU metric. Incorporating additional metrics such as ROUGE, GREEN, or RadFact could enhance the assessment of clinical relevance.\n3. **T-SNE Visualizations:** It would be beneficial to compare the embeddings with CT-CLIP to examine clustering behavior.\n4. **Ablation Study on Masking:** The masking technique lacks thorough investigation, and no ablation studies are performed. A baseline using an anatomical cropping approach could clarify the advantages of the proposed masking method.\n5. **Model Limitations:** The model is restricted to CT scans due to its reliance on anatomical segmentations from TotalSegmentator, limiting its applicability to available classes.\n\n**Minor Comments:**\n- Clarification of the adaptation of 2D models to 3D would improve the manuscript‚Äôs clarity.\n- A clearer explanation of evaluation metrics across tables would enhance the understanding.\n- Minor typographical revisions are needed throughout the document for better presentation.\n\n**Summary Paragraph:**  \nThis study addresses significant challenges in CT image interpretation through the innovative fVLM and a large-scale dataset. While the contributions are noteworthy, particularly in contrastive learning and addressing false negatives, the work suffers from incomplete experiments and limitations in the robustness of evaluation metrics. The reproducibility and broader applicability of the model warrants further exploration, especially considering its constraints on anatomical segmentations. The review indicates that while there are promising aspects, several improvements could enhance the manuscript's overall contribution to the field.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a fine-grained vision‚Äìlanguage model (fVLM) designed to enhance CT image interpretation by linking specific anatomical regions to corresponding textual descriptions from radiology reports. To support this approach, the authors introduce *MedVL-CT69K*, a large dataset comprising 272,124 CT scans. The proposed model incorporates anatomy-level contrastive learning and a dual false-negative reduction strategy to overcome limitations of previous global contrastive methods. The paper is generally well written and clear in its exposition, with promising performance results suggesting state-of-the-art achievements in CT image‚Äìreport alignment.\n\n**Major Comments**  \n1. **Incomplete Experimental Evaluation:** The experimental design is inconsistent across Tables 1‚Äì3. Some key methods (CT-CLIP, CT-VocabFine, CT-LiPro) are not evaluated across all tables, and the authors do not explain why. It is also unclear how 2D methods were adapted for 3D data, and why evaluation metrics differ between tables. The comparison in Table 2 should include other CT-CLIP variants, not only models trained on natural images.  \n2. **Limited Report Generation Metrics:** The evaluation of the report generation task uses only a single NLU metric. Incorporating additional metrics such as ROUGE, GREEN, or RadFact would provide a more comprehensive assessment, particularly regarding clinical relevance.  \n3. **Embedding Visualization:** The t-SNE visualization could be strengthened by including comparisons with CT-CLIP to verify whether similar clustering patterns are observed.  \n4. **Ablation on Masking Strategy:** The masking-based approach is insufficiently investigated. An ablation comparing this to a simpler anatomical cropping baseline would clarify its benefit.  \n5. **Scope Limitation:** The method depends on anatomical segmentation from TotalSegmentator, restricting applicability to CT data and to the anatomical classes provided by that tool.  \n\n**Minor Comments**  \n- Clarify the reproducibility plan, specifying whether code, model weights, and particularly the dataset will be released, as this would be valuable for the community.  \n- Minor inconsistencies in table labeling and explanations could be revised for clarity.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper offers a significant and innovative contribution by introducing fine-grained anatomy-level vision‚Äìlanguage alignment and a large CT dataset. However, evaluation completeness and breadth are limited, leaving some uncertainty about the robustness of the results. The work is technically sound but would benefit from more extensive ablation studies and comparative analyses. Reproducibility is uncertain pending clarity on dataset and code release plans.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The study introduces a promising and potentially impactful approach, but it requires more thorough experimental comparisons, expanded evaluation metrics, and clearer reproducibility assurances before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors present an vision-language model for  anatomy-aware medical report generation, adressing the task of false negative reduction.  A large (private) dataset is curated and good performances on public datasets are presented.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n* The study addresses an interesting multi-modal learning problem from the medical domain\n* The presented false negative reduction is a meaningful integration to vision-language model report generation\n* The results are promising indicating the relevance of this addition\n\n### Weaknesses\n\n* The methodology is not quite clear\n* There is no codebase, preproduction may be a challenge.\n\n### Questions\n\n* Image Encoder Setup: The authors mention using a ViT base model with MAE pretraining on ImageNet-1K and a patch size of 16 x 16 x 32 (referenced in A.2). However, it‚Äôs unclear how they process 3D volumes with 3D patch embeddings through a 2D ViT model. Although they provide details on the training procedure, the overall model architecture remains vague. Given the lack of source code, a clearer description of the model's structure is crucial. Can the authors improve?\n* Comparison with 2D CLIP Methods: The comparisons made with 2D CLIP methods in Table 1 are not fully explained. Specifically, it's unclear how they adapted these methods to a 3D context and how the training was conducted. Please clarify.\n* Evaluation with CT-CLIP: In Table 3, the comparison with CT-CLIP models is ambiguous. Did the authors fine-tune a pre-trained model (based on their in-house data) on CT-RATE, or was there a different approach taken?\n* t-SNE Consistency: To enhance the consistency of the embedding spaces shown in Fig. 7, the authors could consider setting a random seed before calculating the t-SNE embeddings.\n* HU Value Clipping: The authors chose to clip HU values between -300 and 400, which raises a question: Does this range cover all abnormalities? For instance, lung-related abnormalities are typically closer to -1000 (representing air), while bone-related abnormalities can reach values close to 1000. Since they utilized both non-contrast and contrast-enhanced CT scans (arterial, venous, and delayed phases), did they apply this same HU preprocessing across all scans? Additionally, did the authors use this range for the CT-RATE training ablations (which is different than the original CT-RATE paper)? Please clarify.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nnone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a vision-language model aimed at generating anatomy-aware medical reports, specifically targeting the reduction of false negatives. The authors have curated a substantial private dataset and demonstrate promising performances on public datasets, contributing to the field of multi-modal learning in medical applications.\n\n### Major Comments\n1. **Methodology Clarity**: The methodology lacks clarity, particularly regarding the model architecture and processing of 3D volumes with a 2D ViT model.\n2. **Codebase Availability**: The absence of a codebase may pose challenges for reproducibility.\n3. **Model Architecture**: The overall model architecture is vaguely described, necessitating a more comprehensive explanation.\n4. **Comparative Analysis**: The adaptation of 2D CLIP methods to a 3D context is not fully elaborated, particularly in the training details.\n5. **CT-CLIP Evaluation**: The evaluation method involving CT-CLIP models is ambiguous, and further clarification on fine-tuning practices is required.\n6. **HU Value Clipping**: Questions arise regarding the clipping of HU values between -300 and 400, particularly in relation to covering all potential abnormalities. Clarification on the uniform application of this clipping across different CT scans is needed.\n\n### Minor Comments\n- The embedding spaces presented in Fig. 7 could benefit from consistency improvements by setting a random seed before calculating t-SNE embeddings.\n\n### Summary Paragraph\nOverall, the study presents significant innovations in the intersection of vision and language models for medical report generation, showcasing an important contribution towards improving accuracy in this domain. However, the methodological clarity and reproducibility aspects need considerable attention. Specifically, the lack of detailed explanations regarding the model architecture and processing approaches undermine the evaluation's robustness. While the study exhibits promising findings, further clarifications and improvements are essential to meet the rigor required for reproducibility and significance in the field.\n\n### Decision Recommendation\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a vision‚Äìlanguage model for anatomy-aware medical report generation, targeting the reduction of false negatives. The authors curate a large private dataset and report promising results on several public benchmarks. The topic is timely and relevant to multimodal learning in medical imaging, and the overall presentation demonstrates potential, though methodological clarity and reproducibility are limited.\n\n---\n\n**Major Comments**  \n1. **Methodological Clarity:** The architecture and implementation details are insufficiently described. The model reportedly uses a ViT base with MAE pretraining on ImageNet-1K and a patch size of 16√ó16√ó32, but it remains unclear how 3D volumes are handled through a 2D ViT. Without code, this ambiguity limits reproducibility. A more transparent description of the full architecture and data flow is recommended.  \n2. **Comparison with Baselines:** The adaptation and training procedures used for 2D CLIP methods (Table‚ÄØ1) and CT-CLIP models (Table‚ÄØ3) are insufficiently explained. It is unclear how these 2D models were adjusted for 3D inputs or whether CT-CLIP was fine-tuned on in-house data before evaluation. These details are essential for fair comparison.  \n3. **Evaluation Reproducibility:** The study does not release its codebase, which poses challenges for verification and reuse. Availability of code or detailed pseudocode would enhance reproducibility.  \n4. **Preprocessing Clarification:** The HU clipping range (‚àí300‚ÄØto‚ÄØ400) may not encompass all relevant abnormalities (e.g., air at ‚àí1000, bone near 1000). Clarification is needed on whether this range was uniformly applied across all scan types, including contrast-enhanced variants, and whether it matches the setup used in CT-RATE experiments.  \n\n---\n\n**Minor Comments**  \n- To improve consistency of t-SNE visualizations (Fig.‚ÄØ7), a fixed random seed should be applied.  \n- The paper‚Äôs methodological exposition could be refined for greater clarity and accessibility.  \n\n---\n\n**Summary (TMI Criteria)**  \nThe work is significant in addressing false negative reduction within multimodal report generation and introduces a meaningful addition to the field. Innovation lies in integrating anatomy awareness, though novelty is somewhat constrained by limited architectural transparency. Evaluation results are encouraging but lack full interpretability due to incomplete methodological details. Reproducibility remains a concern given the absence of public code and insufficient procedural documentation.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The study demonstrates promise and relevance but requires clearer methodological explanations, better-documented comparisons, and enhanced reproducibility to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work discusses the fine-grained alignment issue for vision-language clip-like pretraining for CT and reports. To improve the granularity of alignment the authors propose to pre-segment key anatomical structures in CT using a public segmentation model (Totalsegmenter), and to pre-segment descriptions corresponding to individual anatomical structures in the free-text report using an open-access LLM (Qwen). This allows fine-grained contrastive learning at the anatomical structure level. Considerations are made to avoid pushing representations of the same organ/condition away. Improved performances are shown for zero-shot abnormality detection compared with several existing approaches.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe idea of increasing the granularity of contrastive learning by data pre-segmentation using public tools is neat, simple, and straightforward. \n\nThe paper is written with sufficient clarity, where the insights behind each design and the implementations details are well-presented. \n\nImproved performance on zero-shot abnormality detection is shown compared with some existing works.\n\n### Weaknesses\n\nFor a comprehensive assessment of report generation some essential scores are missing, such as BLEU 1-3, ROUGE-L, and METEOR. Also, comparisons with vision encoders from peer vision-language pre-training for CT works may be needed. This is my major concern as the authors have claimed improved performance on report generation task.\n\nDespite improved granularity, attributing sentences of reports `mentioning` a structure to the image feature of that structure may sometimes be misleading especially when manual verification/correction is not accessible. E.g., in practice pancreatitis is often associated with the entire abdomen instead of the pancreas alone and neighboring structures. The authors are encouraged to comment on this.\n\n### Questions\n\nA high-level question: The capability of open-source image segmentation tools (totalsegmenter and/or SAM family) and open-source LLM advance fiercely -- does it imply that the utility of (un-/weakly-supervised) vision-language pretraining for CT will gradually phase out? One may directly construct semantic labels from text reports using strong enough LLM tools and turn the problem back to large scale supervised learning. I am curious to hear the authors‚Äô insight.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nA separate ICLR ethics reviewer might not be needed. However, as the proposed study involves curating and analyzing human CT images and reports, the authors need to make sure that essential ethical approval is obtained, and pertinent privacy laws/regulations are complied with.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript addresses the fine-grained alignment issue in vision-language clip-like pretraining for CT and its corresponding reports. The authors propose an innovative approach to enhance granularity by pre-segmenting key anatomical structures in CT images using the public segmentation model Totalsegmenter, and pre-segmenting descriptions related to these structures in free-text reports with the open-access LLM Qwen. The study demonstrates enhanced performance in zero-shot abnormality detection compared to existing methods, showcasing both clarity in writing and the significance of its contributions.\n\n**Major Comments:**  \n1. Essential evaluation metrics for report generation such as BLEU 1-3, ROUGE-L, and METEOR are omitted, impacting the comprehensive assessment of the proposed method. Comparisons with vision encoders from other peer works should also be included for robustness, as the authors claim improved performance on the report generation task.\n2. While the granularity is improved, linking report sentences that mention a structure to the respective image feature may be misleading. Manual verification is often challenging; for instance, conditions like pancreatitis are associated more with broader regions than singular anatomical features. The authors are encouraged to discuss this limitation.\n\n**Minor Comments:**  \n- The paper presents its ideas clearly, with well-defined insights behind the design and implementation details.  \n- Improvement in zero-shot abnormality detection over existing works is commendable.\n\n**Summary Paragraph:**  \nThis manuscript presents a significant contribution to the field by enhancing the granularity of contrastive learning through effective data pre-segmentation. However, it lacks adequate evaluation metrics and comprehensive comparisons to substantiate claims about its contributions to report generation. Furthermore, attention should be given to the interpretation of report structures in relation to image features. Overall, the work is innovative but requires more extensive evaluation and discussion to strengthen its claims.\n\n**Decision Recommendation:**  \nMinor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the fine-grained alignment problem in vision‚Äìlanguage pretraining for CT images and corresponding reports. The authors propose a strategy that first segments anatomical structures in CT scans using a public segmentation model (TotalSegmentator) and then segments related descriptions in radiology reports using an open-access large language model (Qwen). This enables contrastive learning at the anatomical-structure level. The paper is clearly written, with transparent explanations of design choices and implementation details. Reported experiments indicate improved zero-shot abnormality detection performance compared with several existing methods.\n\n---\n\n**Major Comments**  \n1. **Evaluation metrics for report generation:** The study lacks key quantitative metrics for the report generation task (e.g., BLEU 1‚Äì3, ROUGE-L, METEOR). These are essential for a comprehensive assessment, especially since improved report generation performance is claimed.  \n2. **Comparative experiments:** Additional comparisons are needed with vision encoders from other CT-specific vision‚Äìlanguage pretraining studies to substantiate the generality and competitiveness of the proposed model.  \n3. **Attribution accuracy:** Although finer granularity is beneficial, automatically linking report sentences to image features of the mentioned structure could be misleading without manual verification. For instance, pancreatitis may involve neighboring abdominal structures beyond the pancreas itself. Clarification or discussion of such attribution limitations would strengthen the paper.  \n4. **Conceptual question on future directions:** The reviewer raises a high-level question: given rapid improvements in open-source segmentation and LLM tools, will un-/weakly-supervised vision‚Äìlanguage pretraining for CT gradually lose relevance as direct large-scale supervised learning becomes feasible? The authors‚Äô perspective on this would be valuable.\n\n---\n\n**Minor Comments**  \n- The paper is generally clear and well-organized.  \n- The reviewer notes no specific typographical or formatting issues.  \n- Ethical compliance should be ensured given the use of human CT data and reports.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The work addresses a relevant challenge in fine-grained CT-text alignment for medical imaging.  \n**Innovation:** The approach of using public segmentation models and open LLMs for pre-segmentation is simple yet conceptually interesting.  \n**Evaluation:** Experimental validation is focused on abnormality detection but lacks essential report-generation metrics and broader comparative studies.  \n**Reproducibility:** The use of open tools supports reproducibility, though details on data use and labeling could be clarified. Overall, the contribution is promising but requires stronger evaluation and discussion of limitations.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to‚ÄØTMI** ‚Äì  \nThe paper primarily contributes a *methodological innovation* in medical imaging AI: a new **fine‚Äëgrained vision‚Äìlanguage pre‚Äëtraining strategy (fVLM)** for CT image understanding. It tackles anatomical‚Äëlevel image‚Äìreport alignment and advances multimodal learning beyond global CLIP‚Äëstyle pairing. This fits well within‚ÄØ*TMI*‚Äôs scope on imaging methodology (image reconstruction, representation learning, quantitative analysis, and AI interpretability).\n\n2. **Novelty & Contribution Level** ‚Äì  \nThe work extends foundations of medical vision‚Äìlanguage modeling with (i) explicit anatomy‚Äëlevel alignment rather than implicit attention, and (ii) a dual false‚Äënegative reduction and co‚Äëteaching scheme. Given the novelty of anatomical parsing and disease‚Äëaware contrastive objectives for 3‚ÄëD CT data, the paper exceeds incremental improvement. However, its conceptual originality lies mainly in adapting and integrating known components (segmentation‚Äëbased locality, CLIP‚Äëstyle loss, co‚Äëteaching) rather than entirely new theory. It demonstrates strong empirical advancement.\n\n3. **Technical & Experimental Rigor** ‚Äì  \nThe study uses a very large in‚Äëhouse dataset (69‚ÄØk‚ÄØpatients,‚ÄØ272‚ÄØk‚ÄØCTs) with report parsing via LLMs and external validation on CT‚ÄëRATE and‚ÄØRad‚ÄëChestCT. Ablation, scaling, and visualization analyses are sound. Details of preprocessing, annotation, and statistical significance testing are only partly shown in appendices; clarity on IRB approval and specific error analysis could be improved.  \n\n4. **Clarity and Presentation** ‚Äì  \nThe manuscript is clearly written, with logical structure, extensive comparisons, and readable figures/tables. Minor language polishing would help (grammar, figure referencing, shorthand consistency).  \n\n5. **Ethical and Reproducibility Compliance** ‚Äì  \nDataset creation claims anonymization but does not explicitly mention institutional ethics approval or data‚Äësharing policy for‚ÄØMedVL‚ÄëCT69K. Code availability is stated via GitHub. The use of LLMs (Qwen‚ÄØ2.5) for report parsing should confirm that no protected health information was exposed externally.\n\n---\n\n**Phase‚ÄØ2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \nThis paper introduces **fVLM**, a fine‚Äëgrained vision‚Äìlanguage pre‚Äëtraining framework for 3‚ÄëD CT image interpretation. It segments CT volumes into 36 grouped anatomies, decomposes clinical reports into corresponding anatomical sentences using LLM‚Äëdriven parsing, and performs per‚Äëanatomy contrastive learning. To mitigate false negatives in contrastive objectives, the authors design a *dual false‚Äënegative correction* that accounts for normal‚Äìnormal and disease‚Äëconsistent samples, implemented via a co‚Äëteaching scheme between two models. Evaluations on a newly built large‚Äëscale dataset (MedVL‚ÄëCT69K) and public benchmarks show improved zero‚Äëshot and report‚Äëgeneration performance over CLIP and prior medical VLMs.\n\n2. **Strengths**  \n- Ambitious and timely topic: large‚Äëscale, multimodal CT foundation modeling.  \n- Methodologically more granular than existing VLMs, improving interpretability.  \n- Comprehensive experiments, including ablations, scaling analyses, and multi‚Äëdataset validation.  \n- Demonstrates credible quantitative gains (‚âà‚ÄØ8‚Äì13‚ÄØAUC‚ÄØpoints) and qualitative improvements in attention localization.  \n- Public code release enhances reproducibility.\n\n3. **Weaknesses**  \n- Novelty is somewhat incremental (integration of established ideas) rather than theoretical breakthrough.  \n- Heavy reliance on an internal dataset; lack of detailed description of patient cohort, scanner diversity, and ethical oversight.  \n- The dual false‚Äënegative and co‚Äëteaching strategy needs stronger mathematical or empirical justification (hyperparameter sensitivity, convergence).  \n- Report decomposition via LLMs may introduce unquantified errors; no inter‚Äëannotator or parsing accuracy analysis reported.  \n- Limited discussion of computational cost and inference efficiency for clinical deployment.\n\n4. **Major Comments**  \n1. **Dataset Ethics and Generalizability:**‚ÄØProvide explicit approval or de‚Äëidentification description for‚ÄØMedVL‚ÄëCT69K and clarify whether the data can be publicly released or partially shared.  \n2. **Quantitative Robustness:**‚ÄØAdd confidence intervals or statistical tests for improvements (AUC‚ÄØgains) across tasks and anatomies.  \n3. **LLM‚ÄëBased Report Parsing:**‚ÄØQuantify parsing accuracy against a radiologist‚Äëverified subset; discuss robustness to differing report templates.  \n4. **False‚ÄëNegative Reduction:**‚ÄØOffer an intuition or analytic analysis of how the label‚Äëcorrection weighting (Œ±‚ÄØ=‚ÄØ0.5) affects training stability; consider providing ablation over‚ÄØŒ±.  \n5. **Computational Efficiency:**‚ÄØReport GPU hours and inference speed, comparing to‚ÄØCT‚ÄëCLIP or‚ÄØMerlin, as this informs clinical scalability.  \n6. **Visualization/Interpretability:**‚ÄØInclude qualitative examples illustrating where fVLM improves localization relative to‚ÄØCLIP in 3‚ÄëD context.\n\n5. **Minor Comments**  \n- Check typographical inconsistencies (e.g., ‚ÄúÔ¨Åndings/impression,‚Äù ‚ÄúSpec/Sens‚Äù).  \n- Ensure figure legends are self‚Äëcontained.  \n- Reference numbering for certain works (e.g., duplicate Lin‚ÄØ2023) should be verified.  \n- Clarify abbreviations in tables (GREEN, CoT).  \n- Provide full specification of training hyperparameters (batch size, learning rate).  \n\n6. **Overall Recommendation** ‚Äì **Major‚ÄØRevision**\n\n*Justification:*‚ÄØThe study demonstrates clear methodological and experimental promise highly relevant to‚ÄØ*TMI*, but key clarifications on ethical compliance, dataset transparency, and deeper analysis of the proposed modules are necessary before publication.\n\n7. **Confidence Level** ‚Äì‚ÄØ**4‚ÄØ/‚ÄØ5** (High confidence in technical assessment; limited access to supplementary ethics details)\n\n---\n\n**Guidance to the Authors:**  \nTo elevate the paper to‚ÄØ*TMI* standards, emphasize methodological insight‚Äîformalize the false‚Äënegative correction, validate report parsing quantitatively, and provide transparency on dataset governance. Strengthening theoretical motivation and reproducibility documentation would make fVLM a strong contribution within the medical imaging methodology community.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes fVLM, a fine-grained vision-language model for CT image understanding that addresses misalignment issues in existing CLIP-based approaches. The method performs anatomy-level alignment between CT scan regions and corresponding radiology report descriptions, rather than global image-report contrastive learning. The approach uses TotalSegmentator to parse anatomical structures (Section 3.1, Page 4) and employs LLMs to decompose reports into anatomy-specific descriptions (Figure 2, Page 4). To handle false negatives arising from abundant normal samples and similar pathologies, the authors introduce a dual false negative reduction strategy with co-teaching (Section 3.3, Pages 6-7). The method is evaluated on MedVL-CT69K, a curated dataset of 272,124 CT scans from 69,086 patients. Results show fVLM achieves 81.3% average AUC across 54 disease diagnosis tasks, outperforming CLIP by 12.9% and supervised methods by 8.0% (Table 1, Page 8). Additional evaluations on CT-RATE and Rad-ChestCT benchmarks demonstrate 7.4% and 4.8% absolute AUC improvements respectively (Table 2, Page 9).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - The similarity calculation in Equation 1 (Page 6) uses inconsistent notation where Nj represents \"number of structurally complete samples\" but the summation indices suggest batch-level operations, creating confusion about the actual computation scope\n  - The loss function in Equation 2 lacks clear definition of what constitutes \"structurally complete samples\" and how this affects gradient computation across anatomies with different sample counts\n  - The co-teaching formulation in Equation 3 (Page 7) sets Œ±=0.5 empirically without theoretical justification or sensitivity analysis, undermining the mathematical rigor of the approach\n\n‚Ä¢ **Insufficient experimental validation and evaluation gaps**\n  - The dataset split details show only 1,151 validation patients versus 64,476 training patients (Page 8), representing an unusually small validation set that may not provide reliable model selection and hyperparameter tuning\n  - The comparison with radiologists is relegated to \"Appendix A.3\" (Page 9) without presenting quantitative results in the main paper, limiting assessment of clinical relevance\n  - Missing ablation studies on key architectural choices such as the grouping of 104 anatomical regions into 36 categories (Section 3.1, Page 4), which directly impacts the core fine-grained alignment capability\n\n‚Ä¢ **Technical approach limitations and methodological concerns**\n  - The reliance on TotalSegmentator for anatomy parsing (Figure 2, Page 4) introduces potential error propagation from segmentation failures, but no analysis is provided on how segmentation quality affects downstream performance\n  - The report decomposition using LLMs with string-matching fallback (Section 3.1, Page 5) lacks quantitative evaluation of decomposition accuracy, which is critical since misaligned text-image pairs would undermine the entire training objective\n  - The false negative identification strategy assumes anatomies not mentioned in impression sections are normal (Page 7), but this heuristic may not hold across different reporting styles or institutions, potentially introducing systematic biases\n\n‚Ä¢ **Experimental design and reproducibility concerns**\n  - The co-teaching strategy employs \"different model initialization, data iteration sequences and augmentations\" (Page 8) for model diversity, but these implementation details are not specified, hindering reproducibility\n  - The MedVL-CT69K dataset curation process lacks details about data quality control, institutional diversity, or potential selection biases that could affect generalizability\n  - The zero-shot evaluation setup does not clearly specify how disease categories were defined or annotated, particularly for the 54 diseases across 15 anatomies mentioned in the abstract but not fully detailed until appendix references\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation clarity**\n  - Clearly define Nj in Equation 1 and specify whether it represents per-anatomy sample counts or batch-level indices, and provide explicit algorithmic descriptions for handling variable anatomy presence across samples\n  - Add theoretical analysis or empirical sensitivity study for the Œ±=0.5 choice in Equation 3, demonstrating robustness to this hyperparameter selection\n  - Include formal definitions of \"structurally complete samples\" and provide computational complexity analysis for the anatomy-specific contrastive learning framework\n\n‚Ä¢ **Strengthen experimental validation with comprehensive baselines**\n  - Increase validation set size or provide justification for the current split, and include cross-validation results to demonstrate model stability across different data partitions\n  - Present quantitative radiologist comparison results in the main paper with statistical significance testing and inter-rater agreement analysis\n  - Add comprehensive ablation studies on anatomical grouping strategies, comparing performance across different granularity levels from 104 regions to various grouping schemes\n\n‚Ä¢ **Address technical limitations with robust evaluation**\n  - Quantitatively evaluate TotalSegmentator segmentation quality on the dataset and analyze correlation between segmentation accuracy and final model performance\n  - Develop and report metrics for report decomposition quality, potentially using expert annotations on a subset to validate LLM-based anatomy extraction accuracy\n  - Test the impression-section heuristic across different institutions or report templates, and provide alternative strategies for normal case identification\n\n‚Ä¢ **Improve experimental design and ensure reproducibility**\n  - Provide complete implementation details for the co-teaching diversity strategies, including specific augmentation types, initialization schemes, and iteration order variations\n  - Include detailed dataset curation methodology with quality control measures, institutional sources, and potential bias analysis\n  - Clearly specify disease annotation protocols and provide inter-annotator agreement statistics for the 54 disease categories, with detailed breakdown by anatomy type",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces a fine-grained vision-language model (fVLM) designed for enhanced CT image understanding, specifically focusing on anatomy-level interpretation and diagnosis. The proposed method explicitly aligns anatomical regions of CT images with corresponding descriptions in radiology reports, performing contrastive pre-training for each anatomy individually. To address the challenge of false negatives, the authors propose a dual false negative reduction (FNR) approach. The model is evaluated on a large-scale in-house dataset (MedVL-CT69K) and two public benchmarks (CT-RATE and Rad-ChestCT), demonstrating superior performance compared to existing methods in zero-shot classification tasks. The manuscript is well-written, and the motivation for bridging fine-grained vision-language alignment is clearly articulated.\n\n###\n\n## Major Comments\n1. Novelty and Positioning:\n   The introduction of anatomy-level fine-grained alignment is innovative, but the manuscript needs to better position its contribution relative to existing literature. The authors should provide a more thorough discussion on how their method compares to recent advancements in fine-grained alignment techniques and contrastive learning for medical imaging. This will help clarify the novelty and the specific advancements over previous works.\n\n2. Evaluation Design:\n   The evaluation is primarily conducted on the in-house MedVL-CT69K dataset, which is extensive but proprietary. While the authors demonstrate impressive results, the reliance on a single proprietary dataset limits the generalizability of the findings. Including a broader range of datasets, especially ones with varying patient demographics and imaging protocols, would strengthen the validation of the proposed method.\n\n3. Comparisons:\n   The manuscript provides a comprehensive comparison with several existing methods, but it lacks a detailed discussion on how the proposed method compares to recent advancements in multi-modal pre-training for medical imaging. The authors should include comparisons with recent state-of-the-art models that incorporate domain-specific knowledge or use advanced architectures tailored for medical imaging.\n\n4. Reproducibility:\n   While the authors mention that code will be made available, the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters used. Providing a more detailed methodology section, including pseudocode or a flowchart of the training pipeline, would significantly enhance the reproducibility of the work.\n\n###\n\n## Minor Comments\n1. Figures: Figures 3 and 4 are somewhat cluttered and could benefit from simplification and clearer labeling to improve readability.\n2. Notation: The notation introduced in Section 2.1 is inconsistent and could be clarified for better understanding.\n3. Acronyms: Several acronyms (e.g., \"CT-CLIP\") are used without definition, which can confuse readers.\n4. Typographical Issues: There are minor typographical errors throughout the manuscript (e.g., \"k-spacce\", \"undersampling maskes\").\n\n###\n\n## Summary Paragraph\n(focusing on 4 TMI Editorial Criteria)\nThe manuscript addresses a significant challenge in medical image interpretation by proposing a fine-grained vision-language model (fVLM) for CT image understanding. The method's technical innovation lies in its explicit alignment of anatomical regions with corresponding report descriptions, which is a promising direction in the field. However, the evaluation primarily relies on a single proprietary dataset, which limits the generalizability of the findings. The manuscript's reproducibility is compromised by incomplete descriptions of the training protocols and preprocessing steps. While the proposed method shows substantial potential, the current evidence falls short of fully meeting the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis to include recent state-of-the-art methods, broaden the evaluation to include additional datasets, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Le Lu",
      "Lin Yang",
      "Ling Zhang",
      "Qi Zhang",
      "Ruizhe Guo",
      "Sinuo Wang",
      "Tingbo Liang",
      "Weiwei Cao",
      "Xianghua Ye",
      "Zhongyi Shui",
      "Jianpeng Zhang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2d4fcaeb7d709760b3ac4de319e56760ca9ed11b.pdf",
    "remote_url": "https://openreview.net/pdf/2d4fcaeb7d709760b3ac4de319e56760ca9ed11b.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Vision-language model;  zero-shot enhancement; Structural Representation"
    ],
    "abstract": "Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as prompts for the target disease name during the inference phase.  \nHowever, these methods typically treat prompts as equivalent context to the target name, making it difficult to assign specific disease knowledge based on visual information, leading to a coarse alignment between images and target descriptions. In this paper, we propose StructuralGLIP, which introduces an auxiliary branch to encode prompts into a latent knowledge bank layer-by-layer, enabling more context-aware and fine-grained alignment. Specifically, in each layer, we select highly similar features from both the image representation and the knowledge bank, forming structural representations that capture nuanced relationships between image patches and target descriptions. These features are then fused across modalities to further enhance detection performance.\nExtensive experiments demonstrate that StructuralGLIP achieves a +4.1\\% AP improvement over prior state-of-the-art methods across seven zero-shot medical detection benchmarks, and consistently improves fine-tuned models by +3.2\\% AP on endoscopy image datasets.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a zero-shot object detection model, StructrualGLIP, for medical image-based object detection tasks. The method uses a knowledge bank that stores encoded text prompts, which are later used to select and match relevant image features to form fine-grained structural representations to allow better alignment of image features with the prompt information, achieving accurate, context-aware detection.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. the paper demonstrates a comprehensive evaluation of four medical image modalities.\n2. the paper's representation and structure are mostly clear and easy to follow (although with some language and word choice issues, which will be discussed below).\n\n### Weaknesses\n\n1. The claimed novelty appears to be on the latent knowledge bank and its function to store encoded prompt tokens and later be used at each encoder layer as a vision token selector and vice versa. This is coined as a mutual selection process. The selected information is then merged into the original image and text encoder back at each layer. The entire process is different to a standard contextual prompt method (Fig. 1(a)) but it feels like a quite incremental difference which I don't see much novelty.\n2. The word choice of \"medical detection\" bugs me, in medical science detection refers to \"detecting diseases\" whereas here this is object detection in medical images so this may cause confusion to certain readers.\n3. Line 234 looks unfinished.\n4. Eq. 6&7, perhaps the Top-P/Q^{max} function can be simplified by using argmax/argsort function?\n5. L201/534: \"... fine-grained alignment between target descriptions and medical images\", L:256 \"forming fine-grained structural representations\", can the authors clarify what \"fine-grained\" refers to in those places?\n6. L294: \"...like BLIP Li et al. (2022a)\", is the VQA model BLIP or not? Have you considered other VQA models and would the performance of other VQA models fluctuate your detection results?\n7. L177: I don't entirely agree that zero-shot detection has a real-world clinical need as the clinicians I've encountered would not trust zero-shot settings, in the medical domain, accurate detection/segmentation/diagnosis is the the most important thing.\n\n### Questions\n\nPlease address Weaknesses #1,5,6.\n\n\n------------------\n04/12/2024:\nI thank the authors' detailed responses. Since the reviewer's post deadline has passed so I am writing my final comments here. \n\nMy conclusion is that I will retain my original score. I believe the paper should improve clarity as many new arguments and technical details surfaced during the discussion period which should have been included in the main manuscript. The novelty and settings should be explored in more depth.\n\nMy comments are:\n\n1. Novelty. \n\nI agree with the authors' argument the image and language encoders do not need to be fine-tuned in your zero-shot detection setting. This I already mentioned in my last comment: \"I can understand if the auxiliary branch's language encoder is untrained\". My question was whether the modules in Eq. 8 to 10 need training or not? Especially the MHA module in Eq. 8, GLIP has the X-MHA module, did the author reuse the X-MHA weights for the MHA without retraining? The authors mentioned in L263 \"we employ a multi-head attention (MHA) mechanism\" which reads like it is a new module by the authors' own design. Given an MHA module (as far as I know) should have learnable weights but the authors claim the zero-shot detection setting is training-free, then I find the logic here is contradicted. Furthermore, the authors quoted L210 to demonstrate the training-free paradigm, but that is immediately after Eq.4 and the sentence only describes the encoders. This is an imprecise response to my question so I'm not making a judgment here, but this does make me think the paper should improve clarity. \n\nThe above was not the main point of my original novelty concern, but a spin-off. My original point was that if the authors wanted to demonstrate the \"preventing/addressing domain shifting\" argument for the novelty, you should consider measuring the actual feature distribution difference between GLIP and StructureGLIP.  The authors revealed in the last comments that the domain referred to the features before RPN, and that is what I think you should demonstrate. To recap the authors' comments, the authors claimed the other methods would concatenate prompt so the feature to RPN becomes CLS token of [target_name, prompt] which causes the domain shift, whereas StructualGLIP did the prompts integration in early layers so StructuralGLIP only has CLS of [target_name] to the RPN, and that addresses the domain shift. Conceptually, maybe it is true but please provide empirical evidence. The final performance is a surrogate measure, which does not directly support your domain shift argument. Finally, StructrualGLIP also fuses additional information in the process, so the CLS token should exhibit some domain shift as you are trying to do domain adaptation, these two arguments also have contradictions to each other. \n\nRegarding the \"addressing noise prompt\" argument, I'd imagine the top-P/Q has a certain capability of limiting noise prompts as you only choose the top tokens, so even when you add more prompts, they won't be selected if the network was already trained to have higher attention for the target classes tokens, but that on the other hand, make it trivial argument rather than a novelty.\n\n7. Zero-shot enhancement. After reading the authors' most recent comments, I strongly feel the setting should not be called zero-shot. When the target dataset's labelled data are already used to fine-tune your GLIP base model, then this is weakly supervised. In L175-176, the authors state \"We propose a zero-shot enhancement setting. This involves fine-tuning the model on medical datasets first, and then using prompts to further improve performance on unseen medical images\", initially I considered this as the model was trained on relevant datasets such as same/similar imaging modalities but different tasks or different datasets. However, if the target dataset itself was used for finetuning for training, then the \"further improve performance on unseen medical images\" does not hold.\n\nOther comments:\nThe experiments I requested were for the understanding of how much performance gain was attributable to the design/novelty of StructrualGLIP, i.e., the introduction of the auxiliary branch. The zero-shot enhancement setting (i.e., fine-tuned GLIP) has boosted the performance by a larger margin than the StructualGLIP can bring to GLIP. Also, the choice of the VQA can sometimes affect the AP/AP50 by a noticeable margin (i.e., that 75 AP) but it appears not stable. Then, at least for me, the viability of zero-shot medical detection unfortunately remains questionable.\n\nOnce again I appreciate the authors' effort to address my comments but I'm sorry I will retain my original score based on my thoughts above. The authors are encouraged to delve into the details of the novelty and concepts as well as improve clarity of the manuscript.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents StructrualGLIP, a zero-shot object detection model aimed at enhancing medical image-based object detection tasks through a knowledge bank of encoded text prompts. The authors claim that this approach allows for better alignment of image features with prompts, facilitating context-aware detection. Overall, the clarity and quality of the presentation are mixed, with clear structural organization but notable language issues.\n\n### Major Comments\n1. **Novelty**: The method's novelty relies on the use of a latent knowledge bank for prompt token selection. However, this incremental difference lacks substantial innovation over standard contextual prompt methods, which diminishes its perceived uniqueness.\n2. **Terminology Clarity**: The term \"medical detection\" is misleading as it may imply detecting diseases rather than objects in medical images, potentially causing confusion among readers.\n3. **Technical Clarity**: Line 234 appears incomplete, requiring revision for coherence.\n4. **Equation Simplification**: Equations 6 and 7 might benefit from simplification by utilizing argmax/argsort functions.\n5. **Clarification Needed**: The use of \"fine-grained\" in lines 201/534 and 256 needs clarification regarding what it specifically refers to in context.\n6. **VQA Model Context**: The mention of BLIP as a VQA model raises questions regarding its inclusion. Have the authors considered performance fluctuations with other VQA models?\n7. **Clinical Applicability**: The assertion that zero-shot detection meets real-world clinical needs is questionable; clinicians may not trust zero-shot settings, given that accuracy is paramount in medical diagnosis.\n\n### Minor Comments\n- The manuscript contains several language choices that require revision for clarity.\n- The notation of certain terms and acronyms could use standardization to enhance understanding.\n- There are minor typographical errors that should be corrected for professionalism.\n\n### Summary Paragraph\nIn evaluating the manuscript against the TMI criteria, its significance is diminished by concerns around novelty, which appears incremental rather than groundbreaking. The evaluation design is complex but lacks empirical evidence supporting claims of addressing domain shift. Reproducibility is questionable due to potential misunderstandings around zero-shot settings, and the clarity of the manuscript needs improvement to effectively communicate its contributions.\n\n### Decision Recommendation\nMajor revision is necessary to address clarity issues, refine the novelty claims, and provide more comprehensive evaluations and empirical support related to the methods proposed.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a zero-shot object detection model tailored for medical image analysis. The approach introduces a ‚Äúknowledge bank‚Äù storing encoded text prompts that interact with image features to generate fine-grained structural representations for improved image‚Äìprompt alignment. The manuscript is generally clear and well organized, with comprehensive experiments across four medical imaging modalities. Nevertheless, several aspects of novelty, clarity, and the definition of the claimed ‚Äúzero-shot‚Äù setting require further refinement.\n\n---\n\n**Major Comments**  \n1. **Novelty and Contribution** ‚Äì The proposed latent knowledge bank and mutual selection mechanism appear conceptually similar to existing contextual prompt methods, rendering the contribution incremental rather than fundamentally novel. The claimed innovations (e.g., domain-shift mitigation and noise-prompt suppression) are not convincingly supported by empirical evidence. The reviewer encourages quantitative analysis of feature distribution differences between GLIP and StructuralGLIP to substantiate the domain shift claim.  \n2. **Training-Free Logic** ‚Äì Clarification is needed regarding whether modules in Eqs.‚ÄØ8‚Äì10, particularly the Multi-Head Attention (MHA), possess learnable parameters or reuse pretrained weights. This point appears inconsistent with the claim of a training-free zero-shot paradigm.  \n3. **Definition of Zero-Shot Enhancement** ‚Äì The so-called ‚Äúzero-shot enhancement‚Äù involves fine-tuning on the target dataset, which contradicts the zero-shot premise and aligns more closely with a weakly supervised setting. This requires clearer articulation.  \n4. **Evaluation and Comparisons** ‚Äì The experiments show performance gains, yet the magnitude of improvement due to the new architecture versus fine-tuning remains unclear. Variability introduced by the choice of VQA model also affects stability, questioning reliability.  \n5. **Conceptual Clarity** ‚Äì Several statements use vague terminology, such as ‚Äúfine-grained alignment.‚Äù The meaning of such terms should be explicitly defined. Additionally, some lines (e.g., L234) appear incomplete or unclear.  \n\n---\n\n**Minor Comments**  \n- The phrase ‚Äúmedical detection‚Äù could be confusing; ‚Äúobject detection in medical images‚Äù would be clearer.  \n- Typographical and stylistic issues appear in line references and equations (e.g., simplification of Top‚ÄëP/Q^{max} using argmax/argsort).  \n- Figures and equation references‚Äîespecially Fig.‚ÄØ1(a)‚Äîshould be checked for consistency.  \n- Clarify whether the model compared to BLIP refers specifically to the VQA model version used.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the problem of medical zero-shot detection is relevant, but the proposed novelty remains modest. Regarding **innovation**, the introduction of a knowledge bank and mutual selection process is potentially interesting yet not convincingly distinct from prior methods. The **evaluation** is broad across modalities but leaves unresolved questions about the attribution of performance gains and stability of comparisons. For **reproducibility**, the paper is mostly transparent yet requires clearer explanation of training-free claims and fine-tuning protocols to ensure the results can be independently verified.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial clarifications on novelty, experimental evidence, and consistency of the zero-shot definition are needed before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduce a new zero-shot method for medical Vision-Language Models(VLMs) for detecting unknown targets. To address the prompt umatched with the variations in the medcial images, the authors propose a StructualGLIP desgin with a main and an auxiliary branch encoders for text input and introduce a mutual selcetion mechanism. \nThe author explain that the auxiliary branch would work as a knowledge bank where the main branch can extract latent prompt tokens, while the tokens in the knowledge banked are filtered by the mutual selection process.\nOverall, the motivation, method, and performance of this work is good enough, but I still need some explaination for some detail, please refer to the weakness and question section. I will consider adjust my rating based on the authors response.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. This work is aiming for a vital issue in medical image understanding field, which is the generalization capability of foundation models with limit data access. The motivation of improving the existing work is clear and strong, which is the lack of object level prompt and fail to capture the various feature of images during prompt desgin.\n2. This work present a novel but efficient method, called StructualGLIP, to increase the model's zero-shot/few-shot performance on various datasets. The desgin of StructualGLIP introduce the knowledge bank and mutual selection process to help prompt design process. This method address several shortcomings of current method and is novela and effective.\n3. This method largely increased the zero-shot performance on different medical image datasets across different modalities.\n\n### Weaknesses\n\n1. Line 234, the sentence seems not finished. \n2. One of the major problem of the proposed method is not trainable as the Top-K selection operation is non-differentiable, while previous work is differentiable and thus finetuning would result in better performance. I would suggest include Reparameterization Trick for Gumbel-Softmax to improve your method. Though this work is good enough as a stand-alone method for zero-shot detection. But I see a potential to achieve better performance.\n\n### Questions\n\n1. How to evaluate the quality of the generated prompts by VQA method. As some previous work pointed out, the VLM without medical domain adaptation perform poorly on some medical datasets, especially for radiology datasets.\n2. For Prompt as Knowledge Bank ablation study seciton, I would like to see an experiment on the whether adding noisy knowledge (for example, knowledge for another target) would sharply downgrade the StructureGLIP performance. This experiment would the the robustness of the mutual-selection process.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nNA",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a new zero-shot method for medical Vision-Language Models (VLMs), aimed at detecting unknown targets in medical imagery. The authors introduce a StructuralGLIP design featuring main and auxiliary branch encoders tailored for text input, as well as a mutual selection mechanism. While the motivation and approach are commendable, further clarification on specific details is needed for complete understanding.\n\n**Major Comments:**\n1. The study addresses a significant challenge in the medical image understanding field: the generalization capabilities of foundational models with limited data access. The motivation for improving upon existing methods is clear, particularly regarding the lack of object-level prompts and the failure to capture diverse image features during prompt design.\n2. The proposed StructuralGLIP method is both novel and efficient, enhancing zero-shot and few-shot performance across various datasets through the incorporation of a knowledge bank and mutual selection process, addressing several limitations of current approaches.\n3. The method demonstrably improves zero-shot performance across different medical image datasets and modalities.\n\n**Weaknesses:**\n1. Line 234 contains an incomplete sentence.\n2. A significant drawback of the proposed method is its non-trainability, as the Top-K selection operation is non-differentiable. Previous differentiable approaches may yield better performance through fine-tuning. Including a Reparameterization Trick for Gumbel-Softmax could potentially enhance the method; however, it is adequate as a stand-alone solution for zero-shot detection.\n\n**Questions:**\n1. How is the quality of the generated prompts evaluated using the VQA method, given that prior studies indicate VLMs without specific medical domain adaptation can underperform on medical datasets, especially in radiology?\n2. In the ablation study for the Prompt as Knowledge Bank, an experiment assessing the impact of integrating noisy knowledge (e.g., knowledge for differing targets) on StructuralGLIP performance is recommended to evaluate the robustness of the mutual-selection process.\n\n**Summary Paragraph:**\nThe manuscript addresses relevance, innovation, and evaluation by proposing a new approach to enhance medical VLMs under zero-shot conditions, showing promise in advancing the field. However, concerns regarding non-trainability and prompt evaluation methods need to be addressed. With improvements in these areas, the study would significantly contribute to the reproducibility and practical application of VLMs in medical imaging.\n\n**Decision Recommendation:**\nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a zero-shot framework, *StructuralGLIP*, aimed at improving medical vision-language models (VLMs) for detecting previously unseen targets. The approach employs a dual-branch encoder structure‚Äîmain and auxiliary branches‚Äîfor text prompts, incorporating a mutual selection mechanism. The auxiliary branch functions as a knowledge bank from which the main branch extracts latent prompt tokens filtered via this mutual selection. The study addresses the challenge of mismatched textual prompts and image variations in medical data. Overall, the paper presents a clearly motivated and reasonably explained contribution, though several clarifications are required to strengthen its presentation and technical soundness.  \n\n**Major Comments**  \n1. **Trainability Limitation:** The proposed method‚Äôs Top-K selection operation is non-differentiable, which prevents fine-tuning. In contrast, comparable methods are trainable and tend to achieve better performance with supervised adaptation. The reviewer suggests investigating a reparameterization strategy such as the Gumbel-Softmax trick to mitigate this limitation.  \n2. **Evaluation of Prompt Quality:** It remains unclear how the authors assess the quality of the generated prompts. Given prior evidence that general-purpose VLMs perform suboptimally on certain medical datasets, a clearer evaluation strategy‚Äîparticularly for radiology‚Äîis needed.  \n3. **Ablation Robustness Test:** In the ‚ÄúPrompt as Knowledge Bank‚Äù ablation, an additional experiment could explore robustness by introducing noisy or unrelated knowledge into the bank to assess how sensitively performance degrades.  \n4. **Minor Incomplete Content:** The sentence at Line 234 appears unfinished and should be revised for completeness.  \n\n**Minor Comments**  \n- Several typos and grammatical errors appear (e.g., ‚ÄúStructualGLIP,‚Äù ‚Äúselcetion,‚Äù ‚Äúmedcial‚Äù); these should be corrected.  \n- The authors may wish to improve explanation clarity for the mutual selection mechanism and the auxiliary branch functioning.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript addresses an important issue‚Äîthe generalization of medical VLMs with limited data‚Äîand introduces an innovative architectural concept combining structural prompt design and knowledge retrieval. It demonstrates consistent performance improvements across multiple datasets, indicating potential significance. However, the lack of trainability and limited evaluation details slightly weaken the methodological rigor and reproducibility. With clarification of technical details and further robustness evaluations, the work could form a solid contribution to the field.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper is promising and reasonably innovative, but improvements are needed in methodological clarity, evaluation completeness, and wording accuracy before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors introduce StructuralGLIP, a novel approach to zero-shot medical detection using vision-language models (VLMs). This method leverages structured representations within a dual-branch architecture that enables nuanced alignment between images and textual prompts, significantly enhancing the model's adaptability to new medical scenarios without needing annotated data. StructuralGLIP uses category-level prompts, maintained in a latent knowledge bank, and a mutual selection mechanism for precise cross-modal fusion, thus improving accuracy across diverse medical imaging datasets.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n(1)\tThe paper introduces an effective structural representation by encoding prompts into a knowledge bank and utilizing a dual-branch structure. This approach enables adaptive and context-aware alignment, which is particularly advantageous for complex medical detection tasks.\n(2)\tStructuralGLIP outperforms traditional zero-shot models by effectively handling both instance-level and category-level prompts, achieving significant improvements across various benchmarks in endoscopy, microscopy, radiology, and more.\n(3)\tBy allowing for zero-shot enhancement, the model can be fine-tuned and then further improved with category-level prompts, a feature well-suited for dynamic medical settings where data annotation is scarce.\n\n### Weaknesses\n\n(1)\tThe proposed dual-branch structure with a knowledge bank requires complex engineering and computational resources, potentially limiting its accessibility for practitioners in less resource-rich environments.\n(2)\tThe paper may not adequately address the potential data imbalance present in the datasets used for evaluation. Some diseases or conditions may have significantly fewer examples, which could impact the model's performance and generalizability.\n(3)\tThe model's inner workings, particularly regarding how it selects and utilizes prompts, may be difficult for practitioners to interpret, limiting trust in its decisions and making it harder to diagnose potential failures.\n(4)\tDespite improvements in alignment, there may still be instances of misalignment between visual features and prompts, especially in cases of atypical presentations, which could lead to missed detections.\n\n### Questions\n\n(1)\tTo what extent can the findings be generalized to other medical imaging modalities or less common diseases? Are there plans to evaluate the model on broader datasets?\n(2)\tBesides Average Precision, what other metrics were considered for evaluating model performance? Are there plans to incorporate user feedback or clinical outcomes in future evaluations?\n(3)\tThis paper focuses on zero-shot medical detection, whereas GLIP was initially developed for natural images. Can the proposed method also be applied effectively to natural image datasets?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents StructuralGLIP, an innovative methodology for zero-shot medical detection utilizing vision-language models (VLMs). The authors propose a dual-branch architecture that effectively aligns images with textual prompts, enhancing adaptability to new medical scenarios without the need for annotated data. The clarity and overall quality of the presentation could benefit from improvements, as noted in the evaluation.\n\n### Major Comments\n1. The introduction of a structural representation through a latent knowledge bank and dual-branch architecture demonstrates adaptability and context-awareness, beneficial for complex medical tasks.\n2. StructuralGLIP yields superior performance compared to traditional zero-shot models, adeptly managing both instance-level and category-level prompts, with notable advancements in various medical imaging benchmarks including endoscopy, microscopy, and radiology.\n3. The model's ability to incorporate zero-shot enhancement is advantageous for rapidly evolving medical environments with limited annotated data.\n4. The dual-branch structure and knowledge bank methodology may impose significant engineering demands and computational resources, potentially limiting accessibility in resource-constrained healthcare settings.\n5. The evaluation may not comprehensively address the data imbalance inherent in the utilized datasets, potentially affecting the model's performance and generalizability for less represented diseases.\n6. The inner workings of the model, particularly prompt selection and usage, lack clarity, risking trust and understandability for practitioners and complicating the diagnosis of failures.\n7. Despite advancements in alignment, misalignment issues might still arise, particularly in atypical cases, potentially resulting in missed detections.\n\n### Minor Comments\n- Clarification on the methodology and rationale for performance metrics beyond Average Precision would enhance the paper's robustness.\n- The writing could be refined for clearer communication, specifically in complex sections that may confuse readers.\n\n### Summary Paragraph\nThis study offers a significant contribution to zero-shot medical detection through innovation in methodology and evaluation. While the proposed system shows strength in adaptability and effectiveness, it raises concerns regarding accessibility, data imbalance, and interpretability. These weaknesses impact the overall reproducibility and practical applicability, suggesting a need for further refinement and evaluation across diverse datasets and medical conditions.\n\n### Decision Recommendation\nMinor revision is recommended to address the identified weaknesses and enhance clarity, ultimately increasing the paper's impact and applicability in clinical settings.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a new vision‚Äìlanguage model designed for zero-shot medical detection. It introduces a dual-branch architecture incorporating structured prompts through a knowledge bank and mutual selection mechanism to enhance cross-modal alignment between medical images and textual descriptions. The approach aims to improve adaptability to unseen medical scenarios without requiring additional annotation. The paper is generally clear and technically sound, though parts of the explanation could be more accessible.\n\n---\n\n**Major Comments**  \n1. **Complexity and Resource Requirements:** The dual-branch design and maintenance of a structured knowledge bank appear computationally demanding, which may hinder adoption in resource-limited environments. The authors should comment on efficiency optimization or model scalability.  \n2. **Dataset Imbalance and Generalizability:** The evaluation datasets may contain class imbalance across disease types, potentially influencing model generalization. Further clarification or analysis regarding dataset composition and its impact on performance would strengthen the work.  \n3. **Interpretability of the Prompt Mechanism:** The internal logic governing prompt selection and utilization is insufficiently explained. Greater transparency or visualization of this process would improve user trust and facilitate error diagnosis.  \n4. **Residual Misalignment:** Despite improvements in alignment, occasional mismatches between textual prompts and visual features may persist, particularly for atypical or visually subtle cases. Discussion of mitigation strategies is encouraged.  \n5. **Scope of Evaluation:** It is unclear whether the approach can extend effectively to other modalities or even non-medical images. Evaluation on broader datasets would help establish general applicability.  \n6. **Evaluation Metrics:** Only average precision metrics appear discussed; consideration of complementary metrics (e.g., sensitivity, specificity) or downstream clinical implications could provide a more comprehensive performance assessment.\n\n---\n\n**Minor Comments**  \n- Some sections could benefit from refined figures or examples to clarify the dual-branch mechanism.  \n- Ensure consistent use of acronyms and technical terms throughout the paper.  \n- A few textual explanations of prompt fusion are overly condensed and would benefit from clearer stepwise presentation.\n\n---\n\n**Summary Paragraph (TMI Editorial Criteria)**  \nIn terms of **significance**, the method addresses an important problem‚Äîreducing annotation needs in medical detection. Regarding **innovation**, the structural prompt mechanism represents a meaningful extension beyond conventional vision‚Äìlanguage pairing. However, the paper‚Äôs **evaluation** could be broadened to account for dataset balance and additional metrics, and the **reproducibility** of the model would be improved by clearer disclosure of computational resources, parameter tuning, and internal prompt logic. Overall, the contribution is promising and well-motivated but requires additional clarification and validation.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1‚ÄØ‚Äì Analytical and‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI**  \n‚úÖ‚ÄØThe work proposes **StructuralGLIP**, a novel vision‚Äìlanguage framework for *zero‚Äëshot medical detection*.  Its main innovation‚Äîa dual‚Äëbranch ‚Äúknowledge‚Äëbank‚Äù architecture with layer‚Äëwise mutual selection between image and prompt features‚Äîconstitutes a **methodological advancement** in multimodal learning and object detection for medical imaging.  The manuscript fits well within the IEEE‚ÄØTMI scope (image reconstruction, learning‚Äëbased detection, quantitative analysis).\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel**  \nThe idea of embedding prompts into a hierarchical latent memory and dynamically selecting sub‚Äëtokens for cross‚Äëmodal fusion goes beyond prior GLIP or prompt‚Äëpool methods.  Extending ‚Äúzero‚Äëshot‚Äù to a *zero‚Äëshot‚Äëenhancement* setting for fine‚Äëtuned detectors is also new.  Novelty is therefore moderate‚Äëto‚Äëhigh for TMI standards.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \nThe theoretical description is detailed and mathematically consistent.  Experiments cover eight heterogeneous imaging datasets with multiple baselines, including ablations on hyperparameters P/Q, fusion‚Äëlayer choice, prompt noise, and LLM type.  However, quantitative significance tests and runtime/complexity analyses are missing, and reproducibility details (code, pretrained weights, IRB where needed) should be clarified.\n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation**  \nWriting quality is good overall, though the manuscript is lengthy and occasionally overloaded with notation.  Figures are informative but dense; captions could be more self‚Äëexplanatory.  Minor grammatical polishing and consistent notation would improve readability.\n\n**5.‚ÄØEthical‚ÄØand‚ÄØReproducibility‚ÄØCompliance**  \nDatasets are all public; human/animal subject issues are minimal.  The authors state that code will be released, but this should be verified before publication.\n\n---\n\n**Phase‚ÄØ2‚ÄØ‚Äì IEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n**1.‚ÄØSummary**  \nThe paper introduces **StructuralGLIP**, a dual‚Äëbranch vision‚Äìlanguage model for zero‚Äëshot medical object detection.  A main branch processes image and target names, while an auxiliary branch encodes textual prompts into a *latent knowledge bank*.  A mutual‚Äëselection mechanism retrieves the most relevant prompt and image tokens at each layer to form structural representations that enhance cross‚Äëmodal alignment.  The method supports both instance‚Äë and category‚Äëlevel prompts and extends to a ‚Äúzero‚Äëshot enhancement‚Äù mode that boosts performance of fine‚Äëtuned detectors without extra labels.  Experiments on endoscopic, microscopic, photographic, and radiologic datasets show consistent improvements over GLIP, MIU‚ÄëVL, AutoPrompter, and MPT baselines.\n\n**2.‚ÄØStrengths**\n- Clear methodological innovation in structural prompt encoding and layer‚Äëwise mutual selection.  \n- Comprehensive evaluation across multiple imaging modalities.  \n- Demonstrated ability to exploit category‚Äëlevel prompts efficiently‚Äîreducing reliance on image‚Äëspecific prompt generation.  \n- Zero‚Äëshot‚Äëenhancement scenario is practically relevant for continuous model improvement in clinical settings.  \n- Solid ablation and analysis sections demonstrating mechanism behavior.\n\n**3.‚ÄØWeaknesses**\n- Limited theoretical justification of why mutual selection converges or how Top‚ÄëP/Q are chosen; lacks complexity and timing analysis.  \n- Statistical significance or confidence intervals of the reported AP gains are not provided.  \n- Section organization is somewhat verbose; methodology and experiments could be condensed.  \n- No external clinical validation or reader study to demonstrate impact on real clinical workflows.  \n- Reproducibility statement should include dataset licensing confirm‚Äë pliance and actual code availability date.\n\n**4.‚ÄØMajor‚ÄØComments**\n1. **Quantitative significance**‚ÄØ‚Äì add statistical analysis (e.g., paired t‚Äëtest or bootstrap CIs) of AP improvements to substantiate claimed gains.  \n2. **Computational cost**‚ÄØ‚Äì report model parameters, GPU hours, and inference latency compared to GLIP; discuss scalability for clinical use.  \n3. **Clarify mutual‚Äëselection implementation**‚ÄØ‚Äì specify how Top‚ÄëP/Q are determined and whether gradients flow through selection.  \n4. **Reproducibility**‚ÄØ‚Äì include hyperparameter tables, seeds, and ensure public release of prompt lists and code upon acceptance.  \n5. **Generalizability/clinical translation**‚ÄØ‚Äì discuss how category‚Äëlevel prompts could be standardized or curated in clinical vocabularies (e.g.,‚ÄØSNOMED concepts) to facilitate deployment.\n\n**5.‚ÄØMinor‚ÄØComments**\n- Streamline mathematical notation; unify subscript conventions \\(i\\),‚ÄØ\\(l\\),‚ÄØ\\(v\\).  \n- Improve Figure‚ÄØ1 and‚ÄØ3 captions; label components clearly.  \n- Minor grammar issues (‚Äúfine‚Äëgrained a‚ÄØlignment‚Äù ‚Üí ‚Äúalignment‚Äù, ‚ÄúFinetuned‚Äù ‚Üí ‚ÄúFine‚Äëtuned‚Äù).  \n- Verify reference duplication (Li‚ÄØet‚ÄØal.,‚ÄØ2022‚ÄØb/c appear identical).  \n- Provide units or resolution info for datasets in Appendix‚ÄØC.  \n\n**6.‚ÄØOverall‚ÄØRecommendation**  \n**Major‚ÄØRevision.**  \nThe manuscript offers strong methodological novelty and promising results but needs additional quantitative analysis, presentation tightening, and reproducibility assurances to meet TMI publication standards.\n\n**7.‚ÄØConfidence‚ÄØLevel:**‚ÄØ4‚ÄØ/‚ÄØ5  \n(High confidence in evaluation of novelty and technical quality; moderate confidence regarding implementation‚Äëlevel details due to limited reproducibility information.)\n\n---\n\n**Summary Note to Editors:**  \nStructuralGLIP is scientifically interesting and aligns with the journal‚Äôs emphasis on fundamental methodological advances in imaging AI.  With more rigorous quantitative validation and improved clarity, the paper could become a solid contribution to zero‚Äëshot and prompt‚Äëdriven medical image detection research.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper proposes StructuralGLIP, a zero-shot medical detection framework that addresses coarse image-description alignment in existing grounded vision-language models. The method introduces a dual-branch architecture where prompts are encoded into a latent knowledge bank via an auxiliary branch, while the main branch processes target names and images. A mutual selection mechanism dynamically retrieves relevant visual and linguistic tokens at each layer to form structural representations. The approach supports both instance-level and category-level prompts, with category-level prompts remaining fixed across instances of the same class. Experiments across eight medical datasets (endoscopy, microscopy, photography, radiology) demonstrate superior performance in both zero-shot detection and zero-shot enhancement settings, achieving average improvements of +4.2% AP in zero-shot detection and enabling fine-tuned models to surpass fully supervised baselines.\n\n## Weaknesses\n\n**‚Ä¢ Mathematical Formulation Clarity and Consistency Issues**\n  - The mutual selection mechanism equations (6-7) use unclear notation where similarity calculations are embedded within Top-P/Top-Q operations without explicit definition of the similarity function (Section 3.2, Page 5). This obscures the computational process and reproducibility.\n  - The residual connections in Equation (9) lack mathematical justification for why additive combination is optimal compared to other fusion strategies, affecting theoretical soundness.\n  - Inconsistent notation usage where Ki_v and Ki_l represent both selected tokens and their associated values throughout Section 3.2, creating potential confusion in understanding the selection process.\n\n**‚Ä¢ Insufficient Experimental Validation and Baseline Comparisons**\n  - Limited comparison with recent vision-language medical detection methods beyond GLIP-based approaches, with no comparison to domain-specific medical detection frameworks (Tables 1, 4). This restricts assessment of the method's competitive positioning.\n  - Ablation studies focus primarily on hyperparameters P and Q (Table 5) but lack comprehensive analysis of architectural choices such as the dual-branch design versus alternative fusion strategies, limiting understanding of design necessity.\n  - The zero-shot enhancement setting evaluation lacks comparison with other prompt-based enhancement methods on fine-tuned models, making it difficult to assess the uniqueness of the approach (Section 4.3).\n\n**‚Ä¢ Methodological Limitations and Technical Concerns**\n  - The category-level prompt generation relies heavily on GPT-4 quality without systematic evaluation of prompt robustness across different medical domains (Section 3.3, Figure 3b). This creates potential generalization concerns for rare diseases or specialized imaging modalities.\n  - The mutual selection mechanism's computational overhead during inference is not quantified, particularly concerning the Top-P and Top-Q operations across multiple layers (Equations 6-7), which could impact clinical deployment feasibility.\n  - Limited analysis of failure cases or method limitations, with most qualitative results showing successful detections (Figures A, B, E) rather than systematic error analysis, reducing understanding of method boundaries.\n\n## Suggestions for Improvement\n\n**‚Ä¢ Enhance Mathematical Rigor and Notation Consistency**\n  - Explicitly define the similarity function used in Equations (6-7) with mathematical formulation (e.g., cosine similarity, dot product) and provide theoretical justification for the choice. Include algorithmic pseudocode for the Top-P/Top-Q selection process to improve reproducibility.\n  - Provide theoretical analysis or empirical validation for the additive residual connection design in Equation (9), comparing against alternative fusion strategies (concatenation, learned weighting) to justify the architectural choice.\n  - Establish consistent notation throughout Section 3.2, clearly distinguishing between token indices, selected tokens, and their representations, potentially using subscripts or different symbols to avoid confusion.\n\n**‚Ä¢ Expand Experimental Evaluation and Analysis**\n  - Include comparisons with recent domain-specific medical detection methods and non-GLIP-based vision-language approaches to establish comprehensive competitive analysis. Add comparison with other prompt-based enhancement methods on fine-tuned models.\n  - Conduct systematic ablation studies on architectural components including dual-branch versus single-branch designs, alternative fusion mechanisms, and different knowledge bank organizations to validate design necessity.\n  - Provide detailed failure case analysis, computational cost analysis including inference time and memory overhead, and systematic evaluation of prompt robustness across different medical imaging modalities and rare diseases.\n\n**‚Ä¢ Address Methodological Limitations and Practical Concerns**\n  - Develop systematic evaluation framework for prompt quality across different medical domains, including analysis of GPT-4 limitations for rare diseases and alternative prompt generation strategies for specialized medical contexts.\n  - Quantify computational overhead of the mutual selection mechanism and provide efficiency analysis comparing inference costs with baseline methods, including optimization strategies for clinical deployment.\n  - Include comprehensive error analysis with failure case studies, method limitation discussion, and systematic evaluation of robustness to noisy or incomplete prompts, providing clearer understanding of method applicability boundaries.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces StructuralGLIP, a novel framework for zero-shot medical detection that utilizes a dual-branch architecture to integrate category-level prompts into a latent knowledge bank. The authors argue that this approach enhances the alignment between image features and target descriptions, leading to improved detection performance. The proposed method is evaluated on eight datasets spanning various imaging conditions, demonstrating superior performance compared to existing methods. The manuscript is well-written and presents a compelling argument for the benefits of using category-level prompts in medical detection tasks.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: While StructuralGLIP introduces a promising approach for zero-shot medical detection, the manuscript does not sufficiently distinguish itself from existing works that also leverage vision-language models for medical detection. The authors should provide a clearer comparative analysis to highlight the unique contributions of StructuralGLIP over similar methods.\n\n2. Evaluation Design: The experiments are conducted on a diverse set of datasets, which is commendable. However, the evaluation is predominantly focused on zero-shot detection settings without extensive validation in zero-shot enhancement scenarios. Including more comprehensive evaluations under different conditions, such as prospective and multi-anatomy validation, would strengthen the claims of the method's generalizability.\n\n3. Comparisons: The baseline comparisons are thorough, but the manuscript could benefit from including more recent diffusion-based and transformer-based methods in the comparison to establish a clearer baseline for evaluating StructuralGLIP‚Äôs performance.\n\n4. Reproducibility: Although the authors state that the code will be made available, the manuscript lacks detailed descriptions of training protocols, preprocessing steps, and model hyperparameters. Providing these details is essential for ensuring the reproducibility of the results.\n\n###\n\n## Minor Comments\n1. Figures: Some figures are cluttered and could be improved by showing fewer representative slices with zoomed-in regions to enhance readability.\n   \n2. Notation and Terminology: The notation introduced in Section 2.1 is inconsistent and lacks sufficient explanation. Clarifying these points would aid in understanding the methodology.\n\n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition, which can be confusing for readers. Defining these acronyms would improve clarity.\n\n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" and \"undersampling maskes\" should be corrected.\n\n###\n\n## Summary Paragraph\n(Focusing on 4 TMI Editorial Criteria)\n\nSignificance: StructuralGLIP addresses a significant challenge in medical imaging by improving zero-shot detection capabilities without the need for annotated data. The manuscript demonstrates the potential for enhanced detection performance across various medical imaging modalities, which is clinically relevant.\n\nInnovation: The dual-branch architecture and the integration of category-level prompts into a latent knowledge bank represent innovative contributions to the field. These enhancements offer a promising direction for improving zero-shot detection in medical imaging.\n\nEvaluation: The evaluation is comprehensive but could be strengthened with additional comparisons to recent diffusion-based and transformer-based methods. Furthermore, broader validation across different imaging conditions would provide a more robust assessment of the method‚Äôs performance.\n\nReproducibility: The manuscript lacks detailed methodological descriptions necessary for reproducibility. Providing complete training protocols, preprocessing steps, and hyperparameter settings would greatly enhance the reproducibility of the results.\n\nOverall, while StructuralGLIP offers significant potential, the current manuscript requires additional work to fully meet the standards of rigor and reproducibility expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand comparative analysis, strengthen validation across datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Baochang Zhang",
      "Chunyu Xie",
      "Haoyu Huang",
      "Linlin Yang",
      "Tongfei Chen",
      "Xianbin Cao",
      "Yuguang Yang",
      "Dawei Leng"
    ],
    "url": "pdfs/iclr.cc-2025-conference_42ae230c934ff880d86ebd78533b2cdcd90fd1b8.pdf",
    "remote_url": "https://openreview.net/pdf/42ae230c934ff880d86ebd78533b2cdcd90fd1b8.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Multimodal Learning",
      "Medical Imaging"
    ],
    "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, medical imaging domains introduce two key challenges: dynamic modality fusion and modality-task dependence. The quality and amount of task-related information from different modalities could vary significantly across patient samples, due to biological and demographic factors. Traditional fusion methods apply fixed combination strategies that fail to capture this dynamic relationship, potentially underutilizing modalities that carry stronger diagnostic signals for specific patients. Additionally, different clinical tasks may require dynamic feature selection and combination from various modalities, a phenomenon we term ‚Äúmodality-task dependence.‚Äù To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions. We evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate our method's superiority over state-of-the-art methods under different metrics of classification and segmentation tasks like Accuracy, AUROC, AUPRC, and DICE.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a multi-modal, multi-task, mixture of experts for various medical diagnoses to address the challenges of sample-dynamic modality fusion (and modality-task dependence (selecting the right modalities for a task). Concretely, this is done by using a combination of modality-specific experts and experts shared between modalities and tasks. M4OE shows promising initial results in terms of both absolute performance and enforcing modality utilization.\n\nBased on the weaknesses and questions outlined my score indicates a rejection for now, but I generally like the motivation of the paper, especially the aspect on modality utilization. I am willing to increase my score if my concerns are addressed and questions clarified.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The M4OE is highly effective at enforcing modality utilization - this is a meaningful contribution that many multimodal models suffer from, although I do have some questions about this.\n- The overall performance of the model is outperforming the baseline, even if the results are missing crucial information to validate the statistical significance of the results.\n- Strong visuals that are additive to the understanding of the paper.\n- Good conceptual motivation of the paper, although I believe that the motivation would further benefit from some concrete examples of sample dynamism and clinical examples of tasks that are modality-dependent.\n\n### Weaknesses\n\n- Abstract: the one-liner for sample-dynamic modality fusion is unclear as the specific and shared information always varies per sample unless they are identical. To my knowledge, sample-dynamic spans a much wider field of problems like missingness, robustness to noise, which the manuscript does not consider.\n- Abstract: ‚ÄúResults demonstrate superiority over state-of-the-art methods‚Äù is extremely vague. Along which metric?\n- You claim an expansive space by saying that the method is ‚Äúmulti-modal multi-task‚Äù, but your experiments only look at multi-view settings of a single modality (images). I would encourage you to narrow the scope/claim of the paper as the paper does not consider heterogeneous modalities (images, text, tabular, etc.).\n- Experimental setup: I would encourage you to provide more detail in this section to aid reproducibility. For example, it is unclear whether cross-validation is used. No confidence intervals or standard deviation of results are reported to judge the statistical significance of the results. Additionally, no code was provided in the supplementary materials that would help with the clarification of the experimental setup.\n- Literature: missing out on the largest corpus of literature (intermediate fusion), which many latent variable models for multimodal fusion fall under, many of which are using a mix of modality-specific and shared spaces.\n\n### Questions\n\n- How do you determine which expert sees which task? The connection between Figure 1 and the method section is not very clear.\n- The manuscript talks a lot about sample adaptivity, but how does your experimental setup show that the model handles sample adaptivity effectively? Which aspects of sample adaptivity?\n- Figure 3c suggests that the modality utilization is forced towards the same mean in your method. What about cases where modality dominance/competition is good? For example, if I have one very noisy modality, wouldn‚Äôt it be desirable to have the modality that contains all the signal to get all the model‚Äôs attention? Isn‚Äôt this graph showing that we enforce equal utilisation of all modalities regardless of the signal? Additionally, does this finding not contradict your claim in Figure 1, which is that only some experts are used (as opposed to all experts with a more balanced contribution).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a multi-modal, multi-task mixture of experts (M4OE) framework aimed at improving medical diagnoses by addressing challenges associated with sample-dynamic modality fusion and task-dependent modality selection. The approach combines modality-specific experts with shared experts across modalities and tasks, demonstrating promising initial results in terms of performance and modality utilization. While I appreciate the paper's motivation, I currently recommend rejection based on identified weaknesses and concerns, but I am open to reconsidering if these issues are addressed satisfactorily.\n\n**Major Comments:**\n1. The abstract's description of sample-dynamic modality fusion is vague, as the nature of shared information varies with each sample. It overlooks broader issues such as missingness and robustness to noise.\n2. Assertion of superiority over state-of-the-art methods is unclear; specific metrics need to be provided for clarity.\n3. The claim of being \"multi-modal multi-task\" is misleading as experiments focus solely on multi-view settings of a single modality (images). Consider narrowing this claim to reflect the actual scope.\n4. The experimental setup lacks detail necessary for reproducibility. Information on whether cross-validation was used is missing, and statistical significance metrics (confidence intervals, standard deviations) are not reported. Additionally, no supplementary code has been provided.\n5. The literature review overlooks key works in intermediate fusion, particularly those involving latent variable models that integrate modality-specific and shared spaces.\n\n**Minor Comments:**\n- Provide concrete examples of sample dynamism and modality-dependent clinical tasks to strengthen the motivation.\n- Clarify the connection between Figure 1 and the expert task assignment in the methodology.\n- Address issues regarding how the model effectively captures sample adaptivity in the experimental setup.\n- Figure 3c raises concerns that modality utilization may lead to equal weighting of modalities; this should be examined further, particularly regarding cases of modality dominance or competition.\n\n**Summary Paragraph:**\nThe manuscript exhibits significance in its focus on modality utilization and initial performance improvements. However, it falls short in terms of innovation by not fully addressing the multi-modal aspects as claimed. The evaluation lacks thoroughness, significantly impacting reproducibility due to insufficient experimental details. Overall, while the model demonstrates potential, critical issues remain that hinder its contribution to the field.\n\n**Decision Recommendation:**\nReject.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *M4OE*, a multi-modal, multi-task mixture-of-experts model designed for medical diagnosis applications. The approach aims to address the problem of sample-dynamic modality fusion and task-modality dependence through a combination of modality-specific and shared experts. The paper presents promising initial performance results and meaningful contributions in enforcing modality utilization. The overall motivation and presentation are clear, though several conceptual, methodological, and experimental aspects require clarification and refinement before the work can be accepted.\n\n---\n\n**Major Comments**  \n1. **Scope and Claims:** The manuscript asserts a ‚Äúmulti-modal, multi-task‚Äù framework, yet the experiments are limited to multi-view imaging data. This narrow empirical scope does not support claims about handling heterogeneous modalities (e.g., images, text, tabular data). The claims should be narrowed or additional experiments added.  \n2. **Conceptual Clarity:** The definition of ‚Äúsample-dynamic modality fusion‚Äù in the abstract is ambiguous. The description does not align with the broader meaning of sample dynamics, which can include missing modalities and robustness to noise.  \n3. **Experimental Design and Reproducibility:** The evaluation lacks sufficient methodological details. It is unclear whether cross-validation is used, and there are no confidence intervals or standard deviations to assess statistical significance. The absence of code or supplementary materials further limits reproducibility.  \n4. **Evidence and Metrics:** The statement ‚Äúresults demonstrate superiority over state-of-the-art‚Äù is vague and should specify the metrics of comparison.  \n5. **Literature Coverage:** The work omits discussion of relevant literature in intermediate fusion, particularly latent-variable models combining modality-specific and shared spaces.  \n6. **Methodological Clarifications:** The relationship between Figure‚ÄØ1 and the description of expert-task assignments is unclear. Additional explanation is needed about how experts are matched to tasks and how sample adaptivity is demonstrated experimentally.  \n7. **Analysis of Modality Utilization:** Figure‚ÄØ3c suggests equalized modality contributions, which may not always be desirable if certain modalities are noisy. The authors should clarify how the method handles modality dominance and whether this behavior contradicts the claim of selective expert usage.\n\n---\n\n**Minor Comments**  \n- The abstract could be refined for clarity and precision.  \n- The manuscript would benefit from more concrete clinical examples illustrating sample dynamism and task-specific modality dependence.  \n- Figures are visually strong and aid understanding but require clearer correspondence with the text.  \n- Typographical and stylistic polishing is recommended.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n- **Significance:** The topic of dynamic multimodal fusion is important and highly relevant to medical AI.  \n- **Innovation:** The proposed mixture-of-experts formulation is conceptually interesting, particularly regarding modality utilization.  \n- **Evaluation:** Experimental validation is incomplete, lacking statistical rigor and sufficient evidence to support the claimed superiority.  \n- **Reproducibility:** Missing methodological details and absent code limit reproducibility. With clarified methods and expanded experiments, the paper‚Äôs contribution could become significantly stronger.\n\n---\n\n**Decision Recommendation:** **Reject (encouraged to resubmit after major revision).**  \nThe work presents a promising idea but requires substantial clarification, expanded experiments, and refinement of claims to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this paper, the author present a new framework for training multi-modal networks, called the Multi-modal Multi-task Mixture of Experts. The framework consists of two components:\n- MSoE: Modality specific mixture of experts --> for each modality, they learn a function g that applies:\n column-wise softmax (D) on X times a learnable matrix, multiplied by X, followed by row-wise softmax (C) on the output and a linear combination to compute the prediction.\n- MToE: Modality shared modality task mixture of experts --> connects tasks to input modalities by learning a task embedding shared across experts.\n\nThey also propose a mutual information loss and evaluate the approach on four publicly available medical imaging datasets for breast cancer and OCT.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nThe framework presented is original and interesting. It outperforms existing baseline models. The authors run experiments on multiple datasets and conduct an ablation study.\n\n### Weaknesses\n\n- The paper presentation requires improvement. For example, there is unnecessary use of ; and there is incorrect use of opening quotations \". The authors also repeatedly introduce the abbreviations - this should be done once.\n- I found it difficult to parse through Figure 2 (Can you relate it with the textual explanation of the functions?)\n- The authors only compare to a few baselines, can you incorporate more? There is a lot of literature on multimodal learning now.\n- Are the performance improvements significant? Can you conduct significance testing and provide confidence intervals?\n- The experiments are conducted on medical imaging datasets. How does this apply to other non-imaging modalities where modality competition may be more pronounced. For example, this could be applicable to MIMIC CXR (chest X-rays) and MIMIC EHR where downstream tasks are more dependent on the EHR modality.\n- The main results section in the text should also discuss the quantitative results.\n- What was your hyperparameter tuning strategy? It is unclear if these baselines have been best optimized.\n- Can you also compute AUROC and AUPRC for the classification tasks? Accuracy is not sufficient.\n\n### Questions\n\n- Can the authors discuss the scalability of the framework? What is the computational complexity?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript introduces a novel framework for training multi-modal networks, termed the Multi-modal Multi-task Mixture of Experts (M^3oE). This framework is composed of two main components: the Modality-Specific Mixture of Experts (MSoE) and the Modality-Shared Task Mixture of Experts (MToE). The authors aim to improve learning functions across various modalities, particularly in the context of medical imaging for breast cancer and OCT, supported by evaluation on four publicly available datasets. However, while the conceptual contributions are promising, the presentation and depth of analysis require enhancements for clarity and rigor.\n\n**Major Comments:**\n1. The overall presentation of the paper needs improvements; for instance, there is inconsistent punctuation, specifically the use of semicolons and quotation marks.\n2. Figure 2 is confusing; it would benefit from a clearer relationship to the textual explanation of the functions.\n3. The authors have compared their method to only a limited number of baselines; incorporating a wider range of comparisons from existing literature on multimodal learning is necessary.\n4. The significance of the performance improvements is not adequately tested; conducting significance testing and providing confidence intervals would strengthen the findings.\n5. The applicability of the proposed framework to non-imaging modalities requires discussion, especially considering datasets like MIMIC CXR (chest X-rays) and MIMIC EHR.\n6. The results section should include a discussion of quantitative results alongside qualitative ones.\n7. The hyperparameter tuning strategy is not explicitly stated; clarification on optimization of the baselines is needed.\n8. Additional metrics such as AUROC and AUPRC should be computed for classification tasks, as accuracy alone is insufficient.\n\n**Minor Comments:**\n- The introduction of abbreviations should occur only once to avoid redundancy.\n- There are minor typographical errors to correct throughout the manuscript.\n\n**Summary Paragraph:**\nIn relation to the TMI editorial criteria, the manuscript presents a significant advancement in multi-modal network training (significance), demonstrating originality and outperforming existing baselines (innovation). However, the limited comparison to existing methodologies and lack of robust statistical analysis detract from its overall evaluation quality. The framework's potential relevance beyond medical imaging is also insufficiently addressed, impacting its reproducibility. Overall, while there are noteworthy contributions, several critical issues require attention to meet the necessary standards for publication.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a new framework for training multimodal networks, termed the *Multi‚Äëmodal Multi‚Äëtask Mixture of Experts* (M3oE). The model comprises two main components: (1) modality‚Äëspecific mixtures of experts (MSoE) that learn functions per modality through a softmax‚Äëbased transformation and linear combination; and (2) modality‚Äëshared mixtures of experts for tasks (MToE) that link task embeddings across modalities. The authors also introduce a mutual information loss and evaluate their method on four publicly available medical imaging datasets related to breast cancer and OCT. Overall, the concept is original and promising, though the clarity and presentation of the manuscript need improvement.\n\n**Major Comments**  \n1. **Presentation and Clarity** ‚Äì The paper‚Äôs readability is hindered by grammatical issues and inconsistent abbreviation usage. Unnecessary punctuation and inconsistent notation should be corrected. Figure‚ÄØ2 is difficult to interpret and should be better aligned with the corresponding textual explanation.  \n2. **Comparative Evaluation** ‚Äì Only a few baseline methods are included. Given the extensive literature on multimodal learning, additional comparisons are needed to better contextualize performance gains.  \n3. **Statistical Significance** ‚Äì The manuscript reports improvements but does not assess their significance. Statistical significance testing and confidence intervals should be added.  \n4. **Generalization Beyond Imaging** ‚Äì All experiments focus on medical imaging datasets. Discussion of applicability to non‚Äëimaging modalities (e.g., MIMIC‚ÄëCXR and EHR data) would strengthen the paper.  \n5. **Experimental and Reporting Details** ‚Äì The main results section should explicitly discuss quantitative outcomes. The hyperparameter tuning strategy is unclear, and it is uncertain if baselines were fairly optimized.  \n6. **Evaluation Metrics** ‚Äì For classification tasks, the inclusion of AUROC and AUPRC would provide a more comprehensive performance assessment.  \n7. **Scalability** ‚Äì Clarify the computational complexity and scalability of the proposed framework.\n\n**Minor Comments**  \n- Correct inconsistent punctuation and misuse of quotation marks.  \n- Introduce each abbreviation only once.  \n- Improve figure captions for clarity.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is innovative in proposing a multimodal, multitask mixture‚Äëof‚Äëexperts framework combining shared and modality‚Äëspecific structures with a mutual information loss. Its significance is supported by demonstrated empirical improvements; however, the evaluation lacks statistical rigor, broader comparisons, and discussion on generalizability. Reproducibility would benefit from clearer experimental description and explicit tuning details. Presentation issues and limited scope currently reduce clarity and impact.\n\n**Decision Recommendation**  \n**Major Revision.** The technical contribution is original and potentially significant, but additional analyses, improved clarity, and stronger experimental validation are needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper mainly addresses two challenges in clinical tasks: patient-level and task-level dynamic fusion. For the patient-level fusion, a modality-specific MoE is employed. For the task-level fusion, a modality-task MoE with conditional MI regularization between experts and modalities given tasks is adopted. The experiments using EMBED, RSNA, VinDR, and GAMMA datasets outperform existing methods in both single-task and multi-task settings.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The paper is well-structured and easy to follow. \n- The motivation is clearly stated and convincing. \n- The experiments show promising results over many baselines both in stand-alone and add-on manners.  \n- The paper adopted PID to make a fair comparison of synergy information.\n\n### Weaknesses\n\n- As far as I understand, there have been works leveraging the shared and specific information across modalities and should be included in discussions, see [1-3]. \n- Is there an ablation study for a reduced number of experts? How sensitive is this method when the number of experts decreases compared to other MoE methods? What is the procedure for choosing the number of experts? \n- Please discuss the computational cost compared to the baselines. \n\n\n[1]Wang, Hu, et al. \"Multi-modal learning with missing modality via shared-specific feature modelling.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2]Yao, Wenfang, et al. \"DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.\n\n[3]Chen, Cheng, et al. \"Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion.\" Medical Image Computing and Computer Assisted Intervention‚ÄìMICCAI 2019: 22nd International Conference, Shenzhen, China, October 13‚Äì17, 2019, Proceedings, Part III 22. Springer International Publishing, 2019.\n\n### Questions\n\n- Are there any ablation studies on datasets other than EMBED?\n- Are there any theoretical explanations on why the method mitigates gradient conflict?\n- Is there any clinical interpretation of the results in Figure 5? For example, the difference in modality contribution across different tasks.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**:  \nThe manuscript addresses two significant challenges in clinical tasks: patient-level and task-level dynamic fusion. It employs a modality-specific mixture of experts (MoE) for patient-level fusion and a modality-task MoE with conditional mutual information regularization for task-level fusion. The experiments conducted on datasets such as EMBED, RSNA, VinDR, and GAMMA demonstrate improvements over existing methods in both single-task and multi-task settings.\n\n**Major Comments**:  \n1. The manuscript acknowledges the employment of a modality-specific MoE; however, it should reference existing literature that explores shared and specific information across modalities (see references [1-3]).  \n2. There is no ablation study demonstrated for a reduced number of experts. The authors should clarify the method's sensitivity to changes in the number of experts compared to other MoE approaches. Additionally, the criteria used to select the number of experts should be discussed.  \n3. A discussion regarding the computational cost of the proposed method in comparison to the baseline methods is warranted.\n\n**Minor Comments**:  \n- The paper is well-structured and clear, contributing to reader understanding.  \n- The motivation for the study is articulated and convincing.  \n- The experimental results significantly surpass various baselines, which are evaluated in both standalone and synergistic manners.  \n- The inclusion of PID enables a fair comparison of synergy information.  \n\n**Summary Paragraph**:  \nThe manuscript is significant for its innovation in addressing clinical task challenges using dynamic fusion techniques. It shows potential in evaluation through various datasets and achieves reproducibility in its findings. However, it requires improvements in theoretical discussions, particularly concerning existing literature, ablation studies, and computational costs. These weaknesses may impact the manuscript's overall contributions.\n\n**Decision Recommendation**:  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses two central challenges in clinical multi‚Äëtask learning: patient‚Äëlevel and task‚Äëlevel dynamic fusion. The authors propose a modality‚Äëspecific Mixture of Experts (MoE) for patient‚Äëlevel fusion and a modality‚Äëtask MoE with conditional mutual information regularization to promote disentanglement between experts and modalities. Experiments conducted on four datasets (EMBED, RSNA, VinDR, and GAMMA) demonstrate superior performance to existing baselines under both single‚Äëtask and multi‚Äëtask settings. Overall, the paper is clearly structured, well motivated, and presents its results in an accessible manner.  \n\n**Major Comments**  \n1. **Related Work Coverage:** The discussion should include prior works that explore shared and modality‚Äëspecific feature modeling across modalities, such as Wang et‚ÄØal.‚ÄØ(2023), Yao et‚ÄØal.‚ÄØ(2024), and Chen et‚ÄØal.‚ÄØ(2019). These are directly relevant and would clarify the novelty and positioning of the proposed method.  \n2. **Ablation on Number of Experts:** The review requests an ablation study examining how performance changes with fewer experts. The selection criteria for the number of experts should be explained, and the sensitivity of the model to this parameter should be analyzed relative to other MoE‚Äëbased methods.  \n3. **Computational Efficiency:** A quantitative comparison of computational cost versus baselines would strengthen claims of practicality and scalability.  \n4. **Additional Analyses:** The reviewer asks whether ablation studies were conducted on datasets beyond EMBED, and seeks theoretical reasoning for how the method mitigates gradient conflict.  \n5. **Clinical Interpretation:** The reviewer suggests expanding discussion of Figure‚ÄØ5 to include potential clinical interpretation‚Äîspecifically, how modality contributions vary across different tasks.  \n\n**Minor Comments**  \n- None noted beyond requests for expanded discussion and clarification.  \n- No ethical concerns identified.  \n\n**Summary Paragraph (TMI Criteria)**  \nFrom the standpoint of significance, the paper tackles clinically relevant challenges in multimodal fusion with promising quantitative results. Innovation lies in the dual‚Äëlevel MoE design and the use of conditional MI regularization. The evaluation is extensive across multiple datasets but could be strengthened by additional ablations, cost analysis, and theoretical justification. Reproducibility appears reasonable given the reported methodology, though further detail on hyperparameter choices (e.g., number of experts) would assist reproducibility.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision ‚Äî The paper shows strong potential and competitive performance but requires deeper comparative discussion, additional analysis on expert sensitivity and computational cost, and clearer theoretical and interpretive explanations.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äî Analytical and Scope Audit**\n\n1. **Scope &‚ÄØRelevance to‚ÄØTMI**‚ÄØ‚Äì‚ÄØ‚úÖ  \n   The manuscript presents a new *methodological* framework for multi‚Äëmodal, multi‚Äëtask learning explicitly tailored to medical image analysis.  The focus on dynamic modality fusion, probabilistic modeling of modality‚Äìtask dependence, and interpretability aligns with IEEE‚ÄØTMI‚Äôs methodological emphasis (image representation, learning theory, and validation on medical data).\n\n2. **Novelty &‚ÄØContribution Level**‚ÄØ‚Äì‚ÄØHigh‚ÄìModerate‚ÄØ‚úÖ  \n   Introducing a *Modality‚ÄëTask Mixture‚Äëof‚ÄëExperts‚ÄØ(MToE)* with a conditional mutual‚Äëinformation objective constitutes a distinct methodological layer beyond standard soft‚ÄëMoE or late/early fusion.  While building upon known MoE and information‚Äëdisentanglement principles, the specific probabilistic design and dynamic modality‚Äìtask coupling appear novel in the context of medical‚Äëimaging multitask learning.\n\n3. **Technical &‚ÄØExperimental‚ÄØRigor**‚ÄØ‚Äì‚ÄØGenerally strong  \n   Mathematical formulation is clear and internally consistent. Implementation details (expert counts, optimizer, hyper‚Äëparameters) are sufficient for replication, though training‚Äëtime computation cost, convergence behavior, and statistical significance of results should be more fully characterized. Dataset use spans four independent cohorts‚Äîgood breadth‚Äîbut each dataset‚Äôs train/validation splits and label‚Äëbalancing protocol are not deeply discussed.\n\n4. **Clarity &‚ÄØPresentation**‚ÄØ‚Äì‚ÄØGood  \n   The manuscript reads smoothly, figures are conceptually clear, and ablation and analysis sections are detailed. Minor grammatical issues and typographic spacing need correction (e.g., inconsistent sub‚Äëscripts and Greek symbols).\n\n5. **Ethical‚ÄØ/‚ÄØReproducibility‚ÄØCompliance**‚ÄØ‚Äì‚ÄØAdequate with minor gaps  \n   All datasets are public and anonymized; IRB concerns minimal. Code availability is stated but placeholder link ‚ÄúXXX‚Äù must be replaced by an accessible repository. Authors should confirm license terms and any preprocessing steps adhering to dataset policies.\n\n\n---\n\n**Phase‚ÄØ2 ‚Äî IEEE‚ÄØTMI Review Report**\n\n### 1.‚ÄØSummary\nThe paper introduces **M4oE**, a *Multi‚Äëmodal‚ÄØMulti‚Äëtask‚ÄØMixture of‚ÄØExperts* framework for medical‚Äëimaging tasks such as mammographic risk/density estimation and retinal glaucoma grading‚ÄØ+‚ÄØsegmentation.  \nM4oE integrates two key modules:  \n- **Modality‚ÄëSpecific‚ÄØMoE (MSoE)**‚ÄØto retain modality‚Äëunique signals and reduce ‚Äúmodality competition.‚Äù  \n- **Modality‚ÄëTask‚ÄØMoE‚ÄØ(MToE)**‚ÄØto learn dynamic, task‚Äëdependent fusion by maximizing conditional mutual information between modalities and experts given tasks.  \nExtensive experiments on EMBED,‚ÄØRSNA,‚ÄØVinDr,‚ÄØand‚ÄØGAMMA datasets demonstrate consistent improvement over multi‚Äëmodal and multi‚Äëtask baselines, with additional interpretability analyses at both sample and population levels.\n\n### 2.‚ÄØStrengths\n- **Methodological innovation:** Novel probabilistic coupling of modality, expert, and task variables within an MoE backbone.  \n- **Broad empirical scope:** Multiple imaging domains and heterogeneous datasets, showing generalizability.  \n- **Interpretability:** Quantitative analysis of modality contributions and synergy provides insight beyond performance tables.  \n- **Solid ablations:** Clear demonstration of each component‚Äôs effect (MSoE,‚ÄØMToE, mutual‚Äëinformation loss).  \n- **Potential clinical relevance:** Tailored to realistic diagnostic protocols requiring multiple modalities and tasks.\n\n### 3.‚ÄØWeaknesses\n- **Incremental overlap** with recent MoE‚Äëbased fusion work (e.g., Soft‚ÄëMoE,‚ÄØFlex‚ÄëMoE) not fully differentiated theoretically.  \n- **Computational cost / scalability:** Lacking analysis of training efficiency, parameter count, and inference latency.  \n- **Statistical reporting:** Mean‚ÄØ¬±‚ÄØSDs appear only in appendix; significance testing or confidence intervals for performance improvements absent.  \n- **Dataset partitioning transparency:** Limited details on cross‚Äësite splits, patient‚Äëlevel separation, and how missing modalities or labels are handled.  \n- **Code accessibility pending:** the promised repository is not provided.\n\n### 4.‚ÄØMajor‚ÄØComments\n1. **Clarify methodological novelty vs existing MoE extensions** ‚Äì Elaborate how the conditional‚ÄëMI term and task‚Äëslot architecture achieve behavior unattainable by generic MoE fusions or adapter‚Äëbased parameter gating.  \n2. **Quantify computational complexity** ‚Äì Report parameter counts, FLOPs, runtime, and memory relative to baselines to ensure practicality in large‚Äëscale or clinical deployment.  \n3. **Provide statistical validation** ‚Äì Include paired statistical comparisons (e.g., Wilcoxon signed‚Äërank or t‚Äëtests) across folds to substantiate ‚Äúsignificant improvement.‚Äù  \n4. **Cross‚Äëdataset generalization** ‚Äì Consider hold‚Äëone‚Äëdataset‚Äëout evaluation to demonstrate robustness to institution shift, which is central to TMI readership.  \n5. **Reproducibility documentation** ‚Äì Supply complete code link and detailed preprocessing pipelines; specify random‚Äëseed runs and hardware to facilitate replication.\n\n### 5.‚ÄØMinor‚ÄØComments\n- Correct minor typographic inconsistencies (spacing around mathematical symbols, use of italics).  \n- Ensure figure font sizes readable at print resolution; merge Figures‚ÄØ3‚Äì5 into supplementary if exceeding page limits.  \n- In Section‚ÄØ3.3 equations, clearly define normalization constants and marginalization steps.  \n- Replace placeholder citation ‚ÄúCarr‚ÄØet‚ÄØal.‚Äù with full bibliographic detail.  \n- Clarify variable naming (e.g., whether‚ÄØM or‚ÄØm denotes number of modalities) to prevent ambiguity.\n\n### 6.‚ÄØOverall Recommendation  \n**Major Revision‚ÄØ‚òëÔ∏è**  \nThe work is promising and methodologically interesting, meeting TMI‚Äôs scope, but requires stronger validation and clearer differentiation from existing MoE variants before acceptance. The revisions should focus on (i) sharpening the theoretical contribution, (ii) adding quantitative significance and efficiency analyses, and (iii) improving reproducibility documentation.\n\n### 7.‚ÄØConfidence‚ÄØLevel‚ÄØ=‚ÄØ4‚ÄØ/‚ÄØ5  \n(High confidence in technical interpretation; moderate confidence pending access to code and full quantitative appendix.)\n\n---\n\n**Summary for Editors:**  \nThis manuscript presents a technically sound and potentially influential framework elevating multi‚Äëmodal, multi‚Äëtask modeling in medical imaging.‚ÄØIt sits comfortably within the methodological focus of‚ÄØTMI but would benefit from expanded validation and clearer exposition of novelty relative to contemporary mixture‚Äëof‚Äëexperts approaches.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses multi-modal multi-task learning in medical imaging by proposing M4oE (Multi-modal Multi-task Mixture of Experts). The authors identify two key challenges: dynamic modality fusion where the quality and relevance of different imaging modalities varies across patients, and modality-task dependence where different clinical tasks require dynamic feature selection from various modalities. M4oE consists of Modality-Specific MoE (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module that dynamically decomposes modality-specific and shared information. The framework uses conditional mutual information loss to encourage experts to learn modality-task dependencies. Experiments on four public datasets (EMBED, RSNA, VinDr-Mammo, GAMMA) for breast cancer screening and retinal disease diagnosis demonstrate superior performance over state-of-the-art methods across classification and segmentation tasks, measured by Accuracy, AUROC, AUPRC, and DICE scores.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - The conditional probability definitions in Section 3.3 lack clear denominators and appear inconsistent (e.g., P(Ej|Tk) formula uses summation over modalities in numerator but tasks and experts in denominator)\n  - The conditional mutual information I(M; E|T) formulation in Section 3.3 is presented without sufficient mathematical rigor regarding how probabilities are estimated from the soft routing weights\n  - The relationship between the routing matrix XŒ¶ and probability distributions needs clearer mathematical justification, particularly how soft weights translate to valid probability measures\n\n‚Ä¢ **Limited experimental rigor and evaluation gaps**\n  - Table 1 shows results only for 3 out of 7 tasks on EMBED dataset, with complete results relegated to appendix, limiting assessment of multi-task performance claims\n  - The gradient conflict analysis in Figure 6 uses only a reduced experiment (16 experts, 7 tasks) rather than the full model configuration, weakening the validity of this key claim\n  - Cross-validation methodology is mentioned as 5-fold but detailed experimental setup, statistical significance testing, and error analysis are insufficient across all experiments\n\n‚Ä¢ **Insufficient baseline comparisons and ablation studies**\n  - The paper lacks comparison with recent MoE-based medical imaging methods, with some comparisons mentioned as \"included in Appendix\" (Page 7) but not thoroughly analyzed\n  - Ablation studies in Table 2 are limited to only 3 tasks from EMBED dataset, insufficient to validate component effectiveness across diverse medical imaging scenarios\n  - The modality competition analysis in Figure 3(a) compares only against basic fusion methods rather than more sophisticated multi-modal approaches from recent literature\n\n‚Ä¢ **Questionable interpretability claims and analysis depth**\n  - The sample-level interpretability claims in Figure 5 lack validation against ground truth or clinical expert assessment, making it unclear whether the interpretations are medically meaningful\n  - The synergy analysis using PID theory in Figure 4 requires external domain knowledge not sufficiently explained, and the clinical relevance of the synergy patterns is not validated\n  - Population-level modality contribution analysis lacks statistical validation and comparison with known clinical understanding of modality importance for different tasks\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation consistency**\n  - Provide rigorous derivations for all conditional probability formulations in Section 3.3, clearly defining how soft routing weights are converted to valid probability distributions\n  - Add formal mathematical proofs or detailed explanations for the conditional mutual information objective, including convergence properties and theoretical justifications\n  - Standardize notation throughout the paper and ensure all probability expressions have consistent and correct mathematical forms with proper normalization\n\n‚Ä¢ **Strengthen experimental validation and statistical analysis**\n  - Report complete results for all 7 tasks on EMBED dataset in the main paper with proper statistical significance testing and confidence intervals\n  - Conduct gradient conflict analysis using the full model configuration (128 experts) rather than reduced settings to validate this key technical claim\n  - Provide detailed cross-validation procedures, statistical significance tests, and comprehensive error analysis for all experimental results across all datasets\n\n‚Ä¢ **Expand baseline comparisons and comprehensive ablation studies**\n  - Include thorough comparison with recent MoE-based medical imaging methods in the main paper rather than relegating to appendix\n  - Conduct ablation studies across all four datasets and all tasks to demonstrate consistent component effectiveness across diverse medical imaging scenarios\n  - Compare against more sophisticated multi-modal fusion approaches from recent literature in the modality competition analysis\n\n‚Ä¢ **Validate interpretability claims with clinical ground truth**\n  - Collaborate with clinical experts to validate whether sample-level and population-level interpretability outputs align with known medical understanding\n  - Provide statistical validation of modality contribution patterns against established clinical knowledge and compare with expert radiologist assessments\n  - Include more detailed explanation of PID theory and clinical validation of synergy patterns, demonstrating that the learned modality-task dependencies reflect genuine medical insights",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript presents M4oE, a novel Multi-modal Multi-task Mixture of Experts framework designed for medical imaging applications. The proposed model addresses the challenges of dynamic modality fusion and modality-task dependence by integrating modality-specific and shared information in a sample-adaptive manner. M4oE utilizes a combination of Modality-Specific MoE (MSoE) modules and a Modality-Shared Modality-Task MoE (MToE) module to dynamically learn and fuse information from different modalities and tasks. The authors demonstrate the effectiveness of M4oE on four benchmark datasets for breast cancer screening and retinal disease diagnosis, showcasing superior performance over existing methods.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: The manuscript introduces a sophisticated approach to multi-modal multi-task learning in medical imaging, which is a significant area of interest. However, the novelty of M4oE needs to be more clearly articulated in relation to existing multi-modal and multi-task frameworks. The authors should provide a more detailed comparison with recent advancements in the field, particularly those that also employ MoE architectures for dynamic fusion.\n   \n2. Evaluation Design: The experiments are conducted on four publicly available datasets, which is commendable. However, the validation is limited to a few specific tasks (density classification and cancer risk prediction for mammograms, and glaucoma detection and optical cup segmentation for ophthalmology). The authors should consider extending the evaluation to additional tasks and modalities to better demonstrate the generalizability and robustness of M4oE.\n\n3. Comparisons: While the authors compare M4oE with a variety of baselines, including single-task and multi-task models, the comparison lacks some recent state-of-the-art methods in multi-modal multi-task learning. Including more recent works would provide a more comprehensive benchmark and strengthen the claim of M4oE's superiority.\n\n4. Reproducibility: The manuscript mentions that the code will be released, but the details on the training protocols, preprocessing steps, and hyperparameter settings are somewhat sparse. Providing a more detailed description of the experimental setup and making the code publicly available upon acceptance would significantly enhance the reproducibility of the results.\n\n###\n\n## Minor Comments\n1. Clarity of Figures: Some figures (e.g., Fig. 3) are cluttered and could be improved by showing fewer representative examples with zoomed-in regions for better clarity.\n2. Notation Consistency: The notation introduced in Section 2.1 is sometimes inconsistent, leading to confusion. The authors should ensure uniformity in the usage of symbols and terms throughout the manuscript.\n3. Acronym Definitions: Several acronyms (e.g., \"R=4\") are used without proper definitions, which might be confusing for readers who are not familiar with the specific context. Defining these acronyms would improve readability.\n4. Typographical Errors: There are typographical errors throughout the manuscript (e.g., \"k-spacce\", \"undersampling maskes\"), which need to be corrected.\n\n###\n\n## Summary Paragraph\n(Focusing on 4 TMI Editorial Criteria)\nThe manuscript tackles a significant and clinically relevant problem in medical imaging by proposing M4oE, a framework that addresses dynamic modality fusion and modality-task dependence. The technical innovation of integrating modality-specific and shared information through MoE networks is promising and could lead to meaningful advancements in multi-modal multi-task learning. However, the evaluation could be strengthened by extending it to more diverse datasets and tasks, and the reproducibility could be enhanced by providing more detailed methodological details. Overall, while the manuscript shows promise, it requires additional validation and clarification to fully meet the standards of significance, innovation, evaluation, and reproducibility expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should extend the evaluation to include more diverse datasets and tasks, provide a more comprehensive comparison with recent state-of-the-art methods, and enhance the reproducibility by detailing the experimental setup and releasing the code.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Liyue Shen",
      "Luning Wang",
      "Zhengxu Tang",
      "Zitao Shuai",
      "Chenwei Wu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_59d8664cbd1f44ea7da5f36cba0de2bec3d71759.pdf",
    "remote_url": "https://openreview.net/pdf/59d8664cbd1f44ea7da5f36cba0de2bec3d71759.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Synthesizing Realistic fMRI: A Physiological Dynamics-Driven Hierarchical Diffusion Model for Efficient fMRI Acquisition",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "learning on time series and dynamical systems"
    ],
    "keywords": [
      "Time Series",
      "Diffusion"
    ],
    "abstract": "Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications. While generative models such as diffusion models can synthesize fMRI signals to alleviate these issues, they often underperform due to neglecting the brain's complex structural and dynamic properties.\nTo address these limitations, we propose the Physiological Dynamics-Driven Hierarchical Diffusion Model, a novel framework integrating two key brain physiological properties into the diffusion process: brain hierarchical regional interactions and multifractal dynamics. \nTo model complex interactions among brain regions, we construct hypergraphs based on the prior knowledge of brain functional parcellation reflected by resting-state functional connectivity (rsFC). This enables the aggregation of fMRI signals across multiple scales and generates hierarchical signals. \nAdditionally, by incorporating the prediction of two key dynamics properties of fMRI‚Äîthe multifractal spectrum and generalized Hurst exponent‚Äîour framework effectively guides the diffusion process, ensuring the preservation of the scale-invariant characteristics inherent in real fMRI data.\nOur framework employs progressive diffusion generation, with signals representing broader brain region information conditioning those that capture localized details, and unifies multiple inputs during denoising for balanced integration.\nExperiments demonstrate that our model generates physiologically realistic fMRI signals, potentially reducing acquisition time and enhancing data quality, benefiting clinical diagnostics and machine learning in neuroscience.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nIn this paper, the authors proposed a novel framework named the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) for fMRI analytics. The PDH-Diffusion framework integrates two essential brain physiological properties, hierarchical regional interactions and multifractal dynamics, into the diffusion process. The primary goal is to improve diffusion models‚Äô capability to generate realistic fMRI time series signals by accurately capturing these physiological characteristics.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nOverall, the major strength of this work lies in its novelty. The authors have developed an innovative framework that captures complex interdependencies and multifractal dynamics within synthetic fMRI signals.\n\nSpecifically, their contribution includes integrating three key components into the diffusion process: a hypergraph-based signal generator, a dynamics properties guiding module, and a cross-brain region progressive diffusion model. This integration enhances the realism of the generated signals. The authors provide a robust theoretical foundation for their methods and perform extensive quantitative analysis, demonstrating the framework‚Äôs accuracy and effectiveness in time series forecasting. The paper is well-organized and includes relevant background information. Results from the proposed method outperform multiple peer models in time-series forecasting and diffusion models, as evidenced by superior MAE, MAPE, and RMSE scores, highlighting the model‚Äôs effectiveness.\n\n### Weaknesses\n\nThe reviewers have multiple concerns about the framework and potential impact in this work. \n\n1). The confusion about physiological fMRI. Usually, fMRI are categorized into resting-state and task-based fMRI. The resting-state fMRI is commonly scanned without specific stimulus, whereas task-based fMRI is acquired based on external stimulus, such as 7 tasks in HCP. Is physiological fMRI is either resting-state or task-based signal? The authors do not clarify the concept even in Introdcution section.\n\n2). Limited motivation and impact. In Abstract, the authors mentioned \"Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications.\" It seems that the authros' work can advance the fMRI for clincial application, but the authors do not generate some neurological or psychiatric fMRI to validate. From reviewers' perspective, using the innovative PDH-Diffusion model, it can assist physician to provide lengthy fMRI signal which will denfinitely reduce the inconvenience of patients. Only generating healthy fMRI can impair the motivation and impact of this work.\n\n3). Lacking of qualitative comparison. The authors have provided an extensive quantitative validation of PDH-Diffusion model with other peer methods using MAE, MAPE, and RMSE. Unfortunately, the autors do not provide any qualitative results, such as Functional Connectivity Map or Brain Connectivity Maps, of PDH-Diffusion. That is, although averaging metrics such as MAE, RMSE, MAPE across 10 runs may demonstrate robustness, these metrics  cannot fully capture the quality or realism of the synthesized fMRI signals. Notably, the qualitative results is also vital in clinics, since these results showcase which brain regions are severly impaired by neurological disorders.  Importantly, there is no visual representation given of the generated fMRI signal, which would be valuable for assessing their plausibility. \n\n4). Several technical issues. The variance schedule (parameters $\\alpha_n$ and $\\beta_n$) in the diffusion process may not be fully optimized for different regions or scales, potentially leading to inappropriate noise levels in certain hierarchical levels. This could result in over-smoothing or overfitting at certain levels. Additionally, conditioning on historical data could lead to overfitting if the model becomes too dependent on past values, especially if the training data does not represent the full spectrum of brain dynamics. Without explicit mitigation measures, such as adaptive variance schedules or regularization techniques, these issues may limit the model‚Äôs ability to generalize to new or varied patterns, impacting its robustness and effectiveness.\n\n5). Multiple typographical mistakes. In Section 4.3 (ABLATION ANALYSIS) there are many typographical errors. The term ‚Äúshare radio‚Äù is used instead of ‚Äòshare ratio‚Äô in ‚ÄúThe influence of share radio‚Äù this section. Similar typographical errors appear in the caption and text of Figure 3(b), as well as in Section 3.4. These typographical mistakes impair the redability of this paper.\n\n### Questions\n\nThe reviewers have raised several questions regarding the weaknesses of this work:\n\n1). Qualitative Comparisons: The authors are strongly encouraged to provide qualitative comparisons between the synthesized and real fMRI signals to allow for a qualitative assessment of the model‚Äôs performance.\n\n2). Optimal Range of Brain Regions: The results suggest an ‚Äúoptimal‚Äù range of brain regions that enhances performance. How is this range determined, and is it manually set? Reviewers are concerned about the reliance on manual design for determining this optimal range.\n\n3). Validation of Multifractal Properties: How do the authors validate that the generated signals preserve multifractal properties?\n\n4). Risk of Overfitting: Do the variance schedule parameters ($\\alpha_n$ and $\\beta_n$) and the historical data used in training the PDH-Diffusion model lead to overfitting?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a novel framework called the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion), aimed at enhancing fMRI analytics. The framework integrates hierarchical regional interactions and multifractal dynamics into the diffusion process, with the primary objective of improving the generation of realistic fMRI time series signals. While the work exhibits novelty, the overall clarity and impact could benefit from further elaboration and qualitative evaluations.\n\n### Major Comments\n1. **Confusion Around Physiological fMRI**: The manuscript lacks clarity on whether the physiological fMRI is categorized as resting-state or task-based. This distinction is important and should be addressed in the introduction.\n   \n2. **Limited Motivation and Impact**: The paper suggests potential advancements for clinical applications of fMRI; however, it does not validate the framework with neurological or psychiatric fMRI data, potentially diminishing its impact.\n   \n3. **Lack of Qualitative Comparison**: While extensive quantitative validations using metrics like MAE, MAPE, and RMSE are provided, the absence of qualitative assessments (e.g., Functional Connectivity Maps) hinders a full evaluation of the synthesized fMRI signals' realism and clinical relevance. Visual representations of generated signals are also lacking.\n   \n4. **Technical Issues**: There may be optimization issues with the variance schedule parameters ($\\alpha_n$ and $\\beta_n$), potentially affecting noise levels and leading to over-smoothing or overfitting. Additionally, reliance on historical data could contribute to overfitting if not managed with regularization techniques.\n   \n5. **Typographical Mistakes**: Various typographical errors are present, particularly in Section 4.3 and Figure 3(b), which detract from the readability and professionalism of the manuscript.\n\n### Minor Comments\n- Figures and captions contain typographical issues that need correction.\n- The clarity and professionalism of the presentation could be enhanced through careful proofreading.\n\n### Summary Paragraph\nIn summary, the manuscript demonstrates significant potential in relation to the TMI editorial criteria, particularly in its innovative approach to fMRI signal generation. However, the absence of qualitative evaluations limits its significance and robustness. The evaluation design, though quantitatively sound, requires qualitative assessments to enhance credibility. While the reproducibility of results is noted, there are concerns regarding overfitting and the optimization of model parameters.\n\n### Decision Recommendation\nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Physiological Dynamics‚ÄìDriven Hierarchical Diffusion Model* (PDH‚ÄëDiffusion) for fMRI signal generation. The framework combines two key physiological characteristics‚Äîhierarchical regional interactions and multifractal dynamics‚Äîinto the diffusion process, aiming to produce more realistic synthetic fMRI time series. The paper is generally well organized and theoretically grounded, presenting comprehensive quantitative evaluations that indicate improvements over existing diffusion and time‚Äëseries forecasting models.  \n\n**Major Comments**  \n1. **Conceptual Ambiguity of Physiological fMRI:** The term ‚Äúphysiological fMRI‚Äù is not clearly defined. It remains uncertain whether this refers to resting‚Äëstate or task‚Äëbased fMRI. The introduction should clarify this concept to avoid confusion.  \n2. **Limited Motivation and Impact:** The motivation described in the abstract suggests potential clinical relevance (e.g., reducing scan time or patient inconvenience). However, experiments are conducted only on healthy fMRI data without neurological or psychiatric examples, reducing the potential medical significance.  \n3. **Lack of Qualitative Comparisons:** Although extensive quantitative results (MAE, MAPE, RMSE) are provided, no qualitative analyses such as functional or brain connectivity maps are shown. Visual representations of generated fMRI activity are crucial to assess realism and clinical relevance.  \n4. **Technical Concerns in Diffusion Parameters:** The variance schedule parameters (Œ±‚Çô, Œ≤‚Çô) may not be properly optimized across scales or brain regions, possibly introducing over‚Äësmoothing or overfitting. Conditioning on historical data could further limit generalization if the training set lacks sufficient variability. Mitigation strategies such as adaptive scheduling or regularization should be discussed.  \n5. **Typographical Errors:** Multiple typographical issues (e.g., ‚Äúshare radio‚Äù instead of ‚Äúshare ratio‚Äù in Section 4.3 and Figure 3(b)) detract from readability.  \n\n**Minor Comments**  \n- Clarify how the ‚Äúoptimal‚Äù range of brain regions is determined‚Äîmanual selection versus learned parameterization.  \n- Expand discussion on how multifractal properties are validated in the generated signals.  \n- Ensure consistent terminology and correct remaining minor typographical errors throughout.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThis work is notable for its innovative integration of hierarchical and multifractal physiological dynamics within a diffusion framework, offering potential conceptual advances for fMRI signal modeling. Its significance would increase with clearer definitions, clinical validation, and qualitative visualization. Evaluation is rigorous quantitatively but incomplete qualitatively. The paper is reproducible in structure and presentation but would benefit from additional methodological clarifications and corrected typographical issues.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial clarification, qualitative validation, and correction of technical and typographical issues are needed before the manuscript can be reconsidered.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors proposed a novel algorithm for synthesizing realistic functional MRI (fMRI) via physiological dynamics-driven hierarchical diffusion model. Then, the authors validated the feasibility of synthesized fMRI data by comparing it to other generative mechanisms. The scientific merit of this work mainly comes from the conceptual advances in their proposed algorithm. It is indeed challenging to synthesize realistic fMRI data while preserving unique aspects of the brain system. The authors combined three different modules, each serving different roles, to synthesize brain dynamics with preserved network-like structure and fractal components. Given results of the extensive validation experiment on the large fMRI cohort, the validity of the proposed algorithm is clear; yet, the scientific significance of this work is somewhat limited as there was no experiment demonstrating the practical usefulness of the proposed algorithm.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe major originality of this study comes from its conceptual advances embedded in the proposed algorithm. It is indeed challenging to synthesize realistic fMRI data while preserving unique aspects of the brain system. The quality and clarity of the models in the main text are reasonably strong as well.\n\n### Weaknesses\n\nThe major concern comes from the unclear significance of this work. As the authors argued in the Introduction, acquisition of fMRI is expensive. Thus, synthesizing fMRI signal can be tempting. Although expensive, real fMRI data reflects unique information of individuals. This study, however, was not able to demonstrate the synthesized fMRI still convey unique information of subjects. Slight improvement in forecasting future timepoints of fMRI signal does not suggest the significance or practical usefulness of the model. This works needs additional analysis or applications highlighting the unique advantage of the synthesized fMRI data from the proposed model.\n\n### Questions\n\n1. In table 1, does ‚ÄúT_Pred=32 or 64‚Äù mean predicting 32 or 64 time points of fMRI data?\n2. If my speculation in the former question is right, it is counter-intuitive that forecasting more time points (96 vs. 32) did not lead to an increase in errors. Please clarify this. \n3. According to Fig 1, it looks like the resolution of synthesized data is bounded to the level of ROI. It is more desirable to synthesize fMRI dynamics at the level of vertex. Is the proposed method able to be applied at the vertex level as well?     \n4. In line 451, please double check the definition of MAE.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel algorithm for synthesizing realistic functional MRI (fMRI) data using a physiological dynamics-driven hierarchical diffusion model. The authors validate this algorithm by comparing the synthesized fMRI data against other generative mechanisms. Although the study showcases significant conceptual advancements and demonstrates the feasibility of the proposed method, the practical significance of the work is lacking as no experiments are conducted to highlight the utility of the synthesized data in real-world scenarios.\n\n### Major Comments\n1. **Novelty and Significance**: The originality of the proposed algorithm is commendable; however, the manuscript does not sufficiently demonstrate the practical usefulness or unique advantages of the synthesized fMRI data.\n2. **Evaluation Design**: The validation experiments on the large fMRI cohort establish the algorithm's feasibility, but additional analyses are required to confirm that the synthesized fMRI retains unique individual information.\n3. **Practical Application**: The improvement in forecasting future timepoints of fMRI signals does not convincingly support the significance of the model's practical applications.\n\n### Minor Comments\n- Clarify the meaning of ‚ÄúT_Pred=32 or 64‚Äù in Table 1 regarding the prediction of time points.\n- Explain why forecasting more time points (96 vs. 32) does not result in increased errors, as this is counter-intuitive.\n- In Figure 1, it appears that the synthesized data's resolution is limited to the ROI level. Can the proposed method also apply at the vertex level?\n- Verify the definition of MAE in line 451 for accuracy.\n\n### Summary Paragraph\nIn summary, this study makes valuable conceptual contributions to the synthesis of fMRI data using a novel algorithm demonstrating feasible results through extensive validation. However, the practical significance and innovation are called into question, as the authors do not provide evidence to support the utility of the synthesized data in preserving unique individual information. The evaluation design is solid, but further analysis is needed to enhance the findings' reproducibility and significance regarding real-world applications.\n\n### Decision Recommendation \nMajor revision is recommended to address the identified weaknesses, particularly regarding the demonstration of the synthesized data's practical usefulness and unique advantages.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a physiological dynamics‚Äìdriven hierarchical diffusion model to synthesize realistic functional MRI (fMRI) data. The work integrates three modules designed to preserve network-like brain structure and fractal characteristics during data synthesis. The study‚Äôs main contribution lies in the conceptual novelty of the proposed generative framework. Validation experiments using a large fMRI cohort provide evidence that the algorithm is technically sound and capable of generating plausible synthetic data. However, despite the methodological sophistication, the manuscript‚Äôs scientific significance appears limited due to the absence of experiments demonstrating the practical usefulness or interpretive value of the synthesized data. Overall, the presentation is clear and the modeling is well explained.  \n\n**Major Comments**  \n1. **Significance and Practical Utility:** The main weakness lies in the unclear real-world significance. Although the method synthesizes realistic fMRI signals, it does not show that these synthetic data capture subject-specific or biologically meaningful information. Demonstrating that the generated fMRI data can serve practical research or clinical purposes would considerably strengthen the contribution.  \n2. **Evaluation Design:** The reported improvement in forecasting future time points is modest and does not convincingly convey the advantage of the proposed model. Additional analyses illustrating unique benefits of the synthesized data are needed.  \n3. **Clarification of Prediction Setting:** In Table‚ÄØ1, it is unclear whether ‚ÄúT_Pred‚ÄØ=‚ÄØ32 or‚ÄØ64‚Äù refers to the number of predicted time points. If so, the finding that forecasting more time points (96‚ÄØvs.‚ÄØ32) does not increase errors is counterintuitive and should be explained.  \n4. **Spatial Resolution Limitation:** Figure‚ÄØ1 suggests that the synthetic data are generated at the ROI level. It would be valuable to clarify whether the method can be extended to vertex-level synthesis to better capture local dynamics.  \n\n**Minor Comments**  \n- Please check the definition of MAE in line‚ÄØ451 for correctness.  \n- Ensure all figures and tables clearly state experimental parameters and units.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is methodologically innovative but of moderate overall significance. The conceptual approach demonstrates originality (innovation) and is evaluated on a large dataset (evaluation), yet it falls short in establishing application value or broader scientific impact (significance). The experiments are reproducible in principle, though more details on implementation and parameters would facilitate replication.  \n\n**Decision Recommendation**  \n**Major revision.** The manuscript requires additional analyses or demonstrations that highlight the practical advantages and scientific relevance of the synthesized fMRI data.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a novel approach for generating realistic fMRI data using diffusion models, specifically designed to account for regional interactions and spectral features of the brain. The model captures regional connectivity in a hierarchical structure, where fine-scale signals are conditioned on larger-scale signals. To learn spectral features, it incorporates loss functions that capture fractal characteristics. Results demonstrate improved performance over existing time-series forecasting and diffusion-based models. Additionally, ablation studies validate the effectiveness of each model component.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The proposed method, which captures connectivity and spectral features, is a novel approach. \n2. The method is rigorously validated using multiple benchmarks and ablation studies.\n\n### Weaknesses\n\n1. The paper has limited reproducibility due to missing details about data preparation and experimental setup. Additional information is needed on the dataset used, including whether it involved resting-state or task-based fMRI, whether subjects were healthy or under specific conditions, and the rationale for selecting regions of interest (ROI), which were reduced from 268 to 82. Clarification on data split (e.g., train/test division, sample counts) is also required. If the codebase will not be provided, the paper should include a detailed description of the network architecture (such as layer specifications and activation functions) and the training setup for benchmark methods in an appendix.\n2. The practical implications, particularly the clinical applications of the proposed method, are somewhat unclear and could benefit from further exploration and discussion.\n\n### Questions\n\n1. What is the sampling frequency for the fMRI?\n2. It would be beneficial to analyze the reconstructed signal to determine if the observed patterns align with expectations. Calculating the spectrum and fractal characteristics would provide an important validation of the model‚Äôs effectiveness. Additionally, some neurological features can also be checked. For instance, if the data is from resting-state fMRI, does it reveal the default mode network?  \n3. Line 465 typo 69->96\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a novel methodology for generating realistic fMRI data through diffusion models, focusing on the integration of regional interactions and spectral features of the brain. The proposed model effectively captures hierarchical regional connectivity, linking fine-scale signals to larger-scale signals. It employs specific loss functions to encapsulate fractal characteristics. The findings indicate enhanced performance compared to existing time-series forecasting and diffusion models. Furthermore, ablation studies substantiate the contributions of individual model components.\n\n**Major Comments:**\n1. The reproducibility of the study is limited due to insufficient details regarding data preparation and the experimental setup. Specific information is necessary about the dataset, including whether it encompasses resting-state or task-based fMRI, the health conditions of subjects, and the rationale behind the reduction of regions of interest (ROI) from 268 to 82. Clarification on the data split, including train/test division and sample counts, is also essential. If the codebase is not shared, an appendix should provide comprehensive details about the network architecture, including layer specifications and activation functions, along with the training setup for benchmark methods.\n2. The clinical implications of the proposed method are not clearly articulated and would benefit from further discussion and exploration.\n\n**Minor Comments:**\n- The sample frequency for the fMRI data should be specified.\n- An analysis of the reconstructed signal is recommended to verify alignment with expected patterns. This should include calculating the spectrum and fractal characteristics for additional validation of the model's efficacy. It may also be pertinent to investigate neurological features, such as whether the resting-state data reflects the default mode network.\n- There is a typo noted on line 465, where \"69\" should be corrected to \"96.\"\n\n**Summary Paragraph:**\nIn evaluating the manuscript against the TMI editorial criteria, it demonstrates significance through its innovative approach to fMRI data generation. While the contributions are noteworthy and backed by rigorous validation, significant concerns regarding reproducibility and clarity of clinical applications remain. The manuscript would benefit from the inclusion of crucial experimental details and further exploration of practical implications. \n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a novel diffusion-based method for generating realistic fMRI time-series data. The approach explicitly models regional interactions through a hierarchical network structure, where fine-scale signals are conditioned on broader regional dynamics, and introduces loss functions to capture spectral and fractal characteristics. The work is clearly motivated and demonstrates improved performance over baseline forecasting and diffusion models, supported by comprehensive ablation analyses. Overall, the study is conceptually interesting and technically well executed, though certain aspects require clarification to ensure reproducibility and stronger contextual grounding.  \n\n**Major Comments**  \n1. **Reproducibility and Experimental Details:** The reproducibility of the study is limited by insufficient detail about the dataset and experimental setup. The manuscript should specify whether the data are resting-state or task-based fMRI, the subject population (e.g., healthy or clinical), and the rationale for reducing the number of regions of interest from 268 to 82. Details on data splitting (training/testing proportions and sample counts) should also be provided. If the source code will not be released, a more explicit description of the network architecture, layer types, activations, and training parameters for both the proposed and baseline models should be included, ideally in an appendix.  \n2. **Practical and Clinical Relevance:** The paper does not sufficiently discuss potential clinical or neuroscientific implications. Expanding on how the generated data may support real-world neuroimaging analyses or clinical applications would enhance the significance and impact of the work.  \n\n**Minor Comments**  \n- Clarify the fMRI sampling frequency used in experiments.  \n- Consider analyzing reconstructed signals to verify whether spectral and fractal metrics match expectations and whether relevant brain networks (e.g., default mode network for resting-state fMRI) can be identified.  \n- Typographical correction: line 465, ‚Äú69‚Äù should be ‚Äú96.‚Äù  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is significant in proposing a structured diffusion model that integrates spatial hierarchy and spectral learning for fMRI data synthesis. Its methodological innovation is clear and supported by empirically validated results. However, the evaluation would benefit from greater transparency regarding data and methods to ensure reproducibility. The clinical significance remains to be fully articulated. With more detailed reporting and expanded discussion, the work could make a meaningful contribution to generative modeling in neuroimaging.  \n\n**Decision Recommendation**  \n**Major Revision.** Clarification of data handling, methodological details, and application relevance is required before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**PHASE 1 ‚Äì Analytical and Scope Audit**\n\n**1. Scope & Relevance to IEEE TMI**  \n‚úÖ The paper clearly targets methodological innovation in medical imaging, introducing a new generative modeling framework for *fMRI signal synthesis and forecasting*. It intertwines imaging physics‚Äìinspired priors (functional connectivity and fractal dynamics) with modern diffusion-based generative modeling. This lies squarely within TMI‚Äôs interest in imaging reconstruction, signal modeling, and quantitative imaging theory.  \n\n**2. Novelty & Contribution Level**  \nThe ‚ÄúPhysiological Dynamics‚ÄìDriven Hierarchical Diffusion Model (PDH‚ÄëDiffusion)‚Äù is an ambitious method integrating multiple physiological priors into a generative diffusion model. There is meaningful novelty in:\n- Coupling functional connectivity‚Äìderived *hypergraph structures* with diffusion processes.  \n- Embedding *multifractal dynamics* (generalized Hurst exponent, multifractal spectrum) as explicit conditioning signals.  \n- Using a *progressive multi‚Äëscale diffusion chain* guided by brain‚Äëregion hierarchies.  \n\nThe conceptual integration surpasses standard diffusion‚Äëtime‚Äëseries applications, though the core diffusion machinery is known. Methodological contribution is thus moderate‚Äëto‚Äëstrong; the main innovation lies in the *physiological conditioning design*.  \n\n**3. Technical and Experimental Rigor**  \nThe paper is technically dense and mathematically coherent. Derivations of fractal properties are thorough. Experimental validation uses the HCP dataset; the training pipeline and baselines are well‚Äëchosen. However, shortcomings include:\n- Limited discussion of how ‚Äúphysiologically realistic‚Äù generation is quantitatively validated beyond forecasting metrics (MAE/RMSE/MAPE).  \n- Absence of uncertainty estimates, reproducibility statistics, or clinical interpretability analysis.  \n- Lack of details about subject variability, test‚Äìretest reliability, or whether generated fMRI preserve spatial‚Äìtemporal physiological realism beyond numeric metrics.  \n\n**4. Clarity and Presentation**  \nGenerally well structured, albeit verbose. Figures are helpful but the writing could be tightened for readability. Derivations may be too detailed for TMI‚Äôs page format; clearer schematic flowcharts of the pipeline and loss functions would help.  \n\n**5. Ethical and Reproducibility Compliance**  \nThe dataset (HCP) is public and anonymized; code availability is promised via GitHub. No direct patient study occurs‚Äîminimal ethical concern observed.\n\n\n---\n\n**PHASE 2 ‚Äì IEEE TMI Review Report**\n\n**1. Summary (in reviewer‚Äôs words)**  \nThe authors propose *PDH‚ÄëDiffusion*, a hierarchical diffusion model that synthesizes realistic fMRI time‚Äëseries. The model fuses three components: (i) a hypergraph‚Äëbased hierarchical signal generator modeling multi‚Äëscale functional connectivity among regions; (ii) a dynamics‚Äëguiding module that enforces multifractal temporal behavior via predicted spectra and Hurst exponents; and (iii) a cross‚Äëregion progressive diffusion process where coarse‚Äëscale regional trends guide fine‚Äëscale generation. Evaluated on the Human Connectome Project dataset, PDH‚ÄëDiffusion reportedly surpasses transformer and diffusion baselines in MAE/RMSE/MAPE metrics.\n\n**2. Strengths**\n- Strong methodological concept embedding *neurophysiological priors* into machine learning generation.  \n- Mathematically consistent link between fractal brain dynamics and diffusion conditioning.  \n- Clear ablation study validating the role of each module.  \n- Open‚Äësource intent and usage of a widely recognized dataset.  \n\n**3. Weaknesses**\n- Evaluation metrics focus on generic time‚Äëseries accuracy, not physiological or spatial realism; missing evaluation of dynamic functional connectivity preservation.  \n- Clinical or acquisition‚Äëtime reduction claims are speculative without empirical evidence.  \n- Hyperparameters and model reproducibility details (e.g., initialization, computational cost) are sparse.  \n- Writing density and formula presentation may impede accessibility for imaging readers.  \n\n**4. Major Comments**\n1. **Validation of physiological realism:** Move beyond numerical forecasting errors; include comparisons of fractal spectra, power‚Äëlaw slopes, or rsFC statistics between generated and real signals to support the ‚Äúphysiologically realistic‚Äù claim.  \n2. **Empirical link to acquisition efficiency:** If the model intends to reduce fMRI acquisition time, demonstrate reconstruction/denoising from undersampled or partial scans rather than only forecasting.  \n3. **Ablation breadth:** Provide qualitative visualization (temporal correlations, autocorrelation, PSD comparisons) to reveal how each module preserves dynamic structure.  \n4. **Reproducibility:** Clarify computational cost per subject, diffusion steps, total parameter count, and inference time.  \n5. **Manuscript condensation:** The theoretical exposition on fractal dynamics (Section‚ÄØ2) is excessive; condensing or relocating into supplementary material would improve readability.  \n\n**5. Minor Comments**\n- Figures‚ÄØ1‚Äì2: increase font size; delineate modules with clearer color coding.  \n- Verify consistency in notation (e.g., \\(R\\), \\(r\\), \\(œÉ_r\\)) across sections.  \n- Reference formatting shows unescaped LaTeX; conform to IEEE style.  \n- Typographical errors (‚ÄúA P HYSIOLOGI -CAL‚Äù, ‚Äúb y r esting-state‚Äù) should be corrected.  \n- Future directions could mention potential adaptation to BOLD reconstruction from partially sampled k‚Äëspace.  \n\n**6. Overall Recommendation**  \n**Major Revision.**  \nThe work fits TMI‚Äôs methodological scope and demonstrates interesting novelty by fusing physiological priors with diffusion modeling. However, the current validation and exposition require significant strengthening‚Äîparticularly quantitative physiological evaluation, clarity of contributions, and evidence for claimed acquisition efficiency‚Äîto meet TMI standards.\n\n**7. Confidence Level:** 4/5 (high confidence in assessment; moderate familiarity with diffusion and fMRI generative modeling).",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper addresses fMRI acquisition challenges including lengthy scan times and motion sensitivity by proposing the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) for generating realistic fMRI signals. The method integrates two key brain physiological properties: hierarchical regional interactions modeled through hypergraphs based on resting-state functional connectivity, and multifractal dynamics captured via generalized Hurst exponents and multifractal spectra. The framework comprises three components: a Hypergraph-based Hierarchical Signals Generator that aggregates fMRI signals across multiple spatial scales, a Dynamics Properties Guiding Module that predicts multifractal characteristics to guide diffusion generation, and a Cross-brain Region Guiding Progressive Diffusion Model that uses broader regional signals to condition localized signal generation. Experiments on the Human Connectome Project dataset demonstrate superior performance compared to time series forecasting methods and existing diffusion models across prediction horizons of 32, 64, and 96 timesteps, with ablation studies confirming the contribution of each proposed component.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Equation 1 defines scale-free property with Œ≤‚â•0 but subsequent derivations assume specific relationships without clear justification (Page 3, Section 2.2)\n  - The transition from Equation 2 to Equation 3 lacks mathematical rigor in establishing the self-similarity property, particularly the equality of finite dimensional distributions notation (Page 3)\n  - Equation 9 combines MSE and KL divergence losses without theoretical justification for this specific combination or guidance on relative weighting (Page 7)\n  - Notation inconsistencies appear throughout, such as using both X(t) and Y(t) for time series without clear distinction of their roles (Pages 3-4)\n\n‚Ä¢ **Insufficient experimental validation and baseline comparisons**\n  - Limited to single dataset (HCP) evaluation without cross-dataset validation to demonstrate generalizability (Page 8, Section 4.1)\n  - Missing comparisons with recent fMRI-specific generation methods, focusing only on general time series forecasting and two diffusion models (Table 1, Page 9)\n  - No statistical significance testing reported for performance differences, making it difficult to assess the reliability of claimed improvements (Table 1)\n  - Evaluation metrics (MAE, RMSE, MAPE) are standard forecasting measures but lack fMRI-specific quality assessments such as functional connectivity preservation or physiological plausibility measures (Page 8)\n\n‚Ä¢ **Limited technical novelty and methodological concerns**\n  - Hypergraph construction relies on standard k-hop neighbor methods without novel contributions to brain connectivity modeling (Page 6, Section 3.2)\n  - The multifractal analysis uses established MFDFA techniques without methodological advances (Page 6, Section 3.3)\n  - Progressive diffusion concept is borrowed from existing multi-granularity approaches with limited adaptation to brain-specific requirements (Page 7, Section 3.4)\n  - The integration of multiple conditioning inputs lacks theoretical foundation for why this specific combination is optimal for fMRI generation (Figure 1, Page 5)\n\n‚Ä¢ **Incomplete evaluation of physiological realism**\n  - No direct validation that generated signals preserve actual brain physiological properties beyond multifractal characteristics (Abstract claims realistic signals but provides no physiological validation)\n  - Missing analysis of whether functional connectivity patterns are maintained in generated signals compared to real fMRI data (Claims to model brain regional interactions but doesn't validate preservation)\n  - Lack of clinical or neuroscientific expert evaluation of generated signal quality and biological plausibility (Abstract mentions clinical applications but no clinical validation)\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical rigor and presentation**\n  - Provide complete mathematical derivations for the transition from scale-free properties to self-similarity, particularly justifying the finite dimensional distribution equality in Equation 3\n  - Include theoretical analysis or empirical justification for combining MSE and KL divergence in the fractal loss function, with guidance on hyperparameter selection\n  - Standardize notation throughout the manuscript, clearly distinguishing between different time series representations and their roles\n  - Add mathematical proof or strong empirical evidence for why the proposed conditioning combination is theoretically sound\n\n‚Ä¢ **Expand experimental validation comprehensively**\n  - Validate the approach on additional fMRI datasets beyond HCP to demonstrate cross-dataset generalizability and robustness\n  - Include comparisons with recent fMRI-specific generation methods and more comprehensive diffusion-based time series models\n  - Conduct statistical significance testing using appropriate multiple comparison corrections to establish reliability of performance claims\n  - Incorporate fMRI-specific evaluation metrics such as functional connectivity preservation, network topology maintenance, and physiological signal characteristics\n\n‚Ä¢ **Enhance technical contributions and methodology**\n  - Develop novel hypergraph construction methods specifically tailored to brain connectivity patterns rather than relying on standard k-hop approaches\n  - Propose methodological advances in multifractal analysis for fMRI applications beyond standard MFDFA implementation\n  - Design brain-specific progressive diffusion mechanisms that leverage unique properties of neural signal hierarchies\n  - Provide theoretical framework explaining why the proposed multi-condition integration is optimal for brain signal generation\n\n‚Ä¢ **Validate physiological realism thoroughly**\n  - Implement comprehensive validation of generated signals' physiological properties including spectral characteristics, connectivity patterns, and temporal dynamics\n  - Conduct quantitative analysis of functional connectivity matrix preservation and network topology maintenance in generated versus real fMRI data\n  - Collaborate with neuroscientists or clinicians to provide expert evaluation of generated signal quality and biological plausibility for claimed clinical applications",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript proposes a novel framework, the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion), aimed at synthesizing realistic fMRI signals by integrating brain regional interactions and multifractal dynamics into the diffusion process. The framework consists of a Hypergraph-based Hierarchical Signals Generator, a Dynamics Properties Guiding Module, and a Cross-brain Region Guiding Progressive Diffusion Model. The authors claim that their method produces physiologically realistic fMRI signals, potentially reducing acquisition time and enhancing data quality. The manuscript is well-written and provides a clear rationale for integrating physiological dynamics into the generation process.\n\n## Major Comments\n1. Novelty and Positioning: The proposed method is innovative in its integration of multifractal dynamics and hypergraph-based hierarchical structures. However, the manuscript does not sufficiently distinguish itself from existing works that also incorporate physiological priors or utilize hierarchical structures. A more thorough comparison with related methods is needed to establish the novelty and superiority of the proposed framework.\n\n2. Evaluation Design: The experimental evaluation is conducted exclusively on the HCP dataset, which, while standard, limits the generalizability of the findings. The authors should consider validating their model on additional datasets, including those with different scanning protocols or anatomies, to better demonstrate the robustness and applicability of their approach.\n\n3. Comparisons: The baseline comparisons are somewhat limited, primarily focusing on time series forecasting methods and diffusion models. Including more recent and specialized methods for fMRI synthesis would provide a more comprehensive assessment of the model's performance relative to the state of the art.\n\n4. Reproducibility: While the authors state that the code will be made available, the methodology section lacks some crucial details, such as specific training protocols, hyperparameters, and preprocessing steps. These omissions hinder reproducibility and should be addressed.\n\n## Minor Comments\n1. Figures: Figures 2 and 3 are somewhat cluttered and could benefit from clearer labeling and fewer examples per figure. Including zoomed-in sections or more focused examples would enhance readability.\n\n2. Notation and Terminology: Some notation and terminology are introduced without sufficient explanation, particularly in the Preliminaries section. Definitions and explanations should be provided to ensure clarity for readers unfamiliar with the concepts.\n\n3. Acronyms: Several acronyms (e.g., HCP, rsFC) are used without being defined in the text, which can be confusing for readers. Defining these acronyms upon first mention would improve accessibility.\n\n4. Typographical Errors: There are minor typographical errors throughout the manuscript, such as \"k-spacce\" and \"undersampling maskes,\" which should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in fMRI acquisition by proposing a novel method for generating realistic fMRI signals. The integration of physiological dynamics and hierarchical structures into the diffusion process is technically innovative and has the potential to advance the field. However, the evaluation is limited to a single dataset, and the comparisons lack breadth, which undermines the claims of significance and generalizability. The reproducibility of the approach is also compromised by the lack of detailed methodological information. Overall, while the idea is promising, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis, validate their model on additional datasets, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Wuyang Li",
      "Yixuan Yuan",
      "Yujiang",
      "Yufan Hu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_4e63299434922414da63f4df9505c42a3d79bc3b.pdf",
    "remote_url": "https://openreview.net/pdf/4e63299434922414da63f4df9505c42a3d79bc3b.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Tolga"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "data synthesis",
      "diffusion models",
      "cardiac MRI",
      "lung nodule CT",
      "segmentation"
    ],
    "abstract": "Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR.",
    "decision": "Accept (Spotlight)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a novel 3D lesion inpainting method, LeFusion, which uses diffusion models to address data scarcity in medical imaging. Its primary aim is to generate synthetic lesions in lung CT and cardiac MRI scans for augmenting training data in lesion segmentation tasks. The approach is validated through both visual quality assessments and data augmentation derived segmentation \n\nperformance improvement. Three key contributions can be summarised below: \nLeFusion Model: The authors identify that existing lesion inpainting methods struggle to preserve anatomically accurate backgrounds alongside the inpainted lesion, remarking that modelling the former is both hard and unnecessary. LeFusion is introduced to address this challenge incorporating two distinct features: (a) Training on a lesion focused diffusion loss, which only considers the lesion region. (b) Preserving the background at inference time with RePaint [1] by generating the lesion separately, while integrating forward-diffused background contexts into the reverse diffusion process. This design yields realistic lesions, better preserved backgrounds and improves data augmentation outcomes in both CT and MRI compared to non-lesion-specific models (Cond-Diffusion) both with and without RePaint based sampling. \n\nModality-Specific Variants: Two specialized variants are introduced to address modality-specific challenges. LeFusion-H uses histogram-based conditioning to capture diverse lesion textures in CT, succesfully solving the texture mode collapse observed for the baseline LeFusion. LeFusion-J models multiple tissue subtypes in MRI via multi-channel decomposition, which enables the joint generation of different lesion tissue types typically observed in cardiac lesions. Both variants demonstrate superior data augmentation effectiveness in their respective modalities. \n\nDiffMask for Mask Generation: All variants of LeFusion rely on either existing real masks or handcrafted ones as priors for generating lesions in healthy scans. As a more flexible alternative, DiffMask is a diffusion model that generates synthetic lesion masks from basic spatial constraints, defined as a sphere with user specified location and size. Using the generated masks for data augmentation leads to the largest improvement in segmentation performance relative to the baseline in both CT and MRI.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nLesion generating models are tools with significant potential for mitigating bias in medical vision AI algorithms concerning lesion detection, segmentation and quantification. Advancements in this topic should be highlighted in venues like this. \nThe manuscript is sufficiently well written, all the provided Figures/Tables are insightful and adequately formatted. \nThe choice of a 3D method for this inpainting problem is most adequate for CT and MRI. In these modalities, clinical lesion analysis workflows depend on the visualisation of multiple affected slices and 2D slice-wise inpainting methods would lead to slice-wise discontinuities. \n\nThe proposed method is sufficiently contextualised in the Introduction and Related work sections, where the reseach gap is clearly defined. Beyond that, this gap is empirically demonstrated by experimenting with state-of-the-art approaches (Cond-Diffusion variants and RePaint). \n\nThe proposed methodologies are thoroughly evaluated through comparisons with multiple other approaches, focusing on visual inspection of inpainted lesions (including comparison with real lesions) and their their downstream usability for training segmentation models. The latter evaluation used two different segmentation models, which contributes to the robustness of the findings across different segmentation training strategies. In addition, evaluating the approach on both MRI and CT datasets, ensures that the findings are not only applicable to one imaging domain. \n\nThis paper provides multiple key contributions which not only address the research gap but also deal with modality specific challenges related to lesion texture and shape heterogeneity. The corresponding claims are well supported by the results.\n\n### Weaknesses\n\nWhile S4, the Introduction and Background sections seem to imply that the proposed lesion focused loss is a novel contribution proposed for the first time by the authors. This might not be necessarily true considering that there have been other works that employ similar approaches [2, 3]. While few and perhaps not as thoroughly evaluated, mentioning them could further strengthen the contextualisation of the approach. \n\nThe description of the RePaint method in the experimental section implicitly suggests it consists of Cond-Diffusion using the RePaint [1] inference scheme. If that is the case it should be mentioned explicitly, if not then it should be better described. \nIn the segmentation experiments, it is understood that masks priors for generating lesions in healthy scans (N‚Äô) are either derived from real masks, handcrafted or generated by DiffMask. However, additional information should be provided on how exactly the conditioning histograms in this N‚Äô setting are selected when using LeFusion-H variants. \n\nRegarding DiffMask, the definition and role of boundary mask is not very clear. From Figure 4, it is presumed that it corresponds to the bounding box defining the volume crop centred on the lesion. However, the statement ‚ÄúThe boundary mask removes areas outside the boundary at each diffusion step‚Äù challenges this concept. Further clarity on this point would be appreciated. Furthermore, it is only implicit, that the DiffMask takes the CT/MRI volume crop as an input in addition to the conditioning control sphere. Section 3.3. should be updated to enhance clarity on all these aspects. \n\nAdding supplementary details on how the model training and checkpoint selection was conducted for the RePaint, Cond-Diffusion, Cond-Diffusion (L) would improve transparency. \n \n[Minor]\t \nMore detail on the dataset preprocessing would be beneficial for further reproducibility. A mention to the volume resolution is particularly lacking. \n\nThe choice of the specific crop-size could be further supported on previous work, for instance [4]. In addition, while not critical for acceptance, it would be interesting to study its effect over the results and would maybe answer the question: ‚ÄúHow much local context is it necessary to generate realistic lesion?‚Äù \n\nWhile the purpose of the inpainted lesions is for downstream model training, further validating them using a radiologist would safeguard from potential biases that the generative model might be introducing the lesions. \n\nWhile describing Tables 1 and 2 it would be useful to clarify what is considered as ‚Äúsignificant‚Äù. Since no standard deviations were provided, it is implied that these results were obtained for a single fold, so the concept of significance here is vague. In addition, while S5, the robustness of these findings to the specific data split could still be reinforced by adopting some sort of cross validation strategy. \nThe authors left unclear whether the segmentation model was trained on the volume crops centred on the lesion or on the entire scans. From using the Copy-Paste method in the evaluation, the latter is presumed but it is not explicitly mentioned. \n\nIn the cardiac MRI experiments, the LeFusion baseline of modelling the two lesion tissue types with separate models is mentioned as LeFusion in Table 2 but as LeFusion-S in Figure 5 and in the Appendix. It is suggested that the authors stick to one terminology. \nAs a work mainly focusing on specific diffusion model mechanics for improved lesion inpainting, it makes sense that the evaluation focus on comparing different diffusion based methods. That said, it would still be interesting to see how GAN based approaches like [4, 5] would fair in this comparison. \n \nReferences:  \n[1] Lugmayr, Andreas, et al. \"Repaint: Inpainting using denoising diffusion probabilistic models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. \n[2] Hansen, Colin, et al. \"Inpainting Pathology in Lumbar Spine MRI with Latent Diffusion.\"‚ÄØarXiv preprint arXiv:2406.02477‚ÄØ(2024). \n[3] Rouzrokh, Pouria, et al. \"Multitask brain tumor inpainting with diffusion models: A methodological report.\"‚ÄØarXiv preprint arXiv:2210.12113‚ÄØ(2022). \n[4] Yang, Jie, et al. \"Class-aware adversarial lung nodule synthesis in CT images.\"‚ÄØ2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE, 2019. \n[5] Wu, Linshan, et al. \"FreeTumor: Advance Tumor Segmentation via Large-Scale Tumor Synthesis.\"‚ÄØarXiv preprint arXiv:2406.01264‚ÄØ(2024)\n\n### Questions\n\nFor specific questions please refer to the points made in weaknesses.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph:\nThis manuscript presents a novel 3D lesion inpainting method called LeFusion, utilizing diffusion models to combat data scarcity in medical imaging. The primary objective is to synthesize lesions in lung CT and cardiac MRI scans to enhance training datasets for lesion segmentation tasks. The study is supported by both visual quality assessments and improvements in segmentation performance derived from data augmentation. \n\n### Major Comments:\n1. **LeFusion Model**: The authors note that existing methods fail to preserve anatomically accurate backgrounds while inpainting lesions, introducing LeFusion to tackle this issue through:\n   - Training on a lesion-specific diffusion loss.\n   - Integrating background context during inference via RePaint.\n   This model yields improved realism in lesions and better retention of backgrounds compared to existing methods.\n   \n2. **Modality-Specific Variants**: The introduction of LeFusion-H and LeFusion-J effectively addresses challenges specific to lung CT and cardiac MRI, respectively, enhancing texture diversity and accommodating multiple tissue types.\n\n3. **DiffMask for Mask Generation**: DiffMask provides a flexible alternative for generating synthetic lesion masks; however, specificity on the selection of conditioning histograms for LeFusion-H is needed.\n\n### Minor Comments:\n- The manuscript could benefit from additional details on dataset preprocessing and volume resolution for reproducibility. \n- It would be interesting to explore the effect of crop size on outcomes, as well as to validate inpainted lesions with radiologist input.\n- Clarification on the definitions of \"significant\" results in Tables 1 and 2, as well as the role of the boundary mask in DiffMask, is warranted.\n\n### Summary Paragraph:\nIn relation to significance, the manuscript addresses a critical gap in lesion segmentation, contributing innovative methods that mitigate texture and shape variability. The evaluation is robust through comparisons with established methods across imaging modalities, enhancing reproducibility. Nonetheless, while the core contributions are well-supported, there are areas lacking clarity and context, particularly regarding novel claims and methodological specifics. \n\n### Decision Recommendation:\nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a novel 3D lesion inpainting framework based on diffusion models designed to mitigate data scarcity in medical imaging. The method aims to synthesize realistic lesions in lung CT and cardiac MRI scans for use in lesion segmentation data augmentation. The paper further introduces modality-specific extensions (LeFusion-H for CT and LeFusion-J for MRI) and an auxiliary *DiffMask* model for generating lesion masks from spatial constraints. The manuscript is clearly written, well contextualized in prior work, and supported by comprehensive experiments encompassing both qualitative and quantitative evaluations in two imaging modalities.\n\n---\n\n**Major Comments**  \n1. **Novelty of Lesion-Focused Loss:** The paper suggests that the lesion-focused diffusion loss is novel, though similar concepts have appeared in earlier works. The authors should acknowledge and discuss these related efforts to strengthen the contextual positioning of their contribution.  \n2. **Clarification of RePaint Implementation:** The description of the RePaint method is ambiguous regarding its integration with Cond-Diffusion. The authors should explicitly specify whether it uses the standard RePaint inference scheme or a modified variant.  \n3. **Conditioning Histogram in LeFusion-H:** Additional detail is needed on how histograms are selected for generating lesions in healthy scans (N‚Ä≤) when applying the LeFusion-H variant.  \n4. **DiffMask Explanation:** The role of the boundary mask and whether DiffMask uses image volume crops during inference require clarification. Section 3.3 should clearly define the inputs and processing pipeline.  \n5. **Training and Checkpointing:** Supplementary methodological details on training procedures and checkpoint selection for RePaint, Cond-Diffusion, and Cond-Diffusion(L) would enhance transparency.  \n6. **Experimental Protocols:** Explicitly clarify if segmentation models were trained on lesion-centered crops or entire scans, and justify the crop size selection with reference to prior work.  \n7. **Evaluation Robustness:** Since results lack standard deviations and appear to be from a single fold, claims of ‚Äúsignificance‚Äù need better grounding. Performing cross-validation or providing variance measures would strengthen confidence in performance gains.  \n8. **Terminology Consistency:** In cardiac MRI results, maintain consistent naming for the baseline method (LeFusion vs. LeFusion-S).  \n9. **Comparative Scope:** Although the focus on diffusion models is logical, comparisons to GAN-based lesion synthesis methods (e.g., [4], [5]) would broaden the context and situate the proposed framework more fully.\n\n---\n\n**Minor Comments**  \n- Provide more details on dataset preprocessing, including volume resolution.  \n- Clarify the description of the boundary mask in Figure‚ÄØ4.  \n- Consider radiologist-based evaluation to assess realism and guard against generative bias.  \n- Ensure consistent format of Tables‚ÄØ1‚Äì2 and specify the meaning of ‚Äúsignificant.‚Äù  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper addresses an important and timely problem‚Äîsynthetic lesion generation for improving medical image segmentation‚Äîwith a technically well-motivated diffusion-based approach. It demonstrates solid innovation through lesion-focused modeling and modality-specific adaptations, with convincing empirical validation showing improvements over baselines in both CT and MRI. The study‚Äôs strengths lie in clarity, methodological soundness, and robust two-modality evaluation. However, reproducibility and transparency could be improved with deeper clarifications on training protocols, dataset preprocessing, and evaluation significance. Overall, this work constitutes a meaningful advance in lesion inpainting methodology.\n\n---\n\n**Decision Recommendation:** **Accept (minor revision)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors introduce a latent diffusion model-based method for inserting lesions into healthy medical images while also providing an accompanying mask. They utilize a number of additions to their model to address limitations of prior work or na√Øve approaches to this task (both pre-existing and seemingly novel), such as combining forward-diffused backgrounds with reverse-diffused foregrounds, introducing intensity histogram-conditioning to the diffusion model to control lesion texture, as well as techniques for further control of the shape, size etc. of the generated lesion. They evaluate their method for a variety of experimental scenarios on 3D cardiac MRI lesion and CT lung nodule generation, showing that their technique results in noticeable improvements to existing approaches with respect to using their generated data to train downstream task segmentation models.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\nMajor\n1. The paper is polished, well-written and well-presented. Topics and concepts are organized and presented in a digestible fashion.\n2. Overall, decent technical novelty. This incorporates many techniques which all come together to result in a strongly-performing methods, some pre-existing (such as combined noised backgrounds with denoised foregrounds), and some seemingly novel (such as histogram-based textural control). Also, despite the many components, the approach still seems relatively watertight because these additions are all pretty lightweight/simple (a good thing). No requirement for an additional network or something of that sort.\n3. Overall, results are strong. Clear improvements over baseline methods is basically all cases, using reasonable metrics. They also study a range of training settings, which is good. Clear improvements over Cond-Diffusion, which would be the na√Øve approach that many would think of first trying for this task; the limitations of it as discussed in the introduction are clear from the experiments.\n4. They also have fairly extensive ablation studies for their method, which is important given the number of components that they propose using. There are still a few related questions that I have, but they are minor.\n5. In general, the evaluation is fair and appropriate. The datasets are challenging benchmarks, and I think two is sufficient given the wide range of experiments completed on them. There is also a good number of baseline models, especially considering that this task is relatively niche, so the methodological baselines that they compare to seem strong.\n\nMinor\n1. The motivation for this problem is clear: pathological subjects are indeed rare, especially for screening populations. Your survey of the limitations of existing lesion synthesis approaches also supports the motivation; for example, they result in low quality backgrounds, they lack precise control over generated lesions, etc.\n2. The use of a histogram representation to condition the model on may seem too reductive for some applications, but it seems to work well here (makes sense given the clear correspondence between histogram shape/number of peaks and generated lesion morphology shown in Fig. 3), supported by the clear improvement to your method that including the -H module produced.\n\n### Weaknesses\n\nMajor\n1. Some limitations of impact/scope: This task is clinically important but still fairly niche in medical image analysis, which itself is fairly niche within general machine learning and computer vision. The method (and task itself) also requires that dataset used needs the required annotations, which many medical datasets may not possess, and can be expensive/time-consuming to acquire. Overall, these limit the impact of the work somewhat, in the context of an ML conference at the level of ICLR, compared to a venue a bit more niche like MICCAI.\n\nMinor\n1. The benefits from using multi-channel decomposition (comparing the \"-J\" to no \"-J\" variants of your model in Table 2) are quite small. Can you provide some analysis or discussion of why this is the case, even if just hypothesizing? (However, I am guessing that the computational requirement to adding this component is practically negligible, so there is not really any harm in including it even if it results in only a very small performance improvement.)\n2. You state in the abstract that synthesizing multi-peak and multi-class lesions is a \"major challenge\" I agree with the multi-peak case given how much your histogram-conditioning improved the generation of such lesions, but based on your channel decomposition module's only very small improvements to performance, I'm unsure if generating multi-class lesions could not already be done well by prior methods. Could you clarify this/point to your results that support this, and/or provide quantitative evidence that multi-class synthesis is challenging for prior approaches?\n\nTo summarize, the paper is methodologically solid, with some technical novelty, and demonstrates clear improvements to prior techniques for lesion generation tasks in medical images via well-designed experiments and baselines. However, the main limitation is just that the task is relatively niche within medical image ML, which makes it more niche within general ML, and so may be less impactful at a venue like ICLR as opposed to a medical imaging-focused venue such as MICCAI or MIDL. Still, these limitations do not take away the good things about the paper (of which there are many), so I vote for a marginal accept.\n\n### Questions\n\n1. In the tables (e.g. table 1), what do you mean by the significantly adverse/positive effects denoted by red/blue? Could you please clarify this in the text as well via a small note in the table caption(s)?\n2. My suggestion: move image quality assessment quantitative results in the appendix (Table A2) to the main text if you have room. These are important metrics. You can shorten the related works to make space, that section doesn't need to be quite so extensive (or some of it could be moved to the supplementary).\n    - Also, why didn't you evaluate unpaired perceptual metrics like FID, KID (https://arxiv.org/abs/1801.01401), SWD (https://arxiv.org/abs/1710.10196) etc.? the first two may have limitations for this task given that they use pretrained natural image features, but despite this they are still commonly used metrics for generative medical image models. I would consider adding these for future work, and also explaining why they are not used (particularly for the wider ICLR audience).\n3. For the multiclass lesion case/-J model, did you study how performance/generation quality scales with adding more classes? This point may be a bit moot given how small the changes in performance were measured after adding the channel decomposition module to the base model, but I'm still curious.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel method based on latent diffusion models for inserting lesions into healthy medical images, alongside appropriate masks. The authors introduce several enhancements to their model to overcome the limitations of existing methodologies. Notable innovations include the integration of forward-diffused backgrounds with reverse-diffused foregrounds and an intensity histogram-conditioning technique for controlling lesion texture. The approach is evaluated across various experimental scenarios, specifically focusing on the generation of 3D cardiac MRI lesions and CT lung nodules. Overall, the authors demonstrate significant improvements over existing techniques when using their synthetic data for training segmentation models.\n\n**Major Comments:**  \n1. The manuscript is well-structured and clearly written, presenting topics and concepts in an accessible manner.\n2. The technical novelty is commendable; it successfully integrates a mix of existing and new techniques for a robust model without necessitating complex additional networks.\n3. Results show substantial improvements over baseline methods across various metrics, benefitting from a robust analysis of multiple training settings. The paper effectively highlights the limitations of naive approaches through experimental evidence.\n4. Comprehensive ablation studies lend credence to the proposed method, although some related questions remain minor in nature.\n5. The evaluation framework is appropriate, utilizing challenging benchmark datasets and including a solid range of baseline models, which enhances the findings' reliability.\n\n**Minor Comments:**  \n1. The study's motivation is evident, particularly regarding the scarcity of pathological subjects in screening populations. The authors highlight the limitations of previous synthesis approaches effectively.\n2. The histogram representation approach, while potentially reductive for some applications, demonstrates efficacy within this context, as evidenced by visual results in Figure 3.\n\n**Weaknesses:**  \n1. The clinical relevance of the task may be limited, as it targets a niche area within medical image analysis, which is itself a subset of broader machine learning and computer vision. Furthermore, the requirement for annotated datasets could pose challenges regarding accessibility.\n2. The performance benefits from multi-channel decomposition appear minimal. A discussion or analysis of this observation would enhance clarity, despite negligible computational costs of this addition.\n3. Clarification is needed regarding the complexity of synthesizing multi-class lesions as stated in the abstract, especially in light of the limited performance improvements observed with the channel decomposition module.\n\n**Summary Paragraph:**  \nIn summary, this manuscript is methodologically robust with a notable degree of technical innovation, showcasing clear advancements in lesion generation in medical imaging via well-conducted experiments and thorough baseline comparisons. However, its clinical impact is constrained by the niche nature of the task within medical imaging, which may affect its appeal to broader machine learning conferences such as ICLR. Despite these limitations, the strengths of the work are significant, warranting positive consideration.\n\n**Decision Recommendation:**  \nI recommend a minor revision for the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a diffusion model‚Äìbased approach for realistic lesion insertion into healthy medical images, along with an accompanying lesion mask. The method integrates several architectural and conditioning improvements to overcome the limitations of previous lesion synthesis techniques, including a strategy combining forward-diffused backgrounds with reverse-diffused foregrounds, histogram-based intensity conditioning for texture control, and mechanisms to adjust lesion geometry. The work is evaluated on 3D cardiac MRI and CT lung nodule data, demonstrating consistent gains in generating data that enhance downstream segmentation performance. The paper is clearly written, technically sound, and methodologically coherent.\n\n**Major Comments**  \n1. The method exhibits solid technical novelty, combining known and novel elements (e.g., histogram conditioning) into a cohesive pipeline without added network complexity. The design remains efficient and conceptually clear.  \n2. Experimental results are strong, with consistent improvements across datasets and baselines, supported by appropriate metrics and well-conducted ablation studies.  \n3. The evaluation is comprehensive, covering two challenging benchmarks and multiple baselines, suitable for this task‚Äôs scope.  \n4. A main limitation lies in the work‚Äôs domain specificity. While lesion synthesis is clinically valuable, it represents a relatively narrow niche within medical imaging, limiting impact in broader ML venues. The method also depends on datasets with specific lesion annotations, which may restrict applicability.  \n5. Some clarifications would strengthen the paper‚Äôs presentation: specify the meaning of color-coded significance markers in tables, and explain minor benefits of the multi-channel decomposition (‚Äú‚ÄìJ‚Äù model) and the claim about difficulty of multi-class lesion generation.\n\n**Minor Comments**  \n- Consider moving quantitative image quality scores (Table A2) into the main text, potentially condensing the related works instead.  \n- Briefly justify the non-use of metrics such as FID, KID, or SWD, since readers from broader ML communities may expect them.  \n- Provide a short analysis of how multi-class lesion generation scales with added classes.  \n- Clarify table caption notes regarding red/blue significance indicators.  \n- The histogram-based control module appears effective; the discussion aligns well with visual evidence in Fig. 3.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant for medical image synthesis, presenting an effective diffusion-based lesion generation framework with practical control mechanisms. Innovation lies in the integration of histogram conditioning and the efficient combination of diffusion stages. Evaluation is thorough and demonstrates reproducible improvements across datasets and configurations. While the work‚Äôs impact is tempered by its niche application scope, methodological rigor and reproducibility are strong.\n\n**Decision Recommendation**  \n**Recommendation: Minor Revision (Marginal Accept).**  \nThe paper is technically solid and clearly presented, with convincing results. Minor clarifications and modest reorganization would improve accessibility and completeness for a broader readership.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper focuses on generating lesion-containing images from healthy images to address challenges in downstream segmentation tasks, such as real-world data scarcity and long-tail distribution issues. Previous research on medical image synthesis has primarily concentrated on lesion generation design, often overlooking high-fidelity background preservation. The authors propose a lesion-focused diffusion model, LeFusion, which maintains high-fidelity background by integrating the background from forward diffusion into the reverse diffusion process, thus simplifying the learning process and improving output control. Additionally, two effective strategies are introduced: histogram-based texture control and multi-channel decomposition to address the two main challenges in lesion texture synthesis: 1) multimodal and 2) multiclass lesions. The paper is well-written, with comprehensive experimental comparisons.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The overall paper structure is clear and well-expressed.\n2. A novel diffusion model is redesigned from the perspective of high-fidelity background preservation, with two texture generation control techniques developed to address multimodal and multiclass issues.\n3. The comparative methods are recent benchmarks from the past two years, making the results highly convincing.\n\n### Weaknesses\n\n1.There is a lack of detail on implementation specifics (such as the sampling process) and theoretical support for the method.\n2. Analysis and discussion on the continuity at the fusion boundaries between lesion and background are missing, as well as the impact on downstream tasks.\n\n### Questions\n\n1. The reverse diffusion sampling process is not clearly defined; it appears to rely solely on the transformation in Equation (1), without detailing the sampling process or providing theoretical justification for omitting it.\n2. Although the background from forward diffusion is used as the background in the reverse sampling process, and the loss constraint is applied only to the lesion area, how is continuity and smoothness ensured in the intersecting regions between the lesion and background?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript addresses the generation of lesion-containing images from healthy images, targeting key challenges in segmentation tasks such as data scarcity and long-tail distribution issues. It introduces a novel lesion-focused diffusion model, LeFusion, which integrates high-fidelity background preservation into the diffusion process to enhance output control. The authors further propose two strategies‚Äîhistogram-based texture control and multi-channel decomposition‚Äîto tackle the main challenges in lesion texture synthesis, specifically concerning multimodal and multiclass lesions. The paper is well-structured and includes comprehensive experimental comparisons.\n\n**Major Comments:**  \n1. The paper demonstrates a clear and well-organized structure.\n2. It presents a novel diffusion model with an emphasis on high-fidelity background preservation, which is a fresh perspective in the field.\n3. The comparative analysis employs recent benchmarks from the last two years, supporting the credibility of the results.\n4. There is insufficient detail regarding implementation specifics, such as the sampling process, and a lack of theoretical justification for the method's approach.\n5. The discussion regarding continuity at the boundaries between the lesion and the background is inadequate, particularly concerning its implications for downstream tasks.\n\n**Minor Comments:**  \n- The reverse diffusion sampling process needs clearer definition; it seems to depend on Equation (1) without providing detailed descriptions or theoretical foundation for its omission.  \n- The manuscript does not sufficiently address how continuity and smoothness are maintained in the intersecting areas between the lesion and the background, especially given the loss constraint applied only to the lesion region.  \n\n**Summary Paragraph:**  \nThis review indicates that while the manuscript is significant due to its innovative approach in lesion generation and control techniques, it lacks depth in the evaluation of its methods and reproducibility due to missing implementation details. The strengths, particularly the clarity and recent benchmarking, are notable, but weaknesses regarding theoretical grounding and continuity analysis need addressing to enhance the paper's robustness.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of generating lesion-containing medical images from healthy ones to support downstream segmentation tasks, particularly in situations of limited real-world data and long-tail class distributions. The authors present *LeFusion*, a lesion-focused diffusion model that emphasizes high-fidelity background preservation. By integrating background information from forward diffusion into the reverse diffusion process, the approach simplifies learning and enhances controllability. Two additional strategies‚Äîhistogram-based texture control and multi-channel decomposition‚Äîare implemented to manage multimodal and multiclass lesion texture synthesis. The manuscript is clearly written and provides thorough experimental comparisons.  \n\n**Major Comments**  \n1. **Methodological Detail:** The paper lacks sufficient explanation of implementation details, including the precise formulation of the reverse diffusion sampling process. The method appears to depend primarily on a transformation defined in Equation (1), but theoretical justification or description of the sampling steps is not provided.  \n2. **Theoretical Support:** There is limited discussion supporting the theoretical rationale of the approach. Greater elaboration on how background integration mathematically contributes to fidelity and controllability would strengthen the work.  \n3. **Boundary Continuity:** The continuity and smoothness at the interface between generated lesions and preserved background are not analyzed. The absence of discussion or experimental evidence regarding fusion boundaries raises questions about visual consistency and downstream task performance.  \n4. **Impact on Downstream Tasks:** While lesion generation quality is emphasized, the influence of boundary artifacts or synthesis quality on segmentation outcomes is not deeply discussed.  \n\n**Minor Comments**  \n- Implementation specifics such as sampling settings and parameter values should be clarified for reproducibility.  \n- Minor clarifications on figures and equations would improve comprehensibility.  \n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The work contributes a novel approach to lesion synthesis with potential benefit for medical image segmentation.  \n*Innovation:* The concept of background-preserving diffusion and texture control is original and well-motivated.  \n*Evaluation:* Experiments include recent benchmark comparisons, but discussion of boundary artifacts and downstream implications remains limited.  \n*Reproducibility:* External reproducibility is hindered by incomplete methodological detail and insufficient theoretical clarity.  \n\n**Decision Recommendation:** **Major Revision** ‚Äì The core idea is promising and experimentally well-supported, but improved methodological transparency and a clearer theoretical foundation are needed to ensure scientific rigor and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The paper fits well within *IEEE TMI*‚Äôs methodological scope. It presents a new generative modeling framework (LeFusion) for lesion synthesis in medical imaging, focusing on algorithmic innovation rather than pure application. By modifying diffusion model objectives to isolate lesion synthesis and maintain background fidelity, it contributes to the growing area of generative modeling for medical image augmentation, reconstruction, and fairness ‚Äî all core interests of TMI.\n\n2. **Novelty & Contribution Level**  \nThe paper introduces several methodologically distinct components:\n- Reformulated diffusion process combining forward-diffused background and reverse-diffused lesion inference.\n- A lesion-focused training objective (mask-weighted diffusion loss).\n- Histogram-based texture control for multi-peak lesion synthesis.\n- Multi-channel decomposition for simultaneous multi-class lesion generation.\n- Lesion mask diffusion (DiffMask) for synthesizing controllable lesion masks.\n\nWhile the paper builds conceptually on existing diffusion and inpainting frameworks, its lesion-specific reformulation and multi-level control mechanisms appear novel and well-motivated. The scope is broader than an incremental application; it introduces a coherent set of algorithmic innovations intended to generalize across modalities.\n\n3. **Technical and Experimental Rigor**  \nThe work demonstrates a high level of technical rigor:\n- Mathematical formulations are clearly defined.  \n- Both 3D CT and MRI validation are performed.  \n- Quantitative evaluation includes Dice, NSD, FID/KID/SWD, and PSNR/SSIM, with comparisons to multiple baselines.  \n- Downstream segmentation tasks strengthen the argument for clinical utility.  \nAreas that could be improved:\n  - Reproducibility would benefit from open, verified code links.  \n  - Statistical tests or confidence intervals on segmentation improvements would solidify claims.  \n  - The use of recurrent inference and control parameter selection could be explained theoretically.\n\n4. **Clarity and Presentation**  \nGenerally clear and logically organized, though the paper is dense. The figures (e.g., Fig. 1‚Äì4) are informative but could be simplified for readability. The writing style is occasionally verbose, and consistent terminology (‚ÄúLeFusion-H/J/S‚Äù) could be better introduced. English usage is sound with minor grammatical issues.\n\n5. **Ethical and Reproducibility Compliance**  \n‚úÖ Ethical compliance is satisfactory. Only public datasets (LIDC, Emidec) are used; privacy and IRB concerns are addressed. The reproducibility statement is strong, promising open-source release with detailed documentation.\n\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n**1. Summary**  \nThe manuscript proposes *LeFusion*, a diffusion-based generative framework designed to synthesize realistic lesion-containing image‚Äìmask pairs from normal medical scans. Unlike standard conditional diffusion models that generate entire images, LeFusion focuses the diffusion process on lesion regions while reusing forward-diffused real backgrounds to ensure anatomical consistency. It introduces histogram-based texture control for multi-modal lesion textures, multi-channel decomposition for multiple lesion categories, and a lesion mask diffusion (DiffMask) mechanism enabling control over lesion size and location.  \nThe method is validated on lung CT (LIDC) and cardiac MRI (Emidec), showing improved segmentation performance over conventional synthesis approaches and other diffusion frameworks.\n\n**2. Strengths**\n- Clearly defined methodological novelty beyond prior diffusion/inpainting work.  \n- Comprehensive experimental validation across modalities.  \n- Effective demonstration of downstream benefit with segmentation models (nnUNet, SwinUNETR).  \n- Conceptually elegant solution to background preservation using background-forward diffusion.  \n- Thoughtful design of control mechanisms (texture, mask, class).  \n- Open reproducibility plan and ethical transparency.\n\n**3. Weaknesses**\n- Though novel, the improvements in diffusion formulation remain largely empirical; limited theoretical justification for loss weighting and convergence behavior.  \n- No statistical significance analysis for segmentation improvements.  \n- Comparison limited to diffusion-based baselines; GAN-based or transformer-based generative models might offer complementary benchmarks.  \n- Some sections are repetitive or overly lengthy; a more concise presentation would aid readability.  \n- Qualitative examples mainly show improved realism but lack blinded reader studies.\n\n**4. Major Comments**\n1. **Quantitative robustness** ‚Äì Include confidence intervals or statistical tests (e.g., paired t-tests) on segmentation improvements to validate significance.  \n2. **Ablation justification** ‚Äì While variants (H/J/S) are tested, a structured ablation isolating each design element (histogram control, mask diffusion, multi-channel) would strengthen claims.  \n3. **Theoretical grounding** ‚Äì Provide deeper analysis of why lesion-focused learning improves convergence or fidelity. Discuss trade-offs in model capacity vs. region focus.  \n4. **Comparative scope** ‚Äì Consider discussing performance relative to non-diffusion generative modalities (e.g., GAN or transformer generators) to clarify generalizability.  \n5. **Reproducibility link** ‚Äì Ensure code and pretrained weights are fully accessible with exact experiment scripts before publication.\n\n**5. Minor Comments**\n- Minor typos: e.g., ‚Äúb ackgrounds b y i ntegrating‚Äù (abstract).  \n- Figure 1 labeling could be simplified; arrows and sub-panels could be clearer.  \n- Define abbreviation ‚ÄúNSD‚Äù on first use.  \n- Clarify ‚Äúcontrol sphere‚Äù terminology; consider merging with ‚Äúbounding sphere.‚Äù  \n- Reference formatting: remove redundant ‚ÄúinConference‚Äù artifacts.  \n- Some results tables (Tabs. 1‚Äì2) could use visual grouping or highlight improvements more clearly.\n\n**6. Overall Recommendation:**  \n**Major Revision**  \nThe paper is well-aligned with *IEEE TMI* scope and shows strong potential for publishability. However, to reach TMI‚Äôs methodological and scientific rigor, the authors should better quantify improvement significance, streamline presentation, and add theoretical or statistical validation. With these enhancements, the paper would make a meaningful contribution to generative modeling for medical imaging.\n\n**7. Confidence Level:** 5 (High)\n\n---\n\n**Summary Judgment:**  \nLeFusion offers clear methodological advances in lesion-focused diffusion modeling for medical image synthesis, presenting solid empirical evidence across modalities. With added statistical rigor and improved clarity, it could meet *IEEE Transactions on Medical Imaging*‚Äôs standards for methodological innovation.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses data scarcity and class imbalance in medical AI by proposing LeFusion, a lesion-focused diffusion model for generating synthetic pathological data. The method combines forward-diffused backgrounds with reverse-diffused foregrounds while training only on lesion regions (Figure 1). Key contributions include histogram-based texture control for multi-peak lesions, multi-channel decomposition for multi-class lesions, and lesion mask diffusion (DiffMask) for controlling size, location, and boundary. The approach is validated on lung nodule CT (LIDC dataset) and cardiac lesion MRI (Emidec dataset), demonstrating improvements in downstream segmentation performance with nnUNet and SwinUNETR models. Results show Dice score improvements of up to 5.18% for lung nodules and enhanced performance for cardiac lesions compared to baseline and competing methods (Tables 1-2).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and unclear notation**\n  - Equation (3) uses element-wise multiplication (‚äô) but the dimensions of ot‚àí1 and ÀÜxt‚àí1 are not clearly established to ensure compatibility, particularly in the multi-channel case where dimensions expand to RD√óH√óW√ón (Page 4-5)\n  - The relationship between Œ≤ÃÑt and Œ≤t in Equation (2) is defined inconsistently - Œ≤ÃÑt is defined as ‚àèts=1(1‚àíŒ≤s) but standard notation typically uses ·æ±t for this cumulative product (Page 4)\n  - Equation (7-8) in Appendix C introduces parameters r, s, p, q without proper definition of their values or how they are determined, making the histogram control mechanism theoretically unclear (Page 16)\n\n‚Ä¢ **Inadequate experimental validation and comparison methodology**\n  - The comparison with Cond-Diffusion (L) relies on \"fine-tuned open-source code\" rather than implementing the method under identical conditions, introducing potential implementation bias (Page 8, Section 4.1)\n  - Dataset splits are small with only 10 test cases for cardiac lesions (Emidec), limiting statistical significance of reported improvements (Page 7, Section 4.1)\n  - No statistical significance testing is provided for the reported performance improvements, despite claims of \"significant\" effects marked in blue/red in tables (Tables 1-2)\n  - The evaluation focuses primarily on segmentation downstream tasks but lacks direct assessment of lesion realism through clinical expert evaluation\n\n‚Ä¢ **Technical approach limitations and unclear design choices**\n  - The lesion-focused loss in Equation (4) applies masking only during training but the paper doesn't adequately address how this affects the model's ability to generate coherent lesion-background transitions during inference (Page 5, Section 3.1)\n  - The histogram-based texture control method lacks theoretical justification for why histogram matching should preserve lesion authenticity, and the clustering into three groups (Figure 3) appears arbitrary without validation (Page 5, Figure 3)\n  - Multi-channel decomposition strategy doesn't explain how channel correlations are enforced beyond simple summation, potentially leading to unrealistic multi-class lesion combinations (Page 6, Equation 6)\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and standardize mathematical notation**\n  - Provide explicit dimensional analysis for all tensor operations, particularly showing how ot‚àí1 ‚àà RD√óH√óW√ón and ÀÜxt‚àí1 ‚àà RD√óH√óW√ón are compatible in Equation (3) for both single and multi-channel cases\n  - Use standard diffusion model notation (·æ±t instead of Œ≤ÃÑt) and provide clear definitions of all variance schedule parameters with their typical ranges\n  - Include a complete mathematical derivation of the histogram control mechanism with explicit parameter definitions and their optimization procedure\n\n‚Ä¢ **Strengthen experimental design and validation**\n  - Implement all comparison methods from scratch using identical training conditions, hyperparameters, and computational resources to ensure fair comparison\n  - Expand the cardiac lesion dataset or provide cross-validation results to increase statistical power, and include confidence intervals for all reported metrics\n  - Conduct statistical significance tests (t-tests or Wilcoxon signed-rank tests) for all performance comparisons and remove subjective \"significant effect\" color coding without statistical backing\n  - Include clinical expert evaluation studies to assess lesion realism and diagnostic relevance of synthetic images\n\n‚Ä¢ **Address technical gaps and provide better justification**\n  - Conduct ablation studies showing how the lesion-focused loss affects boundary quality and provide quantitative metrics for lesion-background transition coherence\n  - Validate the histogram clustering approach through medical domain knowledge, compare with established lesion classification schemes, and show that three clusters capture meaningful clinical distinctions\n  - Develop and evaluate explicit correlation modeling mechanisms for multi-channel decomposition, such as attention mechanisms or learned channel interaction layers, with quantitative assessment of multi-class lesion co-occurrence patterns",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces LeFusion, a lesion-focused diffusion model designed to generate high-quality lesion-containing images from lesion-free images. The model aims to address the challenges of data scarcity and long-tail imbalances in medical imaging datasets by integrating lesion-specific texture control and multi-channel decomposition for handling multi-class lesions. The authors validate their approach on 3D cardiac lesion MRI and lung nodule CT datasets, demonstrating improvements in downstream segmentation tasks. The manuscript is well-written and provides a thorough exploration of the proposed method's technical aspects and empirical evaluations.\n\n## Major Comments\n1. Novelty and Positioning: While the manuscript presents a novel approach to lesion synthesis, it is essential to clarify how LeFusion differentiates itself from existing methods. The authors should provide a more detailed discussion on how their method compares conceptually and empirically to recent works on lesion synthesis, particularly those that also use diffusion models or incorporate background preservation techniques.\n\n2. Evaluation Design: The evaluation is primarily conducted on two datasets, LIDC for lung nodules and Emidec for cardiac lesions. While these datasets are standard, the validation could be strengthened by including additional datasets or anatomies to demonstrate the generalizability of LeFusion. Moreover, the inclusion of prospective undersampling scenarios would provide a more robust assessment of the method's practical utility.\n\n3. Comparisons: The manuscript includes comparisons with several baseline methods, including hand-crafted approaches and diffusion-based models. However, it is unclear why certain relevant diffusion-based methods are omitted. Including a broader range of competitive baselines would help establish the relative advantages of LeFusion.\n\n4. Reproducibility: The reproducibility statement mentions that the code and model will be made available. However, the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters, which are crucial for independent reproduction of the results. Providing more comprehensive documentation would significantly enhance the reproducibility of the work.\n\n## Minor Comments\n1. Figure Clarity: Some figures, particularly Figures 3 and 6, are somewhat cluttered. Simplifying these figures by focusing on fewer representative examples and including zoomed-in regions would improve readability.\n\n2. Notation Consistency: The introduction of notation in Section 2.1 lacks sufficient explanation. In particular, the forward operator is introduced inconsistently, which may confuse readers.\n\n3. Acronym Definitions: Several acronyms, such as \"R=4,\" are used without definitions. Explicitly defining these acronyms would aid in comprehension.\n\n4. Typographical Errors: Minor typographical errors exist throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). These should be corrected to maintain clarity.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical imaging: the scarcity and imbalance of pathological data. LeFusion introduces a novel approach to lesion synthesis by focusing on lesion-specific textures and multi-class lesions, which could be a meaningful innovation. However, the evaluation is limited to a narrow set of datasets and anatomies, which reduces the claim of significance and generalizability. The comparison with state-of-the-art methods is thorough but could be expanded to include additional competitive baselines. Reproducibility is addressed, but the manuscript would benefit from more detailed descriptions of the experimental setup and training protocols. Overall, while the idea has promise, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis, broaden the validation across datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Hantao Zhang",
      "Jiancheng Yang",
      "Pascal Fua",
      "Shouhong Wan",
      "Wei Peng",
      "Xinyuan Wang",
      "Yuhe Liu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_c4c499d3163801b947e70b673f9eadca4be34844.pdf",
    "remote_url": "https://openreview.net/pdf/c4c499d3163801b947e70b673f9eadca4be34844.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Medical Image Segmentation",
      "Vision-Language Pre-Training",
      "Zero-Shot Segmentation"
    ],
    "abstract": "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces Malenia, a framework for zero-shot lesion segmentation in 3D medical images, aimed at addressing the challenge of transferring image-level knowledge to pixel-level segmentation tasks. Building on advancements in medical vision-language pre-training, Malenia integrates a multi-scale mask-attribute alignment framework and a Cross-Modal Knowledge Injection module to link visual and textual features, enabling the model to handle previously unseen lesions. The authors evaluate Malenia's performance through experiments on three datasets (two public and one private) across 12 lesion categories.\n\n### Soundness: 2\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\n1. The topic on zero-shot lesion segmentation is valuable, as clinical settings often involve diverse, emerging anomalies, and data collection is challenging, increasing the need for models that handle unseen diseases in an open-set context.\n2. The authors strengthen their claims with both qualitative and quantitative results, enhancing the credibility of their findings and providing a well-rounded evaluation of the proposed approach.\n3. The paper is well-organized, with a clear and logical structure that makes complex concepts accessible and easy to follow for readers.\n\n### Weaknesses\n\n1. The Zero-Shot Inference section lacks sufficient detail on how the model handles test CT images containing unseen tumor types. For instance, if a test image includes both a kidney tumor and a gallbladder tumor, it is unclear how the model predicts these different tumors within the same image. Additionally, the phrase ‚Äúwe obtain the class information for each predicted lesion mask by referencing a clinical knowledge table‚Äù is ambiguous. Further clarification is needed on what this clinical knowledge table entails and how it is used in the inference process.\n2. The comparisons in Table 1 and Table 2 may not be entirely fair. The authors introduce additional ATTRIBUTE DESCRIPTIONS annotations, which provide extra information not available to baseline methods like TransUNet, nnUNet, and Swin UNETR. These baselines rely solely on image data, while the proposed approach leverages attribute annotations, giving it an advantage in performance comparisons.\n3. The fully-supervised performance of baseline methods reported in Table 2 appears unusually low. For instance, in the official nnUNet paper, liver tumor segmentation achieved a Dice score of 76 on the test set, whereas the presented result here is only 61.33. Similarly, lung tumor segmentation originally reported a Dice score of 74, but this paper reports 54.44. This discrepancy raises concerns about the reproducibility and fairness of the comparisons.\n4. In Table 7, the authors compare their method to existing vision-language pretraining strategies, but it is unclear how these methods were reproduced. Most vision-language pretraining approaches require access to diagnostic reports, which are not included in the public MSD and KiTS23 datasets. It would be helpful to understand how the authors conducted these comparisons without complete diagnostic reports. Additionally, the absence of comparisons with important pretraining models like CT-CLIP weakens the evaluation.\n5. Scalability of the proposed method is questionable due to its reliance on ATTRIBUTE DESCRIPTIONS. For practical clinical applications, this approach would require physicians to provide additional lesion descriptions across eight visual attribute aspects, which may not be feasible in all settings. This dependency could limit the method‚Äôs applicability in diverse clinical environments.\n\n### Questions\n\nplease refer to Weaknesses\n\n### Flag For Ethics Review\n\n- Yes, Privacy, security and safety\n\n### Rating: 5\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nThis paper utilizes private clinical data, which may require an ethics review to ensure compliance with privacy, security, and safety standards.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents Malenia, a novel framework for zero-shot lesion segmentation in 3D medical images, addressing the complexities of transferring image-level knowledge to pixel-level tasks. By leveraging advancements in medical vision-language pre-training, Malenia incorporates a multi-scale mask-attribute alignment framework and a Cross-Modal Knowledge Injection module to connect visual and textual features, thus enabling effective segmentation of previously unseen lesions. The authors systematically evaluate Malenia's performance on three datasets, comprising two public and one private, across twelve lesion categories.\n\n### Major Comments\n1. The Zero-Shot Inference section lacks detailed explanations on how the model manages test CT images with different unseen tumor types. For example, the method for segmenting both a kidney and gallbladder tumor in a single image is unclear. Additionally, the reference to a \"clinical knowledge table\" needs further elaboration to clarify its contents and its role in the inference process.\n2. The evaluations in Tables 1 and 2 may not be equitable. The introduction of additional ATTRIBUTE DESCRIPTIONS provides extra insight that baseline methods such as TransUNet, nnUNet, and Swin UNETR lack, as they rely solely on image data.\n3. The reported fully-supervised performance of baseline methods in Table 2 appears to be lower than expected based on previous literature, which raises concerns regarding reproducibility and fairness in these comparisons. For instance, the nnUNet reported a Dice score of 76 for liver tumor segmentation, while this manuscript indicates only 61.33.\n4. In Table 7, comparisons to existing vision-language pretraining strategies are presented without clear information on how these methods were reproduced, especially considering that most require access to diagnostic reports not available in public datasets. Additionally, not comparing against key models like CT-CLIP weakens the evaluation.\n5. The scalability of the method is questionable, given its reliance on ATTRIBUTE DESCRIPTIONS. Requiring physicians to provide detailed lesion descriptions may not be practical in all clinical settings.\n\n### Minor Comments\n- Clarification is needed regarding the terminology and acronyms utilized throughout the paper.\n- Consider proofreading for typographical errors to enhance overall quality.\n\n### Summary Paragraph\nIn summary, while Malenia offers significant innovation in zero-shot lesion segmentation and presents credible evaluations, there are notable concerns related to reproducibility and fairness in comparisons. The reliance on additional annotations, as well as potential challenges in practical scalability and the need for further clarity in technical explanations, may hinder its applicability. Addressing these critiques related to significance, innovation, evaluation, and reproducibility could strengthen the manuscript considerably.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a framework for zero-shot lesion segmentation in 3D medical images. The method aims to bridge the gap between image-level and pixel-level knowledge transfer by integrating a multi-scale mask‚Äìattribute alignment framework and a Cross-Modal Knowledge Injection module that align visual and textual representations. Experiments spanning three datasets (two public, one private) and twelve lesion categories support the evaluation. Overall, the topic is timely and relevant for open-set medical imaging; the paper is clearly organized and easy to follow.\n\n**Major Comments**  \n1. **Zero-shot inference details:** The description of how the model processes test CT images containing multiple unseen tumor types is insufficient. It remains unclear how predictions are generated when several lesion classes (e.g., kidney and gallbladder tumors) appear together. The statement about obtaining ‚Äúclass information ‚Ä¶ by referencing a clinical knowledge table‚Äù lacks clarity‚Äîfurther explanation of the table‚Äôs contents and role in inference is required.  \n2. **Fairness of comparisons:** In Tables‚ÄØ1‚Äì2, the method benefits from additional ATTRIBUTE DESCRIPTIONS annotations not accessible to baseline models (TransUNet, nnUNet, Swin‚ÄØUNETR). This added information may bias results, compromising fairness of comparisons.  \n3. **Baseline performance discrepancies:** Reported Dice scores for supervised baselines appear lower than expected (e.g., nnUNet liver tumor Dice‚ÄØ=‚ÄØ61‚ÄØvs‚ÄØ76 in the original paper; lung tumor‚ÄØ=‚ÄØ54‚ÄØvs‚ÄØ74). These inconsistencies raise concerns about reproducibility and evaluation fairness.  \n4. **Vision‚Äìlanguage pretraining comparisons:** Table‚ÄØ7 includes comparisons to other vision‚Äìlanguage pretraining methods, but reproducibility is uncertain because those methods usually rely on diagnostic reports absent in the public datasets used. Clarification is needed on how such comparisons were performed, and the omission of relevant approaches (e.g., CT‚ÄëCLIP) weakens completeness.  \n5. **Scalability and practicality:** Dependence on ATTRIBUTE DESCRIPTIONS, requiring physicians to specify multiple lesion attributes, may limit clinical feasibility and scalability across diverse settings.\n\n**Minor Comments**  \n- None beyond clarification issues noted above.  \n- Ensure all tables clearly specify data sources and annotation types.\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the topic is important for handling unseen clinical anomalies. Regarding **innovation**, the integration of visual‚Äìtext alignment and cross-modal knowledge components is conceptually sound but may depend heavily on manual attribute annotations. The **evaluation** presents quantitative and qualitative results but lacks clarity on fairness and reproducibility. Concerning **reproducibility**, baseline discrepancies and limited description of pretraining comparisons reduce confidence in the reported performance.\n\n**Decision Recommendation**  \n**Major Revision.** The paper is promising and well presented but requires clarifications on zero-shot inference design, fair comparison methodology, and reproducibility to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces Malenia, a novel framework specifically designed for 3D zero-shot lesion segmentation in medical images. The authors aim to address the challenges of transferring image-level knowledge to pixel-level tasks‚Äîsuch as lesion segmentation‚Äîby proposing a multi-scale lesion-level mask-attribute alignment approach. Malenia improves the alignment between the visual features of unseen lesions with textual representations, leveraging Cross-Modal Knowledge Injection (CMKI) to enhance both visual and textual embeddings. Experimental results across three datasets demonstrate that Malenia outperforms state-of-the-art (SOTA) methods in zero-shot lesion segmentation.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed Malenia framework is novel, particularly in its use of multi-scale mask-attribute alignment for 3D lesion segmentation. This is a meaningful extension of zero-shot segmentation methods to the medical domain, where unseen lesion categories are common (e.g., \"hepatocellular carcinoma,\" mentioned in Sec. 1).\n2. The CMKI module is a strong contribution because it combines visual and textual embeddings, enriching the segmentation prediction with complementary information from both modalities. This is a novel approach that enhances the zero-shot capability of the model.\n3. The paper introduces a multi-positive contrastive loss to fine-tune the alignment between lesion masks and textual attributes. This approach allows the model to generalize better to unseen lesions by learning from shared visual attributes, which distinguishes it from prior works like SAM (Kirillov et al., 2023).\n\n### Weaknesses\n\n1. The method relies on transforming patient reports into structured descriptions of eight visual attributes, which requires expert radiologist involvement. This process may not be scalable or practical in real-world applications where such annotations are not readily available.\n2. As noted in Appendix D, the model performs less effectively on attributes requiring complex visual semantic understanding, such as \"Surface Characteristics\" and \"Specific Features\". This suggests a potential limitation in capturing intricate visual patterns.\n3. While the model shows strong performance on 12 lesion categories, the diversity of lesions in clinical practice is vast. The ability of Malenia to generalize to a wider range of unseen lesions without additional training remains uncertain.\n\n### Questions\n\n1. In Section 3.1, the authors employ the Multi-Positive InfoNCE (MP-NCE) loss for aligning mask embeddings with multiple attribute embeddings. Could the authors provide more details on how the MP-NCE loss is computed and how it facilitates learning with multiple positive pairs? Specifically, how are the positive and negative pairs defined in the context of multiple attributes?\n   Could the authors explain how this formulation effectively handles multiple positive pairs for each mask token?\n2. The model uses a fixed number of mask tokens ($N=16$) and defines $R=8$ attribute aspects. Have the authors conducted any experiments to assess how varying $N$ or the number of attributes affects the model's performance? An ablation study on these hyperparameters could provide insights into the model's sensitivity and optimal settings.\n3. As noted in Appendix D, the model performs less well on attributes like \"Surface Characteristics\" and \"Specific Features\". What are the potential reasons for this lower performance? Could incorporating additional contextual information or advanced features help the model better capture complex visual semantics?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents Malenia, a novel framework aimed at advancing 3D zero-shot lesion segmentation in medical imaging. It addresses the challenge of transferring image-level knowledge to pixel-level tasks through a multi-scale mask-attribute alignment approach. The authors introduce a Cross-Modal Knowledge Injection (CMKI) mechanism to enhance the congruence between visual features of unseen lesions and their textual representations. Experimental results from three datasets indicate that Malenia outperforms existing state-of-the-art methods in zero-shot lesion segmentation.\n\n### Major Comments\n1. **Novelty**: The framework introduces a unique multi-scale mask-attribute alignment for 3D lesion segmentation, marking a significant addition to zero-shot segmentation methods in the medical field, particularly for common unseen lesion types (e.g., \"hepatocellular carcinoma\").\n2. **CMKI Contribution**: The integration of visual and textual embeddings via the CMKI module is a strong contribution that augments the segmentation prediction with complementary data, enhancing the model's zero-shot capabilities.\n3. **Multi-Positive Contrastive Loss**: The introduction of a multi-positive contrastive loss fine-tunes the mask-attribute alignment, helping the model generalize to unseen lesions utilizing shared visual attributes, setting it apart from prior models like SAM (Kirillov et al., 2023).\n\n### Minor Comments\n- The reliance on structured descriptions of visual attributes necessitates expert input, which may limit the practical application of the method.\n- The model's performance diminishes on complex visual attributes (e.g., \"Surface Characteristics\"), indicating possible limitations in understanding intricate visual cues.\n- Although the model excels in 12 lesion categories, its generalizability to a wider array of lesions in clinical practice remains questionable without additional training.\n\n### Summary Paragraph\nThe manuscript highlights several strengths against the TMI editorial criteria. It demonstrates significance through its innovative framework for lesion segmentation and shows promise in evaluation via experimental results. However, concerns about reproducibility arise due to reliance on expert radiologist input and potential limitations in performance with complex visual attributes. The generalizability of Malenia beyond the tested categories also poses a challenge.\n\n### Decision Recommendation\nMinor revision suggested, addressing clarity in the methodology and enhancing insights on performance generalizability.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a novel framework for 3D zero‚Äëshot lesion segmentation in medical imaging. The method aims to bridge image‚Äëlevel knowledge and pixel‚Äëlevel segmentation tasks through a multi‚Äëscale lesion‚Äëlevel mask‚Äìattribute alignment strategy. Central to the approach is the Cross‚ÄëModal Knowledge Injection (CMKI) module, which jointly optimizes visual and textual embeddings to enhance feature alignment. Experiments on three datasets show that Malenia consistently outperforms state‚Äëof‚Äëthe‚Äëart zero‚Äëshot segmentation methods. Overall, the paper is clearly written, the motivation is well articulated, and the contributions are substantial for the medical imaging community.  \n\n**Major Comments**  \n1. **Annotation scalability:** The method depends on converting patient reports into structured descriptions of eight visual attributes. This step requires expert radiologist input, which may limit scalability and real‚Äëworld applicability where such detailed textual annotations are scarce.  \n2. **Attribute‚Äëlevel performance:** According to Appendix D, the approach struggles on attributes that involve complex visual semantics (e.g., ‚ÄúSurface Characteristics‚Äù and ‚ÄúSpecific Features‚Äù). This limitation indicates that the model may not fully capture intricate morphological patterns.  \n3. **Generalization scope:** Although validated on 12 lesion categories, the diversity of clinical lesions is far greater. The generalization potential of Malenia to unseen lesion types without retraining remains unclear.  \n4. **Technical clarification:** Section 3.1 introduces the Multi‚ÄëPositive InfoNCE (MP‚ÄëNCE) loss for aligning mask and attribute embeddings. Further explanation is needed regarding the computation of MP‚ÄëNCE, definition of positive and negative pairs, and how multiple positives per mask token are managed.  \n5. **Hyperparameter sensitivity:** The model uses a fixed number of mask tokens (N‚ÄØ=‚ÄØ16) and attributes (R‚ÄØ=‚ÄØ8). It would be valuable to include an ablation examining how varying these parameters influences performance.\n\n**Minor Comments**  \n- Clarify the lower performance for certain attributes‚Äîcould additional contextual or semantic cues help?  \n- Ensure consistent terminology and notation when referring to attributes and mask tokens.  \n- Figures and appendices could better highlight where CMKI provides the largest gains.\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in addressing zero‚Äëshot segmentation for 3D medical images, a challenging and under‚Äëexplored area. Its **innovation** lies in combining multi‚Äëscale alignment and cross‚Äëmodal knowledge injection. The **evaluation** is convincing, covering multiple datasets and comparisons with SOTA models, though further ablations would strengthen the analysis. Regarding **reproducibility**, methods and datasets are described clearly, but the reliance on expert‚Äëderived textual attributes may hinder full replicability in practical settings.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nThe paper is innovative and promising but requires additional clarification on loss formulation, ablation analyses, and discussion of attribute scalability before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work presents a zero-shot medical image semantic segmentation framework that is based on maskformer and attribute-based visual-language feature fusion. To achieve fine-grain alignment between visual and textual concepts, the authors propose to decompose radiological reports into attributes describing lesions of interest, assisted by LLMs and human annotators. The obtained attributes allows fine-grain fusion and alignment between lesion features and textual concepts, yielding improved generalization performance upon unseen lesions. The proposed framework is evaluated on tumor segmentation tasks, and demonstrates improved segmentation results compared with existing zero-shot segmentation approaches on unseen lesions.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe idea of decomposing free-text reports into attributes for fine-grain training and inference is intuitive and feasible. It can significantly reduce the noise and ambiguity associated with unstructured free-text reports and yields improved performance on unseen lesions.\n\nThe key components and the detailed implementations are described in good detail. \n\nThe proposed approach demonstrates performance improvements compared with previous zero-shot image segmentation methods.\n\nDetailed ablation studies are presented to verify the effectiveness of key components.\n\n### Weaknesses\n\nDespite stronger segmentation performance, the major framework looks similar to ZePT (Jiang et al., 2024): maskformer backbone + interacting visual and textual features that are obtained from detailed description of the lesions. Therefore, the key take-home message for readers may be a bit unclear: should the readers interpret the key technical contribution as explicitly decomposing textual descriptions/reports into categorized attributes for the ease of learning and inference?\n\nReaders may argue that using GPT-4 for attribute construction from radiological reports is often infeasible in practice due to the privacy and legal concerns. The authors are encouraged to argue if switching to local open-source LLMs would yield a similar level of performance gain. \n\nThe authors may want to decompose some of technical details in Figure 1 into separate figures and move them closer to corresponding paragraphs.\n\n### Questions\n\nConsidering the practicality issue, would replacing GPT-4 with an open-source LLM for attribute construction (probably also without human annotators?) reaching a similar level of performance gain (as mentioned above)?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript introduces a zero-shot medical image semantic segmentation framework that utilizes MaskFormer and integrates attribute-based visual-language feature fusion. The authors aim to enhance fine-grain alignment between visual and textual concepts by decomposing radiological reports into attributes that describe lesions, leveraging the support of large language models (LLMs) and human annotators. Their approach achieves improved segmentation performance on unseen lesions in tumor segmentation tasks compared to existing zero-shot methods.\n\n### Major Comments\n1. **Novelty and Contribution**: While the framework shows improved segmentation results, it bears similarities to the ZePT model by Jiang et al. (2024), which also combines a MaskFormer backbone with interacting visual and textual features. This resemblance raises questions about the distinct contributions of the current work, particularly regarding how the decomposition of textual descriptions into categorized attributes serves the framework.\n2. **Evaluation Design**: The authors should clarify whether using GPT-4 for attribute construction is practical due to privacy and legal concerns. An exploration of the potential use of local open-source LLMs as an alternative and its impact on performance would strengthen the evaluation.\n3. **Technical Clarity**: Some details in Figure 1 could be better presented by decomposing them into separate figures that are positioned closer to their respective discussion sections to enhance clarity.\n\n### Minor Comments\n- Consider addressing minor typos throughout the manuscript for improved clarity.\n- The title could benefit from additional specificity to reflect the innovative aspects of the framework.\n\n### Summary Paragraph\nThis study makes a notable contribution to medical image segmentation by presenting an innovative framework that enhances performance on unseen lesions through attribute-based feature fusion. However, the overlap in approach with existing models raises questions regarding the distinctiveness of its contributions. While the representation of results is sound and the implementation details are adequately described, the practical implications of utilizing GPT-4 warrant further discussion. Overall, the manuscript exhibits strengths in its methodologies but requires clarifications that address the novelty and practicality of its methods.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a zero-shot medical image semantic segmentation framework based on a MaskFormer architecture combined with attribute-based visual‚Äìlanguage feature fusion. Radiology reports are decomposed into attribute sets describing lesions, assisted by large language models (LLMs) and human annotators, to enable fine-grained alignment between visual and textual concepts. The method is evaluated on tumor segmentation tasks, demonstrating improved performance on unseen lesions compared with existing zero-shot segmentation approaches. The paper is clearly written, with sufficient technical detail and a convincing set of experiments.\n\n**Major Comments**  \n1. **Novelty and Relation to Prior Work:** While performance improvements are shown, the proposed framework appears conceptually similar to ZePT (Jiang et al., 2024), which also combines MaskFormer with interacting visual‚Äìtextual features derived from detailed lesion descriptions. The distinctive technical contribution relative to ZePT‚Äînamely, the explicit decomposition of textual descriptions into categorized attributes‚Äîshould be further clarified to strengthen the paper‚Äôs novelty.  \n2. **Practicality and Feasibility:** The use of GPT‚Äë4 for attribute extraction from radiological reports raises feasibility and privacy concerns in real-world clinical deployment. The authors should discuss whether local or open-source LLMs could replicate comparable performance and how privacy issues might be mitigated.  \n3. **Evaluation and Presentation:** Figure‚ÄØ1 condenses several important technical components, and decomposition into multiple figures aligned with the text would improve clarity. More explicit discussion of how attribute extraction contributes quantitatively to zero-shot generalization would also enhance the evaluation section.  \n4. **Open Question:** Could an open-source LLM‚Äîpossibly without human annotators‚Äîachieve similar performance in attribute extraction? Addressing this experimentally or conceptually would improve completeness.\n\n**Minor Comments**  \n- The manuscript‚Äôs structure and writing are clear, though figure placement could better correspond to the narrative flow.  \n- Verify consistent terminology for ‚Äúattributes,‚Äù ‚Äúfeatures,‚Äù and ‚Äúdescriptors‚Äù to avoid confusion.  \n- Minor typographical issues are minimal.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses a significant challenge in zero-shot medical image segmentation, offering a method that leverages textual attribute decomposition for better generalization. Its innovation lies in aligning decomposed textual features with visual representations. The evaluation is convincing with strong ablations, though differentiation from prior work could be clearer. Reproducibility is supported through detailed implementation, but reliance on GPT‚Äë4 introduces practical limitations that may hinder adoption.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a new methodological framework ‚Äî *Malenia* ‚Äî for **3D zero‚Äëshot lesion segmentation**, combining cross‚Äëmodal representation learning, multi‚Äëscale alignment, and a novel cross‚Äëmodal fusion (CMKI) mechanism. The contribution squarely targets core topics of *IEEE Transactions on Medical Imaging*: imaging informatics, segmentation methodology, and cross‚Äëmodal learning in radiology.\n\n2. **Novelty & Contribution Level**  \nThe paper claims two central technical novelties:  \n- **Multi‚Äëscale mask‚Äëattribute alignment** bridging global image‚Äìtext pre‚Äëtraining and dense voxel‚Äëlevel prediction.  \n- **Cross‚ÄëModal Knowledge Injection (CMKI)** to jointly update mask and text embeddings.  \nWhile the work builds on existing transformer backbones (Mask2Former) and CLIP‚Äëstyle pre‚Äëtraining concepts, the formulation of *multi‚Äëpositive contrastive alignment using structured radiology attributes* is original and represents a meaningful extension beyond prior ZePT or SAM‚Äëbased pipelines. Novelty is moderate‚Äëto‚Äëhigh for TMI, assuming reproducibility and dataset access are adequate.\n\n3. **Technical and Experimental Rigor**  \nExperiments are comprehensive: three datasets (MSD, KiTS23, and a private cohort, totaling 12 lesion types) and extensive ablations, including attribute construction, CMKI design, hyper‚Äëparameter studies, and comparisons to multiple baselines. The methodology is mathematically consistent. However:  \n- The reproducibility of the *semi‚Äëautomatic attribute extraction* pipeline using GPT‚Äë4 plus radiologist correction could be better documented (e.g., inter‚Äërater agreement, automation error rate).  \n- Public availability of the processed attribute annotations is only promised, not confirmed.  \n- Assertions regarding clinical interpretability and ‚Äúdiagnostic precision‚Äù might be slightly overstated without expert reader studies.\n\n4. **Clarity and Presentation**  \nOverall well organized, technically clear, and grammatically sound. Figures are dense but informative; minor language editing would improve readability (inconsistent capitalization and spacing, e.g., ‚ÄúZ ERO -SHOT,‚Äù ‚ÄúP ancreas‚Äù). The manuscript length is high but proportional to complexity.\n\n5. **Ethical and Reproducibility Compliance**  \nEthics statements for the private dataset are provided; anonymization appears sound. Code release is stated publicly on GitHub but anonymized for review. For TMI, explicit confirmation that IRB approval and data‚Äësharing guidelines will be satisfied is necessary.\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n1. **Summary**  \nThe authors propose *Malenia*, a **vision‚Äëlanguage pre‚Äëtraining framework** enabling zero‚Äëshot 3D lesion segmentation. The approach integrates structured radiology attributes (e.g., shape, density, enhancement status) into a **multi‚Äëscale mask‚Äëattribute alignment** loss and introduces a **Cross‚ÄëModal Knowledge Injection (CMKI)** module that fuses fine‚Äëgrained visual and textual embeddings to improve segmentation of both seen and unseen diseases. Evaluation on multiple public and private CT datasets demonstrates superior Dice and NSD scores compared with SAM‚Äëbased and prior vision‚Äëlanguage segmentation models.\n\n2. **Strengths**\n- Addresses an important and underexplored problem: zero‚Äëshot lesion segmentation in 3D medical imaging.  \n- Methodologically innovative: multi‚Äëpositive contrastive learning using structured attribute tokens, and bi‚Äëdirectional feature fusion.  \n- Extensive quantitative and qualitative validation, including ablations and fine‚Äëtuning analyses.  \n- Strong empirical improvement over state‚Äëof‚Äëthe‚Äëart baselines.  \n- Clear ethical protocol and commitment to releasing attribute annotations.\n\n3. **Weaknesses**\n- The reliance on proprietary LLMs (GPT‚Äë4) for attribute extraction limits reproducibility; minimal quantification of annotation accuracy.  \n- The architecture largely inherits from Mask2Former/nnUNet; theoretical justification for losses and hyperparameters is empirical rather than analytical.  \n- Discussion of clinical utility remains speculative‚Äîno reader study or uncertainty quantification.  \n- Minor clarity issues and formatting inconsistencies.  \n\n4. **Major Comments**\n1. **Reproducibility of Attribute Generation:** Provide quantitative assessment of LLM‚Äëbased attribute extraction (e.g., consistency with radiologist edits, Œ∫‚Äëscores) and describe how these structured attributes might generalize across institutions.  \n2. **Statistical Validation:** Include significance testing for Dice/NSD improvements to substantiate ‚Äústate‚Äëof‚Äëthe‚Äëart‚Äù claims.  \n3. **Ablation Transparency:** Clarify the relative contribution of CMKI vs. multi‚Äëpositive loss on the same training split; consider a formal cross‚Äëvalidation summary instead of selective examples.  \n4. **Clinical Interpretability:** The claim that Malenia ‚Äúidentifies specific disease categories via a knowledge table‚Äù borders on diagnostic inference; specify that this is attribute‚Äëdriven label matching, not verified clinical diagnosis.  \n5. **Open‚ÄëSource and Data Access:** Specify timelines and scope of planned code and attribute releases to meet IEEE data‚Äëavailability expectations.\n\n5. **Minor Comments**\n- Normalize typographic artifacts (e.g., hyphenation, mixed uppercase).  \n- Improve figure readability‚ÄîFigure‚ÄØ1 and‚ÄØ2 contain small fonts.  \n- Provide network parameter counts per module in supplemental for clarity.  \n- Verify that all references cited as 2024 preprints are in acceptable public form.\n\n6. **Overall Recommendation**  \n**‚≠ë Major Revision** ‚Äì The manuscript exhibits strong methodological novelty and promising results, aligning well with TMI's scope. However, additional substantiation of reproducibility, statistical rigor, and clearer separation of methodological versus application claims are required before acceptance.\n\n7. **Confidence Level:** **4‚ÄØ/‚ÄØ5** ‚Äì High confidence in the technical evaluation, moderate uncertainty about dataset and annotation reproducibility.\n\n---\n\n**Summary Judgment:**  \n*Malenia* represents a creative and potentially influential step toward cross‚Äëmodal zero‚Äëshot segmentation in 3D medical imaging. With improved documentation of its annotation pipeline, quantitative statistical analyses, and clarified claims, the work could merit publication in *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents Malenia, a vision-language pre-training framework for 3D zero-shot lesion segmentation in CT scans. The method addresses the challenge of transferring image-level knowledge to pixel-level segmentation tasks by introducing multi-scale mask-attribute alignment and a Cross-Modal Knowledge Injection (CMKI) module. The approach decomposes medical reports into eight structured visual attributes (location, shape, density, etc.) and aligns these with mask representations at multiple scales using a multi-positive contrastive loss. The CMKI module enhances both visual and textual features through cross-attention mechanisms. Experiments across three datasets (MSD, KiTS23, in-house) and 12 lesion categories demonstrate superior performance compared to existing methods, with improvements of 6.40% DSC on MSD, 8.14% on KiTS23, and 9.08% on the in-house dataset over the best competing method.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Equation (2) defines the multi-positive contrastive loss but lacks clear explanation of how the expectation over positive pairs is computed in practice (Section 3.1, Page 5), which affects reproducibility\n  - The notation S(m_j, t_k) = m_j¬∑t_k/œÑ assumes normalized embeddings but this normalization is not explicitly stated (Section 3.1, Page 5), creating ambiguity in implementation\n  - The scaling factor ‚àöC in Equation (5) lacks justification for why this specific normalization is chosen over alternatives (Section 3.2, Page 6), undermining theoretical grounding\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - The private dataset contains only 30 cases per lesion type (Table 5, Page 16), which is insufficient for robust evaluation of zero-shot generalization capabilities\n  - Cross-validation methodology is inconsistent across datasets, with 5-fold cross-validation only reported for seen lesions (Table 2, Page 9) but not for unseen lesions, limiting statistical reliability\n  - The comparison with SAM-based methods may be unfair as they require manual prompts during testing (Table 1, Page 8), while Malenia uses stored embeddings from training, creating different evaluation conditions\n\n‚Ä¢ **Technical approach and novelty concerns**\n  - The multi-scale alignment strategy lacks theoretical justification for why three specific resolution levels (H/32, W/32, D/32) to (H/8, W/8, D/8) are optimal (Section 3.1, Page 4), appearing arbitrary\n  - The eight visual attributes are derived from expert consultation but lack systematic validation or comparison with alternative attribute taxonomies (Section 3.1, Page 4), questioning their generalizability\n  - The CMKI module design shows limited novelty as it primarily combines standard cross-attention and self-attention mechanisms (Equation 3, Page 6) without significant architectural innovation\n\n‚Ä¢ **Data dependency and generalization issues**\n  - The method requires extensive manual annotation by radiologists to create structured attribute descriptions (Section B.2, Page 16), which may not scale to diverse clinical settings\n  - The clinical knowledge table used during inference (Table 11, Page 25) appears manually crafted and disease-specific, potentially limiting generalization to new lesion types not covered in the predefined taxonomy\n  - The semi-automatic pipeline using GPT-4 for attribute extraction introduces dependency on proprietary models (Section 3.1, Page 5), raising concerns about reproducibility and accessibility\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and implementation clarity**\n  - Explicitly state the normalization requirements for embeddings in the similarity computation and provide pseudocode for the multi-positive contrastive loss implementation\n  - Justify the choice of ‚àöC scaling factor by comparing with alternative normalization schemes (e.g., temperature scaling, learnable parameters) in ablation studies\n  - Include detailed derivations or references for the mathematical formulations to ensure reproducibility and theoretical soundness\n\n‚Ä¢ **Strengthen experimental validation and statistical rigor**\n  - Increase the private dataset size to at least 100 cases per lesion type or provide power analysis justifying the current sample size for meaningful statistical conclusions\n  - Apply consistent cross-validation methodology across all experiments, including confidence intervals and statistical significance tests for performance comparisons\n  - Conduct fair comparisons by either providing manual prompts for SAM-based methods or developing prompt-free versions of competing approaches\n\n‚Ä¢ **Provide systematic justification for technical choices**\n  - Conduct ablation studies on different numbers of resolution levels and specific scale choices to demonstrate the optimality of the current multi-scale design\n  - Compare the proposed eight-attribute taxonomy with alternative attribute sets through systematic evaluation or provide theoretical frameworks for attribute selection\n  - Benchmark the CMKI module against other cross-modal fusion strategies (e.g., early fusion, late fusion, attention-based fusion) to demonstrate its superiority\n\n‚Ä¢ **Address scalability and accessibility limitations**\n  - Develop automated or semi-automated methods for attribute extraction that reduce dependency on manual annotation and proprietary models\n  - Create a systematic framework for expanding the clinical knowledge table to new lesion types, potentially using hierarchical or embedding-based approaches\n  - Provide open-source alternatives to GPT-4 for attribute extraction and validate their effectiveness in maintaining segmentation performance",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces Malenia, a novel framework for zero-shot 3D lesion segmentation that leverages vision-language pre-training and incorporates a multi-scale mask-attribute alignment strategy. The proposed method bridges the gap between global image-level representations and fine-grained pixel-level segmentation tasks by aligning lesion mask representations with elemental disease attributes extracted from structured medical reports. Additionally, Malenia introduces a Cross-Modal Knowledge Injection (CMKI) module to enhance both visual and textual feature representations, further refining the segmentation process. The paper is well-written, and the authors provide comprehensive experimental results across three datasets, demonstrating the superior performance of Malenia compared to existing methods.\n\n## Major Comments\n1. Novelty and Positioning: While the multi-scale mask-attribute alignment and the CMKI module are innovative contributions, the manuscript needs to more clearly differentiate itself from recent advancements in vision-language pre-training for medical image segmentation. There is a risk that the novelty might be overshadowed by the existing literature on similar approaches. The authors should emphasize the unique technical advancements and empirical improvements provided by Malenia over existing methods.\n\n2. Evaluation Design: The evaluation is conducted on three datasets, which is commendable. However, the datasets primarily consist of specific lesion types (e.g., kidney tumors, liver cysts), and the evaluation could benefit from a broader range of lesion types and anatomical regions to validate the model's generalizability. Additionally, the inclusion of cross-modality evaluations (e.g., MRI) would strengthen the claim of the model's versatility and applicability to different imaging modalities.\n\n3. Comparisons: The comparison against existing methods is thorough, but the manuscript should include a more detailed discussion of the limitations and challenges faced by competing methods, particularly in terms of their inability to handle unseen lesion types effectively. This would help highlight the advantages of Malenia more clearly.\n\n4. Reproducibility: The authors mention that code will be made available, which is good. However, the manuscript should provide more explicit details regarding the training protocols, hyperparameters, and preprocessing steps. This would ensure that the results can be independently reproduced and validated by other researchers.\n\n## Minor Comments\n1. Figure 3: The figure is somewhat cluttered. Showing fewer representative slices with zoomed-in regions would improve readability.\n   \n2. Section 3.1: The introduction of notation in the multi-scale mask-attribute alignment section is somewhat lacking in clarity. Additional explanations of the forward operator and other key notations would be beneficial.\n   \n3. Acronyms: Some acronyms (e.g., \"R=4\") are used without definition, which can be confusing for readers who are not familiar with the terminology.\n   \n4. Typographical Issues: Minor typographical errors, such as \"k-spacce\" (page 6) and \"undersampling maskes\" (page 7), should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical imaging: achieving robust zero-shot lesion segmentation across diverse lesion types. The proposed multi-scale mask-attribute alignment strategy and the CMKI module are innovative technical contributions that improve the alignment between visual and textual features, leading to superior segmentation performance. However, the evaluation could be strengthened by including a broader range of lesion types and anatomical regions, as well as cross-modality assessments. The reproducibility of the approach is satisfactory, but more detailed methodological descriptions are necessary to ensure complete transparency. Overall, Malenia demonstrates promising performance and has the potential to advance the state of the art in zero-shot lesion segmentation.\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis, broaden the scope of the evaluation to include a wider range of lesion types and anatomical regions, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Shaoting Zhang",
      "Wenhui Lei",
      "Xiaofan Zhang",
      "Yankai Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_743b761d4a84ae02565015701e9c3f189a2f413f.pdf",
    "remote_url": "https://openreview.net/pdf/743b761d4a84ae02565015701e9c3f189a2f413f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "MONICA: Benchmarking on Long-tailed Medical Image Classification",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Justin"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Long-tailed Learning",
      "Benchmark",
      "Medical Image Classification"
    ],
    "abstract": "Long-tailed learning is considered to be an extremely challenging problem in data imbalance learning. It aims to train well-generalized models from a large number of images that follow a long-tailed class distribution. In the medical field, many diagnostic imaging exams such as dermoscopy and chest radiography yield a long-tailed distribution of complex clinical findings. Recently, long-tailed learning in medical image analysis has garnered significant attention. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often leads to unfair comparisons and inconclusive results. To help the community improve the evaluation and advance, we build a unified, well-structured codebase called Medical OpeN-source Long-taIled ClassifiCAtion (MONICA), which implements over 30 methods developed in relevant fields and evaluated on 12 long-tailed medical datasets covering 6 medical domains. Our work provides valuable practical guidance and insights for the field, offering detailed analysis and discussion on the effectiveness of individual components within the inbuilt state-of-the-art methodologies. We hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning. The codebase will be publicly available on GitHub.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a framework and codebase for structured benchmarking of long-tailed (LT) learning methods on various medical image classification tasks. The benchmark, MONICA, implements over 30 LT learning methods, with comprehensive experiments assessing performance across 12 LT medical image classification datasets spanning 6 different modalities. The experiments analyze which methods, and categories of methods, provide the most benefit to LT medical image classification across tasks in a controlled environment.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- This work addresses an important problem, namely the variability in dataset/hyperparameters/etc. when evaluating LT learning methods for medical image classification tasks. These variations in setting make head-to-head comparisons difficult, so MONICA serves to provide a ‚Äúfair playing ground‚Äù for these LT learning methods.\n- The framework will become publicly available and should serve as an extensible resource going forward for LT medical image classification research.\n- The organization and presentation quality of the paper is strong, with helpful use of formatting (typesetting, color, etc.) and high-quality figures.\n- Experiments are very thorough, spanning many relevant methods, datasets, and tasks.\n- Discussion is thoughtful, going beyond simply displaying all benchmark results. The authors try to synthesize takeaways, provide caveats/limitations, assess out-of-distribution performance, and more.\n\n### Weaknesses\n\n- Writing can be improved throughout. See specific comments below for examples of awkward wording, inconsistent naming, grammatical errors, etc.\n- It is possible that choosing a fixed set of hyperparameters across methods unintentionally advantages certain methods. Ideally, one could argue that each method should be individually tuned on each task; however, I am aware that this would require a vast amount of resources and time, so I do not consider this a major limitation. More practical solutions to enhance the benchmark would be the following: (i) uncertainty estimates should be provided (e.g., bootstrapped confidence intervals or standard deviations over multiple runs), and (ii) multiple performance metrics should be provided (e.g., AUROC).\n\n### Questions\n\n- Is it possible that the chosen hyperparameters used across all methods happen to be more advantageous for certain methods and suboptimal for others? In one sense, using the same set of hyperparameters across methods appears ‚Äúfair‚Äù; however, it may actually be more fair to individually tune each method on each task. I recognize the difficulty of conducting fair comparisons in such a large-scale experimental setting, where it is costly to, e.g., run multiple trials of all experiments. I am not asking the authors to necessarily perform such experiments, but rather to consider this point and perhaps comment on it as a limitation/consideration.\n- Can the authors provide a summary of practical suggestions for which methods to use in a few sentences near the Conclusion?\n- I might suggest including the **rank** of each method on a given task in all tables. This would also enable you to *quantitatively* assess method performance across tasks (which method has the lowest average/median rank overall?). To make this work logistically (fit all columns in the table), you may need to reduce the precision to one decimal place, e.g.\n- The two paragraphs ‚ÄúLTMIC improves out-of-distribution detection‚Äù and ‚ÄúUsing imbalanced validation dataset for checkpoint selection‚Äù are not properly set up. For the former, what does it mean to use ‚ÄúImageNet as OOD samples, with 1,000 randomly selected images‚Äù? What exactly is the task, how is it formulated, and how are experiments conducted? Further, why do we care about this model behavior? For the former, Figure 4 and its findings are confusing ‚Äì why exactly does this demonstrate ‚Äústable convergence‚Äù? My general advice: **Use the methods section to describe and prepare the reader to understand everything that appears in the results**. When I come to these results sections, I should already have an idea of what experiments you have performed.\n\n**Minor comments/questions:**\n- Avoid editorializing with value judgments: ‚Äúbenchmark is **meticulously** designed‚Äù; ‚Äúwe‚Ä¶ develop a‚Ä¶ **well-structured** codebase‚Äù; ‚Äúour work provides **valuable** practical guidance‚Äù. Simply present your work and let the reader make these judgments!\n- ‚Äúdata imbalance learning‚Äù is not a phrase I have heard. Perhaps ‚Äúimbalanced learning‚Äù?\n- ‚Äúunified, strictly formulated, and comprehensive benchmark‚Äù. Unsure what ‚Äústrictly formulated‚Äù means. Could simply say ‚Äúunified, comprehensive benchmark‚Äù\n- ‚Äúwe build a‚Ä¶ codebase‚Ä¶, which implements over 30 methods‚Ä¶ and evaluated on 12‚Ä¶ datasets‚Äù. It seems that ‚Äúevaluated on‚Äù is the wrong tense; also, what is being evaluated?\n- This does not belong in an abstract: ‚ÄúWe hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning.‚Äù\n- Often unnecessary inclusion of ‚Äúthe‚Äù before concepts: ‚ÄúThe deep learning techniques‚Äù; ‚Äúthe collected image datasets‚Äù; ‚Äúthe long-tailed imbalance‚Äù\n- ‚ÄúThe deep learning techniques have proven effective for most computer vision tasks benefiting from the grown-up dataset scale.‚Äù Remove ‚ÄúThe‚Äù; what does ‚Äúgrown-up dataset scale‚Äù mean? ‚ÄúGrown-up‚Äù is not the right adjective ‚Äì be more concrete.\n- Refrain from claims like ‚Äúalways result‚Äù (line 57) ‚Äì soften to ‚Äúusually‚Äù or similar\n- Confused by this justification: ‚Äúit is vital to recognize these rare diseases in real-world practice, as they are relatively rare for doctors and may also lack diagnostic capacity.‚Äù This reads as ‚Äúit is vital to recognize rare diseases because they are rare‚Äù.\n- Line 65: can change ‚Äúcontributions, i.e.,‚Äù -> ‚Äúcontributions:‚Äù\n- Be consistent with capitalization/presentation of terms: ‚ÄúRe-sampling‚Äù vs ‚Äúre-sampling‚Äù; ‚ÄúModule improvement‚Äù vs. ‚ÄúModule Improvement‚Äù; ‚Äúmnist‚Äù vs. ‚ÄúMNIST‚Äù; ‚Äúmixup‚Äù vs. ‚ÄúMixUp‚Äù; etc.\n- Line 82: ‚Äúwe are still curious to explore‚Äù. Perhaps just ‚Äúwe aim to explore‚Äù?\n- ‚ÄúThe partition schemes are vita important‚Äù. What does ‚Äúvita‚Äù mean?\n- The last two paragraphs of the introduction are probably better off being formatted as bulleted or numbered lists. Also, it is unclear why these numbered lists are formatted differently: **1) xxx.** vs. (1) xxx.\n- ‚Äúof class $k$ where $\\rho$ denoted as imbalance ratio‚Äù. The phrase ‚Äúdenoted as‚Äù is awkward + need a comma after $k$\n- ‚Äúa common assumption in long-tailed learning is when the classes are sorted by cardinality in decreasing order‚Äù I‚Äôm not sure what this means or why this represents an ‚Äúassumption‚Äù. I would just remove this sentence since it does not seem to be used later.\n- Line 144: ‚Äúis a long-tailed version constructed from‚Äù. Need to say it is a version ‚Äúof‚Äù something; alternatively, use a word other than ‚Äúversion‚Äù like ‚Äúdataset‚Äù\n- Inconsistent spacing/use of commas in numbers. ‚Äú10, 015‚Äù -> ‚Äú10,015‚Äù; ‚Äú3200 fundus images‚Äù -> ‚Äú3,200 fundus images‚Äù\n- Inconsistent spacing around commas and colons: ‚Äútraining/off-site testing / on-site testin‚Äù; ‚Äú7 : 1: 2‚Äù; etc.\n- ‚ÄúLiver Tumor Segmentation Benchmark‚Äù -> ‚Äúthe Liver Tumor Segmentation Benchmark‚Äù\n- I realize it is hard to categorize some methods into one bin but GCL loss going in Information Augmentation is interesting, particularly since all other losses fall under re-sampling. It seems to also have module improvement as well.\n- ‚ÄúCausal classifier (Tang et al., 2020) resorted to causal inference for keeping the good and removing the bad momentum causal effects in long-tailed learning.‚Äù The phrase ‚Äúresorted to‚Äù is strange and has a negative connotation; also, what do ‚Äúgood‚Äù and ‚Äúbad‚Äù mean?\n- ‚ÄúAll these designs are for the fairness and the practicality of the comparison on the benchmark.‚Äù Too vague ‚Äì in what specific way do these support fairness?\n- Table 3: Inconsistent ‚ÄúAvg‚Äù vs ‚ÄúAvg.‚Äù vs ‚Äúavg‚Äù\n- Table 4: Consider using a line break occasionally (so one loss function occupies two rows). This would allow you to use a larger font size. Also, be consistent ‚ÄúCrossEntropy‚Äù vs ‚ÄúCE‚Äù?\n- ‚Äúassessing MixUp based solely on performance is not fair‚Äù. Soften to ‚Äúmay not be fair‚Äù\n- ‚Äúled to a significant performance decline, e.g,‚Äù. Refrain from saying ‚Äúsignificant‚Äù without statistical significance test + change ‚Äúe.g,‚Äù -> ‚Äúe.g.,‚Äù\n- ‚ÄúUse two-stage training as a general paradigm‚Äù sounds like a command. Perhaps ‚ÄúUsing‚Äù?\n- Define ‚ÄúSSL‚Äù acronym at first use\n- ‚ÄúModify classifier to reduce prediction bias‚Äù -> ‚ÄúClassifier modification to reduce prediction bias‚Äù\n- ‚ÄúIn Fig. 2, We visualize‚Äù -> ‚ÄúIn Fig. 2, we visualize‚Äù\n- Table 5 indicates the meaning of asterisk, which is never used in the table.\n- ‚Äúmodels with larger parameters‚Äù. The parameters are not ‚Äúlarger‚Äù ‚Äì could say ‚Äúmore parameters‚Äù or ‚Äúa larger parameter count‚Äù perhaps.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a framework and codebase, named MONICA, for structured benchmarking of long-tailed (LT) learning methods applied to various medical image classification tasks. With its implementation of over 30 LT learning methods, the paper provides extensive experiments assessing performance across 12 datasets covering 6 different modalities. Overall, the manuscript is well-organized and the presentation quality is commendable, enabling readers to access a ‚Äúfair playing ground‚Äù for evaluating LT learning methods.\n\n**Major Comments:**  \n1. The writing throughout the manuscript needs improvement; specific examples of awkward wording, inconsistent naming, and grammatical errors are provided below.  \n2. The choice of a fixed set of hyperparameters may unintentionally favor certain methods. While individual tuning per method and task would be ideal, practical solutions such as providing uncertainty estimates and multiple performance metrics should be considered enhancements.  \n3. The results evaluation could benefit from clarity regarding the experiments and methodology, especially in sections discussing \"LTMIC improves out-of-distribution detection\" and ‚ÄúUsing imbalanced validation dataset for checkpoint selection.\" Further detail on task formulation and experimental procedures is needed. \n\n**Minor Comments:**  \n- Avoid editorializing language in the presentation of work (e.g., \"meticulously designed\").  \n- Consider using \"imbalanced learning\" instead of ‚Äúdata imbalance learning.‚Äù   \n- The term \"strictly formulated\" is vague; simplify to \"unified, comprehensive benchmark.\"  \n- Correct grammatical inaccuracies, such as ‚Äúevaluated on‚Äù in the context used.  \n- Remove non-essential phrases from the abstract, e.g., ‚ÄúWe hope this codebase serves‚Ä¶‚Äù  \n- Ensure consistency in terms, such as capitalizations and phrasing, throughout the manuscript.  \n\n**Summary Paragraph:**  \nThe manuscript addresses the critical issue of variability in evaluating LT learning methods for medical image classification, representing an important contribution to the field. However, writing clarity and methodological rigor need attention to enhance reproducibility and the significance of results. While MONICA serves as a robust resource for ongoing research, the need for diverse performance metrics and better-defined hyperparameter settings remains. \n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MONICA**, a benchmarking framework and accompanying codebase designed for structured evaluation of long-tailed (LT) learning methods applied to medical image classification. It integrates over 30 LT learning algorithms and reports results across 12 datasets spanning six modalities. The study aims to enable fair and reproducible comparisons by standardizing experimental conditions. The paper is generally well-organized, clearly presented, and supported by comprehensive experiments and well-designed figures.\n\n---\n\n**Major Comments**  \n1. **Evaluation Design and Fairness:** The use of fixed hyperparameters across all methods, while intended to ensure fairness, may unintentionally favor certain models. An alternative would be per-method tuning per task, though this may be computationally infeasible. The authors should at least acknowledge this as a potential limitation.  \n2. **Statistical Reporting and Metrics:** The benchmark would be strengthened by including uncertainty estimates (e.g., standard deviations or bootstrapped confidence intervals) and a broader selection of performance metrics such as AUROC.  \n3. **Result Presentation:** Consider presenting method **ranks** within tables to facilitate quantitative comparison of relative performance across tasks.  \n4. **Experimental Clarity:** The sections describing ‚ÄúLTMIC improves out-of-distribution detection‚Äù and ‚ÄúUsing imbalanced validation dataset for checkpoint selection‚Äù require clearer setup in the Methods section. The OOD task definition, dataset composition, and rationale should be explicitly stated to help readers interpret these results.  \n5. **Writing and Style:** The manuscript would benefit from systematic language editing to correct awkward phrasing, inconsistent naming, and minor grammatical issues.\n\n---\n\n**Minor Comments**  \n- Avoid subjective qualifiers (e.g., ‚Äúmeticulously designed,‚Äù ‚Äúvaluable guidance‚Äù).  \n- Replace uncommon or unclear terms (‚Äúdata imbalance learning,‚Äù ‚Äúgrown-up dataset scale‚Äù) with standard wording.  \n- Ensure consistent terminology, formatting, capitalization, number spacing, and table labeling (e.g., ‚ÄúAvg‚Äù vs. ‚ÄúAvg.‚Äù).  \n- Revise ambiguous expressions such as ‚Äúresorted to causal inference‚Äù or ‚Äúlarger parameters.‚Äù  \n- Define acronyms on first use and maintain uniform style in figure and table captions.  \n- Simplify or reformat the introduction‚Äôs final sections for readability and structural consistency.\n\n---\n\n**Summary Paragraph (TMI Editorial Criteria)**  \nThe work‚Äôs **significance** lies in providing the first extensive, standardized benchmark addressing reproducibility challenges in LT medical image classification. The **innovation** is primarily infrastructural‚Äîestablishing a unified evaluation framework rather than proposing new algorithms. The **evaluation** is broad and well-controlled but would benefit from adding uncertainty measures and clearer methodological exposition. Regarding **reproducibility**, code release and dataset coverage are strong positives, though clearer documentation and improved writing would further enhance transparency.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The study is technically solid and potentially impactful, but revisions are needed to strengthen methodological transparency, statistical reporting, and textual clarity before it is ready for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents a unified benchmark for long-tailed learning in the medical domain by integrating several existing datasets and implementing a complete pipeline from data loading to model training and evaluation. The authors claim that the benchmark supports over 30 methods for comparison and provides an analysis of their performances.\n\n- Update after the discussion phase:\n\nThank you for the detailed responses! I've raised my score. While my concerns are not entirely resolved, I believe with careful revisions, its future version has the potential to be accepted.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe paper attempts to provide a comprehensive benchmark for long-tailed medical image classification. The idea of integrating multiple existing methods and datasets into a unified platform could potentially be useful for researchers who want to compare various methodologies under a standardized framework.\n\n### Weaknesses\n\n1. Motivation.¬†The paper lacks sufficient justification for evaluating long-tailed problems specifically in medical imaging tasks. While the authors mention some motivations at the beginning, these arguments are not convincing. Is there a fundamental difference between long-tailed problems in medical imaging and those in conventional tasks? Would this difference necessitate different methodologies? Even if the data modalities and evaluation methods are distinct (e.g., balanced vs. imbalanced test sets), would this lead to fundamentally different approaches? The paper analyzes multiple methods based on this premise but fails to provide insightful conclusions, which further deepens my skepticism about the motivation.\n\n2. Dataset Contribution.¬†Although the paper claims to use 12 datasets, 7 of these come from MedMNIST, and several of them are derived from previous work. This reduces the originality of the dataset contribution. Furthermore, the split between multi-class and multi-label datasets is 9/3, respectively. It is worth noting that many existing studies have already utilized MedMNIST for long-tailed learning (https://scholar.google.com/scholar?cites=11226954386823169312&scipsc=1&q=long+tail). Given that 7 out of the 12 datasets in this paper are from MedMNIST, why should users choose MONICA over MedMNIST, which already has extensive use and coverage in the medical imaging field? Additionally, the experimental methods used for multi-class and multi-label datasets are almost entirely different, and the analysis of multi-label results is limited to a single vague statement that multi-label classification is more challenging. This gives the impression that multi-label datasets were included just for the sake of completeness, rather than being a key focus.\n\n3. Code Contribution.¬†The code is not provided in the appendix, nor is there an anonymous GitHub link, which means the authors' claims about the code cannot be verified. By comparison, the NeurIPS D&B track (single-blind review) usually includes dataset or code links, along with information about author affiliations, licenses, and ethics. Although such links may be added after acceptance, this suggests that such work may not be well-suited for ICLR's double-blind review process.\n\n    Additionally, the description of the code structure in Section 3.1 is not particularly informative. The modular design described is basic and lacks novel insights. A more impactful modular design, like in mmdetection, which breaks down components into backbone, neck, and bbox head, would have been more meaningful. As it stands, the description feels unnecessary.\n\n4. Experimental Analysis Lacks of Insights. Comments below:\n\n    - Despite using multiple datasets, the authors only provide a generic / systematic comparison of the methods without analyzing differences across domains. For example, there is no discussion about which methods are better suited for dermatology versus ophthalmology. Almost all discussion is very general, without any specific insights related to medical applications. This diminishes the value of using 12 datasets, as the conclusions drawn are not substantially different from what could be obtained from a single dataset.\n\n    - The analysis in Section 4.2 is poorly organized. There is no clear structure, with the discussion jumping from evaluation metrics (e.g., \"Curse of shot-based group evaluation\") to re-sampling methods, MixUp, two-stage training, and even self-supervised learning in a seemingly random fashion. Many claims are also not supported by data. The overall takeaway from the experimental section is unclear, and I did not gain any insights on how to design better models.\n\n    - In Section 4.1, there is inconsistency in the training strategies used: some methods use a unified training strategy, while others use the one specified in the original paper (e.g., SAM, Line 306), with no explanation for this discrepancy.\n\n    - There are issues with the tables, such as Table 2, where it is unclear what methods like ERM, cRT, and LWS represent, as they are not referenced properly. Additionally, Section 3.2.3 does not fully align with the table.\n\n    - The categorization of methods is confusing. The authors categorize methods into three types‚Äîclass re-sampling, information augmentation, and module improvement‚Äîbut later mention that re-sampling and MixUp are used in many methods, making the classification in Tables 2/3 somewhat meaningless.\n\n    - The discussion on self-supervised learning (Line 398) appears out of place, as it is not introduced earlier.\n\n    - Similarly, the mention of OOD detection (Line 421) is abrupt and lacks context.\n\n    - The section on using an imbalanced validation dataset for checkpoint selection is unclear about its purpose. The conclusion seems to be that GCL exhibits lower fluctuations, but the reasoning and implications are not well explained. Additionally, Figure 4 lacks labels for the x and y axes, making interpretation difficult.\n\n    - Line 475 suddenly states that multi-label classification is more challenging without providing adequate context or analysis.\n\n    - Line 504 claims that \"the most advanced long-tailed learning methods no longer focus on improving a single strategy,\" but this claim is not well-supported by the preceding analysis.\n\n### Questions\n\nSee weakness sections. Some more questions below:\n\nCould you elaborate on why the results of self-supervised learning and OOD detection are relevant in this paper? They seem out of place given the main focus on long-tailed classification.\n\nWhy did the authors not include a domain-specific analysis (e.g., which methods work better for certain medical fields)? It seems like an important missed opportunity.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript proposes a unified benchmark for long-tailed learning specifically in the medical domain by consolidating several existing datasets and detailing a comprehensive pipeline for data loading, model training, and evaluation. The authors assert that this benchmark supports over 30 methods for comparative analysis. While the manuscript demonstrates potential contributions, overall clarity and justification for specific methodologies are lacking.\n\n**Major Comments:**\n1. **Motivation:** The reasons for focusing on long-tailed challenges in medical imaging are inadequately articulated. The manuscript fails to convincingly differentiate between long-tailed issues in medical imaging versus conventional contexts. More insight is necessary to justify the need for distinct methodologies in medical circumstances.\n   \n2. **Dataset Contribution:** The claim of utilizing 12 datasets is weakened by the reliance on MedMNIST, from which 7 datasets are derived. This raises questions about originality and why users should prefer this benchmark over already established datasets like MedMNIST. The uneven experimental methods between multi-class and multi-label datasets are concerning, compounded by a lack of depth in the analysis of multi-label results.\n\n3. **Code Contribution:** There is no code provided for verification, nor is there a link to an anonymous GitHub repository. The description of the code structure appears basic, lacking innovative details. This absence of practical accessibility contradicts expectations for transparency in this research community.\n\n4. **Experimental Analysis:** The analysis reveals several critical shortcomings:\n   - A lack of domain-specific insights, as no discussions delineate method performance distinctions in different medical fields.\n   - Poor organization in Section 4.2, making it challenging to grasp the analysis‚Äôs aim and conclusions.\n   - Inconsistent training strategies being employed without justification.\n   - Tables lack adequate context and clarity regarding methodologies.\n   - Sudden assertions regarding multi-label challenges and self-supervised learning seem irrelevant and poorly integrated into the overall narrative.\n\n**Minor Comments:**\n- The acronym definitions should be clarified upon first use.\n- Figures necessitate axis labels for better comprehension.\n- Several claims are presented without sufficient empirical support.\n\n**Summary Paragraph:**\nIn evaluating the manuscript against the four TMI editorial criteria, its significance is recognized through the attempt to standardize long-tailed learning evaluation in medical imaging, yet its innovation is hindered by the reliance on previously established datasets. Evaluation methods lack rigor and depth, especially in addressing domain-specific nuances, which undermines the paper's reproducibility potential. While the paper has strengths in proposing a benchmarking framework, it falls short in delivering comprehensive insights and justifications for its approaches.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a unified benchmark for long-tailed learning in medical image classification, combining multiple existing datasets and providing an end-to-end pipeline for data loading, model training, and evaluation. The benchmark reportedly supports more than thirty methods and analyses their performance. The paper aims to facilitate standardized comparisons in medical long-tailed learning. The overall presentation is adequate, though the motivation, dataset originality, and experimental insight are limited.  \n\n---\n\n**Major Comments**  \n1. **Motivation and Scope:** The justification for studying long-tailed problems specifically in medical imaging is weak. The paper does not convincingly demonstrate how long-tailed learning in medical tasks fundamentally differs from conventional domains or warrants specialized methodologies. The analyses performed do not yield domain-specific insights, reducing the perceived relevance of the medical focus.  \n\n2. **Dataset Contribution:** The dataset composition limits originality. Of the twelve datasets, seven derive from MedMNIST, and the rationale for creating a new benchmark instead of using existing ones is not well articulated. The imbalance between multi-class and multi-label datasets (9/3) is noted, and the handling of multi-label data appears superficial, giving the impression of inclusion for completeness rather than substantive study.  \n\n3. **Code Availability and Design:** Absence of code or repository link undermines reproducibility. The textual description of the code structure is generic and lacks meaningful modular innovations. Compared to other frameworks, the design appears simplistic and of limited technical contribution.  \n\n4. **Experimental Analysis and Interpretation:** The evaluation lacks depth and organization. Cross-domain insights (e.g., dermatology vs. ophthalmology) are missing. Section 4.2 mixes topics such as metrics, re-sampling, and MixUp without clear logical flow. Key tables and figures lack clarity, and several claims are presented without empirical support. Method categorization is inconsistent, and sections on self-supervised learning, OOD detection, and imbalanced validation appear disconnected or unexplained.  \n\n---\n\n**Minor Comments**  \n- Clarify notations for methods such as ERM, cRT, and LWS in tables.  \n- Ensure alignment between text descriptions (Section 3.2.3) and tables.  \n- Improve labeling in Figure 4 (axes missing).  \n- Maintain consistent training strategies across compared methods.  \n- Several abrupt statements (e.g., on multi-label difficulty, advanced methods) should be substantiated.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The paper addresses an important problem‚Äîbenchmarking long-tailed learning in medical imaging‚Äîbut its added value over existing efforts remains uncertain.  \n**Innovation:** The integration of multiple datasets is practical but insufficiently novel. Limited methodological or architectural contribution is evident.  \n**Evaluation:** The experimental design is extensive but weak in interpretive depth and cross-domain analysis. Organization issues reduce clarity.  \n**Reproducibility:** Currently low due to missing code and incomplete methodological transparency.  \n\n---\n\n**Decision Recommendation:** **Major Revision** ‚Äî The benchmark concept is potentially valuable, but stronger motivation, improved organization, domain-specific analysis, clearer presentation, and verified code release are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors introduce the problem of long-tailed medical image classification and challenges in the field. Then they develop MONICA which is a package to benchmark various methods ranging from different loss functions, augmentations, etc on the benchmark datasets across medical image classification tasks. They provide an overview of datasets and methods and experiment results on the datasets. The authors additionally share learnings and observations.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- A good overview of the datasets curated for this work\n- important contribution of decoupling the codebase\n- A good overview of the method approaches\n- practically useful to AI researchers in medical imaging\n\n### Weaknesses\n\n- It would help to expand the benchmark datasets and bring in a canonical set for a field such as Camlyon for Pathology, etc. WILDS (medical subset) is a great example of a dataset to bring in to this benchmarking codebase\n- Resnet-50 is used as a backbone but the community has generally moved on to more complex backbones such as ConvNext / Swin or foundation model backbones for different datasets. \n- Generally the community uses pretrained backbones rather than training the backbones from the scratch.\n- The same backbone is used for every task for fairness but generally a sweep over backbones would help since different modalities and tasks require different approaches\n- Top-1 accuracy is an in appropriate metric for model selection in imbalances settings and AUROC, AUPRC, F1 should be used\n- error bars are missing in experiments\n- More thorough error analysis\n- Clearer articulation of novel insights\n- Better connection to clinical relevance\n- More detailed ablation studies\n\n### Questions\n\n- Are their any key trends that you'll observed across the board to narrow down the design space for the future across the general task space? The results are not convincing in any one direction across the board on tasks and methods\n- Do you'll think stronger backbones can help learn better features?\n- Did you'll consider trying complex augmentation techniques such as AugMix or even learned augmentations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript addresses the challenges of long-tailed medical image classification and introduces MONICA, a benchmarking package designed to evaluate various methodologies, including different loss functions and augmentations, on established medical image classification datasets. The authors present overviews of the datasets, methods, and experimental results, along with valuable observations and insights gained throughout their research.\n\n**Major Comments:**\n1. The benchmark datasets could be expanded to include canonical sets, such as Camelyon for pathology and the medical subset of WILDS, to enhance the benchmarking codebase.\n2. Although ResNet-50 is utilized as a backbone, the field has largely transitioned to more sophisticated options such as ConvNext or Swin models. Consideration of these alternatives is warranted.\n3. The use of pretrained backbones is prevalent in the community; therefore, relying solely on training from scratch may not align with current best practices.\n4. Employing the same backbone across all tasks for fairness may overlook the benefits of exploring a range of backbones, as various modalities and tasks might require tailored approaches.\n5. The reliance on top-1 accuracy as the model selection metric is inappropriate in imbalanced settings; metrics such as AUROC, AUPRC, and F1-score are recommended.\n6. Missing error bars in the experiments reduce the interpretability of results.\n7. The manuscript could benefit from a more thorough error analysis to elucidate areas for improvement.\n8. There is a need for clearer articulation of novel insights derived from the study.\n9. A stronger connection to clinical relevance would enhance the practical implications of the research.\n10. More comprehensive ablation studies would provide better insight into the contributions of individual components of the proposed methods.\n\n**Minor Comments:**  \n- The manuscript would benefit from clearer structure and presentation of figures.\n- Review language for typographical errors and clarify notation and acronyms.\n\n**Summary Paragraph:**  \nThe study presents a significant effort in benchmarking medical image classification methods, showcasing innovation through the development of MONICA. However, there are weaknesses regarding dataset comprehensiveness, appropriate metrics for model evaluation, and the clarity of insights. The evaluation design could benefit from broader applicability and relevance to contemporary practices in model selection. Although the manuscript contributes valuable resources to the AI medical imaging community, there are areas where reproducibility and clarity could be improved.\n\n**Decision Recommendation:**  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of long-tailed medical image classification and introduces *MONICA*, a benchmarking framework designed to evaluate various methods‚Äîincluding different loss functions and data augmentations‚Äîacross multiple medical image classification datasets. The authors provide a structured overview of datasets, methods, and experimental results, offering insights and observations from their findings. Overall, the paper presents a practically useful tool for researchers, though aspects of experimental design and evaluation could be strengthened to enhance the work‚Äôs rigor and clarity.\n\n---\n\n**Major Comments**  \n1. **Benchmark Scope:** The benchmark could be expanded to include more canonical datasets widely recognized in the field, such as Camelyon for pathology or the medical subset of WILDS. This would improve generalizability and visibility.  \n2. **Model Backbone:** The study relies on ResNet-50 as the backbone, but more recent architectures (e.g., ConvNeXt, Swin, or foundation models) are now prevalent. Using stronger, pretrained backbones could lead to more representative and competitive results.  \n3. **Task-Specific Variation:** Employing the same backbone for all tasks ensures fairness but may not suit diverse modalities. A systematic exploration of alternative backbones would provide more insight into task-specific performance differences.  \n4. **Evaluation Metrics:** Top-1 accuracy is suboptimal for imbalanced datasets. Metrics such as AUROC, AUPRC, or F1 should be preferred for a more reliable assessment.  \n5. **Statistical Reporting:** Experimental results lack error bars, which limits assessment of robustness. A more thorough error analysis and detailed ablation studies would enhance credibility.  \n6. **Interpretation and Relevance:** The discussion could articulate novel insights more clearly and strengthen connections to clinical relevance.  \n7. **Trends and Design Insight:** The results do not show convincing trends across tasks and methods; further synthesis could help delineate design guidance for future work.\n\n---\n\n**Minor Comments**  \n- Clarify observed cross-task trends and limitations.  \n- Consider evaluating learned augmentations (e.g., AugMix) to test augmentation sensitivity.  \n- Improve presentation of numerical results for clarity.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in that it offers a unified benchmarking framework for long-tailed medical imaging tasks and a decoupled codebase beneficial for the research community. Its innovative contribution lies in integrating and standardizing evaluation approaches, though novelty in methodology is modest. Evaluation design could be improved through stronger baselines, additional metrics, and proper uncertainty quantification. Reproducibility is facilitated by open benchmarking resources but would be enhanced by including more datasets and detailed experimental documentation.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1. Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript, *MONICA: Benchmarking on Long-Tailed Medical Image Classification*, develops and releases a large-scale benchmark and modular codebase for evaluating long-tailed learning (LTL) algorithms within medical imaging classification tasks. While the work does not introduce a novel algorithm or theoretical contribution in imaging methodology per se, it addresses a significant methodological gap ‚Äî the lack of standardized, reproducible evaluation and fair comparison across long-tailed medical imaging datasets. Given that IEEE TMI occasionally publishes benchmark or methodological synthesis papers that materially advance the reproducibility and methodological understanding of imaging AI systems, the scope can be considered **conditionally relevant**. However, the contribution is more infrastructural and empirical than fundamentally methodological.\n\n**2. Novelty & Contribution Level**  \n‚ö†Ô∏è The core novelty resides in curating and unifying 12 medical datasets, incorporating 30+ long-tailed methods into a coherent codebase, and empirically analyzing their cross-domain generalization. These are useful engineering and benchmarking contributions, but the methodological novelty is low‚Äîno new loss function or model architecture is proposed. The paper‚Äôs unique contribution is thus its comprehensiveness, reproducibility, and analysis rather than scientific innovation. This positions the work **below the typical TMI novelty threshold**, although it has practical value.\n\n**3. Technical and Experimental Rigor**  \nThe paper demonstrates strong experimental organization: unified training pipelines, cross-validation protocols, extensive comparisons, and consistent baselines. Tables 2‚Äì5 are detailed, and implementation transparency is high (code release promised). However:  \n- Statistical significance or uncertainty analysis (e.g., standard deviations, CIs) is missing.  \n- Many experimental decisions (hyperparameters, group thresholds) appear heuristic.  \n- Multi-label evaluation could be more rigorous; e.g., exact mean average precision values and per-class calibration are not included.  \n- No theoretical analysis or in-depth model interpretability assessment.  \n\n**4. Clarity and Presentation**  \nOverall clear and readable. The manuscript is lengthy but well structured; tables and figures are legible. Minor grammar and phrasing inconsistencies appear (‚Äúvita important,‚Äù ‚Äúwe will shortly introduce‚Äù etc.). Figures could use higher resolution or graphical consistency (e.g., Fig. 3‚Äì4 fonts).  \n\n**5. Ethical and Reproducibility Compliance**  \nThe datasets referenced are all public (e.g., ISIC, CheXpert, MedMNIST). There is no human data collection in this work. Ethical compliance and anonymization are hence adequate. Reproducibility is strong, with clear modular design and promised GitHub release.  \n\n---\n\n**Phase 2. IEEE TMI Review Report**\n\n**1. Summary**  \nThis paper presents **MONICA**, a comprehensive open-source benchmark for **long-tailed medical image classification (LTMIC)**. It integrates 12 publicly available datasets across 6 medical domains (dermatology, ophthalmology, radiology, pathology, hematology, histology, gastroenterology) and over 30 long-tailed learning algorithms grouped into resampling, data augmentation, and module-modification methods. The authors evaluate these under uniform training conditions (mainly ResNet‚Äë50 backbone, Adam optimizer) and provide detailed cross-dataset comparisons, highlighting empirical insights into what factors drive performance differences across imbalance levels and dataset modalities.\n\n**2. Strengths**  \n- First unified and reproducible benchmark addressing long-tailed imbalance for medical imaging.  \n- Integrates diverse datasets and 30+ SOTA LTL algorithms, providing immediate utility to the community.  \n- Transparent and modular codebase structure; practical resource for fair comparison.  \n- Methodological findings (e.g., interplay between resampling and classifier normalization, importance of two-stage training) offer empirical guidance.  \n- Strong reproducibility plan and detailed documentation.\n\n**3. Weaknesses**  \n- Limited methodological novelty: mostly repackages and systematizes existing methods.  \n- Statistical rigor is limited ‚Äî no confidence intervals, hypothesis testing, or extensive ablation studies.  \n- Evaluation metrics focus on accuracy/mAP; lacks calibration, uncertainty, or clinical interpretation analysis relevant to medical imaging applications.  \n- Multi-label LTMIC discussion is brief and not deeply explored.  \n- Conclusions are largely descriptive without deeper methodological insight or theoretical generalization.\n\n**4. Major Comments**  \n1. **Methodological Novelty & Positioning** ‚Äì Clarify the positioning of MONICA: Is it proposed as a *benchmark paper* or an *algorithmic contribution*? For TMI readership, emphasize its utility for methodological validation and reproducibility, rather than as a new method. Consider reframing contributions accordingly.  \n2. **Statistical and Uncertainty Analysis** ‚Äì Report variance over multiple training runs and statistical comparisons between methods. Include significance testing or confidence intervals for at least a subset of key results (Table 2‚Äì3).  \n3. **Evaluation Metrics** ‚Äì Expand to include calibration metrics (e.g., ECE, Brier score) to align better with TMI‚Äôs focus on clinically reliable AI.  \n4. **Multi-label Classification Section** ‚Äì Deepen this section: describe evaluation metrics (exact mAP computation), address label co-occurrence quantitatively, and elaborate on class-wise distributions.  \n5. **Clinical Interpretation & Relevance** ‚Äì Discuss how the benchmark insights might influence clinical deployment or development of medically robust algorithms (e.g., rare disease detection, regulatory validation).  \n6. **Benchmark Maintenance** ‚Äì Describe future update strategy (versioning, dataset expansion) for reproducibility and long-term community usage.\n\n**5. Minor Comments**  \n- Minor wording issues: correct typographical errors (‚Äúvita important,‚Äù ‚ÄúRe-search‚Äù ‚Üí ‚ÄúResearch‚Äù).  \n- Standardize the capitalization and naming of methods and datasets.  \n- Some tables (e.g., Table 2‚Äì3) are dense‚Äîconsider highlighting top-performing methods in each category.  \n- Include GitHub link upon release for verification.  \n- Clarify figures (axes labels in Fig. 2‚Äì4 somewhat small).  \n\n**6. Overall Recommendation**  \n‚ö†Ô∏è **Reject (Out of Scope / Low Novelty)**  \nWhile technically solid and valuable as a benchmarking resource, the work predominantly provides an organizational framework and comparative analysis rather than a new methodological advancement in imaging science. IEEE TMI prioritizes methodological innovation or theoretical insight; this paper would be more suited to journals emphasizing open resources and applied benchmarking (e.g., *Medical Image Analysis ‚Äì Datasets and Benchmarks track* or *Scientific Data*).  \nIf the authors can include novel evaluation methodology (e.g., new metric for imbalance calibration, theoretical rationale for algorithm behaviors) or deeper analysis of medical imaging‚Äìspecific LTL phenomena, re-submission to TMI could be reconsidered.\n\n**7. Confidence Level:** 4 / 5  \n\n**Summary Judgment:**  \nValuable benchmarking contribution with strong engineering and reproducibility merit but insufficient methodological innovation for IEEE TMI‚Äôs core scope. Recommend redirection to a benchmark- or data-oriented venue, or substantial reframing to include analytic or theoretical advancement suitable for TMI.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MONICA (Medical OpeN-source Long-taIled ClassifiCAtion), a comprehensive benchmark for evaluating long-tailed medical image classification methods. The authors implement over 30 state-of-the-art long-tailed learning methodologies across 12 medical datasets spanning 6 medical domains (dermatology, ophthalmology, radiology, pathology, hematology, histology, gastroenterology). The benchmark addresses the lack of unified evaluation frameworks in long-tailed medical image learning by providing a modular codebase with standardized experimental protocols. Key findings include: (1) methods combining re-sampling, information augmentation, and module improvements (like GCL) achieve superior performance, (2) two-stage training paradigms offer significant flexibility, (3) simple re-sampling strategies remain effective baselines, and (4) multi-label classification presents additional complexities compared to multi-class tasks. The work aims to establish fair comparison standards and provide practical guidance for researchers developing long-tailed medical image classification systems.\n\n## Weaknesses\n\n‚Ä¢ **Insufficient mathematical formulation clarity and consistency**\n  - The problem definition in Section 2.1 (lines 108-119) uses inconsistent notation, where œÄ represents both a vector and individual frequencies, creating ambiguity in mathematical interpretation\n  - The imbalance ratio definition œÅ = n1/nk lacks clear specification of which classes n1 and nk represent, particularly given the ordering assumption\n  - Multi-label formulation introduces LCard metric (line 118) without proper mathematical context or connection to the main problem formulation\n\n‚Ä¢ **Limited experimental rigor and statistical validation**\n  - No statistical significance testing is reported across the extensive results in Tables 2, 3, and 5, making it impossible to distinguish meaningful performance differences from noise\n  - Single-run experiments without confidence intervals or multiple trials weaken the reliability of comparative conclusions, particularly given the modest performance differences observed\n  - Hyperparameter selection methodology is not systematically described (Section 4.1, lines 281-288), potentially introducing bias in method comparisons\n\n‚Ä¢ **Inadequate baseline establishment and evaluation protocol**\n  - The choice of ResNet-50 as the unified backbone (line 284) is not justified against other architectures, limiting generalizability of conclusions\n  - Missing ablation studies to isolate the contribution of individual components within complex methods like GCL, making it difficult to understand which aspects drive performance improvements\n  - Validation set construction methodology varies inconsistently across datasets (Table 1), compromising the fairness of cross-dataset comparisons\n\n‚Ä¢ **Incomplete coverage of relevant methodologies and recent advances**\n  - Several important recent long-tailed learning methods are mentioned as implemented but excluded from results presentation without clear justification criteria\n  - Self-supervised learning exploration (lines 407-420) is dismissed too readily based on limited experimentation, despite its potential relevance to medical imaging\n  - Multi-label long-tailed learning receives insufficient treatment given its clinical importance, with methods evaluation limited to basic adaptations\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical precision and notation consistency**\n  - Revise Section 2.1 to use distinct notation for the frequency vector œÄ and individual class frequencies œÄk, providing clear mathematical definitions for all symbols\n  - Explicitly define the class ordering convention and specify that œÅ represents the ratio between most frequent and least frequent classes\n  - Provide rigorous mathematical formulation for multi-label metrics and their relationship to the core long-tailed learning problem\n\n‚Ä¢ **Strengthen experimental methodology with statistical validation**\n  - Conduct multiple independent runs for all experiments and report mean performance with confidence intervals or standard deviations\n  - Apply appropriate statistical significance tests (e.g., paired t-tests) to validate performance differences between methods\n  - Establish systematic hyperparameter selection protocols using validation sets and document the optimization process for reproducibility\n\n‚Ä¢ **Develop comprehensive baseline evaluation framework**\n  - Justify the backbone architecture choice through systematic comparison with alternatives (e.g., Vision Transformers, EfficientNets) on representative datasets\n  - Include detailed ablation studies for complex methods to isolate the contribution of individual components\n  - Standardize validation set construction methodology across all datasets to ensure fair comparison protocols\n\n‚Ä¢ **Expand methodological coverage and analysis depth**\n  - Include results for all implemented methods with clear criteria for inclusion/exclusion in the main results presentation\n  - Conduct more systematic exploration of self-supervised learning with proper hyperparameter tuning and augmentation strategies tailored to medical imaging\n  - Develop specialized evaluation protocols and metrics for multi-label long-tailed learning that account for label co-occurrence patterns and clinical relevance",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces MONICA, a comprehensive benchmark for evaluating long-tailed medical image classification (LTMIC) methodologies across 12 diverse datasets spanning six medical domains. The authors implement over 30 long-tailed learning methods, including re-sampling, information augmentation, and module improvement techniques. The paper discusses the evaluation results, highlighting the strengths and weaknesses of various methods and offering practical insights for future research. The manuscript is well-written and provides a valuable resource for researchers working in LTMIC.\n\n## Major Comments\n1. Novelty and Positioning: \n   - While MONICA is a valuable contribution to the field, it is important to clarify how it distinguishes itself from existing benchmarks like CIFAR-100-LT and ImageNet-LT. The authors should provide a clearer comparison and position MONICA within the landscape of existing long-tailed learning benchmarks.\n   \n2. Evaluation Design:\n   - The evaluation covers a broad range of datasets and methods, but the inclusion of only a subset of state-of-the-art methods might limit the scope of the comparison. The authors should consider expanding the list of methods to include recent advancements in long-tailed learning, particularly those that have shown significant improvements in natural image classification.\n   \n3. Generalizability:\n   - The paper emphasizes the importance of generalizing long-tailed learning methods across different medical domains. However, the datasets used are relatively small and may not fully capture the variability found in real-world medical imaging scenarios. The authors should discuss potential limitations and future directions for scaling up the benchmark to include larger, more diverse datasets.\n   \n4. Reproducibility:\n   - The authors mention that the code will be publicly available, but the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters used for each method. Providing a more comprehensive documentation of these aspects would greatly enhance the reproducibility of the results.\n\n## Minor Comments\n1. Figures: Some figures, such as Fig. 2 and Fig. 3, are somewhat cluttered and could benefit from a clearer layout with fewer representative slices or zoomed-in regions to improve readability.\n2. Notation Consistency: The notation used in Section 2.1 for the forward operator is inconsistent and should be clarified.\n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without being defined, which can confuse readers who are not familiar with the terminology.\n4. Typographical Errors: Minor typographical errors exist throughout the text, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7).\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge in medical image classification: dealing with long-tailed class distributions. The introduction of MONICA as a comprehensive benchmark is innovative and provides a valuable resource for researchers. However, the evaluation could be strengthened by including a broader range of state-of-the-art methods and larger, more diverse datasets. The reproducibility of the results is a concern due to the lack of detailed methodological descriptions. Overall, while the manuscript has significant potential, it requires further refinement to fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include a wider range of state-of-the-art methods, provide a more comprehensive description of the training protocols and preprocessing steps, and address the minor comments to improve the clarity and reproducibility of the work.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Huimin Lu",
      "Lie Ju",
      "Peibo Duan",
      "Siyuan Yan",
      "Xiaodan Xing",
      "Yang Nan",
      "Yukun Zhou",
      "Zongyuan Ge"
    ],
    "url": "pdfs/iclr.cc-2025-conference_42d1ec064be79233cea3b21fed2023b04f20381e.pdf",
    "remote_url": "https://openreview.net/pdf/42d1ec064be79233cea3b21fed2023b04f20381e.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Precision Medicine; Patient-Specific Segmentation; Out-of-Distribution Patient Adaptation"
    ],
    "abstract": "Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}ersonalized}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the selected part-aware prompt, we propose a distribution-similarity-based retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^{\\text{2}}\\text{SAM}$ improves the performance by $\\texttt{+} 8.0$% and $\\texttt{+} 2.0$% mean Dice score within two patient-specific segmentation tasks, and exhibits impressive generality across different domains, $\\textit{e.g.}$, $\\texttt{+} 6.4$% mIoU on the PerSeg benchmark. Code will be released upon acceptance.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe article addresses challenges in personalized treatment within modern precision medicine, particularly in the context of medical image segmentation. The key issues it aims to solve include:\n\n1.\tPatient Variability: There is considerable variability among different patients, which complicates the segmentation of tumors and critical organs in medical images.\n2.\tLimited Annotated Data: Many existing segmentation algorithms rely on large amounts of annotated training data. However, personalized treatment often encounters a shortage of such data for individual patients, making it difficult to train models effectively.\n\nTo overcome these obstacles, the authors propose a new approach formulated as an in-context segmentation problem, leveraging the promptable segmentation mechanism of the Segment Anything Model (SAM). Their method, named P2SAM (Part-aware Personalized Segment Anything Model), allows for seamless adaptation to new, out-of-distribution patients using only one-shot patient-specific data.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The manuscript is clearly expressed and presents the research in a logical and structured manner, although some sentences are lengthy and could benefit from simplification to enhance readability.\n2. The paper presents substantial qualitative results and includes experiments across multiple datasets, demonstrating a considerable amount of work.\n\n### Weaknesses\n\n1. Representation: The overall logic of the paper appears problematic to me. The title and introduction emphasize \"Precision Medicine,\" yet the writing primarily focuses on highlighting the general applicability of the proposed method. Numerous examples and results are presented from the natural image domain, while the content related to \"Precision Medicine\" is notably limited, which may cause confusion for readers.\n\n2. The motivation for the \"Part-aware Prompt Mechanism\" is unclear: what is the reasoning behind this approach, and how does it address challenges in medical tasks? The architecture diagram is also based on natural image applications, leaving it unclear how the proposed method tackles issues specific to precision medicine. Additionally, there is no discussion on how this approach handles different modalities in medical imaging, which the paper should address.\n\n3. Method: Several aspects require further clarification and enhancement. \n\n     a. Firstly, the decision to use only a single negative point per cluster raises concerns regarding its sufficiency. A more robust approach would involve utilizing multiple negative points to enhance model generalization.\n\n     b. Secondly, while the method is designed for patient-specific segmentation, it raises concerns about its application to multi-segmentation tasks. A discussion on how the part-aware prompt mechanism could adapt to scenarios involving multiple segmentations would improve the methodology's applicability.\n\n     c. Additionally, the manuscript relies on 2D segmentation, which requires numerous points for effective performance. This issue remains unaddressed, and the choice not to utilize native 3D models is not sufficiently justified. Given the advantages of 3D models in capturing spatial relationships and providing more comprehensive context in medical imaging, exploring their potential application in this study would strengthen the methodology.\n\n4. Experiments: Several areas require clarification and enhancement.\n\n     a. Firstly, it is unclear whether the number of clusters set for K-means is consistent across different datasets. A detailed explanation of how the clustering parameters are determined for each dataset would improve the robustness of the results and ensure comparability.\n\n     b. Secondly, the comparison methods lack the inclusion of the latest medical-related benchmarks. Integrating more recent studies or state-of-the-art approaches would provide a more comprehensive evaluation of the proposed method's performance. This would help contextualize the results within the current landscape of medical imaging segmentation research.\n\n5. The paper lacks comparisons with state-of-the-art (SOTA) methods. Numerous improvements to SAM tailored for medical imaging have been proposed, yet many of them are omitted in this work‚Äôs comparative experiments. Furthermore, several segmentation baselines widely used in the medical domain are not referenced, which casts doubt on the demonstrated effectiveness of the proposed method.\n\n### Questions\n\n1. Is the proposed method truly aimed at addressing \"Precision Medicine,\" as suggested in the title and introduction, or is it intended as a more general approach applicable to both natural image and medical imaging domains? These are fundamentally different focuses, and clarity on this point is essential.\n\n2. What is the motivation behind the proposed method, and how does it address some of the fundamental challenges in the medical imaging field, such as unclear lesion boundaries and significant variations across different imaging modalities? Clarity on how these issues are tackled is crucial.\n\n3. Since the method claims to address patient-specific segmentation, it must consider the significant variability in characteristics across different diseases, such as liver cancer, pancreatic cancer, and rectal cancer. How does the proposed method ensure effectiveness when dealing with patients suffering from various diseases with distinct features?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript addresses challenges in personalized treatment within modern precision medicine, specifically focusing on medical image segmentation. It aims to tackle patient variability and the scarcity of annotated data, proposing a novel approach called P2SAM (Part-aware Personalized Segment Anything Model). This approach utilizes the Segment Anything Model (SAM) to facilitate adaptation to new patients with minimal data. Although the manuscript is well-structured, it presents substantial issues that impact its clarity and overall contribution.\n\n### Major Comments\n1. **Representation Issues**: The paper emphasizes \"Precision Medicine\" in the title and introduction, but much of the content relates to general applications of the method, primarily featuring examples from the natural image domain. This inconsistency may confuse readers regarding the paper's focus.\n  \n2. **Clarity of Motivation**: The rationale for the \"Part-aware Prompt Mechanism\" remains unclear. The architecture is based on natural images, and there is a lack of discussion on how this approach addresses challenges in medical tasks or different imaging modalities.\n\n3. **Methodological Concerns**: \n   - The decision to use a single negative point per cluster is insufficient; utilizing multiple negative points could enhance generalization.\n   - The application to multi-segmentation tasks needs elaboration, particularly concerning the part-aware prompt mechanism.\n   - The reliance on 2D segmentation without justification for not utilizing 3D methods diminishes the study‚Äôs methodological robustness.\n\n4. **Experimental Clarity**: \n   - Consistency in the number of K-means clusters across datasets is unclear, and an explanation of parameter selection is necessary.\n   - Inclusion of recent medical benchmarks in comparison methods is lacking, which affects the comprehensiveness of the evaluation.\n\n5. **Comparative Analysis**: The manuscript omits comparisons with numerous state-of-the-art methods tailored for medical imaging, raising concerns about the demonstrated effectiveness of the proposed approach.\n\n### Minor Comments\n- Sentence length could be simplified in several areas for better readability.\n- There are ambiguities in figures and notation that should be clarified.\n- Minor typographical errors need correction.\n\n### Summary Paragraph\nIn summary, the manuscript‚Äôs significance lies in its attempt to address key challenges in personalized medical image segmentation. However, the lack of clarity regarding its focus‚Äîprecision medicine versus general applicability‚Äîdiminishes its innovative potential. While the methodological design introduces a new approach, the issues surrounding reproducibility and insufficient comparative analysis with state-of-the-art benchmarks present considerable weaknesses. \n\n### Decision Recommendation\nMajor revision is recommended to enhance clarity, address methodological concerns, and improve the overall rigor of the analysis presented in the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses challenges in personalized treatment within precision medicine, focusing on patient-specific medical image segmentation. The proposed method, P2SAM (Part-aware Personalized Segment Anything Model), is presented as an in-context segmentation approach leveraging the promptable mechanism of the Segment Anything Model (SAM). It aims to adapt efficiently to new patients using only one-shot data. The paper is clearly structured and presents substantial experimental work across several datasets, though the writing could be more concise.\n\n---\n\n**Major Comments**  \n1. **Focus and Scope Misalignment:** The title and introduction emphasize ‚ÄúPrecision Medicine,‚Äù but much of the paper concentrates on the general adaptability of the proposed approach, including examples and figures from natural image applications. The relevance to precision medicine is insufficiently demonstrated and may confuse readers.  \n2. **Motivation and Relevance of the Part-aware Prompt Mechanism:** The rationale behind this mechanism and its specific advantages for medical imaging tasks are unclear. The architecture diagram draws from natural image examples, providing little insight into adaptation across medical imaging modalities.  \n3. **Methodological Clarifications:**  \n   - The use of a single negative point per cluster may be inadequate; multiple negative points could support better generalization.  \n   - The applicability of the part-aware mechanism to multi-segmentation tasks is not discussed.  \n   - The restriction to 2D segmentation is insufficiently justified; exploring or explaining the exclusion of native 3D models would improve rigor.  \n4. **Experimental Design:**  \n   - It remains unclear whether clustering parameters (e.g., number of K-means clusters) are consistent across datasets.  \n   - Comparisons omit several recent and relevant state-of-the-art (SOTA) methods in medical imaging. Inclusion of these would better contextualize the reported performance.  \n5. **Comparative Evaluation:** The experimental results lack benchmarking against prominent medical imaging baselines and recent SAM-based improvements for clinical applications, limiting the strength of the performance claims.\n\n---\n\n**Minor Comments**  \n- Some sentences are long and could be simplified for better readability.  \n- Clarify terms and symbols in figures; ensure consistent terminology between text and diagrams.  \n- Include a brief discussion on the handling of different imaging modalities.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper proposes an interesting adaptation of SAM for patient-specific segmentation, potentially contributing to personalized treatment workflows. However, its *significance* is reduced by unclear alignment with precision medicine goals. The *innovation* is moderate but underexplained due to limited motivation for the part-aware mechanism. The *evaluation* lacks comprehensive and up-to-date comparisons, and several critical methodological details are missing, which also weakens *reproducibility*. Overall, while promising in direction, the work requires substantial clarification and strengthening of its medical relevance.\n\n---\n\n**Decision Recommendation:** **Reject** (Rating: 1, Confidence: 5)",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors target patient-specific segmentation by proposing a new segmentation pipeline: (1) obtaining multiple point-prompts from the part-aware prompt mechanism, and (2) feeding these point-prompts to a prompt-based segmentation network, such as SAM. The authors also propose a similarity-based refinement to control the number of prompts found during step 1. Main experiments were conducted on the NSCLC and CVC-ClinicDB datasets where the proposed method achieves the state-of-the-art performances. Further experiments were conducted to show the method's performance against tracking algorithms and on the one-shot segmentation task.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The paper is well-written and easy to follow.\n2. The experiments are thorough, covering various important and related topics. \n3. The proposed refinement strategy on the number of prompts is conceptually easy and seems effective.\n\n### Weaknesses\n\n1. Unclearness in writing: (a) The motivation for Table 3 is unclear. It overlaps with Table 1 and 2 and brings little additional information. (b) For the baseline methods: direct-transfer and fine-tune, looks like no prompts are provided during the evaluation stage. Can the authors verify this is true?\n2. The contribution on the Part-aware Prompt Mechanism is unclear. Although the authors have demonstrated the effectiveness of the overall method, it is unclear if the improvements are from the part-based sampling strategy or merely because of having multiple prompts.\n3. (Minor) The application range of the method is limited to patients with multi-exams.\n\n### Questions\n\n1. To demonstrate the effectiveness of the Part-aware Prompt Mechanism, could the author compared to a modified version of PerSAM where instead of selecting \"two points with the highest and lowest confidence values\", the authors may select the top K and bot K' points where K and K' matches P^2SAM. In this way, the authors can demonstrate that points found by the Part-aware Prompt Mechanism are more effective. \n2. Since authors have shown better one-shot segmentation performance, I am curious to see if the authors can show their method's performance beyond patients with multi-exams. For example, for some medical segmentation datasets without patients with multi-exams, can the author randomly select one image-mask pair as the prior and test the model's performance on other pairs.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel segmentation pipeline aimed at patient-specific segmentation. This pipeline consists of obtaining multiple point-prompts through a part-aware prompt mechanism and utilizing these prompts in a prompt-based segmentation network, such as SAM. The authors conduct comprehensive experiments on the NSCLC and CVC-ClinicDB datasets and report state-of-the-art performance. Additionally, the manuscript explores the method's efficacy in comparison to tracking algorithms and its application in a one-shot segmentation task.\n\n**Major Comments:**  \n1. **Clarity of Writing:** \n   - The motivation for Table 3 is not well-defined, as it appears to overlap with Tables 1 and 2, offering minimal additional insight.\n   - There is ambiguity regarding baseline methods such as direct-transfer and fine-tune, particularly relating to prompt usage during evaluation. Verification from the authors is needed on this aspect.\n   \n2. **Contribution Clarity:** \n   - The impact of the Part-aware Prompt Mechanism remains unclear. While the overall method's effectiveness is demonstrated, it is uncertain whether improvements arise from the part-based sampling strategy or simply from employing multiple prompts.\n\n3. **Application Range:** \n   - The methodology's applicability is predominantly restricted to patients with multiple exams, indicating a limitation in generalizability.\n\n**Minor Comments:**  \n- The proposed refinements regarding prompt quantity are conceptually straightforward and appear effective, although the practical application is limited. \n\n**Summary Paragraph:**  \nIn relation to TMI editorial criteria, the manuscript showcases significant potential in terms of innovation and evaluation. It presents a method that pushes the boundaries of current capabilities in segmentation, albeit with limitations in clarity and the scope of application. While the experiments are rigorously conducted, the rationale for certain choices, such as in Table 3 and the baseline evaluations, detracts from full reproducibility. The need for a clearer demonstration of the Part-aware Prompt Mechanism's effectiveness and potential performance beyond multi-exam patients is noted as a key area for improvement.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a patient-specific medical image segmentation approach using a two-step pipeline. First, multiple point-prompts are derived via a part-aware prompt mechanism; second, these prompts are input to a prompt-based segmentation model such as SAM. A similarity-based refinement is introduced to control the number of generated prompts. The method is evaluated primarily on the NSCLC and CVC-ClinicDB datasets, where it achieves state-of-the-art performance. Additional experiments assess its tracking-related and one-shot segmentation capabilities. Overall, the paper is well written and organized, with clearly described experiments and motivated objectives.\n\n**Major Comments**  \n1. **Clarity and Redundancy in Reporting:** The motivation and distinct contribution of Table 3 are unclear, as its content appears to overlap with Tables 1 and 2 and adds limited new insight.  \n2. **Baseline Configuration:** For the baseline methods (‚Äúdirect-transfer‚Äù and ‚Äúfine-tune‚Äù), it seems that no prompts are used during evaluation. The authors should confirm whether this is accurate, as it affects the validity of the comparisons.  \n3. **Attribution of Improvements:** The individual contribution of the *Part-aware Prompt Mechanism* is not fully disentangled. The observed performance gains could stem from multiple prompts rather than the part-aware strategy itself.  \n4. **Experimental Scope:** The method‚Äôs applicability appears confined to patients with multiple examinations. Exploring performance in datasets without multi-exam patients would strengthen generalizability.\n\n**Minor Comments**  \n- The description related to Table 3 could be revised for conciseness and clearer motivation.  \n- Clarify notation and ensure all acronyms are defined at first use.  \n- Minor comment: explicitly state any datasets or patient conditions assumed in the experiments.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe proposed work addresses a relevant problem and provides an intuitively effective refinement strategy. Its main strength lies in a practical framework and careful evaluation design. However, aspects of methodological novelty and attribution of improvements require further clarification. While the experiments are comprehensive and reproducible in principle, certain ablation studies (e.g., isolating the part-aware mechanism) would enhance the evaluation‚Äôs strength. The work is significant for patient-specific segmentation but somewhat narrow in scope due to dependency on multi-exam data.\n\n**Decision Recommendation**  \n**Recommendation:** **Major Revision** ‚Äî The paper has solid potential but requires clearer justification of methodological novelty, improved presentation of overlapping tables, and clarification of baseline settings.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis study introduces a segmentation model called P2SAM which enables efficient, personalized medical image segmentation based on one-shot patient-specific data, improving segmentation performance across different patients and domains.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n1. The work introduces a \"part-aware prompt mechanism\" that leverages one-shot, patient-specific data for segmentation without additional fine-tuning. This feature is particularly suited to precision medicine applications where data annotation is minimal.\n\n2. Unlike standard SAM applications, P2SAM integrates a distribution-similarity-based retrieval approach that optimizes the selection of prompts, enhancing segmentation performance and reducing ambiguities.\n\n3. P2SAM is demonstrated to be effective across a variety of medical and natural image datasets, making it versatile for both patient-specific and general segmentation tasks.\n\n4. Quantitative Improvements: The model shows notable improvement over existing methods (PerSAM and Matcher), with significant gains in Dice scores and mIoU in various datasets, proving the robustness of the approach in handling out-of-distribution medical images.\n\n5. The paper presents extensive comparisons across multiple datasets and includes qualitative results that illustrate the model‚Äôs adaptability and performance in challenging segmentation tasks.\n\n### Weaknesses\n\n1. Complexity and Computational Cost: The proposed method, while innovative, involves complex modules (e.g., retrieval approach for optimal prompt selection) that may increase computational demands, potentially impacting usability in real-time clinical applications.\n\n2. Limited Generalization to Other Modalities: Although versatile within specific settings, the method's performance in more diverse medical imaging modalities (e.g., MRI or ultrasound) remains unexplored.\n\n3. Ambiguity in Module Contributions: The study could benefit from additional ablation studies to further clarify each component's role, such as the part-aware prompt mechanism versus the distribution-similarity-based retrieval.\n\n4. Dependence on SAM Backbone: While P2SAM leverages SAM effectively, it is still heavily reliant on SAM‚Äôs architecture. Any significant updates to SAM or its successors may require substantial adaptation of P2SAM.\n\n### Questions\n\n1. Scalability to Other Imaging Tasks: Could P2SAM be effectively adapted to other domains, such as pathological imaging, or expanded to handle 3D volumetric data with higher efficiency?\n\n2. Integration with SAM 2: The authors briefly mention SAM 2. How would the proposed retrieval mechanism and prompt system adapt to SAM 2's expanded capabilities in video and sequential data processing?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a segmentation model named P2SAM aimed at enhancing personalized medical image segmentation using one-shot patient-specific data. This model improves segmentation outcomes across diverse patients and domains. Overall, the study is clear and informative, with a solid contribution to the field of precision medicine.\n\n**Major Comments:**  \n1. **Innovation and Contribution:** P2SAM introduces a ‚Äúpart-aware prompt mechanism‚Äù that utilizes one-shot patient-specific data without requiring additional fine-tuning. This is essential for applications in precision medicine where data annotation is often limited.  \n2. **Performance Evaluation:** The model employs a distribution-similarity-based retrieval approach that optimizes prompt selection, leading to improved segmentation performance and reduced ambiguities compared to standard SAM applications.  \n3. **Versatility:** P2SAM is validated on various medical and natural image datasets, showing efficacy in both patient-specific and general segmentation tasks.  \n4. **Quantitative Results:** Significant improvements over existing methods (PerSAM and Matcher) are reported, with notable gains in Dice scores and mIoU across datasets, demonstrating robustness with out-of-distribution medical images.  \n5. **Comparative Analysis:** Extensive comparisons and qualitative results included in the paper further highlight the model‚Äôs adaptability and effectiveness in challenging tasks.\n\n**Minor Comments:**  \n- The complexity and computational cost associated with P2SAM could impact usability in real-time clinical settings due to the sophisticated modules involved.  \n- The method‚Äôs generalization to other imaging modalities (e.g., MRI, ultrasound) requires further exploration.  \n- Additional ablation studies are necessary to clarify the contributions of various components, such as the part-aware prompt mechanism and distribution-similarity retrieval.  \n- The dependency on the SAM architecture warrants caution, as any significant changes to SAM could necessitate modifications to P2SAM.\n\n**Summary Paragraph:**  \nThe study meets the TMI editorial criteria with significant innovation presented through P2SAM. The evaluation demonstrates notable improvements over existing methods, enhancing reproducibility across different datasets. However, challenges such as computational demands and the need for broader modality generalization are factors that could affect the study's real-world applicability and scalability. \n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *P2SAM*, a segmentation framework designed for efficient and personalized medical image segmentation using one-shot, patient-specific data. The study aims to improve cross-patient and cross-domain segmentation performance by integrating a part-aware prompt mechanism and a distribution-similarity-based retrieval strategy. Overall, the paper is clearly written, and the proposed method appears technically sound with convincing experiments and comparisons.\n\n**Major Comments**  \n1. **Methodological Complexity and Computational Cost:** While innovative, the proposed architecture incorporates several complex components‚Äîparticularly the retrieval mechanism for optimal prompt selection‚Äîwhich may impose substantial computational overhead and limit practical use in time-sensitive clinical contexts.  \n2. **Generalization to Diverse Modalities:** The study demonstrates broad applicability but does not explore performance across truly heterogeneous medical modalities such as MRI or ultrasound, leaving generalizability uncertain.  \n3. **Clarity on Module Contributions:** The paper would benefit from deeper ablation studies to clarify the relative impact of the part-aware prompt module and the distribution-similarity-based retrieval component.  \n4. **Dependence on SAM Backbone:** P2SAM‚Äôs reliance on the SAM architecture may constrain adaptability; any major revisions in SAM or newer versions could require significant reengineering, raising concerns about long-term sustainability.  \n\n**Minor Comments**  \n- It would be useful to discuss computational efficiency metrics, such as inference time per case, to contextualize the practical cost.  \n- The authors briefly mention SAM 2 but do not explore how the proposed mechanisms could transfer to sequential or video-based segmentation; elaboration would improve completeness.  \n- Clarify whether patient-specific adaptation requires additional memory or storage resources beyond standard SAM implementations.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in advancing personalized segmentation with minimal annotation. Its innovation lies in combining prompt-based adaptation with distribution-aware retrieval, which is demonstrated through extensive evaluations on both medical and natural image datasets. The results show strong quantitative improvements over comparable methods, indicating solid evaluation rigor. However, reproducibility could be strengthened with clearer documentation of module contributions and computational constraints. Despite some open questions about generality and practicality, the study offers a meaningful step toward personalized medical image analysis.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial clarifications regarding computational efficiency, component contributions, and generalization are needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase‚ÄØ1‚ÄØ: Analytical‚ÄØand‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI‚ÄØ‚Äì‚ÄØ‚úÖ**  \nThe manuscript proposes *P¬≤SAM ‚Äì Part‚Äëaware Personalized Segment Anything Model*, a segmentation strategy intended for **patient‚Äëspecific medical imaging**. The work introduces a new prompt‚Äëbased mechanism and a distribution‚Äësimilarity retrieval procedure compatible with general promptable models (SAM,‚ÄØSAM‚ÄØ2). It squarely targets methodological innovation in image segmentation, not an application‚Äëonly study. Thus, the paper fits TMI‚Äôs methodological scope.\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel‚ÄØ‚Äì‚ÄØMedium‚ÄëHigh**  \nInnovation lies in formalizing *patient‚Äëspecific segmentation as an in‚Äëcontext segmentation problem* and augmenting SAM with multi‚Äëpoint, *part‚Äëaware* prompts plus a *distribution‚Äësimilarity retrieval* criterion to select the number of parts automatically. These ideas extend prior PerSAM and Matcher works by introducing (i) a feature‚Äëdriven prompt generation process and (ii) a principled adaptation heuristic. While this builds on existing SAM foundations, it provides recognizable novelty and conceptual contribution at the method level ‚Äî likely within TMI‚Äôs novelty threshold.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor‚ÄØ‚Äì‚ÄØModerate‚ÄØto‚ÄØGood**  \nExperiments cover multiple medical domains (CT lung, endoscopy) and compare P¬≤SAM against strong baselines on both *in‚Äëdistribution* and *out‚Äëof‚Äëdistribution* datasets, with ablation and cross‚Äëdomain extension to natural images. Implementation details, hyperparameters, and retrieval settings are extensive (Appendices‚ÄØB‚ÄìC).  \nHowever:  \n- Reproducibility of one‚Äëshot ‚Äúprior‚Äù data setup could benefit from explicit configuration scripts or public code.  \n- Uncertainty analysis and statistical significance testing are absent.  \n- Although IRB aspects are likely met via public datasets, explicit ethical statements should be added.\n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation‚ÄØ‚Äì‚ÄØGood‚ÄØbut‚ÄØdense**  \nThe manuscript is technically detailed and readable for an imaging‚Äëmethodology audience. Nonetheless, figure readability (visual scale/labels), equation typesetting, and excess references lead to occasional clutter. Editorial tightening would aid flow.  \n\n**5.‚ÄØEthical‚ÄØand‚ÄØReproducibility‚ÄØCompliance‚ÄØ‚Äì‚ÄØPartially‚ÄØaddressed**  \nAuthors use public datasets (TCIA,‚ÄØKvasir,‚ÄØCVC), promising code release on acceptance. Explicit mention of IRB exemption and patient anonymization should appear in the Methods.\n\n---\n\n## **Phase‚ÄØ2‚ÄØ:‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n### **1.‚ÄØSummary**\nThe paper presents *P¬≤SAM*, a **training‚Äëfree, patient‚Äëspecific segmentation framework** leveraging the Segment‚ÄëAnything model‚Äôs promptable structure. Instead of fine‚Äëtuning, P¬≤SAM clusters reference‚Äëimage features into part‚Äëlevel embeddings that guide multi‚Äëpoint positive/negative prompts for a new patient image. A *distribution‚Äësimilarity retrieval* (using Wasserstein distance) then determines the optimal number of parts. Demonstrations on CT lung tumor and endoscopy polyp segmentation show performance gains of‚ÄØ‚âà‚ÄØ8‚ÄØ%‚ÄØDice over comparable methods, with cross‚Äëdomain validation on natural‚Äëimage few‚Äëshot benchmarks.\n\n### **2.‚ÄØStrengths**\n- Clear methodological innovation in prompt selection and adaptive part‚Äëcount retrieval.  \n- Sound integration with foundation models‚ÄØ(SAM/SAM‚ÄØ2) without retraining.  \n- Comprehensive experiments, including ablations and extension to multiple domains.  \n- Addresses a clinically relevant scenario ‚Äî *patient‚Äëspecific (one‚Äëshot) segmentation* ‚Äî under the precision‚Äëmedicine paradigm.  \n\n### **3.‚ÄØWeaknesses**\n- Improvements over PerSAM after domain adaptation are modest; novelty may appear incremental relative to other prompt‚Äëselection strategies.  \n- Statistical validation of performance differences is missing.  \n- Ethical and data‚Äëuse compliance needs explicit statement.  \n- Paper length and figure density affect readability; discussion could better isolate clinical significance.  \n- Some domain‚Äëadaptation steps are tangential and could be streamlined.  \n\n### **4.‚ÄØMajor‚ÄØComments**\n1. **Clarify novelty relative to Matcher and PerSAM.** Provide algorithmic and conceptual comparisons (e.g., computational cost, robustness) in a dedicated subsection.  \n2. **Statistical significance and uncertainty.** Report variance or confidence intervals for Dice/mIoU; possibly perform Wilcoxon or paired t‚Äëtests on patient‚Äëwise folds.  \n3. **Reproducibility.** Supply pseudocode for the retrieval process, specify seed control, and release a minimal code snapshot.  \n4. **Ethics statement.** Explicitly confirm datasets are public/anonymized and no new patient data or identifiable information were used.  \n5. **Discussion depth.** Expand on clinical feasibility‚Äîe.g., execution time per case and how one‚Äëshot prior images would be collected in clinical pipelines.  \n\n### **5.‚ÄØMinor‚ÄØComments**\n- Improve figure legends and resize multi‚Äëpanel figures for readability.  \n- Fix minor typographic errors (spacing, subscripts).  \n- Cite recent TMI works on adaptive or federated segmentation to situate within field.  \n- Reorder Appendix‚ÄØsections: AMI summary first, less critical visualizations last.  \n- Include a short limitations paragraph in the main text, not only the Appendix.  \n\n### **6.‚ÄØOverall‚ÄØRecommendation**\n**Major‚ÄØRevision** ‚Äî The paper presents a valuable methodological contribution suitable for TMI, yet requires clearer articulation of novelty, stronger statistical evidence, and tighter presentation before acceptance. With these revisions, it could reach publishable quality.\n\n### **7.‚ÄØConfidence‚ÄØLevel‚ÄØ**\n**4‚ÄØ/‚ÄØ5‚ÄØ‚Äì‚ÄØHigh confidence.** I am familiar with SAM‚Äëbased segmentation and medical imaging adaptation. The assessment reflects domain understanding and TMI standards.\n\n---\n\n**Summary‚ÄØVerdict:**  \nP¬≤SAM extends SAM to patient‚Äëspecific one‚Äëshot segmentation with a novel part‚Äëaware prompting framework. The study is methodologically relevant, technically sound, and potentially impactful for personalized imaging, but requires additional statistical rigor, explicit ethical compliance, and improved clarity prior to publication in *IEEE‚ÄØTransactions‚ÄØon‚ÄØMedical‚ÄØImaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes P2SAM (Part-aware Personalized Segment Anything Model), a data-efficient segmentation method for patient-specific applications. The method addresses the challenge of adapting segmentation models to new patients using only one-shot prior data without model fine-tuning (Section 1). P2SAM introduces a part-aware prompt mechanism that clusters foreground features from reference images into multiple parts using k-means++ clustering (Equation 1, Section 3.2), then selects multiple-point prompts based on cosine similarity between part-level features and target image features (Equation 2). A distribution-similarity-based retrieval approach using Wasserstein distance determines the optimal number of parts for each case (Section 3.2, Figure 5). The method can be integrated into various promptable segmentation models like SAM and SAM 2. Experiments on two patient-specific segmentation tasks show improvements of +8.0% and +2.0% mean Dice score, with additional validation on natural image benchmarks achieving 95.7% mIoU on PerSeg (Tables 1-2, Table 4).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation lacks precision and clarity**\n  - Equation 1 uses the symbol \"‚ó¶\" for mask selection without formal definition, making the operation ambiguous for readers unfamiliar with this notation\n  - The clustering procedure in Section 3.2 mentions k-means++ but doesn't specify how the number of clusters n relates to the retrieval range, creating inconsistency between methodology and implementation\n  - The Wasserstein distance computation for distribution similarity (Section 3.2, Table 7) lacks mathematical formulation, making it difficult to reproduce the approach\n\n‚Ä¢ **Experimental design contains methodological inconsistencies**\n  - Table 1 shows the \"Meta huge\" model achieves only 28.52% Dice score while fine-tuned versions reach 60%+, but no analysis explains this dramatic performance gap or validates the fairness of comparison\n  - The retrieval ranges in Tables 9-10 vary significantly across datasets and models without principled justification, suggesting ad-hoc hyperparameter tuning rather than systematic methodology\n  - Comparison with MedSAM in Table 3 uses human-provided box prompts during inference while P2SAM uses only first-visit ground truth, creating an unfair evaluation setup that undermines the claimed advantages\n\n‚Ä¢ **Limited technical novelty and insufficient baseline coverage**\n  - The core contribution of using multiple prompts instead of single prompts is incremental, as acknowledged in Section 2 where SAM's original paper states \"ambiguity is much rarer with multiple prompts\"\n  - Missing comparisons with recent medical segmentation methods beyond MedSAM, particularly other SAM-based medical approaches mentioned in Section F but not evaluated\n  - The k-means++ clustering for part-level features (Section 3.2) is a standard technique without novel adaptation for medical imaging characteristics, limiting the technical contribution\n\n‚Ä¢ **Evaluation methodology shows significant limitations**\n  - The patient-specific segmentation tasks only include 13 patients with multiple visits from 4D-Lung dataset (Section 4.1), which is insufficient for robust statistical validation of the claimed improvements\n  - Qualitative results in Figures 6-9 show cherry-picked examples without systematic analysis of failure cases or statistical significance testing of the quantitative improvements\n  - Cross-domain generalization claims are undermined by the large performance variations across datasets (Tables 1-2) without adequate analysis of when and why the method succeeds or fails\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and reproducibility**\n  - Provide formal mathematical definition for the mask selection operator \"‚ó¶\" in Equation 1 and specify the exact clustering algorithm parameters\n  - Add complete mathematical formulation for the Wasserstein distance computation used in the retrieval approach, including implementation details\n  - Establish principled criteria for determining retrieval ranges instead of the current ad-hoc approach shown in Tables 9-10\n\n‚Ä¢ **Strengthen experimental validation and fairness**\n  - Conduct systematic analysis explaining the performance gap between Meta and fine-tuned models, including statistical significance tests for all reported improvements\n  - Implement standardized comparison protocols ensuring all methods use equivalent information during inference, particularly for the MedSAM comparison\n  - Establish unified hyperparameter selection methodology across all datasets and provide sensitivity analysis for key parameters like the number of parts n\n\n‚Ä¢ **Expand technical contributions and baseline coverage**\n  - Develop novel clustering or feature selection techniques specifically adapted for medical imaging characteristics rather than relying solely on standard k-means++\n  - Include comprehensive comparisons with recent SAM-based medical segmentation methods mentioned in the related work but omitted from experiments\n  - Provide deeper analysis of the relationship between prompt selection strategies and segmentation performance to establish clearer technical insights beyond the incremental multiple-prompt approach\n\n‚Ä¢ **Improve evaluation comprehensiveness and statistical rigor**\n  - Expand the patient cohort for validation beyond the current 13 patients, and include power analysis to determine adequate sample sizes for statistical validation\n  - Implement systematic failure case analysis and provide statistical significance testing for all quantitative comparisons rather than relying primarily on qualitative examples\n  - Develop comprehensive analysis framework explaining performance variations across different medical imaging modalities and domains to better understand method limitations and applicability",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces P2SAM (Part-aware Personalized Segment Anything Model), a novel approach for patient-specific segmentation tasks. P2SAM leverages one-shot patient-specific data and integrates a part-aware prompt mechanism and a distribution-similarity-based retrieval approach to select multiple-point prompts, enhancing the generalization capabilities of the segmentation model. The method is evaluated on two patient-specific segmentation tasks‚ÄîNSCLC segmentation and polyp segmentation‚Äîand demonstrates significant improvements over existing methods. The manuscript is well-written, with clear motivations and detailed methodologies, although some aspects require further clarification and expansion.\n\n## Major Comments\n1. Novelty and Positioning: While P2SAM introduces a promising part-aware prompt mechanism, the manuscript could benefit from a more thorough discussion of how this mechanism distinguishes itself from existing works such as Matcher and PerSAM. The novelty should be more explicitly highlighted, particularly regarding the part-aware prompt mechanism and the retrieval approach.\n\n2. Evaluation Design: The evaluation is mainly performed on two datasets, which limits the generalizability of the results. Including more diverse datasets or anatomies would strengthen the claim of P2SAM's robustness and versatility. Additionally, the manuscript should include more comprehensive comparisons with recent works, such as MedSAM and other SAM adaptations, to better contextualize the performance improvements.\n\n3. Comparisons and Baselines: The manuscript compares P2SAM with several baselines, including Matcher and PerSAM, but it lacks a detailed comparison with MedSAM, which is a strong baseline in the medical image segmentation domain. Incorporating MedSAM would provide a more rigorous benchmark and help substantiate the claimed improvements.\n\n4. Reproducibility: The manuscript states that code will be released upon acceptance, but it is essential to provide more details about the implementation, such as hyperparameters, training protocols, and preprocessing steps. This information is crucial for reproducibility and further development of the method.\n\n## Minor Comments\n1. Figures: Figures 3, 6, and 7 are somewhat cluttered and could be improved by showing fewer representative slices with zoomed-in regions to enhance readability.\n   \n2. Notation: The introduction of notation in Section 2.1 lacks sufficient explanation, particularly regarding the forward operator, which is denoted inconsistently.\n   \n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition.\n   \n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7) need correction.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical image segmentation‚Äîpersonalizing segmentation for out-of-distribution patients with limited annotated data. The proposed P2SAM method introduces a part-aware prompt mechanism and a distribution-similarity-based retrieval approach, which contribute to its technical innovation. However, the evaluation is somewhat limited to two specific tasks, and broader validation is necessary to fully support the generalizability claims. The manuscript also requires more detailed comparisons with strong baselines like MedSAM. While the reproducibility of the approach is promised, the methodological details need to be more thoroughly documented to ensure independent reproduction. Overall, P2SAM holds promise, but the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand comparative analysis, strengthen validation across datasets and anatomies, provide more detailed methodological details for reproducibility, and include comprehensive comparisons with strong baselines such as MedSAM.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ],
    "url": "pdfs/iclr.cc-2025-conference_10d7d4d4e6a581994367bb8103dcf41b5a52cd66.pdf",
    "remote_url": "https://openreview.net/pdf/10d7d4d4e6a581994367bb8103dcf41b5a52cd66.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Single Domain Generalization for Rare Event Detection in Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Deep Learning",
      "Knowledge",
      "Rare Event Detection",
      "Out-of-distribution detection"
    ],
    "abstract": "Single Domain Generalization (SDG) addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset. In the medical diagnosis and disease detection domain, where data is often limited and events of interest are rare, deep learning (DL) models frequently exhibit suboptimal performance, leading to poor generalization across datasets. In multi-center studies, disparate data sources, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare event characteristics. This paper addresses this challenge by first leveraging a pre-trained large vision model to rank classes based on their similarity to the rare event class, allowing focused handling of the most similar class, and then integrates domain-invariant knowledge on rare event with DL to accurately classify the rare event class. By carefully incorporating expert knowledge with data-driven DL, our technique effectively regularizes the model, enhancing robustness and performance even with limited data availability. We present a case study on seizure onset zone detection using fMRI data, demonstrating that our approach significantly outperforms state-of-the-art vision transformers, large vision models, and knowledge-based systems, achieving an average F1 score of 90.2% while maintaining an overall F1 score of 85.0% across multi-center datasets.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors introduce an algorithm for generalizing to unknown domains using only training data from a single known domain, \"RareSaGE\", which proposes to integrate domain-invariant expert knowledge with neural networks for the problem of rare class classification. Their method achieves improved performance for this task compared to several neural network- and expert knowledge-based techniques.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper focuses on a lesser known, but still seemingly important problem which deserves attention. They test on a realistic dataset comprised of medical data sampled from real centers, demonstrating real life applicability.\n2. The method is seemingly fairly technically novel. The rarity quantification doesn't seem novel (they cite Li et al), but overall, the combination of expert knowledge (an old field) with modern neural network models is interesting and unusual.\n3. Improvements over the compared baselines are significant for all metrics (except maybe precision) (Table 1)\n4. The intuition behind the method is reasonable, and the formalism discussed does help with understanding this. The reasoning behind \"rationale for definition 3.1\" makes sense, i.e., explaining the challenges in working with rare classes with respect to class entropy.\n5. I think the four characterizations of a rare event make sense (not sure if these are novel or not?), although discrimination and significance may be a bit redundant.\n\n### Weaknesses\n\n**Major:**\n1. **Generally limited experiments (don't even begin until page 9):** I would suggest moving many of the less-important dataset, algorithm and method details into the supplementary, and filling the missing space with more experiments, especially those considering ablation studies, failure cases for your method, computational analyses, and others. The biggest issues are as follows.\n    1. For a method defined in such generality, the experiments are quite limited: just a case study based on one fMRI datasets. This is a big negative, because it is unclear if the method could generalize to other problems, or if it was bespoke to (or developed specifically for) this case study. This hurts the paper's suitability for ICLR, as the larger impact on machine learning is not clear to me, which would require testing on other datasets/problem settings/etc.\n    2. Moreover, while the improvements over the compared baselines are significant for all metrics (except maybe precision) the comparison/baseline models may not be sufficient:\n        - I'm not an expert in domain generalization, but are the comparison models in Table 1 really proper baselines (maybe another reviewer can chime in)? Simple training or finetuning on one domain and testing on another domain (as well as the knowledge-based systems) doesn't seem like a strong, appropriate baseline. Are there better unknown domain generalization techniques which should have been tested?\n        - Alternatively, what about comparing to one-class-classification methods, i.e. OOD/anomaly-detection methods? This is also a large body of work which seems suitable. At the very least, why weren't these discussed in the related work?\n        - why wasn't AUC presented if sensitivity and precision were also presented? This would be a better general performance metric than accuracy for accounting for class imbalance.\n\t\n2. **Lack of ablation/hyperparameter sensitivity studies:**\n    1. this method has quite a few moving parts, which each could be brittle. The more of these moving parts that there are, the more ways that the method could fail when extended to new datasets/problems. It is important to consider how changing or removing one component would affect the performance, in order to judge how reliant to algorithm is on that component. However, the paper critically lacks any ablation/sensitivity studies for such hyperparameters/settings. I will discuss potential ablation studies which could be done, but I challenge the authors to think of more. Some examples:\n        1. On pg. 4, a rare class is defined by a 2-sigma distance from the mean class entropy. Why/how was 2 sigma chosen? This is seemingly an important parameter, yet you do no ablation/sensitivity studies on the effect of different values for this (number of sigmas used to define a rare class)\n        2. Algorithm 1 is quite complex, additionally with certain steps not quantitatively/explicitly defined.\n        3. The use of PCA on clip features may have limitations, as its linearity may be too much of a restriction. Why wasn't some form of nonlinear component/degrees-of-freedom analysis tested as well?\n        3. The form of the expert knowledge in the tested scenario (Eq. 6) is quite specific. Are there viable alternatives to this formulation that could have been tested on this dataset? Also, in general, what is the feasibility for converting expert knowledge into this format?\n\t\t\t\n**Minor:**\n1. More clear, technical method details are needed in the abstract and introduction to be clear what these contributions are. For example:\n    1. In the abstract: \"This paper addresses ‚Ä¶ even with limited data availability.\" This really lacks in explicit details on how the method actually works. how is this ranking done, specifically? is it via a novel algorithm? also, what does \"focused handling\" mean? and how is domain-invariant knowledge \"integrated\"? These questions could be gleaned from the main text of the paper, but a better level of detail could still be provided in the abstract with less vague wording.\n    2. Similarly in the introduction, more technical details are needed. Your repeatedly describe the \"integration\" of domain invariant expert knowledge with deep learning, but I'm unclear from reading this what this actually explicitly describes.\n2. Misleading claims/vague wording:\n    1. In the introduction, you say \"This concept is particularly crucial in the field of artificial intelligence (AI) for medicine, where the aim is to accurately diagnose new patient cases across centers\". This is misleading. AI for medicine extends far beyond diagnosis, including tasks such as segmentation, registration, harmonization, etc‚Ä¶ and that's just for images, not including AI for medicine beyond images.\n    2. \"In medical imaging, there are various factors ‚Ä¶  acute disorders\" also in the introduction; what are the \"classes\" being described here? I'm not sure if this is really intra-class variability (at least since you don't explain what the classes are here), its really just various factors that can contribute to domain shift problems in medical datasets.\n    3. \"In the medical diagnosis and disease detection domain, where data is often limited ‚Ä¶ variability in rare event characteristics. \" in the abstract is phrased confusingly. in the first sentence, is performance poor on the data because of limited data, rare cases of interest, or both? Maybe say that the second sentence describes a problem which exacerbates the problem described in the first sentence.\n3. Other missing technical details:\n    1. how is feature overlap/resemblance (described in the first paragraph of Sec. 3.2 actually computed? cosine similarity of the feature vectors?\n    2. Algorithm 1 shouldn't really be labeled as an \"algorithm\" in my opinion, because many of the steps are not defined explicitly, and left up to interpretation. More details are needed.\n    3. They provided anonymous code link in Sec. 4.1 is broken. It would have been helpful to see how such a novel and relatively complex algorithm was implemented in practice.\n4. Writing/clarity issues: the paper suffers from issues in writing quality, including clarity and typos, discussed next. For example:\n    1. The paper can be a bit challenging to read, not because of technical details, but because redundant information is often provided (see for example, \"solution overview\" and \"solution details\" could certainly be shortened and combined in 3.2)\n    2. Many sentences are terse and relatively telegraphic. The paper is still understandable, but could have better flow and general writing.\n    3. There is a too-long run-on sentence in \"To improve SDG for rare events, leveraging expert knowledge such as clinical opinion on ‚Ä¶, has immense potential\" in the introduction.\n5. Typos/formatting issues:\n    1. In the abstract: \"Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset\"\n        1. also, this is phrased confusingly. maybe say extensively studied in classification of balanced datasets?  But is having an imbalanced dataset not just kind of a triviality?\n    2. also in abstract \"integrates domain-invariant knowledge on rare event\" another typo.\n    3. Also in the intro, page 2 \"with DL using pre-trained large vision model (LVM) \"\n    4. There seems to be an issue in how in-text citations are formatted: no spacing between them, not parenthetical, etc. Makes it a bit harder to read the paper.\n    5. Why are the equations written with such tiny text? They require zooming in just to read. I'm not sure if this formatting modification is allowed for ICLR.\n\n### Questions\n\n1. Is \"single domain generalization\" a common term when describing your problem scenario? it sounds like it describes generalizing TO a single domain, not training on a single one and being able to generalize to unknown ones.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents an algorithm named \"RareSaGE\" aimed at generalizing to unknown domains using training data exclusively from a known domain, integrating domain-invariant expert knowledge with neural networks for rare class classification. The authors report improved performance compared to existing neural network and expert knowledge-based methods. Overall, the paper addresses a significant issue, though there are limitations concerning the breadth and depth of experimentation.\n\n### Major Comments\n1. **Limited Experiments**: The experiments do not begin until page 9, limiting the manuscript‚Äôs depth. Important details about the dataset, algorithm, and method could be relocated to supplementary materials to include more experiments, such as ablation studies and analyses of failure cases.\n   - The single case study based on fMRI datasets raises concerns about the generalizability of the method to other problems, impacting the relevance of the study for ICLR.\n   - The chosen comparison models in Table 1 may not establish appropriate baselines. More robust unknown domain generalization techniques and one-class classification methods could enhance the rigor of comparisons.\n   - The absence of AUC metrics is notable; presenting this would provide a more comprehensive assessment considering class imbalance.\n  \n2. **Lack of Ablation/Hypothetical Sensitivity Studies**: The algorithm‚Äôs complexity introduces several moving parts that may affect reliability. The absence of ablation studies inhibits understanding of how the method's performance is dependent on its components.\n   - The definition of a rare class based on a 2-sigma distance lacks sensitivity analysis for alternative values.\n   - The paper does not fully define the complex steps in Algorithm 1, and there may be limitations in linear techniques like PCA applied to clip features.\n   - The specific formulation of expert knowledge presented raises questions about the testability of alternative approaches.\n\n### Minor Comments\n1. The abstract and introduction require clearer, more technical details about the method's contributions.\n2. Certain claims and terminology, particularly regarding the scope of AI in medicine, are misleading and require clarification.\n3. Technical details such as feature overlap computation are not defined clearly, and the anonymous code link provided is broken.\n4. The writing quality, including clarity and flow, can be improved; redundancy and typos abound, affecting readability.\n\n### Summary Paragraph\nThe manuscript presents a method that shows potential in addressing a recognized problem within the machine learning community. However, its significance is tempered by limited experimentation and questionable baselines, affecting its reproducibility. There is some innovation in combining expert knowledge with neural networks, yet the lack of rigorous evaluation and clarity detracts from its overall contribution. Improvements in method presentation and a deeper exploration of variants would benefit the manuscript.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RareSaGE*, an algorithm designed to generalize to unseen domains when only a single known domain is available for training. It targets the issue of rare class classification by combining domain-invariant expert knowledge with neural network modeling. Experiments on real medical imaging data indicate improved results compared to both neural and expert knowledge‚Äìbased baselines. The paper addresses a relevant and underexplored topic, is technically interesting, and demonstrates real-world applicability, though the overall experimental scope and clarity of presentation remain limited.\n\n---\n\n**Major Comments**  \n1. **Limited Experimental Evaluation:** The experimental section begins relatively late (page 9) and focuses only on one fMRI dataset. This narrow evaluation raises concerns about generalizability to other domains or modalities and limits broader impact. The dataset, algorithmic, and methodological details could be moved to supplementary material to allow space for deeper experimentation, including ablation, computational, and failure analyses.  \n   - The choice of baselines may be insufficient: it is unclear whether the compared methods in Table 1 represent strong domain-generalization candidates. Additional baselines such as one-class or anomaly detection approaches should be considered or at least discussed.  \n   - Reporting of performance metrics could be improved: including AUC would better represent class-imbalanced performance than accuracy alone.  \n\n2. **Missing Ablation and Sensitivity Studies:** The approach includes several components and hyperparameters that might affect robustness, yet none have been evaluated systematically. Examples include:  \n   - The use of a 2‚Äësigma rule to define rare classes‚Äîits selection and influence remain unexplored.  \n   - Algorithm¬†1 contains vaguely defined steps; specific quantitative definitions are required.  \n   - Reliance on linear PCA for feature dimensionality might constrain performance; nonlinear alternatives could be examined.  \n   - The formulation of expert knowledge (Eq.¬†6) seems specific; its adaptability to other forms should be discussed.\n\n---\n\n**Minor Comments**  \n- **Abstract and Introduction:** Provide clearer descriptions of the method‚Äôs technical details (how ranking, ‚Äúfocused handling,‚Äù and integration of expert knowledge operate).  \n- **Terminology and Clarity:** Some phrases are misleading or overly broad (e.g., ‚ÄúAI for medicine‚Äù limited to diagnosis). Clarify references to ‚Äúclasses‚Äù in discussions of intra‚Äëclass variability.  \n- **Missing Details:** Specify how feature resemblance is computed, and note that the anonymous code link is broken.  \n- **Writing and Structure:** Merge redundant sections (‚Äúsolution overview/details‚Äù), improve sentence flow, and correct run‚Äëon sentences and typos. Ensure readable equation formatting and proper citation spacing.\n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study tackles an important and relatively underexplored problem‚Äîrare‚Äëclass generalization from a single domain‚Äîand introduces a conceptually appealing synthesis of expert knowledge with neural models. The approach appears innovative, and the presented results show promise. However, limited experimental breadth, lack of ablation and sensitivity analysis, and incomplete methodological clarity restrict confidence in the claims and reproducibility. Improving the evaluation scope and technical precision would strengthen the paper‚Äôs overall impact.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents RareSaGe, a novel approach for SGD in detecting rare events in medical imaging. First, this method handles the most similar classes by using a pre-trained large vision model (LVM) to rank classes based on their similarity to the rare event class, which aims to effectively classify the rare event. Then, it further integrates expert knowledge with deep learning to enhance robustness and address challenges related to limited data. Experiments conducted on multi-center datasets for seizure onset zone (SOZ) detection in fMRI data demonstrate that RareSaGe achieves high generalization performance, significantly outperforming other models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The manuscript is written in clear English and is relatively easy to follow.\n2. The experimental results demonstrate the effectiveness of the proposed framework.\n3. The motivation for developing the method makes sense\n\n### Weaknesses\n\n1. The title is focused on MEDICAL IMAGING, but the data is too singular, validated only on fMRI data and limited to two categories (two centers).\n2. The introduction to the dataset in Section 4.1 and the Supplementary Material is relatively limited.\n2. There are missing many important ablation studies, such as validating the effectiveness of the two types of expert knowledge on the method.\n\n### Questions\n\n1. The title is focused on MEDICAL IMAGING, but the data is too singular, validated only on fMRI data and limited to two categories (two centers).\n2. In Section 4.1, the link to the dataset is unavailable; it was not previously a publicly available dataset. The introduction to the dataset in Section 4.1 and the Supplementary Material is relatively limited. \n3. Additionally, there are no visual results of the dataset presented in the manuscript, leading to insufficient feasibility of the results.\n4. In Section 3.4.2 EXPERT KNOWLEDGE ON RS-fMRI, it is mentioned that \"These locations can be extracted employing established image processing algorithms.\" The visual results obtained from these algorithms for specific locations should be presented.\n5. There are many important ablation studies missing, such as validating the effectiveness of the two types of expert knowledge on the method.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript introduces RareSaGe, an innovative approach designed for stochastic gradient descent (SGD) to detect rare events in medical imaging. By utilizing a pre-trained large vision model (LVM) to rank class similarities and integrating expert knowledge with deep learning, this method aims to enhance classification accuracy despite the challenge of limited data. The experiments conducted on multi-center datasets for seizure onset zone (SOZ) detection in fMRI show that RareSaGe notably outperforms other existing models, indicating its potential contribution to the field.\n\n**Major Comments:**\n1. The manuscript's title emphasizes MEDICAL IMAGING, yet the dataset is narrowly focused, validated only on fMRI data and limited to two categories from two centers.\n2. The introduction to the dataset in Section 4.1 and Supplementary Material is insufficiently detailed.\n3. Important ablation studies are missing, particularly regarding the validation of the two types of expert knowledge utilized in the method.\n\n**Minor Comments:**\n- The title should reflect the limited scope of the dataset to avoid misleading readers.\n- Section 4.1 lacks a link to the dataset; it was not previously available publicly.\n- The absence of visual results from the dataset in the manuscript limits the feasibility assertions.\n- In Section 3.4.2, visual results from established image processing algorithms should be included to illustrate the extracted locations.\n  \n**Summary Paragraph:**\nIn reviewing the manuscript against TMI editorial criteria, the significance of RareSaGe in the context of rare event detection is evident, particularly due to its novel integration of expert knowledge. However, its innovation is somewhat constrained by the dataset's limited size and the lack of comprehensive evaluation, particularly in ablation studies. While the reproducibility of results based on the current dataset is promising, it cannot be fully assessed without broader validation.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RareSaGe*, a novel strategy for stochastic gradient descent (SGD) aimed at detecting rare events in medical imaging. The method leverages a pre-trained large vision model (LVM) to rank classes by similarity to a rare event class and integrates expert knowledge to strengthen model robustness under limited data conditions. Experiments on multi-center datasets for seizure onset zone (SOZ) detection in fMRI demonstrate promising generalization and superior performance compared with other approaches. Overall, the paper is clearly written, logically organized, and presents a well-motivated problem.  \n\n**Major Comments**  \n1. **Scope of Validation** ‚Äì The application focus suggested by the title (‚Äúmedical imaging‚Äù) is broad, yet the evaluation is restricted to fMRI data from only two centers, which limits generalizability across other imaging modalities.  \n2. **Dataset Description and Availability** ‚Äì The dataset used in Section‚ÄØ4.1 is insufficiently described. The provided link is unavailable, and details in the Supplementary Material are minimal. More transparent dataset access and clearer documentation are needed.  \n3. **Ablation and Expert Knowledge Validation** ‚Äì Key ablation analyses are missing, particularly to confirm the contribution of the two forms of expert knowledge incorporated in the model. Without these, it is difficult to assess how much each component contributes to the reported gains.  \n4. **Visualization and Algorithmic Outputs** ‚Äì The paper mentions extraction of anatomical locations using ‚Äúestablished image processing algorithms‚Äù (Section‚ÄØ3.4.2), yet no visual examples or representative outputs are shown. Including visualizations would substantiate the feasibility of the approach.  \n\n**Minor Comments**  \n- Supplement Section‚ÄØ4.1 could include basic dataset statistics and representative images.  \n- Figures or qualitative examples would enhance clarity and interpretability.  \n- The text is generally clear, though a more balanced clarification of data sources and centers would benefit readers.  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of *significance*, the study addresses an important challenge‚Äîrare event detection in medical data‚Äîbut its impact is constrained by limited dataset diversity. The proposed integration of LVM-based similarity ranking and expert knowledge is *innovative*, yet the lack of ablation analyses limits evaluation of novelty strength. The *evaluation* is solid within the given dataset but would need broader validation to confirm robustness. Regarding *reproducibility*, incomplete dataset accessibility and missing implementation details reduce transparency.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper discusses Single Domain Generalization (SDG) in the context of rare event classification, particularly in medical diagnosis, where limited data and imbalanced datasets pose significant challenges. The authors propose a method that utilizes a pre-trained large vision model to rank classes by their similarity to rare events, enabling focused classification. By integrating domain-invariant expert knowledge with data-driven deep learning, the approach improves model robustness and performance, even with scarce data. A case study on detecting seizure onset zones using fMRI data shows that this method achieves an average F1 score of 90.2%.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis paper addresses an interesting and important problem: how to leverage large vision models (LVMs) to tackle rare diseases, which involve imbalanced, infrequent cases and domain shifts. While there are many SDG methods available, the authors claim that none have been specifically applied to rare cases in medical data. It appears that the authors are addressing a new problem.\n\n### Weaknesses\n\nProblematic Settings\n\n1. While there are currently no methods addressing SDG in the context of limited data and rare diseases, this setting appears to combine elements of SDG with few-shot imbalanced classification. If a SDG method is sufficiently robust, it could potentially be integrated with existing imbalanced learning or rare disease fine-tuning techniques to effectively address this issue. However, the authors lack experiments to validate this point, which raises questions about the relevance of this particular setting in the field. The authors have the opportunity to tackle a significant challenge: how to integrate large vision models (LVMs) for rare disease classification, taking into account that rare diseases can vary in size, may be in-domain or out-of-domain, and may be either imbalanced or balanced, thereby providing a more comprehensive framework.\n\nLimited Discussions and Comparisons of Results\n\n2. The authors have not compared their approach with existing SDG and imbalanced learning methods, which diminishes the convincing nature of their experiments and calls into question their claim of being state-of-the-art (SOTA).\n\n### Questions\n\nPlease see my comments above\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript investigates Single Domain Generalization (SDG) as applied to rare event classification in medical diagnosis, addressing the significant challenges posed by limited and imbalanced datasets. The authors propose a novel method that leverages a pre-trained large vision model to rank classes based on their similarity to rare events, thereby enhancing focused classification. By merging domain-invariant expert knowledge with data-driven deep learning, the approach aims to improve model robustness and performance, even under data scarcity. A case study analyzing seizure onset zones using fMRI data demonstrates an impressive average F1 score of 90.2%.\n\n**Major Comments:**\n1. **Problematic Settings:** Although no methods exist that address SDG specifically in the context of limited data and rare diseases, the proposed setting appears to blend elements of SDG and few-shot imbalanced classification. A robust SDG method could incorporate existing imbalanced learning or rare disease fine-tuning techniques to effectively address these challenges. However, the lack of experimental validation raises concerns about the relevance of the current approach.\n   \n2. **Limited Discussions and Comparisons of Results:** The authors do not compare their proposed method to existing SDG and imbalanced learning approaches. This absence of comparisons undermines the credibility of their findings and raises doubts about their claim to being state-of-the-art (SOTA).\n\n**Minor Comments:**\n- Ensure clarity in defining terms related to imbalanced datasets and rare diseases.\n- Consider including a table summarizing experimental results for better clarity.\n- Review grammar for typos and ensure consistent notation across the manuscript.\n\n**Summary Paragraph:**  \nIn terms of significance, the study tackles an important issue by exploring the integration of large vision models for rare disease classification, an innovative angle in the realm of SDG. However, the evaluation lacks a robust comparison to existing methods, limiting its reproducibility and generalizability. While the proposed method shows promise in performance metrics, the foundational experiments and discussions require strengthening to present a more comprehensive framework that acknowledges the variability inherent in rare diseases.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of Single Domain Generalization (SDG) in rare event classification, particularly within medical diagnosis tasks where data scarcity and class imbalance are major challenges. The authors propose leveraging a pre-trained large vision model (LVM) to rank classes according to their similarity to rare events, thereby enabling targeted classification. By integrating domain-invariant expert knowledge with data-driven learning, the method aims to enhance robustness and performance in low-data regimes. A case study on seizure onset zone detection using fMRI data reportedly yields an average F1 score of 90.2%. Overall, the paper presents an interesting attempt to combine SDG and LVMs for rare disease detection, though certain aspects of motivation, validation, and comparison require strengthening.  \n\n**Major Comments**  \n1. **Problem Setting and Validation:** While the problem‚ÄîSDG under limited and imbalanced data‚Äîis compelling, it appears to merge aspects of SDG and few-shot imbalanced classification. The paper does not sufficiently justify this combined setting or validate whether the proposed approach meaningfully differs from or improves upon existing techniques in these adjacent areas. Further experiments examining integration with standard imbalanced-learning or rare-disease fine-tuning methods would better establish the necessity and relevance of the proposed framework.  \n2. **Comparative Analysis:** The manuscript lacks quantitative or qualitative comparisons with existing SDG and imbalanced learning baselines. Without such benchmarks, the claims of state-of-the-art performance remain unsubstantiated. Including comparisons with leading SDG and related rare-event methods would notably improve the empirical rigor and credibility of the results.  \n\n**Minor Comments**  \n- The manuscript would benefit from a clearer discussion delineating SDG, few-shot, and imbalanced learning settings to aid contextual understanding.  \n- Clarify whether ‚Äúrare diseases‚Äù in the experiments correspond to in-domain or out-of-domain distributions.  \n- Ensure consistent terminology when describing large vision models and normalization terms throughout the text.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses a timely and significant medical AI problem, proposing an innovative integration of pre-trained visual models for SDG in rare conditions. The conceptual direction is promising, but current evidence is limited by insufficient comparison and unclear framing of novelty. Evaluation lacks breadth, and reproducibility could be improved through more detailed experimental validation and discussion of generalization boundaries. With revisions targeting these issues, the work could make a more convincing contribution.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI:**  \n‚úÖ The manuscript proposes *RareSaGe*, a methodological framework for *Single-Domain Generalization (SDG)* targeting *rare event detection in medical images*. Its focus on integrating discriminative expert knowledge with data-driven deep learning (DL) for improved generalization is methodological rather than purely application-oriented. The case study ‚Äî seizure onset zone (SOZ) detection from fMRI ‚Äî is used to validate the general method. This aligns with *IEEE TMI‚Äôs methodological scope* (imaging algorithms, integration of machine learning with imaging).  \n\n**2. Novelty & Contribution Level:**  \nThe paper identifies a gap in prior SDG literature: lack of techniques handling *rare class problems* (imbalanced and limited data). It defines a new theoretical framing (entropy-based rarity criterion) and integrates symbolic expert knowledge with DL to handle overlap classes. Methodological novelty lies in:  \n- an entropy-driven definition of ‚Äúrare class,‚Äù  \n- algorithmic framework (RareSaGe) combining DL and expert knowledge via a cascading classifier design,  \n- demonstration of domain invariance and single-domain generalization without target data.  \nThis is *conceptually distinct* but somewhat incremental in its integration aspects (DL + knowledge-based models). The contribution would reach TMI‚Äôs novelty threshold if claims on generalizability and cross-center validation are well substantiated.  \n\n**3. Technical and Experimental Rigor:**  \n- Mathematical formalization (entropy-based rarity, CRLB rationale) is solid, though some derivations could benefit from clearer connection to performance metrics.  \n- Experimental design includes cross-center (A‚ÜîB) and aggregated validations, with appropriate metrics (accuracy, precision, sensitivity, F1).  \n- Reported F1 > 90% across domains, demonstrating robustness ‚Äî yet statistical significance or uncertainty quantification beyond SDs is limited.  \n- Datasets are realistic (fMRI from pediatric/adult centers), IRB-approved, anonymized; however, generalizability beyond SOZ detection is not empirically shown.  \n- Reproducibility: code link provided, though anonymized (acceptable for review).  \n\n**4. Clarity and Presentation:**  \nGenerally clear and well organized. However:  \n- Long sentences and heavy use of inline references hinder readability.  \n- Figures and tables are appropriate but schematic (Fig.‚ÄØ1 could better indicate data flow and model interaction).  \n- Some grammatical errors and formatting inconsistencies (e.g., equation alignment).  \n\n**5. Ethical and Reproducibility Compliance:**  \n‚úî Explicit IRB approval and anonymization are stated.  \n‚úî Code and data (anonymized link) provided.  \nOverall compliance is adequate.  \n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThe manuscript presents *RareSaGe*, a novel algorithm for detecting rare events in medical imaging under single-domain settings. It introduces an entropy-based quantification of rarity and integrates expert knowledge‚Äìderived symbolic features through a quadratic optimization classifier with deep learning classifiers that handle overlapping classes. Using resting-state fMRI data for seizure onset zone localization across two centers, the method demonstrates strong cross-center and aggregated performance, reportedly outperforming standard CNNs, Vision Transformers, and knowledge-only systems.\n\n**2. Strengths**  \n- Clear methodological focus on the *SDG + rare-event* intersection.  \n- Innovative fusion of domain knowledge with DL in a structured, theoretically supported framework.  \n- Empirical validation across institutions, with substantial improvement in F1 scores.  \n- Conceptual interpretability through propositional knowledge features, enhancing trust and explainability in clinical AI.\n\n**3. Weaknesses**  \n- Limited generalization evidence beyond a single clinical application (SOZ detection).  \n- Some theoretical constructs (entropy definition vs Fisher information) are insufficiently validated empirically.  \n- Absence of ablation study quantifying the contribution of each module (e.g., knowledge integration vs DL alone).  \n- Presentation occasionally dense; method flow and parameter choices (e.g., choice of tc =‚ÄØ0.9) need clearer justification.  \n- Comparisons to related hybrid or physics-informed learning techniques could be expanded.  \n\n**4. Major Comments**  \n1. **Methodological clarity:** Please provide a detailed pseudocode or diagram clarifying how DL and knowledge modules interact at inference (currently only briefly described).  \n2. **Ablation analysis:** Quantify how much each component (entropy selection, overlap separation, knowledge QO machine) contributes to the final performance.  \n3. **Theoretical justification:** Validate the entropy‚Äìinformation relationship empirically (show correlation between class entropy and misclassification rate).  \n4. **Generalizability:** Discuss applicability beyond the fMRI task‚Äîwould RareSaGe generalize to other modalities (CT, histopathology)?  \n5. **Statistical robustness:** Add confidence intervals or statistical tests comparing baseline vs proposed method.  \n\n**5. Minor Comments**  \n- Improve figure captions and ensure all acronyms (e.g., QO, RSN) are defined where first used.  \n- Check reference formatting for consistency (e.g., MICCAI volumes, doi spacing).  \n- Correct some grammar/typographical errors.  \n- Clarify whether the hybrid method was tuned using validation from the same domain only (to ensure proper SDG evaluation).  \n\n**6. Overall Recommendation:**  \n**Major Revision** ‚Äì The work demonstrates substantial methodological potential and solid experimental basis, but requires additional clarification, broader generalization evidence, and minor writing improvement to meet TMI publication standards.\n\n**7. Confidence Level:** 4/5 (high confidence in assessment; moderate confidence in reproducibility of quantitative claims).\n\n---\n\n**Summary Statement:**  \n*RareSaGe* presents a promising framework linking expert knowledge with data-driven learning to address single-domain generalization for rare-event imaging. Technically sound and novel enough for *IEEE‚ÄØTMI*, contingent upon stronger experimental substantiation and clearer presentation.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses Single Domain Generalization (SDG) for rare event detection in medical imaging, focusing on seizure onset zone (SOZ) identification from fMRI data. The authors propose RareSaGe, which integrates domain-invariant expert knowledge with deep learning to handle challenges of limited data, class imbalance, and domain shifts. The method first uses CLIP to identify the \"overlap class\" most similar to the rare event class, then employs deep learning for overlap/non-overlap classification and knowledge-based quadratic optimization for rare/non-rare distinction. Expert knowledge is encoded as atomic propositions about anatomical brain regions and activation patterns. Evaluation on two-center datasets (pediatric and adult patients) demonstrates F1 scores of 90.2% for across-trial validation and 94.7% for aggregate-trial validation, outperforming state-of-the-art vision transformers and large vision models.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Definition 3.1's entropy-based rarity criterion uses 2œÉ threshold but lacks theoretical justification (Section 3.1, Equations 1-3)\n  - The connection between Cramer-Rao Lower Bound theory and class entropy in Equation 4 is stated without rigorous derivation (Page 5)\n  - Algorithm 1 references \"significant change in validation accuracy\" in line 2 but this termination criterion is never formally defined or used (Page 6)\n\n‚Ä¢ **Experimental design limitations and evaluation gaps**\n  - Limited dataset size with only two centers (52 and 31 patients) raises questions about generalizability claims (Section 4.1)\n  - No comparison with domain adaptation or multi-source domain generalization methods that the paper criticizes (Table 1)\n  - Cross-validation methodology unclear for aggregate trials - whether patient-level or image-level splits were used affects validity (Section 4.2)\n\n‚Ä¢ **Methodological concerns regarding knowledge integration**\n  - Expert knowledge formula Œ∫SOZ in Equation 6 appears ad-hoc without validation of individual atomic propositions (Section 3.4.2)\n  - CLIP similarity threshold of 0.78 vs 0.74 for overlap class selection lacks sensitivity analysis (Section 3.4.3)\n  - Confidence threshold tc set to 0.9 without systematic optimization or ablation study (Section 3.4.3)\n\n‚Ä¢ **Insufficient technical details and reproducibility issues**\n  - Missing hyperparameters and training details for baseline methods make comparison validity questionable (Table 1)\n  - SMOTE application for SOZ class balancing mentioned but parameters not specified (Section 3.4.2)\n  - No statistical significance testing reported despite claims of superior performance (Tables 1-2)\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical foundations and provide rigorous derivations**\n  - Provide theoretical justification for the 2œÉ threshold in Definition 3.1 through empirical analysis or literature support\n  - Include complete derivation linking Barron's entropy-Fischer information relationship to the CRLB argument\n  - Formalize the termination criterion in Algorithm 1 or remove the undefined validation accuracy condition\n\n‚Ä¢ **Expand experimental evaluation and address dataset limitations**\n  - Include additional medical centers or synthetic domain shifts to strengthen generalizability claims beyond two-center validation\n  - Add quantitative comparisons with domain adaptation and multi-source methods using the same datasets\n  - Clarify cross-validation splits and ensure patient-level separation to avoid data leakage, particularly for aggregate trials\n\n‚Ä¢ **Validate knowledge integration approach systematically**\n  - Conduct ablation studies on individual atomic propositions in Equation 6 to demonstrate their necessity and sufficiency\n  - Perform sensitivity analysis on CLIP similarity thresholds and provide principled selection criteria\n  - Optimize confidence threshold tc systematically through grid search or cross-validation rather than fixed selection\n\n‚Ä¢ **Enhance technical documentation and statistical rigor**\n  - Provide complete hyperparameter specifications and training protocols for all baseline methods to ensure fair comparison\n  - Specify SMOTE parameters and justify synthetic sample generation strategy for the rare SOZ class\n  - Include statistical significance tests (e.g., McNemar's test, confidence intervals) to validate performance improvements over baselines",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces RareSaGe, a novel algorithm that integrates domain-invariant expert knowledge with deep learning (DL) techniques for single domain generalization (SDG) in rare event detection in medical imaging. The focus is on addressing the challenges posed by limited and imbalanced data, as well as overlapping data and knowledge rules between classes. The proposed approach utilizes a large vision model (LVM) and class-wise entropy to identify the most similar class to the rare event class, and then employs both DL and knowledge-based models to enhance the detection of rare events. The paper presents a case study on seizure onset zone (SOZ) detection using fMRI data, demonstrating that the proposed method significantly outperforms state-of-the-art DL techniques and knowledge-based systems, achieving an average F1 score of 90.2% across multi-center datasets.\n\n## Major Comments\n1. Novelty and Positioning: The manuscript introduces a promising approach for rare event detection in medical imaging, leveraging expert knowledge alongside DL. However, the novelty is somewhat diminished by the fact that the integration of expert knowledge with DL models has been explored in previous literature. The authors need to provide a clearer distinction and comparison with existing methods that combine DL and expert knowledge, particularly in the context of rare event detection.\n\n2. Evaluation Design: The evaluation is performed on two datasets from different centers, with a focus on cross-validation and aggregated validation. While these experiments are thorough, the reliance on only two datasets limits the generalizability of the findings. The authors should consider expanding the evaluation to include more diverse datasets, possibly from different imaging modalities, to further validate the robustness of their approach.\n\n3. Comparisons: The performance comparisons against state-of-the-art DL techniques and knowledge-based systems are compelling, but the authors should provide more detailed comparisons with recent advancements in rare event detection, particularly those that incorporate expert knowledge or domain-specific constraints. This would help establish the incremental contribution of the proposed method.\n\n4. Reproducibility: The manuscript mentions that code and data are available, which is commendable. However, the reproducibility section should be expanded to include more detailed descriptions of the experimental setup, hyperparameter tuning procedures, and any preprocessing steps. This would facilitate independent replication of the results.\n\n## Minor Comments\n1. Clarity of Figures: Some figures (e.g., Figure 1) could benefit from additional labels and annotations to improve clarity. Specifically, the steps involved in the model schematic should be more clearly delineated.\n\n2. Notation Consistency: There are inconsistencies in the notation used throughout the manuscript, particularly in the definitions and descriptions of class-wise entropy and overlap class extraction. The authors should ensure that the notation is consistent and well-defined.\n\n3. Acronyms and Terminology: Several acronyms and technical terms are introduced without clear definitions (e.g., \"LVM\", \"CLIP\"). Providing a glossary or defining these terms when they are first introduced would enhance the readability of the manuscript.\n\n4. Typographical Errors: There are a few typographical errors throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). These should be corrected to maintain the professional quality of the paper.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: detecting rare events in medical imaging datasets that are often limited and imbalanced. The proposed RareSaGe algorithm, which integrates expert knowledge with DL, represents a potentially valuable innovation in the field of single domain generalization. However, the evaluation is currently limited to two datasets, which may not fully demonstrate the robustness and generalizability of the approach. The reproducibility of the method is also somewhat uncertain due to incomplete details in the methodological description. Overall, while the idea has promise, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include a broader range of recent advancements in rare event detection, strengthen validation across diverse datasets, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Ayan Banerjee",
      "BIN XU",
      "Payal Kamboj",
      "Sandeep Gupta"
    ],
    "url": "pdfs/iclr.cc-2025-conference_6b804922fe464f4b49ffe111b1d8d47e1d6592be.pdf",
    "remote_url": "https://openreview.net/pdf/6b804922fe464f4b49ffe111b1d8d47e1d6592be.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Training Physics-Driven Deep Learning Reconstruction without Raw Data Access for Equitable Fast MRI",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Computational Imaging",
      "Fast MRI",
      "Unsupervised Learning",
      "Compressed Sensing",
      "Deep Learning",
      "Equity"
    ],
    "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Even though PD-DL offers higher acceleration rates compared to existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. One impediment for their deployment is the difficulties with generalization to pathologies or population groups that are not well-represented in training sets. This has been noted in several studies, and fine-tuning on target populations to improve reconstruction has been suggested. However, current training approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and underserved areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose CUPID for high-quality PD-DL training, using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates the goodness of the output with a compressibility-based approach, while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show that CUPID achieves similar quality compared to well-established PD-DL training strategies that require raw k-space data access, while outperforming conventional compressed sensing (CS) and state-of-the-art generative methods. We also demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden. As an approach that radically deviates from existing strategies, CUPID presents an opportunity to provide equitable access to fast MRI for underserved populations in an attempt to reduce the inequalities associated with this expensive imaging modality.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe article proposes a deep-learning reconstruction method without having access to multi-coil $k$-space data. For the full-column-rank forward operator $E_{\\Omega}$ and the under-sampled $k$-space $\\textbf{y}$, the authors assume access to $E_{\\Omega}^+ \\textbf{y}$ where $+$ denotes the Moore-Penrose inverse. Compared to having access to fully sampled $k$-space as in fully supervised reconstruction, or undersampled $k$-space as in self-supervised reconstruction, this assumption is less restrictive. The proposed method was shown to perform on par with PD-DL methods requiring $k$-space data at relatively low acceleration rates ($4\\times$).\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The data access assumption is less restrictive compared to fully- and self-supervised reconstruction methods, and the article focuses on a real-life problem, as raw k-space access is a known limitation in this field.\n2. The method is physics-based; thus, it is expected to be less prone to hallucinations compared to methods that do not utilize the forward operator.\n3. The method was tested in various settings, focusing on both retrospective and prospective undersampling.\n\n### Weaknesses\n\n1. The quantitative results table is missing the standard error of the mean, which is crucial for a fair comparison.\n2. It is incorrect to say that equispaced undersampling patterns were not explored in score-based models. In fact, the authors have already cited at least one article (\"Robust Compressed Sensing MRI with Deep Generative Priors\") that explored equispaced undersampling, discussing it not only in the appendix but also in the main text (e.g., Fig. 2, 9, 10). \n3. The generative method baseline is weak and outdated, given how rapidly state-of-the-art methods are advancing.  Several approaches published in the last 12 months can achieve better results. Including this particular method is fine, but labeling it as state-of-the-art is misleading.\n\n### Questions\n\n1. Does the clinical PI reconstruction readily allow saving the complex-valued image without any research agreements, or does it only allow the magnitude image to be saved? I am asking because it is rare to see a DICOM dataset containing anything other than magnitude images.\n2. Regardless of whether ScoreMRI is state-of-the-art, its performance appears unexpectedly low. Is it possible that it was significantly under-trained? In our experience, these models require much longer training times compared to unrolled networks, which might explain why it underperforms even at $4\\times$.\n3. The choice of perturbations seems somewhat arbitrary. Could you elaborate on this choice? Learning the perturbations simultaneously with reinforcement learning could also be a potential direction for future work.\n4. In Figure $2$, loss function, the left hand side was written as $\\mathcal{L}(\\mathbf{x}^{(k)}, \\mathbf{x_{PI}})$ but the $\\mathcal{L}_{\\text{comp}}$ on the right hand side has $\\mathbf{x}^{(m)}$ rather than $\\mathbf{x}^{(k)}$. Is this intentional or a typo? \n5. Line $151-152$, while unrolled methods achieved top positions in reconstruction challenges three years ago, it is unknown if they remain the highest performers. To the best of my knowledge, no recent, fair comparison between state-of-the-art diffusion and unrolled methods has been published, including a reader study to compare their clinical effectiveness.\n\n**Minor comments** \n\n* The fastMRI knee matrix size is $320\\times 320$, not $320\\times 368$. Please see Table 1 in DOI: 10.1148/ryai.2020190007\n* Line $42-43$, demand have shown -> demand has shown\n* Line $269$, shapes that has different intensity values -> shapes that have different intensity values\n* Line $46$, Fig.$1$ -> Fig. $1$, the space was omitted in most, if not all, references to figures, tables, and sections.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a deep-learning reconstruction method that operates without access to multi-coil \\(k\\)-space data. The authors propose an approach utilizing the Moore-Penrose inverse of the full-column-rank forward operator \\(E_{\\Omega}\\) and the under-sampled \\(k\\)-space data \\(\\textbf{y}\\). This assumption is less restrictive than those found in fully supervised or self-supervised reconstruction methods. The results indicate that the proposed method performs comparably to PD-DL techniques that require \\(k\\)-space data at low acceleration rates of \\(4\\times\\).\n\n**Major Comments:**  \n1. The assumption of data access is notably less restrictive, addressing a critical limitation in the field.\n2. The physics-based approach is expected to reduce the likelihood of hallucinations, distinguishing it from non-physics-based methods.\n3. The method underwent testing in diverse settings, assessing both retrospective and prospective undersampling.\n4. **Weaknesses:**\n   - The quantitative results table lacks the standard error of the mean, critical for comparisons.\n   - Claims regarding equispaced undersampling patterns in score-based models are incorrect as previous literature, including their own citations, has explored these.\n   - The generative method baseline is outdated, as recent advances in the field could render it insufficient; labeling it as state-of-the-art misrepresents its relevance.\n\n**Minor Comments:**  \n- The fastMRI knee matrix should be referenced as \\(320\\times 320\\), not \\(320\\times 368\\) (see Table 1 in DOI: 10.1148/ryai.2020190007).\n- Correct grammatical errors such as \"demand have shown\" to \"demand has shown\" (Lines 42-43).\n- Adjust \"shapes that has different intensity values\" to \"shapes that have different intensity values\" (Line 269).\n- Ensure consistent formatting, e.g., \"Fig. 1\" rather than \"Fig.$1$\" (Line 46).\n\n**Summary Paragraph:**  \nIn relation to the TMI editorial criteria, the manuscript demonstrates significance by addressing a practical issue in data accessibility for deep-learning reconstruction. Its innovation lies in utilizing a less restrictive assumption regarding data access. Evaluation could be improved with more detailed quantitative results, including standard errors, and by adopting a more representative baseline for comparison. Reproducibility may be questioned due to potential training inadequacies in the comparison model, ScoreMRI, which could affect findings.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a deep-learning‚Äìbased MRI reconstruction framework designed to operate without requiring access to multi-coil \\( k \\)-space data. The approach assumes access to the Moore‚ÄìPenrose inverse of the under-sampled acquisition, which is a less restrictive condition than those typically imposed in fully or self-supervised methods. The algorithm integrates the physical forward operator into the learning process and demonstrates comparable reconstruction performance to PD-DL approaches on low-acceleration (\\( 4\\times \\)) data. The paper is clearly written and technically sound, addressing a practically relevant problem where raw \\( k \\)-space availability is often limited.\n\n**Major Comments**  \n1. **Quantitative Evaluation:** The reported results lack standard error of the mean values, which are necessary to assess statistical significance and ensure fair comparison across methods.  \n2. **Literature Coverage:** The claim that equispaced undersampling has not been studied in score-based models is inaccurate. The cited paper *‚ÄúRobust Compressed Sensing MRI with Deep Generative Priors‚Äù* explicitly discusses this sampling strategy in the main text and figures, not only in the appendix.  \n3. **Baselines:** The generative model baseline used appears out of date; several stronger methods introduced in the past year outperform it. While inclusion of this baseline is acceptable, describing it as state-of-the-art is misleading.  \n4. **Methodological Details and Clarifications:**  \n   - Clarify whether the clinical PI reconstruction allows saving complex-valued images directly or only magnitude reconstructions.  \n   - Provide details on the training duration and optimization of ScoreMRI, as its relatively low performance may be due to under-training.  \n   - Explain how perturbations were chosen and whether joint or learned perturbation strategies could improve results.  \n   - In Figure 2, verify consistency between the left-hand loss term \\(\\mathcal{L}(\\mathbf{x}^{(k)}, \\mathbf{x}_{PI})\\) and its right-hand counterpart using \\(\\mathbf{x}^{(m)}\\).  \n   - Update Lines 151‚Äì152 to accurately reflect the current landscape of unrolled versus diffusion-based methods, as recent comparative studies are lacking.\n\n**Minor Comments**  \n- Correct ‚ÄúfastMRI knee matrix size‚Äù to \\(320 \\times 320\\) (see DOI: 10.1148/ryai.2020190007).  \n- Line 42‚Äì43: ‚Äúdemand have shown‚Äù ‚Üí ‚Äúdemand has shown.‚Äù  \n- Line 269: ‚Äúshapes that has‚Äù ‚Üí ‚Äúshapes that have.‚Äù  \n- Add missing spaces before figure, table, and section references (e.g., ‚ÄúFig.$1$‚Äù ‚Üí ‚ÄúFig.¬†1‚Äù).  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript addresses an important practical limitation‚Äîrestricted access to \\( k \\)-space‚Äîthrough a physics-informed deep-learning reconstruction approach. Its innovation lies in relaxing data-access assumptions while maintaining competitive performance. The method is well evaluated on multiple sampling scenarios, though the absence of uncertainty measures and limited comparison breadth reduce the robustness of conclusions. Reproducibility appears feasible but would benefit from additional training details and clarified implementation aspects. Overall, the work is methodologically sound and of moderate significance, with clear room for improvement in evaluation completeness.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper present an novel MRI reconstruction method that only requires dicom data (even undersampled data), with the motivation that rural regions won't have the raw data access. The key idea is through using random perturbations.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\n1. I like the idea of using random image domain perturbations, sounds like a good way to capture pior distribution.\n2. I appreciate the authors for the experiments on the prospective undersampled MRI reconstruction, which is a very important experiment for all MRI reconstruction tasks.\n\n### Weaknesses\n\nOverall, there are a few weaknesses that concerns me:\n1. I didn't really buy the idea of this rural region raw-data access. why we need raw-data? its for the training purpose! we already have a great number of raw k-space data (from fastMRI, mridata.org and others), as long as we have a good model trained, the rural regions can just use it without any training. In the end of the day, all MRI scanner will sample raw k-space data, so the whole motivation doesn't sound solid to me. \nMeanwhile, from what i understand from your method, the input is CG-SENSE of undersampled MRI, is it realistic? the input quality is not really diagnoistic.\n\n2. The presentation is not very clear and the paper is not well-writtened, all the method section for your method is condensed to one *single* page with a figure (figure 2), i have to go back and forth to understand whats R, whats DF, and how you do the perturbations, thats supposed to be the main part of the paper.\n\n3. The score-MRI results look kinda weird to me, try to look at other generative models. It shouldn't be that bad.\n\n4. Figure 3, the result doesn't look good to me, over smoothing textures.\n\n### Questions\n\n1. how do your method compared to single-coil DL-based reconstruction.\n2. how sensitive is your method to different input? (different vendors tend to have different reconstruction pipelines.)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph:\nThe manuscript presents a novel MRI reconstruction method that leverages DICOM data, including undersampled datasets, addressing the challenges faced in rural regions with limited access to raw data. The central innovation lies in employing random perturbations to enhance reconstruction quality.\n\n### Major Comments:\n1. The justification for focusing on rural regions and raw data access appears weak. The reliance on raw data seems unnecessary for training, as there are numerous available datasets (e.g., fastMRI, mridata.org). A well-trained model should suffice for rural applications without requiring additional training. This undermines the paper's motivation.\n2. The method's input, CG-SENSE of undersampled MRI data, raises concerns about realism and diagnostic quality.\n3. Presentation clarity is lacking; the methods section is condensed into a single page accompanied by a figure (Figure 2). This leads to confusion regarding key terms (e.g., R, DF) and the perturbation process, which are fundamental to the paper.\n4. The score-MRI results need further scrutiny, as they present inconsistencies compared to existing generative models‚Äîthis raises questions about validity.\n5. The results shown in Figure 3 indicate over-smoothing of textures, further questioning the effectiveness of the proposed method.\n\n### Minor Comments:\n- Consider revising the clarity of figures and terminology used throughout the manuscript to enhance reader comprehension.\n- There are some grammatical issues that need to be addressed, particularly ‚Äúwell-writtened.‚Äù\n\n### Summary Paragraph:\nIn summary, the manuscript demonstrates potential significance in addressing MRI reconstruction within rural contexts, showing innovation through random perturbations. However, evaluation appears flawed due to weak justification regarding the need for raw data and real-world applicability. Reproducibility is also a concern, given issues with clarity and presentation. Overall, while the paper captures important issues in MRI technology, revisions are required to strengthen its contributions and delivery.\n\n### Decision Recommendation:\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a novel MRI reconstruction method that operates directly on DICOM data, including undersampled images, addressing the scenario where raw k-space data may not be accessible‚Äîparticularly in resource-limited settings. The approach introduces random perturbations in the image domain to capture data priors and improve reconstruction quality. While the idea is interesting and potentially useful, the paper‚Äôs presentation and clarity are limited, and the motivation for the proposed setting is not fully convincing.\n\n---\n\n**Major Comments**  \n1. **Motivation and Practicality:** The rationale regarding rural regions lacking raw data access is unconvincing. In practice, raw k-space data are widely available through public datasets (e.g., fastMRI, mridata.org) for training purposes. Once trained, models can be deployed broadly without requiring local raw data. Furthermore, MRI scanners inherently collect raw data for reconstruction, which challenges the stated motivation.  \n2. **Input Data and Realism:** The method appears to use CG-SENSE reconstructions of undersampled MRI as input, but the diagnostic realism of such inputs is questionable. The manuscript should clarify the realism and feasibility of this assumption.  \n3. **Method Description and Clarity:** The methods section is condensed into roughly one page and lacks sufficient detail. It is difficult to follow key definitions (e.g., ‚ÄúR,‚Äù ‚ÄúDF‚Äù) and the perturbation process. This section should be expanded and more clearly explained.  \n4. **Experimental Results:** The Score-MRI reconstructions appear unusual compared with other generative reconstruction methods and should be critically analyzed. Figure‚ÄØ3 results exhibit oversmoothing and loss of texture detail.  \n5. **Comparative Evaluation:** A comparison with single-coil deep learning‚Äìbased reconstructions would provide valuable context. Sensitivity analysis to input variations from different vendors or reconstruction pipelines should also be discussed.\n\n---\n\n**Minor Comments**  \n- The overall writing and presentation need significant improvement for readability.  \n- Figures could be labeled more clearly, and figure captions should provide sufficient explanatory detail.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study introduces an interesting concept of DICOM-based reconstruction via random perturbations, which could broaden the accessibility of advanced MRI reconstruction. However, its significance is diminished by a weak practical motivation and unclear methodological explanations. While innovative in concept, the evaluation is insufficient and comparative analyses are limited. The paper‚Äôs current presentation also hinders reproducibility.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis manuscript proposed an unsupervised approach to improve MR DICOM image quality by introducing the loss term $l_{comp}+\\lambda l_{pif}$, where $l_{comp}$ is to suppress noise and $l_{pif}$ is to keep fidelity to the input.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThis manuscript is well-written, and the figures are well-drafted.\n\n### Weaknesses\n\n1. The motivation is weak. Starting reconstruction from DICOM output rather than raw k-space is unusual for MRI practitioners, as it does not allow us to construct the encoding operator, $\\mathrm{E}$.\n\n2. The title is misleading: claiming a \"physics-driven\" approach while dismissing the need for raw k-space data is contradictory. In MRI, k-space data represents the underlying physical imaging process.\n\n3. The reported results for the baseline method, scoreMRI, are questionable, with performance significantly below that in the original publication.\n\n4. The novelty and contribution of this work are limited. The author attempted to improve the image quality with the proposed loss term $l_{comp}+\\lambda l_{pif}$, where $l_{comp}$ is to suppress noise and $l_{pif}$ is to keep fidelity to the input. In my eyes, this is a simple combination of existing approaches for MRI image quality enhancement.\n\n### Questions\n\n1. Refer to Weakness 1, how the coil sensitivity and sampling pattern are handled in the $\\mathrm{E}$?\n\n2. Refer to Weakness 2, how to construct the $\\mathrm{E}$ for scoreMRI?\n\n3. The proposed method resembles SSDU, yet its performance is reportedly better to CUPID. Could you clarify the reason for this?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents an unsupervised approach aimed at improving the quality of MR DICOM images by introducing the loss term \\(l_{comp} + \\lambda l_{pif}\\), where \\(l_{comp}\\) targets noise suppression and \\(l_{pif}\\) maintains fidelity to the input data. While the writing quality is commendable and the figures are well-prepared, the overall contribution of the manuscript to the field appears limited.\n\n**Major Comments:**  \n1. The motivation for the proposed approach is weak. Initiating reconstruction from DICOM output instead of raw k-space is atypical for MRI practitioners, hindering the ability to construct the encoding operator, \\(\\mathrm{E}\\).  \n2. The title claims a \"physics-driven\" approach, which is contradictory given the dismissal of raw k-space data, a critical component of the MRI imaging process.  \n3. Results reported for the baseline method, scoreMRI, raise concerns, as performance is significantly lower than in the original publication.  \n4. The novelty of this work is questionable. The proposed loss term \\(l_{comp} + \\lambda l_{pif}\\) appears to be a basic combination of existing methods aimed at enhancing MRI image quality.\n\n**Minor Comments:**  \n- Consider revising the title for clarity to accurately reflect the manuscript‚Äôs content.  \n- Ensure all figures are properly labeled and referenced within the text.  \n- Clarify notation for terms used in the proposed methodology to enhance understanding.  \n\n**Summary Paragraph:**  \nIn terms of significance, the manuscript presents a potentially relevant area of research in MRI image quality enhancement; however, the innovation is limited as it builds on existing methodologies without significant advancement. The evaluation design lacks clarity, particularly regarding the handling of the encoding operator, \\(\\mathrm{E}\\), and baseline performance metrics appear unreliable. Reproducibility remains uncertain given the methodology employed. \n\n**Decision Recommendation:**  \nMajor revision is recommended to address the identified weaknesses, particularly in motivation, novelty, and methodological clarity before reconsideration for publication.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents an unsupervised approach aimed at improving MR DICOM image quality through a composite loss function, \\( l_{comp} + \\lambda l_{pif} \\), where \\( l_{comp} \\) suppresses noise and \\( l_{pif} \\) preserves fidelity to the input data. The paper is clearly written and accompanied by well-prepared figures. However, concerns arise regarding the motivation, physical grounding, and the level of novelty in the proposed approach.  \n\n**Major Comments**  \n1. **Motivation and Methodological Basis:** The choice to perform reconstruction from the DICOM output rather than raw k-space data is unconventional in the MRI field. This prevents proper construction of the encoding operator \\( \\mathrm{E} \\), which is essential in modeling the imaging process. The rationale behind this design choice requires stronger justification.  \n2. **Conceptual Consistency:** The title‚Äôs reference to a ‚Äúphysics-driven‚Äù approach is misleading, as the method ignores raw k-space information‚Äîthe core representation of the physical imaging process in MRI. This inconsistency undermines the claimed scientific framing.  \n3. **Baseline Comparisons:** The performance of the baseline method, scoreMRI, appears significantly lower than reported in its original publication, raising concerns about the correctness or fairness of the comparison.  \n4. **Novelty and Contribution:** The proposed loss function appears to be a straightforward combination of existing terms for noise suppression and fidelity preservation, which limits its originality and impact relative to existing MRI image enhancement approaches.  \n\n**Minor Comments**  \n- Clarify how coil sensitivity maps and sampling patterns are handled within \\( \\mathrm{E} \\).  \n- Explain how the encoding operator is constructed for scoreMRI.  \n- Provide insight into why the proposed method outperforms CUPID while resembling SSDU.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe manuscript addresses a relevant problem in improving MR image quality but offers limited methodological innovation and weak physical justification. The evaluation lacks strong baselines and sufficient explanation of comparative methods, affecting the work‚Äôs significance and reproducibility. While the presentation quality is acceptable, the overall contribution does not substantively advance the field under TMI‚Äôs criteria of significance, innovation, evaluation, and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial methodological clarification, strengthened motivation, and more rigorous experimental validation are required before this work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äì Analytical‚ÄØ&‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI**  \n‚úÖ‚ÄØThe manuscript proposes a new learning framework for physics‚Äëdriven deep‚Äëlearning MRI reconstruction that eliminates the need for raw‚ÄØk‚Äëspace data.‚ÄØThis is a clear methodological advance in image‚Äêreconstruction theory and thus lies squarely within the technical scope of‚ÄØIEEE‚ÄØTMI.\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel**  \nThe work introduces *CUPID‚ÄØ(Compressibility‚Äëinspired‚ÄØUnsupervised‚ÄØLearning‚ÄØvia‚ÄØParallel‚ÄØImaging‚ÄØFidelity)*‚Äîan unsupervised, zero‚Äëshot training loss for PD‚ÄëDL networks requiring only routine DICOM images.  Unlike prior self‚Äësupervised or generative schemes, it removes dependency on raw‚ÄØdata by combining a re‚Äëweighted‚ÄØ‚Ñì‚ÇÅ compressibility term with a novel ‚Äúparallel‚Äëimaging‚Äëfidelity‚Äù constraint based on synthetic perturbations.  This represents a substantive conceptual innovation.‚ÄØNovelty and potential impact are high.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \nTheory and equations are consistent, the relationship to existing PD‚ÄëDL formulations is well grounded, and the method is evaluated on both retrospective (fastMRI‚ÄØdatasets) and prospective‚ÄØ7‚ÄØT‚ÄØacquisitions.‚ÄØComparisons against multiple baselines, ablation on Œª‚ÄØweight and perturbation number are provided.‚ÄØHowever, two aspects merit clarification:  \n- Details of perturbation generation and statistical reproducibility (randomness, parameter sensitivity).  \n- Statistical significance of quantitative metrics and error bars across subjects.\n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation**  \nOverall readability and organization are strong.‚ÄØFigures are informative, though crowded; certain visual panels (e.g., Figs.‚ÄØ4‚Äì5) could benefit from consistent scale bars and annotation fonts.‚ÄØMinor language polishing would further improve flow.\n\n**5.‚ÄØEthical‚ÄØ&‚ÄØReproducibility‚ÄØCompliance**  \nThe text indicates datasets from fastMRI‚ÄØ(public) and an institutional 7‚ÄØT‚ÄØstudy with subject anonymization; code release is planned.‚ÄØThese satisfy typical TMI reproducibility standards, but explicit mention of IRB‚ÄØapproval for the in‚Äëhouse study would be advisable.\n\n---\n\n**Phase‚ÄØ2 ‚Äì‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n###‚ÄØ1.‚ÄØSummary  \nThe paper introduces‚ÄØ*CUPID*, a physics‚Äëdriven unsupervised training strategy for deep‚Äëlearning MRI reconstruction that operates without access to raw‚ÄØk‚Äëspace.‚ÄØThe approach combines a compressibility‚Äëbased regularization with a parallel‚Äëimaging‚Äëfidelity loss implemented through designed perturbations, enabling network training directly from scanner‚Äëexported DICOMs.‚ÄØExperiments on knee and brain datasets show performance comparable to supervised or self‚Äësupervised PD‚ÄëDL methods that require raw‚ÄØdata, and superior to compressed‚Äësensing and generative‚Äëprior baselines.‚ÄØZero‚Äëshot adaptation on prospectively undersampled‚ÄØ7‚ÄØT‚ÄØdata further demonstrates practicality for deployment in resource‚Äëlimited settings.\n\n###‚ÄØ2.‚ÄØStrengths\n- **High novelty:** first demonstration of PD‚ÄëDL training using only image‚Äëdomain data.  \n- **Strong methodological foundation:** well‚Äëformulated loss combining compressibility and physics consistency.  \n- **Comprehensive evaluation:** multi‚Äëdataset, retrospective‚ÄØ+‚ÄØprospective tests, and thoughtful ablation studies.  \n- **Potential clinical impact:** removes data‚Äësharing and vendor‚Äëaccess barriers, promoting equitable dissemination of advanced MRI‚ÄØreconstructions.\n\n###‚ÄØ3.‚ÄØWeaknesses\n- Perturbation design, randomness control, and computational overhead not fully quantified.  \n- Limited statistical analysis of performance variation across subjects or scanners.  \n- Minor clarity issues in figures and notation.  \n- No explicit statement of IRB‚ÄØor consent procedures for prospective experiments.\n\n###‚ÄØ4.‚ÄØMajor‚ÄØComments\n1.‚ÄØ**Clarify perturbation strategy** ‚Äì‚ÄØProvide algorithmic detail or pseudocode defining amplitude, support, and sampling of the perturbation‚ÄØp‚ÄØsets; discuss sensitivity of results to these design choices and potential domain dependence.  \n2.‚ÄØ**Quantify uncertainty / statistics** ‚Äì‚ÄØInclude standard deviation or confidence intervals for PSNR/SSIM across test subjects and report statistical significance versus baselines.  \n3.‚ÄØ**Resource and runtime analysis** ‚Äì‚ÄØContrast training time and compute requirements of CUPID versus SSDU or supervised PD‚ÄëDL to substantiate claims of minimal fine‚Äëtuning burden.  \n4.‚ÄØ**Ethics and data use** ‚Äì‚ÄØState whether prospective data collection was IRB‚Äëapproved and whether subjects provided informed consent.‚ÄØAlso specify anonymization and data‚Äësharing plans.  \n5.‚ÄØ**Clarify limitations** ‚Äì‚ÄØExpand on conditions where only magnitude DICOMs are available and outline realistic strategies for extending CUPID to such cases.\n\n###‚ÄØ5.‚ÄØMinor‚ÄØComments\n- Line‚ÄØ53: ‚ÄúPHYSICS‚ÄØ‚Äì‚ÄØDRIVEN‚Äù ‚Äì‚ÄØremove space and hyphen for consistency.  \n- Verify consistent notation between‚ÄØx_PI‚ÄØand‚ÄØxPI.  \n- Figures‚ÄØ3‚Äì5: add scale bars, identical windowing, and mark zoom locations.  \n- References: harmonize citation style (author‚ÄØet‚ÄØal.,‚ÄØyear) across text.  \n- Typographical edits for hyphenation and spacing throughout.\n\n###‚ÄØ6.‚ÄØOverall‚ÄØRecommendation  \n**Major‚ÄØRevision.**  \nThe paper presents a clearly novel and methodologically significant contribution appropriate for‚ÄØTMI.‚ÄØBefore acceptance, additional methodological clarity, statistical analysis, and minor presentation/ethical details are needed to meet the journal‚Äôs standard of rigor and reproducibility.\n\n###‚ÄØ7.‚ÄØConfidence‚ÄØLevel  \n**5‚ÄØ/‚ÄØ5**‚ÄØ‚Äì‚ÄØHigh confidence in assessment based on familiarity with PD‚ÄëDL‚ÄØMRI reconstruction literature.\n\n---\n\n**Summary‚ÄØJudgment:**  \n*CUPID* offers a genuinely new direction for unsupervised PD‚ÄëDL reconstruction without raw‚ÄØdata access.‚ÄØWith additional methodological transparency and expanded quantitative validation, the work has strong potential for publication in‚ÄØIEEE‚ÄØTransactions‚ÄØon‚ÄØMedical‚ÄØImaging.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis work proposes CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), a novel training method for physics-driven deep learning (PD-DL) MRI reconstruction that operates without requiring raw k-space data access. The method addresses the limitation that current PD-DL approaches require raw MRI measurements, which are typically unavailable outside specialized research centers, particularly in rural and underserved areas. CUPID combines two loss terms: a compressibility-based term using reweighted ‚Ñì1 norm on wavelet coefficients (Equation 8) and a parallel imaging fidelity term that ensures consistency through carefully designed perturbations (Equation 9). The approach can operate in both database training and zero-shot subject-specific modes using only routine clinical DICOM images. Experimental validation on fastMRI knee and brain datasets with retrospective R=4 undersampling and prospective R=9 brain acquisition demonstrates performance comparable to supervised and self-supervised methods while outperforming compressed sensing and generative approaches.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - The parallel imaging fidelity loss in Equation 9 lacks theoretical justification for why the expectation over perturbations should approximate true parallel imaging consistency, as no convergence analysis is provided\n  - The notation switches inconsistently between f(yŒ©, EŒ©; Œ∏) in Section 2.2 and f(xPI, EŒ©) in Section 3 without clear explanation (Page 5, line 226-227)\n  - The reweighted ‚Ñì1 formulation in Equation 8 uses x(m) from \"mth reweighting step\" but the iterative reweighting procedure is not mathematically defined or described\n\n‚Ä¢ **Limited experimental validation scope**\n  - Only uniform/equidistant undersampling patterns are evaluated (Section 4.1, line 294-295), which represents a narrow subset of clinical acceleration strategies and limits generalizability assessment\n  - The prospective validation uses only a single high-field 7T acquisition at R=9 (Section 4.1, lines 309-311), providing insufficient evidence for clinical translation across different field strengths and acceleration rates\n  - Comparison with generative methods is limited to ScoreMRI only (Table 1), missing evaluation against other state-of-the-art diffusion-based reconstruction approaches mentioned in Section 2.3\n\n‚Ä¢ **Perturbation design lacks principled foundation**\n  - The choice of perturbations as \"randomly rotated and positioned letters, numbers, card suits\" (Page 5, lines 251-252) appears arbitrary without theoretical or empirical justification for why these specific shapes are optimal\n  - The fold-over constraint design is described only conceptually (Page 5, lines 249-250) without mathematical formulation or validation that perturbations truly satisfy parallel imaging resolvability\n  - The ablation study on perturbation number (Appendix B.1, Figure 7) shows only K‚àà{1,3,6,10} without exploring the convergence behavior or computational trade-offs systematically\n\n‚Ä¢ **Insufficient analysis of clinical deployment constraints**\n  - The discussion of filtering operations in Section 4.6 (lines 491-500) acknowledges limitations with vendor processing but provides no quantitative assessment of robustness to these common clinical modifications\n  - The magnitude-only DICOM limitation (Section 4.6, lines 507-518) is mentioned as future work but represents a significant practical barrier that undermines the claimed accessibility for underserved areas\n  - No computational cost analysis is provided for the training procedure, which is crucial for assessing feasibility in resource-limited settings as claimed in the motivation\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical rigor and notation consistency**\n  - Provide theoretical analysis demonstrating that the expectation over perturbations in Equation 9 converges to a meaningful parallel imaging consistency measure, including convergence rates and required number of samples\n  - Standardize notation throughout the manuscript, clearly defining when and why the function signature changes from f(yŒ©, EŒ©; Œ∏) to f(xPI, EŒ©) with explicit parameter handling\n  - Formalize the iterative reweighting procedure for Equation 8 with algorithmic description, convergence criteria, and relationship to standard reweighted ‚Ñì1 minimization\n\n‚Ä¢ **Expand experimental validation comprehensively**\n  - Include evaluation on multiple undersampling patterns (random, variable density, radial) across different acceleration rates (R=2,3,6,8) to demonstrate broader clinical applicability beyond uniform patterns\n  - Conduct prospective validation across multiple field strengths (1.5T, 3T, 7T) and anatomical regions with varying acceleration rates to establish generalization capabilities for clinical deployment\n  - Compare against additional state-of-the-art generative reconstruction methods including recent diffusion models and score-based approaches beyond ScoreMRI\n\n‚Ä¢ **Develop principled perturbation methodology**\n  - Establish theoretical criteria for perturbation design based on parallel imaging theory, providing mathematical conditions that ensure resolvability and optimal reconstruction performance\n  - Formalize the fold-over constraint mathematically with explicit conditions relating perturbation spatial frequency content to acceleration factor and coil sensitivity patterns\n  - Conduct systematic ablation studies exploring perturbation characteristics (shape complexity, intensity distribution, spatial frequency content) with convergence analysis for different K values\n\n‚Ä¢ **Address practical deployment considerations quantitatively**\n  - Evaluate robustness to common vendor filtering operations through controlled experiments with simulated and real clinical processing pipelines, providing quantitative performance degradation analysis\n  - Implement and validate solutions for magnitude-only DICOM reconstruction including joint phase and coil sensitivity estimation within the CUPID framework\n  - Provide detailed computational cost analysis comparing training time, memory requirements, and hardware specifications needed for CUPID versus existing methods in resource-constrained environments",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), a novel approach for training physics-driven deep learning (PD-DL) models for fast MRI reconstruction using only routine clinical reconstructed images (DICOM format) without requiring raw k-space data. The proposed method combines compressibility-based loss terms with parallel imaging fidelity to ensure high-quality reconstructions. The authors demonstrate the effectiveness of CUPID through extensive evaluations on both retrospective and prospective undersampled datasets, comparing it against various supervised and unsupervised baselines. The manuscript is well-written, and the motivation for addressing the limitations of raw k-space data access is compelling.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: While CUPID is a novel approach, the manuscript could benefit from a more detailed discussion on how it compares to existing methods that aim to train models without raw k-space data. Specifically, the authors should clarify how CUPID's compressibility-based loss and parallel imaging fidelity differ from other self-supervised and generative methods that operate without raw data access.\n   \n2. Evaluation Design: The evaluation primarily focuses on retrospective undersampling and includes a limited prospective study. More extensive prospective evaluations on different anatomies and acceleration rates would strengthen the claim of generalizability. Additionally, the inclusion of more diverse datasets and anatomical sites would enhance the validity of the findings.\n\n3. Comparisons: The manuscript compares CUPID with a variety of methods, including supervised PD-DL, self-supervised learning (SSDU), and generative models (ScoreMRI). However, the comparisons lack some recent advancements in unsupervised and self-supervised learning for MRI reconstruction. Including these methods would provide a more comprehensive benchmark.\n\n4. Reproducibility: The authors mention that code will be released upon acceptance, which is positive. However, the manuscript could be improved by providing more detailed descriptions of the experimental setup, including hyperparameters and training protocols. This would facilitate reproducibility and allow for independent verification of the results.\n\n###\n\n## Minor Comments\n1. Clarity in Notation: Some sections, such as the introduction of the parallel imaging fidelity term, could benefit from clearer explanations and notation. The notation for the forward operator and perturbations needs to be consistent and well-defined.\n   \n2. Figures and Tables: Figures 3 and 4 are somewhat cluttered. Providing fewer representative slices with zoomed-in regions would improve readability. Additionally, the figures could benefit from clearer labeling and legends to aid in understanding.\n   \n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definitions. Providing a glossary or defining acronyms when they are first introduced would enhance clarity.\n   \n4. Typographical Issues: There are a few typographical errors throughout the manuscript, such as \"k-spacce\" and \"undersampling maskes\". Careful proofreading would improve the manuscript's readability.\n\n###\n\n## Summary Paragraph\n(focusing on 4 TMI Editorial Criteria)\nThe manuscript addresses a significant challenge in fast MRI reconstruction by proposing CUPID, a method that enables high-quality PD-DL training using only routine clinical reconstructed images. The innovation lies in combining compressibility-based loss terms with parallel imaging fidelity, which is a novel approach to mitigate the need for raw k-space data. The evaluation is comprehensive, covering both retrospective and prospective undersampling, and the results demonstrate that CUPID performs comparably to or better than existing methods that require raw data access. However, the evaluation could be strengthened with more diverse datasets and anatomical sites. The reproducibility of the approach is promising, but more detailed methodological descriptions are needed to ensure independent replication. Overall, the manuscript has significant merit and addresses a pressing issue in MRI reconstruction, but further enhancements in evaluation and reproducibility would elevate its impact.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis to include recent advancements in unsupervised and self-supervised learning for MRI reconstruction. They should also conduct more extensive prospective evaluations across different anatomies and acceleration rates. Additionally, providing more detailed descriptions of the experimental setup and training protocols would enhance reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Mehmet Akcakaya",
      "Merve Gulle",
      "Yasar Utku Alcalar"
    ],
    "url": "pdfs/iclr.cc-2025-conference_058122326c88eb7ecc35a0739e0ce1f6c8ad76ad.pdf",
    "remote_url": "https://openreview.net/pdf/058122326c88eb7ecc35a0739e0ce1f6c8ad76ad.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Lecture Learning",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "Multimodal assistant",
      "surgical",
      "multimodal instruction-following data",
      "dataset"
    ],
    "abstract": "Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images. Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos. One major contributing factor is the absence of datasets in the surgical field. In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far. To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. We will release our code, model, and the instruction-tuning dataset.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal large language model designed as a conversational assistant for surgical applications. To support this, the authors developed Surg-QA, a large-scale dataset containing 102,000 surgical video-instruction pairs, generated through a structured two-stage question-answer pipeline. This pipeline helps extract structured knowledge from surgical lecture videos, enabling the LLaVA-Surg model to understand complex surgical procedures and answer open-ended questions in a zero-shot setting. The model leverages CLIP for visual encoding and is fine-tuned on Surg-QA to specialize in surgical video question-answering, achieving superior performance compared to existing general-domain models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe authors provide a novel dataset, Surg-QA, which is a significant resource for training multimodal surgical models, covering diverse surgical procedures and question-answer pairs.\n2.\tThe two-stage pipeline for question-answer generation mitigates hallucinations in LLM outputs, resulting in higher quality and reliability of generated data.\n3.\tLLaVA-Surg demonstrates notable improvements over general multimodal models in zero-shot surgical video question-answering tasks, showcasing its efficacy in understanding surgical context.\n\n### Weaknesses\n\n1. The paper should compare its model with recent multimodal LLM approaches, specifically ReAct (Yao et al., 2023), which combines reasoning and action for complex tasks.\n[1] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023, January). ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations (ICLR).\n2. Using CLIP for frame-by-frame encoding lacks temporal modeling and increases processing costs and redundancy, burdening the LLM as frame count grows.\n3. The paper lacks an in-depth error analysis, especially regarding potential hallucinations or misunderstandings in complex surgical scenarios. Although the authors claim to reduce hallucinations, achieving perfect performance seems challenging.\n4. The model‚Äôs adaptability to other medical or clinical fields is unclear, as broader evaluations on datasets like RAD, SLAKE, and PathVQA are missing, which may limit its wider applicability.\n\n### Questions\n\n1. Does splitting video into frames for CLIP‚Äôs visual encoder lead to a loss of spatiotemporal information, and wouldn‚Äôt a video encoder like Video Swin Transformer [2] better capture temporal dynamics?\n[2] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., & Hu, H. (2022). Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 3202-3211).\n2. How does LLaVA-Surg perform compared to other state-of-the-art multimodal methods? In addition to general multimodal models, a detailed comparison with models like ReAct would provide a more comprehensive evaluation. Has comparison with other two-stage methods [3] in VQA task been overlooked?\n[3] Gai, X., Zhou, C., Liu, J., Feng, Y., Wu, J., & Liu, Z. (2024). MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale. arXiv preprint arXiv:2404.12372.\n3. Is the two-stage question-answer generation process applicable to other medical fields, and if so, what adjustments would be required? Additionally, validating the method‚Äôs performance on public datasets like RAD, SLAKE, and PathVQA would strengthen its generalizability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents LLaVA-Surg, a multimodal large language model intended to function as a conversational assistant for surgical applications. The authors introduce Surg-QA, a comprehensive dataset comprising 102,000 surgical video-instruction pairs generated through a structured two-stage question-answer pipeline. This approach allows for the extraction of structured knowledge from surgical lectures, enabling LLaVA-Surg to comprehend complex procedures and respond to open-ended questions in a zero-shot scenario. The model employs CLIP for visual encoding and is fine-tuned on Surg-QA, demonstrating superior performance relative to existing general-domain models.\n\n**Major Comments:**  \n1. The authors should compare LLaVA-Surg with recent multimodal LLM approaches, particularly ReAct (Yao et al., 2023), which integrates reasoning and action for complex tasks.  \n2. The use of CLIP for frame-by-frame encoding may lack temporal modeling capabilities, resulting in higher processing costs and redundancy as frame count increases.  \n3. There is insufficient in-depth error analysis regarding potential hallucinations or misunderstandings in complex surgical contexts, despite claims of reducing such occurrences.  \n4. The adaptability of LLaVA-Surg to other medical or clinical fields requires clarification, as broader evaluations on datasets like RAD, SLAKE, and PathVQA are absent, which limits its potential applications.\n\n**Minor Comments:**  \n- The manuscript would benefit from improved clarity in its presentation, particularly in illustrating the two-stage pipeline methodology.  \n- Clarifications are needed for acronyms and terminology used throughout the paper.  \n- A few typographical errors were noted that should be corrected for improved readability.\n\n**Summary Paragraph:**  \nIn evaluating the manuscript against TMI editorial criteria, LLaVA-Surg demonstrates significance through its novel dataset and contributions to surgical procedures. While it showcases innovation in multimodal question-answering, the evaluation lacks comparisons with state-of-the-art methods, and the reproducibility of its findings may be questioned due to limited testing on broader datasets. Overall, the strengths lie in its unique resource and improved performance, while the weaknesses include a need for additional comparisons and evaluations.\n\n**Decision Recommendation:**  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **LLaVA-Surg**, a multimodal large language model designed as a conversational assistant for surgical applications. It introduces a supporting dataset, **Surg-QA**, containing 102,000 surgical video‚Äìinstruction pairs generated through a structured two-stage question‚Äìanswer pipeline. This process aims to extract meaningful knowledge from surgical lecture videos, enhancing the model‚Äôs capability to interpret and respond to complex surgical procedures in a zero-shot setting. The model integrates CLIP for visual encoding and fine-tunes it on Surg-QA, demonstrating improved performance compared with general multimodal models. Overall, the paper is clearly written and introduces a valuable resource for surgical AI, though some methodological and comparative aspects warrant further clarification.  \n\n**Major Comments**  \n1. **Comparative Evaluation** ‚Äì The model should be compared with recent multimodal LLMs such as *ReAct* (Yao et al., 2023), which integrates reasoning and action for complex tasks. Including such baselines would strengthen the assessment of novelty and performance.  \n2. **Lack of Temporal Modeling** ‚Äì Using CLIP for frame-by-frame visual encoding omits temporal dependencies and results in higher computational costs and redundancy, posing scalability concerns.  \n3. **Error Analysis and Hallucination Assessment** ‚Äì The paper does not provide a detailed error analysis. Although the two-stage pipeline aims to reduce hallucinations, the model‚Äôs behavior in complex or ambiguous surgical scenarios remains insufficiently analyzed.  \n4. **Generalizability** ‚Äì The model‚Äôs applicability to other medical domains is unclear. Broader testing on publicly available datasets (e.g., RAD, SLAKE, PathVQA) would help evaluate transferability and robustness.  \n\n**Minor Comments**  \n- Clarify whether the two-stage question‚Äìanswer generation process could generalize to non-surgical domains, and specify what modifications may be needed.  \n- Consider evaluating temporal visual encoding using methods like the Video Swin Transformer (Liu et al., 2022).  \n- Provide quantitative comparisons with two-stage medical VQA approaches such as *MedThink* (Gai et al., 2024).  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in proposing a domain-specialized multimodal assistant and introducing the large-scale Surg-QA dataset. Its innovation lies in the structured two-stage Q&A generation pipeline, which helps reduce hallucinations. However, evaluation could be more comprehensive‚Äîparticularly with inclusion of recent multimodal baselines and temporal modeling analyses. Reproducibility appears feasible with well-documented datasets and methods, though more detailed error and generalization studies would enhance rigor.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a novel surgical multimodal dataset, which consists of over 102,000 video-instruction pairs generated through a two-stage pipeline, aimed at enhancing the understanding and conversational capabilities of surgical videos.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n1. With over 102,000 video-instruction pairs, this dataset is the largest in the surgical field.\n2. Structured data annotation pipeline using LLMs minimizes the risk of generating inaccurate or nonsensical content, improving dataset reliability.\n3. Releasing the dataset, model, and code publicly fosters further research and development in the surgical AI domain.\n4. The dataset can be a valuable resource for training and education, helping surgical trainees learn through interactive Q&A about real procedures.\n\n### Weaknesses\n\n1. The paper does not address how the data's quality is maintained as the videos are obtained from the web. The clinicians have reviewed the output of their MLLM model, but the paper does not confirm whether clinicians or domain experts have reviewed the raw data to ensure accuracy and reliability.\n2. Concerns regarding the release, privacy, and permission risks associated with using sensitive surgical videos are not adequately discussed.\n3. The paper lacks comprehensive validation across essential surgical downstream tasks and other surgical QA datasets, which are crucial for demonstrating clinical usability. There is also a need for more rigorous benchmarking against a broader range of state-of-the-art video MLLM architectures to establish the dataset's utility and the model's performance more robustly.\n4. The comparison of the proposed methods with SOTA methods is limited and does not include the latest works. The manuscript also lacks evaluations with models trained on other surgical datasets, limiting the assessment of the proposed model's generalizability across different surgical scenarios.\n5. The paper may need to evaluate the visual quality of the surgical videos.\n\n### Questions\n\n1. How can the quality of the data be ensured? The data collected may already contain a lot of noise and has been reprocessed by an LLM. Is there any person or clinician reviewing these raw data?\n2. Can the data be released? Are there privacy and permission risks associated with the collected data?\n3. The authors need to conduct more zero-shot evaluations on downstream tasks relevant to the surgical field, such as phase recognition, action/instrument classification, and other surgical domain VQA data to demonstrate the clinical usability of their method.\n4. The authors need to compare with more state-of-the-art methods. The comparison methods in Table 3 were all first released in 2023.\n5. The authors may verify their dataset on more benchmarks of SOTA Video MLLM architectures.\n6. Also, the authors need more zero-shot comparisons with the same VLM trained on other surgical datasets, to showcase the generalizability of their proposed dataset.\n7. The authors may evaluate the visual quality of the surgical videos themselves, as they are obtained from the website.\n\n### Flag For Ethics Review\n\n- Yes, Legal compliance (e.g., GDPR, copyright, terms of use)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPotential copyright problem for online data.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel surgical multimodal dataset consisting of over 102,000 video-instruction pairs, developed through a two-stage pipeline. This dataset aims to enhance the understanding and conversational capabilities within surgical videos, proposing significant contributions to the field of surgical AI.\n\n**Major Comments:**\n1. The paper does not sufficiently address the quality maintenance of the data sourced from the web. While clinician review of the MLLM model output is mentioned, the manuscript fails to confirm if domain experts have assessed the raw data for accuracy.\n2. Privacy and permission risks related to the use of sensitive surgical videos are not adequately considered in the paper.\n3. There is a lack of comprehensive validation across essential surgical downstream tasks and other surgical QA datasets, which is critical for establishing clinical usability. Additionally, more rigorous benchmarking against a wider array of state-of-the-art (SOTA) video MLLM architectures is necessary to validate the dataset's utility.\n4. Comparisons between proposed methods and SOTA techniques are limited and exclude the most recent studies. The lack of evaluations involving models trained on other surgical datasets restricts the assessment of generalizability across various surgical scenarios.\n5. An evaluation of the visual quality of the surgical videos would enhance the overall rigor of the work.\n\n**Minor Comments:**\n- Clarify how the quality of the raw data is ensured, considering potential noise from online extraction and LLM processing.\n- Discuss potential privacy and permission risks involved in data release.\n- Conduct further zero-shot evaluations on downstream tasks relevant to surgery, including phase recognition and action/instrument classification.\n- Expand the comparison with state-of-the-art methods, especially those released beyond 2023.\n- Verify dataset efficacy against more benchmarks of SOTA video MLLM architectures.\n- Include additional zero-shot comparisons with the same VLM on other surgical datasets to demonstrate generalizability.\n- Evaluate the visual quality of surgical videos as they originate from external sources.\n\n**Summary Paragraph:**  \nThe manuscript demonstrates notable strengths, including the generation of the largest dataset in the surgical field and the structured annotation pipeline leveraging LLMs. However, it also has significant weaknesses: insufficient quality controls, privacy concerns, and inadequate validation and benchmarking hinder the overall impact of the work. In terms of significance, innovation, evaluation, and reproducibility, the dataset offers potential contributions but falls short in ensuring quality and demonstrating clinical applicability.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a large-scale multimodal surgical dataset consisting of over 102,000 video‚Äìinstruction pairs created through a two-stage pipeline. The primary goal is to enhance understanding and conversational capabilities for surgical video analysis. The paper describes the dataset generation process, model design, and intended use in surgical education and AI research. The overall presentation is understandable, but the manuscript lacks depth in validation and ethical discussion.  \n\n**Major Comments**  \n1. **Data Quality Assurance:** The provenance and quality of web-sourced videos are insufficiently discussed. The paper confirms clinician review of the generated model outputs but not of the raw data itself, leaving uncertainty about dataset accuracy and reliability.  \n2. **Privacy and Ethical Considerations:** There are potential risks regarding the release of sensitive surgical videos without confirmed consent or appropriate anonymization. Privacy and permission compliance (e.g., GDPR, copyright) need thorough clarification.  \n3. **Insufficient Evaluation:** The dataset‚Äôs utility is not fully demonstrated through downstream surgical tasks such as phase recognition, instrument classification, or surgical VQA evaluation. Broader and more rigorous benchmarking is necessary.  \n4. **Limited Comparison to State of the Art:** The methods compared in the paper are mostly from 2023, omitting several recent SOTA models. Comparisons with existing surgical datasets and newer multimodal architectures are required to assess generalizability and performance robustness.  \n5. **Visual Quality Assessment:** The study does not evaluate the visual clarity or suitability of the web-collected videos, which may affect model training quality and usability.  \n\n**Minor Comments**  \n- Some figures and tables could benefit from clearer labeling of comparison methods.  \n- The manuscript would be strengthened by specifying the pre-processing steps used before model training.  \n- Terminology should be standardized‚Äîfor example, clearly distinguishing ‚ÄúLLM,‚Äù ‚ÄúVLM,‚Äù and ‚ÄúMLLM.‚Äù  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the dataset size and open release represent a valuable contribution to surgical AI research. Regarding **innovation**, the structured two-stage annotation pipeline shows methodological novelty. However, weaknesses in **evaluation**‚Äîespecially lacking downstream task validation and broader comparisons‚Äîlimit the evidence supporting clinical utility. **Reproducibility** is strong due to planned public release but weakened by unclear data sourcing and ethical compliance.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper contributes an important resource but requires stronger evaluation, clearer ethical transparency, and enhanced benchmarking to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal conversational assistant based on surgical videos. Additionally, they introduce a new dataset with 102,000 question-answer pairs for training multimodal LLMs. The authors provide details of their data generation procedure, which is carefully designed to avoid hallucinations. The paper provides detailed comparisons with existing general-purpose and surgical-purpose datasets. Lastly, the authors provide a human and LLM evaluation of the dataset, showing consistent scores.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- **Clarity**: The paper is well-written and easy to follow. \n- **Contributions**: This work makes a significant contribution to the development of surgical chat assistants. The dataset contains a wider range of surgical QAs compared to previous works. The proposed model and dataset may be valuable resources for researchers in this area.\n\n### Weaknesses\n\n- **Dataset Availability**: The surgical videos are available on WebSurg and are not a contribution of the authors. Therefore, the data availability may be subject to license changes from the content owners and WebSurg.\n- **Hallucinations and Data Quality**: As the authors mentioned, there may be hallucinations in the dataset, since it is automatically generated. The authors provide chatGPT and human evaluations, but that is not enough to infer the data quality.\n- **Model Availability**: It is not possible to reproduce the results since the model is not available yet, but enough details are provided to support the paper.\n\n### Questions\n\nThe paper is very well written and addresses its objectives. It also supports its claims and provides adequate experiments. Therefore, I am leaning toward accepting this paper, but I have some minor concerns regarding the legality of using WebSurg's surgical videos. I also have some questions:\n1. The authors mention that the model is limited by hallucinations, which is a serious concern for a surgical chatbot. Could you please provide more details, and types of hallucinations, and give some examples?\n2. Would it be possible to evaluate LLaVA-Surg on the SSG-VQA dataset? I am interested in knowing more about the breadth of your dataset and if it contains enough information for cross-dataset generalization.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nI am wondering about the WebSurg's policies on using their videos to train deep learning models, but I could not find any information about this in their terms of use.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents LLaVA-Surg, a multimodal conversational assistant designed for surgical videos, along with a new dataset comprising 102,000 question-answer pairs specifically tailored for training multimodal large language models (LLMs). The authors outline a thorough data generation process aimed at mitigating hallucinations, and they provide comprehensive comparisons with both general-purpose and surgical-focused datasets. In addition, the paper includes evaluations conducted by humans and LLMs, revealing consistent scoring across assessments.\n\n**Major Comments**  \n1. **Dataset Availability**: The surgical videos utilized are sourced from WebSurg and should not be considered a novel contribution by the authors. The stability of data availability might be impacted by licensing agreements with content owners.\n2. **Hallucinations and Data Quality**: The authors acknowledge the potential for hallucinations within the automatically generated dataset. While evaluations by ChatGPT and human reviewers are presented, these alone do not sufficiently ensure high data quality.\n3. **Model Availability**: The results cannot be reproduced as the model is currently unavailable, although the manuscript provides adequate details to support its claims.\n\n**Minor Comments**  \n- The manuscript is well-written and clear.\n- The study makes a notable contribution to the field of surgical chat assistants, presenting a more extensive collection of surgical QAs than previous works.\n- Concerns exist regarding the legality of utilizing WebSurg‚Äôs surgical videos for model training.\n\n**Summary Paragraph**  \nThe review of LLaVA-Surg highlights significant innovation in developing a conversational assistant for surgical contexts, coupled with a valuable dataset. However, concerns regarding the legal status of video usage and potential data quality issues due to hallucinations warrant attention. While the manuscript meets basic evaluation criteria, the lack of model reproducibility remains a notable limitation.\n\n**Decision Recommendation**  \nMinor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *LLaVA‚ÄëSurg*, a multimodal conversational assistant for surgical videos, along with a new dataset of approximately 102,000 question‚Äìanswer pairs designed for training multimodal large language models (LLMs). The work details the data generation pipeline, emphasizing safeguards against hallucinations, and presents comparative analyses with existing general‚Äëpurpose and surgical‚Äëspecific datasets. Overall, the paper is clearly written, well‚Äëorganized, and presents a coherent narrative that can be followed easily.\n\n**Major Comments**  \n1. **Dataset Availability and Licensing:** The dataset relies on surgical videos sourced from WebSurg, which are not the authors‚Äô own contribution. As data access depends on WebSurg‚Äôs licensing terms, availability and long‚Äëterm reproducibility could be affected by changes in external permissions. Clarification on the legality and sustainability of using these videos is warranted.  \n2. **Data Quality and Hallucinations:** Although the authors attempt to minimize hallucinations during data generation, the automatic nature of the process inherently introduces potential errors. The provided evaluations (ChatGPT and human review) are helpful but insufficient to fully assess dataset reliability for safety‚Äëcritical applications such as surgical chatbots.  \n3. **Reproducibility and Model Availability:** The model itself is not publicly released, which currently limits reproducibility of results. Nevertheless, the paper includes sufficient methodological details for conceptual understanding and partial replication of the described pipeline.  \n4. **Further Evaluation:** Extension of experiments to other surgical datasets‚Äîsuch as SSG‚ÄëVQA‚Äîcould strengthen claims about dataset comprehensiveness and cross‚Äëdataset generalization performance.\n\n**Minor Comments**  \n- The manuscript is written clearly and maintains consistent terminology.  \n- The authors could expand discussion on typical hallucination types with specific examples.  \n- Ensure that any mention of data sources aligns precisely with WebSurg‚Äôs official usage policies.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe submission is significant for advancing multimodal surgical AI dialogs and provides a valuable dataset with broader coverage than existing resources. Its innovation lies in adapting large multimodal language modeling to the surgical domain. Evaluation is reasonable but could be enhanced through independent quality assessments and external dataset testing. Reproducibility remains limited until both dataset and model access are formally resolved and clarified.  \n\n**Decision Recommendation**  \n**Recommendation: Minor Revision.** The work is strong and nearly publishable but requires clarification of data licensing, expanded discussion on hallucination types, and potentially additional evaluations to reinforce data validity and compliance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to‚ÄØTMI**  \n   The paper introduces **LLaVA‚ÄëSurg**, a multimodal vision‚Äìlanguage model for surgical video understanding and conversation, together with **Surg‚ÄëQA**, a 102‚ÄØk‚Äëpair surgical video question‚Äìanswer dataset generated by a structured two‚Äëstage pipeline.  Although the target use case is the surgical domain, the contribution is largely methodological: building and evaluating a multimodal AI system that couples LLMs with video encoders.  That aligns reasonably with *IEEE‚ÄØTMI*‚Äôs scope of imaging methodology, machine learning, and computational analysis rather than purely clinical application. ‚úÖ\n\n2. **Novelty & Contribution Level**  \n   The novelty lies in (i) a two‚Äëstage structured LLM pipeline to generate video‚ÄìQA pairs that aim to reduce hallucination, and (ii) adaptation of visual‚Äìlanguage architectures (CLIP‚ÄØ+‚ÄØLLaMA) to model spatiotemporal surgical video data.  The proposed dataset scale is unprecedented for surgery videos.  However, parts of the model architecture and evaluation pipeline are close to prior‚ÄØart (Video‚ÄëLLaVA, Video‚ÄëChatGPT), so the conceptual advance is moderate though useful.  The main novelty is practical‚Äîdata generation and domain translation‚Äîrather than a new imaging theory.\n\n3. **Technical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \n   ‚Äì‚ÄØImplementation details are explicitly documented: dataset sources (WebSurg,‚ÄØCholecT50), number of procedures, training setup, hardware, and hyperparameters.  \n   ‚Äì‚ÄØThe two‚Äëstage data pipeline is well motivated but only partially validated; quality control of automatically generated QA pairs appears anecdotal.  \n   ‚Äì‚ÄØQuantitative evaluation relies heavily on GPT‚Äëbased automatic scoring, supplemented by a small (n‚ÄØ=‚ÄØ60) clinician validation showing correlation. Statistical significance or inter‚Äërater reliability is not reported.  \n   ‚Äì‚ÄØReproducibility is strong: authors state release of code, weights, and data; therefore experiments could be replicated.\n\n4. **Clarity & Presentation**  \n   Manuscript is clear, with appropriate figures, tables, and consistent terminology.  Some minor grammatical and spacing issues (‚ÄúLLaV‚ÄØA‚ÄëSurg‚Äù artifacts) and typographical inconsistencies need correction, but overall readability is high.\n\n5. **Ethical‚ÄØand‚ÄØReproducibility‚ÄØCompliance**  \n   Data originate from publicly available, de‚Äëidentified surgical videos (WebSurg); no patient PII is included.  Authors commit to open‚Äësourcing under Creative‚ÄØCommons‚ÄØBY‚ÄëNC‚ÄØ4.0.  Ethical and licensing aspects appear properly considered.\n\n---\n\n**Phase‚ÄØ2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \n   The manuscript proposes **LLaVA‚ÄëSurg**, a surgical video conversational assistant built by fine‚Äëtuning a vision‚Äìlanguage model (CLIP‚ÄØ+‚ÄØLLaMA) on **Surg‚ÄëQA**, a new dataset of 102‚ÄØk automatically generated surgical video QA pairs.  The dataset is produced via a two‚Äëstage pipeline that extracts structured observations, reasons, and plans from transcribed surgical lectures using‚ÄØLLM prompting, aiming to reduce hallucination and cost.  The model is evaluated on zero‚Äëshot surgical video QA tasks, outperforming general‚Äëdomain multimodal baselines (Video‚ÄëLLaVA,‚ÄØVideo‚ÄëChatGPT) and correlating well with clinician scoring.\n\n2. **Strengths**  \n   * First large‚Äëscale multimodal conversational dataset for surgery videos.  \n   * Practical innovation in structured Q&A generation mitigating hallucination.  \n   * Demonstrated improvement over multimodal baselines and quantitative clinician validation.  \n   * Clear reproducibility plan with public release of data and code.\n\n3. **Weaknesses**  \n   * Architectural novelty is incremental; the framework closely follows prior general‚Äëdomain models.  \n   * Evaluation heavily depends on GPT‚Äëscoring without strong human‚Äëstudy validation.  \n   * Lack of rigorous uncertainty or error analysis; limited ablation on two‚Äëstage pipeline benefits.  \n   * Clinical or imaging physics insights limited‚Äîtargets more at AI engineering than imaging methodology per‚ÄØse.  \n   * Minor clarity and formatting issues.\n\n4. **Major‚ÄØComments**  \n   1. **Positioning for TMI readers.**‚ÄØClarify how the work advances medical‚Äëimaging methodology beyond dataset creation.  Discuss broader implications for quantitative surgical video analysis (e.g., phase recognition, tool tracking).  \n   2. **Validation robustness.**‚ÄØProvide a larger‚Äëscale human evaluation, or complementary objective metrics (e.g., BLEU/ROUGE, factual accuracy vs‚ÄØreference).  Report inter‚Äërater consistency between clinicians and‚ÄØGPT.  \n   3. **Ablation on pipeline.**‚ÄØQuantify how the two‚Äëstage generation reduces hallucination relative to end‚Äëto‚Äëend prompting across multiple samples, not just anecdotal examples.  \n   4. **Generalizability.**‚ÄØEvaluate transfer to unseen surgical categories or other datasets (e.g.,‚ÄØCholec80‚ÄëVQA) to demonstrate domain robustness.  \n   5. **Ethical/compliance detail.**‚ÄØAdd explicit statement of patient de‚Äëidentification and reuse permissions from WebSurg to conform to‚ÄØTMI reproducibility policies.\n\n5. **Minor‚ÄØComments**  \n   * Fix typographical spacing (‚ÄúLLaV‚ÄØA‚ÄëSurg‚ÄØ‚Üí‚ÄØLLaVA‚ÄëSurg‚Äù), unify variable notation.  \n   * Some figures (e.g., Fig‚ÄØ4) contain small text‚Äîenlarge for print quality.  \n   * Refer to ICLR in‚Äëreview footers should be removed for journal submission.  \n   * Include statistical uncertainties (mean‚ÄØ¬±‚ÄØSD) in Table‚ÄØ3.  \n   * Verify citation formatting consistency (author‚Äìyear vs numeric).  \n\n6. **Overall‚ÄØRecommendation**  \n   **Major‚ÄØRevision.**  \n   The work is promising and methodologically sound but requires strengthened experimental validation and clearer alignment to TMI‚Äôs methodological focus.  With improved quantitative and statistical analyses, the contribution could meet TMI standards.\n\n7. **Confidence‚ÄØLevel:**‚ÄØ4‚ÄØ/‚ÄØ5‚ÄØ‚Äì‚ÄØHigh confidence in technical assessment; moderate confidence in ultimate journal fit.\n\n---\n\n**Summary Judgment:**  \nA solid and timely paper bridging multimodal LLMs and surgical video understanding.  Methodological rigor is acceptable, novelty moderate, and reproducibility excellent.  Recommend *Major‚ÄØRevision* to add comparative and statistical depth and to better articulate methodological innovation for medical‚Äëimaging readership.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces LLaVA-Surg, a multimodal large language model designed for surgical video understanding and conversation. The authors create Surg-QA, a dataset of 102,000 surgical video-instruction pairs derived from 44,000 video clips across 2,201 surgical procedures (Page 1, lines 006-009). The core methodological contribution is a novel two-stage question-answer generation pipeline that first extracts structured information (observation, reason, plan, deduction) from surgical lecture transcripts using Llama-3-70B, then generates instruction-tuning data (Figure 2, Section 3). LLaVA-Surg builds upon Video-ChatGPT architecture, combining CLIP visual encoder with Llama language backbone (Page 2, lines 054-056). Evaluation on zero-shot surgical video question-answering shows LLaVA-Surg achieving 2.45/5.0 score compared to 1.32 for Video-LLaVA and 1.04 for Video-ChatGPT (Table 3). The authors validate their GPT-based evaluation framework against clinician assessments (Figure 5).\n\n## Weaknesses\n\n‚Ä¢ **Limited mathematical rigor in model formulation**\n  - Equation 1 provides only high-level notation without specifying video encoding dimensions or feature extraction details\n  - Equation 2 lacks mathematical precision in defining multi-turn conversation structure, particularly the conditional logic\n  - The temporal-fusion operation description (Page 7, lines 324-330) lacks formal mathematical definition of average-pooling operations and concatenation dimensions\n  - No formal loss function or training objective is provided despite claims of end-to-end instruction tuning\n\n‚Ä¢ **Insufficient evaluation methodology and baselines**\n  - GPT-3.5-turbo evaluation on only 4,359 test pairs (Page 7, lines 377-381) may not capture full model capabilities across surgical specialties\n  - Clinician validation limited to 60 samples from only two experts (Page 8, lines 419-427), insufficient for statistical significance\n  - No comparison with recent surgical VQA methods like SurgicalGPT (Seenivasan et al., 2023) which is cited but not benchmarked\n  - Accuracy@1 and accuracy@all metrics (Table 3) lack clear mathematical definition and clinical relevance justification\n\n‚Ä¢ **Dataset quality and bias concerns**\n  - Two-stage generation pipeline claims to reduce hallucination (Page 4, lines 189-195) but provides only anecdotal evidence in Figure 3\n  - Heavy reliance on WebSurg source (2,151 videos, Page 4, line 216) introduces potential institutional and procedural bias\n  - No inter-annotator agreement or quality control metrics for the 102K generated pairs\n  - Surgical knowledge pyramid (Figure 1) categorization lacks validation from surgical education experts\n\n‚Ä¢ **Technical architecture limitations**\n  - Architecture description (Page 6, lines 324-330) lacks novelty, essentially adapting existing Video-ChatGPT without surgical-specific modifications\n  - Training details show only 6 hours on 8 A100 GPUs (Page 7, lines 366-367) which may be insufficient for complex surgical reasoning\n  - No ablation studies on key components like temporal fusion or the structured information extraction approach\n  - Missing details on handling variable-length surgical procedures and multi-stage operations\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical formalization and technical rigor**\n  - Provide detailed mathematical formulation of the temporal-fusion operation including tensor dimensions and operations\n  - Define formal loss functions for instruction tuning and specify optimization objectives clearly\n  - Add rigorous mathematical definition of the multi-turn conversation structure beyond Equation 2\n  - Include formal analysis of the two-stage generation pipeline's effect on reducing hallucination probability\n\n‚Ä¢ **Strengthen evaluation framework and expand baselines**\n  - Conduct evaluation on larger test sets with stratification across surgical specialties represented in Figure 4c\n  - Recruit more clinical experts (minimum 5-10) and evaluate larger sample sizes (500+ cases) for statistical significance\n  - Include comprehensive comparison with SurgicalGPT and other recent surgical VQA methods with standardized metrics\n  - Provide clear mathematical definitions and clinical relevance justification for accuracy@1 and accuracy@all metrics\n\n‚Ä¢ **Address dataset bias and quality validation**\n  - Implement systematic quality control with inter-annotator agreement metrics for generated question-answer pairs\n  - Include datasets from multiple sources beyond WebSurg to reduce institutional bias\n  - Validate the surgical knowledge pyramid framework with surgical education specialists\n  - Provide quantitative analysis of hallucination reduction beyond the single example in Figure 3\n\n‚Ä¢ **Improve technical contributions and experimental design**\n  - Introduce surgical-specific architectural modifications rather than directly adapting Video-ChatGPT\n  - Conduct comprehensive ablation studies on temporal fusion, structured extraction, and multi-stage training\n  - Extend training duration and provide learning curves to demonstrate convergence\n  - Add detailed analysis of performance across different surgical procedure types and complexity levels",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces LLAVA-Surg, a multimodal conversational assistant designed to understand and respond to open-ended questions about surgical videos. The authors address the lack of specialized datasets for training multimodal large language models (LLMs) in the surgical domain by creating Surg-QA, a large-scale dataset comprising over 102,000 surgical video-instruction pairs. The dataset is generated using a novel two-stage question-answer generation pipeline that leverages LLMs to extract and generate high-quality instructional content from surgical lecture videos. LLAVA-Surg is then trained on this dataset and evaluated on zero-shot surgical video question-answering tasks, demonstrating superior performance compared to existing general-domain models.\n\n###\n\n## Major Comments\n1. Novelty and Positioning. The creation of Surg-QA and the development of LLAVA-Surg represent significant advancements in the field of multimodal surgical video understanding. However, the manuscript could benefit from a clearer distinction between LLAVA-Surg and existing multimodal models that handle video data. The authors should more explicitly discuss how LLAVA-Surg's approach to integrating visual and textual data differs from recent advancements in multimodal LLMs.\n   \n2. Evaluation Design. While the authors present comprehensive evaluations on Surg-QA, the scope of the evaluation could be expanded to include a broader range of surgical procedures and video modalities. Including additional datasets or synthetic video scenarios could provide a more robust validation of LLAVA-Surg's generalizability and effectiveness across different surgical contexts.\n   \n3. Comparisons. The comparisons with existing models are thorough, but the manuscript could benefit from a more detailed discussion of the limitations and potential biases inherent in the Surg-QA dataset. For instance, the reliance on surgical lecture videos might introduce biases that affect the model's performance on real surgical procedures. Addressing these limitations would strengthen the manuscript's argument for the significance of LLAVA-Surg.\n\n4. Reproducibility. The authors state that the code, model, and dataset will be released, which is commendable. However, the manuscript could provide more details on the specific conditions under which the data generation and model training were conducted. Clearer descriptions of the preprocessing steps, training protocols, and hyperparameters would enhance the reproducibility of the results.\n\n###\n\n## Minor Comments\n1. Figures and Tables: Some figures, particularly Figures 4 and 5, are cluttered and could benefit from a more organized layout. For example, the treemap in Figure 4c could be simplified to improve readability.\n   \n2. Notation and Terminology: The manuscript occasionally uses inconsistent terminology, such as \"LLaV A-Surg\" and \"LLAVA-Surg\". Standardizing this notation would improve clarity.\n   \n3. Acronyms: Acronyms like \"VQA\" are used without definition in the text. Providing a brief explanation or defining these terms upon first use would be beneficial for readers unfamiliar with the terminology.\n   \n4. Typographical Issues: Minor typographical errors, such as \"video-instruction\" instead of \"video instruction,\" should be corrected for consistency.\n\n###\n\n## Summary Paragraph\n(Focusing on 4 TMI Editorial Criteria)\nThe manuscript addresses a significant gap in the availability of specialized datasets and models for surgical video understanding. The introduction of Surg-QA and the development of LLAVA-Surg represent innovative contributions to the field, offering promising advancements in multimodal conversational capabilities for surgical videos. However, the evaluation is somewhat limited to a single dataset, which affects the generalizability of the findings. The manuscript's reproducibility is strengthened by the planned release of code and data, but additional methodological details are needed. Overall, while the work has substantial merit, the current evidence does not fully meet the standards of rigor expected for publication in TMI, particularly in terms of broader validation and detailed reproducibility information.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis, strengthen the validation across different surgical datasets and procedures, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Brian R Quaranto",
      "Garrett Skinner",
      "Gene Yang",
      "Jiajie Li",
      "Jinjun Xiong",
      "Peter C W Kim",
      "Steven D Schwaitzberg"
    ],
    "url": "pdfs/iclr.cc-2025-conference_04d73daf100581d96e3a971dd358d0aad68ebdd1.pdf",
    "remote_url": "https://openreview.net/pdf/04d73daf100581d96e3a971dd358d0aad68ebdd1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Hallucination Benchamrk",
      "Hallucination Evaluation Method",
      "Medical Large Vision Language Model"
    ],
    "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations‚Äîa significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MedihallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MedihallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work have been released.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a hallucination detection and evaluation benchmark in the medical multimodal domain. Recognizing that large vision-language models often inherit hallucination tendencies from foundational large-language models, the authors propose a framework for more accurate and detailed hallucination assessment in high-stakes medical applications. \nThe paper makes three primary contributions: (i) Med-HallMark, the benchmark dedicated to medical hallucination detection (ii) MediHallDetector, a hallucination detection model tailored for the medical domain; and (iii) MediHall Score, an evaluative metric designed for hallucinations of different severity and types. \nThrough extensive experiments, the paper claims that MediHall Score provides more nuanced insights than traditional metrics and that MediHallDetector enhances hallucination detection performance in medical LVLMs.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well-motivated in that it addresses a critical and underexplored area in medical LVLMs: detecting and evaluating hallucinations, which is crucial for ensuring safety in clinical applications. The work frames hallucination detection specifically for the medical domain. The MediHall Score introduces a nuanced metric that prioritizes clinical impact by differentiating hallucinations based on their severity and type. The ablation study of different SFT methods for training the detector also adds robustness to the study.\n\n### Weaknesses\n\nThe paper raises several questions about primarily the soundness of evaluation and the reliability of the proposed metric (detailed in the **Questions** section). For example, the paper limits its IRG evaluation to only one domain-specific model, XrayGPT, which is problematic given that IRG tasks are where hallucination metrics are particularly valuable. Additionally, results for XrayGPT in IRG appear inconsistent with those of Minigpt-4, which was shown to underperform in traditional metrics but achieved the highest MediHall Score among all evaluated models, calling into question the reliability of the MediHall Score. The paper could have strengthened its findings by evaluating other state-of-the-art LVLMs, such as open-source, domain-specific models like BiomedGPT or Med-Flamingo, or proprietary MLLMs like GPT-4 and Gemini Pro, to validate the metric‚Äôs robustness. Finally, the experimental design lacks a thorough investigation into the role of image inputs in tuning MediHallDetector. The study could benefit from ablation studies that assess performance with and without image inputs, especially given that the image encoder and connector taken from LLaVA are frozen during training and could have introduced extra errors due to its lack of visual understanding ability. These design choices suggest that the current benchmark version may be limited in scope and could benefit from further refinement and validation.\n\n### Questions\n\n**Construction of the benchmark:**\n1. Could you elaborate on the motivation behind designing confidence-weakening and counterfactual questions beyond the \"conventional\" questions in established test sets like SLAKE and RAD-VQA? Regarding counterfactual question generation, considering the GPT model is provided with limited information (text-only, single question with its ground-truth answer), how is the quality of GPT-generated counterfactual questions and answers ensured?\n\n2. Robust evaluation of hallucinations in open-ended IRG tasks is inherently challenging, particularly due to synonymous terms (in frameworks like ICD-10, multiple expressions might refer to the same condition). Since the ground truth for open-ended IRG scenarios in this paper derives from medical reports in MIMIC and OpenI test sets, how does the evaluation framework account for potential synonyms in model responses given the limited set of 1800 images and their reports seen by MediHallDetector?\n\n3. In using LLaVA-Med to generate GT responses for Med-VQA, why not utilize the GT already provided in the dataset, given that the questions originate from an established source? For the IRG scenario, are model-generated responses also exclusively from LLaVA-Med? If so, might this narrow distribution affect the generalizability of MediHallDetector, as it is fine-tuned specifically on LLaVA-Med responses?\n\n**Experimental Design:**\n1. How much does the image come into play in tuning the MedihallDetector model? It could be necessary to ablate w./w.o. image input, especially when you freeze the image encoder and connector during training.\n\n**Results**\n1. For medical VQA tasks (Table 1), why introduce a MediHall Score when accuracy already exists as a metric for Med-VQA? Notably, XrayGPT, with an accuracy of only 0.02, has a MediHall Score of 0.36. Could this indicate potential inflation in the MediHall Score?\n\n2. For medical IRG tasks (Table 2), the results for XrayGPT as measured by MediHallDetector seem counter-intuitive compared to Minigpt-4. In the original XrayGPT study [1], Minigpt-4 was a baseline against which improvements were demonstrated using ROUGE (Table 1 in [1]). Here, XrayGPT does outperform Minigpt-4 in all conventional metrics, but in the MediHall Score, with Minigpt-4 obtaining the highest MediHall Score among evaluated models. Could this raise concerns regarding the reliability of the MediHall Score metric?\n\n3. Could you clarify why models like BLIP2, LLaVA-Med, and RadFM do not receive MediHall Scores for medical IRG tasks? Open-ended IRG tasks are particularly significant since accuracy is already used as a metric for VQA. Presently, only XrayGPT is evaluated in IRG tasks using the proposed model and metric; incorporating additional state-of-the-art models would enhance the proposed metric's robustness. For instance, open-source domain-specific models such as BiomedGPT [2] and Med-Flamingo [3], which perform well on established tasks, or widely used proprietary MLLMs like GPT-4, Claude 3.5, and Gemini Pro could be valuable benchmarks.\n\n**Typo**\n1. In Figure 2(a), ‚Äúpairs‚Äù is misspelled.\n2. Could you clarify whether XrayGPT is used in constructing the benchmark? It appears in Figure 1(b) but is not referenced in section 3.4.\n\n[1] https://aclanthology.org/2024.bionlp-1.35.pdf\n[2] https://arxiv.org/abs/2305.17100\n[3] https://arxiv.org/abs/2307.15189\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nn/a",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a benchmark for hallucination detection and evaluation within the medical multimodal domain. Recognizing the propensity for hallucination in large vision-language models (LVLMs) derived from foundational large-language models, the authors propose a framework aimed at improving the accuracy of hallucination assessments in critical medical applications. The contributions include Med-HallMark, a benchmark tailored for medical hallucination detection; MediHallDetector, a model for detecting hallucinations in medical contexts; and the MediHall Score, a metric designed to evaluate hallucinations of varying severity and types. Through comprehensive experiments, the authors claim that their proposed metrics and models yield improved insights compared to existing methodologies.\n\n### Major Comments\n1. The evaluation of the proposed methodology primarily relies on a single domain-specific model, XrayGPT. This limitation raises concerns regarding the robustness of the evaluation framework.\n2. Results indicate discrepancies in performance where XrayGPT's results diverge from those of Minigpt-4, suggesting potential issues with the reliability of the MediHall Score.\n3. The study lacks evaluation of additional state-of-the-art LVLMs, which could provide a more comprehensive validation of the proposed metrics.\n4. The experimental design does not sufficiently investigate the impact of image inputs on the performance of MediHallDetector, which should be addressed through further ablation studies.\n\n### Minor Comments\n- Figure 2(a) contains a typographical error; ‚Äúpairs‚Äù is misspelled.\n- Clarification is needed on XrayGPT's role in constructing the benchmark, as it appears in Figure 1(b) but is not discussed in section 3.4.\n\n### Summary Paragraph\nThe manuscript aligns with TMI editorial criteria by highlighting a significant issue‚Äîhallucinations in medical LVLMs‚Äîmaking its contribution noteworthy for future clinical applications (significance). The innovation is evident in the introduction of Med-HallMark and the MediHall Score, as they provide tailored evaluation metrics for the medical domain. However, the limitations in experimental scope, particularly regarding the evaluation across various models and the role of image inputs, raise questions about reproducibility and overall robustness.\n\n### Decision Recommendation\nMajor revision is recommended to address the methodological concerns raised, specifically regarding the evaluation framework's scope and robustness.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a hallucination detection and evaluation benchmark tailored to the medical multimodal domain, addressing the challenge that large vision-language models (LVLMs) often inherit hallucination tendencies from general-purpose language models. The work‚Äôs main contributions include: (i) *Med-HallMark*, a benchmark for detecting medical hallucinations; (ii) *MediHallDetector*, a model for domain-specific hallucination detection; and (iii) *MediHall Score*, a novel evaluation metric differentiating hallucination severity and type. Overall, the study is well-motivated and emphasizes clinical safety, though issues in the evaluation design and reliability of results limit its soundness.\n\n**Major Comments**  \n1. **Evaluation Scope and Reliability:** The study evaluates primarily on XrayGPT, limiting the generalizability of MediHall Score and MediHallDetector. Broader benchmarking with additional state-of-the-art or domain-specific LVLMs (e.g., BiomedGPT, Med-Flamingo, GPT‚Äë4, Gemini Pro) is necessary to validate robustness.  \n2. **Metric Consistency:** Discrepancies between conventional metrics and MediHall Score raise questions about the latter‚Äôs reliability. For instance, XrayGPT‚Äôs high MediHall Score despite low accuracy suggests possible inflation or inconsistency in metric calibration.  \n3. **Benchmark and Data Construction:** The motivation for generating ‚Äúconfidence-weakening‚Äù and ‚Äúcounterfactual‚Äù questions beyond those in established datasets (e.g., SLAKE, RAD-VQA) should be clarified, as should the quality control of GPT‚Äëgenerated counterfactuals.  \n4. **Handling Synonyms in IRG Evaluation:** Since open-ended image‚Äìreport generation (IRG) tasks involve synonymous or equivalent medical terms, the evaluation framework‚Äôs strategy for handling lexical variation needs to be specified.  \n5. **Experimental Design and Ablation:** The contribution of image input during MediHallDetector training is unclear. Given that the image encoder and connector are frozen, an ablation study with and without image input would clarify their role and potential limitations.  \n6. **Use of LLaVA-Med Outputs:** The rationale for using LLaVA-Med to generate ground truth responses instead of dataset-provided ones requires explanation, as dependence on a single generator may reduce generalizability.  \n7. **Incomplete Reporting:** Some models (BLIP2, LLaVA-Med, RadFM) lack MediHall Scores for IRG tasks, constraining comparative analysis.\n\n**Minor Comments**  \n- Clarify whether XrayGPT contributes to benchmark construction, as implied by Figure 1(b).  \n- Correct the typo in Figure‚ÄØ2(a) (‚Äúpairs‚Äù misspelled).  \n- Some figures and results tables could benefit from clearer cross-referencing within the text.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe topic is significant and understudied, addressing hallucination detection in medical LVLMs‚Äîa high-impact area for safety and reliability. The benchmark and metric are conceptually innovative, introducing domain-specific constructs. However, evaluation suffers from limited scope, inconsistent metric behavior, and insufficient validation across diverse models. The reproducibility of results would improve with more transparent dataset construction and ablation analyses. Overall, the paper‚Äôs novelty is balanced by methodological weaknesses that currently hinder confidence in its findings.\n\n**Decision Recommendation**  \n**Major Revision.** The work presents an important and timely contribution but requires broader evaluation, clearer methodological justification, and additional validation to establish soundness and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes an LLM/LVLM evaluation benchmark, especially for the hallucination in the medical domain tasks. It also proposes a metric to evaluate the severity of hallucination in LVLMs, and a fine-tuned evaluator model is released to perform the universal evaluation for given set information based on (input image I, the original prompt P, the LVLM answer A and the ground truth GT).\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper proposes a novel benchmark for evaluation the LLMs specially in the Medical domain, and provides a comprehensive framework from dataset, metric, and the fine-tuned evaluation model, which is a completed work through the evaluation pipeline.‚ÄØ \n‚óè\tThe paper is writing in a smooth way, easy to follow and simple to understand.‚ÄØ \n‚óè\tThe work is of good significance, should be a meaningful angle to advance the LLM/LVLMs applications in real-world healthcare domain.\n\n### Weaknesses\n\n‚óè\tThe paper did not emphasize the special challenges in Healthcare domain, after the reading, except for the first part of the dataset that involves the `Medical` multi-modality data such as CT, the evaluation process is on the common hallucination challenges from any Vision-language model, it raises the concerns that whether the proposed model stands out from common LVM hallucination evaluation, is other existing work able to solve the same question by simply adapting to the medical data? What is the advantage of the proposed fine-tuned model over other baseline methods?‚ÄØ \n‚óè\tThe hierarchical definition of the MediHALL score seems a bit intuitive, which is a simple evenly scaled value on 5 categories.‚ÄØ \n‚óè\tThe paper lacks discussion on the relevant work that conduct similar hallucination detection in LLMs at Healthcare domain.‚ÄØ \n‚óè\tIt is suggested that the paper could make further clarification on certain questions as in the Question section, if most critical concerns can be addressed, it is plausible to raise the score for the paper‚Äôs quality evaluation.\n\n### Questions\n\n1. The authors explicitly defined 5 types of hallucination levels, among which: Catastrophic Hallucination, Critical Hallucinations, Minor Hallucination are severity levels, but the attribute, prompt-induced ones is indeed the `cause` of the hallucinations. There raise doubts of the rationale of such empirical classifications including attribute, prompt-induced to severity levels. \n \n2. As in line 215, the conventional Q_{conv} questions are generated by the GPT3.5, how is the quality guaranteed? How is the initial conv question obtained?‚ÄØ \n \n3. LLaVA-Med was used to infer answers, what is the size of the whole dataset? Even though the authors claim that the data has been examined, how is the quality guaranteed?‚ÄØ‚ÄØ \n \n4. Also, for the dataset part that is relevant to the IRG scenario, the authors use a sampling method to draw 1,800 images and their corresponding medical reports from existing datasets: MIMIC-test and OpenI datasets. This weakens the contribution towards the data as proposed in line 065.‚ÄØ \n \n5. In line 255, ‚ÄúThis fine-grained metric xxx‚Äù, starts abruptly, since the previous paragraph discusses the existing metrics drawbacks. Then it directly refers to ‚ÄúThis metric‚Äù, which is not transitioning naturally.‚ÄØ \n \n6. The MediHALL score is relatively simple, which is built upon the pre-categorized types of hallucination levels and assigns different values in a hierarchical way.‚ÄØ Based on this, the latter human-annotation and fine-tuning are conducted, are they able to maintain the objectiveness?‚ÄØ \n \n7. In line 318, which directs the training data details to Figure, it explicitly shows the categories, and the types of training data covered for MEDIHALLDETECTOR, but what about the amount of the data? And how is the instruction pair data derived?‚ÄØ \n \n8. While the paper is positioned as the first benchmark for medical LM hallucination evaluation, there is relevant work worth referencing:‚ÄØ \n\n[1] MEDHALU: Hallucinations in Responses to Healthcare Queries by Large Language Models \n[2] MEDIC: TOWARDS A COMPREHENSIVE FRAMEWORK FOR EVALUATING LLMS IN CLINICAL APPLICATIONS \n[3] Faithfulness Hallucination Detection in Healthcare AI\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph:\nThe manuscript presents a framework for evaluating Large Language Models (LLMs) and Large Vision-Language Models (LVLMs), specifically addressing hallucination challenges within the medical domain. It introduces a novel benchmark along with a metric to assess the severity of hallucinations, complemented by a fine-tuned evaluator model designed for universal evaluation based on a combination of input images, prompts, LVLM outputs, and ground truths. The paper is generally well-structured, making it accessible and understandable for readers.\n\n### Major Comments:\n1. The manuscript does not sufficiently emphasize the unique challenges associated with the healthcare domain. Most of the evaluation appears to address typical hallucination challenges present in vision-language models, raising questions about the distinctiveness and advantages of the proposed evaluation compared to existing methods.\n2. The hierarchical definition of the MediHALL score may lack depth, appearing rather intuitive and based on simple scaling across five categories.\n3. A discussion surrounding existing work in hallucination detection relevant to LLMs in the healthcare domain is noticeably absent.\n4. Further clarification is needed on several aspects raised in the \"Questions\" section; addressing critical concerns could improve the manuscript's overall evaluation score.\n\n### Minor Comments:\n- The classification of hallucination levels raises doubts about the rationale behind including 'prompt-induced' as a cause of hallucinations.\n- The quality control of questions generated by GPT-3.5 should be elaborated upon, specifically regarding their sourcing.\n- Information regarding the size and quality assurance of the dataset utilized with LLaVA-Med is required.\n- The sampling method of drawing from MIMIC-test and OpenI datasets appears to diminish the proposed contribution towards the dataset.\n- A smoother transition is needed in the discussion of the fine-grained metric, as it jumps from metrics' drawbacks to the proposed solution abruptly.\n- The objectiveness of the human-annotation and fine-tuning process following the MediHALL score framework should be questioned.\n- Details regarding the amount of training data for MEDIHALLDETECTOR are lacking, as is a description of how the instruction pair data was derived.\n- Relevant references to prior work should be included to substantiate claims in the field.\n\n### Summary Paragraph:\nIn summary, the manuscript addresses a significant issue in evaluating hallucinations in LLMs, enhancing innovation in the medical field. However, its evaluation design lacks clear distinctiveness from existing frameworks, which affects its reproducibility. While it provides useful contributions in terms of metrics and a novel evaluation model, more rigorous discourse on existing literature and unique challenges in healthcare is necessary to fulfill the evaluation criteria thoroughly.\n\n### Decision Recommendation:\nMinor revision is recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a benchmark and evaluation framework for assessing hallucinations in large language and vision-language models (LLMs/LVLMs) within the medical domain. It further proposes a new metric, MediHALL, intended to quantify hallucination severity across multiple levels, and presents a fine-tuned evaluator model for universal scoring based on multimodal inputs (image, prompt, answer, and ground truth). The paper is clearly written, conceptually complete, and potentially significant for advancing reliability assessment in medical AI applications.\n\n---\n\n**Major Comments**  \n1. **Domain-Specificity and Novelty:** The study does not sufficiently highlight the unique challenges of hallucination detection specific to the healthcare domain. Much of the evaluation appears transferable from general LVLM hallucination work, raising questions about the distinct advantage of this medical-focused benchmark and fine-tuned model.  \n2. **Metric Design:** The hierarchical definition of the MediHALL score appears somewhat ad hoc, based on evenly scaled categories rather than a justified empirical framework. This simplification may limit interpretability and validity.  \n3. **Related Work:** The discussion overlooks existing work in medical hallucination benchmarking (e.g., MEDHALU, MEDIC, and Faithfulness Hallucination Detection in Healthcare AI). These should be explicitly compared to clarify novelty.  \n4. **Classification Rationale:** The inclusion of ‚Äúattribute‚Äù and ‚Äúprompt-induced‚Äù hallucinations as severity levels seems conceptually inconsistent, as they represent causes rather than degrees of severity.  \n5. **Dataset Quality and Scope:** More details are needed on dataset size, sampling rationale, and quality control, especially for MIMIC-test and OpenI subsets. The reliance on 1,800 sampled images may reduce the dataset‚Äôs originality.  \n6. **Evaluation and Clarity Issues:** Transitions between sections (e.g., line 255) are abrupt. The explanation of training data composition and instruction pairs for MEDIHALLDETECTOR (line 318) lacks quantitative clarity.  \n7. **Quality Assurance:** The generation of conventional questions via GPT‚Äë3.5 (line 215) requires justification regarding quality assurance and reproducibility of initial prompts.\n\n---\n\n**Minor Comments**  \n- The paper is generally clear and well written, though minor stylistic improvements could enhance flow.  \n- Ensure objective phrasing when describing annotation and fine-tuning procedures; clarify how human evaluation consistency is maintained.  \n- Minor inconsistencies in figure referencing and paragraph transitions should be addressed.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work offers a timely attempt to formalize hallucination evaluation in medical LVLMs, an area of high significance. Its strength lies in integrating dataset, metric, and evaluator components into one framework. However, innovation is limited by conceptual overlap with general hallucination benchmarks, underdeveloped metric justification, and insufficient discussion of prior art. Evaluation design and dataset description need greater transparency to enhance reproducibility. With stronger empirical grounding, clearer distinctions from non-medical benchmarks, and inclusion of comparative baselines, the contribution could become substantially more robust.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a new benchmark, Med-HallMark, and proposes evaluation tools specifically tailored to medical contexts. Med-HallMark provides a multi-dimensional framework to identify and assess hallucinations generated by LVLMs. The benchmark includes multi-tasking hallucination support, diverse data for hallucination detection, and a hierarchical categorization of hallucinations based on clinical impact. Additionally, the paper presents MediHall Score, a metric designed to evaluate hallucinations in medical text output, and MediHallDetector, a specialized LVLM aimed at improved hallucination detection through multi-task training.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The introduction of Med-HallMark and MediHall Score fills a significant gap by addressing hallucination detection in medical LVLMs.\n- The proposed categorization of hallucinations is innovative and highly relevant, allowing for analysis of potential model impacts on medical decision-making.\n- The authors conduct extensive experiments comparing popular LVLM models on Med-HallMark.\n\n### Weaknesses\n\n- Is this very different from the accuracy? After reading the paper, I just feel that this paper just makes a more fine-grained classification of errors, and the classification standard needs to be discussed and re-designed. For example, in the report generation task, the model outputs a nonsense sentence for the chest X-ray, \"This is a chest X-ray of a person\", which is obviously correct, but it is not what we want. How should this be judged?\nSo this makes me wonder whether such a benchmark is necessary.\n- Some recent related work [1,2,3,4,5,6] is missing. \n- The overall data scale is relatively small, and the medical image modalities involved are limited to radiology.\n- How to ensure the accuracy of annotation? \n- Since doctors are hired to do the annotation, have the possible ethical risks been resolved? For example, IRB approval, etc.\n\n\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Yan Q, He X, Wang X E. Med-HVL: Automatic Medical Domain Hallucination Evaluation for Large Vision-Language Models[C]//AAAI 2024 Spring Symposium on Clinical Foundation Models. 2024.\n\n[3] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[4] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n[5] Nan Y, Zhou H, Xing X, et al. Beyond the Hype: A dispassionate look at vision-language models in medical scenario[J]. arXiv preprint arXiv:2408.08704, 2024.\n\n[6] Yan Q, He X, Yue X, et al. Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA[J]. arXiv preprint arXiv:2405.20421, 2024.\n\n### Questions\n\n- The format of the reference is weird. Please check it.\n- The figure 2 (d) is vague.\n\n### Flag For Ethics Review\n\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nSince doctors are hired to do the annotation, have the possible ethical risks been resolved? For example, IRB approval, etc.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents Med-HallMark, a new benchmark aimed at improving hallucination detection in medical Large Vision Language Models (LVLMs). It introduces evaluation tools designed specifically for medical contexts, offering a multi-dimensional framework for identifying and assessing hallucinations. Notably, the paper details the MediHall Score‚Äîa metric for evaluating hallucinations in medical text‚Äîand MediHallDetector, a specialized LVLM that enhances hallucination detection via multi-task training.\n\n**Major Comments:**  \n1. The distinction between Med-HallMark and accuracy is unclear; it appears to refine classification of errors without sufficient discussion on standardization. For instance, in the report generation task, the model may produce valid yet irrelevant sentences, raising questions about assessment criteria.\n2. Several relevant recent works are omitted from the literature review, including references [1] to [6].\n3. The dataset appears limited in size and primarily focuses on radiological images, potentially restricting generalizability.\n4. Concerns regarding the accuracy of annotations need addressing, particularly in how they are verified.\n5. The ethical implications of using medical professionals for annotation should be clarified, specifically regarding Institutional Review Board (IRB) approval and associated risks.\n\n**Minor Comments:**  \n- The formatting of the references needs correction for consistency.  \n- Figure 2(d) lacks clarity and could benefit from improved exposition.\n\n**Summary Paragraph:**  \nThe manuscript's strengths lie in its innovative approach to hallucination detection through Med-HallMark and MediHall Score, addressing a critical gap within medical LVLM evaluation frameworks. However, potential weaknesses include the need for a clearer distinction from existing accuracy measures, the omission of recent literature, and concerns about the limited dataset and annotation practices. In terms of significance, the work presents meaningful contributions; on innovation, it introduces relevant categorization; however, the evaluation and reproducibility aspects require further enhancement to ensure robustness and ethical compliance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Med-HallMark*, a new benchmark designed to evaluate hallucinations produced by large vision‚Äìlanguage models (LVLMs) in medical contexts. The work also presents two associated components: *MediHall Score*, a metric quantifying hallucination in text outputs, and *MediHallDetector*, an LVLM trained via multi-task learning to enhance hallucination detection. The benchmark aims to provide a multi-dimensional and clinically oriented framework for hallucination identification and classification. Overall, the manuscript is clear and well-organized, addressing a timely problem in medical AI evaluation.\n\n**Major Comments**  \n1. **Definition and necessity of the benchmark:** While the paper distinguishes different types of hallucination, the conceptual difference between hallucination detection and generic accuracy/error analysis is not fully convincing. The current categorization may largely resemble fine-grained error classification, and the criteria for classifying hallucinations could be refined.  \n2. **Benchmark validity:** The practical necessity of introducing a new benchmark is questioned, as the example provided (‚ÄúThis is a chest X-ray of a person‚Äù) illustrates challenges in definition and assessment. This raises doubts about the benchmark‚Äôs distinct contribution.  \n3. **Related work coverage:** Several recent and highly relevant studies ([1]‚Äì[6]) are missing and should be discussed to position the contribution within current literature.  \n4. **Dataset limitations:** The dataset size is relatively small and restricted to radiology images, which may limit generalizability.  \n5. **Annotation reliability and ethics:** The review queries how annotation accuracy is ensured and whether ethical clearance (e.g., IRB approval) has been obtained, given that physicians were involved in labeling.\n\n**Minor Comments**  \n- The references should follow a consistent citation format.  \n- Figure‚ÄØ2(d) lacks clarity and should be improved for readability.\n\n**Summary Paragraph (TMI Criteria)**  \nThe manuscript addresses a significant issue concerning the trustworthiness of medical LVLMs (significance), and its hierarchical categorization of hallucination types represents an innovative element. However, questions remain about the conceptual rigor and necessity of the benchmark (innovation), the sufficiency of experimental validation given limited data scope (evaluation), and reproducibility of results due to annotation details and ethics processes not being fully described (reproducibility).\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1: Analytical and Scope‚ÄØAudit**\n\n**1.‚ÄØScope &‚ÄØRelevance to‚ÄØTMI** ‚Äì‚ÄØ  \nThe manuscript introduces *Med‚ÄëHallMark*, a benchmark for identifying and quantifying hallucinations in large vision‚Äëlanguage models (LVLMs) applied to medicine, proposes a new metric (*MediHall‚ÄØScore*), and develops a dedicated hallucination‚Äëdetection model (*MediHallDetector*).  The work concerns multimodal machine‚Äëlearning methodology for medical image interpretation rather than a specific clinical application.  It falls within TMI‚Äôs scope of computational‚Äêimaging methodology, particularly assessment and trustworthiness of machine‚Äëlearning systems.‚ÄØ‚úÖ‚ÄØProceed with full review.\n\n**2.‚ÄØNovelty &‚ÄØContribution** ‚Äì‚ÄØ  \nThe authors combine three original components:‚ÄØ(1)‚ÄØa curated benchmark with hierarchical hallucination labels specific to clinical severity,‚ÄØ(2)‚ÄØa fine‚Äëgrained quantitative metric, and‚ÄØ(3)‚ÄØa detection network trained to classify hallucination levels.  While benchmarks and evaluation metrics have appeared in general LVLM literature, adaptation to the medical domain with clinically meaningful categories appears novel.  The contribution is methodological yet primarily evaluative rather than a new imaging‚Äëphysics or reconstruction method.  Novelty is moderate.\n\n**3.‚ÄØTechnical &‚ÄØExperimental‚ÄØRigor** ‚Äì‚ÄØ  \nThe paper describes dataset sources, annotation workflows, and multi‚Äërater validation.  The methodology for question generation, hallucination taxonomy, and score calculation is clearly detailed.  Experiments cover multiple LVLM baselines with quantitative tables.  Statistical uncertainty and inter‚Äëannotator agreement could be more fully reported; claims of superiority of MediHallDetector would benefit from statistical significance testing and clearer definitions of accuracy/recall in Table‚ÄØ5.  Reproducibility appears feasible but relies on released resources.\n\n**4.‚ÄØClarity &‚ÄØPresentation** ‚Äì‚ÄØ  \nOverall well organized and grammatically sound, though the text is lengthy with overlapping explanations between main and supplementary sections.  Figures are informative but crowded.  Some acronyms (e.g., ‚ÄúIRG,‚Äù ‚ÄúVQA‚Äù) should be defined at first mention in the main text.  English usage and formatting are acceptable for IEEE‚ÄØTMI style after copy‚Äëediting.\n\n**5.‚ÄØEthical‚ÄØ&‚ÄØReproducibility‚ÄØCompliance** ‚Äì‚ÄØ  \nAll datasets originate from CC‚Äëlicensed public sources (SLAKE,‚ÄØVQA‚ÄëRAD,‚ÄØMIMIC‚ÄëCXR,‚ÄØOpenI); identifiers were removed.  No new patient data were collected, complying with ethical standards.  The authors state that all resources are released and open source.‚ÄØ‚úÖ‚ÄØCompliant.\n\n---\n\n**Phase‚ÄØ2: IEEE‚ÄØTMI Review‚ÄØReport**\n\n**1.‚ÄØSummary**  \nThe paper addresses the absence of a formal evaluation framework for hallucinations in medical LVLMs.  It introduces the *Med‚ÄëHallMark* benchmark covering medical‚ÄëVQA and imaging‚Äëreport generation tasks with multilayer clinical‚Äëimpact labels; defines the *MediHall‚ÄØScore* metric for granular hallucination scoring; and presents *MediHallDetector*, a supervised fine‚Äëtuned LVLM able to categorize hallucination types.  Baseline results for multiple existing LVLMs and ablation studies demonstrate feasibility and illustrate weaknesses of conventional NLP metrics in reflecting medical factuality.\n\n**2.‚ÄØStrengths**\n- Fills a clear gap: domain‚Äëspecific hallucination detection/benchmarking for medical LVLMs.  \n- Defines a clinically interpretable, hierarchical annotation scheme linking hallucination severity to diagnostic impact.  \n- Provides open resources likely to foster reproducibility and community comparison.  \n- Demonstrates comprehensive experimental coverage across model families.  \n\n**3.‚ÄØWeaknesses**\n- Novelty lies mainly in dataset design and metric engineering; limited algorithmic innovation beyond an SFT variant of existing LVLMs.  \n- Lacks quantitative statistical analysis (confidence intervals or significance testing).  \n- No external expert validation of the hierarchical severity mapping or evidence that MediHall‚ÄØScore correlates with clinical harm.  \n- Presentation is verbose; several results tables could be condensed.  \n\n**4.‚ÄØMajor‚ÄØComments**\n1.‚ÄØ**Validation of Hierarchical Labels:**‚ÄØProvide quantitative inter‚Äëannotator agreement (Œ∫‚ÄØstatistics) to demonstrate reliability of the five‚Äëlevel taxonomy.  \n2.‚ÄØ**Statistical‚ÄØSignificance:**‚ÄØReport variance and perform significance testing for improvements of MediHallDetector over baselines.  \n3.‚ÄØ**Metric‚ÄØJustification:**‚ÄØExplain how numeric weights (0‚Äì1) in the MediHall‚ÄØScore were chosen and whether results are sensitive to these values.  \n4.‚ÄØ**Generalizability:**‚ÄØDiscuss applicability of Med‚ÄëHallMark beyond chest radiography (e.g., CT,‚ÄØMRI) and potential extension to non‚ÄëEnglish datasets.  \n5.‚ÄØ**Comparison with Prior Benchmarks:**‚ÄØClarify differentiation from general hallucination datasets (CHAIR,‚ÄØPOPE) and illustrate unique medical features beyond label granularity.  \n\n**5.‚ÄØMinor‚ÄØComments**\n- Unify notation (e.g., ‚ÄúMediHallDetector‚Äù capitalization).  \n- Fix typographical inconsistencies (missing spaces and superscripts).  \n- Condense redundant dataset descriptions in main text; leave details to appendix.  \n- Ensure figures, especially Fig.‚ÄØ1 and Fig.‚ÄØ2, have legible fonts for publication.  \n- Verify IEEE‚ÄØreference formatting.\n\n**6.‚ÄØOverall‚ÄØRecommendation**‚ÄØ‚Äì‚ÄØ**Major‚ÄØRevision**‚ÄØ  \nThe study is methodologically important and well executed but primarily proposes a benchmark; to meet TMI‚Äôs methodological‚Äërigor expectations the paper requires clearer quantitative validation, succinct presentation, and evidence of the metric‚Äôs clinical relevance.\n\n**7.‚ÄØConfidence‚ÄØLevel**‚ÄØ‚Äì‚ÄØ4‚ÄØ/‚ÄØ5‚ÄØ(High)  \n\n---\n\n**Summary Judgement:**‚ÄØThis submission introduces valuable resources for evaluating hallucinations in medical multimodal models‚Äîa timely and relevant contribution for trustworthy medical imaging AI.  With strengthened analysis and improved clarity, it could merit publication in IEEE‚ÄØTMI after major revision.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses hallucination detection in Large Vision Language Models (LVLMs) within medical contexts by introducing three main contributions. First, Med-HallMark is presented as the first benchmark for medical multimodal hallucination detection, supporting both Medical Visual Question Answering (Med-VQA) and Imaging Report Generation (IRG) tasks with hierarchical categorization of hallucinations into five severity levels (Section 3). Second, MediHall Score is proposed as a domain-specific evaluation metric that assigns scores based on hallucination severity (Section 4). Third, MediHallDetector is developed as a specialized model for detecting medical hallucinations through supervised fine-tuning (Section 5). The benchmark encompasses 7,341 samples across conventional, confidence-weakening, counterfactual, and image depiction questions (Figure 1). Experimental results demonstrate that existing LVLMs exhibit significant hallucination rates, with the proposed MediHall Score providing more nuanced assessment compared to traditional metrics (Tables 1-2).\n\n## Weaknesses\n\n‚Ä¢ **Insufficient mathematical rigor in metric formulation**\n  - The MediHall Score calculation lacks formal mathematical notation and precision (Section 4, lines 270-275). The averaging formula Hoverall = 1/k Œ£·µ¢‚Çå‚ÇÅ·µè H·µ¢ oversimplifies complex hierarchical relationships between different hallucination types.\n  - Score assignments (Hc = 0.0, Hcr = 0.2, etc.) appear arbitrary without statistical justification or sensitivity analysis supporting these specific values.\n  - No confidence intervals or uncertainty quantification is provided for the proposed metric, limiting its statistical interpretability in clinical contexts.\n\n‚Ä¢ **Limited scope and generalizability of benchmark construction**\n  - The dataset relies heavily on only four source datasets (SLAKE, VQA-RAD, MIMIC, OpenI) as stated in Section 3.4, potentially limiting diversity of medical conditions and imaging modalities.\n  - The hierarchical categorization system is developed with assistance from \"experienced clinicians\" (Section 3.3) but lacks details on inter-annotator agreement, annotation guidelines, or validation across different medical specialties.\n  - GPT-based augmentation for expanding questions (Section 3.4, lines 225-227) may introduce systematic biases that are not adequately controlled or measured.\n\n‚Ä¢ **Inadequate experimental validation and baseline comparisons**\n  - The evaluation of MediHallDetector against GPT-3.5 and GPT-4 is limited to only 300 samples (Section 6.3), which may not provide sufficient statistical power for robust conclusions.\n  - Missing comparisons with other specialized medical hallucination detection methods, as the paper claims to be \"the first\" but does not adequately survey related work in medical AI safety.\n  - Tables 1-2 show inconsistent performance patterns across models without adequate statistical significance testing or error bars to support the claimed superiority of the proposed approach.\n\n‚Ä¢ **Questionable clinical relevance and practical applicability**\n  - The paper lacks validation from actual clinical practitioners beyond the initial annotation phase, limiting evidence of real-world utility (no direct evidence found in the manuscript for clinical validation).\n  - The five-level hierarchical categorization system may be too complex for practical clinical implementation, as evidenced by the low consistency rates of even advanced models like GPT-4 (Figure 2b shows only 76.19% consistency).\n  - No discussion of computational costs, inference time, or scalability considerations for deployment in clinical settings where real-time performance is crucial.\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical formulation and statistical rigor**\n  - Provide formal mathematical definitions with proper notation for all components of the MediHall Score, including explicit handling of edge cases and missing data scenarios.\n  - Conduct sensitivity analysis to justify the specific score assignments and demonstrate robustness to parameter variations through systematic experimentation.\n  - Include confidence intervals and statistical significance testing for all metric comparisons, with appropriate corrections for multiple comparisons.\n\n‚Ä¢ **Expand benchmark diversity and validation methodology**\n  - Include additional medical imaging datasets from different institutions and geographical regions to improve generalizability and reduce potential dataset bias.\n  - Provide detailed inter-annotator agreement statistics (Cohen's kappa, Fleiss' kappa) and establish clear annotation protocols that can be replicated by other research groups.\n  - Replace or supplement GPT-based augmentation with human-generated questions to eliminate potential systematic biases in question formulation.\n\n‚Ä¢ **Strengthen experimental design and comparative analysis**\n  - Expand the evaluation dataset to at least 1,000-2,000 samples with stratification across different medical conditions and hallucination types for more robust statistical analysis.\n  - Include comparisons with domain-adapted versions of existing hallucination detection methods and general-purpose fact-checking systems applied to medical contexts.\n  - Report statistical significance tests, effect sizes, and confidence intervals for all performance comparisons, with proper correction for multiple hypothesis testing.\n\n‚Ä¢ **Validate clinical utility and practical deployment considerations**\n  - Conduct user studies with practicing radiologists and clinicians to evaluate the practical utility and interpretability of the proposed hierarchical categorization system.\n  - Simplify the categorization system based on clinical feedback, potentially reducing to 3-4 levels that align better with clinical decision-making workflows.\n  - Include comprehensive analysis of computational requirements, inference latency, and scalability metrics to demonstrate feasibility for clinical deployment scenarios.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Dingkang Yang",
      "Dongling Xiao",
      "Jiawei Chen",
      "Ke Li",
      "Lihua Zhang",
      "Mingcheng Li",
      "Shunli Wang",
      "Tong Wu",
      "Xiaolu Hou",
      "Yue Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_e3a42a649db419e8ec7cf4e6173f60905540569e.pdf",
    "remote_url": "https://openreview.net/pdf/e3a42a649db419e8ec7cf4e6173f60905540569e.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning to Teach: Improving Mean Teacher in Semi-supervised Medical Image Segmentation with Dynamic Decay  Modulation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "transfer learning, meta learning, and lifelong learning"
    ],
    "keywords": [
      "Meta learning",
      "Medical image segmentation",
      "semi-supervised learning"
    ],
    "abstract": "Medical image segmentation is essential in medical diagnostics but is hindered by the scarcity of labeled three-dimensional imaging data, which requires costly expert annotations. Semi-supervised learning (SSL) addresses this limitation by utilizing large amounts of unlabeled data alongside limited labeled samples. The Mean Teacher model, a prominent SSL method, enhances performance by employing an Exponential Moving Average (EMA) of the student model to form a teacher model, where the EMA decay coefficient is critical. However, using a fixed coefficient fails to adapt to the evolving training dynamics, potentially restricting the model's effectiveness. In this paper,\nwe propose Meta MeanTeacher, a novel framework that integrates meta-learning to dynamically adjust the EMA decay coefficient during training. We approach proposed Dynamic Decay Modulation (DDM) module in our Meta MeanTeacher framework, which captures the representational capacities of both student and teacher models. DDM heuristically learns the optimal EMA decay coefficient by taking the losses of the student and teacher networks as inputs and updating it through pseudo-gradient descent on a meta-objective. This dynamic adjustment allows the teacher model to more effectively guide the student as training progresses.\nExperiments on two datasets with different modalities, i.e., CT and MRI, show that Meta MeanTeacher consistently outperforms traditional Mean Teacher methods with fixed EMA coefficients. Furthermore, integrating Meta MeanTeacher into state-of-the-art frameworks like UA-MT, AD-MT, and PMT leads to significant performance enhancements, achieving new state-of-the-art results in semi-supervised medical image segmentation.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces the Meta Mean Teacher framework, a novel approach to improve semi-supervised medical image segmentation. Traditional Mean Teacher models use a fixed Exponential Moving Average (EMA) decay coefficient to update the teacher model, but this fixed value often limits model effectiveness. Meta Mean Teacher addresses this limitation by introducing a Dynamic Decay Modulation (DDM) module that adaptively adjusts the EMA decay coefficient based on training dynamics. This dynamic adjustment optimizes the student-teacher learning process, enabling better performance in tasks with limited labeled data.\n\nKey contributions of this work include:\n\n1. Adaptive EMA Decay: The DDM module optimizes the EMA decay coefficient, enhancing the model's adaptability and enabling it to capture richer training representations.\n2. Plug-and-Play Architecture: Meta Mean Teacher is designed to integrate seamlessly into existing models, improving performance across various Mean Teacher-based methods.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. Dynamic Adaptability: By incorporating the Dynamic Decay Modulation (DDM) module, the framework dynamically adjusts the EMA decay coefficient (Œ±) during training. This adaptability ensures that the teacher model evolves effectively with the student model, allowing more precise guidance as training progresses. This approach addresses a common limitation in fixed-coefficient Mean Teacher models, which often fail to account for varying training dynamics.\n\n2. Plug-and-Play Module: The Meta Mean Teacher framework is designed as a modular system, making it highly compatible with existing models based on the Mean Teacher architecture. This modularity allows easy integration into various semi-supervised frameworks like UA-MT, AD-MT, and PMT.\n\n3. Enhanced Stability and Robustness: The framework benefits from the Mean Teacher method‚Äôs inherent stability due to EMA but improves upon it by learning an optimal decay coefficient through meta-learning techniques.\n\n### Weaknesses\n\n1. High Computational Overhead: The adaptive EMA adjustment via DDM introduces complexity and requires more computational resources. The dynamic adjustment process, which includes cloning and iterative updates of both teacher and student models, may not be feasible for real-time or resource-limited applications, especially when processing large 3D medical imaging data.\n\n2. Limited Exploration of Other Adaptive Techniques: While the paper focuses on dynamically adjusting the EMA decay coefficient, other hyperparameters (like learning rates or loss weight factors) could also impact model performance in semi-supervised learning. The focus on only one parameter might restrict the overall optimization potential, as additional adjustments could further enhance the segmentation quality.\n\n### Questions\n\n1. On the impact of Œ±=0.01: Why does the model show improvement when Œ± is set to 0.01? This result seems to contradict the explanation provided in Section 3.1.\n\n2. The use of fixed Œ± in the ablation experiments in Section 4.3: In Section 4.3, why was the average fixed Œ± method chosen for comparison? From Figure 1, we can see that lower Œ± values ‚Äã‚Äã(such as 0.03, 0.05, and 0.1) significantly degrade the performance. In contrast, Œ± of 0.97 achieves performance higher than 0.84. Doesn't this general average comparison seem a bit biased?\n\n3. The impact of Œ± greater than 0.5: From Table 1, we can see that when Œ± is greater than 0.5, its impact on performance becomes less significant. Test whether randomly selected Œ± values ‚Äã‚Äãgreater than 0.5 are beneficial, thereby potentially improving the results?\n\n4. Suspicion about the data in Section 4.4 compared with other methods: Why is your experimental setting different from that in \"Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation\", but most of the state-of-the-art data (including LA and Pancreas-NIH datasets) are the same as the data in that paper? Does this indicate that the data is directly borrowed?\n\n5. The m symbol in Figure 2: What does \"m\" mean in Figure 2? Is this symbol redundant?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents the Meta Mean Teacher framework, a novel method aimed at enhancing semi-supervised medical image segmentation. Unlike traditional Mean Teacher models, which rely on a fixed Exponential Moving Average (EMA) decay coefficient, this approach introduces a Dynamic Decay Modulation (DDM) module that adaptively adjusts the EMA decay coefficient according to training dynamics. This innovation aims to optimize the student-teacher learning process, particularly in scenarios with limited labeled data.\n\n### Major Comments\n1. **Adaptive EMA Decay**: The DDM module successfully enhances the adaptability of the EMA decay coefficient, allowing the model to capture richer training representations.\n2. **Plug-and-Play Architecture**: The Meta Mean Teacher framework can be integrated seamlessly with existing models, thus improving performance across various Mean Teacher-based approaches.\n3. **High Computational Overhead**: The adaptive mechanism introduces complexity and demands substantial computational resources, potentially limiting feasibility for real-time or resource-constrained scenarios, especially with large 3D medical images.\n4. **Limited Exploration of Other Adaptive Techniques**: The focus on the dynamic adjustment of the EMA decay coefficient neglects other hyperparameters that could significantly affect model performance. This singular focus may restrict overall optimization and segmentation quality.\n\n### Minor Comments\n- Clarify the impact of setting Œ± = 0.01, as this contradicts the explanation in Section 3.1.\n- Explain the choice of using average fixed Œ± in the ablation studies of Section 4.3, given the significant performance variation observed with different Œ± values.\n- Investigate whether selecting random Œ± values greater than 0.5 could yield better results, as indicated by Table 1.\n- Address the variation in experimental settings compared to previous studies, such as the paper \"Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation.‚Äù\n- Clarify the meaning of \"m\" in Figure 2 and whether it is necessary.\n\n### Summary Paragraph\nIn terms of significance, the Meta Mean Teacher framework offers a promising direction in enhancing semi-supervised medical image segmentation by mitigating limitations found in fixed-decay Mean Teacher models. Its innovative use of a DDM module presents a notable advancement, although the dependency on adaptive techniques introduces considerable computational demands that may hinder practical applications. While the evaluation demonstrates engagement with model adaptability and performance, there is room for further exploration of additional hyperparameters. Overall, the work shows potential but requires further validation to address its computational demands and optimization scope.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the *Meta Mean Teacher* framework, designed to enhance semi-supervised medical image segmentation by overcoming limitations of conventional Mean Teacher models that rely on a fixed Exponential Moving Average (EMA) decay coefficient. The proposed model introduces a *Dynamic Decay Modulation* (DDM) module that adaptively adjusts the EMA decay parameter based on training dynamics, aiming to improve student-teacher co-evolution and segmentation performance in low-label scenarios. The work is clearly written, introducing a modular approach that can be integrated into various existing Mean Teacher-based methods.\n\n---\n\n**Major Comments**  \n1. **Computational Overhead:** The adaptive DDM process significantly increases computational complexity. The requirement to clone and iteratively update both teacher and student models may hinder scalability and practicality, particularly for large 3D datasets or resource-constrained environments.  \n2. **Limited Scope of Adaptivity:** The study only explores adaptation of the EMA decay coefficient. Extending the approach to other hyperparameters (e.g., learning rate, loss weighting) could yield further performance improvements and broader generalization.  \n3. **Experimental Clarifications:**  \n   - Please clarify why the model achieves improvement specifically at Œ± = 0.01, which appears inconsistent with the discussion in Section 3.1.  \n   - In Section 4.3, justify the use of a fixed Œ± average for comparison, as performance differences across Œ± values (e.g., lower values degrading results, Œ± = 0.97 performing better) suggest that the averaging strategy may bias outcomes.  \n   - Table 1 indicates that performance stabilizes when Œ± > 0.5. Would random selection of Œ± in this range still maintain or enhance results?  \n4. **Data and Experimental Consistency:** There is uncertainty regarding the dataset usage in Section 4.4, as it closely mirrors that used in the paper *Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation*. Please clarify whether data or configurations were directly reused.  \n\n---\n\n**Minor Comments**  \n- The meaning of the symbol ‚Äúm‚Äù in Figure 2 should be clarified; if redundant, consider removal.  \n- Presentation and figures are generally clear but could benefit from additional annotations aiding reader interpretation.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe *Meta Mean Teacher* framework is an innovative extension of the Mean Teacher paradigm, introducing dynamic adaptation of the EMA coefficient through meta-learning. It offers conceptual novelty and potential performance improvements in semi-supervised medical image segmentation. However, its elevated computational cost and narrow focus on a single adaptive parameter limit both practical applicability and generalizability. The evaluation is thorough but raises concerns about experiment consistency and clarity of comparative baselines. Reproducibility would benefit from further disclosure on data usage and training configurations. Overall, the contribution is promising but requires clarification and further validation.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper explores the EMA decay coefficient within the MT semi-supervised framework, fully tapping into the potential of the MT framework. Additionally, it introduces a novel meta-learning strategy to dynamically find the optimal EMA decay coefficient during the training process. Experiments conducted on two medical image datasets demonstrate that this method achieves superior performance.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper introduces a novel meta-learning strategy to adjust the EMA decay coefficient, fully tapping into the potential of the MT semi-supervised framework.\n2. This paper introduces a strategy to adjust the EMA decay coefficient to improve semi-supervised segmentation performance, which could be a meaningful contribution to this field.\n3. The extensive experimental results show the effectiveness of the proposed method.\n\n### Weaknesses\n\n1. I have not observed many innovative aspects in the application of meta-learning to the field of semi-supervised medical image segmentation. Part of the reason for this is the clarity of the writing; it is currently unclear what significant differences exist between the proposed DDM and previous meta-learning strategies. If there are no substantial differences, then the methodological contribution of this approach appears to be quite limited.\n2. Could the authors explain what potential drawbacks a fixed EMA decay coefficient might have on the MT framework, particularly in the context of medical image processing?\n3. The motivation is unclear. I do not understand why a dynamic change in $\\alpha$ would have a greater advantage compared to a fixed value. $\\alpha$ can be understood as the weight distribution between the teacher model‚Äôs parameters and the student model‚Äôs parameters during the iterative update process, with the teacher model‚Äôs weight being overwhelmingly dominant. I question the assumption that dynamically varying $\\alpha$ between 0.95 and 0.99 is necessarily better than a fixed value of 0.97. Could you provide a plot showing how $\\alpha$ changes dynamically over training iterations in the experiments?\n4. In Equation 2, what criteria does DDM use to derive Œ±m? Is there a relationship between $\\alpha_m$ and these two losses? For example, if the teacher model has a lower loss, should $\\alpha_m$ be larger? Please explain.\n5. The notation in the paper is somewhat confusing. In Equation 1, what are the differences between $\\Theta_s^*$ and $\\Theta_s$, and between $\\Theta_s$ and $\\theta_s$? Additionally, what is meant by meta data $\\mathcal{D}_m$, and how does it differ from labeled data and unlabeled data? Furthermore, the $\\mathcal{L}_m$ formula is missing; I suggest adding it.\n6. What is the initial value of $\\alpha$? Has an ablation study been conducted to verify the impact of $\\alpha$?\n7. The authors mention that DDM can be promoted as a plug-and-play component for different models. However, I think DDM has limitations. For instance, how would DDM be applied to commonly used pseudo-labeling methods based on CPS (Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision)?\n\n### Questions\n\nPlease refer to the weakness.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript investigates the Exponential Moving Average (EMA) decay coefficient within the Multi-Teacher (MT) semi-supervised framework, leveraging the capabilities of the MT architecture. It presents a novel meta-learning strategy that dynamically determines the optimal EMA decay coefficient throughout training. The experiments conducted on two medical image datasets indicate that the proposed method achieves superior performance.\n\n**Major Comments:**\n1. Limited innovation: The application of meta-learning in semi-supervised medical image segmentation lacks significant novelty. The writing clarity hampers understanding of the differences between the proposed Dynamic Decay Mechanism (DDM) and previous meta-learning approaches, suggesting limited methodological contributions.\n2. Fixed EMA decay coefficient drawbacks: Please elucidate potential drawbacks of employing a fixed EMA decay coefficient within the MT framework, especially in medical image processing contexts.\n3. Motivation clarity: The rationale for a dynamic EMA decay coefficient (Œ±) versus a fixed value remains unclear. Given the dominance of the teacher model's weights during parameter updates, please clarify why varying Œ± between 0.95 and 0.99 is preferable over a constant value of 0.97. A plot showing the changes in Œ± throughout training iterations would be beneficial.\n4. Derivation criteria for Œ±_m: In Equation 2, please specify the criteria used by DDM to derive Œ±_m, and clarify the relationship between Œ±_m and the two losses. Should Œ±_m increase if the teacher model exhibits lower loss?\n5. Notation confusion: Clarify distinctions between notations such as Œò_s*, Œò_s, and Œ∏_s in Equation 1. Define meta data (ùìì_m) and its differentiation from labeled and unlabeled data. Also, consider including the missing formula for ùìõ_m.\n6. Initial Œ± value and ablation study: What is the initial value for Œ±, and has an ablation study been performed to assess the impact of varying Œ±?\n7. Limitations of DDM: The authors state that DDM can serve as a plug-and-play component for various models. However, please explain how DDM could be integrated with commonly used pseudo-labeling methodologies based on Cross Pseudo Supervision (CPS).\n\n**Minor Comments:**  \n- Clarify unclear terms and concepts throughout.\n- Ensure consistency in notation and provide missing equations.\n\n**Summary Paragraph:**  \nThe manuscript exhibits significant strengths in its experimental validation and novel contribution towards optimizing the EMA decay coefficient using a meta-learning strategy. However, concerns regarding the innovative aspects of the approach, clarity of motivation, and notation consistency are notable weaknesses. In terms of significance, innovation, evaluation, and reproducibility, the work shows promise but requires revisions to enhance clarity and justification of its contributions to the field.\n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the role of the exponential moving average (EMA) decay coefficient in the Mean Teacher (MT) semi-supervised learning framework. It proposes a new meta-learning approach to dynamically optimize the EMA decay coefficient during training, aiming to enhance segmentation performance. Experiments on two medical image datasets demonstrate improved results. Overall, the paper is clearly structured and scientifically sound, though some aspects of novelty and clarity in the methodological description could be strengthened.  \n\n**Major Comments**  \n1. The claimed innovation‚Äîusing meta-learning to dynamically adjust the EMA decay coefficient‚Äîappears incremental. The manuscript should clarify how the proposed Dynamic Decay Module (DDM) differs from existing meta-learning strategies used in semi-supervised medical image segmentation. Without this distinction, the methodological contribution seems limited.  \n2. The motivation for dynamically updating the EMA decay coefficient is not fully convincing. The authors should explain why varying Œ± during training is expected to outperform a fixed coefficient (e.g., 0.97) and provide empirical visualization such as a curve showing Œ±‚Äôs evolution across iterations.  \n3. The paper should clarify what potential drawbacks a fixed EMA decay coefficient introduces within the MT framework, particularly in medical image analysis contexts.  \n4. Equation (2) lacks sufficient explanation: how is Œ±‚Çò derived, and what relationship exists between Œ±‚Çò and the loss terms? If the teacher model yields a lower loss, should Œ±‚Çò be larger? This reasoning should be provided.  \n5. Notation is confusing in places. The differences between Œò‚Çõ*, Œò‚Çõ, and Œ∏‚Çõ are unclear. The definition and role of metadata ùíü‚Çò, and how it differs from labeled and unlabeled data, require clarification. The formula for ùìõ‚Çò should also be included.  \n6. The initial value of Œ± should be specified, and an ablation study assessing its impact is needed.  \n7. While the authors claim DDM is plug-and-play, its applicability beyond the MT framework‚Äîe.g., to methods like Cross Pseudo Supervision (CPS)‚Äîremains uncertain and should be discussed.  \n\n**Minor Comments**  \n- Improve notation consistency and define all variables upon first use.  \n- Ensure equations include all referenced terms.  \n- Strengthen clarity in the description of DDM‚Äôs workflow.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses an important challenge in semi-supervised medical image segmentation and provides a well-motivated empirical evaluation. Its significance lies in improving the adaptability of the MT framework through dynamic parameter adjustment. However, the degree of methodological innovation over prior meta-learning approaches remains unclear. The experiments appear solid, but the lack of ablation details and missing formula definitions weaken interpretability and reproducibility. More comprehensive analyses and clearer exposition would enhance the paper‚Äôs credibility.  \n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The study shows potential but requires additional justification of novelty, clearer mathematical explanations, and more complete empirical validation to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents the 'Meta Mean Teacher', an approach for semi-supervised medical image segmentation. Building on the Mean Teacher model, which leverages exponential moving average (EMA) to create a stable teacher model from a student model, this framework introduces the Dynamic Decay Modulation (DDM) module. DDM dynamically adjusts the EMA decay coefficient based on both the student and teacher losses, improving the model's adaptability during training.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nThe paper addresses semi-supervised learning in medical image segmentation with a novel meta-learning approach, introducing the Dynamic Decay Modulation (DDM) module to adjust the EMA decay coefficient dynamically. \n\nThe paper strengthens its empirical evaluation by testing on three datasets, covering different imaging modalities.\n\n### Weaknesses\n\nWhile the paper builds on the Mean Teacher model, which is well-established in semi-supervised learning, it may lack substantial novelty as the framework mainly modifies an existing approach. Although the Dynamic Decay Modulation (DDM) module adds a new layer of adaptability, many similar extensions to Mean Teacher already exist, potentially limiting the paper's contribution to novel methodology.\n\n\nThe experimental scope appears limited as it only includes limited number of baseline methods, i.e. Mean Teacher variations like UAMT with UNet and VNet, models that have already been well-explored in this context. The paper‚Äôs experiments may be restricted by a limited range of labeled-to-unlabeled data ratios, which does not fully capture the model‚Äôs performance across different semi-supervised settings. Testing with a wider variety of label-scarcity scenarios would offer more robust insights into the framework's adaptability and practical applicability in real-world cases where data availability varies.\n\n### Questions\n\n(1) How do you ensure that comparisons are fair in semi-supervised learning scenarios? For example, I understand that in some cases, we can control the percentage of labeled and unlabeled data, such as using 5% or 10% labeled data. However, the feature distribution of labeled and unlabeled data cannot be guaranteed to be the same.\n\n\n(2) The exclusive use of VNet as the backbone may limit the generalizability of the results, as it does not reflect performance across more commonly used architectures like UNet or newer ViT-based UNets.\n\n(3) In Table 2, I observe that VNet‚Äôs performance is significantly lower than others when only 5% of data is labeled, but it is only slightly lower when 10% is labeled. Could you explain why this discrepancy occurs? Additionally, could you provide more results for cases with 20%, 50%, 80%, and 90% labeled data, if available?\n\n(4) In table 3, why VNet outperforms UA-MT when 20% are labeled.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript introduces the 'Meta Mean Teacher', an innovative approach designed for semi-supervised medical image segmentation. It builds upon the established Mean Teacher model, incorporating an adaptive mechanism through the Dynamic Decay Modulation (DDM) module, which dynamically adjusts the exponential moving average (EMA) decay coefficient based on both student and teacher losses. The clarity of the presentation is adequate, but improvements could enhance understanding.\n\n**Major Comments:**\n1. **Novelty:** The framework modifies an existing Mean Teacher approach, which may limit its novelty. Although DDM enhances adaptability, similar extensions to the Mean Teacher have been previously proposed, challenging the paper‚Äôs contribution to innovative methodology.\n2. **Experimental Scope:** The experimentation relies on a narrow selection of baseline methods, including variations of Mean Teacher such as UAMT with UNet and VNet, which may have been extensively explored. Expanding the experimental design to include additional baseline models would enhance the robustness of the results.\n3. **Generalization:** The exclusive use of VNet as the backbone architecture may hinder generalizability. Including more commonly adopted architectures such as UNet or ViT-based UNets would provide a broader performance perspective.\n4. **Label Ratios:** The limited range of labeled-to-unlabeled data ratios in the experiments might not showcase the model's performance comprehensively across various semi-supervised settings. Evaluating a wider spectrum of label scarcity would yield greater insights into adaptability and real-world applicability.\n\n**Minor Comments:**\n- Clarification needed on how to ensure fair comparisons in semi-supervised learning, particularly regarding feature distribution discrepancies between labeled and unlabeled data.\n- The performance discrepancy in VNet across varying labeled data percentages requires further explanation (e.g., the significant drop at 5% labeled data but a small drop at 10%).\n- Additional results for greater labeled data percentages (20%, 50%, 80%, and 90%) would be beneficial.\n  \n**Summary Paragraph:**\nIn relation to TMI criteria, the manuscript presents significant contributions in addressing semi-supervised learning in medical image segmentation; however, it struggles with innovation due to its reliance on an existing model. The evaluation could be strengthened by a broader experimental design, allowing for more robust comparisons, and enhancing reproducibility across various scenarios. Overall, the work offers a sound basis but raises concerns regarding its novelty and evaluation scope.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Meta Mean Teacher* framework for semi-supervised medical image segmentation. Building on the established Mean Teacher paradigm, the paper proposes a *Dynamic Decay Modulation (DDM)* module that dynamically adjusts the exponential moving average (EMA) decay coefficient based on both student and teacher losses, aiming to enhance adaptability during training. The authors evaluate their method across three datasets encompassing different imaging modalities. Overall, the paper is clearly written and addresses a relevant problem in semi-supervised segmentation, though its methodological novelty and evaluation breadth appear limited.\n\n---\n\n**Major Comments**  \n1. **Novelty and Contribution:** The approach largely extends the traditional Mean Teacher framework rather than introducing a fundamentally new semi-supervised learning paradigm. Although DDM offers an adaptive mechanism, similar modifications to EMA-based methods are already present in prior work, reducing the level of methodological innovation.  \n2. **Experimental Scope:** The evaluation includes only a narrow set of baseline comparisons‚Äîmainly Mean Teacher variants such as UAMT with UNet and VNet. A broader selection of methods would better position the proposed approach within the current landscape.  \n3. **Data Regime Coverage:** The experiments explore only a limited range of labeled-to-unlabeled ratios, restricting insights into how the model performs under varying levels of supervision. Additional tests (e.g., 20%, 50%, 80%, 90% labeled data) would strengthen claims of robustness and adaptability.  \n4. **Backbone Generalization:** The exclusive use of VNet as the backbone limits assessment of architectural generalizability. Benchmarking with more common or recent architectures, such as UNet or ViT-based models, would improve confidence in the results.  \n5. **Fairness of Comparisons:** Clarification is needed on how fairness is ensured in semi-supervised setups, particularly regarding differences in labeled/unlabeled data distributions.\n\n---\n\n**Minor Comments**  \n- Clarify performance discrepancies observed in Tables 2 and 3, such as why VNet performs worse at 5% labeled data but improves at higher proportions, and why it surpasses UA-MT at 20%.  \n- Ensure tables include all relevant experimental details for reproducibility.  \n- English presentation and figure quality are adequate; only minor polishing may be required.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important topic‚Äîimproving semi-supervised segmentation using adaptive teacher-student consistency. Its significance lies in potential enhancements to model stability under limited annotations. However, innovation is modest since it mainly adapts an existing framework. The evaluation demonstrates cross-modality testing but remains narrow in baseline diversity and data regime coverage. Reproducibility is acceptable given methodological clarity, though more architectural and experimental variation would strengthen generalizability.\n\n---\n\n**Decision Recommendation:** **Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a methodological innovation in *semi-supervised medical image segmentation*, introducing a *meta-learned dynamic decay mechanism* for the Mean Teacher framework. The contribution squarely targets algorithmic and training methodology, relevant to TMI‚Äôs focus on methodological advances in image analysis rather than pure application studies.\n\n**2. Novelty & Contribution Level**  \nThe proposed *Dynamic Decay Modulation (DDM)* and *Meta Mean Teacher* framework represent a conceptual and technical contribution: using meta-learning to adapt a critical hyperparameter (EMA decay) during training. While the Mean Teacher and meta-learning foundations are not new, their integration for dynamic control of the EMA coefficient is a novel idea with plausible generality. The incremental benefit (~0.5‚Äì1.5% improvement in Dice or Jaccard) is modest but consistent, demonstrating methodological value.  \n‚Üí **Assessment:** Methodologically moderate-to-strong novelty relative to existing semi-supervised segmentation paradigms.\n\n**3. Technical and Experimental Rigor**  \nThe experimental design includes:  \n- Comparisons on two standard 3D datasets (LA MRI, Pancreas-NIH CT).  \n- Ablation testing between fixed, teacher-guided, and student-guided DDM update strategies.  \n- Integrations into multiple Mean Teacher derivatives (UA-MT, AD-MT, PMT).  \nThe details of optimization and hyperparameters are adequately stated. However, documentation of *dataset splits*, *statistical significance of improvements*, *runtime overhead*, and *reproducibility assurances* (e.g., code sharing specifics) are limited. The theoretical justification adds conceptual clarity but remains informal‚Äînot rigorous mathematics.  \n\n**4. Clarity and Presentation**  \nOverall clear and fluent English with a professional structure (abstract, introduction, method, experiments). Figures and tables are informative. Minor issues: formatting glitches (e.g., subscript/superscript spacing, occasionally inconsistent symbol rendering), some sentence-level redundancy, and overlength explanations. Grammar is good but readability could be improved by tighter phrasing and clearer figure captions.\n\n**5. Ethical and Reproducibility Compliance**  \nDatasets used (LA, Pancreas-NIH) are public and anonymized. There is no mention of institutional review board (IRB) approvals or patient data handling details, but expected for public datasets. The authors *state* that code will be released upon acceptance‚ÄîTMI recommends release at submission or acceptance. Reproducibility would benefit from inclusion of random seeds and precise training-validation-test partitioning.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe paper proposes *Meta Mean Teacher*, a new semi-supervised segmentation framework combining the classical Mean Teacher approach with *Dynamic Decay Modulation (DDM)*. DDM employs a small meta-learned network that dynamically adjusts the exponential moving average (EMA) decay coefficient (Œ±) between student and teacher models based on their respective losses. The authors introduce a ‚Äústudent-guided‚Äù meta-learning strategy, theoretically argue its advantage, and empirically validate it on two 3D medical image segmentation benchmarks (LA MRI and Pancreas CT). The approach serves as a plug-and-play extension to current Mean Teacher‚Äìstyle methods, yielding consistent but moderate performance improvements.\n\n**2. Strengths**\n- Clear methodological scope relevant to semi-supervised medical image segmentation.  \n- Conceptually elegant integration of meta-learning for dynamic hyperparameter control.  \n- Broad validation across multiple MT-based architectures.  \n- Theoretical motivation clarifies underlying intuition.  \n- Demonstrated generality on multiple imaging modalities.  \n\n**3. Weaknesses**\n- Incremental gains: improvements are small relative to added complexity.  \n- Limited statistical analysis or robustness studies (no variance over multiple runs).  \n- Theoretical analysis is heuristic rather than formal proof-based.  \n- No exploration of runtime or computational burden of the DDM (an important consideration for practical adoption).  \n- Only two datasets; broader validation (other segmentation tasks or modalities) would strengthen claims.  \n- Reproducibility could be improved by earlier public code release and complete dataset split documentation.\n\n**4. Major Comments**\n1. **Quantitative significance** ‚Äì Provide variance/error bars or multiple runs to demonstrate robustness of improvements. Are observed gains statistically significant given stochastic training variance?  \n2. **Complexity analysis** ‚Äì Report computational cost, training time overhead, and parameter count associated with DDM.  \n3. **Generalization** ‚Äì Consider adding experiments on a different anatomical task or at least cross-validation between modalities to claim methodological generality.  \n4. **Clarity on meta-learning setup** ‚Äì Expand explanation of meta-dataset selection: how is *Dm* separated from labeled/unlabeled data, and how does this affect data efficiency?  \n5. **Code and reproducibility** ‚Äì Strongly recommend providing a GitHub repository link or detailed pseudocode, per TMI reproducibility policy.  \n6. **Theoretical exposition** ‚Äì The current ‚Äúfunction space‚Äù argument is qualitative. Strengthen with a clearer mathematical formulation or intuition figures.  \n\n**5. Minor Comments**\n- Fix isolated formatting errors (e.g., missing spaces in formulas and variable superscripts).  \n- In Figures 1‚Äì3, ensure high-resolution and clearer captions explaining all abbreviations.  \n- Recheck references for consistent IEEE style capitalization.  \n- Eliminate redundant sentences in Introduction (lines 056‚Äì080 overlap with Abstract).  \n- Clarify units and voxel size descriptions consistently (LA dataset section).\n\n**6. Overall Recommendation**  \n**Major Revision** ‚Äì The work presents a promising and conceptually novel enhancement to Mean Teacher‚Äìbased semi-supervised segmentation frameworks with methodological relevance for TMI. However, the manuscript needs additional experimental rigor (statistical significance, efficiency analysis) and clearer methodological exposition to reach publishable standard.\n\n**7. Confidence Level:** 4/5  \n(High confidence in technical assessment; moderate uncertainty about expected generalization beyond tested datasets.)\n\n---\n\n**Reviewer Summary Comment to Editor**  \nThe submission introduces an interesting meta-learning‚Äìbased mechanism for dynamic teacher-student consistency adaptation, fitting within TMI‚Äôs methodology scope. The idea is novel enough to warrant consideration, though improvements are numerically small and the paper lacks some reproducibility and statistical analyses expected for an IEEE TMI-level contribution. With a strong revision focusing on experimental substantiation and presentation polish, it may reach publishable quality.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses semi-supervised medical image segmentation by proposing Meta Mean Teacher, which dynamically adjusts the Exponential Moving Average (EMA) decay coefficient in Mean Teacher frameworks. The authors introduce a Dynamic Decay Modulation (DDM) module that learns optimal EMA coefficients through meta-learning, using student and teacher model losses as inputs. The method employs a student-guided meta-learning approach where DDM parameters are updated via pseudo-gradient descent on meta-data. Experiments on LA Heart (MRI) and Pancreas-NIH (CT) datasets show improvements when DDM is integrated into existing frameworks (UA-MT, AD-MT, PMT), with reported performance gains of 0.91% Dice improvement on LA dataset with 5% labeled data.\n\n## Weaknesses\n\n‚Ä¢ **Theoretical Analysis Lacks Rigor**\n  - The probability argument in Equation 3 (Page 6, lines 274-275) states P[Œ∏* ‚àà span(Œ∏t, Œ∏s)] ‚âà 0 without mathematical justification or dimensionality assumptions\n  - The Universal Approximation Theorem application in Equation 5 (Page 6, line 284) ignores practical constraints like network capacity and training dynamics\n  - The claim that teacher-guided approach is theoretically inferior (Page 6, Section 3.2) lacks formal proof and contradicts empirical evidence in some cases\n\n‚Ä¢ **Experimental Design and Evaluation Issues**\n  - Table 1 (Page 7) shows DDM degrading performance for UA-MT on multiple metrics (95HD: +11.88, ASD: +3.33), contradicting the method's claimed universality\n  - Missing MCF results for 5% labeled data (Table 2, Page 8) and 10% labeled data (Table 3, Page 9) prevent comprehensive comparison\n  - Algorithm 1 (Page 5) lacks specification of meta-iteration count, learning rates Œ∑D, and convergence criteria, making reproduction difficult\n\n‚Ä¢ **Technical Implementation Concerns**\n  - The DDM architecture details are not specified beyond \"small neural network, typically MLP\" (Page 4, lines 195-196), limiting reproducibility\n  - The bilevel optimization in Equation 1 (Page 5, line 253) may suffer from optimization instability, but no convergence analysis is provided\n  - The meta-data splitting strategy is not described, potentially leading to data leakage between training and meta-learning phases\n\n‚Ä¢ **Limited Scope and Generalizability**\n  - Only two datasets tested, both from medical imaging domain, limiting evidence of broader applicability claimed in abstract\n  - The method requires additional meta-data partitioning, reducing already scarce labeled data available for semi-supervised learning\n  - No computational overhead analysis provided despite introducing additional meta-learning iterations and DDM forward passes\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen Theoretical Foundation**\n  - Provide rigorous mathematical proof for the probability claim in Equation 3 with explicit dimensionality assumptions and geometric constraints\n  - Replace the informal Universal Approximation Theorem argument with analysis specific to EMA constraints and finite network capacity\n  - Develop formal convergence guarantees for the bilevel optimization problem or provide empirical convergence analysis across different datasets\n\n‚Ä¢ **Address Experimental Inconsistencies**\n  - Investigate and explain why DDM degrades UA-MT performance on boundary metrics (Table 1), potentially through additional ablation studies or method refinement\n  - Complete missing baseline comparisons (MCF results) and ensure all competing methods are evaluated under identical experimental conditions\n  - Specify all hyperparameters in Algorithm 1 including meta-iteration counts, learning schedules, and provide sensitivity analysis for key parameters\n\n‚Ä¢ **Enhance Technical Clarity and Reproducibility**\n  - Provide detailed DDM architecture specifications including layer dimensions, activation functions, and initialization strategies\n  - Analyze and discuss potential optimization instabilities in the bilevel formulation, possibly including gradient clipping or other stabilization techniques\n  - Clearly describe the meta-data partitioning strategy and verify no information leakage occurs between different learning phases\n\n‚Ä¢ **Expand Experimental Validation**\n  - Evaluate on additional datasets beyond medical imaging (e.g., natural images, other semi-supervised domains) to support generalizability claims\n  - Develop strategies to minimize meta-data requirements or demonstrate performance when meta-data is very limited\n  - Provide comprehensive computational cost analysis including training time, memory usage, and inference overhead compared to baseline methods",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces Meta Mean Teacher, a novel framework for semi-supervised medical image segmentation that dynamically adjusts the Exponential Moving Average (EMA) decay coefficient during training using a Dynamic Decay Modulation (DDM) module. The proposed framework is designed to improve the performance of the Mean Teacher model by adapting the EMA decay coefficient based on the losses of both student and teacher models. The authors claim that their method outperforms traditional Mean Teacher approaches with fixed EMA coefficients and achieves state-of-the-art results when integrated into other semi-supervised frameworks. The manuscript is well-written, and the motivation for dynamic adjustment of the EMA decay coefficient is clear.\n\n###\n\n## Major Comments\n1. Novelty and Positioning:\n   - The introduction of the DDM module for dynamic adjustment of the EMA decay coefficient is innovative. However, the manuscript needs to provide a more thorough discussion of how this approach differs from existing works that also address the issue of adaptive hyperparameters. For example, the manuscript should clarify how the proposed method compares to recent advancements in meta-learning and adaptive hyperparameter tuning.\n   \n2. Evaluation Design:\n   - The experiments are conducted on two datasets: LA Heart and Pancreas-NIH. While these datasets cover different modalities (CT and MRI), the evaluation is somewhat limited in scope. The authors should consider expanding the validation to include additional anatomical structures and datasets to better establish the generalizability of their approach.\n   \n3. Comparisons:\n   - The manuscript compares the proposed method with several state-of-the-art frameworks, including UA-MT, AD-MT, and PMT. However, the baseline comparisons lack some recent semi-supervised segmentation methods, particularly those that employ advanced regularization techniques or novel network architectures. Including these comparisons would provide a more comprehensive assessment of the proposed method's performance.\n   \n4. Reproducibility:\n   - The authors mention that they will release the code upon acceptance. However, the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters used in the experiments. Providing a more complete methodology section is crucial for ensuring reproducibility.\n\n###\n\n## Minor Comments\n1. Figures: Figures 1 and 3 are informative but could be improved by providing clearer legends and labels. Some figures are cluttered, making it difficult to interpret the results.\n2. Notation: The notation used in the method section is sometimes inconsistent. For example, the use of `fŒ∏t` and `fŒ∏s` for the teacher and student models should be consistently defined.\n3. Acronyms: Several acronyms are used without proper definitions, such as \"DDM\" and \"MT\". Defining these early in the text would improve clarity.\n4. Typographical Issues: There are minor typographical errors, such as \"k-spacce\" and \"undersampling maskes\", which should be corrected.\n\n###\n\n## Summary Paragraph\nThe manuscript addresses the significant challenge of semi-supervised medical image segmentation by proposing a novel framework that dynamically adjusts the EMA decay coefficient. The proposed DDM module offers a promising approach to enhance the Mean Teacher framework. However, the manuscript could benefit from a more comprehensive evaluation across a wider range of datasets and anatomical structures. The comparative analysis could also be strengthened by including additional recent methods. Despite these limitations, the manuscript presents a technically sound and potentially impactful contribution to the field. The reproducibility of the approach is currently insufficient due to incomplete methodological details, which should be addressed in a revised version.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis to include additional recent methods, broaden their validation across multiple datasets and anatomical structures, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chen Chen",
      "Le Wang",
      "Ning Gao",
      "Sanping Zhou"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a2ba00a164f58076d0fbf59a182260f8e32b0178.pdf",
    "remote_url": "https://openreview.net/pdf/a2ba00a164f58076d0fbf59a182260f8e32b0178.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Conformal confidence sets for biomedical image segmentation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Deep learning",
      "neural networks",
      "uncertainty quantification",
      "confidence sets"
    ],
    "abstract": "We develop confidence sets which provide spatial uncertainty guarantees for the output of a black-box machine learning model designed for image segmentation. To do so we adapt conformal inference to the imaging setting, obtaining thresholds on a calibration dataset based on the distribution of the maximum of the transformed logit scores within and outside of the ground truth masks. We prove that these confidence sets, when applied to new predictions of the model, are guaranteed to contain the true unknown segmented mask with desired probability. We show that learning appropriate score transformations on an independent learning dataset before performing calibration is crucial for optimizing performance. We illustrate and validate our approach on polyps colonscopy, brain imaging and teeth datasets. To do so we obtain the logit scores from deep neural networks trained for polyps, brain mask and tooth segmentation segmentation. We show that using distance and other transformations of the logit scores allows us to provide tight inner and outer confidence sets for the true masks whilst controlling the false coverage rate.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes a conformal prediction based method to quantify the uncertainty for medical image segmentation. The proposed method is particularly designed for pre-trained segmentation models which notoriously make overconfident and wrong predictions. The proposed method learns thresholds using the maximum logit scores from a calibration set for the inside and outside of the ground truth masks and apply them on the logit scores of the test image to return conformalized segmentation prediction which guarantees to include the ground truth segmentation. The paper shows that naively learning the outside thresholds on max logits is not optimal and propose to transform the scores using a distance to make sure that far away pixels have lower scores. The method is validated on a single dataset for polyp segmentation and the results show that the proposed method produces conformal sets with narrower boundaries compared to using scores which are not transformed.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 1\n\n### Strengths\n\n- The idea of using transformed max logit scores is simple but quite effective strategy to produces conformal segmentation sets.\n- The presented experiments show the effectiveness of the method compared to using non-transformed logits.\n\n### Weaknesses\n\n1- Although I found the proposed idea of transforming max logit scores interesting, I don't think that the paper presents enough contribution to be presented in ICLR. The idea of applying conformal prediction to max logits for inside and outside of the boundaries is a direct extension of initial conformal prediction methods developed for segmentation, and applying transformations based on distance is an intuitive choice to refine predicted boundaries.\n\n2- The paper does not present any comparisons with the existing conformal prediction works for image segmentation.\n\n[1] Mossina et al. Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty, CVPR Workshops, 2024,\n\n3- The method is evaluated on only a single dataset. Multiple datasets should be included to make sure that the performance generalizes across datasets.\n\n4- In many segmentation tasks, we are interested in segmenting multiple structures. The paper only focuses on binary segmentation. I think the method should be validated on multi-class setting to make sure that it is also applicable in that setting.\n\n5- The explanation of how the method is applied at test time could also be clearer. As I understand it, during testing, the method applies the inner threshold on max logits to find inner boundaries, then applies a distance transformation based on each pixel‚Äôs distance from these inner boundaries, and finally applies an outer boundary threshold. However, the exact steps of the algorithm during test time need more clarification.\n\n6- In conventional uncertainty quantification algorithms for segmentation such as [2, 3] the uncertainty is quantified by the variance of the segmentation samples generated from the posterior distribution. How can the quantification be done in this case? Is it the margin between the inner and outer boundaries? Is the uncertainty quantified by the algorithm correlates with the uncertainty in the input image? For example, does the method output larger margins when there is greater disagreement between the segmentations of different experts?  \n\n[2] Kohl et al. A Probabilistic U-Net for Segmentation of Ambiguous Images\n[3] Erdil et al. MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\n\n7- The margin between the inner and outer boundaries appears quite large and there can be many unplausible segmentations within this area. For practical applications, an uncertainty quantification method should ideally produce a set of plausible segmentation samples within this margin, rather than simply indicating a large margin that may or may not include the ground truth segmentation. How could one obtain a plausible segmentation sample from this margin?\n\n### Questions\n\n- How does the results generalize to other datasets and segmentation of multiple structures?\n- How does the uncertainty quantified by the proposed method relates with the real uncertainty (assuming it can be measured by the disagreement between multiple experts)?\n- How one can use the proposed method in a practical application? Can we get samples of plausible segmentations within the margin outputted by the algorithm?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a method utilizing conformal prediction to quantify uncertainty in medical image segmentation, specifically aimed at addressing the overconfident predictions of pre-trained segmentation models. By learning thresholds from maximum logit scores within a calibration set and applying these to test images, the method generates conformalized segmentation predictions that are guaranteed to encompass the ground truth. While the approach shows promise through its validation on a polyp segmentation dataset, its contributions appear limited.\n\n### Major Comments\n1. The core idea of transforming max logit scores for improved segmentation is effective, yet the contribution seems minimal for ICLR consideration. The methodology seems to extend existing conformal prediction techniques without significant novelty.\n2. There is a lack of comparisons with existing conformal prediction works in image segmentation, such as those by Mossina et al. (2024).\n3. Evaluation is conducted on a single dataset, which raises concerns about the generalizability of the method's performance across diverse datasets.\n4. The focus on binary segmentation limits the applicability; validation in a multi-class setting is recommended.\n5. Clarity regarding test-time application of the method is insufficient. A step-by-step explanation of the process involving thresholds and distance transformations is needed.\n6. The comparison of uncertainty quantification with conventional methods remains unclear. It is unclear if the method‚Äôs margins effectively correlate with input image uncertainty or reflect expert disagreement in segmentation.\n7. The large margins between inner and outer boundaries suggest potential implausibility in segmentations within that area, indicating the need for a method that produces plausible segmentation samples.\n\n### Minor Comments\n- Figure captions could be clearer.\n- Some terminology and acronyms require definitions or additional context for clarity.\n- There are minor typographical errors that should be addressed.\n\n### Summary Paragraph\nThe paper offers an innovative approach to uncertainty quantification in medical image segmentation, showing potential effectiveness through transformed max logit scores. However, the significance and novelty of the contribution raise concerns, as the method largely relies on existing techniques without thorough comparisons or validation across multiple datasets. Issues related to clarity in methodology and questions regarding the real-world applicability of the uncertainty measures also detract from the overall impact of the work.\n\n### Decision Recommendation\nMajor revision is recommended, focusing on enhancing the novelty of the contributions, providing comparative analyses with prior work, expanding the evaluation across multiple datasets, and improving the clarity of the methodological exposition.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a conformal prediction‚Äìbased method for quantifying uncertainty in medical image segmentation, targeting pre-trained models that may produce overconfident predictions. The approach calibrates thresholds for inside and outside regions of ground-truth masks using maximum logit scores and applies these thresholds to test images. A distance transformation is incorporated to adjust scores such that pixels farther from boundaries receive lower confidence. The method is validated on a single polyp segmentation dataset, demonstrating narrower conformal boundaries than versions without distance transformation. The paper is clearly written but limited in scope and contribution.\n\n---\n\n**Major Comments**  \n1. **Contribution and Novelty:** While the idea of transforming maximum logit scores is conceptually sound, the novelty is limited. The method extends existing conformal prediction frameworks for segmentation in a straightforward way. Using distance-based transformations appears as an intuitive refinement rather than a substantial methodological advance.  \n2. **Comparative Evaluation:** The paper lacks direct comparisons with prior conformal prediction approaches for image segmentation, such as Mossina et‚ÄØal., CVPR Workshops,‚ÄØ2024. Including such baselines would strengthen claims of improvement.  \n3. **Dataset Limitation:** Evaluation is restricted to a single dataset. Experiments on multiple datasets are needed to demonstrate generalizability.  \n4. **Scope of Segmentation Tasks:** The study only addresses binary segmentation. Validation on multi-class segmentation would be necessary to confirm applicability to broader clinical settings.  \n5. **Clarity of Method Description:** The procedure at test time is insufficiently explained. The sequence of applying inner thresholds, distance transformations, and outer thresholds should be clearly described.  \n6. **Quantification of Uncertainty:** The manuscript does not clarify how uncertainty is explicitly measured or interpreted. It remains unclear whether the margin between inner and outer boundaries meaningfully correlates with true uncertainty or expert disagreement.  \n7. **Practical Interpretability:** The large margins between boundaries may encompass many implausible segmentations. The paper should discuss how plausible segmentation samples could be derived within these conformal regions.\n\n---\n\n**Minor Comments**  \n- Improve clarity in the methodological description, especially regarding test-time implementation steps.  \n- Define how the proposed uncertainty measure relates to conventional metrics (e.g., variance-based approaches).  \n- Clarify notation for thresholds and logit transformations.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses a relevant problem‚Äîuncertainty quantification in medical image segmentation‚Äîthrough conformal prediction. Its innovation lies in applying distance-transformed logits, though the conceptual novelty and breadth of validation remain modest. Experimental evidence is limited, and the reproducibility could be improved through clearer methodological details. The study modestly advances the field but falls short in terms of generalization, comparison, and interpretability of uncertainty estimates.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors formally present an approach that aims at inferring uncertainty margins to segmentations. They propose either take the logit score of a CNN and to threshold it to obtain this margin, or to threshold at a certain distance to the predicted segmentation. Threshold and type of margin (logit score / distance) is to be identified experimentally for a given dataset. Experiments on one public dataset  are shown (containing still images from minmally invasive surgery).\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* The authors present the problem in a formal manner, relating it to existing work.  \n* The overall problem addressed is relevant.\n\n### Weaknesses\n\n* The motivation for the scores functions (logit, distance, ...) is weak. The necessity to choose the type and to even mix them gives the overall approach a bit of a heuristic touch. (While I do understand that you would consider your contribution here to be in the formal derivation of underlying theory, i.e., very much the opposite of a heuristic.)\n* The experiments only provide insights into one very narrow application. they are merely fulfilling the purpose of an illustation of the problem, but not a validation.\n\n### Questions\n\n* You are testing on public data. Has your pretrained polyp segmentation algorithm been trained on the same public data? \n* Are there any susequent video frames in the dataset, or images of the same polyp / patient? If there are, did you stratify your training / testing set accordingly?  \n* Please remove the reference to tumors throughout the paper. Polyps may be precursors to tumors, but they aren't any. \n* You are using a dataset from different centers, there may be systematic differences in how the polyp areas are annotated - some annotators being more inclusive with respect to surrounding tissue, others being less. How does this variability impact on your measure? \n*  I might have missed it but what is the accuracy of your underlying segmentation algorithm? I would be under the impression that it is a well performing algorithm on a rather easy segmentation task? How does your approach relate to extrema in algorithmic performance, i.e., perfect segmentations or complete misses? \n* You are stating \"In order to make efficient use of the data available, the learning dataset can in fact contain some or all of the data used to train the image segmentor.\" Your training data may be fairly overfittet impacting on your logit score and, hence, your choice of margin (logit/distance, thresholds). Wouldn't it be a safer approach to generate cross-validated logit functions and use them in the comparison?\n* I understand that the primary contribution of this study is the theory offered. Still, you are stressing that your algorithm is a very lightweight addition to any pretrained segmentation algorithm. And there are a lot of standard computer vision / biomedical image data sets for segmentation available, as well as pretrained algorithms. Would you be able to generate segmentations maps for predefined certainty levels, and compare these levels with the testing performances across a larger set of applications? It would be quite convincing, if e.g., your 90% certainty map of the outer margin would indeed include 90% pixels of a test set or lead to a sufficiently large overlap (that has previously been defined) in 90% of all test cases.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3\n\n### Details Of Ethics Concerns\n\nnone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents an approach for inferring uncertainty margins in segmentations using logit scores from Convolutional Neural Networks (CNNs) or distance-based thresholds relative to predicted segmentations. The method is explored through experiments conducted on a public dataset comprising still images from minimally invasive surgeries. While the problem addressed is relevant, the clarity and soundness of the contribution require improvement.\n\n**Major Comments:**  \n1. The motivation behind the choice of scoring functions (logit vs. distance) is inadequately justified, resulting in a heuristic character to the methodology.\n2. The experiments focus solely on a very narrow application, serving more as an illustration rather than comprehensive validation of the approach.\n3. The training of the polyp segmentation algorithm on the same public dataset should be clarified to ensure validity.\n4. It is essential to address whether subsequent video frames or images of the same polyp/patient exist and if the training/testing sets are appropriately stratified.\n5. The manuscript makes repeated references to tumors, which should be corrected as polyps are precursors rather than tumors themselves.\n6. The variability in annotation across different centers could impact the measures; this factor requires discussion.\n7. The accuracy of the underlying segmentation algorithm needs to be stated and discussed relative to its performance extremes.\n8. Concerns arise regarding overfitting if the learning dataset contains images used for training the segmentor; cross-validated logit functions should be considered for comparison.\n9. The authors should explore generating segmentation maps at predefined certainty levels and compare these to testing performances across a larger set of applications.\n\n**Minor Comments:**  \n- Remove references to tumors throughout the manuscript.  \n- Clarify the accuracy of the segmentation algorithm.  \n- Revise figures and notation for improved clarity.  \n- Address typos found within the text.\n\n**Summary Paragraph:**  \nIn conclusion, while the study presents a theoretically sound approach to addressing segmentation uncertainty, it struggles with empirical validation and clarity regarding its methodological foundations. The significance of the contribution is somewhat tempered by the limited scope of experiments, which impacts reproducibility. There remains a need for innovation within a broader context and a clearer linkage between methodology and algorithmic performance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a formal framework for estimating uncertainty margins in image segmentations. The authors propose two methods‚Äîusing the CNN logit score or distance from the predicted segmentation‚Äîto define the margin. The threshold and margin type are determined experimentally for a specific dataset. Experiments are demonstrated on a single public dataset composed of still images from minimally invasive surgery. The paper addresses a relevant problem, and the formalization is clearly related to existing literature, though the presentation and validations remain limited.  \n\n**Major Comments**  \n1. **Motivation and Theoretical Coherence:** The justification for using logit or distance scores as uncertainty functions is weak. The need to manually select or even combine these adds a heuristic quality that contradicts the paper‚Äôs claim of formal rigor.  \n2. **Experimental Breadth and Validation:** The evaluation is restricted to a narrow dataset and mostly serves an illustrative purpose rather than a validation demonstrating generality or robustness.  \n3. **Training Protocol Clarifications:** It is unclear whether the pretrained polyp segmentation algorithm was trained on the same public data used for testing. Additional information about sample stratification is needed, particularly regarding possible overlap of frames or patients.  \n4. **Dataset and Annotation Variability:** The dataset originates from multiple centers. Differences in annotation style could systematically affect uncertainty estimates; the manuscript should address this potential bias.  \n5. **Base Model Performance:** The accuracy of the underlying segmentation model is unspecified. Clarifying this is necessary to interpret the proposed uncertainty margins, especially for extreme cases of very good or poor segmentation.  \n6. **Overfitting and Validation Strategy:** The suggestion to use cross-validated logits instead of potentially overfitted ones should be considered to ensure unbiased margin derivation.  \n7. **Generalizability of Approach:** Extending the experiments to additional standard datasets could strengthen the claims, e.g., validating whether a nominal 90% certainty margin corresponds to desired coverage across different tasks.  \n\n**Minor Comments**  \n- Remove references to ‚Äútumors,‚Äù as polyps are not tumors.  \n- Clarify whether sequential frames or repeated samples occur in the dataset.  \n- Ensure consistent terminology and fix minor typographical errors.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is conceptually relevant and formalizes a practical uncertainty estimation framework. However, innovation is tempered by heuristic design choices and limited empirical validation. The experimental evaluation is too narrow to confirm general significance, and reproducibility would depend on explicit disclosure of training/testing splits, base model performance, and threshold selection.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors propose a conformal prediction method that computes confidence sets with spatial uncertainty guarantees in image segmentation from any machine learning model. They illustrate the usefulness of the proposed method on medical images.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper is well-written and clear, although it took a second read-through to fully understand. The proposed method seems to work very well, and the presented experiments are convincing.\n\n### Weaknesses\n\nI am missing more quantitative results. For instance, aggregated coverage scores (e.g., mean; or other metrics, e.g., evaluate Equations 1 and 2) for the different versions on more than one dataset. This comparison should then also include some existing methods, to illustrate the relative strengths of different methods.\n\nAs just mentioned, for the results to be more convincing, I would also like to see examples on more than just one dataset.\n\nAlso, there must be other score transformation functions that could also be evaluated. Testing a couple more could strengthen the results and make it more convincing.\n\n### Questions\n\n- Couldn't a related/similar smooth distance be defined using kernels?\n- What is called \"original scores\", is this when you use the identity score transformation?\n- What are the dashed lines in Figures 4 and 5?\n\nMajor comments:\n- Add labels and/or legends to the rows and columns of the figures.\n\nMinor comments:\n- The word \"polyp\" is misspelled in different ways in almost every instance. Do check this.\n- It says \"... the set a side [num] images ...\", or something similar, a few times. Check the grammar there.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a conformal prediction method designed to compute confidence sets with spatial uncertainty guarantees specifically for image segmentation using any machine learning model. The authors illustrate its applicability through experiments on medical images, aiming to contribute to the field of image analysis under uncertainty.\n\n**Major Comments:**  \n1. The manuscript lacks sufficient quantitative results. It would benefit from aggregated coverage scores (e.g., mean or other metrics) for the different versions tested across multiple datasets. \n2. The comparison should also include existing methods to highlight the strengths and limitations of the proposed approach.\n3. Convincing results could be enhanced by providing examples from more than one dataset.\n4. The evaluation of additional score transformation functions is recommended, as this could further strengthen the results.\n\n**Minor Comments:**  \n- Figures should include labels and/or legends for both rows and columns to enhance clarity.\n- The word \"polyp\" is misspelled in various ways throughout the manuscript; please verify consistency.\n- There are grammatical issues present in the phrase ‚Äú... the set a side [num] images ...,‚Äù found in multiple instances; a revision is necessary.\n\n**Summary Paragraph:**  \nThis study presents a new conformal prediction method that demonstrates potential significance in the field of image segmentation. While the innovation is noteworthy, the evaluation lacks depth due to insufficient quantitative data and comparative analysis against existing methodologies. The reproducibility of results is questionable, given the reliance on a single dataset for illustrative examples. Strengths include clear writing and a well-structured presentation, yet further data and comparative results are needed to enhance the overall robustness of the findings.\n\n**Decision Recommendation:**  \nMajor revision is recommended to address the outlined weaknesses and provide a more comprehensive assessment of the proposed method.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a conformal prediction framework for image segmentation that generates spatially grounded confidence sets applicable to outputs from any machine learning model. The authors demonstrate the method‚Äôs utility on medical imaging data. Overall, the paper is clearly written and the methodology appears sound, although some aspects required rereading for full comprehension. The reported experiments indicate promising performance.\n\n**Major Comments**  \n1. **Quantitative evaluation:** The results section lacks comprehensive quantitative analysis. It would strengthen the work to report aggregated coverage scores (e.g., mean values or other relevant metrics based on Equations‚ÄØ1 and‚ÄØ2) across multiple versions and datasets.  \n2. **Comparative analysis:** The study would be more convincing if it included results on more than one dataset and compared performance with existing methods to contextualize relative strengths.  \n3. **Score transformation evaluation:** Additional experiments exploring alternative score transformation functions are recommended, as other viable options may yield different or improved outcomes.  \n4. **Figure labeling:** Figures should include clear labels or legends for rows and columns to enhance readability and interpretation.  \n\n**Minor Comments**  \n- The word *polyp* is inconsistently spelled in several places and should be standardized.  \n- The phrase ‚Äúset a side [num] images‚Äù appears multiple times with incorrect grammar and should be corrected.  \n- Clarify in the manuscript: (a) whether ‚Äúoriginal scores‚Äù refers to using the identity score transformation, (b) the meaning of dashed lines in Figures‚ÄØ4 and‚ÄØ5, and (c) whether a related smooth distance could be defined using kernel functions.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important problem by introducing spatially aware uncertainty quantification for segmentation models, a topic of clear significance. The approach offers innovative use of conformal prediction in this context. However, the evaluation remains limited in breadth and comparison, which reduces the strength of the evidence supporting its claims. The method appears reproducible, but additional datasets and clarified experimental details would improve confidence in the results.\n\n**Decision Recommendation**  \n**Major revision.** The concept is promising and the paper is generally well written, but stronger quantitative validation and broader experimental comparison are needed to substantiate the contribution.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The paper is clearly methodological rather than application‚Äëoriented. It develops a new mathematical and statistical framework for constructing spatial **conformal confidence sets** for image segmentation outputs, with theoretical coverage guarantees. This relates directly to quantitative imaging analysis and statistical image inference, fitting well within the IEEE‚ÄØTMI scope.\n\n2. **Novelty & Contribution Level**  \nThe central novelty is the *adaptation of conformal inference* to generate *spatially aware uncertainty regions* for segmentation masks, including:\n- Use of image‚Äëlevel exchangeability to obtain rigorous finite‚Äësample guarantees.\n- Derivation of inner and outer confidence sets with probabilistic coverage of the full segmentation mask, not just pixelwise or FDR‚Äëcontrolled uncertainty.\n- Introduction of *learned score transformations* (notably the distance transformation) to improve the geometric tightness of the sets.\n- Theoretical results (Theorems‚ÄØ2.1‚Äì2.8) linking coverage control to Hausdorff distance and bounding‚Äëbox formulations.  \nThis is a meaningful methodological advance beyond prior ‚Äúpixelwise conformal‚Äù or ‚Äúrisk‚Äëcontrolled‚Äù approaches.  \n\n3. **Technical & Experimental Rigor**  \nTheory is formally developed with proofs, assumptions stated, and multiple validation studies on independent datasets (polyps, brain, teeth). Experiments demonstrate both coverage control at nominal levels and comparative efficiency of score transformations.  \nPotential issues: limited discussion of computational complexity, sensitivity to segmenter quality, and reliance on reasonably large calibration datasets. Reproducibility seems supported through code availability.\n\n4. **Clarity & Presentation**  \nThe manuscript is generally clear and well‚Äëstructured, though mathematically dense. Figures are numerous and informative, though some legends and notations could be streamlined. Minor English style and formatting corrections would improve readability.\n\n5. **Ethical & Reproducibility Compliance**  \nAll datasets are publicly available and anonymized. The text explicitly mentions compliance and provides code for replication. No ethical concerns detected.\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThe authors propose a general framework for constructing conformal confidence sets that provide spatially resolved uncertainty guarantees for biomedical image segmentation. Instead of pixel‚Äëlevel conformal p‚Äëvalues, the method calibrates thresholds based on maxima of transformed logit scores within and outside ground‚Äëtruth masks. Distance‚Äëbased and other transformations can be learned on an auxiliary dataset to produce tight outer and inner bounds. Theoretical coverage results are proven under exchangeability. Experiments on colon‚Äëpolyp, brain‚Äëmask, and dental CT segmentations validate finite‚Äësample coverage and show practical utility for uncertainty quantification.\n\n**2. Strengths**\n- Strong theoretical grounding with well‚Äëdefined probabilistic guarantees.  \n- Novel extension of conformal inference to spatial segmentation masks.  \n- Demonstrated generality across different imaging modalities and neural‚Äënetwork backbones.  \n- Practical insights on transformation learning (e.g., distance transform) that markedly improve confidence‚Äëset precision.  \n- Clear link to clinical uncertainty needs and easy computational implementation.\n\n**3. Weaknesses**\n- Some derivations assume ideal exchangeability and independence between segmenter and calibration data; discussion of violations (e.g., domain shift) is brief.  \n- Evaluation focuses mainly on qualitative visualizations and coverage‚Äërate plots; limited quantitative metrics on set tightness versus segmentation accuracy.  \n- The method‚Äôs scalability to 3‚ÄëD volumetric data is not fully evaluated.  \n- Writing densely mathematical; minor editing needed for accessibility to broader TMI readership.\n\n**4. Major Comments**\n1. **Exchangeability and Independence** ‚Äì elaborate on how violations (e.g., correlated calibration/test subjects, retraining same network) might affect validity, and potential remedies (weighted conformal, domain adaptation).  \n2. **3‚ÄëD and Computational Cost** ‚Äì extend discussion/experiments to full volumetric data efficiency to demonstrate feasibility for standard medical‚Äëimage sizes.  \n3. **Quantitative Evaluation of Efficiency** ‚Äì include standardized metrics (Dice of inner/outer bounds, average Hausdorff width) to better quantify practical informativeness.  \n4. **Transformation Learning Strategy** ‚Äì clarify selection criteria, hyperparameters, and avoid any circularity when limited data are available.  \n5. **Potential Clinical Integration** ‚Äì briefly discuss how these spatial uncertainty bounds could be used in downstream decision‚Äëmaking or uncertainty visualization interfaces.\n\n**5. Minor Comments**\n- Correct small typographical and spacing errors (e.g., ‚Äúpolyps colonscopy,‚Äù ‚Äúacheived‚Äù).  \n- Improve figure captions to be self‚Äëcontained.  \n- Consolidate repetitive references to sections/appendices for conciseness.  \n- Ensure consistent notation for Œ±‚ÇÅ,‚ÄØŒ±‚ÇÇ across figures and text.\n\n**6. Overall Recommendation**  \n**Major Revision**  \nThe manuscript offers significant theoretical and methodological innovation of clear relevance to TMI, but a stronger empirical section, clearer exposition of assumptions, and expanded discussion of computational/clinical aspects would materially strengthen it before publication.\n\n**7. Confidence Level:** 5 (high confidence ‚Äî topic and methods align with reviewer expertise).",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper develops conformal confidence sets for biomedical image segmentation that provide spatial uncertainty guarantees with probabilistic coverage. The authors adapt split-conformal inference by computing thresholds on calibration data based on the distribution of maximum transformed logit scores within and outside ground truth masks. The method constructs inner and outer confidence sets guaranteed to contain the true segmented mask with desired probability under exchangeability assumptions (Assumption 1, Section 2.1). Key theoretical contributions include marginal coverage results (Theorems 2.1-2.2), joint coverage guarantees (Theorem 2.6), and analysis of distance transformations showing improved performance when segmentation models are accurate (Theorem 2.8). The approach is validated on three biomedical datasets: polyp colonoscopy (Section 3), brain imaging (Section 4), and teeth segmentation (Section 5), demonstrating that distance transformations of predicted masks provide tighter outer confidence sets compared to raw logit scores.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical notation inconsistencies and clarity issues**\n  - The notation switches between MÃÇ(X) and MÀÜ(X) for predicted masks without explanation (pages 2-3)\n  - Definition of sign(A,v) in distance transformation (page 5, line 252) could be clearer about boundary cases\n  - The relationship between score function s and predicted mask MÃÇ(X) is stated as assumed but never formally defined (page 2, lines 108-109)\n\n‚Ä¢ **Limited theoretical analysis of score transformation optimality**\n  - No theoretical guidance provided for selecting optimal transformations beyond distance transforms (Section 2.4)\n  - Theorem 2.8 only covers distance transformations but applications use other methods like smoothing (Section 5, teeth segmentation)\n  - The learning dataset approach lacks theoretical justification for when including training data is appropriate (pages 4-5, lines 216-228)\n\n‚Ä¢ **Experimental validation limitations**\n  - Small dataset sizes limit generalizability, particularly brain imaging (524 subjects, Section 4) and teeth (598 subjects, Section 5)\n  - No comparison with existing conformal methods for image segmentation beyond brief mention of Angelopoulos et al. (2024) approach\n  - Validation procedure uses only 1000 random splits which may not capture full variability in coverage performance (Figure 4, Section 3.3)\n\n‚Ä¢ **Insufficient analysis of computational complexity and scalability**\n  - Algorithm 1 (page 16) shows computational steps but provides no complexity analysis\n  - Runtime reported only for downscaled polyp data (0.03 seconds) with limited scalability assessment (page 10, lines 537-539)\n  - No discussion of memory requirements for large medical images or 3D volumes\n\n‚Ä¢ **Incomplete treatment of practical implementation challenges**\n  - Limited guidance on hyperparameter selection (e.g., FWHM values for smoothing kernels in teeth application)\n  - No analysis of sensitivity to calibration set size or composition\n  - Assumption 2 (independence of scores) may be violated when learning dataset overlaps with training data but this is not addressed theoretically\n\n## Suggestions for Improvement\n\n‚Ä¢ **Standardize mathematical notation and improve clarity**\n  - Use consistent notation throughout (either MÃÇ(X) or MÀÜ(X)) and define all symbols upon first use\n  - Provide explicit definition of sign(A,v) with examples for boundary cases\n  - Formally define the relationship between score function s(X) and predicted mask MÃÇ(X) in the setup section\n\n‚Ä¢ **Develop theoretical framework for transformation selection**\n  - Provide theoretical analysis or guidelines for selecting optimal score transformations beyond distance transforms\n  - Extend theoretical results to cover smoothing and other transformations used in applications\n  - Establish formal conditions for when training data can be safely included in learning datasets without violating independence\n\n‚Ä¢ **Strengthen experimental evaluation**\n  - Include larger datasets or multi-center validation to improve generalizability claims\n  - Add quantitative comparisons with existing conformal image segmentation methods (e.g., Mossina et al. 2024, Bates et al. 2021)\n  - Increase validation runs beyond 1000 or provide statistical analysis of coverage estimate uncertainty\n\n‚Ä¢ **Add comprehensive computational analysis**\n  - Provide theoretical or empirical complexity analysis for all algorithmic components\n  - Include scalability experiments on larger images and 3D volumes with memory usage analysis\n  - Compare computational efficiency with alternative conformal approaches\n\n‚Ä¢ **Enhance practical implementation guidance**\n  - Develop systematic approaches for hyperparameter selection with validation on learning datasets\n  - Analyze sensitivity to calibration set characteristics (size, distribution, quality)\n  - Address potential violations of Assumption 2 when learning and training data overlap, possibly through empirical validation or relaxed theoretical conditions",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces a methodology for constructing conformal confidence sets for the output of black-box image segmentation models, ensuring spatial uncertainty guarantees. The authors adapt conformal inference techniques to the imaging context by leveraging score transformations, particularly distance transformations, to generate tight inner and outer confidence sets for the segmented masks. The work is illustrated and validated on datasets from polyps colonoscopy, brain imaging, and teeth segmentation, demonstrating the utility of the proposed approach in providing meaningful spatial uncertainty bounds.\n\n## Major Comments\n1. Novelty and Positioning: While the manuscript presents a novel approach to constructing conformal confidence sets for image segmentation, it is essential to clarify how this work distinguishes itself from existing literature on conformal inference and risk control. The authors should explicitly discuss the contributions of recent works on conformal prediction and risk control (e.g., Bates et al., 2021; Angelopoulos et al., 2021) and how their proposed method offers improvements or alternatives.\n\n2. Evaluation Design: The evaluation is conducted on three specific datasets: polyps colonoscopy, brain imaging, and teeth segmentation. Although these datasets provide a diverse set of applications, the manuscript should consider broader validation across different types of imaging data to strengthen the generalizability claim. Additionally, the manuscript should explore the performance of the method under varying conditions, such as different levels of noise or varying image resolutions.\n\n3. Comparisons: The manuscript compares the proposed method with baseline approaches using logit scores and bounding box transformations. However, it would be beneficial to include comparisons with more advanced baselines, such as other score transformations or conformal inference methods specifically designed for image segmentation. This would help to establish the relative performance and advantages of the proposed approach.\n\n4. Reproducibility: The manuscript states that code will be released, but it lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters used for the image segmentation models and the conformal inference procedures. Providing a more comprehensive description of these aspects is crucial for ensuring the reproducibility of the results.\n\n## Minor Comments\n1. Figures: Some figures are cluttered and could benefit from a clearer presentation. For example, Figure 2 could be improved by showing fewer representative slices with zoomed-in regions to highlight the differences between the score transformations more effectively.\n\n2. Notation: The notation used in Section 2.1 is inconsistent, particularly regarding the forward operator. Clarifying and standardizing this notation would improve the readability of the manuscript.\n\n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition, which can be confusing for readers. Providing a list of acronyms or defining them upon first use would enhance clarity.\n\n4. Typographical Errors: There are typographical issues throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). Carefully proofreading the document would eliminate these errors.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in biomedical imaging: providing reliable uncertainty quantification for image segmentation models. The proposed method of adapting conformal inference through score transformations, especially distance transformations, offers a promising approach to generating meaningful spatial uncertainty bounds. The theoretical foundations are sound, and the empirical validation on multiple datasets demonstrates the practical utility of the method. However, the evaluation could be strengthened by broader validation across different imaging modalities and varying conditions. The reproducibility of the approach is compromised by the lack of detailed methodological descriptions, which should be addressed to facilitate independent replication. Overall, the manuscript has substantial merit and meets many of the standards for publication in TMI, but it requires further refinement in terms of comparative analysis and reproducibility.\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis to include more advanced baselines, strengthen validation across different imaging datasets and conditions, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Samuel Davenport"
    ],
    "url": "pdfs/iclr.cc-2025-conference_1bdddb81fd08be607c052dee4379dfe48532b252.pdf",
    "remote_url": "https://openreview.net/pdf/1bdddb81fd08be607c052dee4379dfe48532b252.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Is classification all you need for radiology report generation?",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "other topics in machine learning (i.e., none of the above)"
    ],
    "keywords": [
      "radiology report generation",
      "large language model",
      "multi-modalities"
    ],
    "abstract": "Automatic radiology report generation is an advanced medical assistive technology capable of producing coherent reports based on medical images, akin to a radiologist. However, current generative methods exhibit a notable gap in clinical metrics when compared to medical image classification. Recently, leveraging diagnostic results to improve report quality has emerged as a promising approach. We are curious whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. To investigate this question, this study designs an evaluation framework that integrates long-tail scenarios and summarizes potential combinations of LLM-based report generation models. We assess the impact of classification on report quality across four benchmarks. Initially, we introduce LLM-based language and clinical metrics and develop a pipeline to evaluate the model's performance on both in-domain and out-of-distribution (OOD) long-tail scenarios. Subsequently, we conduct a systematic evaluation of all potential model combinations. Our findings reveal that: 1) the impact of classification on report quality is positively correlated with the performance of classifiers, but the gap still exists, and 2) while classification can enhance report quality in in-domain long-tail scenarios, its benefits for OOD scenarios are limited.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nCurrent RRG systems exhibit a notable gap in clinical metrics when compared to classification models. The authors investigate whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. Key findings show that classification helps improve report quality in in-distribution settings but exhibits limited benefits in OOD scenarios.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper evaluates whether classification models can help improve the quality of radiology reports. This is a novel and interesting insight, with potential for aiding in real-world clinical workflows.\n2. The authors perform extensive evaluations across different strategies for incorporating classification information as well as in-domain vs. OOD settings. \n3. The authors also introduce novel LLM-based metrics for assessing the quality of radiology reports, which evaluate how well reports capture long-tail observations. This metric is an interesting contribution and has potential for aiding future works in the domain of radiology report generation.\n\n### Weaknesses\n\nThe key weakness of this paper is inadequate evaluations, as discussed below.\n\n1. **Inadequate evaluations of classifier:** The incorporation of the classifier is inadequately evaluated, making it difficult to understand the settings in which incorporating a classifier is useful, as detailed below. The paper could have benefitted from more nuanced insights on when including the classifier helps vs. detracts from the quality of generated reports.\n\n    a. **Classifier Performance:** Although the incorporation of the classifier is the key contribution of this paper, the authors do not provide any evaluations on the quality of the actual classifier. What conditions does the classifier perform well on and which conditions does the classifier perform worse on? Does this correlate with report generation performance on the subset of images with particular conditions when the classifier is included? Does improving the performance of the classifier improve report generation quality? In OOD settings, the evaluated classifier displays poor performance  (12.0-14.0 F1 points), and as a result, it is not surprising that incorporating the classifier will not result in benefits to the RRG model in OOD settings; does this result change if the classifier is instead trained on the PadChest or IU X-ray datasets? All of these questions are critical for understanding when/how classification helps in RRG, but none of these are evaluated.\n\n    b. **Upper Bound:** It would be useful to establish an upper bound on performance of the report generation model by utilizing an ‚Äúoracle‚Äù classifier (i.e. a hypothetical classifier that predicts every condition correctly). This would establish whether classification, in the optimal setting, makes useful contributions to the report generation task. As of now, the evaluations are limited by the poor performance of the evaluated classifier, and the key findings/takeaways from the paper are based on this specific classifier.\n\n2. **Inadequate evaluations of proposed metrics:** Although several new metrics are presented as key contributions of this paper, the quality of these metrics is not evaluated. How accurate is the OpenAI GPT-4o model at extracting disease categories? Did the authors check for false negatives / positives in this extraction procedure? Does this metric align with ground-truth labels for the conditions where labels are provided? Evaluating the quality of the metrics via a dataset like ReXVal would have been useful [1].\n\n[1] https://physionet.org/content/rexval-dataset/1.0.0/\n\n### Questions\n\nMy questions are detailed above in the weaknesses section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript investigates the gap in clinical metrics between current Radiology Report Generation (RRG) systems and classification models. The authors explore whether including a classifier for long-tailed and rare diseases can enhance report robustness. Key findings indicate that while classification improves report quality in in-distribution settings, its benefits are limited in out-of-distribution (OOD) scenarios. The manuscript presents a compelling inquiry into the potential integration of classification in clinical workflows.\n\n### Major Comments\n1. **Inadequate Evaluations of Classifier**: The paper lacks thorough evaluation of the classifier's performance, limiting the understanding of its utility in improving report quality. Detailed insights on when classifier inclusion aids or hinders report generation are missing.\n   - **Classifier Performance**: No evaluations on the classifier's effectiveness are provided. Specific conditions for optimal classifier performance, correlation with report generation quality, and potential performance variations based on dataset choice (PadChest or IU X-ray) remain unexamined.\n   - **Upper Bound**: Establishing an upper performance limit using an \"oracle\" classifier could clarify the potential contributions of classification in report generation. The findings are hindered by low performance results from the assessed classifier.\n\n2. **Inadequate Evaluations of Proposed Metrics**: The manuscript introduces several new metrics without assessing their quality. Key concerns include the accuracy of the OpenAI GPT-4o model in disease category extraction, as well as the presence of false positives/negatives. A quality evaluation against a dataset like ReXVal would have been valuable.\n\n### Minor Comments\n- Consider improving clarity in the distinction between in-domain and OOD results.\n- There are minor grammatical errors and inconsistencies in terminology that should be addressed for readability.\n- Figures should be better labeled to enhance understanding.\n\n### Summary Paragraph\nIn summary, the manuscript demonstrates significance through its evaluation of classification models in the context of RRG, showing innovative ideas in the introduction of LLM-based metrics. However, the evaluations of both the classifier and the proposed metrics lack depth, which affects the overall rigor and reproducibility of the findings. While the potential for impact on clinical workflows is evident, clearer demonstrations of performance and robustness are necessary.\n\n### Decision Recommendation\nMajor revision is recommended to address the outlined weaknesses and enhance the overall quality of the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether incorporating a classification model that accounts for all possible long‚Äëtailed and rare disease categories can improve the robustness and clinical utility of radiology report generation (RRG) systems. The study aims to address the existing performance gap between RRG systems and classification models in standardized clinical metrics. Overall, the work is clearly written, explores an interesting direction for improving clinical report generation, and introduces novel evaluation metrics based on large language models (LLMs). However, the empirical support for the central claims is limited, primarily due to insufficient evaluation of the classifier and the proposed metrics.\n\n---\n\n**Major Comments**  \n\n1. **Evaluation of Classifier Integration**  \n   The analysis of how and when incorporating the classifier benefits report generation is insufficient. The paper does not clarify the contexts in which classifier inclusion improves or degrades performance. A more nuanced evaluation would strengthen the conclusions.  \n\n   a. **Classifier Performance Assessment:** The manuscript lacks direct evaluation of the classifier‚Äôs quality. Key questions remain unanswered: under which conditions does the classifier perform well or poorly? How does this relate to report generation outcomes? In the out‚Äëof‚Äëdistribution (OOD) setting, the classifier performs poorly (12‚Äì14 F1 points), which likely explains the lack of improvement when incorporated into the RRG model. It would be valuable to test alternative training datasets (e.g., PadChest or IU X‚ÄëRay) to see if this behavior changes.  \n\n   b. **Upper‚ÄëBound Comparison:** The study should estimate an upper bound using an oracle classifier (perfect predictions) to show whether, in principle, an accurate classifier could meaningfully enhance RRG. As it stands, the conclusions depend solely on a specific, low‚Äëperforming classifier.\n\n2. **Evaluation of Proposed Metrics**  \n   Although the LLM‚Äëbased metrics are an interesting contribution, their validity and reliability are untested. The manuscript does not assess how accurately GPT‚Äë4o extracts disease categories or whether false positives/negatives occur. Comparison against datasets with verified labels (e.g., ReXVal) would help establish metric fidelity and interpretability.\n\n---\n\n**Minor Comments**  \n- Clarify acronyms (e.g., RRG, OOD) upon first use.  \n- Provide brief methodological details for the metric extraction process to support reproducibility.  \n- Include quantitative validation or inter‚Äëmetric correlation for the newly proposed metrics.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** Addresses a meaningful problem in clinical AI‚Äîbridging classification and report generation.  \n**Innovation:** Moderately innovative in exploring classifier‚Äëinformed RRG and proposing LLM‚Äëbased metrics.  \n**Evaluation:** The empirical evidence is not yet robust; key analyses of classifier quality and metric validity are missing.  \n**Reproducibility:** Limited by absent details on classifier performance analysis and missing code/data validation checks.  \nOverall, the concept is promising, but the evaluation requires substantial strengthening.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper explores the use of classification models in enhancing radiology report generation, specifically examining if a classifier encompassing all possible radiological findings can improve the robustness of generated reports. The study finds that while classification models improve report quality in in-domain scenarios, their impact is limited in out-of-distribution (OOD) settings, especially for long-tail disease categories. The authors propose a comprehensive evaluation framework with novel LLM-based metrics and conduct experiments across multiple benchmarks, concluding that classification aids in in-domain accuracy but may introduce errors in OOD cases due to model biases.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe study tackles a critical challenge in medical AI‚Äîhow to generate reliable and clinically relevant radiology reports that generalize well across diverse clinical scenarios. By showing that classification models can enhance in-domain performance while introducing potential errors in OOD settings, this paper offers valuable insights into the limitations and trade-offs of using classification-aided generation models. The paper is well-structured, with each section building logically upon the previous one. The problem formulation, methodology, and experimental setup are clearly articulated, enhancing accessibility for readers. The experimental design is thorough, incorporating multiple benchmarks and a diverse set of evaluation metrics, including novel LLM-based metrics, to assess model performance across both in-domain and OOD settings. The rigorous use of benchmarks and detailed comparisons with existing methods underscore the study's robustness and the reliability of its findings.\n\n### Weaknesses\n\nThe primary weakness of this work lies in its motivation, or key argument, which has already been discussed in a prior publication that is not referenced in this submission. The previous study, Medical Report Generation Is A Multi-label Classification Problem (Fan et al., IEEE MedAI 2024), introduces a similar concept: that classification accuracy significantly impacts the quality of medical report generation. Both works share the same foundational idea, though they differ in specific model designs and classification categories. From a performance perspective, the previous work, which uses a cross-attention approach, demonstrates better outcomes and introduces the concept of ceiling performance. This concept suggests that the theoretical upper bound for this approach would be achieved if all ground-truth labels were provided to the model, given that real-world classifiers cannot reach 100% accuracy. \n\nWhile this submission‚Äôs strength lies in its extensive experiments, broader metrics, and a variety of models, its core insights are similar to the prior work, without addressing ceiling performance or providing a comparison to it. Additionally, the reason why the 28 classification categories used in the previous study outperform disease-based categories is not examined here. These points, along with further experimental and discussion-based additions, would significantly enhance the paper.\n\n### Questions\n\nplease read the weakness part.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript investigates the role of classification models in enhancing the generation of radiology reports, focusing on whether a classifier that includes all potential radiological findings can improve report robustness. The authors find that while classification models generally improve report quality in in-domain scenarios, their effectiveness diminishes in out-of-distribution (OOD) settings, particularly for long-tail disease categories. They propose a comprehensive evaluation framework utilizing novel LLM-based metrics, conducting experiments across various benchmarks. Ultimately, the study concludes that classification aids in in-domain accuracy but may introduce errors in OOD contexts.\n\n### Major Comments\n1. **Novelty**: The work's primary argument closely mirrors a previous study (Fan et al., IEEE MedAI 2024) without sufficient acknowledgment.\n2. **Evaluation Design**: Although the paper provides extensive experiments and multiple evaluation metrics, it fails to discuss ceiling performance, which could offer deeper insights into classification accuracy's impact.\n3. **Comparisons**: The submission does not compare its findings with the previous study's results, which could strengthen its argument regarding the chosen classification categories.\n4. **Reproducibility**: While thorough in its methodology, more details about replicating experimental conditions would enhance reproducibility.\n\n### Minor Comments\n- The motivation section would benefit from additional context regarding previous literature.\n- Consider incorporating a direct comparison of the classification categories with disease-based categories.\n- Minor formatting issues were noted in Section 3; please ensure consistency across all sections.\n\n### Summary Paragraph\nIn terms of **significance**, this paper addresses a pressing concern in medical AI by exploring how to achieve reliable radiology report generation across varied clinical scenarios. Regarding **innovation**, it attempts to enhance classification model performance, though its contributions are largely derivative. For **evaluation**, the thoroughness of experimental design is commendable, yet the lack of comparisons to previous findings diminishes the results' impact. Finally, in terms of **reproducibility**, further clarity on experimental replication would be beneficial. Overall, while the study presents valuable findings regarding in-domain classifications, it falls short of providing novel insights or addressing critical previous work.\n\n### Decision Recommendation\nMajor revision is recommended to address the identified weaknesses, enhance comparisons with prior studies, and clarify the contributions to the field.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the role of classification models in improving automatic radiology report generation. Specifically, it evaluates whether incorporating classifiers that predict a comprehensive set of radiological findings enhances the robustness and accuracy of generated reports. The study reports that classification-aided models improve report quality within in-domain data but perform less effectively in out‚Äëof‚Äëdistribution (OOD) scenarios, particularly for rare disease categories. The paper introduces a novel evaluation framework with large language model (LLM)-based metrics and conducts experiments across several benchmarks. Overall, the manuscript is clearly organized, methodologically sound, and offers a detailed empirical analysis.\n\n**Major Comments**  \n1. **Overlap with Prior Work:** The main conceptual contribution‚Äîframing medical report generation as a classification-based problem‚Äîhas been presented previously (Fan et‚ÄØal., IEEE‚ÄØMedAI‚ÄØ2024). The current study does not cite or contrast itself with that work, despite addressing a similar premise.  \n2. **Novelty and Positioning:** Because the earlier publication already demonstrated the relationship between classification accuracy and report quality, the novelty here lies primarily in the empirical scope rather than conceptual innovation. The authors should explicitly acknowledge and differentiate their approach, clearly outlining advances beyond prior findings.  \n3. **Missing Comparison to Ceiling Performance:** The related work introduced the notion of ceiling performance (upper bound given perfect classification). This submission does not adopt or analyze that concept, leaving unclear how the proposed method positions itself relative to theoretical limits.  \n4. **Category Design and Analysis:** The paper uses disease-based classification categories but does not investigate why prior label‚Äëset designs with 28 classification categories achieved better performance. An empirical or discussion‚Äëlevel analysis of this discrepancy would strengthen interpretability.\n\n**Minor Comments**  \n- The manuscript presentation is clear, but referencing prior related work is incomplete.  \n- Terminology around ‚Äúin‚Äëdomain‚Äù and ‚ÄúOOD‚Äù could be more precisely defined.  \n- Ensure all figures and metrics are described consistently across sections.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant in highlighting the trade‚Äëoffs between classification‚Äëaided accuracy and model generalization in radiology report generation. Its innovation lies in the comprehensive evaluation and introduction of LLM‚Äëbased metrics, though the conceptual basis closely parallels prior research. The evaluation is rigorous and reproducible, drawing on multiple benchmarks and transparent comparisons. However, the limited novelty and absence of direct comparisons to prior established frameworks reduce its overall impact.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial clarification of novelty, inclusion of comparisons to related work, and deeper analysis of category selection and theoretical performance bounds are required before the paper can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper studies the question: will a classifier enhance LLM-based report generation models' performance in long-tail OOD scenarios? The authors explore different architectures on the combination of vision encoders, classifiers, and LLM, and perform lots of experiments. They get the findings that classification can enhance report quality in in-domain long-tail scenarios but is bounded by the performance of the classifier.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n1. Many research focuses on  LLM-based report generation models, which is an important task for the medical AI domain, the question this paper investigated may provide some useful information for future research.  \n2. The research comprehensively evaluates different architectural combinations (vision encoders, classifiers, and LLMs) across multiple diverse datasets (MIMIC-CXR, CXR-LT, PadChest, and IU X-Ray).\n\n### Weaknesses\n\n1. The comparison between the four baseline models lacks analysis. The metrics give inconsistent results, how could the author get the results that C+V+LLM is better than V+LLM, and the comparison between Refining and C+V+LLM, giving statistical significance analysis of the results would be helpful.\n\n2. The poor performance of the 'Expanding' approach may be due to the prompt rather than inherent limitations.  I do not find details about the prompts used (apologize if I overlooked them). The 'Expanding' prompt should include standard medical report templates to prove a fair comparison.\n\n3. The classifier only provides binary disease presence information without crucial details like location and severity. This limitation makes hallucinations in LLM-generated reports inevitable when using classification-only inputs. Instead of comparing with these inherently limited training-free approaches, the paper would be more valuable if it explored different variants of end-to-end Vision + LLM full training. \n\n4. The paper's title 'Is classification all you need for radiology report generation?' raises a clear question, but fails to provide the answer. While the findings show that classification helps with in-domain cases but struggles with OOD scenarios, the paper leaves crucial questions unanswered: Is better classification the solution? Or should we focus on pursuing end-to-end training approaches? If the answer is ' we need better classification models', then everyone knows it.\n\n### Questions\n\n1.  Have you optimized the Expanding and Refining prompt? we can't tell whether the performance difference is due to this method not being good enough or the prompt being the problem.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript investigates whether classifiers can improve the performance of LLM-based report generation models in long-tail out-of-distribution (OOD) scenarios. The authors examine various architectures combining vision encoders, classifiers, and LLMs through extensive experimentation. They conclude that classification can enhance report quality in in-domain long-tail situations, although this improvement is limited by the classifier's performance.\n\n**Major Comments:**\n1. The comparison between the four baseline models lacks sufficient analysis, as the metrics yield inconsistent results. Clarifications on how the authors determined that C+V+LLM outperforms V+LLM, as well as a statistical significance analysis between Refining and C+V+LLM, would strengthen the findings.\n2. The poor performance of the 'Expanding' approach may stem from the prompt rather than its inherent limitations. The manuscript does not provide details on the prompts used; including standard medical report templates in the 'Expanding' prompt would enable a fair standard for comparison.\n3. The classifier's limitation to binary disease presence information fails to incorporate essential aspects like location and severity. This restriction may lead to hallucinations in LLM-generated reports. Exploring end-to-end Vision + LLM models with full training would add significant value compared to the classification-only inputs assessed in the paper.\n4. While the manuscript's title, \"Is classification all you need for radiology report generation?\" poses a relevant question, it does not provide a definitive answer. Although the findings indicate the utility of classification in in-domain cases, they do not adequately address whether improved classification is the solution or if end-to-end training should be prioritized.\n\n**Minor Comments:**\n- Clarify the optimization status of the Expanding and Refining prompts, as it is unclear whether observed performance differences result from the method's efficacy or prompt design.\n\n**Summary Paragraph:**\nIn summarizing the review based on the TMI editorial criteria, the manuscript is significant due to its exploration of LLM-based report generation in medical AI, and it presents innovative combinations of architectures. However, evaluation weaknesses arise from inadequate comparisons and lack of comprehensive analysis, particularly around prompts and classifier limitations. Reproducibility may also be impacted due to unclear details about methodology. The overall contribution is noteworthy, though it leaves several critical questions unresolved.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether integrating a classifier can enhance large language model (LLM)-based radiology report generation, particularly in long-tail and out-of-distribution (OOD) scenarios. The study systematically examines combinations of vision encoders, classifiers, and LLMs across multiple datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray). The paper is generally sound in concept and has value for ongoing research in medical AI, though clarity and analytical depth are somewhat limited.  \n\n**Major Comments**  \n1. **Comparative Analysis and Statistical Support:** The comparisons among the four baseline models are insufficiently analyzed. Reported metrics produce inconsistent outcomes, yet the authors conclude that C+V+LLM outperforms V+LLM. Statistical significance testing or clearer justification is needed for these claims, including comparisons between the Refining and C+V+LLM approaches.  \n2. **Prompt Design and Fairness of Comparison:** The weak performance of the ‚ÄúExpanding‚Äù approach may be influenced by prompt construction rather than model limitations. Details on the prompts used are missing. Incorporating standard medical report templates could improve fairness and reproducibility.  \n3. **Classifier Information Limitation:** The classifier provides only binary disease presence, omitting key elements such as lesion location or severity. This design risks hallucinations in generated reports. A more informative classifier or a shift toward end-to-end Vision + LLM training variants would strengthen the study.  \n4. **Unresolved Research Question:** The paper‚Äôs title implies a definitive exploration (‚ÄúIs classification all you need for radiology report generation?‚Äù), yet conclusions remain inconclusive. The results show classification helps in-domain but not OOD; however, the manuscript does not sufficiently address whether better classifiers or alternative end-to-end strategies are the path forward.  \n\n**Minor Comments**  \n- Clarify the prompts used for ‚ÄúExpanding‚Äù and ‚ÄúRefining‚Äù to help interpret results.  \n- Improve presentation quality (organization, figure clarity, and conciseness).  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is significant in addressing an important medical AI challenge and provides extensive experiments across multiple datasets. Its innovation lies in systematically examining the role of classification in LLM-based report generation. However, limitations in analysis, prompting detail, and interpretability reduce its overall impact. While the evaluation is broad, reproducibility suffers from missing methodological specifics.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî Strengthen comparative analysis, clarify prompts and classifier design, and refine conclusions to align with the posed research question.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** ‚Äì  \n   ‚úÖ The paper fits the methodological scope of *IEEE Transactions on Medical Imaging*. It introduces new benchmark design, evaluation metrics, and architectural design space analysis for *automatic radiology report generation (ARRG)* ‚Äî a problem at the intersection of multimodal learning and medical imaging. Although it couples vision-language modeling with NLP, the study is anchored in imaging methodology (classifier‚Äìencoder‚ÄìLLM integration, long-tail robustness, and clinically oriented metrics), making it relevant to TMI‚Äôs focus on imaging informatics and quantitative analysis.\n\n2. **Novelty & Contribution Level** ‚Äì  \n   The novelty lies primarily in an **evaluation and analysis framework** rather than a fundamentally new model. Contributions include:  \n   - An **LLM-based evaluation framework** combining traditional and ‚Äúclinical‚Äù metrics with LLM-derived metrics for nuanced assessment.  \n   - A **systematic exploration of model design space** (vision encoder, classifier, LLM) and training paradigms (end-to-end, refining, expanding).  \n   - Empirical findings on the limited benefit of classifiers for out-of-distribution long-tail radiology data.  \n   While conceptually interesting, much of the architecture and methodology (Swin, DINO, Vicuna, Llama-based modules) build on existing components. Hence, the innovation is more **empirical and benchmarking-oriented** than algorithmic.\n\n3. **Technical and Experimental Rigor** ‚Äì  \n   - The experiments are thorough: multiple datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray), carefully separated in-domain vs. OOD testing, and use of robust baselines.  \n   - Metrics and ablation studies are exhaustively detailed, with appropriate reporting of macro/micro F1, BLEU/ROUGE, and LLM-based metrics.  \n   - However, **quantitative uncertainty or statistical significance** testing is not reported, and many analyses rely on heuristic interpretation of observed trends.  \n   - Some aspects (e.g., use of GPT-4-based evaluation) introduce subjectivity and reproducibility concerns, since API-based results can vary.\n\n4. **Clarity and Presentation** ‚Äì  \n   - The manuscript is generally clear and well-structured, though lengthy with some redundancy across sections.  \n   - Figures and tables are informative but dense; Figure 1 and the multi-page tables may require reformatting for journal readability.  \n   - Minor grammar and phrasing issues appear but do not hinder comprehension.  \n   - Equation-level or theoretical formulations are minimal; the work is largely experimental.\n\n5. **Ethical and Reproducibility Compliance** ‚Äì  \n   - The authors explicitly use **public datasets** (MIMIC-CXR, PadChest, IU X-Ray) with proper citations.  \n   - All datasets are de-identified; ethical compliance appears adequate.  \n   - Code availability is not explicitly stated‚Äîreproducibility could be strengthened by releasing model code, prompt templates, and evaluation scripts, especially for LLM-based scoring.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n1. **Summary**  \n   The manuscript investigates whether improved image classification performance translates into higher-quality radiology report generation. The authors propose a comprehensive **benchmark and evaluation framework** that tests multiple combinations of vision encoders, classifiers, and large language models (LLMs). They introduce **two LLM-based evaluation metrics** addressing the shortcomings of traditional language and clinical metrics. Through experiments on four datasets (in-domain and OOD, long-tail distributions), the study concludes that while classification outputs can improve report quality for common in-domain cases, their benefits do not generalize to out-of-distribution long-tail pathologies, sometimes even degrading performance.\n\n2. **Strengths**  \n   - Timely topic addressing the **integration of large language models in medical imaging**.  \n   - Systematic exploration and well-structured benchmark that could serve the community.  \n   - Insightful empirical observation clarifying the limited utility of diagnostic classifiers for OOD generation.  \n   - Introduction of LLM-based metrics that qualitatively enhance the evaluation of generated clinical text.  \n   - Extensive ablation and dataset diversity improve credibility of trends.\n\n3. **Weaknesses**  \n   - Limited *methodological novelty*: the framework largely reuses established architectures and metrics, with limited theoretical advancement.  \n   - Reproducibility depends heavily on proprietary LLM APIs (GPT‚Äë4), reducing accessibility.  \n   - Lacks formal statistical analysis or uncertainty quantification for comparative results.  \n   - Discussion of the *mechanistic reasons* for classification‚Äìgeneration gaps remains descriptive; no modeling insight or mathematical analysis is provided.  \n   - Some contextual framing (e.g., clinical interpretability, alignment with diagnostic reasoning) could be expanded to emphasize imaging science impact.\n\n4. **Major Comments**  \n   1. Clarify the **novelty boundary**: is this primarily an evaluation paper or a modeling contribution? The abstract and title may overstate methodological innovation relative to the actual scope.  \n   2. Strengthen **methodological rationale** for new LLM-based metrics‚Äîdescribe how they are validated against expert judgment or inter-rater consistency.  \n   3. Discuss **statistical significance and variability** in reported results; include confidence intervals or standard deviations where possible.  \n   4. Provide **code and pipeline availability** for LLM-based evaluation; otherwise, the benchmark cannot be adopted or verified.  \n   5. Improve **discussion of clinical relevance**: how might these findings inform the next generation of clinically trustworthy ARRG systems?  \n   6. Consider reducing redundancy in Results/Appendix and enhancing visual clarity (condensed tables, key examples).  \n\n5. **Minor Comments**  \n   - Carefully proofread for minor grammatical inconsistencies (‚Äúaim to,‚Äù ‚Äúwould improve‚Äù) and typographical errors.  \n   - In Figure 1, ensure architectural variants are legible in grayscale print.  \n   - Explicitly define all abbreviations at first use (e.g., ICL, RJ-n).  \n   - Move dataset preprocessing details to supplementary material for length reduction.  \n   - Update the references to the latest *TMI* or *MedIA* works on multimodal LLM evaluation if available.\n\n6. **Overall Recommendation** ‚Äì **Major Revision**  \n   The work is thorough, relevant, and potentially impactful as a community benchmark, but it falls short of TMI‚Äôs threshold for publication-ready novelty and methodological generality in its present form. Clarify conceptual contributions, enhance reproducibility, and solidify empirical rigor to justify acceptance.\n\n7. **Confidence Level** ‚Äì **4 / 5**  \n   Confident in the assessment; findings and claims are well-supported but rely on external LLM-based evaluation that may evolve.\n\n---\n\n**Summary Decision:** *Major Revision ‚Äî potentially valuable benchmark contribution; requires stronger articulation of novelty, reproducible methodology, and quantitative rigor to meet IEEE TMI standards.*",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper investigates whether incorporating classification information can improve the robustness of automatic radiology report generation (ARRG), particularly for long-tail and out-of-distribution scenarios. The authors design an evaluation framework that combines LLM-based report generation models with diagnostic classifiers, proposing four baseline architectures: vision+LLM, classifier+vision+LLM, expanding, and refining approaches (Figure 1). They introduce LLM-based evaluation metrics including LLM-RadJudge and Long-tailed & OOD F1 scores to assess performance beyond conventional language and clinical metrics (Section 2.3.2). Experiments across four datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray) reveal that while classification information improves report quality in in-domain scenarios, it provides limited benefits for out-of-distribution long-tail cases when using powerful foundation models like Llama 3.1 70B and GPT-4 (Tables 2, 4, 5).\n\n## Weaknesses\n\n‚Ä¢ **Insufficient Mathematical Rigor in Model Formulation**\n  - The paper lacks formal mathematical definitions for the core components (vision encoder, classifier, LLM integration) described in Section 2.2, making it difficult to assess technical soundness\n  - No explicit loss functions or optimization objectives are provided for the different training paradigms (end-to-end vs. training-free), which is critical for reproducibility\n  - The connector architecture is only briefly mentioned as \"multi-layer perceptron (MLP) with GELU activations\" (Section 2.1) without mathematical specifications\n\n‚Ä¢ **Limited Experimental Design and Statistical Analysis**\n  - Tables 1-2 show results without confidence intervals, standard deviations, or significance tests, undermining the reliability of claimed improvements\n  - The choice of 100 and 200 long-tail categories for classifier training (Table 2) appears arbitrary with no justification or ablation study\n  - Case studies are limited to individual examples (Figure 3, Figure 5) without systematic analysis across the test sets\n\n‚Ä¢ **Questionable Evaluation Framework Validity**\n  - The LLM-based metrics (LLM-RadJudge, Long-tailed & OOD F1) rely on GPT-4o for ground truth extraction (Section 2.3.2), introducing potential bias and circularity in evaluation\n  - The paper acknowledges that \"varying granularity in observation names across datasets requires semantic transformation\" but doesn't address how this affects metric reliability\n  - No validation of the proposed metrics against radiologist assessments or existing clinical standards\n\n‚Ä¢ **Unclear Methodology and Reproducibility Issues**\n  - The data preprocessing pipeline lacks detail, particularly for the OOD datasets where Spanish reports are translated to English using GPT-4 (Section 2.3.1)\n  - Implementation details are relegated to the appendix with insufficient description of hyperparameter selection and model training procedures\n  - The \"refining\" approach methodology is not clearly explained in the main text, making it difficult to understand how classification information is integrated\n\n‚Ä¢ **Weak Analysis of Core Research Question**\n  - The title question \"Is classification all you need?\" is not systematically addressed through controlled experiments isolating classification effects\n  - The paper conflates multiple variables (model architecture, training paradigm, dataset domain) making it difficult to attribute performance differences to classification integration\n  - Finding 2 states limitations for OOD scenarios but doesn't provide sufficient analysis of why this occurs or potential solutions\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance Mathematical Formulation and Technical Clarity**\n  - Provide formal mathematical definitions for each model component including input/output dimensions and transformation functions\n  - Specify explicit loss functions and optimization objectives for each training paradigm with proper mathematical notation\n  - Include detailed architectural specifications for the connector and classification integration mechanisms\n\n‚Ä¢ **Strengthen Experimental Rigor and Statistical Validation**\n  - Report confidence intervals and conduct statistical significance tests for all performance comparisons across tables\n  - Provide systematic ablation studies justifying the choice of long-tail category numbers and other key hyperparameters\n  - Expand case studies to include quantitative analysis across larger sample sets rather than cherry-picked examples\n\n‚Ä¢ **Validate and Improve Evaluation Framework**\n  - Validate LLM-based metrics against expert radiologist evaluations to establish reliability and remove potential circular reasoning\n  - Address the acknowledged issues with cross-dataset observation granularity through systematic semantic alignment studies\n  - Include traditional clinical validation metrics alongside the proposed LLM-based approaches\n\n‚Ä¢ **Clarify Methodology and Ensure Reproducibility**\n  - Move essential implementation details from appendix to main text and provide comprehensive hyperparameter justification\n  - Describe the data preprocessing pipeline in detail, particularly for cross-lingual translation and its potential impact on results\n  - Provide clear algorithmic descriptions of the \"refining\" approach and other training-free methods\n\n‚Ä¢ **Systematically Address the Core Research Question**\n  - Design controlled experiments that isolate the effect of classification information while holding other variables constant\n  - Provide deeper analysis of why classification helps in-domain but fails for OOD scenarios, potentially through attention visualization or ablation studies\n  - Restructure the paper to more directly answer the title question with clear experimental evidence and theoretical justification",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript investigates the impact of integrating a comprehensive classifier into LLM-based radiology report generation models to improve their robustness, particularly in long-tail and out-of-distribution scenarios. The authors propose a systematic evaluation framework that assesses the model's performance across four benchmarks, using both conventional and LLM-based metrics. Their findings suggest that while classifier integration improves report quality in in-domain scenarios, it may exacerbate errors in long-tail and out-of-distribution settings. The paper is well-written and provides a thorough analysis of the interaction between classification and report generation.\n\n## Major Comments\n1. Scope of Evaluation: The evaluation is extensive, covering multiple datasets and scenarios. However, the reliance on a limited set of long-tail and out-of-distribution datasets might not fully generalize the findings to a wider range of clinical environments. Additional validation on more diverse datasets would strengthen the conclusions.\n\n2. Impact of Classifier in OOD Scenarios: The findings that classifier integration can worsen performance in OOD scenarios highlight a significant limitation. The authors should explore potential reasons for this behavior, such as biases in the training data or limitations of the classifier in handling unseen distributions. Further investigation into mitigating these effects could be valuable.\n\n3. Comparison with State-of-the-Art Methods: While the manuscript compares with several state-of-the-art methods, it lacks a direct comparison with the most recent advancements in the field. Including more recent works would provide a clearer picture of the proposed method's standing.\n\n4. Reproducibility: The manuscript mentions that code will be released, but the details provided about training protocols, preprocessing, and hyperparameters are insufficient. Providing a more comprehensive description of the experimental setup is crucial for reproducibility.\n\n## Minor Comments\n1. Figures: Figures 3 and 5 are cluttered and could benefit from simplification, such as showing fewer representative slices with zoomed-in regions for clarity.\n\n2. Notation and Terminology: The notation introduced in Section 2.1 is inconsistent and could be clarified. Definitions for acronyms such as \"R=4\" should be provided.\n\n3. Typographical Errors: Minor typographical issues, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7), should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in radiology report generation by investigating the impact of classifier integration into LLM-based models. The proposed framework and findings contribute meaningfully to the understanding of how classification can influence report quality. However, the evaluation is somewhat limited by the scope of datasets used, and the observed deterioration in OOD scenarios needs further exploration. The reproducibility of the approach is currently lacking due to incomplete methodological details. While the work is promising, it requires additional validation and clarification to fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor Revision: The authors should expand their comparative analysis, strengthen validation across a broader range of datasets, and provide more complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Bo Wang",
      "Dongsheng Li",
      "Guanzhou Ke",
      "XINYANG JIANG",
      "Xiaoli Wang",
      "Xufang Luo",
      "Yang Yu",
      "Yifan Yang",
      "Zilong Wang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_d7f9b710e10bdd36261f817262f60d57d47921bf.pdf",
    "remote_url": "https://openreview.net/pdf/d7f9b710e10bdd36261f817262f60d57d47921bf.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "SeLoRA: Self-Expanding Low-Rank Adaptation of Latent Diffusion Model for Medical Image Synthesis",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Text-to-Image Synthesis",
      "Low-Rank Adaptation",
      "Medical Imaging",
      "Parameter Efficient Finetuning"
    ],
    "abstract": "The persistent challenge of medical image synthesis posed by the scarcity of annotated data and the need to synthesize \"missing modalities\" for multi-modal analysis, underscored the imperative development of effective synthesis methods. Recently, the combination of Low-Rank Adaptation (*LoRA*) with latent diffusion models (LDMs) has emerged as a viable approach for efficiently adapting pre-trained large language models, in the medical field. However, the direct application of *LoRA* assumes uniform ranking across all linear layers, overlooking the significance of different weight matrices, and leading to sub-optimal outcomes. Prior works on *LoRA* prioritize the reduction of trainable parameters, and there exists an opportunity to further tailor this adaptation process to the intricate demands of medical image synthesis. In response, we present *SeLoRA*, a Self-Expanding Low-Rank Adaptation module, that dynamically expands its ranking across layers during training, strategically placing additional ranks on crucial layers, to allow the model to elevate synthesis quality where it matters most. Our analysis shows that *SeLoRA* strikes the best balance between synthesis quality and training efficiency. The proposed method not only enables LDMs to fine-tune on medical data efficiently but also empowers the model to achieve improved image quality with minimal ranking. The code of our *SeLoRA* method is publicly available at https://anonymous.4open.science/r/SeLoRA-980D.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a new parameter-efficient fine-tuning method SeLoRA (Self-Expanding Low-Rank Adaptation) for adapting Stable Diffusion to generate chest x-ray images. The main contribution of the paper lies in dynamically expanding the rank of LoRA during the training process, allowing it to adapt the rank according to the importance of different layers and thereby improving the quality of the synthesized images. The novelty of this approach is in using Fisher Information to guide the rank expansion, avoiding the limitations of the traditional LoRA method, which uses a \"uniform rank,\" especially when dealing with models (like Stable Diffusion) that have diverse weight matrix shapes. The paper demonstrates the effectiveness of SeLoRA on two chest x-ray datasets and provides detailed comparative experiments with other LoRA variants.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. \nThe papers proposes SeLoRA, a dynamic rank-expanding method using Fisher Information to guide rank expansion during fine-tuning large models with LoRA. This is novel and more applicable to models with diverse weight matrix shapes.\n2.\nExperimental results demonstrate the effectiveness of the proposed method.\n3. \nThe paper is well-organized.\n\n### Weaknesses\n\n1.\nThe contribution is vague. As a paper focusing on adaptive parameter efficient fine-tuning methods, the paper utilizes LoRA to adapt Stable Diffusion for Chest X-ray synthesis, limiting its technical contribution. For a paper dedicated to adapting foundation model like Stable Diffusion for Chest X-ray (medical image) synthesis, the exploration is also limited and does not compare with previous work (e.g. Chambon et al., 2022a;b). Through visual comparison with image displays in Chambon et al., 2022a;b, the proposed method seems at a disadvantage.\n2.\nExperiments are conducted on relatively small datasets. Large image-report paired Chest X-ray datasets exist (e.g. MIMIC-CXR). Is this because of the heavy burden of large model like Stable Diffusion, or may be also related to the proposed method? Can the authors provide training time comparisons between SeLoRA and the compared methods? Also, as the test set of IU X-RAY and Montgomery County CXR dataset has only contains 100~200 images, the validation of the effectiveness of the method is weak.\n3.\nEvaluation and explanation are insufficient. Using a CLIP model trained purely on natural images and a maximum text token length of 76 to compute CLIP-score may not faithfully reflect how good the text-image alignment is for Chest x-ray images.\nThe training/validation/testing split is strange, could the author explain why the test set only contain 4% of the data? Are Table 1 values computed with valid data or test data? This is unclear. The paper also lacks in-depth discussion of the distribution of final rank (Figure 4,5,6) and why other LoRA methods fail on Montgomery county CXR data (Figure7). How accurate can the model generate an image given the disease or abnormal findings in the text prompt? This may be revealed using pretrained Chest X-ray classification models or manually inspect a small subset of generated results.\n\nOther non-important issues:\n4.\nthe paper title is about \"medical image syntheis‚Äù; but it only focuses on chest x-ray image.\n5.\nThe formula derivation in section3.3 is unclear.\n6.\nThe Stable Diffusion model is trained to generate a resolution of 512x512, using a resolution of 224x224 may limit the performance.\n\n### Questions\n\nPlease answer the questions mentioned in the above 'weakness' section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a novel parameter-efficient fine-tuning technique named SeLoRA (Self-Expanding Low-Rank Adaptation) designed for adapting Stable Diffusion to generate chest X-ray images. The primary innovation lies in the dynamic expansion of the rank of LoRA during training, guided by Fisher Information, which enhances the model's ability to generate high-quality images. The paper claims to outperform traditional LoRA methods that utilize a uniform rank, particularly in the context of models with diverse weight matrix shapes. The authors demonstrate the effectiveness of SeLoRA through experiments on two datasets, alongside a detailed comparison with existing LoRA variants.\n\n### Major Comments\n1. The contribution of the paper is unclear. While it aims to adapt Stable Diffusion for chest X-ray synthesis using LoRA, it lacks depth in exploration and comparison with previous works (e.g., Chambon et al., 2022a; 2022b), where the proposed method appears less effective based on visual evaluations.\n2. The experiments utilize relatively small datasets, raising questions about the implications on model performance. Larger datasets like MIMIC-CXR could be investigated. A comparison of training times between SeLoRA and alternative methods is warranted. The test sets from IU X-RAY and Montgomery County datasets, containing only 100-200 images, weaken the validation of the findings.\n3. Evaluation criteria and rationale need strengthening. Utilizing a CLIP model trained exclusively on natural images and with a fixed text token length of 76 for score computation may not accurately reflect text-image alignment effectiveness for chest X-ray images. Clarification on the unusual training/validation/testing splits is needed, particularly why the test set represents only 4% of the data. Furthermore, it's unclear whether Table 1 values were drawn from validation or test data. More detailed discussions on rank distribution (Figures 4, 5, 6) and the performance of other LoRA methods on the Montgomery County dataset (Figure 7) would enhance transparency. Lastly, insights on how accurately the model can generate images based on disease descriptions in prompts should be explored.\n\n### Minor Comments\n1. The title suggests a focus on \"medical image synthesis,\" yet concentrates solely on chest X-ray images.\n2. The formula derivation in Section 3.3 could be more clearly articulated.\n3. The original Stable Diffusion model targets a resolution of 512x512; using 224x224 may hinder performance.\n\n### Summary Paragraph\nThis study addresses significant issues in medical image generation, presenting a potentially innovative but somewhat narrowly focused approach. While the novelty of the rank expansion tied to Fisher Information is commendable, the contributions appear limited by inadequate comparisons and the use of small datasets. Evaluation methodologies require enhancement to establish robustness and reproducibility, especially in assessing the generated image alignments with respect to the medical application. Overall, while the manuscript demonstrates value in its proposed method, the gaps in evaluation and context necessitate further work.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *SeLoRA* (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning method designed to adapt Stable Diffusion for generating chest X-ray images. The central idea is to dynamically expand the LoRA rank during training, guided by Fisher Information, thus adapting model capacity to layer importance and improving output quality. Experiments on two chest X-ray datasets are provided to demonstrate the method‚Äôs effectiveness. Overall, the paper is well-organized and presents an innovative concept, although its technical and experimental contributions are somewhat limited.\n\n---\n\n**Major Comments**  \n1. **Scope and Contribution** ‚Äì The paper‚Äôs contribution appears vague. While it proposes an adaptive LoRA method, the technical advance beyond existing LoRA-based fine-tuning approaches is modest. Furthermore, its application is limited to Stable Diffusion-based chest X-ray synthesis, with little exploration beyond this narrow setting. Comparisons with prior related works (e.g., Chambon et al., 2022a,b) are missing, and qualitative inspection suggests the proposed method may be at a disadvantage relative to these baselines.  \n2. **Experimental Limitations** ‚Äì The datasets used are small (IU X-Ray, Montgomery County CXR), weakening the empirical validation. Larger chest X-ray datasets such as MIMIC-CXR are not utilized. Clarification is needed on whether dataset choice was due to computational constraints or the proposed method itself. Training-time comparisons with other LoRA variants would be valuable. Given the small test sets (100‚Äì200 images), the evidence for effectiveness is weak.  \n3. **Evaluation Clarity and Methodology** ‚Äì Evaluation choices and explanations are insufficient. The CLIP model used for text‚Äìimage alignment was trained on natural images and may not appropriately assess medical imagery; the 76-token limit further restricts evaluation fidelity. The data split is also unclear (only 4% test data). It should be specified whether results in Table‚ÄØ1 use validation or test data. Figures‚ÄØ4‚Äì7 require better discussion‚Äîespecially regarding rank distribution and why some LoRA methods fail on specific datasets. Additionally, alignment between generated images and textual disease descriptions could be assessed using pretrained X-ray classifiers or manual inspection.  \n4. **Additional Issues** ‚Äì  \n   - The title refers to *medical image synthesis*, but the experiments focus exclusively on chest X-rays.  \n   - Derivation in Section‚ÄØ3.3 lacks clarity.  \n   - Reducing image resolution from 512√ó512 (native SD resolution) to 224√ó224 may constrain performance.  \n\n---\n\n**Minor Comments**  \n- Clarify formula presentation in Section‚ÄØ3.3.  \n- Ensure consistency in naming (e.g., \"Chest X-ray\" vs. \"chest x-ray\").  \n- Correct minor typographical and grammatical issues.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work introduces a novel idea of dynamically expanding LoRA rank using Fisher Information, contributing to innovation in parameter-efficient fine-tuning. However, its significance is limited by narrow application scope and small-scale validation. The evaluation lacks rigor and appropriate metrics for the medical imaging domain, impairing significance. Methodological details, dataset splits, and reproducibility aspects require clarification to strengthen confidence in the findings.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper present SeLoRA, a Self-Expanding Low-Rank Adaptation module, that dynamically expands its ranking across layers during training. The proposed method increases the rank from 1 gradually. FI-Ratio and parameter \\lambda were used to determine when to expanding the rank.  The experiment is performed with stable diffusion and X-ray dataset and synthesis the X-ray image with text prompt.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper introduces a parameter-efficient method to e fine-tune stable diffusion models for generating X-ray images based on text (radiology) prompts. And the proposed method can progressive expansion in the rank of LoRA. FI-Ratio is used to guiding SeLoRA to expand its rank. The rank of different layers was given. The experiment shows the result is promisingly.\n\n### Weaknesses\n\n1. Computational overhead. While SeLoRA reduces the number of trainable parameters, its dynamic rank expansion mechanism introduces additional computational complexity. Computing Fisher information increases the overhead, which may become significant for larger datasets or more complex models.\n2. Limited dataset evaluation. Experiments were limited to two 2D X-ray datasets, with no evaluation of SeLoRA‚Äôs performance on other modalities, such as MRI or CT scans. Further validation on additional modalities would help confirm the generalizability of the method.\n3. Visual result is limited. The visual result is not satisfactory, such as in Figure 8, the contrast and details are not good.\n\n### Questions\n\n1. Computational complexity analysis A comparison of training time, memory usage and FLOPs between SeLoRA, LoRA, and other variants is needed to quantify the computational trade-offs introduced by dynamic rank expansion.\n2. Evaluation on a wider range of datasets Evaluating SeLoRA on larger datasets, such as MIMIC-CXR (containing approximately 377,000 images), would provide more insights into its scalability. Future work could also validate SeLoRA on MRI, CT, or ultrasound datasets, as testing on diverse datasets would better demonstrate its robustness and versatility.\n3. Incorporating related work. The idea of dynamically adjusting the rank of the LoRA matrix in SeLoRA is conceptually similar to the recently proposed ALoRA (NAACL 2024). However, the two methods differ in implementation: ALoRA utilizes pruning and redistribution strategies, while SeLoRA relies on Fisher information as the adjustment criterion. Insights from ALoRA could provide valuable inspiration for future improvements of SeLoRA.\n4. Evaluation with medical doctor may help verify the experiment results.\nOther questions\n1. Unconventional split of the IU X-Ray dataset. The 80:16:4 split results in a relatively small test set, which could compromise the robustness of the evaluation. A more conventional split (e.g., 80:10:10) might provide more reliable insights.\n2. Small sample size in the Montgomery County CXR dataset. With only 138 samples, the Montgomery County dataset is too small for deep learning applications, which may impact the stability and generalizability of the model‚Äôs results\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents SeLoRA, a Self-Expanding Low-Rank Adaptation module that expands its rank dynamically across layers during training. This parameter-efficient method fine-tunes stable diffusion models to generate X-ray images based on text prompts. Overall, the clarity of the manuscript is adequate, though certain aspects, particularly computational implications and evaluation breadth, require attention.\n\n**Major Comments:**\n1. **Computational Overhead:** While SeLoRA reduces the number of trainable parameters, its dynamic rank expansion introduces additional computational complexity. The computation of Fisher information could significantly increase overhead for larger datasets or more complex models.\n2. **Limited Dataset Evaluation:** The experiments were restricted to two 2D X-ray datasets. Testing SeLoRA on other modalities, such as MRI or CT scans, will be essential to confirm the method's generalizability.\n3. **Limited Visual Results:** The visual output, particularly as shown in Figure 8, shows inadequate contrast and detail, raising concerns about the quality of generated images.\n\n**Minor Comments:** \n- A computational complexity analysis comparing training time, memory usage, and FLOPs among SeLoRA, LoRA, and other variants is necessary.\n- A broader evaluation on larger datasets like MIMIC-CXR, which contains about 377,000 images, would enhance understanding of scalability.\n- Insights from related work, particularly ALoRA, which differs in methodology, may inspire future improvements to SeLoRA.\n- Evaluating SeLoRA's performance with medical professionals could help verify results from the experiments.\n\n**Summary Paragraph:**  \nIn terms of significance, the manuscript addresses a notable method for generating X-ray images using text prompts, highlighting an innovative approach to low-rank adaptation. However, the evaluation is limited, and the method's reproducibility may be affected due to insufficient dataset diversity. Enhancing the visual quality of results and incorporating broader evaluation metrics will strengthen the study's contributions.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *SeLoRA (Self‚ÄëExpanding Low‚ÄëRank Adaptation)*, a parameter‚Äëefficient fine‚Äëtuning module that dynamically increases the rank of adaptation layers during training. The rank expansion is guided by a Fisher Information (FI) ratio and a threshold parameter Œª. The method is applied to stable diffusion models for generating X‚Äëray images from text prompts. Overall, the paper is clearly written and introduces a technically sound and conceptually interesting approach, though the experimental validation and computational analysis are limited.\n\n---\n\n**Major Comments**  \n1. **Computational Overhead:** While SeLoRA reduces the number of trainable parameters, the dynamic rank‚Äëexpansion process introduces added computational burden, particularly from Fisher Information computation. The associated overhead should be quantitatively compared (e.g., training time, memory, FLOPs) with existing LoRA variants.  \n2. **Limited Dataset Evaluation:** Experiments are restricted to two small 2D X‚Äëray datasets (IU X‚ÄëRay and Montgomery County). The small test set in the 80:16:4 split and the very limited size of the Montgomery dataset undermine the robustness and generalizability of results. Validation on larger or multimodal datasets (e.g., MIMIC‚ÄëCXR, MRI, CT, ultrasound) would strengthen the claims.  \n3. **Quality of Visual Results:** The generated images exhibit limited visual quality‚Äîe.g., Figure‚ÄØ8 shows poor contrast and insufficient detail. More convincing visual comparisons or metrics are needed.  \n4. **Relation to Prior Work:** The concept of dynamically adjusting LoRA rank is related to the recently proposed ALoRA (NAACL‚ÄØ2024). A comparison or discussion highlighting the conceptual and implementation differences would clarify SeLoRA‚Äôs novelty.  \n5. **Clinical Evaluation:** Involving radiologists or medical experts in qualitative assessment would provide practical validation of image realism and diagnostic plausibility.\n\n---\n\n**Minor Comments**  \n- Clarify dataset split rationale and provide consistency with standard practice (e.g., 80/10/10).  \n- Correct minor grammatical issues (‚Äúe fine‚Äëtune‚Äù ‚Üí ‚Äúfine‚Äëtune,‚Äù ‚Äúpromisingly‚Äù ‚Üí ‚Äúpromising‚Äù).  \n- Ensure all figures are high‚Äëcontrast and properly labeled with clear captions.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe paper is significant in proposing an efficient fine‚Äëtuning strategy for diffusion models in medical imaging. It shows moderate innovation through adaptive rank expansion, though similar ideas exist in related work. Experimental evaluation is preliminary and limited in scope and scale, reducing the strength of evidence. Reproducibility appears feasible given the described method, but additional computational benchmarks and broader validation are necessary to confirm robustness.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The concept is promising, but the current evaluation, computational analysis, and discussion of related methods require substantive expansion before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces SeLoRA (Self-Expanding Low-Rank Adaptation), an extension of the LoRA (Low-Rank Adaptation) technique, designed for the fine-tuning of large diffusion models specifically in medical image synthesis. The core idea is to dynamically expand the rank of low-rank matrices during training, based on a criterion derived from Fisher information. This adaptation is applied selectively across different layers, allowing for a more effective distribution of ranks that aligns with each layer's significance in the model, particularly within the denoising U-Net of the Stable Diffusion framework.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper is well-organized and easy to follow.\n\n2. The idea of adaptive computation rank selection is interesting and highly relevant for using pre-trained models for downstream tasks in a memory efficient fashion\n\n### Weaknesses\n\n- While the paper mainly focusses on making a more efficient LoRA design as claimed by authors, there is no Analysis of Training Efficiency in the paper.\n\n- Lack of detailed explanation of the procedure of selecting hyper-parameters needed.\n\n### Questions\n\n- How is the proposed method compared to other baselines in terms of max training GPU memory, training speed, and\ntraining time costs? An analysis of these criteria would strengthen the paper.\n\n- The paper mentions thresholds (Œª and t) for triggering rank expansion. How sensitive is the method to these hyper-parameters?\n\n- In figure 7, why are the other methods not generating similar xray images? As per my understanding, they should at-least make a similar image like figure 8. \n\n- The proposed method is just compared to methods with rank pruning methods (dylora and adalora), could you mention why there is no comparing with adaptive rank selection papers? as they seem to be a similar approach to your paper.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNo ethics review needed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents SeLoRA (Self-Expanding Low-Rank Adaptation), an extension of the Low-Rank Adaptation (LoRA) technique aimed at enhancing the fine-tuning of large diffusion models for medical image synthesis. The primary innovation involves the dynamic rank expansion of low-rank matrices during training, driven by a criterion based on Fisher information. This selective adaptation across different model layers enhances rank distribution according to the significance of each layer, particularly within the denoising U-Net of the Stable Diffusion framework. \n\n**Major Comments:**\n1. The manuscript lacks a detailed analysis of training efficiency, despite focusing on improving LoRA design.\n2. There is insufficient explanation regarding the selection of hyper-parameters needed for the proposed method.\n3. The comparison of the proposed method against other baselines concerning maximum training GPU memory, training speed, and time costs is missing. This comparison would substantiate claims of efficiency.\n4. The manuscript needs to address the sensitivity of the proposed method to the thresholds (Œª and t) mentioned for triggering rank expansion.\n5. In Figure 7, the disparity in x-ray image generation among methods is unclear; one would expect some similarity to the images in Figure 8.\n6. The paper focuses on comparisons with rank pruning methods (e.g., DyLoRA and AdaLoRA), yet does not include comparisons with adaptive rank selection papers, which appear relevant and similar to the proposed approach.\n\n**Minor Comments:**\n- The manuscript is well-organized and clear in presentation.\n- The concept of adaptive computation rank selection is intriguing and pertinent for efficiently deploying pre-trained models for downstream tasks.\n\n**Summary Paragraph:**  \nIn terms of significance, the introduction of SeLoRA demonstrates innovation through its approach to rank adaptation in large models, although a thorough evaluation of training efficiency and hyper-parameter sensitivity is lacking. The reproducibility of results would benefit from clearer comparisons with other methodologies and an analysis of resource utilization. While the study is organized and the main idea is relevant, the weaknesses in evaluation and parameter selection need to be addressed for greater clarity and rigor.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *SeLoRA* (Self-Expanding Low-Rank Adaptation), an extension of LoRA tailored for fine-tuning large diffusion models in medical image synthesis. The key innovation lies in dynamically expanding the rank of low-rank matrices during training, guided by Fisher information, and applying this expansion selectively across model layers. This approach aims to improve memory efficiency and model adaptation within the U-Net architecture of Stable Diffusion. Overall, the paper is clearly organized and easy to follow, with a conceptually interesting contribution relevant to efficient adaptation of pre-trained generative models.\n\n**Major Comments**  \n1. Although the paper claims improvements in LoRA efficiency, there is no analysis or quantitative evidence regarding *training efficiency* (e.g., computational cost, speed, GPU memory usage). Including such evaluation would substantiate the claimed benefits.  \n2. The procedure for selecting hyperparameters, particularly the thresholds (Œª and *t*) controlling rank expansion, lacks sufficient explanation. A sensitivity analysis would improve reproducibility and understanding.  \n3. Comparisons are restricted to rank-pruning methods (DyLoRA and AdaLoRA). It would strengthen the contribution to include or at least justify the omission of comparisons with adaptive rank selection methods, which are conceptually closer.  \n4. Figure 7 raises questions about qualitative consistency: other methods do not generate x-ray images similar to those of SeLoRA, though one might expect partial similarity (as seen in Figure 8). Clarification of this difference would help interpret the results.\n\n**Minor Comments**  \n- Provide more details on hyperparameter choices for reproducibility.  \n- Clarify the criteria or computation of Fisher information used for adaptive expansion.  \n- Ensure figures and captions clearly indicate methodological differences.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study introduces a potentially useful adaptive rank selection mechanism with relevance for efficient medical diffusion model training (innovation, significance). However, its evaluation omits critical efficiency metrics and broader comparisons, limiting assessment of practical gains (evaluation). Reproducibility is also hindered by incomplete description of hyperparameter selection and sensitivity. The paper‚Äôs clarity is good, but methodological completeness should be improved to meet typical TMI standards.\n\n**Decision Recommendation**  \n**Major Revision.** The idea is promising and well-presented but requires added empirical analysis, clearer methodological detail, and broader comparisons to substantiate claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI:**  \n   ‚úÖ The manuscript presents a methodological contribution to *medical image synthesis* using diffusion-based generative models. Specifically, it introduces **SeLoRA** (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning (PEFT) mechanism for adapting latent diffusion models to medical imaging tasks. The work focuses on rank-adaptive LoRA techniques, which are methodological innovations in model adaptation rather than purely application studies. The application to X-ray and histopathology image synthesis serves to validate methodological claims. Thus, the work fits well within TMI‚Äôs scope of *imaging methods and computational modeling*.\n\n2. **Novelty & Contribution Level:**  \n   The core novelty lies in introducing a **dynamic rank-expansion strategy guided by Fisher information** to improve parameter efficiency in diffusion model adaptation. Compared with prior adaptive LoRA or rank-pruning variants (DyLoRA, AdaLoRA, ALoRA), SeLoRA‚Äôs ‚Äúself-expanding‚Äù mechanism is distinct ‚Äî it allows rank growth rather than pruning and uses a theoretically motivated Fisher Information Ratio test. While LoRA-based adaptation itself is established, the proposed incremental-learning mechanism provides a methodological increment rather than a radical advance. The contribution is thus *moderately novel but technically meaningful and well-motivated.*\n\n3. **Technical and Experimental Rigor:**  \n   The algorithmic formulation (Equations 2‚Äì5, Algorithm 1) is clearly presented, and convergence considerations are discussed (Sec.‚ÄØ3.3). The evaluation is conducted on multiple public datasets (IU X-Ray, Montgomery CXR, PatchGastricADC22) and multiple resolutions, with ablation on threshold Œª. Comparisons include LoRA, AdaLoRA, and DyLoRA baselines with careful control of parameter counts. Metrics (FID, CLIP) and figures are standard and support conclusions. However:\n   - Datasets used are relatively small; external validation beyond X-ray (e.g., CT or MR) remains limited.\n   - Statistical significance or confidence intervals for FID are minimal.\n   - Limited reporting of computational cost beyond memory/time averages (no discussion of scalability or stability).\n\n4. **Clarity and Presentation:**  \n   The manuscript is clearly structured and well-written, with sufficient mathematical detail and readable figures. Minor typographical issues (spacing, capitalization, and occasional OCR artifacts like ‚ÄúS ELF -EXPANDING‚Äù) should be corrected. Figures 3‚Äì6 are informative but somewhat dense; legends could better explain abbreviations (q,‚ÄØk,‚ÄØv,‚ÄØattn1,‚ÄØattn2).  \n\n5. **Ethical and Reproducibility Compliance:**  \n   The authors state publicly available code and use of publicly released datasets (IU‚ÄØX-Ray, Montgomery County CXR, PatchGastricADC22). No identifiable patient data are introduced, and dataset use appears consistent with public repository terms; IRB exemption would be appropriate. Reproducibility is adequately supported through provided hyperparameters and algorithm pseudocode.  \n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThis paper presents *SeLoRA*, a Self‚ÄëExpanding Low‚ÄëRank Adaptation method for fine‚Äëtuning latent diffusion models in medical image synthesis. The approach starts with a minimal rank and dynamically expands it per layer, guided by a Fisher Information Ratio (FI‚ÄëRatio) criterion. This allows the model to allocate parameters strategically where adaptation is most beneficial. SeLoRA is applied to X‚Äëray and histopathology datasets for text‚Äëto‚Äëimage synthesis, demonstrating improved Fr√©chet Inception Distance and CLIP scores relative to LoRA, AdaLoRA, and DyLoRA while using fewer trainable parameters. The paper also analyzes rank allocation patterns and training efficiency.\n\n**2. Strengths**\n- Introduces a *methodologically motivated* dynamic rank‚Äëexpansion mechanism guided by Fisher Information‚Äînovel among LoRA variants.  \n- Demonstrates consistent improvements in synthesis quality and parameter efficiency.  \n- Offers theoretical intuition for convergence and empirical ablations across Œª thresholds.  \n- Well‚Äëstructured, reproducible implementation with code availability.  \n- Applied to multiple medical imaging datasets and modalities, suggesting general applicability.\n\n**3. Weaknesses**\n- The experimental datasets are relatively small; results may not generalize to larger or clinically diverse cohorts.  \n- The presented FID/CLIP improvements, while positive, are modest on certain datasets and lack statistical testing.  \n- Limited theoretical depth beyond heuristic Fisher scoring‚Äîno formal proof of optimality or convergence.  \n- Figures and tables sometimes contain formatting anomalies; some numerical results (e.g., FID‚ÄØ64/192 units) could be better explained.  \n- Comparison restricted to LoRA variants; missing baseline vs. full fine‚Äëtuning or non‚ÄëLoRA PEFT methods in vision‚Äëlanguage diffusion settings.\n\n**4. Major Comments**\n1. **Novelty Contextualization:** Clarify how SeLoRA‚Äôs Fisher‚Äëguided expansion differs mathematically from existing ALoRA or adaptive‚Äërank LoRA variants. A concise comparison table would help establish novelty.  \n2. **Scalability Evaluation:** Include an experiment on a larger dataset (e.g., MIMIC‚ÄëCXR subset or similar) or a 3D imaging task to strengthen claims of general applicability.  \n3. **Ablation on FI‚ÄëRatio Threshold and t:** Present quantitative curves of performance vs. parameter growth to better show the efficiency trade‚Äëoff.  \n4. **Uncertainty and Statistical Reporting:** Provide mean‚ÄØ¬±‚ÄØSD for FID/CLIP across multiple runs and confidence intervals for improvements.  \n5. **Clinical Relevance Discussion:** Expand on how synthesized images could be used in downstream clinical or training pipelines, addressing potential ethical use.\n\n**5. Minor Comments**\n- Correct typographical inconsistencies and OCR artifacts (e.g., ‚ÄúFisher information Ratio‚Äù, ‚ÄúS ELF‚ÄëEXPANDING‚Äù).  \n- Improve figure readability‚Äîespecially Figures‚ÄØ3‚Äì6‚Äîby using consistent color scales and adding legends.  \n- Ensure all dataset citations (e.g., IU‚ÄØX‚ÄëRay, Montgomery CXR, PatchGastricADC22) include proper repository URLs or DOIs.  \n- Clarify units for memory/time tables and refer consistently to ‚ÄúGPU‚Äëhours‚Äù or ‚Äúmemory‚ÄØ(GB)‚Äù.  \n- Verify that all hyperparameters (Œª,‚ÄØt,‚ÄØr) are defined upon first mention in the main text.\n\n**6. Overall Recommendation**  \n**Major‚ÄØRevision.**  \nThe paper presents a promising and well‚Äëimplemented methodological contribution within TMI‚Äôs scope. However, it requires additional evidence of generalizability and more rigorous comparative and statistical analysis to meet TMI‚Äôs standards for methodological maturity. With extended validation and stronger framing of novelty, SeLoRA could be a valuable addition to the literature on parameter‚Äëefficient adaptation for medical imaging generative models.\n\n**7. Confidence Level:** 4‚ÄØ/‚ÄØ5 ‚Äî Confident in technical assessment and TMI‚ÄØscope alignment.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents SeLoRA (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning method for adapting latent diffusion models to medical image synthesis. The authors address the limitation that standard LoRA applies uniform ranking across all linear layers, which may be suboptimal for the diverse weight matrix shapes in diffusion models' U-Net architecture. SeLoRA dynamically expands ranks during training using Fisher information as a guide, starting from rank r=1 and strategically placing additional ranks on crucial layers. The method is evaluated on chest X-ray datasets (IU X-RAY and Montgomery County CXR) for text-conditioned medical image synthesis. Results show that SeLoRA achieves superior FID scores and CLIP scores compared to standard LoRA, AdaLoRA, and DyLoRA while using fewer trainable parameters (Section 4, Tables 1-2). The rank allocation analysis demonstrates that SeLoRA concentrates higher ranks on attention layers where text and image embeddings interact (Figures 3-4).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - The FI-Ratio definition in Equation 5 uses inconsistent notation where the denominator should logically be larger than numerator for meaningful interpretation, but the paper suggests expansion when FI-Ratio ‚â• Œª > 1, creating conceptual confusion about what constitutes \"improvement\"\n  - Algorithm 1 lacks mathematical precision in the expansion condition, particularly in how the FI-Scores are computed for the expanded matrices A‚Ä≤ and B‚Ä≤ without parameter updates\n  - The convergence analysis in Section 3.3 provides only intuitive arguments without formal mathematical proof, and the approximation of FI-Ratio lacks rigorous derivation of the assumed relationship between rank and Fisher information\n\n‚Ä¢ **Limited experimental validation and dataset scope**\n  - Evaluation is restricted to only two small chest X-ray datasets (3,955 and 138 images respectively as noted in Section 3.4), which limits generalizability claims for medical image synthesis\n  - No comparison with full fine-tuning baselines or other parameter-efficient methods beyond LoRA variants, missing important performance context for the claimed efficiency gains\n  - The PatchGastricADC22 results in Appendix show SeLoRA performing worse than LoRA on the primary FID metric (Table 6), contradicting the main narrative of consistent superiority\n  - Statistical significance testing is absent despite reporting standard deviations in Tables 1-2, making it unclear whether observed differences are statistically meaningful\n\n‚Ä¢ **Methodological concerns and incomplete analysis**\n  - The Fisher information calculation in Equation 3 uses empirical estimation over single batches, which may be noisy and unstable for reliable expansion decisions during training\n  - Hyperparameter selection (Œª=1.1, t=40) appears arbitrary with insufficient justification or sensitivity analysis beyond the limited ablation in Table 3\n  - The rank expansion strategy lacks theoretical grounding for why Fisher information should be the optimal criterion for determining when to expand, and no comparison with alternative expansion criteria is provided\n  - Missing analysis of computational overhead during training, particularly the cost of repeatedly computing FI-Scores every t steps for expansion testing\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and strengthen mathematical foundations**\n  - Reformulate the FI-Ratio definition in Equation 5 with clear explanation of why this specific ratio indicates improvement, and provide formal mathematical justification for the expansion criterion\n  - Add rigorous algorithmic details in Algorithm 1 specifying exactly how FI-Scores are computed for expanded matrices, including gradient computation procedures without parameter updates\n  - Develop formal convergence proof for the rank expansion algorithm with clear bounds, and derive the FI-Ratio approximation in Section 3.3 with explicit assumptions and mathematical steps\n\n‚Ä¢ **Expand experimental validation comprehensively**\n  - Include evaluation on larger and more diverse medical imaging datasets beyond chest X-rays to demonstrate broader applicability across medical imaging modalities and scales\n  - Add comparisons with full fine-tuning and other PEFT methods (e.g., adapters, prompt tuning) to provide complete performance context and justify the focus on LoRA-based approaches\n  - Conduct statistical significance testing for all reported metrics and provide detailed analysis of why SeLoRA underperforms on PatchGastricADC22, addressing apparent contradictions in performance claims\n  - Report comprehensive computational analysis including training time, memory usage, and FLOPs comparison across all methods with proper statistical analysis\n\n‚Ä¢ **Strengthen methodological rigor and theoretical justification**\n  - Investigate stability of Fisher information estimation and propose more robust alternatives, including analysis of how batch size and gradient noise affect expansion decisions\n  - Conduct thorough hyperparameter sensitivity analysis across multiple datasets and provide principled guidelines for selecting Œª and t values based on dataset characteristics\n  - Compare Fisher information criterion with alternative expansion strategies (e.g., gradient magnitude, loss-based criteria) and provide theoretical analysis of why Fisher information is optimal for this task\n  - Add detailed computational cost analysis of the expansion testing procedure and propose efficiency improvements to reduce the overhead of frequent FI-Score computation",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces SeLoRA (Self-Expanding Low-Rank Adaptation), a novel method for fine-tuning latent diffusion models (LDMs) for medical image synthesis. SeLoRA dynamically adjusts the rank of the low-rank adaptation (LoRA) matrices during training, allowing for more flexible and efficient adaptation of pre-trained models to medical datasets. The authors demonstrate the efficacy of SeLoRA through quantitative and qualitative evaluations on X-ray datasets, showcasing improvements in both FID scores and CLIP scores compared to other LoRA variants. The manuscript is well-written and clearly articulates the technical innovations and motivations behind SeLoRA.\n\n## Major Comments\n1. Novelty and Positioning: The introduction of dynamic rank adjustment in LoRA is a significant contribution to the field of parameter-efficient fine-tuning. However, the manuscript could benefit from a more detailed comparison with existing methods that also aim to optimize rank allocation, such as DyLoRA and AdaLoRA. The authors should explicitly discuss the unique contributions and distinctions of SeLoRA in terms of both methodology and performance.\n\n2. Evaluation Design: The evaluation is primarily conducted on two X-ray datasets, which limits the generalizability of the findings. Including a wider range of medical imaging modalities (e.g., CT, MRI) would provide a more comprehensive assessment of SeLoRA's capabilities. Additionally, the experiments are performed at different resolutions, but it would be beneficial to include a more extensive analysis of the impact of resolution on the performance of SeLoRA.\n\n3. Comparisons: The comparisons with LoRA, AdaLoRA, and DyLoRA are thorough, but the manuscript should also consider including state-of-the-art methods for medical image synthesis as benchmarks. This would help establish the relative performance of SeLoRA in the broader context of medical imaging research.\n\n4. Reproducibility: The manuscript states that the code is publicly available, which is commendable. However, the description of the training protocols and hyperparameters is somewhat sparse. Providing a more detailed description of the training setup, including the exact configurations and hyperparameters used, would greatly enhance the reproducibility of the results.\n\n## Minor Comments\n1. Figures: Figures 3 and 4 are cluttered and could be improved by showing fewer representative slices with zoomed-in regions for clarity.\n   \n2. Notation and Terminology: The notation for the forward operator is introduced without sufficient explanation. Definitions for acronyms like \"R=4\" are missing.\n   \n3. Typographical Issues: Minor typographical errors exist, such as \"k-spacce\" on page 6 and \"undersampling maskes\" on page 7.\n\n## Summary Paragraph\nThe manuscript addresses the significant challenge of synthesizing high-quality medical images from limited annotated data, which is a crucial issue in medical imaging. SeLoRA offers a promising approach to parameter-efficient fine-tuning by dynamically adjusting the rank of LoRA matrices. The technical innovation is compelling and has the potential to advance the field of medical image synthesis. However, the evaluation is currently limited to a few datasets and modalities, which weakens the claims of broad applicability. The reproducibility of the approach is satisfactory due to the availability of the code, but more detailed methodological descriptions are needed. Overall, while the idea has merit, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include a broader range of medical imaging modalities, provide more detailed descriptions of the training protocols, and clarify the unique contributions of SeLoRA in the context of existing methods.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chengjia Wang",
      "Giorgos Papanastasiou",
      "Guang Yang",
      "Hongwei Bran Li",
      "Wei Pang",
      "Yuchen Mao"
    ],
    "url": "pdfs/iclr.cc-2025-conference_f6f8edf9c65dcb297d770a2a371e14a022229bb1.pdf",
    "remote_url": "https://openreview.net/pdf/f6f8edf9c65dcb297d770a2a371e14a022229bb1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Transformer-Based CT Anomaly Detection and Auto-Segmentation of Sparse Lung Nodules",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Transformer",
      "CT scans",
      "lung nodules",
      "anomaly detection",
      "auto-segmentation",
      "Deformable-DETR",
      "sparse data",
      "medical imaging",
      "self-attention",
      "multi-scale learning",
      "object detection",
      "Focal Loss",
      "segmentation"
    ],
    "abstract": "Accurate segmentation of lung nodules in computed tomography (CT) scans is challenging due to extreme class imbalance, where nodules appear sparsely among healthy tissue. Lung tumor boards often review these scans manually, a time-consuming process. This paper introduces a novel two-stage approach for lung tumor segmentation by framing the problem as anomaly detection. The method is divided into two stages, allowing each model to leverage its strengths. Stage 1 focuses on region proposal, employing a custom Deformable Detection Transformer with Focal Loss to overcome class imbalance and localize sparse tumors. In Stage 2, the predicted bounding boxes are refined into pixel-wise segmentation masks using a fine-tuned variant of Meta's Segment Anything Model (SAM) for semantic segmentation. To address the challenge of nodule sparsity and improve spatial context, a 7.5 mm Maximum Intensity Projection (MIP) is applied, aiding in the differentiation between nodules, bronchioles, and vascular structures. The model achieves a Dice coefficient of 92.4%, with 95.2% sensitivity and 93.2% precision on the LUNA16 dataset, demonstrating robust performance in real-world clinical conditions where nodule sparsity is 5%.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a two-stage pipeline for lung nodule segmentation in CT scans, designed to support lung tumor boards by enhancing segmentation accuracy and efficiency. The first stage employs a custom Deformable Detection Transformer (DETR) architecture to detect sparse lung tumors, leveraging deformable attention to improve sensitivity to small nodules. The second stage utilizes a fine-tuned Segment Anything Model (SAM), enhanced with medical imaging capabilities (MedSAM), to refine the bounding boxes into pixel-level segmentation masks, ensuring precision in differentiating nodules from surrounding anatomy.\n\nTo address the class imbalance in CT data - where lung nodules are rare compared to healthy tissue - the framework incorporates focal loss, reducing model bias towards non-tumor areas and enhancing detection accuracy for hard-to-detect nodules. Achieving a 94.2% F1 score for bounding box prediction and a 92.4% Dice coefficient in segmentation accuracy, this pipeline demonstrates strong potential to improve clinical workflows, enhance tumor board decision-making, and contribute to better patient outcomes by streamlining nodule detection in a clinical setting.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper introduces a novel two-stage approach for lung tumor segmentation by framing the task as anomaly detection, addressing the challenges of sparse nodule identification in CT scans. This innovative structure uses a Deformable Detection Transformer (DETR) for region proposals and a fine-tuned Segment Anything Model (SAM) for precise segmentation, effectively handling class imbalance and complex image features.\n\nAdditionally, the paper‚Äôs clear, logical structure and well-explained methodology make complex concepts accessible. Detailed quantitative results further highlight the framework's effectiveness, making it a valuable and readable contribution to medical imaging and clinical decision support.\n\n### Weaknesses\n\n1. The paper‚Äôs experimental section lacks depth, with insufficient analysis to thoroughly validate the proposed method. \n\n2. There is no ablation study provided, which limits insight into how each component - such as the use of Deformable Detection Transformer (DETR), the fine-tuned Segment Anything Model (SAM), and the customized focal loss - contributes to overall performance. Without this breakdown, it‚Äôs difficult to assess which aspects of the framework are most effective. \n\n3. The paper relies solely on quantitative evaluation, omitting any qualitative assessment, such as visual comparisons among different methods, which could provide a clearer understanding of the model's segmentation accuracy and real-world applicability.\n\n4. The presentation of results is also weak, with layout issues that detract from readability and professionalism. For instance, Table 2 extends beyond the page margin, rendering the data difficult to interpret. Additionally, there are inconsistencies and errors in in-text citations, which may confuse readers and hinder the paper‚Äôs credibility. These issues in presentation and citation detract from the paper's overall clarity and polish, suggesting the need for more careful formatting and editing. There are also several grammatical errors, which make the paper somewhat challenging to read.\n\n### Questions\n\n- Why is there only quantitative evaluation/comparison provided in the manuscript? Could you provide some qualitative results, such as visual examples of segmentation outputs, to illustrate the model‚Äôs performance?\n\n- Can you elaborate on how the class imbalance was handled during training? Were any additional strategies (besides focal loss) considered or tested to further address this issue?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a two-stage pipeline aimed at lung nodule segmentation in CT scans, intended to enhance accuracy and efficiency for lung tumor boards. The first stage features a custom Deformable Detection Transformer (DETR) for detecting sparse tumors, utilizing deformable attention to bolster sensitivity towards small nodules. The second stage employs a refined Segment Anything Model (SAM), supplemented with medical imaging capabilities (MedSAM), to convert bounding boxes into precise pixel-level segmentation masks, effectively distinguishing nodules from adjacent anatomical structures.\n\n**Major Comments:**  \n1. The experimental section lacks comprehensive depth; there is insufficient analysis to validate the proposed method adequately.  \n2. An ablation study is absent, limiting insights into the contributions of components such as Deformable Detection Transformer (DETR), the fine-tuned Segment Anything Model (SAM), and the customized focal loss to overall performance.  \n3. The evaluation is solely quantitative, with no qualitative assessments provided, such as visual comparisons among methods, which would foster a clearer understanding of segmentation accuracy and real-world applicability.  \n4. Result presentation has layout issues detracting from readability, exemplified by Table 2 exceeding page margins, making data interpretation difficult. Additionally, inconsistencies and errors in in-text citations may cause confusion and undermine the manuscript's credibility, alongside several grammatical errors impacting readability.\n\n**Minor Comments:**  \n- The clarity of figures and tables could be improved.  \n- There is a need for careful proofreading to address grammatical mistakes.  \n- In-text citation formatting requires consistency.\n\n**Summary Paragraph:**  \nThis paper demonstrates significant potential in enhancing clinical workflows and decision-making via an innovative approach to lung nodule segmentation, scoring a 94.2% F1 for bounding box prediction and a 92.4% Dice coefficient in segmentation accuracy. However, while the contribution is notable in terms of novelty and structure, weaknesses in experimental validation, absence of qualitative data, and presentation issues affect its evaluation and reproducibility.\n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage deep learning framework for lung nodule segmentation in CT scans, aiming to assist lung tumor boards by improving segmentation accuracy and efficiency. The first stage employs a Deformable Detection Transformer (DETR) for sparse nodule detection, while the second utilizes a fine-tuned Segment Anything Model (SAM/MedSAM) to generate precise pixel-level masks. By incorporating focal loss to mitigate class imbalance between nodules and healthy tissue, the approach achieves high quantitative performance (94.2% F1 for detection and 92.4% Dice for segmentation). Overall, the paper is clearly written, methodologically sound in concept, and potentially impactful for clinical decision support, though its empirical validation remains limited.  \n\n**Major Comments**  \n1. **Experimental Depth:** The evaluation lacks sufficient analytical depth. The results are reported clearly but without detailed interpretation, limiting confidence in the robustness of the findings.  \n2. **Ablation Studies:** No ablation or component-wise analysis is provided. The contribution of individual elements‚Äîsuch as DETR, MedSAM, and focal loss‚Äîcannot be disentangled, making it difficult to assess which innovations drive the observed performance.  \n3. **Qualitative Evaluation:** Only quantitative results are presented. Including qualitative examples (e.g., visual segmentation outputs) would strengthen the evidence and demonstrate clinical applicability.  \n4. **Presentation and Formatting:** Certain layout problems, notably Table 2 exceeding page margins, compromise readability. In-text citation inconsistencies and minor grammatical errors further detract from clarity and professional polish.  \n\n**Minor Comments**  \n- Improve formatting of tables and figures for readability.  \n- Correct inconsistent citations and typographical issues.  \n- Clarify details of the class imbalance strategy beyond focal loss; if alternatives were considered, briefly mention them.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper addresses a significant clinical imaging challenge with an innovative combination of DETR and SAM frameworks, thoughtfully adapted to medical data. Its conceptual novelty and strong quantitative metrics reflect potential impact. However, the evaluation is incomplete‚Äîlacking ablation and qualitative evidence‚Äîthus weakening claims of generalizability and reproducibility. Presentation issues, while secondary, limit readability. Overall, the contribution is promising but requires additional validation and revision for publication readiness.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists. However, the primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists.\n\n### Weaknesses\n\nThe primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Questions\n\nPreprocessing is performed with 1 mm isocubic resolution, yet the method for generating 7.5 mm MIP using five slices in Figure 1-c needs clarification.\n\nThe authors should analyze the histogram of nodules under 10 mm from the LIDC dataset and include these results and discussions in the paper.\n\nThe paper lacks external validation, which raises concerns about the generalizability of the findings. A discussion on this limitation is recommended.\n\nIn Figure 1-b, lung segmentation is reportedly performed using Otsu segmentation; however, accuracy metrics such as DSC should be presented.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPublic dataset",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript proposes a two-stage approach for lung tumor segmentation through anomaly detection. The first stage involves region proposal using a deformable detection transformer with focal loss, followed by fine-tuning with a Segment Anything Model (SAM). Notably, the study employs the Maximum Intensity Projection (MIP) method to tackle issues related to nodule sparsity and spatial context‚Äîa technique often utilized by radiologists. Overall, the clarity of the manuscript can be improved, especially concerning its methodology.\n\n### Major Comments\n1. The entire preprocessing and modeling process is conducted in 2D. This may hinder the differentiation of diseased lungs or lung cancers that are located near the thoracic wall. \n2. Nodules with subsolid or ground-glass opacity (GGO) characteristics might not be effectively visualized in thicker MIP slices; thus, these types should be assessed separately.\n3. While the model achieves superior performance compared to prior models, it lacks subclass analysis and an ablation study, which limits its technical novelty.\n4. The paper lacks external validation, raising concerns regarding the generalizability of the findings.\n\n### Minor Comments\n- Clarification is needed on how preprocessing at a 1 mm isocubic resolution produces a 7.5 mm MIP using five slices, as shown in Figure 1-c.\n- The authors should analyze and include the histogram of nodules under 10 mm from the LIDC dataset in their discussion.\n- The segmentation performed with Otsu method as indicated in Figure 1-b should have accompanying accuracy metrics, such as Dice Similarity Coefficient (DSC).\n\n### Summary Paragraph\nIn terms of significance, the manuscript addresses an important issue in lung tumor segmentation but lacks depth in evaluation and analysis of results. The innovation is present through the novel use of MIP; however, the predominance of 2D processing and lack of performance validation diminishes its potential impact. The evaluation does provide some improvements over previous efforts, but the absence of thorough testing and validation limits reproducibility. Overall, while the manuscript shows promise, it requires substantial revisions to fulfill editorial criteria.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage method for lung tumor segmentation based on anomaly detection. Stage one involves region proposal using a deformable detection transformer with focal loss, and stage two applies a fine-tuned SAM model. The authors additionally employ Maximum Intensity Projection (MIP) to mitigate issues of nodule sparsity and contextual information‚Äîan approach commonly used by radiologists. While the paper is clear in motivation and design, concerns remain regarding the methodological choices, particularly the reliance on 2D processing and insufficient evaluation depth.\n\n**Major Comments**  \n1. **Dimensionality Limitation:** All preprocessing and modeling steps are performed in 2D, which may limit the model‚Äôs ability to accurately distinguish lung cancers adjacent to the thoracic wall. A 3D approach could better preserve spatial information.  \n2. **Nodule Visibility:** For subsolid or ground-glass opacity (GGO) nodules, visibility is potentially reduced in thicker MIP slices. Separate analysis for these subtypes would improve model robustness and interpretability.  \n3. **Lack of Analytical Studies:** The work omits crucial analyses such as an ablation study or subgroup performance evaluation, weakening claims of technical contribution and novelty.  \n4. **External Validation:** Absence of validation on external datasets reduces the generalizability of the reported results; discussion of this limitation is necessary.  \n5. **Preprocessing Clarification:** The process of generating the 7.5‚ÄØmm MIP from five slices (Figure‚ÄØ1‚Äëc) requires clearer explanation.  \n6. **Performance Metrics:** Lung segmentation is reportedly performed using Otsu thresholding (Figure‚ÄØ1‚Äëb), yet quantitative accuracy metrics (e.g., Dice similarity coefficient) are missing. Their inclusion would strengthen the evaluation.\n\n**Minor Comments**  \n- The manuscript should include an analysis of nodules under 10‚ÄØmm from the LIDC dataset and corresponding discussion.  \n- Ensure figures and captions clearly describe preprocessing parameters and segmentation outputs.  \n- Minor editorial refinements could improve readability.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important clinical problem and achieves encouraging quantitative results. However, its significance is reduced by methodological constraints tied to 2D processing, limiting clinical applicability. Innovation is moderate, as the integration of MIP and SAM is incremental rather than conceptually novel. Evaluation lacks comprehensive validation and ablation analyses, and reproducibility would benefit from clearer methodological detail and external dataset testing.\n\n**Decision Recommendation**  \n**Major Revision.** The work presents promising preliminary results but requires expanded analysis, methodological clarification, and additional validation to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis manuscript presents a novel two-stage approach for automating lung nodule segmentation using transformer models. In the data preprocessing phase, Maximum Intensity Projection (MIP) enhances spatial features, helping to distinguish nodules from bronchioles and vessels in CT images. Next, region proposal bounding boxes are generated using the Deformable-DETR model. In Stage 2, these bounding boxes are processed by the SAM model to achieve pixel-level segmentation. To address class imbalance within the dataset, focal loss is incorporated into the original DETR loss function. The results demonstrate superior performance compared to state-of-the-art (SOTA) methods.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nIn preprocessing, maximum intensity projection is applied across slices to enhance visibility. The two-stage framework, combining DETR and SAM models, offers a straightforward approach. Additionally, the common issue of class imbalance in medical datasets is addressed. As a result, segmentation performance is significantly improved. The paper is well organized.\n\n### Weaknesses\n\nThis manuscript lacks novel insights, as the deep learning models used in each stage are well-established, and focal loss is widely applied across various domains. Additionally, the ROI-based segmentation approach is considered somewhat conventional. There is no ablation studies on original SAM performance and some critical models to compare or discuss are missing.\n\n### Questions\n\n1. What is the technical insight of this work? The concept of ROI-based segmentation is not new, and both DETR and SAM are well-established models. As a result, the framework appears to lack novelty, which is a critical concern.\n\n2. There are several existing studies on lung nodule segmentation in CT images. For instance, the IEEE TMI paper, \"Closing the Gap between Deep Neural Network Modeling and Biomedical Decision-Making Metrics in Segmentation via Adaptive Loss Functions,\" addresses not only lung segmentation but also class imbalance. It would be beneficial for the authors to compare or discuss their work in relation to such prior studies.\n\n3. Does the model function in an end-to-end learning manner? If it does or does not, the authors should provide a discussion on the merits and limitations of the learning method used in this framework.\n\n4. Which stage of the framework provides the most significant performance improvement? \n\n5. Additionally, what is the baseline performance of the original SAM model pretrained with MedSAM? Are the datasets used in MedSAM aligned with the LUNA dataset, and how does this impact model performance?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript proposes a two-stage approach for automating lung nodule segmentation using transformer models. The first stage employs Maximum Intensity Projection (MIP) for data preprocessing, which improves spatial feature visibility to differentiate nodules from bronchioles and vessels in CT images. In the second stage, bounding boxes generated using the Deformable-DETR model are processed by the SAM model for pixel-level segmentation. The integration of focal loss into the DETR loss function addresses class imbalance within the dataset, and the reported results indicate superior performance compared to state-of-the-art (SOTA) methods.\n\n### Major Comments\n1. **Novelty**: The manuscript lacks novel insights, as the deep learning models employed (DETR and SAM) are well-established in the field, and focal loss is commonly utilized across various applications. This raises significant concerns regarding innovation.\n2. **Evaluation Design**: The ROI-based segmentation approach is somewhat conventional and fails to present innovative methodologies or direct comparisons with critical studies. Moreover, no ablation studies are conducted to evaluate the original SAM performance.\n3. **Comparisons**: Missing discussions regarding existing literature on lung nodule segmentation, like the IEEE TMI paper that tackles segmentation and class imbalance, should be included for context.\n4. **Learning Method**: There is no discussion on whether the model functions in an end-to-end learning manner, and further exploration of the advantages and limitations of the chosen learning method is warranted.\n5. **Performance Metrics**: Clarification is needed on which stage of the framework contributes most significantly to performance enhancement, as well as baseline performance metrics for the original SAM model pretrained with MedSAM.\n\n### Minor Comments\n- The paper is well organized, but clarity in some technical explanations could be enhanced.\n- Figures could benefit from clearer labels and descriptions.\n- Avoid typographical errors and ensure consistent notation and acronyms throughout the manuscript.\n\n### Summary Paragraph\nOverall, the work demonstrates potential significance in lung nodule segmentation but suffers from limitations in innovation and novelty, as established methods dominate the proposed framework. The evaluation design lacks comparative analysis with current literature, impacting the reproducibility and broader applicability of results. While the manuscript is organized and addresses class imbalance effectively, it would benefit from deeper exploration of the model's performance and methodologies applied.\n\n### Decision Recommendation\nMinor revision is recommended to address the highlighted concerns and improve overall clarity and depth of the analysis.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a two-stage framework for automated lung nodule segmentation from CT scans, combining a Deformable-DETR model for region proposal with the SAM model for pixel-level segmentation. Preprocessing employs Maximum Intensity Projection (MIP) to enhance nodule visibility and distinguish lesions from vascular and bronchial structures. The method incorporates focal loss to mitigate dataset imbalance and reports superior segmentation performance relative to state-of-the-art methods. Overall, the paper is clearly written and well organized.  \n\n**Major Comments**  \n1. **Novelty and Technical Insight:** The approach offers limited novelty since both DETR and SAM are established models, and ROI-based segmentation strategies are widely used. The manuscript would benefit from a clearer articulation of technical innovation beyond model combination.  \n2. **Lack of Ablation and Comparative Analyses:** No ablation studies are provided to assess the contributions of individual components, such as the original SAM versus the proposed integration. Comparative analysis with key prior works‚Äîparticularly those addressing class imbalance with adaptive loss functions‚Äîis missing.  \n3. **Framework Discussion:** Clarification is needed on whether the model operates in an end-to-end manner, along with a discussion of advantages or drawbacks of the chosen training strategy.  \n4. **Performance Attribution:** The paper does not specify which stage of the framework contributes most significantly to performance gains; such insight would strengthen the technical interpretation.  \n5. **Dataset and Pretraining Alignment:** The relationship between the datasets used for MedSAM pretraining and the LUNA dataset should be discussed to contextualize performance improvements and ensure fair comparison baselines.  \n\n**Minor Comments**  \n- Minor language and formatting adjustments could improve presentation clarity.  \n- Ensure that all models and acronyms are defined at first mention.  \n- Figures and tables could more explicitly indicate the comparative baselines.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses a meaningful medical imaging problem with a well-structured pipeline and solid implementation. However, its significance is limited by modest methodological innovation. The evaluation lacks sufficient ablation and comparative experimentation to convincingly establish novelty or performance attribution. Presentation and reproducibility are good, but clearer methodological detail and dataset transparency are needed for validation.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** ‚Äì  \n‚úÖ The paper presents a methodological contribution within medical imaging, focusing on a novel two-stage transformer-based framework for CT nodule segmentation. The combination of Deformable-DETR for localization and fine-tuned SAM for pixel-level segmentation constitutes a methodological pipeline, aligning with TMI‚Äôs emphasis on imaging computation rather than clinical application alone.\n\n2. **Novelty & Contribution Level** ‚Äì  \nModerate-to-strong novelty. The integration of Deformable DETR and MedSAM within a unified workflow for sparse nodule segmentation is new, especially accompanied by dedicated preprocessing (7.5mm MIP) and the use of focal loss to address extreme data imbalance. Nonetheless, each component (DETR, SAM, focal loss, MIP) is known; thus, the innovation primarily stems from architectural integration and adaptation for real-world, low-prevalence CT data. The level of methodological insight is solid but not fully foundational ‚Äî best classified as a hybrid systems contribution.\n\n3. **Technical and Experimental Rigor** ‚Äì  \nGenerally rigorous: dataset preparation and preprocessing are transparently described; the LUNA16 dataset is correctly characterized. Proper train/validation/test splitting, balance between realism and feasibility, and inclusion of augmentation are clear strengths. Some empirical concerns remain: only a single dataset (LUNA16) was used, and no cross-dataset generalization or ablation beyond Œ≥/Œ± tuning is presented. The grounding in 2D MIP versus 3D context warrants further comparison. Uncertainty analysis or statistical significance testing is absent.\n\n4. **Clarity and Presentation** ‚Äì  \nHigh clarity and polished narrative; figures are described adequately, although small inconsistencies in reference formatting and equation typography exist. Some long sentences could be edited for conciseness. The presentation style is coherent and close to IEEE standard.\n\n5. **Ethical and Reproducibility Compliance** ‚Äì  \nDataset is open (LUNA16), ensuring compliance; however, there is no explicit mention of code availability or reproducibility plan. IRB not applicable as no private patient data were used.\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n### 1. Summary\nThis manuscript introduces a two-stage transformer-based framework for automated segmentation of lung metastases on CT. Stage‚ÄØ1 employs a customized Deformable‚ÄØDETR detector with focal loss on 7.5‚ÄØmm‚ÄØMIP slices to propose bounding boxes, addressing tumor sparsity and class imbalance. Stage‚ÄØ2 refines these proposals using a fine‚Äëtuned variant of MedSAM to generate pixel‚Äëlevel masks. Tested on the LUNA16 dataset, the approach achieved a Dice‚ÄØ‚âà‚ÄØ92‚ÄØ% and sensitivity‚ÄØ‚âà‚ÄØ95‚ÄØ%, outperforming recent CNN‚Äëbased baselines. The work aims to integrate into tumor‚Äëboard workflows for decision support.\n\n### 2. Strengths\n- Strong alignment with current interests in transformer architectures for medical imaging.\n- Clear methodological articulation and preprocessing transparency (resampling, MIP generation, augmentation).\n- Quantitative improvement over published baselines on a standard dataset.\n- Addressing clinical realism by preserving low nodule prevalence in the test set.\n- Writing is accessible and figures assist conceptual understanding.\n\n### 3. Weaknesses\n- Limited validation: only one dataset and no external testing or cross‚Äëcomparison to human annotators.\n- Methodological innovation lies in pipeline integration rather than a new theoretical contribution.\n- Absence of ablation or sensitivity analyses (e.g., effect of MIP slab thickness, focal loss parameters, SAM fine‚Äëtuning depth).\n- Unclear computational cost and inference time, crucial for clinical workflows.\n- Reproducibility discussion (code/data policy) lacking.\n- The ‚Äútumor‚Äëboard‚Äù framing is more aspirational than demonstrated.\n\n### 4. Major Comments\n1. **Clarify Novelty Relative to Prior DETR/SAM Applications** ‚Äì The manuscript would benefit from explicitly differentiating how the joint DETR‚ÄìSAM pipeline qualitatively advances beyond sequential detection‚Äësegmentation schemes in existing literature.  \n2. **Include External or Cross‚ÄëValidation Dataset** ‚Äì Demonstrating performance on at least one additional dataset (e.g., LIDC‚ÄëIDRI full set, or variation in CT reconstruction kernels) is essential for TMI to assess methodological robustness.  \n3. **Ablation and Sensitivity Analysis** ‚Äì Quantify contributions from each component (MIP, focal loss, deformable attention, two‚Äëstage structure).  \n4. **Computational Efficiency** ‚Äì Provide benchmark timings (inference per scan, GPU type, memory footprint).  \n5. **Statistical Rigor** ‚Äì Report confidence intervals or statistical tests across folds; a single best Dice value is insufficient at TMI standard.  \n6. **Reproducibility Statement** ‚Äì Add explicit mention of code availability, trained model weights, and details sufficient for replication.  \n7. **Clinical Integration Claim** ‚Äì Tone down assertions regarding ‚Äútumor board application‚Äù unless validated by user study or workflow integration trial.\n\n### 5. Minor Comments\n- Correct minor grammatical inconsistencies (e.g., ‚Äúis impractical as it is time‚Äëconsuming‚Äù ‚Üí streamline).  \n- Ensure consistent superscript formatting of equations and symbols.  \n- Add journal and year details to some references; check duplicated Lin‚ÄØet‚ÄØal.‚ÄØ(2017a/2017b) citations.  \n- Figures‚ÄØ1‚Äì4 should include scale bars and resolution information.  \n- Re‚Äëevaluate section numbering (minor spacing and caption formatting per IEEE guidelines).\n\n### 6. Overall Recommendation\n**Major‚ÄØRevision** ‚Äì The paper is promising and within TMI scope but needs additional methodological validation and experimental depth (cross‚Äëdataset testing, ablation, performance metrics) to meet TMI‚Äôs rigor and generalizability standards. If these are addressed, publication could be justified as a substantive systems innovation paper.\n\n### 7. Confidence Level\n**4‚ÄØ/‚ÄØ5** ‚Äì Reviewer is confident in the assessment based on experience with transformer‚Äëbased segmentation methods and TMI publication standards.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents a two-stage transformer-based approach for lung nodule segmentation in CT scans to support tumor board applications. The method addresses class imbalance challenges where nodules appear in only 5% of CT slices. Stage 1 employs a custom Deformable Detection Transformer with Focal Loss for region proposal to localize sparse tumors. Stage 2 uses a fine-tuned Segment Anything Model (SAM) initialized with MedSAM weights for pixel-wise segmentation refinement. The preprocessing pipeline includes 7.5mm Maximum Intensity Projection (MIP) to enhance nodule visibility and differentiate nodules from vascular structures. Evaluated on LUNA16 dataset, the method achieves 92.4% Dice coefficient, 95.2% sensitivity, and 93.2% precision, with an F1 score of 94.2% for region proposal. The approach demonstrates superior performance compared to existing CNN and hybrid CNN-transformer methods, particularly for medium and large nodules.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and notation issues**\n  - Equation (1) presents resampling factor R with inconsistent vector notation, showing R as both a vector and individual ratios without clear dimensional specification (Page 3, lines 117-120), which creates ambiguity in implementation\n  - The computational complexity formula O(H¬≤W¬≤C) in Section 3.3 incorrectly states C=1 for grayscale images when describing feature map channels from ResNet-50 backbone (Page 4, lines 214-216), as ResNet-50 typically outputs multi-channel feature maps\n  - Focal loss parameters Œ≥=2 and Œ±t=0.25 are mentioned as optimal through hyperparameter tuning (Page 7, lines 340-343) but no systematic evaluation or comparison with other parameter combinations is provided\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - Training set contains 12.7% nodule-positive images while test set has only 5% (Page 4, lines 180-184), creating a significant domain shift that may overestimate real-world performance and compromise model generalizability\n  - Stage 2 evaluation lacks independent assessment, as Table 1 shows only overall Dice coefficient of 92.4% without breakdown by nodule size categories (Page 6, Table 1), preventing detailed analysis of segmentation quality across different tumor characteristics\n  - Comparison with existing methods in Table 2 lacks statistical significance testing and confidence intervals (Page 7, Table 2), making it difficult to assess whether reported improvements are statistically meaningful\n\n‚Ä¢ **Technical implementation and reproducibility concerns**\n  - MIP slab thickness of 7.5mm selection is mentioned as \"suitable compromise\" (Page 3, lines 138-140) but no systematic evaluation of different thickness values or quantitative justification is provided\n  - Stage 1 and Stage 2 training details are incompletely specified, with missing information about convergence criteria, early stopping, and final model selection procedures (Page 5, Section 3.5), hindering reproducibility\n  - The claim of being \"trained from scratch\" for Deformable-DETR (Page 4, line 199) contradicts the use of ResNet-50 backbone, which typically uses pre-trained weights, creating confusion about the actual training methodology\n\n‚Ä¢ **Limited clinical validation and generalizability assessment**\n  - Evaluation is restricted to LUNA16 dataset only, with no validation on independent clinical datasets or multi-center data (throughout results section), limiting evidence for real-world tumor board applicability\n  - No analysis of computational requirements, inference time, or practical deployment considerations for clinical integration (no direct evidence found in the manuscript), despite the stated tumor board application focus\n  - Missing discussion of failure cases, edge conditions, or limitations in detecting specific nodule types or anatomical locations (no direct evidence found in the manuscript), which is crucial for clinical decision support systems\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify mathematical formulations and provide rigorous notation**\n  - Revise Equation (1) to use consistent vector notation and specify dimensional units clearly, providing implementation pseudocode to eliminate ambiguity in the resampling procedure\n  - Correct the computational complexity analysis by accurately describing the multi-channel feature maps from ResNet-50 and provide detailed complexity comparison with standard DETR\n  - Include systematic hyperparameter sensitivity analysis for focal loss parameters, showing performance curves across different Œ≥ and Œ±t values with statistical error bars\n\n‚Ä¢ **Strengthen experimental design and provide comprehensive evaluation**\n  - Conduct additional experiments with matched nodule prevalence between training and test sets, and report performance degradation analysis when prevalence varies to better estimate real-world applicability\n  - Provide detailed Stage 2 evaluation breakdown by nodule size categories, including per-class Dice coefficients, hausdorff distances, and boundary accuracy metrics to enable thorough segmentation quality assessment\n  - Add statistical significance testing with confidence intervals for all performance comparisons, including bootstrap sampling or cross-validation to establish statistical validity of reported improvements\n\n‚Ä¢ **Enhance technical documentation and ensure reproducibility**\n  - Conduct systematic evaluation of MIP thickness parameters (e.g., 5mm, 7.5mm, 10mm, 15mm) with quantitative metrics showing trade-offs between nodule visibility and structural overlap\n  - Provide complete training specifications including convergence criteria, validation monitoring, learning curves, and model selection procedures, along with computational resource requirements and training time analysis\n  - Clarify the exact pre-training strategy for all model components, specify which weights are initialized versus trained from scratch, and provide detailed architecture modifications made to standard Deformable-DETR\n\n‚Ä¢ **Expand clinical validation and address practical deployment considerations**\n  - Validate the approach on additional independent datasets beyond LUNA16, including multi-center clinical data and different CT scanner manufacturers to demonstrate broader generalizability\n  - Include comprehensive computational performance analysis with inference times, memory requirements, and hardware specifications needed for practical tumor board integration\n  - Provide detailed failure case analysis, including examples of missed nodules, false positives, and challenging anatomical scenarios, along with guidelines for clinical interpretation and limitations",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces a novel two-stage approach for lung nodule segmentation in CT scans, aimed at enhancing the workflow efficiency of lung tumor boards. The method employs a custom Deformable Detection Transformer (Deformable-DETR) with Focal Loss for region proposal and refines these proposals using a fine-tuned Segment Anything Model (SAM) for pixel-wise segmentation. The authors claim that their approach effectively handles the extreme class imbalance typical of lung nodule datasets, achieving a Dice coefficient of 92.4% and high sensitivity and precision on the LUNA16 dataset. The manuscript is well-written, and the motivation for addressing the challenges of nodule sparsity and class imbalance is clearly articulated.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: The proposed two-stage approach leveraging Deformable-DETR and SAM is innovative, particularly in the context of lung nodule segmentation. However, the manuscript does not sufficiently discuss how this approach compares to recent advancements in transformer-based medical imaging models. A more comprehensive comparison with existing works would strengthen the novelty argument.\n   \n2. Evaluation Design: The experiments are conducted exclusively on the LUNA16 dataset, which, while standard, limits the generalizability of the results. Broader validation on multiple datasets, including those with varying degrees of nodule sparsity, would be necessary to substantiate the claims of robust performance across different clinical settings.\n\n3. Comparisons: The manuscript lacks a detailed comparison with recent transformer-based models and other state-of-the-art methods that address class imbalance. Including additional baselines would provide a clearer picture of the relative performance and advantages of the proposed method.\n\n4. Reproducibility: While the authors state that code will be released, the current manuscript lacks sufficient detail regarding training protocols, preprocessing steps, and model hyperparameters. Providing a clear and detailed methodology is crucial for reproducibility.\n\n###\n\n## Minor Comments\n1. Figure Clarity: Figures 1 and 4 are somewhat cluttered. Simplifying these figures by focusing on fewer representative slices and providing zoomed-in regions would enhance readability.\n   \n2. Notational Consistency: The notation introduced in Section 2.1 is inconsistent and lacks sufficient explanation, particularly for the forward operator. Clarifying this notation would aid in understanding the mathematical formulations.\n   \n3. Acronym Definitions: Several acronyms (e.g., \"R=4\") are used without definitions. Defining these acronyms would improve clarity for readers.\n   \n4. Typographical Errors: There are typographical issues throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). These should be corrected.\n\n###\n\n## Summary Paragraph\nThe manuscript addresses the significant clinical challenge of accurately segmenting lung nodules in CT scans, a task complicated by extreme class imbalance and nodule sparsity. The proposed two-stage approach combining Deformable-DETR and SAM shows promise in handling these challenges. However, the evaluation is limited to a single dataset, and the manuscript could benefit from a more thorough comparison with recent transformer-based models. The reproducibility of the approach is also uncertain due to incomplete methodological details. Overall, while the idea is promising, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor Revision: The authors should expand comparative analysis, broaden validation across multiple datasets, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Hooman Ramezani"
    ],
    "url": "pdfs/iclr.cc-2025-conference_3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "remote_url": "https://openreview.net/pdf/3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "A New 3D Image Block Ranking Method Using Axial, Coronal and Sagittal Image Patch Rankings for Explainable Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "interpretability and explainable AI"
    ],
    "keywords": [
      "convolutional neural networks",
      "feature selection",
      "gradcam",
      "medical imaging",
      "disease diagnosis",
      "image classification"
    ],
    "abstract": "Although a 3D Convolutional Neural Network (CNN) has been applied to explainable\nmedical imaging in recent years, understanding the relationships among input\n2D image patches, input 3D image blocks, extracted feature maps, top-ranked\nfeatures, heatmaps, and final diagnosis remains a significant challenge. To help\naddress this important challenge, firstly, we create a new 2D Grad-CAM-based\nmethod using feature selection to produce explainable 2D heatmaps with a small\nnumber of highlighted image patches corresponding to top-ranked features. Secondly,\nwe design a new 2D image patch ranking algorithm that leverages the newly\ndefined feature matrices and relevant statistical data from numerous heatmaps to\nreliably rank axial patches, coronal patches, and sagittal patches. Thirdly, we create\na novel 3D image block ranking algorithm to generate a ‚ÄúBlock Ranking Map\n(BRM)‚Äù by using the axial patch ranking scores, coronal patch ranking scores, and\nsagittal patch ranking scores. Lastly, we develop a hybrid 3D image block ranking\nalgorithm to generate a reliable hybrid BRM by using different block ranking\nscores generated by the 3D image block ranking algorithm using different top feature\nsets. The associations between brain areas and a brain disease are reliably\ngenerated by using hybrid information from ChatGPT and relevant publications.\nThe simulation results using two different 3D data sets indicate that the novel hybrid\n3D image block ranking algorithm can identify top-ranked blocks associated\nwith important brain areas related to AD diagnosis and autism diagnosis. A doctor\nmay conveniently use the hybrid BRM with axial, coronal, and sagittal views\nto better understand the relationship between the top-ranked blocks and medical\ndiagnosis, and then can efficiently and effectively make a rational and explainable\nmedical diagnosis.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nAuthors present a method for modifying Grad-CAM feature attribution maps that is able to identify the most important 'blocks' in a 3D MRI brain image. The method is applied to output features of a trained CNN by sectioning each 3D image axis into an equal number of patches. The features from Grad-CAM are passed through a feature selection (FS) method consisting of a combination of standard library functions including recursive feature elimination (RFE). The resulting `k` ranked features are accumulated axis- (or patch-) wise, and the patches from different combinations of FS methods are passed into final step that aggregates the 2D features into 3D block features. The blocks that contain the most patches with highest ranking features are selected as the most important blocks. Authors evaluate their method by verifying that the selected blocks correspond with those that are know to be important in the literature, as well as asking Chat-GPT. Author's provide clinical reasoning to explain the attributions.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* The target of the Authors' work is important and moves towards a more explainable and trustworthy result for use by clinicians.\n* Authors are thorough in their definitions and attempt to give the reader the detail to reproduce their work.\n\n### Weaknesses\n\n* In general, this paper is badly formatted and overly verbose. The notation is difficult to keep consistent and often badly defined. Further comments to this are made in the `Questions` section below.\n* Authors spend very little time reviewing prior work and setting their method in context - Authors should add explanations of related prior work including methods for using statistical analysis of Grad-CAM attribution and saliency maps to identify significant regions in input data.\n* Authors inexplicably use Chat-GPT to test the reliability of their method by asking it to verify the important brain regions associated with AD - there is no explanation for this, and no reason to be doing that instead of actually asking clinicians.\n* Authors present their method only on a single 3D MRI dataset and do not discuss its applicability to other modalities, or indeed any other domain. Authors cannot claim that this is a method for \"Explainable Medical Imaging\" when results on a single dataset are reported.\n\n### Questions\n\n### Major Comments\n* The definitions and notation presented in the paper are cumbersome, and difficult to read. This starts with the statement `P (H-bar/H) x (W-bar/W) patches for P=HW` (line 92) that is repeated several times (187, 214 etc.) which could be replaced by a less obtuse definition of patch size. Authors also do not define _n_ in this Section 2. Authors should better define their variables. Additionally, the cumbersome notation in definitions 1-6 is largely unnecessary - Authors can simplify this section by removing superfluous 'definitions' and describing the process through which they yield the ranking matrices: this will avoid repetition of the `i` and `j` indices and shorten this bloated paper.\n* On a similar note, Authors introduce additional notation for the 'top feature map' `T^Q`. This terminology is confusing. It is not a top \"feature map\", but rather an aggregated \"top k features\" map combining the Grad-CAM features selected by 'some feature selection method'. Authors should consider re-wording this.\n* Lines 221-234 - Authors present their 8 steps for Image Block Ranking algorithm. This is presented badly - the reader is capable of understanding that the same steps are applied to the 3 different axis without making each step of this method so verbose. Figure 1 shows this much better in fact. Authors should describe steps 1-7 on a single axis to improve readability.\n* Authors show a visualization of important blocks identified by their method in Figure 2b. The full Grad-CAM output without feature selection is shown in Figure 3b. It is evident that applying some thresholding to the full feature map, and even just applying the brain-boundary regional constraints, would yield a similar map to their own. Can author's comment on the significant differences between using the Grad-CAM values directly in this way, rather than the additional steps in their method? It would have been helpful to show some quantitative comparison in their results given this is supposed to be an extension of Grad-CAM -based methods.\n* Why did Authors use Chat-GPT in their evaluations? What is the benefit of this over asking the clinicians which Authors repeated claim that this system is aimed at helping?\n* Have Authors used this method on other domains or modalities to demonstrate its effectiveness?\n\n### Minor Comments\n* The paragraph from lines 57 contains a lot of repetition and should be pared down: this sentiment is reflected in many parts of the paper. The reader is clear that having better explainability is important and that a clinician can use this information to inform their diagnosis.\n* Line 147 \"... makes a more impact on the decision\" - to what decision are the Authors referring?\n* Line 148 \"... we use a trained CNN to generate L heatmaps...\" - the language here is confusing: Authors are referring to the model on which they are performing Grad-CAM, not a random trained model that generates heatmaps.\n* Table 2 is unclear - why is much of the table blank?\n* Line 489 - this is confusing: what is meant by the 'the 9th ... patches are shown'?. There are 16x16 patches in each of the 3 axis images shown, and a single patch at (6, 10, 9) is highlighted.\n* Section 4.1 - it is absolutely unnecessary to bloat this section with the indicies that correspond to the brain boundary - put this in the appendix, or show it as an image if the Authors feel it adds to their explanation.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nI question the validity of using Chat-GPT in this work - but no significant Ethics concerns.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a method for enhancing Grad-CAM feature attribution maps to identify significant 'blocks' in 3D MRI brain images. The proposed approach segments each 3D image axis into patches and applies a feature selection technique, including recursive feature elimination (RFE), to rank features. The selected patches are consolidated into 3D block features based on their importance. The authors assess their method's validity by comparing the identified blocks to established critical brain regions in the literature and by consulting Chat-GPT for additional verification. While the contribution towards improving explainability in medical imaging is notable, the manuscript's clarity and organization require significant enhancement.\n\n**Major Comments:**\n1. The definitions and notation used throughout the paper are cumbersome and inconsistent, creating unnecessary complexity. The notation in line 92, repeated throughout, could be simplified for clarity. Additionally, variables such as _n_ in Section 2 remain undefined.\n2. The term 'top feature map' `T^Q` leads to confusion; it should accurately represent 'aggregated top k features' derived from Grad-CAM.\n3. The explanation of the Image Block Ranking algorithm is overly verbose; a concise description focusing on a single axis would enhance readability.\n4. There is a lack of quantitative comparison between the proposed method and traditional Grad-CAM outputs, which is essential given that this work claims to extend Grad-CAM methodologies.\n5. The rationale for utilizing Chat-GPT for evaluations is unclear. Direct consultations with clinicians would be more fitting for the method's intended purpose.\n6. There is no discussion of the method's applicability to other modalities or domains beyond the presented dataset.\n\n**Minor Comments:**\n- The paragraph on lines 57 is repetitive and could be condensed to enhance clarity.\n- The statement in line 147 regarding 'impact on the decision' is vague and requires specification.\n- The phrase in line 148 is misleading; the model referred to should be clearly defined as the one under Grad-CAM analysis.\n- Table 2 is unclear due to many blank entries; clarification is needed.\n- Line 489's reference to 'the 9th ... patches are shown‚Äô is confusing and requires adjustment.\n- Section 4.1 includes excessive details regarding indices related to the brain boundary, which may be better suited for an appendix or visual representation.\n\n**Summary Paragraph:**  \nThis manuscript addresses significant challenges in explainable medical imaging and aims to provide methods that enhance the interpretability of CNN outputs. However, there are critical weaknesses, including issues with notation, a lack of thorough analysis of related work, and an unclear evaluation design. While the overall significance of the research is commendable, its innovation is limited by the narrow focus on a single dataset, which undermines reproducibility. The authors need to address these issues to strengthen their contribution.\n\n**Decision Recommendation:**  \nMajor revision is recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a method to modify Grad-CAM feature attribution maps for identifying the most important 3D ‚Äúblocks‚Äù within MRI brain images. The approach partitions each 3D image into patches, applies recursive feature elimination and other selection methods to Grad-CAM outputs, and aggregates results across axes to identify key regions. The study compares these identified blocks to known disease-relevant regions and includes additional qualitative verification using ChatGPT and clinical interpretations. While the research target‚Äîenhancing interpretability of CNN outputs for clinical use‚Äîis valuable, the paper suffers from formatting and clarity issues that hinder comprehension and reproducibility.\n\n---\n\n**Major Comments**  \n1. **Notation and definitions:** The mathematical notation is cumbersome and inconsistently defined. Key variables (e.g., _n_) are missing, and Definitions 1‚Äì6 add unnecessary complexity. Simplifying notation and focusing on the process leading to ranking matrices would improve readability.  \n2. **Terminology clarity:** The ‚Äútop feature map‚Äù \\(T^Q\\) terminology is misleading‚Äîit refers to aggregated top _k_ Grad-CAM features rather than a feature map. Rewording is recommended.  \n3. **Algorithm presentation:** The 8-step ‚ÄúImage Block Ranking‚Äù procedure is verbose and repetitive. Describing the process for one axis and referencing repetition across others would streamline presentation; Figure 1 already conveys this clearly.  \n4. **Evaluation design:** The comparison between the proposed method and direct Grad-CAM thresholding is lacking. Authors should explain whether the additional processing meaningfully improves interpretability, ideally through quantitative assessment.  \n5. **Validation approach:** Using ChatGPT for validation is inappropriate and unexplained. Clinician input would provide more credible evaluation for a medically oriented study.  \n6. **Generality and scope:** The method is tested on a single MRI dataset without discussion of applicability to other modalities or domains, limiting claims of generalizability.\n\n---\n\n**Minor Comments**  \n- Many paragraphs (e.g., line 57) are repetitive and could be condensed.  \n- Clarify ‚Äúmakes a more impact on the decision‚Äù (line 147) and specify what decision is referenced.  \n- Rephrase ‚Äúwe use a trained CNN to generate L heatmaps‚Äù (line 148) to specify that Grad-CAM is applied to a pre-trained network.  \n- Table 2 lacks clarity‚Äîexplain blank cells.  \n- Line 489: clarify description of patch indices; only (6, 10, 9) is highlighted out of 16√ó16√ó16.  \n- Section 4.1 excessively details brain boundary indices; move to appendix or depict graphically.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The goal of improving model explainability is important for clinical adoption.  \n**Innovation:** The concept of aggregating Grad-CAM outputs via feature selection is somewhat novel but not convincingly differentiated from simpler thresholding.  \n**Evaluation:** Current validation is weak and relies on ChatGPT rather than expert review or quantitative metrics.  \n**Reproducibility:** Although detailed, the overcomplicated notation and unclear definitions hinder reproducibility. Overall, the manuscript requires substantial revision before it can meet publication standards.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper attempt to provide a more explainable 3D GradCam map by combining ideas of GradCam in each of the respective projections (coronal, axial, sagittal) of MRI data with feature selection concepts. The feature maps produced in the maxpooling convolutional layers are used to derive various representations such as heatmap matrics, feature matrices, and the values within them are ranked individually per view and then combined into a 3D block ranking. The authors claim the resulting visualization gives better indication of the disease and demonstrate this on ADNI data for Alzheimer's disease. \n\nThe paper is poorly written and many of the details seem to be automatically written through a translation software or perhaps LLM judging by the language used. For example, reading the abstract had a lot of details  that typically seen in results sections later rather than focusing on a high-level summary of the approach.  Another example is a sentence in line 164-165 which reads \"Different from traditional CAM-based methods without FS a new FS-Grad-CAM methods uses a FS method to select the top k features from m flattened features.\" - Is this referring to their proposed approach. Normally we would phrase it as \"Unlike traditional CAM-based methods, we propose a new method called FS-Grad_CAM where we employ a feature selection method to select the top K feature from m flattened features first before applying GradCAM.\" \n\n Many details are unclear including the novelty with respect to other 3DGradCam methods (see eblow). Overall, it needs a major rewrite and using clinically relevant terminology with better motivation of the healthcare problem addressed.\n\n### Soundness: 1\n\n### Presentation: 1\n\n### Contribution: 1\n\n### Strengths\n\nPaper is about explainable AI showing clinicians relevant features useful for diagnosis in the 3D MRI images by taking slices in multiple views and offering top-ranked patches that are correlated with disease and understand the relationship between the top blocks and the decisions made by 3D CNN. As such, the paper attempts an important problem, namely, making the disease classification more explainable to clinicians. The familiar mechanisms of GradCam are used and attempt is made to process complex MRI datasets in multiple views. If the method could be clearly explained, one could even see value in the technique for 3DGradCam in general although other 3D GradCam tools are available. The main argument appears to be that GradCam should be applied after feature selection in the feature maps.\n\n### Weaknesses\n\nAs mentioned above, the paper is poorly written to determine if the idea being proposed is a variation of 3D gradCam. No comparisons are made to any methods to even see its merit. Several open questions arise (see below). \nThere are several tools available for 3DGradCam such as these. If they are not relevant for comparison, it would be helpful to at least explain how your method differs from these. \nhttps://github.com/fitushar/3D-Grad-CAM\nhttps://www.researchgate.net/publication/357899396_Automated_grading_of_enlarged_perivascular_spaces_in_clinical_imaging_data_of_an_acute_stroke_cohort_using_an_interpretable_3D_deep_learning_framework\n\nThe paper in current problem needs a full rewrite starting with explanation of the MRI disease visualization problem, the role of existing 3D GradCam and the need for feature selection prior to GradCam. The whole idea of class activation maps was to allow us to see the rationale for the classification with the visualization itself in a way doing regional feature selection. By applying a separate feature selection operator a priori, what would be the impact on the gradient operators and the resulting activation maps?\n\nChatGPT is briefly mentioned and it is not clear what it is being used for.\nWhat does it mean to say ChatGPT is used to verify if a brain area is associated with a disease? What is the prompt used? What are the input, only text or text and image, is a bounding box and a prompt given as input. How accurate is ChatGPT in identifying  the brain areas associated with the disease. All these should be added to explain the use of ChatGPT.\n\n### Questions\n\n1. What is the rationale for a top-ranked 3Dimage block being correlated with diseases? Real-life experiences with 2d heat maps alone indicate they are not always a reliable indicator of a disease. Since a disease may be seen in some view better than other views, the method of fusion is important between the 3 views. Provide more discussion or justification for the correlation between top-ranked blocks and diseases, and the fusion method would help clarify better. \n\n2. How is the ranking of the blocks done? In general, what does ranking mean in your context, is it just a matter of selecting high-valued entries in the feature and heat map matrices?\n\n3. What is the purpose of ChatGPT in the work? It is said it is to verify if a brain area is associated with a disease. How does it work? Are both prompt text and image supplied as input? How accurate is ChatGPT in identifying  the brain areas associated with the disease. This discussion is brief and unconvincing so elaborating on how exactly ChatGPT is used and what its inputs are would explain this section better.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript aims to enhance the interpretability of 3D GradCam maps by integrating GradCam with feature selection techniques across different MRI projections (coronal, axial, sagittal). The authors claim that their visualization approach improves the understanding of Alzheimer's disease by analyzing data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, the paper suffers from significant clarity issues and appears poorly written, which undermines its overall quality.\n\n### Major Comments\n1. **Clarity and Writing Quality**: The paper contains many sentences that seem to be generated through translation software, as seen from the abstract's structure and grammar, which lacks a high-level summary.\n2. **Novelty**: The authors do not clearly articulate how their approach differs from existing 3D GradCam methods. Comparisons with established tools are missing.\n3. **Explanation of Methodology**: There is insufficient discussion regarding how feature selection affects the GradCam outputs and the rationale behind their approach.\n4. **Use of ChatGPT**: The role of ChatGPT in verifying disease association with brain areas is unclear, including its input requirements and accuracy.\n\n### Minor Comments\n- The terminology used throughout the paper often deviates from clinically relevant language, confusing the reader.\n- A specific sentence (lines 164-165) lacks clarity and should be rewritten for better understanding. \n- References to established 3D GradCam tools should be more explicit and detailed.\n- The description of the correlation between top-ranked 3D image blocks and disease lacks supporting discussion.\n\n### Summary Paragraph\nIn terms of the TMI editorial criteria, the manuscript addresses an important healthcare problem concerning explainable AI in clinical settings. However, the significance of the proposed technique is diminished by writing quality issues and lack of clarity about its novelty and methodology. In terms of reproducibility, there are significant gaps in explaining the ranking mechanisms and the role of ChatGPT. Overall, the paper requires substantial revisions to improve its clarity, comprehensiveness, and connection to existing research.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes an explainable AI framework intended to generate more interpretable 3D Grad-CAM visualizations for MRI data by combining projection-wise Grad-CAM maps (axial, coronal, sagittal) with feature selection. The approach ranks activations within convolutional layers per view and fuses them into a 3D block ranking to indicate disease-relevant regions, demonstrated on ADNI data for Alzheimer‚Äôs disease. While the topic‚Äîimproving interpretability of 3D CNNs in medical imaging‚Äîis important, the paper is poorly written, with unclear methodological descriptions and evident linguistic and organizational issues that hinder comprehension.\n\n**Major Comments**  \n1. **Clarity and Writing Quality:** The manuscript appears to suffer from translation or automated text generation issues, leading to confusing or grammatically incorrect statements. The structure and flow do not follow conventions; for instance, the abstract contains result details rather than an overview, and technical sections are ambiguously phrased.  \n2. **Novelty and Relation to Prior Work:** The distinction from existing 3D Grad-CAM methods is unclear. The paper should explicitly compare or conceptualize differences with prior tools such as publicly available 3D Grad-CAM implementations.  \n3. **Methodological Explanation:** The process of feature selection before Grad-CAM is inadequately described. The rationale for combining feature selection with Grad-CAM and how this affects gradient-based activation mapping remains unexplained.  \n4. **Use of ChatGPT:** The inclusion of ChatGPT for verifying disease-region associations is insufficiently justified. The paper must clarify the inputs (text, image, prompts) and discuss the validity and purpose of this step.  \n5. **Experimental Evaluation:** No baseline comparisons or quantitative evaluation are provided. The claimed interpretability improvement is not demonstrated against existing methods.  \n6. **Scientific Rationale:** The claim that top-ranked 3D blocks correlate with disease lacks theoretical or empirical justification. Clarification is needed on the block ranking procedure and fusion across multiple MRI views.\n\n**Minor Comments**  \n- Numerous language errors and mistranslations throughout; a full rewrite is recommended.  \n- Use consistent and clinically appropriate terminology when describing MRI slices and disease visualization.  \n- Clarify notation (e.g., k, m, feature maps) and reformat equations and sentences for readability.\n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The goal of improving interpretability of deep learning for Alzheimer‚Äôs MRI analysis is valuable.  \n*Innovation:* Combining Grad-CAM with feature selection could be novel, but the implementation and differentiation from prior work are unclear.  \n*Evaluation:* No solid experimental validation or comparison is presented, limiting confidence in the method‚Äôs utility.  \n*Reproducibility:* The current text lacks sufficient methodological detail for replication. Major reorganization and clearer exposition are required.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The manuscript requires substantial rewriting for clarity, complete methodological explanation, meaningful comparison to existing approaches, and validation before it can be properly evaluated.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a feature-selected (FS) Grad-CAM method to generate more focused explainable heatmaps with smaller highlighted areas. Additionally, a novel 2D image patch ranking algorithm was developed to reliably rank patches along the axial, sagittal, and coronal axes using features extracted by FS-Grad-CAM. These ranked scores are then used to create a Block Ranking Map (BRM) via a newly developed 3D block ranking algorithm. The resulting block-ranked scores are further refined through a novel hybrid 3D block ranking algorithm to produce a reliable hybrid BRM. The method was validated on Alzheimer‚Äôs Disease (AD) data and identified the top 10 ranked blocks associated with AD.\n\n### Soundness: 1\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper used the 2D images in axial, sagittal and coronal axes to rank the 3D images, which makes the research novel.  This paper has detailed explanation on the algorithm, which makes the method replication easier.\n\n### Weaknesses\n\n1. Lack of competition: The cited papers (He et al., 2019 and Selvaraju et al., 2017) in the introduction part have compared their new algorithm (Grad-CAM) with other common machine learning algorithm (e.g. logistic regression) to demonstrate their better performance in the binary classification. But this paper doesn‚Äôt have compared with any baseline to demonstrate its superiority. There exists 3D medical imaging visual explanation algorithm (e.g. Respond-CAM) which might be a good benchmark to compare with.\n2. Lack of generality: The paper only evaluated on one dataset, which cannot guarantee the generality of the proposed method. More tests are needed to justify the statement.\n\n### Questions\n\n1. It would be great if any qualitative/quantitative comparison with similar algorithm could be added to help the readers to better evaluate the performance of the proposed method.\n2. It would be ideal if more dataset could be used to evaluate the performance of the proposed method (e.g. LUNA 16 lung nodule dataset)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel approach by introducing a feature-selected (FS) Grad-CAM method aimed at generating more precise explainable heatmaps with smaller highlighted areas. In addition, it develops a new 2D image patch ranking algorithm to rank patches across axial, sagittal, and coronal axes using features extracted by FS-Grad-CAM. The authors further enhance this approach with a Block Ranking Map (BRM) generated through a 3D block ranking algorithm, which is refined by a hybrid 3D block ranking algorithm. The method is validated on Alzheimer‚Äôs Disease (AD) data, showing promise in identifying significant blocks related to the disease.\n\n**Major Comments:**\n1. **Lack of Competition:** The manuscript does not compare the proposed algorithm with established baselines such as Respond-CAM, which hinders the assessment of its competitive edge. Relevant citations (He et al., 2019; Selvaraju et al., 2017) illustrate comparisons in their works that highlight the importance of benchmarking against common machine learning algorithms.\n2. **Lack of Generality:** Testing is confined to a single dataset, raising concerns about the broader applicability of the proposed method. Expansion of the evaluation to include multiple datasets is necessary to substantiate the claims made.\n\n**Minor Comments:**\n- Consider adding qualitative or quantitative comparisons to enhance the evaluation of the proposed method.\n- Additional datasets like the LUNA 16 lung nodule dataset would be beneficial for performance assessment.\n- The explanation of the algorithm is detailed, which aids in replication.\n\n**Summary Paragraph:**  \nIn accordance with the TMI editorial criteria, the manuscript shows innovation through its unique ranking methods and focused explainable heatmaps. However, it falls short on evaluation and generality by not benchmarking against existing methods and limiting testing to a single dataset, which affects its reproducibility. Addressing these weaknesses could enhance the manuscript's overall significance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a feature-selected Grad-CAM (FS-Grad-CAM) approach aimed at generating more localized and interpretable heatmaps. Building on this, the authors develop a 2D image patch ranking algorithm that orders patches across axial, sagittal, and coronal planes and integrates these rankings into a 3D Block Ranking Map (BRM) using a hybrid 3D block ranking algorithm. The work is applied to Alzheimer‚Äôs disease (AD) imaging data to identify the top-ranked brain regions associated with AD. The manuscript is generally clear in describing the algorithmic steps and provides sufficient methodological details to facilitate reproducibility.  \n\n**Major Comments**  \n1. **Lack of Comparative Evaluation:** The study does not include comparisons against baseline methods or existing 3D medical imaging explainability algorithms (e.g., Respond-CAM). Prior works, such as He et al. (2019) and Selvaraju et al. (2017), benchmarked their methods against standard classifiers, demonstrating clear advantages. Similar comparative analysis would strengthen this paper‚Äôs claims.  \n2. **Limited Dataset and Generality:** The method is evaluated only on a single Alzheimer‚Äôs disease dataset, which limits assessment of its generalizability. Validation on additional datasets would provide stronger evidence for the robustness of the approach.  \n\n**Minor Comments**  \n- The explanation of the method is detailed and supports reproducibility.  \n- Including qualitative and quantitative comparisons with existing algorithms would improve the clarity of the method‚Äôs relative performance.  \n- Testing on additional datasets, such as the LUNA16 lung nodule dataset, could enhance the paper‚Äôs scope.  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the paper tackles an important problem in medical image explainability, specifically for 3D data. The **innovation** lies in integrating 2D feature selection with 3D block ranking for improved interpretability. Regarding **evaluation**, the absence of baseline comparisons and use of a single dataset weaken evidence of superiority and generalizability. In terms of **reproducibility**, the methodological description is comprehensive, supporting replication. Overall, while the concept is original, the evaluation requires significant strengthening.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript proposes new algorithms for *explainable 3D medical imaging*, including a feature‚Äëselection‚Äëbased Grad‚ÄëCAM method, a 2D patch‚Äëranking algorithm, and a hybrid 3D image‚Äëblock ranking framework integrating axial, coronal, and sagittal views. These are methodological contributions to medical image analysis and visualization of learned representations‚Äîsquarely within the methodological scope of *IEEE Transactions on Medical Imaging*.\n\n2. **Novelty & Contribution Level**  \nThe integration of feature selection into Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM) and the multiview fusion of 2D patch rankings to form a 3D ‚ÄúBlock Ranking Map‚Äù represent incremental but nontrivial extensions of existing explainability strategies.  \nHowever, the novelty is partly algorithmic engineering rather than theoretical innovation. The extensive definition of matrices and ranking functions demonstrates effort toward formalization, yet conceptual advances beyond existing explainability frameworks (e.g., Grad‚ÄëCAM++, Score‚ÄëCAM, SHAP‚ÄëCAM) are modest. Claims about clinical interpretability through ChatGPT‚Äëassisted verification are unconventional and not well‚Äëfounded scientifically.  \n‚Üí **Borderline for TMI novelty**: between a methodological extension and an application framework.\n\n3. **Technical and Experimental Rigor**  \nThe manuscript includes substantial mathematical notation but limited quantitative validation. There is no ablation or statistical significance test demonstrating that FS‚ÄëGrad‚ÄëCAM or hybrid block ranking improve interpretability or classification beyond baseline CAM methods. The use of ChatGPT to ‚Äúverify‚Äù brain‚Äëregion associations is not scientifically valid for peer‚Äëreviewed evidence. Dataset use (ADNI, Autism datasets) is appropriate, but reproducibility (code, parameters, number of subjects per split, model hyperparameters) is sparse. Overall rigor is *medium*.\n\n4. **Clarity and Presentation**  \nThe paper is generally readable, yet lengthy, repetitive, and formally dense. There are numerous typographic inconsistencies (spacing, equation numbering, variable notation). Figures and tables are described but might be visually crowded. The hierarchical organization could be streamlined. English is adequate but could benefit from editing for conciseness and academic tone.\n\n5. **Ethical and Reproducibility Compliance**  \nPublic datasets (ADNI, Kaggle autism) are cited appropriately. No mention is made of IRB or subject consent, though likely not needed for secondary anonymized data. No code availability statement. Use of generative AI (‚ÄúChatGPT used to verify clinical associations‚Äù) raises *ethical and methodological concern*‚Äîshould be omitted or reframed as literature-based validation only.\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n1. **Summary**  \nThe authors introduce a feature‚Äëselection‚Äëbased Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM) to generate more interpretable 2D heatmaps by focusing on top‚Äëranked features, a five‚Äëfactor 2D patch‚Äëranking algorithm that combines multiple feature and activation matrices, and a hybrid 3D image‚Äëblock ranking approach that fuses axial, coronal, and sagittal patch rankings into a 3D Block Ranking Map (BRM). Applied to ADNI and an autism brain‚ÄëMRI dataset, the framework identifies small 3D ‚Äúimportant blocks‚Äù purportedly associated with disease-related brain regions.\n\n2. **Strengths**\n- Addresses explainability of 3D CNNs‚Äîa current and significant challenge in medical deep learning.\n- Proposes a structured approach combining feature selection and multiview fusion.  \n- Provides detailed definitions and reproducible conceptual framework.  \n- Demonstrates feasibility on two public 3D brain‚ÄëMRI datasets.  \n- Potentially useful as a visualization and interpretability aid.\n\n3. **Weaknesses**\n- Limited scientific novelty; largely an extension of existing Grad‚ÄëCAM/Fusion ideas.  \n- Validation lacks quantitative metrics for interpretability or diagnostic utility.  \n- Reliance on ChatGPT outputs to confirm neuroanatomical associations undermines scientific rigor.  \n- The methods are excessively complicated without theoretical justification and not benchmarked against recent explainability methods.  \n- Writing and formatting need significant refinement.  \n- No error bars, cross‚Äëvalidation, or statistical tests; reproducibility not demonstrated.\n\n4. **Major Comments**\n1. **Quantitative evaluation** ‚Äì Include comparative experiments vs. standard Grad‚ÄëCAM, Grad‚ÄëCAM++, and other explainability methods using objective interpretability metrics or human expert assessment.  \n2. **Remove ChatGPT verification** ‚Äì Replace with expert review or established neuroimaging literature only. Discussion should emphasize model‚Äëderived findings rather than AI text confirmation.  \n3. **Ablation and Sensitivity** ‚Äì Demonstrate contribution of each component: FS‚ÄëGrad‚ÄëCAM, patch ranking factors, and hybrid fusion, ideally with measurable improvement.  \n4. **Method transparency** ‚Äì Provide clear mathematical formulation for the ranking function `f(¬∑)` and weighting scheme; otherwise the method is not reproducible.  \n5. **Visualization and clarity** ‚Äì Condense definitions, properly label equations, unify notation, and illustrate the pipeline with one concise figure.  \n6. **Statistical rigor** ‚Äì Report classification performance with standard deviation over multiple splits and include significance tests.  \n7. **Broader applicability** ‚Äì Evaluate on at least one non‚Äëbrain dataset or discuss generalization to other modalities.  \n\n5. **Minor Comments**\n- Grammar and style require editing for conciseness.  \n- Check repeated references (e.g., Zhou et‚ÄØal. 2024a/b).  \n- Ensure consistent variable naming in equations.  \n- Figures should have readable fonts and color legends.  \n- Provide code/parameter details in Supplementary Material.  \n\n6. **Overall Recommendation:**  \n**Major Revision (borderline below TMI novelty threshold).**  \nWhile the problem is relevant and the approach is conceptually coherent, the paper in current form lacks quantitative evidence, clear methodological novelty, and professional presentation required for *IEEE TMI*. If the authors provide rigorous comparative validation, remove unscientific elements, and substantially clarify exposition, it might reach publishable quality.\n\n7. **Confidence Level:** 4/5 ‚Äì High confidence in assessment based on domain familiarity with deep‚Äëlearning explainability and medical imaging methods.\n\n---\n\n**Summary Statement to the Editor:**  \nThis manuscript tackles an important problem‚Äîexplainable 3D deep learning in neuroimaging‚Äîthrough a highly detailed but somewhat convoluted ranking‚Äëbased approach. Its novelty is moderate and technical rigor insufficient for immediate acceptance at *IEEE TMI*, but with major revision focusing on quantitative and methodological soundness, it could evolve into a useful contribution to interpretability in medical imaging.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses explainability challenges in 3D medical imaging by proposing a novel ranking method for 3D image blocks. The approach combines a new FS-Grad-CAM method with feature selection to generate explainable 2D heatmaps, followed by a 2D patch ranking algorithm using five informative matrices (feature distribution, feature ranking, average feature ranking, heatmap activation, and heatmap strength matrices). The core contribution is a hybrid 3D image block ranking algorithm that integrates axial, coronal, and sagittal patch rankings to create Block Ranking Maps (BRMs). The method is evaluated on ADNI dataset (982 3D brain images) for Alzheimer's disease diagnosis and an autism dataset (286 3D images), achieving testing accuracies of 0.9309-0.9897. Results show that top-ranked blocks correspond to brain regions associated with AD and autism diagnosis, verified through ChatGPT and literature citations. The hybrid BRM enables visualization across three anatomical views for medical interpretation.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical notation inconsistencies and unclear formulations**\n  - Definition 6 appears twice (lines 153 and 162) with different content, creating confusion about heatmap count matrix versus average feature ranking matrix\n  - Equation complexity in Definition 5 (lines 147-150) with floor division notation ¬µ = ‚åäIp/n‚åã lacks clear explanation of how feature index mapping works\n  - The monotonically non-decreasing function Œ∏ij = f(dij, r‚Å∞ij, rÃÑij, uij, sij) in Algorithm 1 (line 225) is never explicitly defined, making reproducibility impossible\n\n‚Ä¢ **Insufficient experimental validation and comparison**\n  - No comparison with existing 3D explainability methods beyond brief mentions of 3DGradCAM (line 176), limiting assessment of relative performance\n  - Evaluation relies heavily on subjective verification through ChatGPT and literature matching (Tables 1-2, pages 7-9) rather than quantitative explainability metrics\n  - Testing accuracies vary significantly (0.9309-0.9897, line 317) across axial/coronal/sagittal views without explanation of this variance or its impact on final rankings\n\n‚Ä¢ **Methodological complexity without clear justification**\n  - The five-factor ranking approach (Algorithm 1, lines 216-228) lacks ablation studies showing individual factor contributions or necessity\n  - Multiple feature selection methods are combined sequentially (Chi2, mutual_info_classif, f_regression, f_classif, RFE, lines 350-356) without justification for this specific ordering\n  - Hybrid ranking requires \"different conditions\" (Algorithm 3, line 296) but these conditions are not systematically defined or validated\n\n‚Ä¢ **Limited scope and generalizability concerns**\n  - Evaluation limited to brain imaging applications (AD and autism) without demonstration on other organ systems or pathologies\n  - Dataset preprocessing constraints (resizing to 64√ó64√ó64, extracting middle slices 22-41, lines 307-309) may not generalize to clinical workflows\n  - Heavy reliance on \"ebrains\" software tool for brain region identification (line 339) creates dependency on specific external tools\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and standardize mathematical formulations**\n  - Resolve the duplicate Definition 6 by renumbering and clearly distinguishing between heatmap count matrix and average feature ranking matrix concepts\n  - Provide explicit mathematical formulation for the ranking function Œ∏ij = f(dij, r‚Å∞ij, rÃÑij, uij, sij) with specific weights or combination rules\n  - Add detailed explanation of the feature index mapping process in Definition 5, including worked examples showing how ¬µ values translate to spatial coordinates\n\n‚Ä¢ **Strengthen experimental validation framework**\n  - Include quantitative comparisons with at least 2-3 existing 3D explainability methods using standard metrics like localization accuracy or faithfulness scores\n  - Replace subjective ChatGPT verification with established ground truth datasets or expert radiologist annotations for brain region validation\n  - Investigate and explain the accuracy variance across anatomical views, potentially through cross-validation or statistical significance testing\n\n‚Ä¢ **Justify and validate methodological design choices**\n  - Conduct ablation studies removing individual factors from the five-factor ranking to demonstrate each component's necessity and contribution\n  - Provide theoretical or empirical justification for the specific sequential ordering of feature selection methods, or compare against alternative orderings\n  - Define the \"different conditions\" in Algorithm 3 systematically and validate their contribution to ranking stability through sensitivity analysis\n\n‚Ä¢ **Expand evaluation scope and reduce external dependencies**\n  - Demonstrate method applicability on non-brain imaging datasets (lung, liver, or cardiac imaging) to establish broader medical imaging utility\n  - Evaluate performance under different preprocessing parameters (various resizing dimensions, slice selection strategies) to assess robustness\n  - Reduce dependency on \"ebrains\" software by incorporating alternative brain atlases or developing atlas-independent validation approaches",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript presents a novel method for ranking 3D image blocks in medical imaging using axial, coronal, and sagittal image patch rankings. The approach integrates feature selection with Grad-CAM to generate explainable heatmaps, and employs a hybrid 3D image block ranking algorithm to identify top-ranked blocks associated with important brain areas relevant to Alzheimer's disease (AD) and autism diagnosis. The authors demonstrate their method using the ADNI dataset and a separate dataset for autism, showing promising results in identifying key brain regions linked to these conditions.\n\n## Major Comments\n1. Novelty and Positioning: While the proposed method introduces a unique approach to integrating feature selection with Grad-CAM for 3D image block ranking, the manuscript could benefit from a clearer differentiation from existing methods. Specifically, the authors should discuss how their approach compares to other recent advancements in explainable AI and feature ranking in medical imaging. This would help readers understand the novelty and potential advantages of the proposed method.\n\n2. Evaluation Design: The evaluation is primarily conducted using the ADNI dataset and a smaller dataset for autism. While these datasets are well-known, the evaluation could be strengthened by including additional datasets or modalities to validate the robustness and generalizability of the proposed method. Additionally, the manuscript could benefit from a comparison against state-of-the-art methods for 3D image block ranking and explainability in medical imaging.\n\n3. Comparisons: The manuscript mentions several related works but does not provide detailed comparisons with recent methods that integrate feature selection and explainability in medical imaging. Including comparative analyses with these methods would provide a clearer picture of the method's performance and contributions.\n\n4. Reproducibility: The manuscript states that the code will be made available, but there are gaps in the methodological details necessary for reproducibility. For instance, the manuscript lacks clear descriptions of the preprocessing steps, hyperparameter settings, and the exact implementation of the feature selection and ranking algorithms. Providing comprehensive details in supplementary material or a dedicated section would significantly enhance reproducibility.\n\n## Minor Comments\n1. Figures: Figures 2 and 3 are dense and could benefit from additional labeling or simplification to improve clarity.\n   \n2. Terminology: There are inconsistencies in the notation used for defining the feature selection and ranking processes. Clarifying these definitions and ensuring consistent terminology throughout the manuscript would improve readability.\n\n3. Definitions: Some of the mathematical definitions are overly complex and could be simplified or explained more clearly to aid comprehension.\n\n4. Typographical Errors: Minor typographical errors such as \"saggital\" instead of \"sagittal\" should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical imaging: enhancing the explainability and interpretability of 3D CNN models. The proposed hybrid 3D image block ranking algorithm shows promise in identifying key brain regions relevant to AD and autism diagnosis. However, the evaluation could be strengthened by including additional datasets and comparative analyses with recent methods. While the method has potential, the reproducibility of the approach needs improvement due to incomplete methodological details. Overall, the manuscript demonstrates an innovative approach, but further validation and clarification are necessary to meet the standards expected for publication in TMI.\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis, broaden the validation across datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Luna Zhang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_727c34225dfc91a5f7681403d6688e41dc3bcb20.pdf",
    "remote_url": "https://openreview.net/pdf/727c34225dfc91a5f7681403d6688e41dc3bcb20.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Build your own cell: Diffusion Models for Multichannel 3D Microscopy Image Generation",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "3D Diffusion Models"
    ],
    "abstract": "Three-dimensional (3D) cellular morphology is a critical indicator of cellular function, disease states, and drug responses. However, capturing and interpreting the complex relationships between cell shape, treatment conditions, and their biological implications remains a challenge. To address this, we present \"Build Your Own Cell'' (BYOC), a multichannel 3D generative framework that combines vector quantisation and diffusion models to synthesise biologically realistic 3D cell structures. BYOC captures intricate morphological changes induced by different drug treatments, enabling high-throughput in silico simulations and screening of cell shapes in response to varied conditions. This novel framework represents a significant step towards accelerating pre-clinical drug development by synthesising high-resolution, biologically realistic 3D cells, potentially reducing reliance on labour-intensive experimental studies. By ensuring phenotypic consistency between cell and nucleus volumes through joint modelling, BYOC provides high-fidelity reconstructions that could facilitate downstream analyses, including drug efficacy evaluation and mechanistic studies.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces a multi-channel 3D diffusion model designed for generating two-channel cell images from volumetric fluorescence microscopy data. By focusing on the coupling of the two channels within the diffusion process, the model aims to improve the quality of generated dual-channel 3D cell images. The results presented show an improvement over the current state-of-the-art in this area.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- Addresses a challenging and pertinent problem in the field of biomedical microscopy, specifically in cellular imaging.\n- The overall motivation behind the proposed methodological enhancements is generally clear.\n- The experimental outcomes demonstrate promising improvements over existing methods.\n\n### Weaknesses\n\n- The biological rationale behind the model is not thoroughly convincing or well-articulated.\n- Some concrete methodological choices lack clear motivation or detailed explanation, leading to potential confusion (e.g. a clear motivation why and how to use VQGANs would be nice).\n- Some details are missing or inadequately explained in the formal equations and overall framework.\n- The manuscript tends to be imprecise in its language, which affects clarity and understanding.\n- The conclusion lacks specificity regarding the contributions, limitations and future directions of the methods-aspects of the work.\n\n### Questions\n\n1. How could biological or mechanistic understanding arise from generative models in your context? Can you expand and provide a stronger motivation for this idea?\n2. You mention that \"GANs often struggle with generating coherent latent representations.\" Since GANs do not inherently produce latent representations in the same way as e.g. Variational Autoencoders, could you clarify what \"coherent latent representations\" means in the context of GANs, and how this specifically relates to your proposed method's advantages?\n3. The claim that multiple color channels can be treated as distinct modalities is not clearly explained in my opinion but is crucial to the suggested method. Do you have examples from related work where color channels have been treated as distinct modalities? Could you explain the biological basis for considering cell and nucleus channels as separate modalities?\n4. In Equation 1, are the variables h,w,d the same dimensions as H,W,D? If not, what is their relationship? Similarly, in Equation 2, the depth dimension d seems to be omitted‚Äîwas this intentional or a typo? Please add a brief explanation of these variables and their relationships directly after the equations.\n5. How does the simultaneous recovery of both channels relate specifically to latent diffusion? Can you provide a specific example or illustration of how the simultaneous recovery process works in your model, and how it differs from standard latent diffusion approaches?\n6. What is the reason for using unquantized embeddings in your framework? If they drift from the codebook vectors, how does this affect the model, and what is the underlying motivation?\n7. In Equation 12, the variable t should be defined. Additionally, in Equation 13, what exactly is Œº_Œ∏_cn(‚ãÖ) computing‚Äîonly the mean or is there an associated variance? If not, what is the variance of your Gaussian?\n8. There is an existing WNet in medical imaging literature [1]. To avoid confusion, would you consider renaming your model?\n9. Can you provide more precise details about your dual-channel 3D architecture, perhaps with references or a schematic in the supplementary material?\n10. On page 6, you state that attention mechanisms are \"strategically placed\" to focus on regions of interest. Could you elaborate on the strategy behind their placement and how regions of interest are determined?\n11. The numerical differences in Table 1 are hard to interpret without context. Could you explain or hint to what these differences mean in terms of image quality and their significance in your application? Can you provide a brief interpretation guide for the FID and MMD scores, perhaps indicating what range of differences would be considered significant in this context? You could also include a qualitative comparison of images corresponding to different score ranges to help readers understand the practical implications of these differences.\n12. Could you provide more context about the ResNet50 model used‚Äîfor instance, what type of medical images it was trained on?\n\n**Additional Feedback for Improvement:**\n\n- In Figure 1, please explain what the rows and columns represent to enhance understanding.\n- In the introduction, you mention that \"single-cell data is often high-dimensional and inconsistent in size.\" Could you clarify whether this inconsistency refers to the images, cells, biological structures, or image resolutions?\n- In the Related Work section, the statement about discriminative frameworks needing a \"deep understanding of the underlying input distribution\" is unclear. Providing an example or reference could help clarify this point.\n- It might be beneficial to first introduce and describe the dataset before delving into implementation details like volume padding.\n- Please specify the size and resolution of the microscopy images. Are the single-cell images crops from larger stacks, or are they the direct output from the microscope?\n- Consider citing relevant works such as the 3D U-Net architecture [2] to situate your work within the existing literature.\n- The conclusion would be stronger if it discussed potential methodological developments and acknowledged limitations of the current approach.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript presents a multi-channel 3D diffusion model intended to generate two-channel cell images from volumetric fluorescence microscopy data. The model emphasizes the coupling of the two channels within the diffusion process, with the aim of enhancing the quality of generated dual-channel 3D cell images. The results indicate a notable improvement over existing methodologies in the field.\n\n**Major Comments:**\n1. The biological rationale for the proposed model is not articulated convincingly, which may undermine its relevance.\n2. Several methodological choices lack sufficient justification or detail, particularly the use of VQGANs.\n3. Important details are missing from the formal equations and overall framework, potentially leading to confusion.\n4. The language used throughout the manuscript is often imprecise, impacting clarity and comprehension.\n5. The conclusion fails to clearly outline the contributions, limitations, and future directions associated with the proposed methods.\n\n**Minor Comments:**\n- In Figure 1, it is essential to clarify what the rows and columns represent for better understanding.\n- In the introduction, clarification is needed regarding whether the inconsistency of \"high-dimensional data\" pertains to images, cells, biological structures, or image resolutions.\n- The statement regarding discriminative frameworks in the Related Work section needs elaboration or an example for clarity.\n- It would be beneficial to introduce and describe the dataset before discussing implementation details, such as volume padding.\n- Specify the size and resolution parameters of the microscopy images and clarify if single-cell images are crops or direct outputs from the microscope.\n- References to related works, such as the 3D U-Net architecture, would help position this study within existing literature.\n- Enhance the conclusion by discussing possible methodological advancements and acknowledging the limitations of the current approach.\n\n**Summary Paragraph:**\nThis study addresses a significant challenge in biomedical microscopy by developing an innovative approach for generating dual-channel 3D cell images. While the proposed model demonstrates potential for improved evaluation and quality, concerns regarding its biological rationale, clarity in methodology, and overall presentation may hinder reproducibility. The manuscript‚Äôs strengths lie in its novel contributions and promising results; however, weaknesses in detail and clarity need to be addressed to meet the editorial expectations of significance, innovation, evaluation, and reproducibility.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a multi-channel 3D diffusion model aimed at generating dual-channel cell images from volumetric fluorescence microscopy data. By coupling two image channels within the diffusion process, the method seeks to enhance the realism and consistency of generated 3D cell images. The paper is motivated by challenges in biomedical microscopy imaging and reports quantitative improvements compared to current state-of-the-art approaches. The presentation is generally clear, though certain biological and methodological aspects could be better justified and explained.\n\n**Major Comments**  \n1. The biological rationale for modeling two image channels as coupled modalities is insufficiently argued. Clarifying the biological or mechanistic insight that could emerge from generative modeling would strengthen the paper.  \n2. Several methodological choices lack adequate motivation or description‚Äîfor example, the use of VQGANs and how unquantized embeddings influence the model.  \n3. Key equations (e.g., Equations 1, 2, 12, 13) contain undefined variables and inconsistencies in notation (e.g., h,w,d vs. H,W,D, missing depth term, undefined t). These should be clearly defined and related.  \n4. The claim that multiple color channels represent distinct modalities is not sufficiently justified. Related literature or biological reasoning should support this assumption.  \n5. The relationship between simultaneous dual-channel recovery and latent diffusion processes requires a more explicit conceptual and empirical explanation.  \n6. The placement strategy of attention mechanisms and the design of the dual-channel 3D architecture need clearer description, possibly with a schematic.  \n7. Quantitative results (Table 1) lack interpretability‚Äîadditional context for FID and MMD score differences, or qualitative visual examples, would be helpful.\n\n**Minor Comments**  \n- Clarify Figure‚ÄØ1‚Äôs layout (rows/columns).  \n- Specify image sizes, resolutions, and whether volumes are cropped or raw outputs.  \n- Introduce the dataset before implementation details.  \n- Clarify ambiguous statements, e.g., ‚Äúsingle-cell data inconsistencies‚Äù and ‚Äúdeep understanding of the input distribution.‚Äù  \n- Include references such as 3D‚ÄØU‚ÄëNet to situate the method.  \n- Consider renaming the model to avoid confusion with existing ‚ÄúWNet‚Äù architectures.  \n- Minor language imprecision throughout should be revised for clarity.\n\n**Summary Paragraph**  \nIn terms of **significance**, the work tackles an important problem in microscopy image synthesis. **Innovation** lies in coupling multi-channel diffusion, but the biological and conceptual motivation require reinforcement. The **evaluation** shows numerical gains, yet lacks interpretive depth and qualitative context. **Reproducibility** would benefit from clearer presentation of equations, architectural details, dataset description, and implementation parameters. Strengths include topic relevance and promising results; weaknesses concern motivation, clarity, and methodological completeness.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.**  \nSubstantial clarification of the biological context, methodological details, and evaluation interpretation is necessary before reconsideration.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper presents the novice Build Your Own Model (BYOL), a multichannel generative framework leveraging the diffusion model, to generate a population of simulated 3D multichannel data that shows the morphological changes in cells perturbed by drug treatments. The model captures the relation between the nuclear and cytoplasmic channels used for model training when generating the simulated images and presents a high spatial resolution of the images. The authors benchmarked the model against already available models like GAN-based models and MedicalDiffusion, useful for 3D image generation for the same test case, and found the best overall model performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\nThe model outperforms existing models, generating nuanced morphological changes due to perturbations like drug treatments. Compared to existing models, it can also accurately capture the 3D resolved morphology of the cellular tags. The model is best at generating cellular data and matches real data.\n\n### Weaknesses\n\nThe model captures the morphological changes associated with the perturbations it has been trained on but has not yet been shown to be generalizable to different cell types and drug treatments. This has been marked as a future prospect of the study. This is important in biological studies as tagging and imaging the markers are expensive for generating training data and a very important domain where biology communities would benefit.\n\n### Questions\n\nThe metrics used to evaluate the model are good for evaluating the overall model performance. But do the metrics evaluate the inter- and intra-channel prediction accuracy? The authors stated that it is biologically relevant and an improvement brought by the work. But how can you evaluate this specific aspect using relevant metrics from a biological point of view?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces the Build Your Own Model (BYOL), a novel multichannel generative framework utilizing the diffusion model to simulate 3D multichannel data that illustrates morphological changes in cells subjected to drug treatments. This model effectively captures the relationship between nuclear and cytoplasmic channels during image generation, achieving high spatial resolution. The authors benchmark BYOL against existing models, including GAN-based models and MedicalDiffusion, demonstrating superior overall performance.\n\n**Major Comments:**  \n1. **Novelty**: The model shows significant improvements over existing frameworks by accurately capturing nuanced morphological changes.\n2. **Evaluation Design**: While the model performs well, it has not been demonstrated to generalize across different cell types and drug exposures, which limits its applicability. This remains a future direction for the research.\n3. **Comparisons**: The authors benchmark their model against notable existing frameworks, but further clarification on inter- and intra-channel prediction accuracy metrics is needed to enhance the evaluation.\n4. **Reproducibility**: The manuscript does not address how biological data generation can be generalized, a crucial factor in biological research due to costs associated with tagging and imaging.\n\n**Minor Comments:**  \n- Ensure clear explanations are provided for the evaluation metrics employed, particularly in relation to biological relevance.\n- The manuscript would benefit from concise delineation of acronyms and terms used within the context.\n\n**Summary Paragraph:**  \nThe BYOL framework significantly contributes to the field by innovatively generating simulated cellular data, addressing the significance and novelty criteria. However, concerns regarding its evaluation and reproducibility remain, as the model‚Äôs applicability to varied biological contexts is not yet established. The authors should strengthen the evaluation framework by providing clearer metrics that reflect biological relevance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel diffusion-based generative framework, Build Your Own Model (BYOL), designed to produce simulated 3D multichannel cellular images that reflect morphological alterations under drug perturbations. The approach models the relationship between nuclear and cytoplasmic channels to generate high-spatial-resolution images and is benchmarked against existing 3D generative models such as GAN-based methods and MedicalDiffusion. The paper is clearly presented, and the experimental comparisons demonstrate strong performance relative to prior approaches.  \n\n**Major Comments**  \n1. **Generalizability:** While the framework effectively captures morphological changes for the perturbations it was trained on, it remains unclear how well it generalizes to other cell types or drug treatments. The authors mention this as future work, but a discussion of potential strategies or limitations would help clarify scope and applicability.  \n2. **Evaluation Metrics:** The metrics employed are suitable for assessing overall model performance; however, it is uncertain whether they adequately measure inter- and intra-channel prediction accuracy. As this is a key claim of biological relevance, additional clarification or targeted evaluation metrics would reinforce the study‚Äôs biological interpretability.  \n\n**Minor Comments**  \n- Clarify whether training data diversity might influence generalization claims.  \n- Ensure all acronyms are clearly defined at first mention.  \n- The presentation and figures are clear, though a brief visual comparison with baseline outputs could enhance comprehension.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in its application of diffusion models for biologically realistic 3D multichannel image generation, potentially reducing the need for costly experimental imaging. The innovation lies in modeling inter-channel morphology within simulated perturbations. The evaluation is adequate but would benefit from more biologically specific performance metrics. Reproducibility appears achievable, though broader validation across additional biological conditions remains to be demonstrated. Overall, the contribution is promising but somewhat limited in scope and validation.  \n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The method is technically sound and innovative but requires further clarification of generalizability and evaluation metrics to strengthen its biological relevance and reproducibility claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors combine vector quantized GANs to learn representations of microscopy images of cells and develop a denoising diffusion model for latent representations. By combining vector quantized representations and the process of diffusion, they seek to generate 3D images of cells that belong to the distribution of realistic microscopy images.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* Strategy: The strategy of using diffusion modeling to improve the accuracy of prediction of GANs is promising.\n\n### Weaknesses\n\n* Incorrect assumptions about microscopy image data: Microscopy images often consist of more than two channels and many of them cannot be just binned into cells and nuclei. The authors seem to be familiar with medical imaging datasets but unaware of datasets such as cell painting (JUMP, CHAMMI), human protein atlas, and virtual staining. These datasets illustrate that microscopy data often consists of channels that encode multiple organelles and cellular compartments.\n* Lack of 3D predictions: Although the paper claims to be the first to build a 3D generative model of microscopy images, all the presented data is 2D. The authors should show orthogonal slices of generated volumes.\n* Relevance of metrics: Fre ÃÅchet Inception Distance and Maximum Mean Discrepancy seem reasonable. However, the authors do not clarify how these metrics may be affected by the typical failure modes of GANs, such as hallucinations of spurious cellular processes.\n\n### Questions\n\n* What is the effect of the diffusion on the quantized codebook? The way diffusion is used during inference was not apparent from the text or figures.\n* Does the approach work only with a specified number of input channels?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel approach combining vector quantized Generative Adversarial Networks (GANs) with denoising diffusion models to generate 3D representations of microscopy images of cells. The authors aim to improve representation learning to align better with real microscopy data distributions. The clarity of the overall presentation and the impact of the methodology on existing models warrant careful evaluation.\n\n### Major Comments\n1. **Assumptions about Microscopy Data:** The authors make incorrect assumptions regarding microscopy image data, which typically comprises more than two channels. The work seems limited due to a lack of familiarity with comprehensive datasets such as cell painting (JUMP, CHAMMI), the Human Protein Atlas, and virtual staining, which show that data often encompass multiple organelles and cellular compartments.\n2. **3D Predictions Absence:** Despite claiming to develop a 3D generative model, the presented examples are solely 2D. The authors should include orthogonal slices of generated volumes in their results for validation.\n3. **Metric Relevance:** While the use of Frechet Inception Distance and Maximum Mean Discrepancy as evaluation metrics is reasonable, the authors do not discuss how these may be influenced by GAN failure modes, such as generation of spurious cellular features.\n\n### Minor Comments\n- The text lacks clarity regarding the effects of diffusion on the quantized codebook.\n- It is unclear if the proposed method is contingent on a specific number of input channels.\n\n### Summary Paragraph\nIn terms of TMI editorial criteria, this study presents significant novelty through its use of diffusion modeling in GANs, although the assumptions about data and the lack of 3D validation raise concerns regarding innovation and reproducibility. The evaluation approach, while using established metrics, requires a more thorough discussion to ensure methodical rigor.\n\n### Decision Recommendation\nMajor revision required to address the identified weaknesses and improve clarity regarding methodology and evaluation.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript combines vector-quantized generative adversarial networks (VQ-GANs) and denoising diffusion models to learn latent representations of microscopy images and generate realistic 3D cellular structures. By integrating vector quantization and diffusion processes, the authors aim to improve generative quality and realism in cellular imagery. The overall presentation is understandable but limited in scope and clarity, and several methodological and conceptual aspects require more substantiation.\n\n**Major Comments**  \n1. **Assumptions about microscopy data** ‚Äì The manuscript makes oversimplified assumptions regarding the structure of microscopy data. Many datasets (e.g., Cell Painting, Human Protein Atlas, virtual staining studies) include multiple imaging channels representing diverse organelles and compartments, not merely nuclei and cells. The proposed approach seems to disregard this complexity, limiting its generalizability.  \n2. **3D data claims versus 2D results** ‚Äì Although the authors claim the method generates 3D microscopy images, all presented demonstrations are based on 2D data. The work would be more convincing with visualization of orthogonal slices or volumetric reconstructions.  \n3. **Evaluation metrics** ‚Äì While Fr√©chet Inception Distance (FID) and Maximum Mean Discrepancy (MMD) are reported, their relevance to microscopy data and robustness to common GAN artifacts (e.g., spurious or hallucinatory structures) are not adequately discussed. A critical analysis of how these metrics behave for this domain is needed.  \n4. **Unclear diffusion process** ‚Äì The manuscript does not make explicit how the diffusion step interacts with the quantized codebook during inference, which hinders reproducibility. Clarification is required concerning model components and channel dependencies.  \n\n**Minor Comments**  \n- Specify whether the method can handle varying numbers of input channels.  \n- Improve clarity of figures to illustrate the inference process explicitly.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study presents an innovative attempt to merge diffusion modeling with latent quantization for cellular image synthesis, suggesting potential for improved realism. However, its significance is reduced by conceptual oversights about microscopy data characteristics, limited empirical validation in 3D, and insufficient methodological transparency. The evaluation metrics are not fully justified for this context, and reproducibility is affected by incomplete procedural details.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The approach is conceptually promising but requires substantial methodological clarification, stronger validation, and alignment with realistic microscopy data standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚úÖ**\n   The paper presents a new generative modeling framework for multichannel 3D fluorescence microscopy, integrating vector quantization (VQ) with diffusion models to synthesize realistic volumetric cellular images. The methodological emphasis on multichannel latent diffusion and inter-channel consistency places it squarely within TMI‚Äôs methodological domain, particularly under *imaging physics/AI-based image formation, synthesis, and reconstruction*.\n\n2. **Novelty & Contribution Level ‚Äì Moderate to Strong**\n   - The **‚Äúlibrary of codebooks‚Äù** for separate biological channels combined with a **multichannel diffusion stage** is a novel integration not previously reported in 3D cell imagery synthesis.\n   - The pragmatic innovation lies in demonstrating *joint-channel generative modeling* for cell and nucleus morphology under drug perturbations.\n   - While the technical blocks (VQGAN, DDPM, UNet) are established, their combination for *multichannel 3D biological imaging* is distinctive and offers generalizable methodological value.\n   - Novelty is more architectural and conceptual rather than theoretical; however, it is sufficient for TMI if rigorously validated.\n\n3. **Technical and Experimental Rigor ‚Äì Good but with Gaps**\n   - Experiments are systematic: dataset description is clear; baselines and metrics are standard (FID, MMD using Med3D features).\n   - Implementation details and ablations are comprehensive.\n   - Missing aspects: uncertainty quantification, statistical significance testing, and evaluation of biological validity beyond pixel-level similarity (e.g., morphological metrics or expert scoring). Dataset splits are consistent, but no external validation set.\n   - Ethical clarity on dataset provenance (cell line, IRB, data sharing license) could be strengthened.\n\n4. **Clarity and Presentation ‚Äì Very Good**\n   - The manuscript is clearly structured with logical flow and meticulous mathematical description.\n   - Figures are likely informative (though text indicates heavy visual content).\n   - Minor typographic and stylistic issues (spacing, capitalization, figure referencing).\n\n5. **Ethical and Reproducibility Compliance ‚Äì Partially Addressed**\n   - GitHub link placeholder suggests intention for open code release.\n   - Dataset origin (melanoma cells) is described but without explicit IRB or licensing statement.\n   - No mention of code availability assurance under anonymization review.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe manuscript, *‚ÄúBuild Your Own Cell: Diffusion Models for Multichannel 3D Microscopy Image Generation,‚Äù* introduces BYOC ‚Äî a generative framework combining VQGAN encoding with a multichannel 3D diffusion process. The model uses independent codebooks for each biological channel (cell, nucleus) and jointly denoises these latent spaces via a dual-channel 3D UNet in the diffusion loop. Evaluations on a 7k-cell light-sheet fluorescence dataset of drug-treated melanoma cells demonstrate that BYOC outperforms GAN-based and diffusion-based baselines (HA-GAN, Œ±-GAN, MedicalDiffusion) on quantitative similarity metrics (FID, MMD) and yields visually realistic multi-channel 3D reconstructions. Ablation experiments confirm the benefit of channel-specific codebooks for morphological consistency.\n\n**2. Strengths**\n- Clear methodological contribution integrating vector quantization and conditional diffusion into a multichannel 3D framework.\n- Comprehensive experimental setup with multiple baselines and ablation studies.\n- Strong qualitative demonstrations of biologically faithful reconstructions across different drug treatments.\n- Potentially impactful for in-silico high-throughput phenotyping and drug-screening applications.\n\n**3. Weaknesses**\n- Biological validation remains limited to visual inspection and distribution-level metrics; lacks expert annotation or morphological feature comparison.\n- Evaluation confined to one cell line and specific drug treatments ‚Äî limited generalizability.\n- No uncertainty analysis or robustness study (e.g., to noise, resolution changes).\n- Insufficient discussion of computational scalability and inference time, relevant for translational use.\n- Ethical/compliance clarifications are minimal (dataset license, data sharing, human/animal sample policy).\n\n**4. Major Comments**\n1. **Validation Beyond Distribution Metrics:**  \n   Extend evaluation with domain-relevant morphological descriptors (e.g., cell/nucleus volume ratios, shape descriptors) or expert morphological score comparisons. This would strengthen biomedical relevance.  \n2. **Dataset and Compliance Clarification:**  \n   Include explicit details on dataset availability, ethical approval (if needed for biological specimens), and licensing terms.  \n3. **Code and Reproducibility:**  \n   Confirm that training and inference code (VQGAN + DDPM pipeline) will be made available; current placeholder link is insufficient for TMI reproducibility standards.  \n4. **Generalizability and Scalability:**  \n   Discuss limitations for other modalities (e.g., confocal, electron microscopy) and computational resource demands.  \n5. **Quantitative Uncertainty and Repeatability:**  \n   Consider assessing variance across multiple random seeds or training repeats to ensure stability given small batch sizes.\n\n**5. Minor Comments**\n- Correct typographical inconsistencies (e.g., ‚ÄúD IFFUSION,‚Äù ‚ÄúC ¬∏ ic ¬∏ek‚Äù) and unify capitalization.  \n- Figure numbering and captions should be concise and self-contained.  \n- Include clear axis labels and units for all quantitative plots.  \n- Reference formatting should adhere to IEEE style (currently hybrid arXiv/journal forms).  \n- Clarify how 3D FID/MMD was computed (feature extraction pipeline).\n\n**6. Overall Recommendation ‚Äì Major Revision**  \nThe work presents a promising, methodologically meaningful step forward in 3D multichannel biomedical image generation, fitting IEEE TMI‚Äôs methodological innovation scope. However, additional experimental validation and stronger reproducibility/ethical transparency are required before acceptance.\n\n**7. Confidence Level ‚Äì 4/5**  \nThe methodological domain and evaluation strategies are within the reviewer‚Äôs expertise; confidence is high in assessing novelty and rigor, moderate for biological relevance interpretations.\n\n---\n\n**Summary Recommendation:**  \n‚ñ∂ **Major Revision** ‚Äî The framework is innovative and potentially impactful but needs bolstered validation (morphological realism assessment, reproducibility details) and improved ethical/data disclosure to meet IEEE TMI expectations.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper presents \"Build Your Own Cell\" (BYOC), a generative framework for synthesizing biologically realistic 3D multichannel cellular structures. The method addresses the challenge of generating high-resolution 3D cell and nucleus volumes that maintain inter-channel consistency and biological accuracy. BYOC combines vector quantization using a library of channel-specific codebooks with multichannel diffusion models. The approach uses adapted VQGAN architecture to learn separate codebooks for cell and nucleus channels (Section 3.1), followed by a DualChannelUNet-based diffusion process to generate coherent multichannel volumes (Section 3.2). The authors evaluate their method on over 7,083 metastatic melanoma cells treated with three different drugs, demonstrating superior performance compared to baselines including MedicalDiffusion, HA-GAN, and W-GAN across FID and MMD metrics (Table 1). The framework aims to enable high-throughput in silico drug screening and morphological analysis.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical Formulation Inconsistencies**\n  - Equation 1 contains a clear error where both encoded representations are defined as E(xnuc), when one should be E(xcell), compromising mathematical clarity and reproducibility\n  - The notation switches inconsistently between ÀÜz and zq without clear distinction in meaning (Equations 3-4)\n  - The joint reverse process notation pŒ∏zÀÜqcell zÀÜqnuc lacks proper mathematical formatting and subscript clarity (Section 3.2)\n\n‚Ä¢ **Limited Experimental Validation Scope**\n  - Dataset restricted to single cell type (melanoma) and only three drug treatments, limiting generalizability claims made in the abstract and introduction\n  - No comparison with recent 3D medical generation methods beyond MedicalDiffusion (2023), missing important baselines from the evolving field\n  - Evaluation metrics (FID, MMD) computed on averaged channels rather than preserving multichannel structure, potentially masking inter-channel consistency issues (Section 4.3)\n  - Cross-validation details mentioned in Table 1 but methodology not described in Section 4.1\n\n‚Ä¢ **Technical Architecture Justification Gaps**\n  - No ablation study comparing the proposed DualChannelUNet against standard 3D UNet architectures for multichannel processing\n  - Library of codebooks concept lacks comparison with joint multichannel codebook approaches beyond the basic \"unimodal\" baseline (Figure 6)\n  - Missing analysis of computational overhead introduced by separate codebooks versus unified representations\n  - Spatial and depth-wise attention mechanisms mentioned (Section 3.2) but no evaluation of their specific contribution\n\n‚Ä¢ **Biological Validation Deficiencies**\n  - No quantitative analysis of biological plausibility metrics such as nucleus-to-cell volume ratios or spatial positioning accuracy\n  - Generated samples show \"slightly smoother\" fine details (page 8) but this degradation is not quantitatively assessed\n  - Claims of \"biologically realistic\" synthesis lack validation from domain experts or comparison with established biological morphology benchmarks\n  - Inter-channel consistency evaluation relies only on visual inspection rather than quantitative spatial relationship metrics\n\n## Suggestions for Improvement\n\n‚Ä¢ **Correct Mathematical Formulations**\n  - Fix Equation 1 to properly define ·∫ëcell = E(xcell) and ·∫ënuc = E(xnuc) with consistent notation throughout\n  - Standardize notation between ·∫ë and zq with clear definitions of when each representation is used\n  - Reformat the joint reverse process notation with proper mathematical typesetting and clear subscript definitions\n\n‚Ä¢ **Expand Experimental Validation**\n  - Include additional cell types and drug treatments to demonstrate generalizability across biological contexts\n  - Add comparisons with recent 3D generative methods and multichannel synthesis approaches from 2023-2024 literature\n  - Compute FID and MMD metrics on full multichannel volumes rather than channel-averaged versions to properly assess inter-channel consistency\n  - Provide detailed cross-validation methodology and statistical significance testing\n\n‚Ä¢ **Strengthen Technical Justification**\n  - Conduct ablation studies comparing DualChannelUNet against standard 3D UNet with multichannel inputs\n  - Compare library of codebooks against joint multichannel codebook learning with multiple architectural variants\n  - Report computational costs and memory requirements for separate versus unified codebook approaches\n  - Quantitatively evaluate the contribution of spatial and depth-wise attention mechanisms through targeted ablations\n\n‚Ä¢ **Enhance Biological Validation**\n  - Implement quantitative biological metrics such as nucleus-cytoplasm ratio, spatial positioning accuracy, and morphological feature preservation\n  - Quantify the \"smoother details\" observation with texture and fine-structure preservation metrics\n  - Collaborate with cell biology experts for qualitative validation or compare against established morphological databases\n  - Develop inter-channel spatial relationship metrics to quantitatively assess biological consistency beyond visual inspection",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces \"Build Your Own Cell\" (BYOC), a novel framework for generating high-resolution, biologically realistic 3D cellular structures using a combination of vector quantisation and diffusion models. The framework is designed to handle multichannel volumetric data, specifically focusing on the simultaneous synthesis of cell and nucleus structures under different drug treatments. The authors demonstrate that BYOC outperforms existing state-of-the-art methods in terms of morphological consistency, structural realism, and fine-grained detail preservation. The manuscript is well-written, and the motivation for addressing the challenges of multichannel 3D cellular data synthesis is clearly articulated.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: The manuscript presents a compelling approach for multichannel 3D cellular structure synthesis, but it lacks a thorough discussion of how this work stands apart from existing literature on multimodal diffusion models and multichannel generative frameworks. The authors should provide a more detailed comparison with recent works that have explored similar problems, such as those mentioned in the related work section.\n   \n2. Evaluation Design: The evaluation is conducted on a single dataset of drug-treated melanoma cells, which limits the generalizability of the findings. While the authors demonstrate superior performance on this dataset, broader validation across different cell types and treatment conditions is necessary to establish the robustness and versatility of BYOC. Additionally, the authors should consider including a more diverse set of metrics beyond FID and MMD to further substantiate the claims of enhanced realism and structural fidelity.\n\n3. Comparisons: The manuscript compares BYOC with several baseline models, including HA-GAN, W-GAN, Œ±-GAN, and MedicalDiffusion. However, the inclusion of more recent diffusion-based methods, such as those specifically designed for 3D medical image generation, would provide a more comprehensive evaluation. The authors should also discuss the limitations of these baseline models in the context of multichannel 3D cellular data synthesis.\n\n4. Reproducibility: The authors mention that the project repository will be made available, but the description of the training protocols, preprocessing steps, and model hyperparameters is somewhat incomplete. Providing a detailed and structured documentation of these aspects is crucial for ensuring reproducibility. Additionally, the authors should include a clear statement regarding the availability of the code and data.\n\n###\n\n## Minor Comments\n1. Figure Clarity: Figures 4A and 4B are cluttered and could benefit from a more organized layout, such as separating the ground truth, MedicalDiffusion, and BYOC samples into distinct subplots.\n2. Notation Consistency: The notation used for the diffusion process (e.g., \\( \\alpha_t \\), \\( \\beta_t \\)) is sometimes inconsistent, which can confuse readers. Standardizing the notation would improve clarity.\n3. Acronym Definitions: Some acronyms, such as \"L1 loss,\" are used without initial definition. Providing definitions for all acronyms would enhance readability.\n4. Typographical Issues: There are several typographical errors throughout the manuscript, such as \"k-spacce\" (page 6) and \"undersampling maskes\" (page 7). These should be corrected.\n\n###\n\n## Summary Paragraph\n(focusing on 4 TMI Editorial Criteria)\nThe manuscript addresses a significant and clinically relevant challenge: synthesising high-resolution, biologically realistic 3D cellular structures. The proposed BYOC framework, which combines vector quantisation and diffusion models, is technically innovative and demonstrates promising results in terms of morphological consistency and structural fidelity. However, the evaluation is limited to a single dataset and lacks broader validation across different cell types and treatment conditions, which weakens the claims of generalizability. The reproducibility of the approach is also somewhat uncertain due to incomplete methodological details. Overall, while the idea has promise, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include more recent diffusion-based methods, strengthen validation across diverse datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chris Bakal",
      "Chris Dunsby",
      "Lucas G Dent",
      "Matt De Vries",
      "Nathan Curry",
      "Olga Fourkioti",
      "Reed Naidoo"
    ],
    "url": "pdfs/iclr.cc-2025-conference_dd6f5c6509a71b82b803fd05cb68835d3440c1b0.pdf",
    "remote_url": "https://openreview.net/pdf/dd6f5c6509a71b82b803fd05cb68835d3440c1b0.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "LVLM",
      "Eye Gaze",
      "Video",
      "Medicine",
      "Medical Image",
      "Chest X-ray",
      "Chest X-ray Report Generation"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists‚Äô eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. The video, featuring a red gaze point overlaid on CXR images, emphasizes regions of focused attention during interpretation. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with a video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 25.4% on Impression generation task and on average 7.9% for all tasks using scaled evaluation metrics. Our approach enhanced open-domain LVLM models, when combined with exemplar reports for in-context learning, outperform medical models as well as those specifically trained for CXR report generation on the benchmark dataset. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks, pointing out a new effective approach of utilising LVLMs in healthcare and beyond.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis article introduces a novel prompting method called RadEyeVideo, which presents radiologists‚Äô eye-tracking data as a video sequence to capture the temporal and spatial order of their gaze (i.e., scan paths). The authors evaluated the effectiveness of this approach in chest X-ray report generation and disease diagnosis tasks using large vision-language models (LVLMs) with video input capabilities. Results show that incorporating eye-gaze video prompts improved model performance by 25.4% on the Impression generation task, with an average performance increase of 7.9% across all tasks.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The RadEyeVideo method converts radiologists‚Äô eye-tracking data into video sequences for use in chest X-ray report generation and diagnostic tasks, significantly enhancing the performance of general large vision-language models (LVLMs). Experimental results indicate a 25.4% improvement in the Impression generation task and an average improvement of 7.9% across all tasks, outperforming models specifically designed for medical applications.\n2. Comprehensive Evaluation of Eye-Tracking Prompting Methods: This study presents the first thorough evaluation of various eye-tracking integration techniques, including static heatmaps, eye-tracking text prompts, and dynamic video prompts. The results indicate that RadEyeVideo outperforms the other methods in terms of diagnostic accuracy and clinical relevance, establishing a new standard for eye-tracking data prompts.\n\n### Weaknesses\n\n- Methodological Clarity: a) The paper lacks sufficient detail about the video prompt integration process, particularly how the video information is encoded and processed by the LVLMs. The technical implementation of combining video sequences with textual prompts could be more thoroughly explained. b) There's limited discussion of potential alternatives to full video sequence processing that might achieve similar results with lower computational costs.\n\n- Data Collection and Generalizability Limitations: a) The methodology requires synchronized eye-tracking data collection during radiologist readings, which is resource-intensive and difficult to scale. b) The approach may not generalize well to scenarios where real-time eye-tracking data is unavailable or impractical to collect. c) The current evaluation is limited to a relatively small dataset (2,298 CXR images), raising questions about broader applicability.\n\n- Computational Efficiency Concerns: The direct use of complete eye-tracking video sequences as prompts likely increases computational overhead significantly. While the authors mention sampling k frames (typically 16) from the total sequence, there's limited analysis of the computational trade-offs or optimal sampling strategies. The method may be computationally prohibitive for real-time clinical applications.\n\n### Questions\n\nPlease address the concern raised in weakness part.\nMoreover, there are some questions may help you to address the weakness.\n1) What is the computational overhead of processing video prompts compared to traditional image-only or text-only prompts?\n2) Have you explored more efficient alternatives to using complete video sequences, such as key frame selection or compressed representations? 3) How do you envision this approach being implemented in real-world clinical settings where real-time eye-tracking data may not be available? 4) Have you considered alternative methods for generating synthetic eye-tracking data that could make the approach more broadly applicable? 5)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel prompting method named RadEyeVideo, which utilizes radiologists‚Äô eye-tracking data in the form of video sequences to enhance chest X-ray report generation and disease diagnosis tasks using large vision-language models (LVLMs). The proposed method demonstrates a significant improvement, with results indicating a 25.4% enhancement in the Impression generation task and an average increase of 7.9% across all evaluation metrics. Overall, the work is clearly articulated and contributes to advancing the application of eye-tracking in medical imaging.\n\n### Major Comments\n1. **Methodological Clarity**: \n   - The paper lacks sufficient detail regarding the video prompt integration, specifically how the video data is encoded and processed by the LVLMs. A detailed explanation of the technical implementation combining video sequences with textual prompts is warranted.\n   - There is limited discussion of potential alternatives to utilizing full video sequences that could yield similar results at lower computational costs.\n  \n2. **Data Collection and Generalizability**: \n   - The requirement for synchronized eye-tracking data during readings is resource-intensive and could limit scalability.\n   - The approach may not generalize effectively in scenarios where real-time eye-tracking data is unattainable.\n   - The evaluation is confined to a relatively small dataset (2,298 CXR images), which raises concerns regarding broader applicability.\n\n3. **Computational Efficiency**: \n   - The reliance on complete eye-tracking video sequences as prompts likely incurs significant computational overhead. While the authors mention sampling k frames, there is insufficient analysis of the computational trade-offs or the optimal sampling strategies needed for practical applications.\n\n### Minor Comments\n- The paper could benefit from enhanced clarity in figures depicting the RadEyeVideo methodology.\n- There are minor typographical errors that require attention throughout the manuscript.\n\n### Summary Paragraph\nThis study introduces significant innovation through the RadEyeVideo method, showcasing notable improvements in model performance within the context of eye-tracking data integration. However, issues regarding methodological clarity, generalizability, and computational efficiency present challenges for implementation. Addressing these concerns is crucial to meet the editorial criteria of significance, innovation, evaluation, and reproducibility effectively.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *RadEyeVideo*, a novel prompting method that represents radiologists‚Äô eye-tracking data as video sequences to capture temporal and spatial gaze order. The approach is evaluated on chest X-ray report generation and disease diagnosis tasks using large vision‚Äìlanguage models (LVLMs) capable of processing video inputs. The study reports substantial performance improvements‚Äî25.4% in the Impression generation task and an average of 7.9% across all tasks‚Äîdemonstrating the potential impact of incorporating dynamic eye-gaze information. Overall, the paper addresses an interesting idea with well-structured experiments but has methodological and generalizability concerns that require clarification.  \n\n**Major Comments**  \n1. **Methodological Clarity:** The paper does not sufficiently explain how video prompts are integrated and processed within the LVLMs. Details about video encoding, temporal representation, and combination with textual prompts are limited. Greater transparency on architectural choices, training setup, and data flow is needed to ensure reproducibility.  \n2. **Alternative and Efficiency Analysis:** There is minimal discussion of computational trade-offs associated with full video sequence processing. Explorations of lighter alternatives‚Äîsuch as key-frame selection, compressed representations, or partial sequences‚Äîcould inform efficiency and practicality while maintaining performance.  \n3. **Data Collection and Generalizability:** The method depends on synchronized eye-tracking data collected during radiologist readings, which is expensive and challenging to scale. The study‚Äôs reliance on a relatively small dataset (2,298 CXRs) further limits generalizability. The approach‚Äôs applicability in clinical scenarios without real-time eye-tracking data should be discussed.  \n4. **Computational Burden:** Although the authors use frame sampling (typically 16 frames), no quantitative analysis of computational cost versus benefit is provided. This raises concerns about feasibility for real-time diagnostic use.\n\n**Minor Comments**  \n- Clarify the process of selecting ‚Äúk‚Äù frames for input and justify sampling choices.  \n- Ensure consistent notation and acronym definitions (e.g., LVLM).  \n- Provide additional details in figures or appendices to illustrate video prompt construction.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe submission offers *significant* potential by introducing a data-driven mechanism that bridges human visual attention with multimodal model reasoning. Its *innovation* lies in transforming gaze data into a dynamic prompt format, showing clear quantitative gains. However, *evaluation* is constrained by limited dataset size and sparse computational analysis. *Reproducibility* suffers from incomplete methodological detail and unclear implementation steps. Addressing these aspects would substantially strengthen the work.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nTo enhance the reliability of large vision-language models (LVLMs) in real clinical environments, this study proposes a novel video prompting method called RadEyeVideo, which integrates radiologists' eye-tracking data as video sequences, capturing the spatial and temporal dynamics of gaze. Red fixation points are superimposed on CXR images to highlight the areas that doctors pay attention to, so as to dynamically present the doctor's eye movement path. Improve the ability of radiologists or AI models to diagnose chest diseases by providing rich contextual information.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed method, RadEyeVideo, combines video, text, and eye movement data to realize the fusion of multi-modal information and improve the accuracy and efficiency of diagnosis.\n2. The authors conducted a thorough assessment of various eye-tracking integration techniques, providing strong empirical support for their claims.\n\n### Weaknesses\n\n1. In Figure 2, the authors illustrate different prompting methods; however, the figure does not clearly distinguish between the use of text descriptions and video inputs. This ambiguity makes it challenging to understand whether the authors used text descriptions to guide the prompt along with the video input or if they only provided video data. \n2. The author's experiments conducted on only one dataset are clearly insufficient in terms of persuasiveness. This limitation may affect the generalizability and applicability of the research findings. To enhance the credibility of the study, it is recommended that the authors validate their approach on different datasets.\n3. When the authors use radiologist's eye-tracking sequences as video input, this approach does enhance the model's understanding of prior knowledge in the diagnostic reading process to a certain extent. However, converting eye-tracking data into video format substantially increases the number of tokens, leading to significant computational resource consumption and longer inference times for the model.\n\n### Questions\n\n1. The RadEyeVideo method integrates the eye movement data of the radiologist as a video sequence to capture the spatiotemporal dynamics of his gaze. However, the article does not explain in detail how this video sequence is specifically generated, for example, how the eye movement data is sampled and converted into video frames, and how these frames contain temporal and spatial information.\n2. In Section 2.3 INPUT REPRESENTATION, to fit the input requirements of LVLM, the authors uniformly sampled the video sequence and selected 16 frames as inputs. However, the article does not explain why 16 frames were chosen and whether this choice had a significant impact on model performance. In addition, whether the impact of using more or fewer frames on model performance has been explored is also a question worth exploring. This helps to further understand the effect of video data length and quality on model performance.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces RadEyeVideo, a novel video prompting method aimed at enhancing the reliability of large vision-language models (LVLMs) in clinical environments. By integrating eye-tracking data from radiologists as video sequences, it captures the spatial and temporal dynamics of gaze to improve diagnosis of chest diseases. The approach aims to enrich diagnostic capabilities for radiologists and AI models alike.\n\n**Major Comments:**  \n1. **Novelty and Contribution:** RadEyeVideo effectively integrates video, text, and eye movement data, advancing multi-modal information fusion to enhance diagnostic accuracy and efficiency. \n2. **Empirical Assessment:** The authors provide substantial empirical support by thoroughly evaluating different eye-tracking integration techniques. \n3. **Clarity in Figures:** Figure 2 does not clearly differentiate between text descriptions and video inputs, resulting in ambiguity regarding their combined use in prompting.\n4. **Dataset Limitations:** Conducting experiments on only one dataset undermines the generalizability of the findings. Validation across multiple datasets is recommended for stronger credibility.\n5. **Computational Efficiency:** The conversion of eye-tracking data to video format increases token count, raising concerns about computational resource usage and inference time.\n\n**Minor Comments:**  \n- The manuscript lacks clarity on how eye-tracking data is sampled and converted into video frames, particularly regarding the generation process and how temporal and spatial information is retained.\n- In Section 2.3, the choice of using 16 uniformly sampled frames as inputs is not justified, and the impact of varying frame numbers on model performance is not explored.\n\n**Summary Paragraph:**  \nIn terms of significance, the study addresses an important need in diagnostic imaging. While the integration of eye-tracking data is innovative, the evaluation design is limited by the use of a single dataset, potentially affecting reproducibility. The lack of clarity in presentation and depth of explanation regarding technical choices invites further scrutiny. Overall, the manuscript displays both strengths and weaknesses, necessitating revisions to bolster its contribution to the field.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RadEyeVideo*, a novel video prompting method designed to improve the reliability of large vision-language models (LVLMs) in clinical diagnostics. The approach integrates radiologists‚Äô eye-tracking data as video sequences, capturing spatial and temporal gaze dynamics to enrich contextual information and support disease diagnosis from chest X-rays. Overall, the paper aims to enhance multimodal understanding in AI-assisted radiology workflows. The submission is generally clear, though certain methodological details and experimental validations require further explanation and expansion.\n\n**Major Comments**  \n1. **Methodological Clarity:** Figure 2 does not clearly differentiate between text-based and video-based prompting. The description of how text and video inputs interact in the prompting process should be clarified to avoid confusion about the experimental setup.  \n2. **Dataset Limitation:** The evaluation is conducted on only one dataset, which limits the generalizability and persuasiveness of the claimed improvements. Testing the approach on additional, independent datasets is necessary to validate robustness and applicability.  \n3. **Computational Efficiency:** While converting eye-tracking data into video sequences can better represent diagnostic behavior, this increases token counts and computational costs, significantly affecting inference speed. A quantitative discussion or mitigation strategy for this trade-off should be included.  \n4. **Generation of Eye-Tracking Videos:** The paper lacks detail on how gaze data are sampled and transformed into video frames, including how temporal continuity and spatial fixation cues are preserved.  \n5. **Frame Selection Rationale:** Section 2.3 mentions uniform sampling of 16 frames, but the choice of this number is not justified. An analysis of the impact of frame count on model performance would strengthen the methodological soundness.\n\n**Minor Comments**  \n- Clarify figure legends to distinguish modalities more explicitly.  \n- Ensure consistent terminology when referring to eye movement data, fixation points, and video sequences.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is innovative in integrating visual attention data with LVLMs, contributing to the interpretability and contextual understanding of diagnostic models. However, significance is undermined by limited empirical validation and incomplete methodological descriptions. The evaluation supports feasibility but does not establish generalizability. Reproducibility could be improved through clearer documentation of preprocessing and sampling strategies.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces an innovative approach that incorporates radiologists' eye-tracking data as video sequences. This method captures both spatial and temporal patterns in gaze, which provides a more accurate representation of radiologists' attention during chest X-ray interpretation. The approach was evaluated using three general-domain LVLMs, showing significant improvements.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- RadEyeVideo's use of video-based eye-gaze data is a unique contribution that effectively captures the temporal and spatial dynamics of radiologists' focus. I like this idea.\n- The study demonstrates substantial improvements, particularly in impression generation, highlighting RadEyeVideo's effectiveness in enhancing diagnostic tasks.\n- The language is clearly presented. The authors use precise and concise language so that the reader can easily understand the methodology, and results of the study.\n\n### Weaknesses\n\n- Although this idea is interesting, it still relies on temporal and spatial information in the inference phase, which is difficult to apply to real clinical scenarios. Do the authors consider involving multiple information inputs only in the training phase and simulating zero-shot scenarios as much as possible in the inference phase?\n- The study‚Äôs findings are limited by the small size of the MIMIC-Eye dataset, which may not fully capture the variability in real-world clinical settings, raising questions about the generalizability of the results.\n- The comparisons primarily focus on selected models with minimal tuning for this domain. Including a wider range of task-specific medical LVLMs could provide a more comprehensive evaluation.\n\n### Questions\n\n- Why the paper lacks an section of related work? e.g., some recent evaluation work of Med-LVLMs [1,2,3]\n- The formats of reference is weird. e.g., in Line 311-321. Please check it.\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[3] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents an innovative methodology that integrates radiologists' eye-tracking data in the form of video sequences, capturing both spatial and temporal patterns of gaze. This approach offers a refined representation of radiologists' attention during chest X-ray interpretation and has been evaluated against three general-domain large vision language models (LVLMs), showing notable improvements in diagnostic efficacy.\n\n### Major Comments\n1. The novelty of RadEyeVideo relies heavily on using temporal and spatial information during the inference phase, which may not translate effectively to real clinical settings. The authors should consider involving multiple information inputs during the training phase while simulating zero-shot scenarios in the inference phase.\n2. The findings are constrained by the limited size of the MIMIC-Eye dataset, which may not adequately reflect the variability present in real-world clinical environments, thus questioning the generalizability of the results.\n3. Evaluations predominantly focus on a select few models with minimal tuning specific to this domain. A more diverse selection of task-specific medical LVLMs would enhance the comprehensiveness of the evaluation.\n\n### Minor Comments\n- The manuscript lacks a section on related work, particularly regarding recent evaluations of Med-LVLMs.\n- The formatting of references (lines 311-321) requires attention and correction to meet standard citation guidelines.\n\n### Summary Paragraph\nIn terms of the TMI editorial criteria, this study demonstrates significant innovation through its unique contribution of video-based eye-gaze data and effective diagnostic enhancement. However, its evaluation is limited by the size of the dataset and the narrow scope of model comparisons. Addressing these weaknesses could improve its impact, particularly in relation to reproducibility and transferability to real clinical practices.\n\n### Decision Recommendation\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RadEyeVideo*, a novel method integrating radiologists‚Äô eye-tracking data as video sequences to model both spatial and temporal gaze patterns during chest X-ray interpretation. This approach aims to enhance large vision‚Äìlanguage models (LVLMs) by incorporating human visual attention dynamics. The authors evaluate the method using three general-domain LVLMs and report notable improvements, especially for impression generation tasks. Overall, the paper is clearly written, and the methodology and results are presented in a straightforward and comprehensible manner.\n\n**Major Comments**  \n1. **Clinical applicability:** While the concept of incorporating spatiotemporal gaze data is innovative, it remains dependent on such information during inference. This reliance limits practical application in real clinical contexts where eye-tracking data are unavailable. The authors should consider strategies that constrain this dependency to the training phase and explore zero-shot or gaze-free inference scenarios.  \n2. **Dataset limitations:** The experiments are based on the relatively small MIMIC-Eye dataset, which may not sufficiently represent the diversity of real-world imaging conditions or reading behaviors. This restricts the generalizability of the conclusions.  \n3. **Comparative evaluation:** The comparison is limited to a few general LVLMs with minimal adaptation to the medical domain. Incorporating additional task-specific or medical LVLMs would provide a more robust and representative evaluation.  \n4. **Related work:** The manuscript currently omits a related work section. It should reference recent benchmarking and evaluation studies of medical LVLMs (e.g., MedVH, MedThink, CARES) to properly situate the contribution.\n\n**Minor Comments**  \n- Reference formatting is inconsistent (e.g., Lines 311‚Äì321) and should be corrected.  \n- Clarify minor typographical or formatting issues throughout the reference list.\n\n**Summary Paragraph**  \nIn terms of *significance*, the work explores an engaging way to integrate human perceptual cues into LVLM training, addressing an underexplored direction. Its *innovation* lies chiefly in the use of video-based eye-gaze representations, which successfully capture temporal‚Äìspatial dynamics. However, *evaluation* is constrained by limited dataset size and restricted model comparisons, and *reproducibility* may be affected by the dependency on specialized gaze data and missing methodological details about model tuning.\n\n**Decision Recommendation**  \n**Major Revision.** The manuscript proposes a novel and promising concept but requires a broader evaluation, clearer positioning within prior work, and discussion on real-world applicability to strengthen its contribution.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1‚ÄØ‚Äì Analytical‚ÄØand‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI**  \nThe manuscript presents *RadEyeVideo*, a method to incorporate radiologists‚Äô eye‚Äëtracking data into large vision‚Äëlanguage models (LVLMs) for chest‚ÄØX‚Äëray report generation and diagnosis. The work sits at the interface of computer vision, multimodal machine learning, and medical imaging.  It proposes a new data representation (eye‚Äëgaze videos) and evaluates several multimodal prompting strategies.  \n‚úÖ‚ÄØ**Within scope**‚ÄØof *IEEE‚ÄØTMI*: it proposes a methodological innovation for human‚Äëin‚Äëthe‚Äëloop medical image analysis with structured experiments on image‚Äìlanguage models.\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel**  \nThe primary novelty lies in encoding gaze fixation sequences as *video prompts* to capture temporal order, contrasting with prior static heatmaps or textual cues. While use of eye‚Äëtracking data for medical imaging AI is known, extending it to video‚Äëbased prompts for LVLMs is a distinctive but *incremental* conceptual step rather than a fundamental algorithmic advance. The contribution is therefore methodological at the data‚Äërepresentation and prompting‚Äëdesign level, not at the level of new model architectures or learning theory.  \nOverall novelty: **moderate**‚Äîpotentially interesting for the community if empirical gains are validated more rigorously.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \n- **Dataset**: built from MIMIC‚ÄëEye (‚âà2.3‚ÄØk images). The authors acknowledge limited sample size and potential train/test overlap; nonetheless, methodology to mitigate bias is described (alpha/beta splits).  \n- **Baselines**: several state‚Äëof‚Äëthe‚Äëart LVLMs, including medical and general‚Äëdomain ones, are compared.  \n- **Metrics**: both lexical (ROUGE,‚ÄØBERTScore) and radiology‚Äëspecific (CheXbert,‚ÄØRadGraph,‚ÄØRaTEScore) used with a scaled‚Äëscore composite.  \n- **Ablation**: frame‚Äëcount, gaze filtering, and in‚Äëcontext learning were examined.  \n- **Weaknesses**: limited statistical analysis‚Äîno confidence intervals or significance testing; clinical relevance of improvements not formally studied; dataset imbalance remains; ethical statement minimal. Reproducibility seems adequate (code indicated).  \n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation**  \nManuscript is generally well organized, with clear figures, equations, and tables. Wording is occasionally verbose; minor grammatical inconsistencies appear but readability is high. Quantitative results are exhaustive, perhaps excessively so for main text.\n\n**5.‚ÄØEthical‚ÄØand‚ÄØReproducibility‚ÄØCompliance**  \nData use complies with MIMIC‚ÄëEye licensing; anonymization stated. Code availability noted. No patient‚Äëidentifiable content. Ethical‚ÄØ&‚ÄØreproducibility information are satisfactory.\n\n---\n\n**Phase‚ÄØ2‚ÄØ‚Äì‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n**1.‚ÄØSummary**  \nThe paper proposes *RadEyeVideo*, which treats radiologists‚Äô eye‚Äëfixation traces as short videos overlaid on chest‚ÄØX‚Äëray images and feeds these into video‚Äëcapable LVLMs. The goal is to capture both spatial and temporal attention signals to enhance report generation and diagnostic accuracy. Evaluations on the MIMIC‚ÄëEye dataset demonstrate up to‚ÄØ25‚ÄØ% improvement in impression‚Äëgeneration metrics compared with baseline prompting and better performance than some medical‚Äëdomain LVLMs.\n\n**2.‚ÄØStrengths**\n- Creative use of expert gaze as dynamic spatio‚Äëtemporal conditioning signal.  \n- Comprehensive comparison against multiple gaze‚Äëintegration modes (heatmap, text, video).  \n- Ablation studies clarify design choices (frame count, gaze filtering, in‚Äëcontext learning).  \n- Solid presentation and connection to human‚ÄëAI collaboration literature.  \n\n**3.‚ÄØWeaknesses**\n- Dataset small, with possible overlap of training and evaluation splits.  \n- Improvement margins reported without statistical validation; significance uncertain.  \n- Method acts mainly as data‚Äërepresentation engineering, not deeper modeling innovation.  \n- Experimental scope limited to chest‚ÄØX‚Äërays; claims of domain generality not demonstrated.  \n- Clinical impact not assessed through radiologist review or reader study.  \n\n**4.‚ÄØMajor‚ÄØComments**\n1.‚ÄØ**Validation‚ÄØDepth**‚ÄØ‚Äì‚ÄØProvide statistical significance (e.g., bootstrap CIs, t‚Äëtests) on improvement percentages and discuss variability across cases.  \n2.‚ÄØ**Dataset‚ÄØBias‚ÄØand‚ÄØOverlap**‚ÄØ‚Äì‚ÄØElaborate on how training/testing contamination was prevented when reusing MIMIC‚ÄëEye images from MIMIC‚ÄëCXR splits.  \n3.‚ÄØ**Clinical‚ÄØUtility**‚ÄØ‚Äì‚ÄØQuantify whether textual improvements correspond to higher diagnostic correctness or fewer critical hallucinations verified by domain experts.  \n4.‚ÄØ**Generality‚ÄØClaims**‚ÄØ‚Äì‚ÄØRestrict or further substantiate broader‚Äëdomain statements; showing results on another modality (e.g., CT) or public benchmark would strengthen TMI‚Äëlevel impact.  \n5.‚ÄØ**Reproducibility**‚ÄØ‚Äì‚ÄØConsider releasing the full gaze‚Äëvideo generation pipeline and scripts for metric scaling to ensure others can replicate the reported gains.\n\n**5.‚ÄØMinor‚ÄØComments**\n- Several typographical errors (‚Äúgeneartion,‚Äù ‚ÄúE‚ÄØNHANCING‚Äù) should be corrected.  \n- Figure‚ÄØcaptions could be more concise; ensure all acronyms are defined upon first use.  \n- Some references are preprints; when possible, replace with peer‚Äëreviewed versions.  \n- Supplementary quantitative tables could be moved to appendix or shared data repository to streamline the main text.  \n\n**6.‚ÄØOverall‚ÄØRecommendation**\n**Major‚ÄØRevision** ‚Äì The paper addresses an interesting methodological direction aligning with TMI‚Äôs emphasis on human‚Äëcentered imaging AI. However, the current study‚Äôs small‚Äëscale evaluation and limited novelty evidence require stronger experimental validation and clearer positioning relative to existing human‚Äëeye‚Äëguided models before acceptance.\n\n**7.‚ÄØConfidence‚ÄØLevel:**‚ÄØ4‚ÄØ/‚ÄØ5  \n(Reviewer is familiar with LVLMs and eye‚Äëtracking‚ÄØAI; moderate‚Äëto‚Äëhigh confidence in technical assessment.)",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces RadEyeVideo, a novel approach that integrates radiologists' eye-tracking data as video sequences to enhance large vision-language models (LVLMs) for chest X-ray analysis. Unlike existing methods that use static heatmaps or textual prompts, RadEyeVideo captures both spatial and temporal dynamics of radiologists' gaze patterns by creating videos with red dots overlaid on CXR images, where dot duration reflects fixation time (Section 2.2, Equations 1-3). The authors evaluate three general-domain video-capable LVLMs (LongVA, VideoLLaMA2, LLaVA-OneVision) on report generation and diagnosis tasks using the MIMIC-Eye dataset (2,298 CXR images with eye-gaze annotations). Results show RadEyeVideo achieves up to 25.4% improvement on Impression generation and 7.9% average improvement across all tasks compared to baseline methods (Table 3). The enhanced general-domain models outperform specialized medical LVLMs like CheXagent and CXR-LLaVA on benchmark datasets.\n\n## Weaknesses\n\n‚Ä¢ **Limited Dataset Scale and Generalizability**: The evaluation relies on a small dataset of only 2,298 images from MIMIC-Eye, with the beta split containing merely 92 images for robust evaluation (Table 1, Page 6). This severely limits statistical power and generalizability claims. The authors acknowledge this limitation (Page 10) but proceed with evaluation on both training and test splits, potentially introducing data contamination issues that could inflate reported performance gains.\n\n‚Ä¢ **Inconsistent Mathematical Formulation and Implementation Details**: The video construction methodology lacks precision in several key aspects. Equation 2 defines frame rate as 10 fps but provides conflicting information about total frame calculation (Fi = ti √ó fps vs Fi = ti √ó 10). The uniform sampling strategy in Equation 3 uses floor division that could lead to frame loss, and the fixed 5-pixel gaze radius lacks justification or sensitivity analysis. The filtering criterion using average fixation duration (Equation 1) appears arbitrary without empirical validation.\n\n‚Ä¢ **Weak Experimental Design and Evaluation Methodology**: The scaled evaluation metric based on CheXagent performance (Equation 7) creates circular reasoning since CheXagent is simultaneously used as both a baseline competitor and the normalization reference. The evaluation lacks statistical significance testing despite small sample sizes, and the temperature parameter of 0 eliminates randomness assessment. The choice of 16 frames appears arbitrary, supported only by a limited ablation study (Figure 4) without theoretical justification.\n\n‚Ä¢ **Insufficient Baseline Comparisons and Analysis**: The paper compares against limited eye-gaze integration methods and lacks comparison with recent human-AI collaboration approaches in medical imaging. The heatmap implementation details are poorly described (Page 7), making reproduction difficult. Performance improvements show high variance across models and tasks (Table 3), with VideoLLaMA2 showing negligible improvement (0.6%) and inconsistent patterns that undermine the claimed superiority of the video approach.\n\n## Suggestions for Improvement\n\n‚Ä¢ **Expand Dataset Scale and Validation Strategy**: Conduct evaluation on larger, more diverse datasets beyond MIMIC-Eye to establish generalizability. Implement proper cross-validation with completely held-out test sets to avoid contamination issues. Include multi-institutional validation data to demonstrate robustness across different radiologist populations and imaging protocols.\n\n‚Ä¢ **Clarify and Validate Mathematical Formulations**: Provide consistent and precise mathematical notation throughout, particularly for frame rate calculations and sampling strategies. Conduct sensitivity analysis for key hyperparameters including gaze radius size, filtering thresholds, and frame numbers. Justify the average-duration filtering criterion through empirical analysis of gaze pattern significance and provide theoretical foundation for uniform sampling approach.\n\n‚Ä¢ **Strengthen Experimental Design with Rigorous Statistics**: Replace the circular CheXagent-based scaling metric with independent evaluation standards or multiple reference points. Include statistical significance testing with confidence intervals and effect size calculations. Implement multiple temperature settings to assess model reliability and add cross-model validation to strengthen claims about video-based prompting superiority.\n\n‚Ä¢ **Comprehensive Baseline Analysis and Methodological Transparency**: Include comparisons with recent human-AI collaboration methods in medical imaging and provide detailed implementation specifications for all baseline approaches. Conduct thorough ablation studies examining individual components of the video creation pipeline and provide qualitative analysis of failure cases to better understand method limitations and boundary conditions.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces RadEyeVideo, a novel approach that integrates radiologists' eye-gaze data as video sequences to enhance general-domain large vision-language models (LVLMs) for chest X-ray (CXR) analysis. By capturing both the spatial and temporal dynamics of eye movements, the proposed method aims to improve the performance of LVLMs in CXR report generation and diagnosis. The authors evaluate their approach using three general-domain LVLMs and demonstrate improvements in performance metrics compared to traditional methods that use heatmaps or textual prompts. The manuscript is well-written, and the motivation for incorporating sequential eye-gaze data is clearly articulated.\n\n## Major Comments\n1. Novelty and Positioning: The manuscript claims that RadEyeVideo offers significant improvements over existing methods by integrating eye-gaze data as video sequences. However, it is not entirely clear how this approach differentiates itself from recent works that also incorporate temporal information. The authors need to provide a more thorough comparison with related works that have explored sequential eye-gaze integration to establish the novelty of their contribution.\n\n2. Evaluation Design: The evaluation is conducted using a relatively small dataset (MIMIC-Eye) that may not be fully representative of real-world CXR scenarios. The authors should consider expanding the evaluation to include a more diverse set of CXR images and radiologists to better assess the generalizability of their approach. Additionally, the use of only the alpha and beta splits from the MIMIC-CXR dataset might introduce biases, and the authors should discuss potential limitations arising from this choice.\n\n3. Comparisons: The baseline comparisons are limited to a few general-domain LVLMs and do not include state-of-the-art medical LVLMs specifically trained for CXR analysis. Including such comparisons would provide a more comprehensive evaluation of RadEyeVideo's performance relative to specialized models. Moreover, the authors should consider including a wider range of performance metrics to provide a more nuanced assessment of their method.\n\n4. Reproducibility: While the authors mention that the code is provided in the supplementary materials, the description of the training protocols, preprocessing steps, and model hyperparameters is somewhat incomplete. Detailed documentation of these aspects is necessary to ensure that the results can be independently reproduced.\n\n## Minor Comments\n1. Figures: Figures 3 and 6 could be improved by showing fewer representative slices with zoomed-in regions to enhance readability and clarity.\n\n2. Section 2.1: The introduction of notation in Section 2.1 is not sufficiently explained, especially regarding the forward operator, which is denoted inconsistently.\n\n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition, which can confuse readers unfamiliar with the terminology.\n\n4. Typographical Issues: Minor typographical errors, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7), should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: enhancing the performance of general-domain LVLMs in CXR analysis through the integration of radiologists' eye-gaze data as video sequences. The proposed method, RadEyeVideo, demonstrates technical innovation by capturing both spatial and temporal aspects of eye movements, which can provide valuable insights into the decision-making process of radiologists. However, the evaluation is somewhat limited by the dataset size and the lack of comparisons with state-of-the-art medical LVLMs, which weakens the claims of significance and generalizability. The reproducibility of the approach is also uncertain due to incomplete methodological details. Overall, while the idea has promise, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand comparative analysis, strengthen validation across datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Honghan Wu",
      "Jinge Wu",
      "Yunsoo Kim"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "remote_url": "https://openreview.net/pdf/a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning Robust Representations for Medical Images via Unifying (Self-)Supervisions",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "medical image pre-training",
      "medical image representation learning"
    ],
    "abstract": "Pre-training medical image encoder to provide robust, task-agnostic representations is highly valuable, as it enhances the understanding of medical images and is important for performing many data-scarce analysis tasks. Current pre-training works are unable to integrate various types of supervisions, including self-supervision and external supervision such as segmentation annotations, while they are highly valuable for medical image understanding. Therefore, in this paper, we take the first step toward exploring unifying all common types of supervisions into a pre-training framework through a same scalable way. This require the pre-training framework being both unified, for accommodating diverse data and extensible, and effective, for making heterogeneous data synergistically assist unknown downstream tasks. To this end, we propose UmiF, whose principle is that once converted into token embeddings in a unified space, all diverse supervisions can be effectively utilized via contrastive learning and mask modeling with a same way. With UmiF, we pre-train on 1.66M samples from 14 public datasets, significantly surpassing previous efforts in terms of the dataset scale. We obtain and release the UmiF model, which achieved state-of-the-art performance across various downstream tasks, including classification, segmentation, and detection, retrieval and VQA.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe Authors propose UmiF, a framework that aims to unify pre-training of any arbitrary Image-Label pair to train a robust encoder for any modality. To achieve this the authors propose three tokenizers, that each allows embedding to a shared unified token space (One Image, One Text, One Image Labels/Segmentation). These image / label tokens are either left split or merged to a certain degree, before being used for SSL training through contrastive training in a CLIP'esque fashion or through a reconstruction task. \nThey train their model on a wide variety of paired pre-training datasets and evaluate it on a broad set of downstream tasks, highlighting the final performance of UmiF's method.\n\nWhile innovative, the experiments are insufficient to highlight the proposed methodology. The authors stack a) a larger pre-training dataset b) Token mixing c) Masked and Contrastive losses together and don't provide experiments that disentangle which part brings performance and which part does not. Moreover, the presentation and language used in this paper are of insufficient quality and need a lot of work.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 3\n\n### Strengths\n\nThe proposal to not only include Image-Report pairs but also Images with other Supervision signals is an interesting and to my knowledge novel premise for the medical domain. Their idea of mixing image and supervision tokens are also innovative.\nMoreover, the amount of experiments conducted is broad, highlighting the generality of the ViT feature extractor.\n\n### Weaknesses\n\nWhile the premises are highly interesting the paper in it's current form has some issues:\n1. **Stacking of contributions** Currently the authors stack a variety of things on-top and don't ablate it properly, namely a) a larger pre-training dataset b) the token mixing block and c) the multiple SSL losses. Currently, it is impossible for a reader to know if their methodology is better downstream than the competing methods, as they create a much larger training data corpus. Maybe it's the masked reconstruction component, maybe not.\n2. **Presentation** The presentation of the paper as of right now is poor. It was very hard to read, as the language quality leaves a lot of room for improvement and should be checked by an English speaker to rework the manuscript. Moreover, Fig. 1 does a bad job of explaining the method contributing to difficulties in understanding the proposed method. Figures in the appendix are badly presented: Fig. 2 text is way too large.  Table 3 does not have a caption, The description of the Image-Segment Dataset (Section A.4) is basically non-existent and should be filled accordingly. \n3. **Reproducibility** Currently the author's don't sufficiently explain their configuration of their methods. How was _r_ chosen ? In the text it is mentioned sometimes 1 and sometimes random between [0,1]. How did the authors split their data? Was there a train-test split during pre-training and fine-tuning? Was there different weightings between the losses?\n\n### Minor Points\n- The ablation of _r_ values does not contain r of 0.9 and 1.0 - It's mentioned in the text that these performed substantially worse, but I would like to have these values included in the table. Moreover this table should provide not only RSNA 1% AUC values. The downstream adaptation are just learning of a linear-layer so please provide ablations on more datasets and all values to show if the mixing of tokens actually provides a benefit.\n- The distinction between what this paper does relative to other paper's feels not well worked out. It would help a lot to see what makes this work distinct.\n- Similarities to MedUniC Paper. This paper's Table 2 is very similar to their Table 2 -- I would prefer to highlight this in the caption. \n- There are so many typos in this manuscripts. E.g. spellings of baseline methods:  MedKLIIP/MedKILP/MedKLIP. It feels like no one proof-read this paper ever.\n- The Algorithm 1 is way too text heavy. If the authors want to go into detail about the sampling of their datasets they should move this into a separate algorithm to keep readability high.\n- The authors mention the importance of sampling smaller dataset more regularly but provide no results. Would be great to see an ablation table on this claim in the appendix.\n\n### Questions\n\n- Q1: Did you ablate the importance of using the masking and the contrastive loss by themselves?\n- Q2: How was _r_ chosen?\n- Q3: Shouldn't r be symmetric around 0.5? r=0 would just flip image embeddings to supervision and the other way around. Is this correct, or if not why not? \n- Q4: Have you tried replacing the mixing with a standard masking/token drop-out layer? Would be interesting to see if one actually has to mix tokens or if the dropping of tokens provides a similar regularizing effect.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Overview of the Manuscript**  \nThe authors present UmiF, a framework designed to unify the pre-training of arbitrary image-label pairs, aiming to enhance encoder training across various modalities. The framework incorporates three distinct tokenizers that create a shared token space for images, texts, and image labels/segmentations. By evaluating their model on diverse paired pre-training datasets and a range of downstream tasks, the authors highlight the performance outcomes of UmiF. While the concept is innovative, the experimental design lacks clarity, and the overall presentation requires significant improvement.\n\n**Major Comments**  \n1. **Ablation Studies**: The authors combine several components (a larger pre-training dataset, token mixing, and multiple SSL losses) without clear ablation studies to determine the contribution of each to performance. This limits the ability to evaluate which aspects of the methodology are effective compared to competing methods.\n2. **Presentation Quality**: The manuscript suffers from poor language quality and lacks clarity, necessitating a thorough review by a proficient English speaker. Additionally, Figure 1 fails to adequately convey the methodology, contributing to reader confusion. Figures in the appendix also have issues, such as overly large text in Fig. 2 and a lack of caption for Table 3. The description of the image-segment dataset (Section A.4) is insufficient and requires further elaboration.\n3. **Reproducibility Issues**: The configuration of methods is inadequately explained (e.g., the selection of parameter _r_ is inconsistent). Clarification on data splitting for pre-training and fine-tuning, as well as weighting between losses, is necessary.\n\n**Minor Comments**  \n- The ablation study of _r_ values omits values of 0.9 and 1.0, which should be included in the table. The table should also present AUC values on additional datasets to assess token mixing efficacy.\n- The differentiation between this manuscript and previous works is unclear and should be better articulated.\n- There are notable similarities with the MedUniC paper that should be acknowledged in the caption of Table 2.\n- Numerous typos are present throughout the manuscript, notably in baseline method names (e.g., MedKLIIP/MedKILP/MedKLIP).\n- Algorithm 1 is text-heavy and would benefit from a separate algorithm for dataset sampling details to enhance readability.\n- Claims regarding the sampling of smaller datasets are not supported by results; including an ablation table in the appendix would be beneficial.\n\n**Summary Paragraph**  \nThe manuscript introduces an innovative framework with potential significance for the medical field; however, it lacks robust evaluations to validate its claimed advantages (evaluation), and the presentation is not suitable for publication (clarity). The reproducibility of the results is hampered by unclear methodological details. Despite its strengths in concept development and broad experimentation, substantial improvements are necessary to meet publication standards.\n\n**Decision Recommendation**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified multimodal pretraining framework designed to handle arbitrary image‚Äìlabel pairs by learning a robust encoder adaptable to different modalities. The approach employs three tokenizers (image, text, and image-label/segmentation) to embed inputs into a shared token space, enabling contrastive and reconstruction-based self-supervised learning. The method is trained on multiple paired datasets and evaluated across diverse downstream tasks. While the idea of unifying image and supervision tokens is novel and supported by a broad experimental setup, the current presentation and experimental analysis are insufficient to clearly validate the proposed contributions.\n\n---\n\n**Major Comments**  \n1. **Disentangling Contributions:** The method combines several components‚Äîlarge-scale pretraining data, token mixing, and dual SSL losses (masked and contrastive)‚Äîwithout clear ablation studies. This stacking makes it impossible to determine which factor drives performance improvements. The paper should provide controlled experiments isolating each contribution.  \n2. **Presentation and Readability:** The manuscript is very difficult to follow due to weak language quality, dense text, and poor figure design. Figure‚ÄØ1 does not effectively convey the method, Figure‚ÄØ2‚Äôs text is oversized, Table‚ÄØ3 lacks a caption, and the Image‚ÄëSegment dataset description (Sec.‚ÄØA.4) is missing. Substantive English editing and clearer visual explanations are needed.  \n3. **Reproducibility:** Key implementation details are missing. The selection and variability of *r* are unclear (sometimes fixed, sometimes random). Data splits for pretraining and fine‚Äëtuning, and relative loss weightings, are not specified. Without this, reproducing the work is infeasible.\n\n---\n\n**Minor Comments**  \n- Include *r*‚ÄØ=‚ÄØ0.9‚ÄØand‚ÄØ1.0 results in ablations and expand evaluation beyond RSNA‚ÄØ1%‚ÄØAUC to additional datasets.  \n- Clarify how this work differs from related approaches (e.g., MedUniC); explicitly note similarity of Table‚ÄØ2 content.  \n- Numerous typos and inconsistent baseline spellings (e.g., MedKLIIP/MedKLIP) require correction.  \n- Simplify Algorithm‚ÄØ1 or split excessive detail; it hinders readability.  \n- Provide evidence for the stated benefit of oversampling smaller datasets.  \n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses an important problem of multimodal representation unification and proposes an original approach that mixes supervision and image tokens. However, insufficient experimental isolation and incomplete methodological reporting undermine the validity of the results. The innovation is promising, yet the weak presentation, unclear evaluation design, and reproducibility gaps significantly limit its current impact.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents UmiF, a pre-training framework for medical image encoders that integrates multiple types of supervision, including self-supervision and annotations like segmentation labels, into a unified approach. UmiF creates a common embedding space with a token grouping strategy to leverage diverse data types for various downstream tasks. Pre-trained on 1.66 million samples from 14 public datasets, UmiF was evaluated in classification, segmentation, detection, retrieval, and VQA tasks.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- The paper introduces unifying representations from multiple supervisions into a single embedding space for self-supervised learning and proposes a grouping strategy for mixed learning of representation vectors.\n\n- The model‚Äôs effectiveness is validated through evaluations across four different downstream tasks.\n\n### Weaknesses\n\n- **Contrastive Learning Design Concerns**: The design of the contrastive learning setup after grouping raises questions. According to the paper, a positive pair is represented by \\(f1_i, f0_j\\) where i and j are indices from different data points, meaning \\(f1_i\\) and \\(f0_j\\) are from different samples. Typically, a positive pair should be \\(f1_i, f0_i\\), where both elements come from the same sample, making the current approach unclear.\n\n- **Unfair Comparisons in Downstream Tasks**: There are substantial fairness issues in the downstream task comparisons. Competing models, such as Med-Unic and MGCA, are pre-trained on datasets with 380K and 217K samples respectively, whereas this study uses 1.66 million data pairs, including 1 million images. The model‚Äôs performance advantage in downstream tasks may stem from this large data disparity, making it difficult to attribute improvements solely to the proposed pre-training strategy.\n\n- **Performance in Table 2**: In Table 2, despite using more training data and supervision than Med-Unic, the proposed model does not achieve the best performance, which raises questions about the efficiency of the approach.\n\n- **Limited Ablation Study on Parameter r**: In the ablation study on the parameter r, only 1% of the RSNA dataset is used, rather than the full dataset, and no similar experiments are conducted on other datasets. It is unclear if the chosen r value on RSNA is robust and generalizable to other tasks, as this limited evaluation does not provide strong evidence of robustness.\n\n- **Inconsistencies Between Text and Figures**: There are inconsistencies between the text and figures. For instance, the text describes vector groups as Group 1 and Group 0, but the figure labels them as Group 1 and Group 2. \n\n- **CLS Token Generation Unclear**: The generation of the CLS token information is not clearly explained.  According to the figure, the CLS token appears to be an output of the Flexible Token Grouping, but the paper does not specify how the CLS token is produced. Further clarification on this process would improve understanding.\n\n### Questions\n\nplease refer to Weaknesses\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\n\nThis manuscript introduces UmiF, a pre-training framework designed for medical image encoders. UmiF combines various types of supervision, including self-supervision and segmentation labels, into a cohesive approach intended to create a common embedding space. It leverages a token grouping strategy to harness diverse data types for several downstream tasks, having been pre-trained on 1.66 million samples from 14 public datasets and evaluated across classification, segmentation, detection, retrieval, and VQA tasks.\n\n### Major Comments\n\n1. **Contrastive Learning Design Concerns**: The positive pair representation indicated by \\(f1_i, f0_j\\) raises uncertainty, as typically, positive pairs are derived from the same sample.\n  \n2. **Unfair Comparisons in Downstream Tasks**: The use of significantly larger training datasets (1.66 million samples) compared to competitors, which used 380K and 217K samples, may unfairly skew performance comparisons, making it hard to attribute effectiveness solely to the proposed pre-training strategy.\n  \n3. **Performance in Table 2**: Despite the increased training data and supervision over Med-Unic, the proposed model does not achieve superior performance, questioning the efficiency of the approach.\n  \n4. **Limited Ablation Study on Parameter r**: The ablation study employs only 1% of the RSNA dataset, and other datasets are not similarly evaluated, leaving the robustness and generalizability of the chosen r value unproven.\n  \n5. **Inconsistencies Between Text and Figures**: Discrepancies exist between the textual descriptions and figure labels regarding vector groups, which could lead to confusion.\n  \n6. **CLS Token Generation Unclear**: The method through which the CLS token is generated remains ambiguous. Although suggested as an output of the Flexible Token Grouping, details on its production are insufficient.\n\n### Minor Comments\n\n- Additional clarity on the generation of the CLS token is required to enhance reader understanding.\n- The manuscript should ensure consistency between text and figures to avoid confusion.\n\n### Summary Paragraph\n\nIn summary, while this study presents a notable contribution to the field by proposing a unified framework for medical image encoders, it faces several challenges related to its design, evaluation, and clarity. The significance and innovation of UmiF are evident, yet the evaluation has limitations that undermine reproducibility and robustness. The strengths lie in the comprehensive approach and extensive dataset used, but weaknesses in methodological transparency and fairness in comparisons must be addressed.\n\n### Decision Recommendation\n\nMajor revision is recommended to address the identified issues before the manuscript can be considered for acceptance.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified pre-training framework for medical image encoders that combines multiple forms of supervision‚Äîincluding self-supervised learning and annotated signals such as segmentation labels‚Äîinto a single embedding space. The approach employs a token grouping strategy to align heterogeneous data sources, aiming to improve performance across diverse downstream medical imaging tasks. The authors pre-train on 1.66 million samples from 14 public datasets and evaluate results on classification, segmentation, detection, retrieval, and visual question answering tasks. Overall, the paper is clearly written, but some methodological choices and comparisons raise concerns about fairness and interpretability.\n\n---\n\n**Major Comments**  \n1. **Contrastive Learning Design** ‚Äì The description of positive and negative pair formation is unclear. The paper defines positive pairs across different samples \\((f1_i, f0_j)\\), whereas contrastive frameworks typically use features from the same sample \\((f1_i, f0_i)\\). This mismatch requires clarification and potentially undermines the soundness of the learning formulation.  \n2. **Fairness of Downstream Comparisons** ‚Äì Competing models (e.g., Med-Unic, MGCA) are trained on substantially smaller datasets (380K and 217K samples) compared to UmiF‚Äôs 1.66 million samples. This disparity raises questions about whether observed improvements stem from methodological advantages or simply from larger data exposure.  \n3. **Performance Results (Table 2)** ‚Äì Despite greater data volume and supervision, UmiF does not surpass Med-Unic in some metrics, suggesting limited efficiency or benefit from the proposed strategy.  \n4. **Ablation Study on Parameter r** ‚Äì The ablation is restricted to 1% of the RSNA dataset and not replicated on other datasets. As such, the claimed generalization of the selected r value is weak.  \n5. **Inconsistencies Between Text and Figures** ‚Äì Figure labels (Group 1, Group 2) do not match the text designations (Group 1, Group 0), causing confusion in interpreting the grouping mechanism.  \n6. **CLS Token Generation** ‚Äì The role and derivation of the CLS token are insufficiently explained. Although shown as an output of the Flexible Token Grouping module, the paper lacks details on how this token is generated.\n\n---\n\n**Minor Comments**  \n- Improve alignment of figure terminology with text to ensure consistency.  \n- Clarify legends and notations in figures related to the grouping process.  \n- Check for small phrasing ambiguities in method descriptions.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study tackles an important problem‚Äîbuilding unified medical image representations across supervision types‚Äîand contributes an innovative grouping strategy. However, methodological ambiguity in contrastive pair design and limited ablation analysis weaken the evaluation of robustness. Comparative advantages are difficult to attribute solely to the method because of notable data imbalance. While reproducibility appears feasible based on public datasets, greater clarity in implementation details (e.g., CLS token, parameter settings) would improve confidence. The work is conceptually interesting but requires stronger experimental justification.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a multi-supervision unification strategy for medical image pretraining. The method allows report, segmentation, and classification (+ some others) types of supervision to jointly train one representation. The used modality is Chest X-ray (CXR). The authors collected a large-scale dataset sourced from the public domain, reaching 1M images and 1.66M supervision labels. It is reported the model, namely, UmiF reaches SOTA for a number of downstream tasks.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\n1. The authors have gone through tremendous effort in collecting and inventorying the datasets. I can imagine the implementation wouldn't be easy to iron out the differences in the datasets and put them together to train one model. For that, I believe the significance of the paper should be pointed out.\n\n2. the benchmarking is comprehensive, ranging over the common medical image analysis tasks.\n\n### Weaknesses\n\n1. The clarity of the paper is a concern. Many places in the text lack proper explanation and are somewhat confusing.  For example, L230 \"This interesting design allows more diverse views and enriches the learning tasks with many possibilities, surpassing previous VL learning approaches.\", The authors should clearly state why it is interesting. What are the many possibilities? What are the other diverse views (isn't the modality just CXR)? What evidence indicates your method surpasses the previous VL learning approach?\n2. Furthermore, Sec 3.2, perhaps the most important section in the paper is not well written, I've read it a few times and I still don't believe I have grasped the exact approach. \n3. I find Figure 1 hard to follow, the quantities in Sec 3.2 should be mapped to the figure. I also don't get the colour coding in Figure 1 for those tokens. The yellow/blue/no boundary cubes are also a very confusing way of presentation.\n4. The improvement over the previous state-of-the-art is marginal around 1 point in various measurements. As the authors claim a large-scale dataset of 1.66M image-supervision pairs vs \"previous effort of mostly limited to 380K image-report pairs or 838K images\", it is worth rethinking whether the effort spent on training such a large model on the twice amount of data makes sense.\n5. The title claims \"learning robust representation for medical images ...\", medical images are not just CXR, I would recommend claiming a lesser scope unless common modalities such as MRI/CT are also used.\n6. In Sec 3.1, the authors use \"modality abstraction\", which sounds cool but I would say it is actually confusing, the procedure is a label format conversion.\n\n### Questions\n\nPlease address weaknesses #2&4. The paper could use some professional editing services.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a multi-supervision unification strategy for the pretraining of medical images, specifically targeting Chest X-ray (CXR) data. The proposed method enables the contemporaneous training of various supervisory signals, including report, segmentation, and classification. The authors have compiled an extensive public dataset consisting of 1 million images and 1.66 million supervision labels. The results indicate that the UmiF model achieves state-of-the-art performance on several downstream tasks.\n\n**Major Comments:**\n1. **Clarity Issues:** The paper's clarity is insufficient, with several sections lacking adequate explanations. For instance, on line 230, the claim that the design \"allows more diverse views and enriches the learning tasks\" is vague. The authors should clarify what these views entail and present evidence for how their method surpasses previous vision-language (VL) learning approaches.\n2. **Key Section Quality:** Section 3.2, a critical part of the manuscript, is poorly articulated. Multiple readings have not clarified the specific approach, necessitating a clearer exposition.\n3. **Figure Presentation:** Figure 1 is difficult to interpret; quantities discussed in Section 3.2 should correspond more directly with the figure elements. The color coding for tokens is unclear, and the use of yellow/blue/no boundary cubes complicates the communication of ideas.\n4. **Marginal Improvements:** The reported advancement over prior state-of-the-art results is marginal, showing only about a 1-point improvement across various metrics. Given the dataset's vast scale, the authors should re-evaluate whether this investment in training a large model is justified.\n5. **Scope of Claim:** The title suggests a broad claim of \"learning robust representation for medical images.\" However, as the work focuses solely on CXR, a more accurate title should reflect this narrower scope unless other modalities like MRI or CT are incorporated.\n6. **Terminology Confusion:** The term \"modality abstraction\" used in Section 3.1 is misleading; it would be clearer to refer to it as label format conversion.\n\n**Minor Comments:**\n- The paper would benefit from professional editing services to enhance overall clarity, particularly in addressing weaknesses noted in comments 2 and 4.\n\n**Summary Paragraph:**  \nIn relation to the editorial criteria, this study demonstrates significance through its extensive dataset and comprehensive benchmarking across common medical imaging tasks. However, clarity and presentation weaknesses are significant drawbacks. The innovation is tempered by marginal performance improvements, raising questions about the justification for the extensive dataset. Evaluation of the methods requires better articulation and clearer linkage between figures and text. Finally, reproducibility may be hindered by localized definitions and terminologies that could confuse readers.\n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a multi-supervision unification strategy for medical image pretraining, termed **UmiF**. The key contribution lies in enabling multiple supervision types‚Äîreport, segmentation, and classification, among others‚Äîto jointly train a single, unified representation using chest X-ray (CXR) data. The authors assemble a large-scale dataset of approximately one million images with 1.66 million supervision labels from public sources. Experimental results suggest that the proposed model achieves state-of-the-art performance on several downstream CXR tasks. While the scope and data collection effort are commendable, the manuscript‚Äôs clarity and justification of design choices require substantial improvement.\n\n---\n\n**Major Comments**  \n1. **Clarity and Explanation** ‚Äì The manuscript lacks adequate detail in several places. Statements such as ‚Äúenriches the learning tasks with many possibilities‚Äù (around L230) should be explicitly supported. The authors should define what constitutes ‚Äúdiverse views,‚Äù provide evidence for performance advantages, and explain how the approach surpasses prior vision-language methods.  \n2. **Method Description (Sec. 3.2)** ‚Äì This section, central to the contribution, is difficult to follow. The exact formulation and procedural steps of the proposed method remain unclear even after multiple readings.  \n3. **Figure 1 Interpretability** ‚Äì The figure is hard to interpret. The notation and quantities described in Sec. 3.2 should be directly linked to figure elements. The color coding (yellow/blue and partitioned cubes) is confusing and needs clearer explanation.  \n4. **Performance Significance** ‚Äì Improvements over previous state-of-the-art methods are marginal (~1 point). Given the dataset‚Äôs significantly larger scale, the paper should discuss whether such incremental gains justify the extensive effort and computational cost.  \n5. **Scope of Claims** ‚Äì The title and text refer broadly to ‚Äúmedical images,‚Äù but the experiments are limited to CXR. The claim should be narrowed to this modality unless evidence for other imaging types (e.g., CT, MRI) is provided.  \n6. **Terminology Precision** ‚Äì The phrase ‚Äúmodality abstraction‚Äù (Sec. 3.1) is misleading; it appears to describe a label format conversion rather than true modality abstraction.\n\n---\n\n**Minor Comments**  \n- The manuscript would benefit from professional language editing to improve readability.  \n- Ensure consistency between textual descriptions and figure notation.  \n- Clarify visual elements and legends in all figures.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the large-scale dataset and unified training approach are valuable contributions. Regarding **innovation**, the concept of multi-supervision unification is interesting but insufficiently articulated. **Evaluation** appears comprehensive across common CXR tasks but shows only modest gains. **Reproducibility** is limited due to unclear methodological details and missing descriptions of implementation procedures. Overall, the work has potential but requires major clarification and more critical discussion of the incremental performance improvement relative to the significant data and model scale.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a methodological contribution in medical imaging: a unified pre‚Äëtraining framework (UmiF) for medical image encoders that integrates multiple supervision types (self‚Äësupervision, segmentation, classification, and vision‚Äëlanguage signals). This is model- and representation-learning methodology directly within TMI scope.\n\n2. **Novelty & Contribution Level**  \nThe work claims to be the first to unify four major forms of supervision in medical image pre‚Äëtraining. The central innovations are (1) a unified token space for diverse modalities (image, text, segmentation mask) and (2) a ‚Äúflexible token grouping‚Äù strategy enabling various contrastive and masked modeling tasks simultaneously. While the conceptual idea‚Äîunifying multimodal supervision‚Äîis inspired by recent general-domain vision‚Äëlanguage models (e.g., OFA, BEiT‚Äëstyle unification), its systematic adaptation to heterogeneous medical datasets constitutes a meaningful advance for the domain. The degree of novelty is moderate‚Äëto‚Äëhigh for medical imaging methodology.\n\n3. **Technical and Experimental Rigor**  \nTechnical formulation is sound and internally consistent. Loss functions for contrastive and consistency learning are detailed. Experimental evaluation is broad‚Äîcovering classification, segmentation, detection, VQA, and zero‚Äëshot tasks‚Äîon public datasets totaling 1.66‚ÄØM pairs. Comparisons to state‚Äëof‚Äëthe‚Äëart (MGCA, Med‚ÄëUniC, MRM) are appropriate. A potential concern is the uniform data harmonization using GPT‚Äë4 translation and report generation, which may introduce bias or unverified text quality; this should be discussed more quantitatively. Reproducibility could also be strengthened by specifying tokenization vocabulary sizes, exact backbone configuration, and ablation hyperparameters.\n\n4. **Clarity and Presentation**  \nThe manuscript is generally well structured and readable. Major figures and tables are clear, though minor grammar issues remain (‚Äúto to acquire‚Äù, ‚Äúbeing both unified, for accommodating diverse data and extensible, and effective‚Äù etc.). Sections are logically organized; however, some sentences are overly long, and acronyms (e.g., UmiF) could be introduced earlier.\n\n5. **Ethical and Reproducibility Compliance**  \nAll datasets are public, and IRB risk is minimal. Authors state intent to release code and checkpoints upon acceptance and clearly acknowledge limitations and bias risks‚Äîappropriate under TMI policy.\n\n---\n\n**Phase 2: IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThis paper proposes *UmiF*, a unified medical‚Äëimage pre‚Äëtraining framework that jointly leverages self‚Äësupervision (contrastive and mask modeling) and external supervision (text reports, captions, class labels, segmentation masks) to learn robust task‚Äëagnostic representations. The approach converts all modalities into a shared token space via modality‚Äëspecific tokenizers and processes them through a single Vision Transformer. A flexible token‚Äëgrouping mechanism randomly partitions tokens across modalities to form paired views for contrastive learning and mask modeling, effectively fusing heterogeneous supervision types. Pre‚Äëtraining on 1.66‚ÄØM samples from 14 datasets, UmiF achieves consistent state‚Äëof‚Äëthe‚Äëart performance across five downstream tasks.\n\n**2. Strengths**\n- Clear methodological innovation in unifying diverse supervision sources under one scalable backbone.  \n- Comprehensive empirical validation across multiple downstream tasks and data‚Äëefficiency regimes.  \n- Solid conceptual grounding linking self‚Äësupervised and vision‚Äëlanguage paradigms.  \n- Demonstrates potential for generalized medical foundation models.\n\n**3. Weaknesses**\n- Limited theoretical justification for why flexible token grouping yields synergy; mostly empirical.  \n- Heavy reliance on GPT‚Äë4‚Äëgenerated or translated reports introduces potential data and linguistic bias not formally evaluated.  \n- Experiments restricted to 2D X‚Äëray data; unclear how easily the approach extends to other modalities (CT, MRI, pathology).  \n- Some reproducibility details (data splits, tokenizer parameters, random seeds) are missing.  \n- Writing quality could be improved for clarity and grammar.\n\n**4. Major Comments**\n1. **Clarify the contribution boundary** ‚Äì Provide more explicit comparison to general-domain unified frameworks (OFA, BEiT, Painter), explaining which design changes are uniquely required for medical data.  \n2. **Quantify generated‚Äëtext quality** ‚Äì Include metrics (BLEU, ROUGE, or radiologist manual ratings) to confirm the reliability of GPT‚Äëgenerated translations and synthetic reports.  \n3. **Explain scalability beyond X‚Äëray** ‚Äì Discuss adaptations needed for volumetric or multi‚Äëchannel modalities.  \n4. **Expand reproducibility details** ‚Äì Document tokenizer vocabulary sizes, ViT configuration, and sampling probabilities.  \n5. **Statistical analysis** ‚Äì Include standard deviation or confidence intervals for performance metrics to support claims of superiority.\n\n**5. Minor Comments**\n- Proofread for redundant words and grammatical errors.  \n- In tables, denote backbone sizes consistently (e.g., ViT‚ÄëB/16).  \n- Add visual examples showing outputs from the grouping strategy to aid intuitive understanding.  \n- Verify citation format consistency.  \n- Clarify timing and ratio of text versus mask tokens during training.\n\n**6. Overall Recommendation:** **Major Revision**  \nThe paper presents an interesting and potentially impactful methodological contribution within TMI scope. However, it requires better justification of novelty relative to existing multimodal frameworks, quantitative validation of synthetic data quality, and detailed reproducibility information. With these revisions, it could reach publishable quality.\n\n**7. Confidence Level:** 4‚ÄØ/‚ÄØ5  \n\n**Summary Decision:** *Major Revision ‚Äì Methodologically promising, requires stronger experimental transparency and clarification of novelty and generalization scope.*",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes UmiF, a unified medical image pre-training framework that integrates diverse supervision types including self-supervision, paired reports, captions, segmentation annotations, and classification labels. The method converts all input modalities (radiology, language, segmentation masks) into a unified token space using modality-specific tokenizers, then applies a flexible token grouping strategy to enable contrastive learning and mask modeling simultaneously. The authors compile a large-scale dataset of 1.66M samples from 14 public datasets, significantly exceeding previous efforts. UmiF uses Vision Transformer as backbone and trains with both alignment loss for contrastive learning and consistency loss for mask modeling. Experiments demonstrate state-of-the-art performance across multiple downstream tasks including classification, segmentation, detection, retrieval, and VQA on medical imaging datasets.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - Equation 1 uses inconsistent indexing notation where positive pairs are denoted as (f1i, f0j) but the loss function ‚Ñì(fi, fj) uses different subscript conventions without clear mapping\n  - The binary sampling strategy in Section 3.2 lacks mathematical rigor in defining the probability distribution for sampling r values beyond stating \"certain probability\" and \"randomly sampled from [0, 1]\"\n  - Equation 2's consistency loss formulation doesn't clearly specify the relationship between teacher network input Concat(Xi, Xs) and the masked student inputs X0, X1\n\n‚Ä¢ **Insufficient technical novelty and contribution clarity**\n  - The core tokenization approach directly borrows from existing works (Section 3.1 acknowledges similarity to Wang et al., 2022b; Zhang et al., 2023) without substantial methodological innovation\n  - The flexible token grouping strategy, while presented as novel, essentially reduces to standard vision-language learning when r=1 (page 5, lines 218-223), suggesting limited conceptual advancement\n  - The unification claim is overstated given that only three modality types are handled, and the abstraction is relatively straightforward (converting coordinates to masks, labels to text templates)\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - Table 2-5 comparisons include methods with different backbone architectures (ResNet-50 vs ViT-B/16), making performance comparisons potentially unfair without proper architectural controls\n  - The ablation study in Table 7 only examines single supervision types versus all combined, lacking systematic analysis of pairwise combinations to understand synergistic effects\n  - Zero-shot evaluation is limited to a single dataset (CXP500 with 500+ images), providing insufficient evidence for generalization claims across diverse medical imaging scenarios\n\n‚Ä¢ **Dataset processing and experimental rigor concerns**\n  - The reliance on GPT-4 for Spanish-to-English translation and report generation (Section 2, Table 1) introduces potential biases and quality inconsistencies not adequately validated\n  - Algorithm 1 and Section 4.1 provide insufficient implementation details regarding the \"certain probability\" for r=1 setting, batch construction specifics, and convergence criteria\n  - The segmentation mask generation process (bottom of Figure 1) lacks technical details about color assignment, resolution matching, and potential information loss during coordinate-to-mask conversion\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation consistency**\n  - Standardize indexing notation throughout Equation 1 by clearly defining the relationship between batch indices i,j and the positive pair construction\n  - Provide explicit probability distributions for the flexible token grouping strategy, including mathematical formulation of the sampling process for r values\n  - Clarify the mathematical relationship in Equation 2 between complete teacher inputs and masked student inputs, potentially with a formal algorithm description\n\n‚Ä¢ **Strengthen technical contributions and positioning**\n  - Conduct deeper analysis comparing the proposed tokenization approach with existing methods, highlighting specific advantages beyond scalability\n  - Provide theoretical or empirical analysis of when and why the flexible grouping strategy outperforms standard vision-language approaches beyond the r=1 case\n  - Expand the modality abstraction framework to handle additional medical data types (e.g., 3D volumes, multi-sequence MRI) to better support the \"unified\" framework claims\n\n‚Ä¢ **Improve experimental comprehensiveness and fairness**\n  - Include systematic comparisons using identical backbone architectures across all baseline methods, or provide separate architectural ablation studies\n  - Design comprehensive ablation studies examining all pairwise and subset combinations of supervision types to quantify individual and synergistic contributions\n  - Expand zero-shot evaluation to multiple datasets and include few-shot learning scenarios to better demonstrate generalization capabilities across medical imaging domains\n\n‚Ä¢ **Strengthen dataset validation and implementation transparency**\n  - Validate GPT-4 generated translations and reports through expert radiologist review or automated quality metrics comparison with original annotations\n  - Provide complete implementation details including hyperparameter sensitivity analysis, convergence behavior, and computational requirements for reproducibility\n  - Include technical validation of the coordinate-to-mask conversion process, potentially comparing with direct coordinate-based supervision methods to justify the design choice",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces UmiF, a unified medical image pre-training framework designed to incorporate a broad spectrum of supervisory signals, including self-supervision and external supervision like segmentation annotations and reports. The framework aims to create robust, task-agnostic representations by integrating diverse modalities into a unified token space and employing a novel flexible token grouping strategy for contrastive learning and mask modeling. The authors claim that UmiF outperforms existing methods across various downstream tasks, including classification, segmentation, and VQA, on multiple datasets. The manuscript is well-written and provides a compelling argument for the importance of integrating multiple supervisory signals for improved medical image understanding.\n\n## Major Comments\n1. Novelty and Positioning: While UmiF introduces an innovative approach to unifying various types of supervisions, the manuscript could benefit from a more detailed comparison to existing works that also attempt to integrate multiple supervisory signals. The novelty of the token grouping strategy and its effectiveness relative to other methods need to be more clearly articulated.\n\n2. Evaluation Design: The experimental setup primarily relies on public datasets, which limits the generalizability of the results. Including a wider range of datasets, especially those from private collections or less common medical imaging modalities, would strengthen the validation of UmiF's robustness and versatility. Additionally, more detailed analyses on the performance gains attributed to different types of supervisory signals would be beneficial.\n\n3. Comparisons: The manuscript compares UmiF to a variety of state-of-the-art methods but lacks a comprehensive discussion on how the proposed framework differs from closely related approaches, such as those that also integrate multiple types of supervisory signals. More direct comparisons and analyses of the unique contributions of UmiF would enhance the manuscript's credibility.\n\n4. Reproducibility: Although the authors state that they will release the code and model checkpoints upon acceptance, the manuscript does not provide sufficient detail regarding the experimental setup, including hyperparameters, data preprocessing steps, and training protocols. Providing a more comprehensive description of these aspects is crucial for ensuring reproducibility.\n\n## Minor Comments\n1. Figures: Figures 1 and 3 are somewhat cluttered and could be improved by showing fewer representative slices with zoomed-in regions to enhance readability.\n   \n2. Notation and Terminology: Some notation and terminology are inconsistent or insufficiently explained. For example, the forward operator and the token grouping strategy could benefit from clearer definitions.\n\n3. Acronyms: Several acronyms are used without proper definition, which can be confusing for readers unfamiliar with the field. Defining acronyms like \"RSNA,\" \"SIIM,\" and \"VQA-RAD\" would improve clarity.\n\n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" and \"undersampling maskes\" should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical image analysis: creating robust, task-agnostic representations that can generalize across various downstream tasks. UmiF's novel approach to integrating diverse supervisory signals into a unified framework is technically interesting and potentially impactful. However, the evaluation is somewhat limited to public datasets and lacks a detailed analysis of the contributions of different supervisory signals. The reproducibility of the approach is also uncertain due to incomplete methodological details. While the idea has promise, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand comparative analysis, strengthen validation across a broader range of datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Dongsheng Li",
      "Hoifung Poon",
      "Lili Qiu",
      "Naoto Usuyama",
      "Sheng Zhang",
      "XINYANG JIANG",
      "Xiaoxuan He",
      "Xufang Luo",
      "Yifan Yang",
      "Yuqing Yang",
      "Zilong Wang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_901796e0b8b3eeca3d767c044473e3b38b2d8cc1.pdf",
    "remote_url": "https://openreview.net/pdf/901796e0b8b3eeca3d767c044473e3b38b2d8cc1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  }
]