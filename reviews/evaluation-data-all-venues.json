[
  {
    "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical Dataset",
      "Breast Cancer Bone Metastasis",
      "Diagnosis",
      "Prognosis",
      "Sparse CT reconstruction",
      "CT",
      "X-ray",
      "Large language model",
      "AI for Science"
    ],
    "abstract": "Breast cancer bone metastasis (BCBM) affects women’s health globally, calling\n for the development of effective diagnosis and prognosis solutions. While deep\n learning has exhibited impressive capacities across various healthcare domains, its\n applicability in BCBM diseases is consistently hindered by the lack of an open,\n large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis\n (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical\n resource, which is derived from a well-accepted murine BCBM model. The unique\n advantage of BoneMet over existing human datasets is repeated sequential scans\n per subject over the entire disease development phases. The dataset consists of\n over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D\n CT scans, and detailed biological data (e.g., medical records and bone quantitative\n analysis), collected from more than five hundreds mice spanning from 2019 to\n 2024. Our BoneMet dataset is well-organized into six components, i.e., Rotation\nX-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further\n show that BoneMet can be readily adopted to build versatile, large-scale AI models\n for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term\n disease monitoring. Our preliminary results demonstrate that BoneMet has the\n potentials to jump-start the development and fine-tuning of AI-driven solutions\n prior to their applications to human patients. To facilitate its easy access and\n wide dissemination, we have created the BoneMet package, providing three APIs\n that enable researchers to (i) flexibly process and download the BoneMet data\n filtered by specific time frames; and (ii) develop and train large-scale AI models for\n precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a novel large-scale dataset (named BoneMet) of Breast tumor bone metastasis for disease diagnosis, prognosis, and treatment management. It consists of six components: Rotational X-Ray Imagery, Reconstructed CT Imagery, Segmented CT Imagery, Registered CT Imagery, Region of Interest CT Imagery, and Mice Medical Records. Besides, the author conducted a series of experiments on the BoneMet dataset by developing various deep learning models to exhibit its applicability and efficiency in managing BTBM disease. The enormous open-source dataset has significant implications for the development of new algorithms in this field beyond doubt.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n(1) The motivation of this study is clear and the background is well presented in the manuscript.\nThe main contribution of this work is to release a large-scale breast tumor bone metastasis dataset BoneMet, for the research community. Upon the subsets of the BoneMet, the researchers are allowed to develop some novel approaches to solve the task of breast tumor bone metastasis diagnosis and prognosis, and thus facilitating the automated analysis for breast tumor bone metastasis.\n(2) Apart from the information of the collected dataset, the authors also developed several deep learning models to validate the applicability and efficacy of BoneMet.\n(3) Dataset, code and tutorials are made available for free use.\n\n### Weaknesses\n\n(1) The technical contributions of this study are quite marginal, as most of the deep learning models are built upon existing methods.\n(2) Some details about the dataset preparation and experiments are missing.\n(3) Some settings and analyses in the experiment may not support the objectives.\n(4) The quality of the labels, particularly the pixel-wise annotation of the bone, might be questionable.\n\n### Questions\n\n1. Authors describe that Seg-CT and Recon-CT are independent. What is the difference between 3D CT scans in Recon-CT and Seg-CT? What is the meaning of the adjective 'segmented'?\n2. Line 341 \"Second, we observe a significant training-test accuracy gap of 20.4% with ViT (w/o STA), indicating a pronounced overfitting issue inherent to the ViT architecture.\"\nI think ViT, as a visual backbone network, is less overfitting compared to other structures such as CNN. In addition, is the decrease in accuracy necessarily overfitting?\n3. Section 3.1, the purpose and analysis of experiments is confusing. Why comparing two ViT models could demonstrate the applicability of the Rotation-X-Ray dataset to manage BTBM disease? What is the relation between the effectiveness of\nSTA and the applicability of the Rotation-X-Ray dataset? All subsections 3.2-4 have similar questions.\n4. The experiment did not report data partitioning.\n5. Table 2 reported metrics on the training and testing data. Is there any evaluation/dev data?\n6. The experiment reported many indicator values, but their actual clinical significance has not been evaluated. For example, what are the Precision/Recall/F1-Score/Accuracy of clinical experts on the BTBM diagnosis? Besides, the 3D-GAN, T-VAE, and ST-VAE methods achieve SSIM values of 0.767, 0.817, and 0.860. So, what role can they play in clinical practice?\n7. The authors used many existing softwares as the processing tools to generate the sub-datasets of BoneMet. Please provide the corresponding reference and specify whether it is free of charge or not. Also, please accurately explain in which case the datasets are processed by the Seg-API, Regist-API and RoI-API that were developed by the authors.\n8. Are Pre-Op and Post-Op refer to pre-operation and post-operation? Please give the full spelling.\n9. For RoI-CT, the tibiae ROI is assigned with different pixel values. As shown in Figure 4, for each CT example, is the processed CT image contains the values of 180, 240, 60 and 0 only? If yes, the original CT turns to a segmentation mask instead, and lots of information would be lost.\n10. In the whole preparation procedure, it seems that the manual annotation of the organs or tumors is not involved, except for the manual cropping of fibular by  CTAn®. How could we ensure the effectiveness of the existing software?\n11. In Table 1, the column \"Spatial Resolution\" might be accurately filled out as most of them indicate the organs. Please revise it.\n12. Some details about the BoneMet dataset are missing. For instance, the number of mice is not introduced. Besides, in Table 1, the images/records are classified into two categories, i.e., positive and negative. Is the label assigned at the slice level or animal level? Besides, we noticed that the numbers of Seg-CT are larger than those of Recon-CT, which makes it confusing, as the Seg-CT is obtained from Recon-CT.\n13. I also have concerns about the MiceMediRec dataset. It mainly consists of the medical record and quantitative analysis results of 3D CT images, simulation and mechanical testing. The authors list only a part of the features in Table S2 in the supplementary file. However, all feature should be provided. The off-the-shelf software, such as CTAn®, was used to generate the quantitative parameters, which took about half an hour for each case of animal. In Section 3.3, the multi-modal prognostic assessment model first leveraged the GAN to produce the CT scan of the future point and use the quantitative data in the MiceMediRec dataset as the ground truth label to validate the predicted reaction force values. However, the details about how to obtain the predicted reaction force values are not clear.\n14. In my opinion, multi-modal learning might refer to a way to integrate the information of multi-source data to build a model. However, the application in Section 3.3 just used CT data as model input. Please consider revising it.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe authors have obtained ethical approval from their institution, and thus, so the ethics review is not needed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents BoneMet, a novel large-scale dataset focused on breast tumor bone metastasis (BTBM) aimed at facilitating disease diagnosis, prognosis, and treatment management. The dataset comprises six components, including various types of CT imagery and medical records from mice. To demonstrate the dataset's applicability, the authors have conducted experiments utilizing multiple deep learning models. The creation of this open-source dataset holds significant promise for advancing research and algorithm development in the field.\n\n### Major Comments\n1. **Novelty and Depth**: The technical contributions are limited, as many deep learning models are based on existing methodologies rather than introducing new concepts.\n2. **Dataset Preparation and Experimentation**: Key details regarding dataset preparation and experimental setup are inadequate and require elaboration.\n3. **Relevance of Experiments**: Some experimental settings and analyses do not clearly align with the study’s objectives.\n4. **Label Quality Concerns**: The quality of pixel-wise annotations, particularly regarding bone labels, appears questionable.\n\n### Minor Comments\n- Clarify the differences between Seg-CT and Recon-CT. Define \"segmented\".\n- Address whether the observed accuracy gap with ViT indicates true overfitting.\n- Provide rationale for comparing two ViT models in terms of the dataset's applicability.\n- Data partitioning details were not mentioned.\n- Clarify the presence of evaluation data in the experimental metrics reported in Table 2.\n- Assess the clinical relevance of reported performance indicators compared to clinical expert benchmarks.\n- Provide references for software used in dataset processing, including details on cost and functionality.\n- Expand on the definitions of Pre-Op and Post-Op.\n- Confirm whether the pixel values in RoI-CT offer sufficient segmentation data without loss of information.\n- Justify the absence of manual annotation in the dataset preparation process.\n- Revise the \"Spatial Resolution\" in Table 1 for accuracy.\n- Include additional details on the BoneMet dataset, such as the number of mice involved and clarification of labeling at slice versus animal level.\n- Ensure comprehensive feature listings for the MiceMediRec dataset in supplementary materials.\n- Clarify the methodology behind predicting reaction force values using the GAN model.\n- Consider revisions to multi-modal learning definitions used in Section 3.3, as they appear limited.\n\n### Summary Paragraph\nIn evaluating the manuscript against TMI editorial criteria, the significance of the BoneMet dataset is evident, given its potential for impactful research in BTBM. The innovation is moderate, primarily due to the reliance on existing deep learning models. For evaluation, while the experimental design showcases various applications of the dataset, critical details are lacking, which affects reproducibility. Overall, the manuscript presents strengths in dataset contribution and accessibility while revealing weaknesses in methodological clarity and experimental validation.\n\n### Decision Recommendation\nMajor revision is recommended to address substantial concerns related to technical contributions and detail in the dataset and experimental framework.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *BoneMet*, a large-scale open-source dataset on breast tumor bone metastasis (BTBM), developed to support research in disease diagnosis, prognosis, and treatment management. The dataset comprises six subcomponents—Rotational X-Ray, Reconstructed CT, Segmented CT, Registered CT, Region of Interest CT, and Mice Medical Records—and is accompanied by code and tutorials. The authors further demonstrate the dataset’s applicability through multiple experimental deep learning models. Overall, the paper is clearly motivated and well organized, offering a valuable data resource, though the technical contributions and certain methodological details could be strengthened.\n\n**Major Comments**  \n1. **Technical Contribution:** The deep learning models mainly build upon existing architectures, resulting in limited methodological novelty.  \n2. **Dataset Clarity:** Key details regarding dataset preparation and processing are missing, including information on data partitioning, labeling strategy (slice vs. animal level), number of subjects, and label quality.  \n3. **Experimental Design:** The experimental setup and interpretation are sometimes unclear. The rationale linking specific model comparisons (e.g., ViT variants) to the stated objectives requires explanation; some analyses do not fully substantiate the conclusions.  \n4. **Annotation and Processing:** The pixel-wise label quality, especially for bone segmentation, is uncertain. It remains unclear how accurately automated tools perform without manual verification.  \n5. **Use of Software Tools:** The manuscript employs various existing tools (e.g., CTAn®, Seg-API, Regist-API, RoI-API) without fully specifying licensing, references, or usage contexts.  \n6. **Clinical Relevance:** Reported quantitative metrics (e.g., SSIM, F1, accuracy) lack comparison with clinical benchmarks, limiting interpretation of the dataset’s practical impact.  \n7. **MiceMediRec Module:** The structure, features, and ground-truth generation for this sub-dataset need clearer description—particularly the process for obtaining predicted and actual force values.  \n8. **Terminology and Concept Use:** Terms such as “multi-modal learning,” “segmented CT,” and “Pre-/Post-Op” require clearer definitions to avoid ambiguity.\n\n**Minor Comments**  \n- Clarify Table 1 entries, particularly “Spatial Resolution.”  \n- Confirm whether Seg-CT and Recon-CT images differ beyond segmentation masks.  \n- In Figure 4, specify pixel-value encoding for ROI-CT images.  \n- Review missing or incomplete references for software tools.  \n- Check typographical consistency and acronym definitions (e.g., STA).  \n\n**Summary Paragraph (TMI Editorial Criteria)**  \nThe dataset addresses a significant and underexplored clinical application, enhancing reproducibility and data accessibility for BTBM research. Its innovation lies primarily in dataset creation rather than modeling advances. While significance and potential impact are high, methodological rigor is reduced by insufficient experimental detail and unvalidated label accuracy. Evaluation is broad but lacks clinical contextualization, and reproducibility would benefit from more complete documentation of preprocessing and partitioning procedures.\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial clarification of dataset composition, labeling, experimental rationale, and clinical relevance is required before the paper can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces BoneMet, the first large-scale, open dataset specifically designed for BTBM research. This dataset offers over 50 terabytes of high-resolution, multi-modal data, including 2D X-rays, 3D CT scans, and comprehensive biological records collected from thousands of mice. Structured into six components, the dataset may be suitable for a range of AI tasks focused on BTBM diagnosis, prognosis, and treatment. The authors also conducted extensive experiments to demonstrate the usability of this dataset. The dataset, APIs for flexible data processing and retrieval, accompanying code, and tutorials are freely available. To some extent, this dataset makes a valuable contribution to this field.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.This paper presents a large-scale, open dataset for the diagnosis and prognosis of breast tumor bone metastasis (BTBM), with a total volume of over 50 terabytes, encompassing various modalities. To some extent, this dataset makes a valuable contribution to this field.\n2.The authors have developed a specialized toolkit for accessing and processing the dataset, facilitating its use for researchers in the field.\n3.The authors conducted detailed experiments that demonstrate the usability of this dataset for some tasks related to BTBM diagnosis and prognosis.\n\n### Weaknesses\n\n1.This dataset is derived from mice, and there are certain differences between mouse skeletons and human skeletons. Can models trained on this dataset be effectively transferred to the diagnosis and prognosis of human BTBM? If applicable, how is the performance? If not, what clinical value does this dataset or model hold?\n2.The authors do not provide sufficient details regarding the experimental setup, particularly how the data was partitioned. The experiments appear to be internal validations conducted solely within the dataset, with no external validation on other datasets. This raises concerns about the generalization ability of the models trained on this dataset. Therefore, it remains uncertain whether this dataset can be used to build versatile large-scale AI models or foundational models.\n\n### Questions\n\nrefer to the weaknesses.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents BoneMet, a pioneering large-scale open dataset developed for research in breast tumor bone metastasis (BTBM). With over 50 terabytes of high-resolution, multi-modal data, including 2D X-rays, 3D CT scans, and extensive biological records from thousands of mice, this dataset aims to support various AI tasks related to BTBM diagnosis, prognosis, and treatment. The authors have also provided APIs, code, and tutorials to facilitate data processing and accessibility. Overall, the contribution of this dataset to BTBM research is noteworthy.\n\n**Major Comments:**  \n1. While the dataset is comprehensive, it is derived from mice, raising concerns about the transferability of models trained on it to human BTBM diagnosis and prognosis. The manuscript lacks clarification on the performance of such models and their clinical value.\n2. The description of the experimental setup is inadequate, particularly concerning data partitioning. The experiments seem to be internal validations within the dataset without external validation, which casts doubt on the generalizability of the models developed.\n\n**Minor Comments:**  \n- Consider providing additional information on the differences between mouse and human skeletons to better contextualize the dataset’s application.\n- Clarify the specific tasks related to BTBM diagnosis and prognosis for which the dataset demonstrates usability.\n\n**Summary Paragraph:**  \nIn terms of significance, BoneMet represents a valuable resource for BTBM research, but concerns about its innovation stem from the lack of validation on human models. The evaluation of the dataset's applicability is limited due to insufficient experimental detail, particularly regarding generalizability. Reproducibility is also undermined by the absence of external validation, which is crucial for assessing the dataset's robustness in diverse applications. \n\n**Decision Recommendation:**  \nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *BoneMet*, a large-scale, open-access dataset specifically designed for research on breast tumor bone metastasis (BTBM). Comprising over 50 terabytes of high-resolution, multi-modal imaging and biological data from thousands of mice, the dataset includes 2D X-rays, 3D CT scans, and detailed biological records. Structured into six components, it aims to support various AI applications related to BTBM diagnosis, prognosis, and treatment. The authors provide accompanying code, APIs, and tutorials and perform experiments to illustrate dataset usability. Overall, the dataset represents a valuable resource, though its broader translational relevance requires further clarification.\n\n---\n\n**Major Comments**  \n1. **Translational relevance and generalizability:** Since BoneMet is derived entirely from mouse data, the anatomical and biological differences between mice and humans raise questions about the dataset’s applicability to human BTBM research. The authors should clarify whether the trained models can be effectively transferred to human data, present supporting performance evidence if available, or discuss the limitations in clinical translation.  \n2. **Experimental design and validation:** The paper lacks sufficient details about the experimental setup, particularly how data partitions were defined. The validation appears to be conducted only within the BoneMet dataset, with no external validation. This limits confidence in the generalizability of the results and leaves uncertain whether the dataset can support more robust or foundation-level AI models.\n\n---\n\n**Minor Comments**  \n- The manuscript would benefit from clearer presentation of the data split strategy and corresponding rationale.  \n- Certain sections could be reorganized to improve readability and coherence in describing dataset structure and usage.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, BoneMet contributes a large, comprehensive dataset for BTBM research, addressing a notable data gap. Its **innovation** lies in the dataset’s scale, multimodal integration, and accompanying tools. However, the **evaluation** is incomplete due to the lack of external benchmarking, and **reproducibility** would improve with more transparent details on data partitioning and experimental protocols. Overall, the work is promising but needs stronger validation and clarification before publication.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors integrated a publicly available large-scale Bone Metastasis (BoneMet) dataset with multimodal medical data, including 2D X-ray images, 3D CT scans, and comprehensive biological data. The primary objective of this database is to facilitate the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well-organized and clearly written.\n\nOur collected dataset is open, large-scale, and consists of multi-modal resources including 2D X-ray images, 3D CT scans, and medical records and quantitative analysis.\n\nThe author provides a tool on the Python Package Index (PyPI) for assisting researchers and practitioners.\n\nThe authors have conducted multiple benchmarks, such as BTBM Diagnosis to exemplify the utilisation of the collated dataset.\n\n### Weaknesses\n\nIn the abstract, the author states, “Breast tumor bone metastasis (BTBM) affects women’s health globally, necessitating the development of effective solutions for its diagnosis and prognosis.” However, the dataset collected was derived from mice rather than human patients.\n\nThe author should perform a comprehensive comparison between the BoneMet dataset and previously available datasets, including those derived from human subjects, to elucidate the differences and scales involved.\n\nAdditionally, the methods employed in each benchmark were limited. The author should justify the selection of the included methods over others.\n\n### Questions\n\nThis paper comprises two main components: the dataset and the benchmark. Both sections could benefit from additional details and comparisons.\n\nThe authors should evaluate and clarify how the collected data can directly benefit human health. For example, they could discuss the potential for training models on their dataset and subsequently transfer the obtained model to human datasets.\n\nFurthermore, the figures in the manuscript require better organization, as the text is currently too small for effective readability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a study that integrates the Bone Metastasis (BoneMet) dataset with multimodal medical data, including 2D X-ray images, 3D CT scans, and comprehensive biological information. The primary aim is to enhance the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors. Overall, the manuscript is well-organized and effectively communicates its objectives, yet some aspects require further clarification and justification.\n\n**Major Comments:**\n1. The authors claim that \"Breast tumor bone metastasis (BTBM) affects women’s health globally,\" but the dataset is derived from mice, not human patients. This discrepancy should be addressed.\n2. A comprehensive comparison of the BoneMet dataset with existing datasets derived from human subjects is necessary to highlight differences and implications for clinical relevance.\n3. The methods used in the benchmarks are limited. Justification for the selection of these specific methods over alternatives is needed.\n\n**Minor Comments:**\n- The dataset and its utility for human health could be better articulated. The authors should discuss the potential for training predictive models on this dataset and the ability to transfer results to human datasets.\n- Figures in the manuscript need improved organization, as the current text size inhibits readability.\n\n**Summary Paragraph:**  \nIn terms of the TMI editorial criteria, the study demonstrates significance through its focus on bone metastasis, though the innovation is compromised by the use of a murine dataset. Evaluation is addressed through benchmark comparisons, but the limited methods raise concerns about robustness. Reproducibility is supported by the open-source dataset and tools provided on the Python Package Index (PyPI), yet requires clearer connections to human health impacts. Overall, while the study has strengths, several key areas necessitate further attention.\n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the development of a large-scale, multimodal Bone Metastasis (BoneMet) dataset that integrates 2D X-ray images, 3D CT scans, and biological/medical data. The stated aim is to advance the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors. The paper is clearly written and well-organized, with an emphasis on facilitating further research through public data availability and benchmarking tools.\n\n**Major Comments**  \n1. **Data Source and Relevance:** Although the abstract emphasizes the global significance of breast tumor bone metastasis in humans, the dataset is derived from mice rather than human patients. The authors should explicitly acknowledge this limitation and clarify the translational relevance of the dataset for human studies.  \n2. **Comparative Analysis:** A comprehensive comparison with existing datasets—particularly those based on human data—is needed to contextualize the BoneMet dataset in terms of scale, modality, and novelty.  \n3. **Benchmark Methodology:** The benchmarks provided (e.g., BTBM Diagnosis) are limited in methodological diversity. The rationale for selecting the specific baseline methods should be elaborated, and additional, more representative methods may strengthen the demonstration.  \n4. **Dataset Utility and Impact:** Further discussion is warranted on how models trained using the BoneMet dataset could transfer to or benefit human clinical research. This connection is important for demonstrating practical significance beyond preclinical contexts.\n\n**Minor Comments**  \n- Figures require improved organization and larger text to enhance legibility.  \n- Documentation or description of the PyPI package could be expanded to improve usability.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in providing an open, multimodal dataset for bone metastasis research and includes associated benchmarking and a Python package to aid reproducibility. Its innovation lies primarily in dataset integration rather than methodological advancement. However, the limited explanation of translational relevance, lack of comprehensive comparison with existing datasets, and narrow benchmarking reduce its perceived novelty and general impact. Reproducibility is reasonably supported through public release, but evaluation and contextualization need strengthening.\n\n**Decision Recommendation**  \n**Major Revision.** The study presents a valuable resource, but clearer justification of data relevance, expanded comparisons, and improved figure presentation are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces BoneMet, a large-scale multi-modal dataset for breast cancer bone metastasis (BCBM) research derived from murine models. The dataset comprises 67.87 TB of data collected from over 500 mice spanning 2019-2024, including six components: Rotation-X-Ray (651,300+ X-ray images), Recon-CT (3D reconstructed scans), Seg-CT (segmented tibiae), Regist-CT (registered scans), RoI-CT (region-of-interest cropped images), and MiceMediRec (medical records and quantitative analysis). The authors demonstrate the dataset's utility through experiments on BCBM diagnosis using Vision Transformers and CNNs, prognostic assessment of bone mechanical properties using generative models, and sparse-angle CT reconstruction using NeRF-based methods. They also provide three APIs for CT segmentation, registration, and RoI cropping. Results show accuracies ranging from 79.1% to 92.0% for diagnosis tasks, with the dataset supporting both 2D and 3D imaging modalities for various deep learning applications.\n\n## Weaknesses\n\n• **Limited validation of ground truth accuracy and annotation quality**\n  - No inter-rater reliability analysis or validation of the tumor/non-tumor labels is provided (Section 2.1, Table 1), which is critical for supervised learning tasks\n  - The global threshold value of 75/255 for segmentation appears arbitrary without justification or validation against manual expert segmentation (Page 4, Section 2.1)\n  - Missing details on how \"positive\" labels are determined at the animal level and potential temporal misalignment between actual lesion onset and imaging timepoints (Page 6, Section 3)\n\n• **Insufficient mathematical rigor in methodology description**\n  - The mutual information maximization process for registration lacks mathematical formulation and optimization details (Page 15, Appendix A), making reproducibility challenging\n  - No equations provided for the filtered backprojection reconstruction process despite mentioning FBP implementation (Page 4, Section 2.1)\n  - The composite image generation with different pixel value assignments (180, 240, 60, 0) lacks mathematical justification for these specific values (Page 5, Section 2.1)\n\n• **Limited experimental evaluation depth and statistical rigor**\n  - Cross-validation methodology is mentioned but detailed statistical analysis of variance across folds is missing (Table 2, Page 7), limiting confidence in reported performance metrics\n  - Comparison baselines are limited and lack recent state-of-the-art methods in medical imaging for each task (Tables 3-5, Pages 7-9)\n  - Sample size justification and power analysis are absent, particularly given the 8:2 train/test split across only 500+ mice (Page 7, Section 3.1)\n\n• **Questionable generalizability and translational potential**\n  - The significant physiological differences between murine and human bone metastasis are acknowledged but insufficiently addressed in terms of model transferability (Page 20, Appendix G)\n  - No validation experiments using human data or comparison with existing human datasets to demonstrate cross-species applicability (throughout manuscript)\n  - The homogeneous inbred mouse population limits diversity compared to human patient variability (Page 20, Appendix G)\n\n## Suggestions for Improvement\n\n• **Enhance ground truth validation and annotation quality assessment**\n  - Conduct inter-observer agreement studies with multiple expert annotators for tumor classification and provide Cohen's kappa or similar reliability metrics\n  - Validate the 75/255 threshold against manual expert segmentations using Dice coefficients and sensitivity/specificity analysis\n  - Provide detailed temporal analysis of lesion onset timing and establish clear criteria for positive/negative labeling with uncertainty quantification\n\n• **Improve mathematical formulation and methodological clarity**\n  - Include complete mathematical formulations for mutual information optimization, registration transformations, and convergence criteria\n  - Provide detailed equations for the FBP reconstruction process including filter specifications and parameter settings\n  - Justify the composite image pixel value assignments through quantitative analysis or provide sensitivity analysis for different value choices\n\n• **Strengthen experimental evaluation and statistical analysis**\n  - Report detailed cross-validation statistics including mean, standard deviation, and confidence intervals for all performance metrics across folds\n  - Include comprehensive comparisons with recent state-of-the-art methods in medical image analysis for each application domain\n  - Provide sample size calculations, power analysis, and statistical significance testing for reported performance differences\n\n• **Address generalizability concerns and demonstrate translational relevance**\n  - Conduct preliminary validation studies using available human BCBM datasets or collaborate with clinical institutions for proof-of-concept validation\n  - Perform systematic analysis of domain adaptation techniques and their effectiveness for mouse-to-human model transfer\n  - Include diverse mouse strains or investigate methods to artificially increase biological variability to better represent human population diversity",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThis manuscript introduces BoneMet, a comprehensive multi-modal dataset designed for breast cancer bone metastasis (BCBM) research, sourced from murine models. It comprises 67.87 TB of data collected from over 500 mice from 2019 to 2024, encompassing six components: Rotation-X-Ray images, 3D reconstructed scans, segmented tibiae, registered scans, region-of-interest cropped images, and medical records. The authors illustrate the dataset's application through experiments involving BCBM diagnosis with Vision Transformers and CNNs, prognostic evaluation of bone properties using generative models, and sparse-angle CT reconstruction facilitated by NeRF-based approaches. Furthermore, three APIs are provided for CT segmentation, registration, and cropping, yielding accuracies between 79.1% and 92.0% for diagnosis tasks.\n\n## Major Comments\n1. **Validation of Ground Truth and Annotation Quality**\n   - Absence of inter-rater reliability analysis or validation of tumor/non-tumor labels is concerning and critical for supervised learning tasks (Section 2.1, Table 1).\n   - The global threshold of 75/255 for segmentation is arbitrary and lacks justification or validation against manual expert segmentations (Page 4, Section 2.1).\n   - There are missing details regarding the determination of \"positive\" labels at the animal level and potential temporal mismatches between lesion onset and imaging time points (Page 6, Section 3).\n\n2. **Mathematical Rigor in Methodology**\n   - The mutual information maximization for registration lacks mathematical formulation and optimization details, challenging reproducibility (Page 15, Appendix A).\n   - No equations are provided for the filtered back-projection reconstruction process, despite its mention (Page 4, Section 2.1).\n   - The assignment of pixel values in composite image generation lacks mathematical justification (Page 5, Section 2.1).\n\n3. **Experimental Evaluation and Statistical Rigor**\n   - Detailed statistical analysis of cross-validation variance is missing, impacting confidence in performance metrics (Table 2, Page 7).\n   - Comparisons with state-of-the-art methods in medical imaging are limited for each task (Tables 3-5, Pages 7-9).\n   - Absent sample size justification and power analysis, particularly with the 8:2 train/test split among 500+ mice (Page 7, Section 3.1).\n\n4. **Generalizability and Translational Potential**\n   - The physiological differences between murine and human bone metastasis are noted but insufficiently addressed regarding model transferability (Page 20, Appendix G).\n   - The study lacks validation using human data or comparisons with existing datasets for cross-species applicability.\n   - The homogeneous inbred mouse population limits variability compared to human patients (Page 20, Appendix G).\n\n## Minor Comments\n- Enhance clarity by providing additional contextual information regarding dataset components.\n- Correct typographical errors in the manuscript for improved readability.\n- Clearly define acronyms upon first use to avoid confusion.\n\n## Summary Paragraph\nThe BoneMet dataset presents significant potential for advancing BCBM research, yet it faces critical limitations regarding validation, methodological rigor, and generalizability. While innovative and rich in data, it lacks comprehensive validation of ground truth accuracy and thorough statistical evaluations, which compromises its reproducibility. The translational relevance is also questioned due to the physiological divergences between murine and human models. These components need to be strengthened to fulfill the TMI editorial criteria of significance, innovation, evaluation, and reproducibility.\n\n## Decision Recommendation\nMajor revision is recommended to address the outlined weaknesses and enhance the manuscript's contribution to the field.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **BoneMet**, a large-scale multimodal dataset for breast cancer bone metastasis research based on murine models. The dataset, totaling 67.87 TB from over 500 mice (2019–2024), includes multiple imaging and metadata components encompassing X-ray, reconstructed and segmented CT scans, and medical records. The authors demonstrate dataset utility across three use cases—diagnosis, prognosis, and reconstruction—using a variety of deep learning architectures and provide associated APIs for segmentation, registration, and ROI cropping. Overall, the work is clearly written and the dataset potentially valuable to the research community, but several methodological, statistical, and translational aspects require improvement before publication.  \n\n---\n\n**Major Comments**  \n1. **Ground Truth and Annotation Validation**  \n   - No inter-rater reliability or validation of tumor/non-tumor labels is provided, limiting confidence in annotation quality.  \n   - The global segmentation threshold (75/255) lacks justification or validation against manual expert annotations.  \n   - Criteria for defining positive cases at the animal level and potential timing mismatches between lesion onset and imaging require clarification.  \n\n2. **Methodological Rigor and Mathematical Formulation**  \n   - The mutual information–based registration is insufficiently described mathematically, hampering reproducibility.  \n   - Equations for filtered backprojection reconstruction are missing despite referencing its implementation.  \n   - Pixel value assignments in composite images are not theoretically or empirically justified.  \n\n3. **Experimental Evaluation and Statistical Analysis**  \n   - Cross-validation lacks variance or confidence interval reporting, precluding assessment of statistical robustness.  \n   - Comparisons omit several recent state-of-the-art baselines.  \n   - Sample size justification and power analyses are absent given the modest cohort size.  \n\n4. **Generalizability and Translational Relevance**  \n   - The manuscript acknowledges murine–human physiological differences but does not adequately quantify their impact on model transferability.  \n   - No validation using human data or public datasets is provided.  \n   - The use of a single inbred mouse strain limits biological diversity and translational scope.  \n\n---\n\n**Minor Comments**  \n- Improve clarity and completeness of mathematical expressions in Appendix A and Section 2.1.  \n- Include definitions for all acronyms at first use.  \n- Ensure tables and figures clearly indicate units and statistical measures.  \n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant in proposing a comprehensive BCBM imaging dataset, which could serve as an important community resource. The innovation lies in the dataset’s scale and multimodality, as well as the integrated API tools. However, the evaluation and methodological sections lack the necessary mathematical rigor, statistical analysis, and cross-species validation to ensure reliability and reproducibility. Without robust annotation verification and translational validation, the impact and generalizability remain limited. Strengthening these aspects would substantially enhance the manuscript’s contribution.  \n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n**1. Scope & Relevance** –  \nThe manuscript describes the creation of **BoneMet**, an open, large‑scale, multi‑modal murine dataset for research on breast‑cancer bone metastasis.  It contributes a reproducible imaging and metadata resource (X‑ray, CT, biomechanical, and clinical measurements) plus open APIs for segmentation, registration, and ROI cropping.  The main contribution is **data/resource and tooling**, not a new image‑reconstruction or ML methodology.  Demonstration experiments use standard deep‑learning models (ViT, Swin‑B, EfficientNet, GANs, NeRF).  \n→ ⚠ While the dataset is valuable, the work is primarily *dataset release and benchmarking*, which lies nearer to **application or data‑paper scope** than TMI’s usual expectation of novel image‑processing methodology.\n\n**2. Novelty & Contribution** –  \nNovelty stems from (i) dataset scale and multimodal longitudinal structure; (ii) integration of curated APIs for reproducible processing; and (iii) open access.  There is little new mathematical or algorithmic formulation.  Demonstrations validate feasibility rather than introducing methodological insight.  Hence the contribution is *infrastructure* innovation, not methodological innovation per se.\n\n**3. Technical and Experimental Rigor** –  \nData acquisition and processing protocols are well documented and include scanner parameters, segmentation thresholds, registration steps, and metadata fields.  Experiments use appropriate train/test splits and eliminate mouse‑level leakage.  Reported quantitative results (accuracy, SSIM, etc.) are consistent.  However, comparisons remain internal; no baselines against other datasets or imaging‑physics analyses are provided.  Statistical uncertainty or reproducibility across runs is not discussed.\n\n**4. Clarity and Presentation** –  \nWriting is clear and grammatically sound, with well‑structured tables and illustrative figures.  Minor typographical inconsistencies (spacing, capitalization, table formatting) appear but are minor.  The manuscript is long due to exhaustive dataset description; condensation would help readability for TMI.\n\n**5. Ethical & Reproducibility Compliance** –  \nIACUC approval, anesthetic protocol, and radiation‑dose justification are explicitly stated.  Data and code accessibility (Hugging Face, PyPI, GitHub) meet open‑science standards.  Ethical compliance appears adequate.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n### 1. Summary\nThe authors present **BoneMet**, a 67‑TB open murine dataset containing longitudinal high‑resolution X‑ray and CT scans with corresponding biological/mechanical metadata for breast‑cancer bone‑metastasis research.  Six sub‑datasets (Rotation‑X‑Ray, Recon‑CT, Seg‑CT, Regist‑CT, RoI‑CT, and MiceMediRec) are accompanied by Python APIs enabling standardized preprocessing.  Benchmark experiments using ViT/CNN models demonstrate diagnosis, prognosis, sparse‑angle CT reconstruction, and generative prediction tasks.\n\n### 2. Strengths\n- First longitudinal, multi‑modal small‑animal resource for bone‑metastasis imaging.  \n- Excellent documentation of acquisition and processing pipelines.  \n- Open release with tooling enhances reproducibility and community value.  \n- Demonstrations illustrate dataset versatility across reconstruction and learning tasks.  \n- Ethical handling and clear dissemination strategy.\n\n### 3. Weaknesses\n- Limited *methodological* novelty; primary advance is data curation.  \n- Demonstration experiments reuse standard architectures without new algorithms or theoretical insight.  \n- Statistical analysis, uncertainty quantification, and cross‑dataset benchmarking are minimal.  \n- Paper length and structure closer to a dataset description than a TMI‑style methodological study.  \n- All data are murine; clinical translation to human imaging is speculative at present.\n\n### 4. Major Comments\n1. **Clarify positioning versus TMI scope** – Emphasize methodological aspects (e.g., quantitative registration accuracy, segmentation reproducibility, radiation‑dose modeling).  Otherwise, the paper may be better suited to a data‑resources venue.  \n2. **Quantitative validation** – Include reproducibility metrics (inter‑scan variability, segmentation overlap scores, registration error) to characterize dataset quality beyond AI performance numbers.  \n3. **Domain translation** – Provide rationale or pilot evidence that murine imaging geometry and contrast are representative for human bone‑metastasis modalities; discuss limitations quantitatively.  \n4. **Benchmark transparency** – Release exact training scripts and hyperparameter configs for reproducibility; report variance over multiple runs.  \n5. **Condense and refocus** – Shorten dataset logistics and highlight methodological potential—e.g., innovate in sparse‑angle reconstruction or multimodal alignment.\n\n### 5. Minor Comments\n- Standardize capitalization (“Our BoneMet Dataset,” “Rotation‑X‑Ray”).  \n- Check for repeated phrases (e.g., dataset component lists).  \n- Improve figure resolutions and annotate axes/scale bars.  \n- Ensure table units (e.g., µm, Gy) are properly formatted.  \n- Reference recent dataset‑oriented papers for comparison (e.g., TCIA resources).\n\n### 6. Overall Recommendation\n**Reject (Out of Scope / Low Novelty)** – The manuscript offers a well‑constructed open dataset but lacks methodological innovation expected for *IEEE Transactions on Medical Imaging*.  It would fit more naturally in a repository or dataset‑focused journal (e.g., *Scientific Data* or *Data in Brief*).  If targeting TMI, a revised version should deepen the imaging‑methodological content (e.g., new registration/reconstruction technique or statistical imaging analysis built upon the dataset).\n\n### 7. Confidence Level\n**5 / 5** – High confidence in the assessment based on domain familiarity and clear manuscript evidence.\n\n---\n\n**Summary Remark:**  \n*BoneMet* is a valuable community resource likely to accelerate AI research in metastatic‑bone‑disease imaging, but as currently written it is primarily a dataset‑descriptor paper without sufficient methodological advancement to meet TMI’s publication criteria.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **BoneMet**, a large‑scale, multi‑modal murine imaging dataset designed for breast‑cancer bone‑metastasis research. It provides high‑resolution X‑ray and CT data, longitudinal biological and biomechanical measurements, and open Python APIs supporting preprocessing, segmentation, registration, and region‑of‑interest extraction. The work is clearly written and technically well documented. However, its primary contribution lies in dataset creation and infrastructure tooling rather than in new image‑processing or reconstruction methodology, placing it closer to a data‑resource paper than to the typical methodological focus expected by *IEEE Transactions on Medical Imaging (TMI)*.\n\n---\n\n**Major Comments**  \n1. **Scope and Positioning:** The manuscript’s emphasis on dataset release may fall outside TMI’s methodological scope. The authors should clarify and emphasize quantitative methodological elements—such as registration accuracy, segmentation reproducibility, or radiation‑dose modeling—to align better with the journal’s aims.  \n2. **Validation and Quality Metrics:** The paper would benefit from quantitative characterization of data quality, including inter‑scan variability, segmentation overlap, and registration errors, not just AI benchmark performance metrics.  \n3. **Generality and Domain Translation:** The dataset is entirely murine. The authors should discuss or test whether geometric and contrast properties are representative of human imaging, acknowledging quantitative limitations in translation.  \n4. **Benchmark Transparency and Reproducibility:** To ensure full reproducibility, exact training scripts, hyperparameters, and result variances across multiple runs should be released and reported.  \n5. **Manuscript Focus:** The text could be condensed by summarizing dataset logistics while highlighting potential methodological innovations—e.g., novel sparse‑angle reconstruction or multimodal registration use cases built on the dataset.\n\n---\n\n**Minor Comments**  \n- Standardize capitalization and naming conventions across dataset components.  \n- Remove repetitive listing of sub‑datasets.  \n- Improve figure resolution and include properly annotated axes and scale bars.  \n- Ensure table units (µm, Gy) are consistently formatted.  \n- Cite other recent dataset‑oriented publications for context and comparison.  \n\n---\n\n**Summary Paragraph (TMI Editorial Criteria)**  \n**Significance:** *BoneMet* is a valuable open resource likely to advance small‑animal and metastasis‑imaging research.  \n**Innovation:** Its originality lies in dataset scale, multimodal design, and open APIs; methodological novelty is limited.  \n**Evaluation:** Experiments demonstrate dataset utility but lack statistical uncertainty analysis, external comparisons, and rigorous cross‑validation.  \n**Reproducibility:** Ethical compliance, open access, and detailed documentation are strong; nevertheless, transparency of training scripts and reproducibility metrics should be strengthened.\n\n---\n\n**Decision Recommendation:** **Reject (Out of Scope / Low Methodological Novelty)** – The dataset is well executed and ethically sound but does not provide sufficient methodological innovation for TMI. A more suitable outlet would be a data‑resource journal such as *Scientific Data* or *Data in Brief*.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript presents **BoneMet**, an extensive open dataset consisting of 67 TB of longitudinal, high-resolution X-ray and CT scans, complete with biological and mechanical metadata pertinent to breast-cancer bone metastasis research. The contribution lies primarily in the provision of a comprehensive reproducible imaging resource and associated APIs for segmentation, registration, and region of interest cropping, rather than in novel image-reconstruction or machine learning methodologies. While the dataset holds potential value for the community, the manuscript aligns more closely with a dataset release framework than the innovative procedural studies typically published in *IEEE Transactions on Medical Imaging* (TMI).\n\n### Major Comments\n1. **Scope Clarification**: The manuscript should better articulate its methodological contributions, such as quantitative registration accuracy and segmentation reproducibility, to align with TMI's expectations. \n2. **Quantitative Validation**: It is recommended to include inter-scan variability metrics, segmentation overlap scores, and registration error analyses to enhance the dataset’s characterization and quality beyond basic AI performance.\n3. **Domain Translation**: Providing evidence that murine imaging geometry and contrast are reflective of human bone metastasis modalities is essential; the limitations should be discussed quantitatively.\n4. **Benchmark Transparency**: Release of training scripts and hyperparameter configurations is necessary for reproducibility, including a discussion of variance across multiple runs.\n5. **Condensation and Focus**: The manuscript’s length could be reduced by streamlining the dataset logistics while emphasizing methodological potential, such as innovations in sparse-angle reconstruction.\n\n### Minor Comments\n- Standardize capitalization within the manuscript (e.g., “Our BoneMet Dataset,” “Rotation‑X‑Ray”).\n- Eliminate repeated phrases, especially in lists of dataset components.\n- Improve figure resolution and adequately annotate axes and scale bars.\n- Ensure proper formatting of table units (e.g., µm, Gy).\n- Reference recent dataset-oriented publications for comparison (e.g., TCIA resources).\n\n### Summary Paragraph\nIn evaluating the manuscript against the TMI editorial criteria, it is evident that while **BoneMet** represents a significant resource for high-resolution imaging in metastatic disease research, its contributions are largely infrastructural rather than methodological. The significance of the data is supported by excellent documentation and ethical considerations, but the innovation is limited. Evaluation metrics are minimal, and reproducibility aspects require further elaboration. Overall, the manuscript shows promise for community use, yet lacks sufficient methodological rigor to meet TMI's standards.\n\n### Decision Recommendation\n**Reject (Out of Scope / Low Novelty)** – Although the manuscript provides a well-constructed dataset, it does not contain the methodological innovations expected for publication in *IEEE Transactions on Medical Imaging*. A revised submission should aim to deepen its methodological insights, potentially by proposing new imaging techniques or analyses made possible by the dataset.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Fudong Lin",
      "Jason Jiang",
      "Liyun Wang",
      "Shubo Wang",
      "Tiankuo Chu",
      "Wiley Jia-Wei Gong",
      "Xu Yuan"
    ],
    "url": "pdfs/iclr.cc-2025-conference_c17c3d4a8e2210abad05684756a47d326d0f63e8.pdf",
    "remote_url": "https://openreview.net/pdf/c17c3d4a8e2210abad05684756a47d326d0f63e8.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "unsupervised anomaly detection",
      "medical images",
      "contrastive reverse distillation",
      "student-teacher"
    ],
    "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors propose a new method for anomaly detection in medical data setting using contrastive reverse distillation. The method uses knowledge distillation as it's foundation, implementing pre-trained teachers models along with a student model trained to either minimize or maximize its distance from the teachers in a feature-map space. They additionally utilize a scale adaptive mechanism to allow for better generalizations across different sizes of anomaly.\n\nTheir method's specific construction utilizes two teacher models with the same weights, one \"bad\" and one \"good\". The good model is given benign images while the bad model is given images with synthetic anomalies. The student model is then trained to maximize its cosine similarity with the per-layer features of the good teacher while minimizing the cosine similarity with the bad teacher. At evaluation time anomaly scores are computed by comparing the scaled, per-layer feature spaces of the good teacher against those of the student. Inputs with high levels of dissimilarity between the good model and student are given high anomaly scores.\n\nTheir evaluation against prior work is very comprehensive, comparing against a total of 18 other detectors. They out perform all other detectors on all three datasets (lung, brain, and skin anomalies) in terms of their main metrics (AUC, F1, and Accuracy). The authors further perform ablations with respect to their main contributions of Contrastive Reverse Distilation and Scale Adaptive Mechanisms.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n* Anomaly detection is somewhat outside my area of expertise, but the paper seems to frame itself well within the current body of literature. \n* The paper is well written and easy to follow. Figures are also informative and easy to understand.\n* The idea of contrastive reverse distillation seems novel (to the best of my knowledge) and intuitive. I'd like to see how future works can iterate upon this framework in other settings or with improved constructions.\n* Motivation is well established for the usage synthetic anomalies, and the choice of simplex noise is clever.\n* Empirical analysis is very comprehensive in terms of datasets, baselines, and ablations.\n* The proposed method out-performs all baselines in terms of AUC, F1, and Accuracy.\n* The proposed method scales well with model architecture size, which is a good sign that the method is sensible.\n\n### Weaknesses\n\nIn general, within my somewhat limited knowledge of this sub-field, I like this paper. In spite of that I think there are a few things the authors may want to consider to further strengthen their work.\n\nI think when discussing detection and AUC scores it is important to give them context within the given domain. See section B of [1] for some example discussion on this topic from a different field. I'd like the authors to consider the following questions:\n\n* Is it more important to maximize the true positive rate in spite of inducing some false positives? \n* Is it better to minimize false positives to increase confidence in the case of a positive prediction?\n* How do you anticipate doctors using these tools in their analysis?\n\nGiven this, it would also be nice if the authors could plot some of the ROCs between their method and some of the baselines (maybe including all of them is too visually messy, but at least the top 5). This way people can compare their method to the baselines at different levels of false positive rate.\n\nI think it would be good if the authors could include some extended discussion, either in the main body or the supplementary material, analyzing the more subtle features of their results. See the next section of my review \"Questions\" for some examples of what I think they can further analyze.\n\nLastly, the results of table 1 are good, but a bit overwhelming. I think it would help if the second best score in each metric is underlined and if the methods are visually grouped based on detector type (e.g. RD4AD, RD++, and ReContrast could be grouped together). Optionally it would be nice if all experiments were averaged over a few runs and given standard deviations or $\\pm$ scores, at least in the supplementary material. I understand if these experiments are too extensive to replicate too many times, though.\n\n [1] \"Membership Inference Attacks From First Principles\" https://arxiv.org/abs/2112.03570\n\n### Questions\n\nHere I include some questions for the authors to consider.\n\n* Why does ISIC seem like the hardest dataset for all the detectors? Why is Brain Tumor the easiest?\n* Why is SAM more impactful in the Brain Tumor dataset while it seems minimally impactful in the RSNA dataset in table 2? Is this related at all to the results of table 3 - where Gaussian noise performs similar to simplex noise on RSNA, but much worse on Brain Tumor?\n* Can the method achieve better generalization by using a mixture of synthetic noise types, perhaps 10% gaussian, 90% simplex? \n* Can we improve performance by including some real world data in the training set, or will that harm generalization?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel approach for anomaly detection in medical data through a technique termed contrastive reverse distillation. By employing knowledge distillation, the authors utilize pre-trained teacher models alongside a student model, optimizing its feature-map distances to improve anomaly detection capabilities. The study is well-articulated, demonstrating clarity in methodology and empirical results across various datasets.\n\n### Major Comments\n1. The novelty of the approach appears robust, particularly the concept of contrastive reverse distillation. However, it would be beneficial for the authors to provide context for their detection and AUC scores within the relevant domain.\n2. Consideration of the clinical implications is necessary. Questions to address include:\n   - Is maximizing the true positive rate more critical than managing false positives?\n   - How might clinicians utilize these tools in practice?\n3. The inclusion of ROC curves contrasting their method with the top five baseline detectors would facilitate comparative analysis.\n4. Extended discussions on the subtleties of the results should be incorporated in either the main text or supplementary materials.\n5. Table 1 results are somewhat overwhelming. Underlining the second-best scores per metric and grouping methods by detector type would enhance readability. Additionally, providing averages and standard deviations of experiments, if feasible, would improve the clarity of the findings.\n\n### Minor Comments\n- Figures are well-designed and informative, contributing positively to the overall readability of the manuscript.\n- There are some grammatical errors that need correction; a thorough proofreading is recommended.\n- The choice of simplex noise for generating synthetic anomalies is noted as a clever decision.\n\n### Summary Paragraph\nThis study is significant because it addresses a critical challenge in medical anomaly detection with an innovative method that shows strong empirical performance. While the evaluation against baselines is comprehensive and results indicate notable advancements in metrics like AUC, F1, and Accuracy, additional contextualization of results within the practical healthcare application would strengthen the manuscript. The reproducibility of the findings is commendable, yet further details on the methodology and potential variability in results would enhance the robustness of the claims.\n\n### Decision Recommendation\nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel framework for anomaly detection in medical imaging based on *contrastive reverse distillation*. The approach builds upon knowledge distillation, employing two teacher models—one “good” trained on benign images and one “bad” using synthetic anomalies—and a student network optimized to align with the good teacher’s features while diverging from the bad one. A scale-adaptive mechanism further enhances generalization across anomaly sizes. The paper is clearly written, with informative figures and strong contextual integration within existing literature. Overall presentation and methodological soundness are satisfactory.  \n\n**Major Comments**  \n1. **Contextualization of Detection Metrics** – The paper reports strong gains in AUC, F1, and accuracy over 18 competing methods on lung, brain, and skin datasets. However, it would benefit from contextual interpretation of these metrics within clinical practice. Discussion should address whether maximizing true positives at the cost of false positives is preferable, or whether minimizing false alarms is more critical for diagnostic reliability.  \n2. **ROC Curve Visualization** – Including ROC curves for key baselines (e.g., top five methods) would help readers interpret the model’s relative performance across operating points and false positive rates.  \n3. **Deeper Analysis of Results** – The results in Table 1, though impressive, are somewhat dense. Highlighting second-best results and grouping methods by detector type (e.g., RD4AD, RD++, ReContrast) would improve interpretability. Including standard deviations or repeated-run averages, even in supplementary materials, would strengthen empirical robustness.  \n4. **Further Discussion Opportunities** – The authors could explore dataset-specific behaviors (e.g., ISIC being hardest, Brain Tumor easiest) and investigate why the Scale Adaptive Mechanism’s impact varies across datasets. Additional experiments on combining synthetic noise types or introducing limited real data might shed light on generalization properties.  \n\n**Minor Comments**  \n- Ensure consistent formatting of tables and figures.  \n- Clarify the role of simplex vs. Gaussian noise in the ablation tables.  \n- Consider placing extended analysis or supplemental discussions in the appendix.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in advancing anomaly detection techniques through a creative contrastive distillation setup, offering promising results across multiple modalities. Its innovation lies in the use of “good” and “bad” teacher networks and the scale-adaptive mechanism. The evaluation is broad and convincing, yet interpretive context and reproducibility details (e.g., standard deviations, multiple runs) could be strengthened. Overall, the study is rigorous and well-presented, with opportunities for additional context and experimental transparency.  \n\n**Decision Recommendation:** **Minor Revision** – The paper is solid and well-executed, needing only expanded discussion and improved result presentation for full clarity.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nTwo issues arise in reverse knowledge distillation for medical imaging, an inability to properly distinguish between different features and an inability to deal with different scales. This work proposes two methods to address these issues: a contrastive student-teacher learning approach that involves using both a “good” and a “bad” teacher, as well as a scale adaptation mechanism. Results demonstrate that on almost every dataset the proposed approach is able to outperform existing techniques.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well written and the ideas are easy to follow. I appreciate how the authors identify the two problems facing medical imaging in this domain and then propose solutions. The new approach outperforms all other methods that the authors compare with in table 1.\n\n### Weaknesses\n\nIssue 1: This paper doesn’t have any formal proofs or theorems to guarantee its effectiveness, therefore the strengths of the paper must rely on the experiments and empirical results. For the datasets I will admit as a review I am not very familiar with them. For example, on the image dataset CIFAR-10, a few years ago a jump from 93% to 98% was considered significant. Today an increase from 99% to 99.9% on CIFAR-10 would not even be considered a major contribution. Likewise, for these datasets it is hard to determine whether a jump of 3.01% and 4.38% (as the authors show) is worthy of publication. Could the authors please clarify how much other papers pushed the accuracy forward in other works, as compared to the increase their method is offering?\n\nIssue 2: The authors say they will release their code upon publication, but I did not see any code given in the supplemental material. This might be an oversight on my part, but where is the code? I would like to see a good faith gesture of releasing an anonymous github for the code or at least a zip file of the code. Too many papers claim SOTA results and never release code. In this day and age it is simply unacceptable to continue this trend. \n\nIssue 3: The authors also don’t mention anything about the runtime of their algorithm. How does the student-teacher setup and training time compare to other existing methods in the field? While this is not a HUGE concern, I think it would be good to have some discussion and analysis of this. For example could you show the training time complexity of your algorithm and the next two best methods in the field? Or even an experimental runtime of the training in seconds of your algorithm and the next two best methods? Without this I feel we don’t have the complete story when comparing methods. \n\nIssue 4: Please see my minor comments about uncited claims in the papers. For acceptance I would like to see citations for all the claims you have made. \n\nIssue 5: I don’t like the terminology “good” and “bad” teacher. It is very non-technical and hard to follow. Can you please change the terminology in your paper and name the teachers more appropriately? \n\nMinor Comments:\n\n=Line 36, “anomalous samples is often prohibitively expensive and time-consuming…” I believe this is correct but I want to see a citation to backup this claim?\n\n=Line 45, “they tend to generalize too well, thereby risking the reconstruction of abnormal regions.” Citation for this claim? \n\n=Table 1 has many entries that are unsorted (aside from the bolded number indicating the best method). It would be better if this table was sorted in order of increasing AUC for the methods.\n\n=Line 167 “Medical images typically exhibit a power law distribution of frequencies, with lower frequency components dominating the image content.” Please give a citation for this bold claim?\n\n### Questions\n\nQ1: Please address each of my issues 1-5. If they are adequately addressed I would be willing to increase my score for the paper accordingly.\n\nEDIT: The reviewers have address my concerns adequately. I have update the score of my review accordingly.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript addresses challenges in reverse knowledge distillation for medical imaging, specifically the difficulties in distinguishing between different features and managing varying scales. The authors propose two methods: a contrastive student-teacher learning framework utilizing “good” and “bad” teachers, as well as a scale adaptation mechanism. The results suggest that this approach outperforms existing techniques across almost all tested datasets, demonstrating promise and potential contributions to the field.\n\n### Major Comments\n1. **Lack of Formal Proofs**: The manuscript lacks formal proofs or theorems supporting the effectiveness of the proposed methods, relying heavily on empirical results. Clarity on the significance of improvements in accuracy, particularly regarding benchmarks like CIFAR-10, is necessary. Comparisons with advancements made in other works should be elucidated to assess the novelty of the presented results.\n  \n2. **Code Availability**: Although the authors mention releasing code upon publication, it is not provided in the supplemental material. The submission would benefit from a commitment to share code, ideally via a public repository like GitHub or through a zip file, to promote transparency and reproducibility in research.\n\n3. **Runtime Analysis**: There is no discussion regarding the runtime efficiency of the proposed algorithm. Comparative analysis of training time against existing methods would enhance the study’s rigor. Recommendations include showcasing training time complexity or providing experimental runtime data.\n\n4. **Citations for Claims**: Several claims in the manuscript are presented without citations. Adequate references should support all assertions to meet rigorous academic standards.\n\n5. **Terminology Issues**: The terms “good” and “bad” teacher are non-technical and may lead to confusion. A more precise nomenclature should be adopted for clarity.\n\n### Minor Comments\n- Line 36: Please provide a citation for the claim regarding the expense and time consumption of handling anomalous samples.\n- Line 45: A citation is needed for the statement about generalization risks affecting reconstruction of abnormal regions.\n- Table 1: Many entries should be sorted in ascending order of AUC for clarity.\n- Line 167: A citation is required for the assertion about the power law distribution of frequencies in medical images.\n\n### Summary Paragraph\nThe paper presents an innovative approach to reverse knowledge distillation, contributing to the field of medical imaging. While the study demonstrates significant potential and shows improvement over existing methods, weaknesses include a lack of formal proofs and runtime analysis, absent documentation of code availability, and insufficient citations for several claims. These issues affect the reproducibility of the work, which is crucial given the technological context of the research.\n\n### Decision Recommendation\nMajor revision is recommended, contingent on addressing the outlined comments and concerns adequately.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses two key challenges in reverse knowledge distillation for medical imaging: distinguishing between heterogeneous features and handling variations in image scale. To overcome these issues, the authors propose a contrastive student–teacher framework employing both a “good” and a “bad” teacher, along with a scale adaptation mechanism. Experimental results indicate consistent improvements over existing approaches across nearly all tested datasets. The paper is clearly written and easy to follow, and the proposed methodology appears thoughtfully motivated.\n\n---\n\n**Major Comments**  \n1. **Empirical significance of results:** The paper lacks theoretical justification or formal proofs of effectiveness, relying entirely on empirical evidence. The reported performance gains (3.01%–4.38%) may be modest depending on dataset difficulty. The authors should contextualize these improvements by comparing with prior work—how does this magnitude of gain compare to historical progress on the same datasets?  \n2. **Code availability and reproducibility:** The manuscript states that code will be made public upon publication, but no implementation is currently available in the supplementary materials. A temporary anonymous code release or archive (e.g., GitHub or zip file) would demonstrate good faith and strengthen reproducibility.  \n3. **Computational and runtime analysis:** The paper does not discuss runtime or complexity. A comparison of training time or computational load between the proposed method and competing approaches is necessary to complete the evaluation.  \n4. **Citation completeness:** Several assertions lack citations. All major claims—particularly those describing dataset properties or general trends in medical imaging—should be appropriately referenced.  \n5. **Terminology choice:** The use of “good” and “bad” teacher is imprecise and informal. More technical terminology should be adopted for clarity.\n\n---\n\n**Minor Comments**  \n- Line 36: Add citation supporting the claim about the cost of collecting anomalous samples.  \n- Line 45: Provide a reference for the statement on over-generalization leading to reconstruction of abnormalities.  \n- Table 1: Consider sorting entries by increasing AUC to improve readability.  \n- Line 167: Add citation for the statement regarding the power-law frequency distribution in medical images.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work introduces an interesting extension to teacher–student frameworks through contrastive guidance and scale adaptation, showing promising empirical results. While the method’s practical value seems reasonable, its broader significance remains uncertain without contextualized evaluation. The innovation is conceptually sound but limited by informal terminology and lack of theoretical grounding. Evaluation is largely empirical yet needs further runtime and comparative analysis. Reproducibility is currently weak due to missing code but can be easily improved with open sourcing.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** — The paper is well written and addresses a relevant problem, but key issues related to empirical context, code release, and completeness of analysis must be resolved before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a twofold approach: on one hand, data augmentation for unlabeled information is achieved by injecting noise, which then serves as the basis for contrastive learning. On the other hand, parameters are extracted at each layer to achieve multi-scale information fusion, enabling the model to better distinguish between positive and negative samples.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe methodology section of this paper is clearly written and well-presented.\n\n### Weaknesses\n\n1. In terms of innovation, this article presents two components that are commonly seen in other works. A straightforward suggestion would be for the authors to explore a more adaptable data augmentation method to address the issue raised in “Question 1.”\n\n2. Regarding the writing, the **Introduction** section of this article is somewhat confusing. Firstly, the paragraphing is suboptimal. In the second paragraph of the introduction, the author discusses the shortcomings of anomaly detection models based on generation. The third paragraph explains existing work aimed at improving these models, followed by an exploration of the development of self-supervised learning. Although self-supervised learning indeed enriches unsupervised learning within deep learning, the third paragraph would be clearer if it unified these two parts, focusing on how self-supervised learning specifically addresses the issues raised in the second paragraph. Secondly, and more importantly, an excessive portion of the introduction is dedicated to discussing related work, lacking sufficient focus on the contributions of this paper. Maybe you could merging the discussion of existing work and self-supervised learning, followed by a clear transition to the paper's contributions. Additionally, you could recommend what key points about the paper's contributions should be emphasized earlier in the introduction.\n\n### Questions\n\n1. In the noise generation process, the intensity of each position is sampled from a uniform distribution. However, in real-world medical imaging, anomalous regions in disease cases appear in specific locations; for example, abnormalities in a chest image typically do not appear outside body regions. For instance, you could suggest incorporating prior knowledge about typical anomaly locations in different types of medical images.\n\n2. Additionally, is each test sample measured at the image level or at the patch level? If it is at the image level, the positive-to-negative anomaly ratio in the dataset is relatively high, which does not quite align with the extremely imbalanced nature of anomaly detection tasks.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a dual approach that incorporates data augmentation for unlabeled information through noise injection, serving as a foundation for contrastive learning. Additionally, it extracts parameters from each layer to enable multi-scale information fusion, allowing the model to improve its discrimination between positive and negative samples. The methodology is clearly articulated; however, several aspects of clarity and innovation require attention.\n\n### Major Comments\n1. **Innovation**: The components introduced in this paper are largely reminiscent of existing work. To enhance novelty, the authors could consider developing a more flexible data augmentation method in response to the concerns highlighted in “Question 1.”\n\n2. **Writing Clarity**: The **Introduction** is confounding in structure. The second paragraph discusses the limitations of anomaly detection models predicated on generation, while the third paragraph shifts to existing efforts aimed at enhancing these models and self-supervised learning. It would benefit from a more cohesive narrative that integrates these concepts. Specifically, the transition to self-supervised learning's contributions should bridge the deficiencies noted in the second paragraph. Moreover, the introduction devotes excessive attention to related work without sufficiently emphasizing the article's unique contributions. Merging the discussions of existing work with self-supervised learning and clearly stating the paper's contributions earlier would improve clarity.\n\n### Minor Comments\n- Consider revising the paragraph structure in the **Introduction** for better flow.\n- Clarify which aspects of the paper's contributions should be prioritized in initial discussions.\n\n### Summary Paragraph\nIn relation to TMI editorial criteria, while the manuscript demonstrates sound methodology and presents a clear framework for data augmentation and contrastive learning, it falls short in innovation and clarity. The discussion of related works overshadows the manuscript’s contributions, compromising its significance. The review of noise generation processes raises valid concerns about real-world applicability in medical imaging, calling for further consideration of anomaly locations. Clarity in writing must be improved to enhance overall evaluation and reproducibility.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a twofold method for medical image anomaly detection. First, it introduces a noise-based data augmentation strategy for unlabeled data that supports contrastive learning. Second, it performs multi-scale information fusion by extracting parameters across layers, allowing better discrimination of positive and negative samples. While the methodology is generally clear and well-presented, the paper’s innovation and writing structure could be strengthened to improve overall clarity and impact.\n\n**Major Comments**  \n1. **Novelty and Innovation:** The proposed dual approach combines two components—noise-based contrastive learning and multi-scale feature fusion—that are already common in the literature. The paper would benefit from exploring more adaptable or advanced data augmentation strategies to enhance originality and address limitations noted in “Question 1.”  \n2. **Clarity and Organization of the Introduction:** The Introduction lacks coherence and focus. Paragraph transitions do not clearly connect the identified challenges of generative anomaly detection with how self-supervised learning remedies these shortcomings. Large portions are devoted to related work, diluting emphasis on the paper’s own contributions. The authors should consider merging discussions on prior work and self-supervised learning, then explicitly highlighting their novel contributions earlier in the section.  \n3. **Evaluation Design:** Questions remain about the experimental setup. The noise intensity is sampled uniformly without accounting for the typical spatial priors of anomalies in medical images. Incorporating anatomical constraints or prior knowledge about expected anomaly locations could yield more realistic augmentation.  \n4. **Task Definition and Data Balance:** It is unclear whether evaluation is performed at the image or patch level. If conducted at the image level, the dataset’s positive-to-negative ratio may be too high, which could misrepresent the true imbalance characteristic of anomaly detection tasks.\n\n**Minor Comments**  \n- The methodology section is clearly presented and technically understandable.  \n- Paragraphing in the Introduction could be optimized for readability.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript addresses a relevant problem in unsupervised medical image anomaly detection. Its methods are conceptually sound but rely on relatively standard strategies, limiting their innovation. The experimental design would benefit from careful consideration of real-world data distributions and clearer articulation of contributions. Reproducibility appears feasible given the described methods, though more details on evaluation protocols would strengthen it.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes a Scale-Aware Contrastive Reverse Distillation (SCRD) model for unsupervised medical anomaly detection. The method addresses two limitations of existing reverse distillation approaches: insufficient feature discriminability and inability to handle anomaly scale variations. The core contributions include: (1) a contrastive student-teacher learning framework with dual teacher encoders (clean and noisy) that processes normal and synthesized anomalous data respectively, (2) a scale adaptation mechanism that learns input-specific weights for multi-scale contrastive distillation losses, and (3) use of simplex noise for generating synthetic anomalies. The method is evaluated on three medical imaging datasets (RSNA chest X-ray, Brain Tumor MRI, ISIC dermoscopy) and achieves state-of-the-art performance with AUC improvements of 3.01%, 4.38%, and 3.08% respectively over second-best methods (Table 1, Page 6).\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and clarity issues**\n  - The loss function in Equation 2 (Page 4) lacks proper normalization terms that were present in the baseline Equation 1, potentially affecting training stability and convergence properties\n  - The reshaping operation mentioned for uk, vk, and zk in Section 2.2.1 (Page 4) is described verbally but not mathematically formalized, creating ambiguity about the exact computational procedure\n  - The scale descriptor formulation in Equation 3 (Page 4) does not specify the dimensionality relationships between the input bottleneck features and output scale weights, making implementation unclear\n\n• **Limited experimental validation of key design choices**\n  - The simplex noise parameters (six octaves, persistence γ=0.6, scaling factor λ=0.2) are presented without systematic justification or comparison with alternative parameter settings beyond basic intensity variations in Figure 3 (Page 9)\n  - The contrastive loss design using ratio formulation in Equation 2 lacks comparison with alternative contrastive learning approaches such as margin-based or InfoNCE-style losses that are standard in contrastive learning literature\n  - The scale adaptation mechanism evaluation is limited to ablation studies (Table 2, Page 7) without analysis of learned scale weight distributions or visualization of how the mechanism adapts to different anomaly scales\n\n• **Insufficient analysis of computational and methodological limitations**\n  - The inference time comparison in Figure 4 (Page 14) shows only wall-clock time without reporting GPU memory usage, batch size effects, or scalability analysis for different image resolutions\n  - The synthetic anomaly generation strategy relies on spatially uniform sampling (Algorithm 1, Page 13) without considering anatomical priors or realistic pathology distributions specific to medical imaging modalities\n  - The comparison with RD++ in Section 2.5 (Page 5) is primarily conceptual without empirical analysis of the claimed fundamental differences in how anomalous information flows through the respective network architectures\n\n• **Dataset and evaluation methodology concerns**\n  - The Brain Tumor dataset combines samples from different sources (Br35H, Saleh et al., Cheng et al.) as mentioned in Section 3.1.1 (Page 5) without addressing potential domain shift or acquisition protocol differences that could artificially inflate performance\n  - The threshold selection strategy using \"best F1 score\" for accuracy and F1 metrics (Section 3.1.2, Page 5) introduces optimistic bias since the threshold is selected on test data rather than through proper validation procedures\n  - The qualitative analysis in Section 3.3.5 (Page 8) relies solely on score distribution visualization (Figure 2) without providing spatial anomaly localization results or failure case analysis\n\n## Suggestions for Improvement\n\n• **Enhance mathematical rigor and implementation clarity**\n  - Provide explicit normalization terms in Equation 2 consistent with the baseline formulation, and include theoretical analysis of how the ratio-based loss affects gradient dynamics compared to standard contrastive objectives\n  - Formalize the reshaping operation with precise mathematical notation specifying input/output dimensions, and provide algorithmic details for the feature alignment procedure between teacher and student networks\n  - Specify dimensional relationships in Equation 3 with explicit matrix dimensions, and include architectural details of the linear transformation from bottleneck features to scale descriptors\n\n• **Strengthen experimental validation through systematic analysis**\n  - Conduct comprehensive hyperparameter sensitivity analysis for simplex noise generation including systematic evaluation of octave counts, persistence values, and spatial sampling strategies with statistical significance testing\n  - Compare the proposed ratio-based contrastive loss against established contrastive learning objectives (InfoNCE, margin-based losses) with ablation studies demonstrating the specific advantages of the chosen formulation\n  - Provide detailed analysis of scale adaptation behavior including visualization of learned scale weight distributions across different datasets and correlation analysis between scale weights and actual anomaly scales\n\n• **Address computational efficiency and methodological rigor**\n  - Expand computational analysis beyond inference time to include GPU memory profiling, batch size scalability studies, and comparison of training computational requirements across competing methods\n  - Integrate anatomically-informed synthetic anomaly generation using medical imaging priors or learned spatial distributions rather than uniform spatial sampling to improve realism of synthetic anomalies\n  - Provide quantitative comparison with RD++ including architectural complexity analysis, feature flow diagrams, and empirical evaluation of information bottleneck properties in both approaches\n\n• **Improve experimental design and evaluation comprehensiveness**\n  - Address dataset heterogeneity by reporting performance on individual data sources within the Brain Tumor dataset and conducting domain adaptation analysis to ensure robust cross-domain generalization\n  - Implement proper validation-based threshold selection procedures using held-out validation sets or cross-validation to eliminate optimistic bias in F1 and accuracy metrics\n  - Include spatial anomaly localization evaluation using pixel-level or region-level metrics, and provide systematic failure case analysis with representative examples of misclassified cases across all datasets",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a Scale-Aware Contrastive Reverse Distillation (SCRD) model aimed at enhancing unsupervised medical anomaly detection. The proposed method addresses key limitations of existing reverse distillation techniques, specifically regarding feature discriminability and adaptability to variations in anomaly scale. The primary contributions include a dual teacher encoder framework for distinct processing of normal and synthetic anomalous data, an innovative scale adaptation mechanism for multi-scale contrastive distillation, and the use of simplex noise for anomaly generation. Evaluation on three medical imaging datasets demonstrates state-of-the-art performance with significant AUC improvements over leading methods.\n\n**Major Comments:**\n1. **Mathematical formulation inconsistencies and clarity issues:**\n   - Equation 2 lacks normalization terms found in baseline Equation 1, potentially impacting training stability.\n   - Reshaping operations in Section 2.2.1 are described verbally, lacking mathematical formalization, causing ambiguity.\n   - Equation 3 does not clarify the dimensionality relationships between bottleneck features and scale weights.\n\n2. **Limited experimental validation of key design choices:**\n   - Parameters for simplex noise are insufficiently justified beyond basic intensity variations exposed in Figure 3.\n   - The contrastive loss method lacks comparisons with established alternatives like margin-based or InfoNCE losses typical in the literature.\n   - Evaluation of the scale adaptation mechanism is confined to ablation studies, lacking further insights into learned scale weight distributions.\n\n3. **Insufficient analysis of computational and methodological limitations:**\n   - Inference time metrics exclude GPU memory use, batch effects, and scalability analysis for varying resolutions.\n   - The anomaly generation method employs uniform sampling without considering medical imaging-specific anatomical priors.\n   - The comparison with RD++ remains largely conceptual without empirical support detailing differences in network information flow.\n\n4. **Dataset and evaluation methodology concerns:**\n   - The Brain Tumor dataset aggregates samples from diverse sources without addressing domain shift, which may inflate results.\n   - The threshold selection based on \"best F1 score\" applies an optimistic bias as it uses test data for selection rather than proper validation.\n   - Qualitative analyses provide only score distribution visualizations without spatial localization of anomalies or failure case discussions.\n\n**Minor Comments:**\n- Clarify normalization and details in Equation 2.\n- Specify reshaping operations mathematically and provide corrections in algorithms.\n- Clarify the dimensional relationships in Equation 3 and include structural details in the model.\n\n**Summary Paragraph:**  \nThe manuscript presents a significant and innovative contribution to the field of medical anomaly detection through the introduction of the SCRD model. However, the evaluation aspects raise concerns regarding the thoroughness of experimental validation, analysis of computational efficiency, and methodological rigor. While the model shows promise and offers advancements in techniques used, it is critical to enhance the reproducibility and robustness of findings by addressing mathematical clarifications and validation procedures.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a Scale-Aware Contrastive Reverse Distillation (SCRD) model for unsupervised medical anomaly detection. It aims to enhance existing reverse distillation methods by improving feature discriminability and accommodating variations in anomaly scale. Key contributions include a dual teacher contrastive framework handling clean and noisy inputs, a scale adaptation mechanism learning input-specific multi-scale weights, and the use of simplex noise for synthetic anomaly generation. Evaluations across three medical imaging datasets (RSNA chest X-ray, Brain Tumor MRI, ISIC dermoscopy) show consistent AUC gains over prior methods. The paper is technically ambitious and addresses an important problem, but several methodological, analytical, and clarity issues limit its reproducibility and experimental robustness.\n\n---\n\n**Major Comments**  \n1. **Mathematical and Implementation Clarity** – Equation 2 lacks normalization terms present in its baseline formulation, raising concerns about training stability. The reshaping operation of latent variables (Section 2.2.1) is verbally described without explicit formalization, and the dimensional relationships in Equation 3 remain unclear.  \n2. **Validation of Design Choices** – The simplex noise parameters are introduced without systematic justification or parameter sensitivity studies. The new contrastive ratio-based loss is proposed without comparison to established objectives (e.g., InfoNCE or margin-based). The scale adaptation mechanism lacks deeper analysis, such as visualization or interpretation of learned weights.  \n3. **Computational and Methodological Limitations** – Inference efficiency analysis omits GPU memory and scalability results. The synthetic anomaly generation strategy employs uniform spatial sampling, which may not reflect medical anatomy. The comparison with RD++ is mainly conceptual and not empirically supported.  \n4. **Dataset and Evaluation Concerns** – The Brain Tumor dataset merges samples from heterogeneous sources without domain shift analysis. The use of test data to select thresholds introduces optimistic bias. Qualitative analyses are limited to score visualizations without spatial localization or failure case studies.\n\n---\n\n**Minor Comments**  \n- Clarify mathematical notation and variable dimensions throughout Section 2.  \n- Ensure consistent reporting of dataset splits and metrics.  \n- Provide detailed captions and higher-resolution figures for key experimental results.\n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper tackles a significant challenge in medical anomaly detection with a novel scale-aware contrastive distillation framework. While the conceptual design appears innovative, its theoretical formulation lacks clarity and transparency. The evaluation demonstrates promising performance but is undermined by limited parameter and threshold analyses. Reproducibility suffers from insufficient algorithmic detail and incomplete reporting of computational metrics. Addressing these issues would substantially strengthen the methodological rigor and reliability of results.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** –  \n✅ The manuscript proposes a novel methodology for unsupervised anomaly detection in medical imaging, introducing a *scale‑aware contrastive reverse distillation* framework. It advances the reverse‐distillation paradigm with both a contrastive student–teacher mechanism and scale adaptation for anomaly size variability. This fits squarely within TMI’s scope of methodological innovation in imaging analysis and machine learning for medical image understanding.\n\n2. **Novelty & Contribution Level** –  \nThe work builds upon existing reverse‑distillation strategies (RD4AD, RD++), but adds two new methodological elements:\n   - A *contrastive* formulation of student–teacher learning using synthetic (simplex‑noise) anomalies.\n   - A learnable *scale adaptation* module for multi‑scale weighting.  \nThese constitute meaningful extensions rather than incremental tuning. The formulation, though evolutionary, has sufficient conceptual novelty for the TMI audience if the technical justification and dataset breadth are clearly established.\n\n3. **Technical and Experimental Rigor** –  \nThe method and loss formulation are mathematically consistent and well defined. Experiments cover three imaging modalities (X‑ray, MRI, dermoscopy) and compare extensively with >15 baselines, showing consistent AUC improvements. Ablation studies and parameter analyses are included. Reproducibility seems good with public code release.  \nHowever, there are some potential gaps:\n   - The realism of synthetic “simplex” anomalies is heuristic; no sensitivity or anatomical plausibility analysis.  \n   - Statistical significance of performance gains is not reported.  \n   - External validation or clinical translation aspects are limited; datasets are public research sets.\n\n4. **Clarity and Presentation** –  \nThe manuscript is technically organized, equations are clear, and figures/ablation tables are useful. Some grammar and typography issues (spacing, minor capitalization errors) derive from preprint formatting but do not obscure meaning. Figure readability could be improved—particularly Figure 1, which is dense.\n\n5. **Ethical and Reproducibility Compliance** –  \nAll datasets used are public and de‑identified; IRB constraints are appropriately implicitly handled. Code availability is confirmed. The reproducibility description is adequate, though parameter listings could specify random‑seed control or runtime environment details.\n\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n**1. Summary**  \nThe paper introduces *Scale‑Aware Contrastive Reverse Distillation (SCRD)* for unsupervised anomaly detection in medical imaging. Within a teacher–student framework, it adds (i) a *contrastive* loss that simultaneously pulls features of normal inputs closer to a “clean teacher” while pushing apart features of synthetically perturbed “noisy teacher,” and (ii) a *scale adaptation mechanism* (SAM) that learns per‑scale weights to better handle anomalies of variable spatial size. The approach achieves state‑of‑the‑art AUC on X‑ray, MRI, and dermoscopy datasets compared to prior reconstruction, memory‑bank, and flow‑based methods.\n\n**2. Strengths**\n- Methodological innovation relative to prior reverse‑distillation algorithms.  \n- Well‑structured quantitative benchmarking and ablation analyses.  \n- Cross‑modality validation indicates good generalization.  \n- Computational efficiency and open‑source code enhance reproducibility.  \n\n**3. Weaknesses**\n- Limited discussion of the biological plausibility of synthetic anomalies (simplex noise).  \n- No uncertainty or statistical analysis (confidence intervals, significance tests).  \n- Evaluation confined to research datasets; absent clinical or prospective validation.  \n- Figure/algorithm clarity could be improved for readability.  \n\n**4. Major Comments**\n1. **Methodological Justification of Synthetic Anomalies:** The assumption that simplex‑noise perturbations approximate real pathology deserves quantitative validation (e.g., frequency‑domain similarity to real lesions or perceptual realism rating).  \n2. **Statistical Significance and Variability:** Report standard deviations or confidence intervals over multiple runs to determine whether observed AUC improvements are robust rather than due to stochasticity.  \n3. **Scale‑Adaptation Analysis:** Elaborate how the learned weights α correlate with true lesion scales; consider visualizing α distributions per dataset.  \n4. **Comparison Fairness:** Clarify whether all baselines are trained under identical preprocessing and input resolution—important for a fair TMI‑level comparison.  \n5. **Practical/Clinical Discussion:** Include brief commentary on runtime integration prospects in clinical workflows and how model explanations could guide radiologists.  \n\n**5. Minor Comments**\n- Correct minor spacing typos (e.g., “I NTRODUCTION”); unify notation \\(x′\\) formatting.  \n- Expand acronym definitions at first mention (SAM, CRD).  \n- Improve figure font size and provide color‑blind‑friendly palettes.  \n- Update the references to conform to IEEE style (author initials, capitalization).  \n- Indicate versions of the public datasets and Random Seed usage for reproducibility.  \n\n**6. Overall Recommendation: ▶ Major Revision**  \nThe paper presents a well‑designed and promising methodological contribution clearly aligned with TMI’s scope. However, stronger justification of the synthetic‑anomaly design, additional statistical rigor, and improved presentation are required before acceptance.\n\n**7. Confidence Level:** 4 / 5 — High confidence in technical and methodological assessment based on familiarity with medical image anomaly‑detection literature.\n\n---\n\n**Summary Recommendation:** *Major Revision* — methodologically sound with clear potential for TMI but needs more rigorous validation and clearer explanation to reach TMI’s publication standards.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Scale‑Aware Contrastive Reverse Distillation (SCRD)*, a novel approach to unsupervised anomaly detection in medical imaging. The method extends the reverse‑distillation paradigm by introducing (i) a contrastive student–teacher mechanism that separates normal from synthetically perturbed anomalies, and (ii) a scale‑adaptation module that adjusts attention across multiple spatial scales. Experiments across X‑ray, MRI, and dermoscopy datasets demonstrate improved AUC performance compared to over fifteen existing baselines. Overall, the paper is clearly written, technically organized, and aligned with TMI’s emphasis on methodological innovation in medical image analysis.  \n\n**Major Comments**  \n1. **Synthetic Anomaly Justification:** The use of simplex‑noise perturbations as proxies for real lesions is heuristic. The manuscript should include quantitative validation of their realism, such as frequency‑domain similarity or perceptual comparability to genuine pathology.  \n2. **Statistical Rigor:** The significance of reported performance gains is unclear. The authors should report standard deviations, confidence intervals, or multiple training runs to confirm robustness.  \n3. **Scale‑Adaptation Analysis:** The relationship between learned scale‑weight parameters (α) and actual lesion sizes is not analyzed. Visualizing α distributions or correlations could enhance interpretability.  \n4. **Baseline Comparison Fairness:** Provide details on whether all comparison methods used identical preprocessing, input resolution, and evaluation metrics to ensure fair benchmarking.  \n5. **Clinical Relevance:** Discuss potential runtime feasibility and integration into clinical workflows, and note how the model’s outputs could offer practical interpretability for radiologists.  \n\n**Minor Comments**  \n- Correct minor formatting and spacing errors (e.g., “I NTRODUCTION”) and unify mathematical notation (e.g., \\(x′\\)).  \n- Define acronyms such as SAM and CRD upon first use.  \n- Improve figure readability with larger fonts and color‑blind‑friendly palettes.  \n- Ensure references follow IEEE style formatting.  \n- Specify dataset versions, random seeds, and environment details to strengthen reproducibility.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nSCRD represents a significant and innovative contribution to unsupervised anomaly detection by enhancing reverse‑distillation with contrastive and scale‑adaptive mechanisms. It demonstrates broad cross‑modality applicability and open‑source reproducibility. Nonetheless, the work would benefit from stronger experimental validation and statistical analysis to substantiate its claims. The absence of clinical domain testing limits immediate translational impact, and minor presentation refinements are needed for clarity and consistency. Overall, the paper meets TMI’s standards of innovation but falls short in evaluation and reproducibility completeness.  \n\n**Decision Recommendation:** **Major Revision** — Promising and methodologically sound, but requires additional justification of synthetic anomaly design, statistical significance reporting, and presentation improvements before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel methodology for unsupervised anomaly detection in medical imaging entitled *Scale-Aware Contrastive Reverse Distillation (SCRD)*. This framework incorporates a contrastive student-teacher mechanism and a scale adaptation module, addressing variability in anomaly size. The work aligns well with TMI’s focus on methodological innovation within imaging analysis and machine learning for medical image understanding, contributing significantly to existing reverse-distillation strategies.\n\n**Major Comments:**\n1. **Methodological Justification of Synthetic Anomalies:** The utilization of simplex-noise perturbations needs quantitative validation to demonstrate their relevance to real pathological conditions, such as assessing frequency-domain similarity to actual lesions.\n2. **Statistical Significance and Variability:** The manuscript should report standard deviations or confidence intervals across multiple runs to clarify the robustness of observed AUC improvements against stochastic variation.\n3. **Scale-Adaptation Analysis:** Additional insight is needed on how the learned weights correlate with actual lesion scales; visualizing weight distributions across datasets would enhance understanding.\n4. **Comparison Fairness:** It is critical to verify that all baseline methods are trained under identical preprocessing and input resolution conditions to maintain fairness in comparison.\n5. **Practical/Clinical Discussion:** The inclusion of commentary on the model's integration into clinical workflows and its potential to assist radiologists would provide valuable context.\n\n**Minor Comments:**\n- Address minor spacing and capitalization errors, such as \"I NTRODUCTION,\" and standardize notation formatting (e.g., \\(x′\\)).\n- Define acronyms (e.g., SAM, CRD) upon first use for clarity.\n- Enhance figure font sizes and utilize color palettes that accommodate color blindness.\n- Update references to comply with IEEE formatting guidelines regarding author initials and capitalization.\n- Specify versions of public datasets used and note random seed controls for reproducibility.\n\n**Summary Paragraph:**  \nIn summary, while the manuscript showcases a well-designed and innovative methodology that aligns with TMI’s scope, there are notable weaknesses regarding the justification of synthetic anomalies, limited statistical analysis, and a lack of clinical validation. Clarity and presentation also require improvement to meet the journal’s standards.\n\n**Decision Recommendation:**  \n**Major Revision** — The work demonstrates methodological soundness and potential but requires rigorous validation, clearer explanations, and enhanced presentation to fulfill TMI’s publication criteria.\n\n**Confidence Level:** 4 / 5 — High confidence in the technical and methodological evaluation based on familiarity with the literature on medical image anomaly detection.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chunlei Li",
      "Jingliang Hu",
      "Lichao Mou",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_8f2f06936cc0020aac0ce13e60997a27679bffa6.pdf",
    "remote_url": "https://openreview.net/pdf/8f2f06936cc0020aac0ce13e60997a27679bffa6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning General-purpose Biomedical Volume Representations using Randomized Synthesis",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "synthetic data",
      "representation learning",
      "medical image analysis",
      "image registration",
      "image segmentation"
    ],
    "abstract": "Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network's features can be used as robust representations of input images for downstream tasks and its weights provide a strong, _dataset-agnostic_ initialization for finetuning on new datasets. As a result, we set new standards across _both_ multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes training a backbone that generalizes across different datasets using synthetically generated dataset. The proposed pre-training strategy has 3 main steps: 1) Given a large datasets of 104 annotated organs, randomly sample anatomies, deform them and create a volume by ensembling these anatomies, 2) add noise and other augmentations to the volumes that are sampled in the previous step to simulate realistic looking synthetic medical images from labels, 3) train a U-Net with a contrastive objective by sampling two volumes that share the same 3D semantic layout but differ in appearance, treating corresponding features at different encoder levels as positives and all others as negatives. The pre-trained backbone is validated on two different tasks: 3D registration and 3D few-shot segmentation; using multiple datasets.  The results show the effectiveness of the proposed backbone in the experiments compared to existing methods.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- Foundational models are showing promising performance lately; however, we lack of a 3D model that work across different modalities in medical imaging. The paper proposes a solution to this important problem using domain randomisation and contrastive learning.\n- The paper contain experiments on multiple datasets both for registration and few shot segmentation, and the results demonstrate the potential of the method.\n- The idea of combining the ideas of domain randomisation and local contrastive learning to train a generic 3D backbone is quite interesting and, to my knowledge, is novel.\n\n### Weaknesses\n\n- One issue I see in the paper is the convoluted description of the data engine step, especially the part creating the label ensemble model in section 3. I understand that this step is mainly based on the domain randomisation idea proposed in the literature. However, it is not really clear to me the steps between 201-207, especially the parts multiplying the masks with a randomly deformed sphere and randomly encasing half of the foreground-masked volumes.\n\n- The images generated in the data engine step do not seem like as real medical images. Do they look like this because the deformation is too large? It is not clear why one would prefer training the model using such unrealistic images.\n\n- The paper does not discuss the recent foundational models that show better generalization performance on many medical image datasets [1]. The downstream task performance of the representations obtained from the proposed backbone should be compared with the those obtained by the representations of a foundational model (e.g. DinoV2 [2]). For example, [3] is a recent paper that uses DinoV2 features for registration; but the same applies for the segmentation experiments. One can use the DinoV2 features for segmentation.\n\n[1] Cekmeceli et al. \"Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?\"\n[2] Oquab et al. \"DINOv2: Learning Robust Visual Features without Supervision\"\n[3] Song et al. \"DINO-Reg: General Purpose Image Encoder for Training-Free Multi-modal Deformable Medical Image Registration\"\n\n### Questions\n\n1- What are the exact steps of the \"label ensemble model\" described in Section 3? Please write elaborate description of these steps.\n2- Why do the generated images not look like real medical images? Is it because the deformation is too large? Why such \"unrealistic\" looking images are preferred rather than more realistic ones obtained with smaller deformation?\n3- How does the quality of the representations obtained by the proposed backbone compares with SoTA foundational models such as DinoV2 or SAM2?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript proposes a novel pre-training strategy for a backbone model aimed at improving generalization across diverse datasets in medical imaging. The method involves generating synthetic medical images through a three-step process that includes anatomical sampling, noise addition, and training a U-Net with a contrastive objective. The effectiveness of the proposed approach is evaluated through experiments in 3D registration and 3D few-shot segmentation on multiple datasets, indicating promising results in comparison to existing methods.\n\n### Major Comments\n1. The description of the data engine, particularly the creation of the label ensemble model in Section 3, is convoluted and lacks clarity, especially the steps between lines 201-207 involving mask multiplication with randomly deformed spheres.\n2. The synthetic images generated appear unrealistic as medical images. Clarification is needed regarding why such images are preferred over more realistic representations that could be achieved through smaller deformations.\n3. The manuscript does not address recent advancements in foundational models that exhibit superior generalization on many medical imaging tasks. A comparative analysis of the proposed backbone's performance against established models like DinoV2 is crucial.\n\n### Minor Comments\n- The manuscript could enhance clarity by providing a more detailed explanation of the \"label ensemble model\" within Section 3.\n- Consider refining the language used to describe the image generation process to aid understanding.\n- Review the comparison between the proposed method and current foundational models for improved comprehensiveness.\n\n### Summary Paragraph\nIn terms of significance, the manuscript addresses a critical need for a robust 3D model across various modalities in medical imaging, demonstrating innovation through its combination of domain randomization and contrastive learning techniques. However, the clarity of the evaluation design is compromised by convoluted descriptions, particularly within the data engine section, and the unrealistic nature of synthetic images raises concerns regarding reproducibility. The omission of comparative benchmarks against leading foundational models also detracts from the evaluation's rigor. \n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a framework for training a general-purpose 3D backbone using synthetically generated datasets. The approach involves three main stages: assembling deformed anatomical volumes from 104 annotated organs, applying noise and augmentations to create realistic synthetic images, and training a U-Net backbone with a contrastive learning objective that pairs volumes sharing semantic layouts but differing in appearance. The pretrained backbone is evaluated on 3D registration and few-shot segmentation tasks across multiple datasets. The paper is clearly written overall, with an interesting concept that aims to improve cross-dataset generalization in medical imaging through domain randomization and contrastive pre-training.  \n\n**Major Comments**  \n1. **Clarity of Data Engine Description:**  \n   The procedure for constructing the “label ensemble model” in Section 3 is difficult to follow, particularly lines 201–207 regarding mask multiplication with deformed spheres and partial foreground encasement. A more explicit, step-by-step explanation of this process is needed to ensure reproducibility.  \n2. **Realism of Synthetic Data:**  \n   The synthetic images appear unrealistic, possibly due to excessive deformation. The authors should clarify why such unrealistic appearances are acceptable or advantageous compared with more realistic simulations using milder transformations.  \n3. **Comparison with Foundational Models:**  \n   The paper lacks discussion and empirical comparison with recent foundation models that have shown strong generalization, such as DINOv2 [2] and its applications in medical imaging [1, 3]. Including such baselines would better position the proposed backbone relative to current state-of-the-art approaches in representation learning.  \n\n**Minor Comments**  \n- Some descriptions in the data generation section could be streamlined for clarity.  \n- Ensure consistent terminology between “label ensemble model” and “data engine.”  \n- Figures illustrating synthetic examples should include representative comparisons to real images for context.  \n\n**Summary (TMI Criteria)**  \nThe work addresses a significant problem—building a generalizable 3D backbone for medical imaging—by combining domain randomization and contrastive learning. The approach is conceptually innovative but the methodology needs clearer exposition and stronger evaluation against contemporary foundational models. The experiments demonstrate potential but reproducibility and comparative evidence are limited. Overall, the paper is moderately sound, with contributions that could become significant once the issues above are addressed.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work proposes a pre-training approach for downstream tasks related to fine-grained volumetric medical data: image registration and semantic segmentation. The authors propose to learn appearance invariance and shapes of human anatomy through synthetic dense pixel volumes. In this process, 3D volumes are synthesized by randomly recomposing 3D anatomical shapes and assigning multiple sets of random pixel values, in together with synthetic noises and deformations. Pairs of synthetic volumes are used for multi-scale contrastive learning. The proposed approach demonstrates improved image registration and low-shot image segmentation results compared to some previous works. Detailed ablation studies on the pre-training configurations toward downstream performances are conducted.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe overall methodology is straightforward and easy to understand. It echoes with the classical computer vision concept of invariance learning in the deep neural network era (although learned through a data-driven approach).\n\nImproved image registration results on two public image registration benchmarks and image segmentation performance on six image segmentation datasets are shown, compared to those of some existing works. \n\nThe paper is well-written with sufficient clarity. The illustrations are self-explanatory. Readers will enjoy reading it.\n\n### Weaknesses\n\nTechnical novelty: The core idea behind the approach is to leverage data-driven approach to learn invariance to pixel values through paired synthetic shapes and different pixel values, and to learn semantic-independent shape representation through random geometric (real-world and pure synthetic) shapes – both key ideas come from the well-established SynthMorph (random Ising shapes with random intensity + synthetic deformation for training registration networks. Hoffmann et al.) and SynthSeg (GMM-like pixel value model for repainting brain structures. Billot et al.) Despite leveraging more anatomical shapes beyond brain structures and applied to a contrastive framework, the essence remains unchanged.  \n\nMany medical decisions are made not only on shape but also on subtle textures, for example, differentiating subtypes of tumors/lesions – toward which the proposed over-simplified appearance model by nature falls short. More sophisticated texture models need to be carefully studied beyond this manuscript.\n\nFor the same reason, high-level global semantic information such as relative locations between anatomical structures cannot be learned due to the nature of this approach. \n\nReal-world value: Given the increasing number of large-scale publicly accessible volumetric image datasets such as CT-RATE (Hamamci et al.), Totalsegmenter (Wasserthal et al.), and AbdomenAtlas (Li et al.), and the derived 3D foundation models, the real-world application of the proposed framework is unclear. Some of these large-scale public datasets come with fine-grain pixel-wise labels and associated radiological reports which provide additional supervision signals and text alignment potentials. The claimed generalization capability can be learned from multi-site large real-world datasets as well, through the intrinsic heterogeneity of big data and possibly through intense data augmentation.\n\n### Questions\n\nThe proposed workflow involves many hyper-parameters (Figure 12) controlling the properties of generated synthetic volumes -- what is the rule of thumb for choosing them?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a novel pre-training method aimed at enhancing downstream tasks in fine-grained volumetric medical data, specifically image registration and semantic segmentation. By leveraging synthetic dense pixel volumes generated through various means, including random recompositions of anatomical shapes and pixel value assignments, the authors demonstrate advancements in multi-scale contrastive learning. The results indicate notable improvements in image registration and low-shot image segmentation relative to prior studies.\n\n**Major Comments:**\n1. **Technical Novelty:** While the manuscript proposes a data-driven approach to learning appearance invariance from paired synthetic shapes, the foundational concepts echo previously established methods such as SynthMorph and SynthSeg. The contribution may not substantially diverge from these works, despite the broader range of anatomical shapes adopted.\n2. **Medical Relevance:** The proposed appearance model is seemingly oversimplified for medical applications that require finer performance, such as differentiating tumor subtypes based on subtle textural differences. More advanced texture modeling should be explored.\n3. **Global Contextual Learning:** The current framework does not facilitate learning of high-level semantic information, particularly concerning the relative placement of anatomical structures, which limits its applicability.\n4. **Real-World Application:** The practical significance of this approach is questionable given the availability of large-scale, publicly accessible volumetric datasets that already provide fine-grained labels and radiological insights, suggesting potential avenues for improved generalization that the current work does not address.\n\n**Minor Comments:**  \n- Figure 12 outlines multiple hyper-parameters for generating synthetic volumes; providing guidance on optimal selection would be beneficial.\n- The paper is generally well-written, with self-explanatory illustrations, contributing to clarity.\n\n**Summary Paragraph:**  \nIn summary, while the manuscript demonstrates statistical significance in enhancing image registration and segmentation through a novel pre-training approach, its innovation appears limited by foundational concepts established in prior works. The evaluation of the presented methodology against real-world applications and its generalization capability remains uncertain, necessitating further exploration of texture modeling and contextual information. However, clarity and organization are notable strengths, as indicated by the satisfactory presentation of content.\n\n**Decision Recommendation:**  \nRecommend major revision, with a focus on addressing the technical novelty critique and enhancing practical relevance through comprehensive analysis.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a pre-training framework designed for downstream medical imaging tasks, particularly volumetric image registration and semantic segmentation. The method aims to learn appearance invariance and anatomical shape representations by synthesizing dense 3D pixel volumes through random recombination of anatomical shapes and assignment of synthetic intensities, noise, and deformations. Pairs of synthetic volumes are used for multi-scale contrastive learning, and the approach shows improved registration and few-shot segmentation performance compared to selected prior works. The paper is clearly written, logically structured, and supported by illustrative figures and ablation studies that contribute to its readability and completeness.\n\n**Major Comments**  \n1. **Technical Novelty:** The core concept—learning invariance to pixel-level appearance and shape variations through synthetic volume generation—closely resembles established approaches such as SynthMorph and SynthSeg. While the present work extends these ideas to a contrastive learning framework and includes a wider set of anatomical structures, the fundamental innovation appears limited.  \n2. **Texture Representation:** Many clinical distinctions rely on fine textural cues (e.g., tumor subtype differentiation), which the simplified intensity model does not capture. The current formulation therefore lacks the capacity to model complex appearance variability essential for certain diagnostic applications.  \n3. **Global Semantics:** The approach’s synthetic recomposition process may prevent learning higher-level spatial relations between anatomical parts, restricting the potential to encode global semantic context.  \n4. **Practical Relevance:** With growing accessibility to large annotated 3D datasets (e.g., CT-RATE, Totalsegmenter, AbdomenAtlas) and foundation models trained thereon, the added real-world value of this synthetic-data-driven framework is uncertain. Comparable generalization could potentially be achieved through training on these heterogeneous real datasets with standard augmentation.  \n5. **Methodological Clarity:** The workflow involves many hyperparameters controlling the synthesis process (as in Figure 12). The manuscript would benefit from guidance or heuristics on how to select these parameters.\n\n**Minor Comments**  \n- Figures are clear and self-explanatory, though a concise table summarizing datasets and evaluation metrics would further improve readability.  \n- Ensure that all acronyms (e.g., GMM, SSIM) are defined at first use.\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important question in pre-training for volumetric medical imaging and presents a well-documented evaluation with consistent results. Its strengths lie in methodological clarity, extensive experiments, and connection to invariance learning principles. However, its degree of novelty is modest, the modeling of texture and global structural information is limited, and practical benefit relative to existing large-scale real-data training remains uncertain. Reproducibility appears achievable thanks to explicit ablations and parameter disclosure.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision** – The paper demonstrates solid experimentation and clear presentation but requires stronger justification of novelty, practical impact, and methodological guidance before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nAuthors present a method to generate highly variable synthetic imaging data which is then used to pre-train a 3D network using contrastive learning. The data generation method consists of drawing samples from a semantically labelled repository of biomedical shape templates to randomly populate an empty 3D volume. The volume is then deformed. Empty space and organ 'envelopes' are also simulated. To simulate different modalities and protocols, random intensity transformation is applied to the deformed 3D volume to yield 2 images. Typical imaging artifacts such a sbias field and blurring are simulated through random augmentations.The two images are fed into a UNet, and contrastive pre-training is performed on features at each decoder layer. An anchor point is chosen in one of the images, and all voxels of that label in both images are considered positive, and everything else negative pairs. The network yields features that can be used to finetune on other modalities and tasks. Importantly, the representations are modality-agnostic and anatomy-agnostic.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n* The paper is very well written - it lays out the prior work and puts the contirbute in context.\n* The approach yields representations that are both modality-agnostic and task-agnostic while removing the need for dataset-specific and anatomy-specific pre-training.\n* Authors present results of several downstream tasks using their features including multi-modality registration image registration and few-shot segmentation on which their method outperform the others compared.\n* Authors perform ablation studies on the various components of their pipeilne.\n* The Authors present extensive visualization and quantitative results in their main text, and supplementary material. Algorithms and parameters are clearly presented too which allows for further scrutiny and improved reproducability.\n* Authors are aware of the limitations of their approach and include these in the paper.\n\n### Weaknesses\n\n* The segmentation task performed using the Authors features may yield better results than the other methods that are compared, however the result still misses significant portions of the anatomical regions they aim to segment. The features require further adjustment and extensive fine-tuning to be useful in diagnosis and treatment.\n\n### Questions\n\n* Authors compare their randomized shape template-based synthetic data engine to one that uses data with no biomedical priors and one using brain regions. Can Authors elaborate more on the intuiton for why their randomly deformed template shapes are so effective? Is there some point at which the extent of the deformation causes the representations to be less useful?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a method for generating highly variable synthetic imaging data, which is employed to pre-train a 3D network using contrastive learning. The data generation involves sampling from a semantically labeled repository of biomedical shape templates to populate an empty 3D volume, which is then deformed. This process integrates random intensity transformations and simulates typical imaging artifacts, ultimately creating two images that serve as input to a UNet model for contrastive pre-training. The results indicate that the generated synthetic representations are modality-agnostic and anatomy-agnostic, enhancing their applicability across multiple tasks.\n\n### Major Comments\n1. The segmentation task using the authors' features does yield improved results compared to other methods; however, significant anatomical regions remain under-segmented. Further refinement and extensive fine-tuning of the features are needed for effective diagnostic and treatment applications.\n2. The comparison between the authors' randomized shape template approach and alternatives lacking biomedical priors or using brain-region data needs further elaboration. The authors should clarify the rationale behind the effectiveness of their randomly deformed template shapes and discuss potential limits in deformation extent that might impact representation utility.\n\n### Minor Comments\n- The writing is clear and well-structured, effectively contextualizing the contributions within prior work.\n- Extensive visualizations and quantitative results are provided in both the main text and supplementary material, enhancing reproducibility.\n- Algorithms and parameters are presented clearly, allowing for further scrutiny.\n\n### Summary Paragraph\nThis manuscript addresses significant issues in synthetic imaging data generation and offers an innovative approach to pre-training models for biomedical applications. The evaluation shows promise, with features being modality-agnostic and applicable to various tasks. However, the reproducibility is somewhat hindered by the current limitations in the segmentation performance. The concerns raised regarding the clarity of comparisons made in the study suggest areas for refinement.\n\n### Decision Recommendation\nMinor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a framework for generating highly variable synthetic 3D biomedical images for contrastive pretraining of neural networks. By sampling from a semantically labeled shape repository, populating 3D volumes, applying random deformations, and simulating imaging effects (intensity changes, bias fields, and blur), the authors create realistic variations across modalities. These synthetic image pairs are used to train a UNet via voxel-level contrastive learning to obtain representations that are both modality- and anatomy-agnostic. The manuscript is clearly written, well contextualized within related work, and supported by detailed methodological and experimental descriptions.\n\n**Major Comments**  \n1. **Performance Limitations:** Although the proposed features outperform comparable methods in multi-modality registration and few-shot segmentation, the segmentation results still omit substantial portions of target anatomy. This suggests that the learned representations may require further refinement or fine-tuning for clinical utility.  \n2. **Effectiveness of Synthetic Deformations:** The comparison between random shape-template generation and other approaches is interesting, but the rationale for why random deformations enhance representation learning could be explained more thoroughly. It would be valuable to discuss whether excessive deformation could reduce representational utility.  \n3. **Evaluation Scope:** While comprehensive ablations and downstream results are shown, further discussion of how these results generalize across unseen modalities or clinical tasks would strengthen reproducibility and external validity claims.\n\n**Minor Comments**  \n- The manuscript’s writing and structure are strong, with clear algorithmic details and reproducible parameter settings.  \n- Typographical errors (e.g., “contirbute,” “pipeilne,” “sbias”) should be corrected for presentation polish.  \n- Ensure consistent terminology for tasks (e.g., “multi-modality registration” vs. “image registration”) and clear definition of acronyms on first use.\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in that it provides a general pretraining approach that eliminates the need for anatomy- or modality-specific datasets. Its **innovation** lies in the synthetic biomedical shape-based data engine and contrastive volumetric feature learning. The **evaluation** is thorough, including multiple downstream tasks and ablations, though some results indicate that fine-tuning remains necessary. The study’s **reproducibility** is high due to clear presentation of methods and parameters. Overall, a strong, well-presented contribution with room for improvement in practical performance and theoretical justification.\n\n**Decision Recommendation**  \n**Recommendation: Minor Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents a representation learning framework for 3D biomedical imaging that addresses domain generalization challenges through synthetic data generation. The method comprises two main components: (1) a data engine that synthesizes highly variable 3D volumes using randomized compositions of biomedical shape templates from TotalSegmentator (Figure 2, Section 3), and (2) a contrastive learning approach that pretrains a 3D U-Net to be stable against nuisance imaging variations while preserving semantic content (Figure 3, Equation 1). The authors evaluate their approach on multi-modality registration (L2RAb and MM-WHS datasets) and few-shot segmentation across six diverse datasets. Results demonstrate state-of-the-art performance on unsupervised multi-modality registration, with ConvexAdam-Ours achieving 11 and 6 Dice point improvements over baseline ConvexAdam on L2RAb and MM-WHS respectively (Figure 5). For few-shot segmentation, the method consistently ranks first or second across datasets while using fewer parameters than competing foundation models (Table 1).\n\n## Weaknesses\n\n• **Mathematical formulation clarity and consistency issues**\n  - The contrastive loss definition in Equation 1 uses notation that could be clearer, particularly the relationship between spatial indices i and label assignments ki, which may confuse readers about the exact positive/negative pair construction\n  - The temperature parameter τ = 0.33 is presented without theoretical justification or comparison to standard contrastive learning practices (typically τ = 0.07 in natural vision), making it difficult to assess the principled nature of this choice\n  - The multi-scale supervision strategy mentions applying the loss \"on multiple decoder layers\" but lacks mathematical specification of how losses across scales are weighted or combined\n\n• **Limited evaluation scope and baseline comparisons**\n  - Registration experiments are confined to only two datasets (L2RAb with 8 pairs, MM-WHS with 20 pairs), providing insufficient evidence for broad generalization claims across radiological domains\n  - The exclusion of recent 2D foundation models like DINOv2 from main comparisons (only appearing in Appendix A.7) undermines the comprehensiveness of the evaluation, especially given concurrent work showing their effectiveness in medical imaging\n  - Few-shot segmentation experiments use very small training sets (1-3 volumes) which may not reflect realistic clinical deployment scenarios where more data might be available\n\n• **Synthetic data engine validation concerns**\n  - The data engine relies heavily on TotalSegmentator templates (45,000 binary volumes from 1,204 CT scans) which may introduce systematic biases toward CT-like anatomical configurations, potentially limiting true domain generalization\n  - No direct validation is provided showing that the synthetic data captures the statistical properties or domain shifts present in real biomedical imaging, making it unclear whether the approach addresses actual deployment challenges\n  - The appearance model using independent Gaussian mixture models (Section 3, Figure 2B) may oversimplify complex tissue appearance relationships found in real medical images\n\n• **Experimental design and reporting limitations**\n  - The paper lacks statistical significance testing for performance differences, making it difficult to assess whether reported improvements are meaningful given the small dataset sizes\n  - Ablation studies in Table 3 show substantial performance variations (e.g., 0.74 vs 0.51 Dice for different pretraining configurations) but provide limited analysis of why certain components are critical\n  - The comparison with foundation models is potentially unfair due to significant parameter count differences (5.9M vs 67.2M parameters), and the matched parameter comparison in Appendix A.4 is relegated to supplementary material\n\n## Suggestions for Improvement\n\n• **Enhance mathematical presentation and theoretical grounding**\n  - Provide clearer notation in Equation 1 by explicitly defining the mapping between spatial coordinates and label assignments, potentially with a supplementary algorithmic description of the positive/negative pair construction process\n  - Include theoretical justification or empirical analysis for the τ = 0.33 choice, comparing performance across different temperature values and relating findings to contrastive learning theory\n  - Specify the mathematical formulation for multi-scale loss combination, including weighting schemes and any normalization applied across decoder layers\n\n• **Expand evaluation comprehensiveness and baseline coverage**\n  - Include additional registration datasets and modality combinations to strengthen generalization claims, particularly datasets with different anatomical regions or imaging protocols not covered by the current two datasets\n  - Move the DINOv2 comparison from appendix to main text and include other recent 2D foundation models adapted for 3D medical imaging to provide more comprehensive baseline coverage\n  - Evaluate performance across a broader range of annotation budgets (e.g., 1, 5, 10, 25 volumes) to better characterize the method's behavior in more realistic clinical scenarios\n\n• **Strengthen synthetic data validation and analysis**\n  - Provide quantitative analysis comparing statistical properties of synthetic versus real data distributions, potentially using domain adaptation metrics or distribution matching techniques\n  - Include experiments using shape templates from multiple anatomical sources beyond TotalSegmentator to demonstrate reduced bias and improved generalization\n  - Validate the appearance model by comparing synthetic intensity distributions with real multi-modal imaging data, potentially including tissue-specific intensity analysis\n\n• **Improve experimental rigor and result interpretation**\n  - Include statistical significance testing (e.g., paired t-tests, bootstrap confidence intervals) for all performance comparisons to establish meaningful differences given small sample sizes\n  - Provide deeper analysis of ablation results explaining why specific components (e.g., foreground masking, temperature settings) are critical for performance\n  - Move parameter-matched comparisons to the main text and provide detailed analysis of performance scaling with model capacity across different foundation model architectures",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript presents a representation learning framework aimed at enhancing 3D biomedical imaging through synthetic data generation to tackle domain generalization challenges. It consists of a data engine that synthesizes varied 3D volumes using biomedical shape templates and a contrastive learning approach that pretrains a 3D U-Net. The evaluation of this methodology on multi-modality registration and few-shot segmentation indicates competitive performance compared to state-of-the-art approaches, though the clarity and comprehensiveness of the evaluation warrant further scrutiny.\n\n**Major Comments:**\n1. **Mathematical formulation clarity and consistency issues:**\n   - The contrastive loss definition in Equation 1 lacks clarity regarding the relationship between spatial indices and label assignments, which may confuse readers.\n   - The temperature parameter τ = 0.33 is presented without sufficient theoretical justification, complicating an assessment of its appropriateness in contrastive learning contexts.\n   - The specification of the multi-scale supervision strategy is vague regarding the combination of losses across scales.\n\n2. **Limited evaluation scope and baseline comparisons:**\n   - The experiments for multi-modality registration are limited to two datasets, which may not substantiate claims of generalization across various radiological domains.\n   - The absence of recent 2D models like DINOv2 in primary comparisons limits the evaluation’s depth.\n   - Few-shot segmentation relies on very small training sets that do not reflect realistic clinical scenarios.\n\n3. **Synthetic data engine validation concerns:**\n   - Heavy reliance on TotalSegmentator templates introduces potential biases, limiting domain generalization.\n   - Lack of validation questioning whether synthetic data captures real biomedical imaging properties poses doubts on its deployment applicability.\n   - The use of independent Gaussian mixture models might oversimplify complex tissue appearance.\n\n4. **Experimental design and reporting limitations:**\n   - Absence of statistical significance testing for performance results challenges the reliability of reported improvements.\n   - Insufficient analysis of ablation study results limits understanding of critical components.\n   - Comparisons among models may be biased due to differences in parameter counts.\n\n**Minor Comments:**\n- Clarify notation in Equation 1 and consider supplementary algorithmic descriptions.\n- Provide justification for the choice of τ = 0.33 and analyze its impact in relation to contrastive learning theory.\n- Improve reporting of performance comparisons by including statistical significance testing.\n\n**Summary Paragraph:**\nIn terms of significance, the manuscript addresses an important challenge in 3D biomedical imaging; however, its innovation is tempered by the clarity of its mathematical frameworks. The evaluation, while showcasing potential, lacks comprehensive validation across diverse datasets and baseline models, thus impacting reproducibility. Suggestions to enhance statistical rigor and broaden the scope of experiments could strengthen the findings and future applications of the framework.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a representation learning framework for 3D biomedical imaging aimed at improving domain generalization through synthetic data generation. The approach integrates (1) a data engine that produces diverse 3D volumes by randomizing compositions of biomedical shape templates from TotalSegmentator, and (2) a contrastive pretraining strategy for a 3D U-Net designed to maintain semantic stability under imaging variability. Evaluation includes multi-modality registration on L2RAb and MM-WHS datasets and few-shot segmentation across six datasets. The method achieves state-of-the-art unsupervised registration and strong segmentation performance with comparatively few parameters. The paper is clearly structured, but certain theoretical, methodological, and validation aspects require refinement.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity**:  \n   - The contrastive loss (Equation 1) introduces unclear notation, particularly concerning spatial indices and label assignments, making positive/negative pair definition ambiguous.  \n   - The choice of temperature parameter (τ = 0.33) lacks theoretical justification or empirical comparison to standard values (e.g., τ = 0.07).  \n   - The multi-scale supervision strategy is insufficiently specified; weighting or combination methodology across decoder layers is not defined.  \n\n2. **Evaluation scope and baseline selection**:  \n   - Registration studies use only two small datasets, limiting evidence for broad domain generalization.  \n   - Comparisons omit key recent foundation models such as DINOv2 from the main results, reducing completeness.  \n   - Few-shot segmentation experiments rely on extremely small training sets that may not mirror realistic clinical conditions.  \n\n3. **Synthetic data validation**:  \n   - Heavy dependence on TotalSegmentator templates risks bias toward CT anatomy.  \n   - No quantitative validation demonstrates that synthetic data emulate real imaging distributions or domain shifts.  \n   - The Gaussian mixture–based appearance model may oversimplify complex tissue intensities.  \n\n4. **Experimental design and reporting**:  \n   - Absence of statistical significance testing weakens claims of improvement.  \n   - Ablation results, though substantial, are underexplained.  \n   - Comparisons to foundation models differ greatly in parameter size, and parameter-matched results are confined to the appendix.  \n\n**Minor Comments**  \n- Clarify notation and include algorithmic pseudocode for Equation 1.  \n- Add explicit weighting formula for multi-scale losses.  \n- Present DINOv2 results and parameter-matched model comparisons in the main text.  \n- Discuss variations in performance across different annotation levels.  \n- Include quantitative analysis comparing real and synthetic data characteristics.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper is strong in conceptual innovation, combining synthetic data generation with contrastive pretraining for 3D imaging. Its potential significance for domain generalization is notable. However, the innovation’s impact is limited by inadequate validation of synthetic data and narrow evaluation coverage. Empirical testing lacks breadth and statistical rigor, and reproducibility could be improved with clearer mathematical exposition and fuller baseline reporting. Strengthening theoretical justification and validation would enhance both credibility and generalizability.  \n\n**Decision Recommendation**  \n**Major Revision.** The framework is promising but requires clearer mathematical formulation, expanded and more equitable evaluation, and stronger validation of synthetic data and experimental claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI:**  \n✅ The paper clearly contributes to medical imaging methodology. It proposes a new data synthesis and contrastive pretraining framework for *volumetric* biomedical imaging, addressing generalization across modalities and anatomical regions. The work is positioned at the intersection of imaging physics (simulation), image representation learning, and algorithmic generalization — all central to IEEE TMI’s mission.\n\n2. **Novelty & Contribution Level:**  \nHigh. The paper introduces a *domain-randomized, biomedically informed synthetic data engine* combined with a novel multi‑positive voxel-level contrastive loss for training “general-purpose” 3D medical imaging networks without real data. Unlike prior foundation models that rely on large real datasets, this approach exploits synthetic variability to achieve robustness to domain shifts. It represents a conceptual advance in dataset‑agnostic volumetric pretraining.\n\n3. **Technical and Experimental Rigor:**  \nThe work is technically thorough. The formulations, augmentation pipelines, and loss functions are clearly described, with implementation details and ablations provided. Experiments include **two key tasks** — multimodality registration and few‑shot segmentation — tested across *diverse public datasets*. Comparisons are fair and include robust baselines. Code release is mentioned, enhancing reproducibility. The main weakness lies in limited theoretical justification for why synthetic variability suffices to replace empirical diversity and incomplete uncertainty analysis.\n\n4. **Clarity and Presentation:**  \nOverall clarity and structure are excellent. Figures effectively illustrate synthetic data, representation stability, and task outcomes. The language is professional, though at times dense. Some sections (Appendices) might be too long for a journal submission and could be condensed.\n\n5. **Ethical and Reproducibility Compliance:**  \nAll used datasets are public or anonymized, appropriate acknowledgments and IRB approval for in‑house data (WUFetal) are included, and code availability is stated. Ethical standards are met.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n1. **Summary**  \nThe manuscript presents a *synthetic‑data–driven pretraining framework* for 3D biomedical imaging. Using a “label ensemble” model that assembles deformed biomedical templates and a randomized Gaussian‑mixture appearance simulator, the authors synthesize diverse volumetric data. A multi‑scale, multi‑positive contrastive objective then trains a single 3D U‑Net to learn appearance‑invariant voxel representations. Trained entirely on synthetic volumes, the model’s features serve effectively for multimodal registration and as initialization for medical segmentation across multiple datasets and modalities, achieving state‑of‑the‑art or second‑best performance without using any real images.\n\n2. **Strengths**\n- Clear methodological innovation in *synthetic‑data generative design* and *contrastive voxelwise pretraining*.  \n- Thorough and carefully controlled experiments across both **registration and segmentation** — rarely demonstrated jointly in prior work.  \n- Strong empirical generalization using no real training images.  \n- Transparent description of implementation details and publicly released resources.\n\n3. **Weaknesses**\n- Limited theoretical analysis of why the synthetic distribution adequately spans real‑image variability or of potential failure cases.  \n- Relatively little evaluation on physics‑based modalities (e.g., ultrasound, PET) or strongly anisotropic reconstructions.  \n- Absence of statistical significance testing for performance differences.  \n- Discussion could better articulate how this framework aligns with or extends standard medical imaging pipelines used in TMI practice.\n\n4. **Major Comments**\n- **Methodological justification:** Provide clearer discussion of the representational properties induced by the contrastive loss — e.g., degree of invariance versus equivariance — and how these relate to information content for clinical imaging tasks.  \n- **Quantitative robustness:** Include uncertainty estimates or confidence intervals for Dice and folding metrics to better substantiate improvements.  \n- **Scope expansion:** Demonstrate or discuss adaptation to additional modalities (ultrasound, PET, microscopy) or physics simulators, as this would strengthen claims of “general‑purpose.”  \n- **Reproducibility:** Consider releasing the label synthesis code in addition to pretrained weights to enable community benchmarking.  \n- **Clinical translation:** Expand discussion on how dataset‑agnostic pretraining might influence clinical workflow integration or regulatory reproducibility standards.\n\n5. **Minor Comments**\n- Minor typographical and formatting issues (spacing in headings, figure captions occasionally crowded).  \n- References are comprehensive but could highlight additional TMI‑published synthetic data approaches.  \n- Condense appendices for journal format; move extended code listings to supplementary materials.\n\n6. **Overall Recommendation:** **Major Revision**  \nThe manuscript exhibits strong novelty and rigor and could reach TMI standards with clarifications and expanded discussion on theoretical grounding and domain coverage. The core idea is compelling and aligns well with TMI’s methodological focus, but additional analysis and polish are needed for journal publication.\n\n7. **Confidence Level:** 5 (High)",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a synthetic‑data–driven pretraining framework for volumetric biomedical imaging. It aims to enable generalizable 3D image representation learning without using real data by combining a domain‑randomized biomedical simulator with a multi‑positive, voxel‑level contrastive learning objective. The manuscript is well structured, clearly written, and relevant to IEEE TMI’s methodological scope, connecting imaging physics, data synthesis, and representation learning. Overall clarity is high, though some sections could be more concise.\n\n---\n\n**Major Comments**  \n1. **Theoretical Justification:** The paper lacks detailed theoretical explanation of why synthetic variability is sufficient to replace empirical diversity and how the proposed contrastive loss shapes invariance or equivariance properties relevant to clinical tasks.  \n2. **Quantitative Robustness:** While experiments are thorough, uncertainty analyses or statistical significance tests (e.g., for Dice and folding metrics) are missing, limiting confidence in reported improvements.  \n3. **Experimental Scope:** The evaluation focuses mainly on MRI‑ and CT‑like data; inclusion or at least discussion of physics‑based modalities such as ultrasound, PET, or anisotropic reconstructions would strengthen claims of generality.  \n4. **Reproducibility Enhancement:** Code availability is promised, but full reproducibility would benefit from releasing the label synthesis engine as well as pretrained models for community benchmarking.  \n5. **Clinical and Practical Context:** Discussion could expand on how dataset‑agnostic pretraining fits within existing medical imaging workflows and regulatory reproducibility frameworks.\n\n---\n\n**Minor Comments**  \n- Minor typographical and formatting inconsistencies (spacing, figure captions).  \n- Appendices are lengthy; consider condensing or moving extended code examples to supplementary materials.  \n- References are comprehensive but could include additional TMI‑published works on synthetic data generation.  \n- Language is professional but occasionally dense — some phrasing could be simplified for accessibility.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is *significant* in advancing synthetic‑data‑based learning for 3D medical imaging and demonstrates *innovation* through the integration of domain‑randomized simulation and voxelwise contrastive objectives. *Evaluation* is convincing, covering registration and segmentation across several datasets, but lacks statistical robustness and theoretical clarity on generalization limits. *Reproducibility* is supported by detailed implementation and planned code release, though access to full data generation tools would further strengthen openness. With improved theoretical explanation, expanded domain coverage, and refined presentation, the study could meet TMI’s standards.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a significant advancement in medical imaging methodology by proposing a new data synthesis and contrastive pretraining framework specifically for volumetric biomedical imaging. It addresses essential challenges regarding generalization across various modalities and anatomical regions, situating its contributions at the convergence of imaging physics, image representation learning, and algorithmic generalization, all of which are central to the IEEE Transactions on Medical Imaging (TMI) mission.\n\n**Major Comments:**  \n1. **Methodological Justification:** Clarify the representational properties induced by the contrastive loss, specifically the distinctions between invariance and equivariance, and their implications for clinical imaging tasks.  \n2. **Quantitative Robustness:** Provide uncertainty estimates or confidence intervals for metrics like Dice and folding scores to substantiate the reported improvements.  \n3. **Scope Expansion:** Discuss potential adaptations of the framework to additional modalities such as ultrasound, PET, and microscopy, which would strengthen claims of its general-purpose applicability.  \n4. **Reproducibility:** Consider releasing the label synthesis code alongside pretrained weights to facilitate community benchmarking.  \n5. **Clinical Translation:** Expand on the implications of dataset-agnostic pretraining for clinical workflow integration and compliance with regulatory reproducibility standards.\n\n**Minor Comments:**  \n- There are minor typographical and formatting issues, including crowded figure captions and inconsistent spacing in headings.  \n- While references are comprehensive, highlighting additional synthetic data approaches published in TMI could enhance the manuscript.  \n- Appendices should be condensed to fit journal formatting, with extended code listings moved to supplementary materials.\n\n**Summary Paragraph:**  \nIn terms of significance, the work addresses a pertinent gap in 3D biomedical imaging methodologies through a novel synthetic-data-driven approach. Its innovation lies in the development of a domain-randomized data engine and contrastive loss mechanism, demonstrating strong empirical performance without reliance on real training images. However, there are weaknesses in theoretical justification and evaluation breadth. The experimental design is rigorous and reproducibility is addressed, yet the manuscript would benefit from further analysis and a broader scope. \n\n**Decision Recommendation:**  \n**Major Revision**  \nThe manuscript demonstrates strong novelty and technical rigor and has the potential to meet TMI's standards with the recommended clarifications and discussions. The core concept is compelling and aligns well with TMI's focus; however, additional analysis and refinement are necessary for publication. \n\n**Confidence Level:**  \n5 (High)",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Adrian V Dalca",
      "Benjamin Billot",
      "Clinton Wang",
      "Ellen Grant",
      "Mengwei Ren",
      "Neel Dey",
      "Polina Golland",
      "Hallee E. Wong"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2d77a9ccf261321c494f73f45590632c7fc2bce6.pdf",
    "remote_url": "https://openreview.net/pdf/2d77a9ccf261321c494f73f45590632c7fc2bce6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Time-to-Event Pretraining for 3D Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Multimodal learning",
      "medical imaging",
      "Electronic Health Records"
    ],
    "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell’s C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\n*Edit: Score increased from 6 to 8 during discussion period.*\n\nThis paper presents a self-supervised learning (SSL) method for 3D medical imaging data that leverages electronic health records (EHR) to provide extra sources of supervision via time-to-event modeling. The proposed method, future-guided pretraining, performs time-to-event (TTE) survival modeling of various medical events in the longitudinal EHR associated with each 3D scan. The authors show that future-guided pretraining consistently improves downstream TTE modeling and prognostic classification tasks – also improving data efficiency – without degrading standard diagnostic classification performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The presentation quality is very high. Care has been taken to logically organize the paper, clearly articulate key points, and straightforwardly present results with concise figures and tables.\n- The core idea is creative, making use of the wealth of longitudinal EHR data associated with each 3D volume for pretraining.\n- Discussion or related work and background is particularly strong.\n- Experiments are sufficiently thorough and easy to interpret – results are convincing.\n\n### Weaknesses\n\n- The actual description of the TTE pretraining approach is brief (lines 184-191) and somewhat unclear. I would advise the authors to flesh out this section. See specific questions below.\n- A description or list of the 8,192 EHR pretraining tasks is never provided. I’m aware there may not be a convenient place to list this many items, but a general description of categories of events or a few illustrative examples would be helpful. Without this information, it’s impossible to assess whether, e.g., one the TTE pretraining tasks is *also* used as a downstream TTE modeling task. In this case, there may be concerns of “label leakage”.\n\nI’m happy to increase my score once these issues are addressed – this is an otherwise strong submission.\n\n### Questions\n\n- What exactly does it mean that Steinberg et al.’s method was used to “[sample tasks to maximize entropy given the frequency distribution of medical codes populating the DAG”? I feel that a basic plain-language description of the motivation for this procedure is needed first: why is this method being applied at all? Are there way more than 8k events and the goal is to settle on a subset of 8k “meaningful”/common ones for pretraining? I don’t understand the motivation.\n- Unless I am misunderstanding, this is the only description of the TTE pretraining procedure and labels used: “We define our TTE task labels by predicting the time until the next occurrence of a medical code.” The previous Section 3 described deep survival modeling in the abstract, so I expected Section 4 to more concretely describe how TTE pretraining works. Is this a “competing risks” approach, where multiple events are being modeled simultaneously (in “multi-label” fashion)?\n- What are the 8,192 EHR tasks/events? I’m aware it would be cumbersome or impossible to list and define them all, but any reasonable attempt to convey information about them would be useful. What kinds of “events” are they? What are some examples?\n- Related to the above point, are the downstream labels *also* present in the set of TTE pretraining tasks? If so, isn’t there concern of “label leakage”, where the model has been pretrained on label information present in the downstream training dataset? Please clarify this.\n\n**Minor comments/questions:**\n- Line 13: Maybe “build” instead of “capture” since you use this word in the next sentence.\n- In-text citation style seems off – should be parenthetical (\\pcite{}) in most cases when used at end of sentence/clause: “Sox et al. (2024)” -> “(Sox et al., 2024)”\n- Change “e.g.” -> “e.g.,” throughout\n- Would include more recent references [1,2] when discussing deep prognosis models on longitudinal medical imaging (first paragraph of Section 2)\n- “i.e. 8192” -> “i.e., 8.192”\n- “Our approach improves training data efficiency, increasing training labels by an average of 3x over labels assigned to patients based on their current EHR visit.” This is a bit unusual to highlight as a main contribution – I don’t think readers will understand what “increasing training labels” means without having read the entire paper (nor why this impact data efficiency). Perhaps clarify language here to indicate that your approach provides 3x as many sources of supervision during SSL + that this is what provides data efficiency benefits.\n- “Pretraining task labels as assigned per-CT scan and vary in density based on pretraining approach, see Figure 2.” Perhaps “as assigned” is meant to be “are assigned”? Also change “, see Figure 2” -> “(Figure 2)”.\n- Be consistent with “c-statistic” vs. “C-statistic”\n\n**References**\n[1] Holste, Gregory, et al. \"Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling.\" NPJ Digital Medicine 7.1 (2024): 216.\n[2] Sriram, Anuroop, et al. \"Covid-19 prognosis via self-supervised representation learning and multi-image prediction.\" arXiv preprint arXiv:2101.04909 (2021).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces a novel self-supervised learning (SSL) method specifically designed for 3D medical imaging data, which integrates electronic health records (EHR) to enhance supervision through time-to-event (TTE) modeling. The proposed future-guided pretraining approach aims to model TTE survival of various medical events associated with longitudinal EHR data linked to each 3D scan. The authors demonstrate that this methodology consistently enhances downstream TTE modeling and prognostic classification tasks, improving data efficiency without compromising standard diagnostic classification performance.\n\n**Major Comments:**\n1. The description of the TTE pretraining approach is notably brief (lines 184-191) and lacks clarity; further elaboration in this section is recommended.\n2. A detailed description or a list of the 8,192 EHR pretraining tasks is absent. While exhaustive detailing may not be feasible, providing categories or illustrative examples would help assess the relevance of the tasks. This is crucial to understanding potential \"label leakage\" concerns.\n3. The method utilizing Steinberg et al.'s approach to \"[sample tasks to maximize entropy given the frequency distribution of medical codes populating the DAG]\" requires clearer explanation and motivation. \n4. The description of TTE task labels, specifically “predicting the time until the next occurrence of a medical code,” lacks concrete exposition; clarification on whether this employs a “competing risks” approach is needed.\n5. Further insights into the nature and examples of the 8,192 EHR tasks/events would be beneficial.\n6. Clarification is needed regarding whether downstream labels overlap with TTE pretraining tasks to address potential \"label leakage.\"\n\n**Minor Comments:**\n- \"Capture\" in line 13 could be replaced with \"build.\"\n- In-text citations should generally be formatted parenthetically: change “Sox et al. (2024)” to “(Sox et al., 2024).”\n- Modify \"e.g.\" to \"e.g.,\" consistently throughout the manuscript.\n- Recent references [1,2] should be included within the discussion of deep prognosis models in Section 2's first paragraph.\n- Correct “i.e. 8192” to “i.e., 8,192.”\n- Refine the phrase about increasing training labels for clarity; specify that the approach increases supervision sources by an average of 3x.\n- The sentence about assignment per-CT scan needs review for grammatical accuracy.\n- Ensure consistency in terminology regarding “c-statistic” and “C-statistic.”\n\n**Summary Paragraph:**  \nThis study presents a significant innovation in SSL for 3D medical imaging, leveraging EHR data to improve TTE modeling. The manuscript is well-organized and the experiments are convincingly thorough, aligning well with TMI's editorial criteria. However, the clarity of key methodological sections and potential ambiguities regarding task overlap present notable concerns that must be addressed to enhance reproducibility and evaluation. The contributions, while substantial, are somewhat diluted by these weaknesses.\n\n**Decision Recommendation:**  \nMinor revision required to address the highlighted issues and improve clarity before acceptance.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a self‑supervised learning (SSL) framework for 3D medical imaging that integrates longitudinal electronic health records (EHRs) through time‑to‑event (TTE) modeling. The approach, termed *future‑guided pretraining*, leverages survival modeling of EHR‑derived medical events to enhance representation learning and improve downstream prognostic and classification tasks. The paper is well structured and clearly written, with logical organization, informative figures and tables, and a thorough discussion of related work. Overall presentation quality is high, and the contribution—linking EHR‑based temporal information with SSL for 3D imaging—is both creative and promising.\n\n---\n\n**Major Comments**  \n1. **Method Description Needs Expansion:** The explanation of the TTE pretraining method (lines 184–191) is overly brief and difficult to follow. Further detail is needed on how TTE modeling is implemented in pretraining and how it differs from standard survival analysis frameworks.  \n2. **Specification of Pretraining Tasks:** The manuscript mentions 8,192 EHR‑based pretraining tasks but never describes their nature or provides examples. Summarizing the types of medical events, or illustrating several representative categories, would help assess the validity and relevance of the pretraining signals.  \n3. **Clarification of Task Sampling Procedure:** The sentence referencing Steinberg et al.’s method for “sampling tasks to maximize entropy” is obscure. A plain‑language explanation of why this approach is used and whether it selects a representative or reduced subset of medical codes is necessary.  \n4. **Potential for Label Leakage:** The review raises an important question regarding overlap between pretraining and downstream TTE tasks. If identical events appear in both, there may be inadvertent label leakage. The authors should explicitly clarify whether task sets are disjoint.  \n\n---\n\n**Minor Comments**  \n- Use “build” instead of “capture” in line 13 for fluency.  \n- Adjust citation style to parenthetical form (e.g., “(Sox et al., 2024)”).  \n- Add comma after “e.g.” and “i.e.” consistently.  \n- Include recent references such as Holste et al. (2024) and Sriram et al. (2021) when discussing prognosis modeling.  \n- Clarify expression “increasing training labels by an average of 3×” to indicate increased supervisory signals during SSL training.  \n- Rephrase “Pretraining task labels as assigned…” → “are assigned” and move “see Figure 2” into parentheses.  \n- Maintain consistency in “c‑statistic” capitalization.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in exploring an innovative intersection between EHR‑derived temporal data and 3D image SSL, potentially benefiting data efficiency and medical prognostic modeling. The novelty and evaluation are convincing, supported by clear experiments and strong discussion. However, reproducibility would be improved by a fuller description of pretraining tasks and clearer methodology for task sampling and TTE formulation. Addressing these clarifications would remove remaining ambiguities and strengthen the technical soundness.  \n\n---\n\n**Decision Recommendation:** **Minor Revision (score = 8, confidence = 4)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors proposed to utilize the time-to-event information in EHR that paired with the imaging data as a form of supervision for the pre-training purpose. A public dataset with both 3D images and EHR notes is employed for the pre-training and downstream applications. Another dataset without the time events is also used for the evaluation of model adaptation. The manuscript is easy to follow. However, it also suffers from several critical flaws, which are detailed below.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- Propose utilizing the time events as pre-training tasks specially designed for prognosis tasks in downstream applications. \n- The manuscript is overall easy to follow\n\n### Weaknesses\n\n- The proposed method is limited in generalization since it will require longitudinal time-to-event EHR data as the supervision for the pre-training. In comparison to the common self-supervised pre-training, the proposed methods are harder to scale up.\n\n- There is no comparison evaluation between the proposed method and prior methods in model pre-training. Only the results of the proposed method with different model architectures are reported. It will be difficult to appreciate the benefits of the proposed method.\n\n- The selected model architecture also raises questions since there are many popular model networks in medical imaging, e.g., 3D-UNet, ViT, etc. It will be helpful to see their performance compared to the vanilla ResNet. \n\n- Baselines without the pre-training process should also be reported.\n\n- The current setting utilizes public data for both pre-training and downstream applications. Having a separate evaluation dataset of a prognosis task will be helpful. \n\n- The proposed method is limited in technical innovation, though utilizing the time-to-event data as a form of supervision is relatively new in the pre-training. Mostly existing techniques are adopted for the pre-training.\n\n### Questions\n\nSee above\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a strategy for leveraging time-to-event information from electronic health records (EHR) combined with imaging data for pre-training purposes in prognostic tasks. Utilizing a public dataset that includes both 3D images and EHR notes, the authors aim to enhance downstream applications. While the manuscript is generally clear and accessible, it contains several significant shortcomings that impact its overall quality.\n\n**Major Comments:**  \n1. The generalization of the proposed method is limited, as it relies heavily on longitudinal time-to-event EHR data for pre-training supervision. This reliance makes scalability challenging compared to standard self-supervised pre-training methods.\n2. The manuscript lacks comparative evaluations of the proposed method against prior methods in model pre-training. Only results for the proposed method using different architectures are presented, making it difficult to assess its advantages.\n3. The choice of model architecture warrants further scrutiny, as many well-established networks, such as 3D-UNet and ViT, are not considered. A comparison of these models with the chosen ResNet would provide insight into performance differences.\n4. Baseline results without pre-training are not reported, which limits the manuscript's rigor.\n5. The use of publicly available data for both pre-training and downstream applications necessitates an additional separate evaluation dataset specifically for prognosis tasks to bolster findings.\n6. Although the concept of using time-to-event data for supervision is relatively novel, the overall technical innovation appears limited, relying primarily on existing techniques for pre-training.\n\n**Minor Comments:**  \n- The manuscript is well-structured, but minor revisions for clarity could enhance understanding.\n- Specific figures and results should include clearer labels and explanations for better comprehension.\n- Attention to typographical errors and proper notations would improve presentation.\n\n**Summary Paragraph:**  \nIn reviewing the manuscript against the TMI editorial criteria, the strengths lie in its innovative approach to integrating time-to-event data with pre-training tasks, enhancing significance in prognostic applications. However, weaknesses include limited innovation, the lack of comprehensive evaluations, and reliance on specific dataset requirements that hinder reproducibility. Addressing these issues would significantly improve the work's overall contribution.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a pre-training strategy that leverages time-to-event information from electronic health records (EHR) paired with 3D imaging data as supervisory signals. A public dataset containing both imaging and EHR data is used for pre-training and downstream analyses, while a separate dataset lacking time-event data is employed for adaptation evaluation. The paper is written clearly and is easy to follow. However, despite a promising motivation, the work has notable methodological and experimental limitations that reduce its technical and scientific impact.\n\n**Major Comments**  \n1. **Generalization and scalability:** The reliance on longitudinal time-to-event EHR data limits the generalization and scalability of the approach compared to standard self-supervised pre-training strategies that can utilize larger unlabeled datasets.  \n2. **Lack of comparative evaluation:** The manuscript provides no comparisons between the proposed pre-training approach and established pre-training methods. Results are restricted to varying model architectures of the proposed method, making it difficult to assess relative benefit.  \n3. **Choice of architecture:** The study employs a basic ResNet model, but more widely used imaging architectures (e.g., 3D U-Net, Vision Transformer) are not explored. Inclusion of these would help verify that improvements arise from the proposed strategy rather than architectural choice.  \n4. **Missing baseline:** Experiments omit a baseline without pre-training, which would clarify the added value of the proposed stage.  \n5. **Evaluation design:** Both pre-training and downstream experiments are conducted on public data from similar sources. Using an independent dataset for downstream prognosis evaluation would help establish external validity.  \n6. **Limited technical innovation:** Although the use of time-to-event data for supervision is somewhat novel, the overall approach mainly reuses existing techniques without substantial algorithmic innovation.\n\n**Minor Comments**  \n- The manuscript is well structured and readable.  \n- Figures and experimental settings are generally clear.  \n- No ethical issues are identified.\n\n**Summary Paragraph**  \nRegarding significance, the idea of integrating survival information into pre-training is interesting but constrained by dataset requirements. In terms of innovation, the method repurposes existing frameworks with modest novelty. The evaluation is incomplete due to missing comparisons and baselines, limiting assessment of performance gains. Reproducibility is reasonable given the use of public data, though broader validation is needed to confirm generalization.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a future-guided pretraining approach using time-to-event supervision to enhance the prognostic capabilities of 3D medical imaging models. By incorporating longitudinal EHR data into the pretraining process and predicting time-until-event, the model outperforms traditional methods across multiple standard tasks, as demonstrated by thorough experiments.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. Innovative Approach: The method creatively leverages EHR data following a medical scan to assist model pretraining, demonstrating better performance compared to imaging-only pretraining.\n2. Comprehensive Evaluation: Extensive comparisons across multiple tasks validate the robustness and efficiency of the TTE-based approach across different architectures.\n\n### Weaknesses\n\n1. Dependence on Large EHR Datasets: This approach relies on extensive, high-quality EHR data, which many medical datasets do not include.\n2. Limited Modality Scope: Tested only on CT images; broader modality testing could validate versatility across imaging types.\n3. Interpretability: The TTE pretraining’s impact on specific pixel-level biomarkers is less clear; additional analysis on feature attribution could help.\n\n### Questions\n\n1. Why start from 3D image scans instead of 2D medical images? Is this due to the dataset choice, or has similar work already been done on 2D data?\n2. How does the choice of time segmentation for EHR data affect model results during pretraining? Specifically, my understanding is that the model predicts the probability of a patient experiencing a certain event at intervals like 1, 2, or 3 years post-scan. How does the granularity of these time segments impact the performance of the pretrained encoder?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript proposes a future-guided pretraining methodology that utilizes time-to-event (TTE) supervision to improve the prognostic capabilities of 3D medical imaging models. By integrating longitudinal electronic health record (EHR) data into the pretraining framework and predicting time-until-event outcomes, the study demonstrates that the new model consistently outperforms traditional imaging-only approaches across various standard tasks, supported by thorough experimental validation.\n\n### Major Comments\n1. **Innovative Approach:** The methodology creatively utilizes EHR data collected post-medical scan to enhance model pretraining, yielding superior performance over conventional methods that rely solely on imaging.\n2. **Comprehensive Evaluation:** The research includes extensive task comparisons, which affirm the robustness and effectiveness of the TTE-based approach across different architectures.\n3. **Dependence on Large EHR Datasets:** The effectiveness of the proposed approach is heavily reliant on the availability of extensive and high-quality EHR data, which may not be accessible in numerous medical datasets.\n4. **Limited Modality Scope:** The study exclusively tests the approach on CT images; examining a broader range of modalities could further establish its versatility across different imaging types.\n5. **Interpretability Issues:** The effects of TTE pretraining on specific pixel-level biomarkers remain ambiguous; a deeper analysis surrounding feature attribution is recommended.\n\n### Minor Comments\n- Clarification needed on the rationale for starting with 3D scans instead of 2D medical images; is this related to dataset selection, or has prior research addressed 2D data?\n- Inquiry about how the segmentation of time intervals for EHR data influences the model’s pretraining outcomes; specifically, what effect does the granularity of these intervals have on the performance of the pretrained encoder?\n\n### Summary Paragraph\nThe manuscript presents significant contributions by introducing an innovative TTE-based pretraining approach that enhances prognostic capabilities using 3D medical imaging in conjunction with EHR data. While the evaluation is comprehensive and robust, the reliance on large EHR datasets and the limited scope of tested modalities present notable challenges. Additionally, interpretability remains a concern, indicating areas for improvement. Overall, the study shows strong potential in terms of innovation and significance, though its evaluation and reproducibility may be hindered by external data dependencies and lack of broad modality validation.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a future-guided pretraining framework that integrates time-to-event (TTE) supervision with longitudinal electronic health record (EHR) data to enhance prognostic modeling from 3D medical images. The model aims to improve representation learning by predicting time-until-outcome following imaging acquisition. Experimental results show consistent performance gains over conventional imaging-only pretraining methods across multiple benchmark tasks. The paper is clearly written and technically sound, though some methodological dependencies and interpretability aspects warrant further clarification.\n\n**Major Comments**  \n1. **Dependence on Large EHR Datasets:** The proposed approach requires substantial, well-curated EHR collections synchronized with imaging data. Such resources are not widely available, limiting the method’s applicability to institutions with large integrated datasets.  \n2. **Limited Modality Evaluation:** Experiments are restricted to CT imaging, leaving open the question of whether the approach generalizes to other modalities (e.g., MRI, X-ray). Demonstrating cross-modality performance would strengthen the paper’s claims of generality.  \n3. **Interpretability and Feature Attribution:** The link between TTE-based pretraining and specific imaging biomarkers remains insufficiently explained. More analysis on which spatial features contribute to improved prognostic performance would improve transparency and trustworthiness.  \n4. **Impact of EHR Time Segmentation:** Further explanation is needed regarding how the temporal granularity of EHR-based time segments (e.g., 1-, 2-, or 3-year intervals) influences model performance. This design choice may substantially affect the learned representations.\n\n**Minor Comments**  \n- Clarify the rationale for focusing on 3D imaging rather than 2D modalities—whether due to dataset availability or prior work in 2D settings.  \n- Ensure consistent terminology when referring to time-to-event intervals and survival probabilities for clarity.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study proposes an innovative fusion of imaging and longitudinal EHR data for representation learning, addressing an important problem in predictive medical imaging. It shows promising improvements (significance and innovation) supported by thorough experiments (evaluation). However, reproducibility may be constrained by data availability and incomplete methodological detail concerning time segmentation and feature interpretation.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The work is innovative and well-executed but requires additional justification on data requirements, temporal design choices, and modality generalization before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces time-to-event (TTE) pretraining for 3D medical imaging models to address the \"missing context problem\" in current self-supervised learning approaches. The authors argue that existing methods fail to capture prognostic biomarkers because they rely on supervision from narrow temporal windows around images, missing long-term disease progression patterns. Their approach leverages longitudinal electronic health records (EHRs) to create 8,192 TTE pretraining tasks from medical codes, using a piecewise exponential neural network (PEANN) for survival modeling. Experiments on 18,945 CT scans from the INSPECT dataset show average improvements of 23.7% in AUROC and 29.4% in Harrell's C-index across 8 benchmark tasks for outcome prediction, without degrading diagnostic classification performance on RSPECT dataset tasks.\n\n## Weaknesses\n\n• **Limited architectural diversity and computational requirements**: The study evaluates only three model architectures and acknowledges significant computational constraints that may limit reproducibility and broader adoption.\n  - SwinUNETR requires 80GB memory GPUs while DenseNet/ResNet need 40GB memory GPUs (Page 10, Limitations section), creating substantial barriers for many research groups\n  - Training costs range from 864 to 1,440 GPU hours per architecture (Table 11, Page 22), representing significant computational expense\n  - The choice of inflating 2D ImageNet weights for 3D adaptation (Page 7, Architectures section) may not be optimal compared to native 3D pretraining approaches\n\n• **Mathematical formulation clarity and derivation gaps**: The piecewise exponential survival modeling contains notation inconsistencies and incomplete mathematical exposition.\n  - Equation 1 uses inconsistent notation where indicator function I(t ≥ Sp) appears but the relationship between time pieces and survival calculation lacks clear explanation\n  - The derivation jumps from Equation 2 to the final loss function without showing intermediate steps, with full derivation relegated to Appendix M (Page 5)\n  - The relationship between the 8 time bins and piecewise intervals is not clearly defined in the main text, creating ambiguity about temporal discretization\n\n• **Evaluation methodology limitations and baseline comparisons**: The experimental design contains several limitations that may overstate the benefits of the proposed approach.\n  - The base/visit baseline uses the same 8,192 tasks but restricts labels to concurrent visits (Page 7), which may not represent the strongest possible baseline for temporal supervision\n  - Statistical significance testing relies solely on bootstrap confidence intervals with Z-tests (Page 8) without correction for multiple comparisons across numerous tasks and metrics\n  - The frozen encoder evaluation strategy (Page 6) may not reflect real-world deployment scenarios where end-to-end fine-tuning is often preferred\n\n• **Dataset scope and generalizability concerns**: The study's focus on a single imaging modality and specific clinical context limits broader applicability.\n  - Experiments are restricted to chest CT scans from pulmonary embolism patients (Tables 1 and 9), limiting generalizability to other anatomical regions and pathologies\n  - The INSPECT dataset, while substantial, represents a relatively narrow clinical population compared to the diversity needed for foundation model evaluation\n  - Cross-dataset evaluation is limited, with RSPECT used only for diagnostic tasks without longitudinal outcomes (Page 7), providing insufficient validation of the approach's broader utility\n\n## Suggestions for Improvement\n\n• **Expand architectural evaluation and provide computational efficiency analysis**: Include more diverse 3D architectures and analyze computational trade-offs to improve accessibility and adoption.\n  - Evaluate additional native 3D architectures beyond the three tested models to demonstrate broader applicability\n  - Provide detailed analysis of computational efficiency trade-offs, including memory usage patterns and training time scaling\n  - Compare inflated 2D weights against native 3D pretraining approaches to justify the architectural choices\n\n• **Clarify mathematical formulations and provide complete derivations**: Improve mathematical exposition to enhance reproducibility and theoretical understanding.\n  - Provide complete step-by-step derivation of the loss function in the main text, not just in appendices\n  - Clearly define the relationship between time bins, piecewise intervals, and the survival function parameters\n  - Ensure consistent notation throughout equations and provide clear definitions for all mathematical symbols\n\n• **Strengthen experimental design with robust baseline comparisons**: Enhance the evaluation methodology to provide more convincing evidence of the approach's benefits.\n  - Include stronger temporal baselines that utilize future information without the full TTE framework to better isolate the contribution of survival modeling\n  - Apply appropriate multiple comparison corrections to statistical tests given the large number of tasks and metrics evaluated\n  - Provide both frozen encoder and end-to-end fine-tuning results to demonstrate versatility across different deployment scenarios\n\n• **Demonstrate broader applicability across modalities and populations**: Expand the evaluation scope to strengthen claims about the general utility of TTE pretraining.\n  - Include additional imaging modalities (MRI, X-ray) and anatomical regions beyond chest CT to demonstrate broader applicability\n  - Evaluate on diverse patient populations and clinical contexts beyond pulmonary embolism to show generalizability\n  - Provide more comprehensive cross-dataset validation with datasets that include both diagnostic and prognostic tasks with longitudinal outcomes",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThe manuscript presents a novel approach to time-to-event (TTE) pretraining for 3D medical imaging models aimed at addressing the \"missing context problem\" in self-supervised learning. The authors claim that traditional methods fall short in capturing prognostic biomarkers due to their reliance on limited temporal supervision. By utilizing longitudinal electronic health records (EHRs), they create a significant number of TTE pretraining tasks and employ a piecewise exponential neural network for survival modeling. Results from experiments on the INSPECT dataset demonstrate notable improvements in outcome prediction metrics while maintaining classification performance.\n\n## Major Comments\n1. **Limited Architectural Diversity and Computational Requirements**: \n   - The evaluation focuses on only three architectures; substantial computational requirements (e.g., SwinUNETR needing 80GB GPU memory) create barriers for reproducibility.\n   - GPU training costs range from 864 to 1,440 hours per architecture, implying high computational expenses.\n   - Using inflated 2D ImageNet weights for 3D adaptation may not be as effective as direct native 3D pretraining methods.\n\n2. **Mathematical Formulation Clarity and Derivation Gaps**: \n   - Inconsistencies in notation and incomplete exposition in the piecewise exponential survival modeling undermine clarity.\n   - The transition from Equation 2 to the final loss function lacks intermediate derivations; detailed derivation is only available in Appendix M.\n   - The definition of time bins and piecewise intervals is ambiguous in the main text, complicating understanding of temporal discretization.\n\n3. **Evaluation Methodology Limitations and Baseline Comparisons**: \n   - The baseline methodology may overstate the advantages of the proposed approach due to reliance on concurrent visit labels.\n   - Statistical significance is assessed using bootstrap intervals without corrections for multiple comparisons.\n   - The evaluation strategy may not represent real-world applications where end-to-end fine-tuning is preferred.\n\n4. **Dataset Scope and Generalizability Concerns**: \n   - The focus on chest CT scans from pulmonary embolism patients limits generalizability across other pathologies and anatomical regions.\n   - The narrow clinical population in the INSPECT dataset restricts evaluation diversity essential for foundation models.\n   - Validation is insufficient as RSPECT is solely used for diagnostics without incorporating longitudinal outcomes.\n\n## Minor Comments\n- Clarify mathematical symbols and ensure consistent notation throughout the equations.\n- Improve visibility of detailed mathematical derivations.\n- Provide references or context for limited evaluation metrics and methodologies.\n\n## Summary Paragraph\nThis manuscript contributes to the field with its innovative TTE pretraining method, which appears significant in enhancing outcome prediction in medical imaging. However, limitations in architectural diversity and evaluation methodology raise concerns about reproducibility and generalizability. The theoretical claims suffer from mathematical clarity issues, potentially hindering the overall understanding of the proposed model. Overall, while the study shows promise in certain metrics, it requires improvement in methodology and broader applicability to satisfy the journal's editorial criteria.\n\n## Decision Recommendation\nMajor revision required.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a time-to-event (TTE) pretraining framework for 3D medical imaging models to address the “missing context problem” in self-supervised learning. By leveraging longitudinal electronic health records, the authors define 8,192 TTE pretraining tasks and apply a piecewise exponential neural network (PEANN) for survival modeling. Experiments on 18,945 CT scans from the INSPECT dataset demonstrate substantial improvements in prognostic performance metrics (average 23.7% AUROC and 29.4% C-index gains) without degrading diagnostic accuracy on the RSPECT benchmark. The paper is clearly written and presents a potentially impactful approach to bridging imaging and temporal clinical data.\n\n---\n\n**Major Comments**  \n1. **Architectural diversity and computational requirements** – Only three architectures are evaluated, all demanding substantial GPU memory (up to 80 GB for SwinUNETR), creating barriers to reproducibility and accessibility. Training costs of 864–1,440 GPU hours per model are high, and reliance on inflated 2D ImageNet weights may limit the advantages of true 3D pretraining.  \n2. **Mathematical formulation clarity** – The presentation of the piecewise exponential survival model contains inconsistencies. Equation 1 introduces an indicator function \\(I(t ≥ S_p)\\) without sufficient explanation of time-piece relationships, and the derivation from Equation 2 to the loss function lacks intermediate steps, relegated to an appendix. The link between time bins and piecewise intervals requires clarification.  \n3. **Evaluation methodology** – Baseline design may inflate the observed benefits: the “base/visit” baseline uses identical tasks restricted to concurrent visits, statistical testing relies only on uncorrected bootstrap Z‑tests, and the frozen encoder protocol may not represent end‑to‑end deployment scenarios.  \n4. **Dataset scope and generalizability** – All experiments involve chest CT studies of pulmonary embolism, limiting extension to other organs, diseases, or imaging modalities. Cross‑dataset evaluation is minimal and lacks prognostic validation beyond INSPECT.\n\n---\n\n**Minor Comments**  \n- Improve consistency of mathematical notation and define all symbols in the main text.  \n- Provide more explicit descriptions of time‑bin definitions and their effect on discretization.  \n- Consider summarizing computational costs and memory demands clearly in one table.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The manuscript tackles an important gap by linking longitudinal EHR data to 3D imaging models, potentially enhancing prognostic modeling.  \n**Innovation:** The TTE pretraining concept using survival objectives is novel, though limited architectural exploration constrains its demonstrated breadth.  \n**Evaluation:** While gains on the INSPECT dataset are notable, stronger baseline selection, statistical rigor, and inclusion of end‑to‑end fine‑tuning would strengthen claims.  \n**Reproducibility:** Heavy computational demands and incomplete mathematical exposition reduce reproducibility, though code or additional documentation could mitigate this.\n\n---\n\n**Decision Recommendation:** **Major Revision** – Promising and innovative work, but revisions are needed to clarify methodology, broaden evaluation, and improve reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** ✅  \nThe manuscript proposes a new large‑scale *time‑to‑event (TTE) pretraining* method for 3‑D medical image encoders that couples CT scans with longitudinal EHR data.  The core advance is methodological—defining a survival‑analysis–based pretraining objective and integrating right‑censoring mechanisms into a 3‑D imaging foundation‑model pipeline—squarely within IEEE TMI’s focus on imaging methodology and quantitative modeling.\n\n2. **Novelty & Contribution Level**  \nThe idea of using survival‑analysis objectives for pretraining medical image encoders is, to my knowledge, unexplored at this scale.  Prior work has examined small 2‑D or single‑task survival models, or multimodal supervision limited to contemporary EHR or text reports.  The presented TTE framework generalizes this to thousands of tasks and directly encodes future risk distributions.  Novelty is therefore high and methodologically significant.\n\n3. **Technical and Experimental Rigor**  \nThe paper is technically sound and mathematically coherent.  The piece‑wise exponential loss is derived explicitly; ablations on task selection, metrics (AUROC, C‑index, IBS, Ctd), and model types are extensive.  Experiments use two public datasets (INSPECT, RSPECT) with clearly described preprocessing, bootstrapped confidence intervals, and statistical testing.  One caveat is limited modality breadth (only CT) and the computational demands that may restrict reproducibility for most groups.  Otherwise rigor is strong.\n\n4. **Clarity and Presentation**  \nWriting is clear, figures legible, and mathematical notation consistent with survival‑analysis literature. Minor stylistic tightening would help (occasional long paragraphs and overlapping figure captions).\n\n5. **Ethical and Reproducibility Compliance**  \nData de‑identified and public; IRB‑exempt status is justified. Code and pretrained models are released; HIPAA‑compliant storage and subgroup analyses for bias are described. Compliance satisfactory.\n\n---\n\n**Phase 2 IEEE TMI Review Report**\n\n### 1. Summary\nThe authors introduce *Time‑to‑Event Pretraining*—a survival‑learning objective for 3‑D medical image encoders that couples CT scans with longitudinal electronic‑health‑record timelines. By converting millions of temporally stamped EHR codes into 8 192 TTE tasks, the method learns to predict not only occurrence but also timing of future clinical events. Pretraining employs a piece‑wise exponential neural network loss to handle censorship. Across multiple backbones (SwinUNETR, DenseNet‑121, ResNet‑152), TTE‑pretrained models show 20–30 % improvements in AUROC and C‑index for prognostic outcomes without loss of diagnostic accuracy.\n\n### 2. Strengths\n- Genuinely methodological innovation: first demonstration of survival‑based large‑scale pretraining for 3‑D imaging.  \n- Solid theoretical grounding in survival analysis with explicit censoring treatment.  \n- Well‑controlled experimental comparisons (base, visit‑limited, multitask controls).  \n- Significant measured gains for prognostic prediction and calibration.  \n- Open‑source code/models and ethical transparency.  \n- Potential high impact on development of prognostic imaging foundation models.\n\n### 3. Weaknesses\n- Scope of data limited to chest CT; unclear generalization to other modalities or institutions.  \n- Compute requirements (up to 1.4 k GPU h) may limit community adoption despite code release.  \n- Method description, while thorough, could benefit from clearer pseudocode for task generation and survival‑loss computation.  \n- No theoretical comparison with alternative censored‑data losses (e.g., DeepSurv pretraining objective).  \n- Clinical interpretability of learned “prognostic pixel biomarkers” not deeply explored beyond Grad‑CAM visualizations.\n\n### 4. Major Comments\n1. **Generalizability** – Please discuss or test whether performance holds for non‑thoracic modalities (MR, abdominal CT) or cross‑institutional CT data. This would strengthen the claim of a broadly applicable pretraining strategy.  \n2. **Survival‑loss sensitivity** – Include an ablation comparing piece‑wise exponential vs. Cox PH vs. DeepSurv loss during pretraining to quantify modeling‑choice effects.  \n3. **Interpretability/clinical insight** – Provide quantitative or qualitative evaluation showing that highlighted regions correspond to plausible prognostic biomarkers to justify the “biomarker discovery” claims.  \n4. **Computational accessibility** – Add commentary or lighter‑weight recipe so other groups can reproduce results on modest resources (e.g., scale‑down experiment).  \n5. **Clarity of task‑selection algorithm** – The entropy‑ranked vertex‑cover procedure is briefly referenced; include concise pseudocode or visual explaining how ontology structure and conditional entropy determine the 8 192 tasks.\n\n### 5. Minor Comments\n- Unify figure numbering and captions; some appendices repeat figure legends.  \n- Polish English (remove redundant phrases such as “This choice also increase…”).  \n- Provide consistent significant‑figure reporting in quantitative tables.  \n- Add dataset license details and checksum for pretrained weights.  \n\n### 6. Overall Recommendation\n**Major Revision.**  \nThe paper demonstrates an original and promising methodological advance, clearly relevant to TMI. With additional clarification on generalizability, computational accessibility, and interpretability, it could reach IEEE TMI’s methodological standard.\n\n### 7. Confidence Level\n**4 / 5** – High confidence based on familiarity with imaging deep learning and survival modeling literature.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a large‑scale *time‑to‑event (TTE) pretraining* framework for 3‑D medical image encoders that integrates chest CT imaging with longitudinal electronic‑health‑record data. The approach formulates a survival‑analysis–based pretraining objective with explicit handling of right‑censored data using a piece‑wise exponential loss. The work is methodologically focused and aligns well with IEEE TMI’s emphasis on quantitative image modeling. The paper is clearly written and technically detailed, though further clarification and validation are needed to support its generalizability and reproducibility.\n\n---\n\n**Major Comments**  \n1. **Generalizability** – The current evaluation is restricted to thoracic CT data. Additional analysis on other modalities (e.g., MR, abdominal CT) or multi‑institutional datasets would substantiate the claim of a broadly applicable pretraining paradigm.  \n2. **Loss‑Function Comparison** – A direct ablation comparing the piece‑wise exponential objective against alternatives such as Cox PH or DeepSurv pretraining would clarify the benefits and limitations of the chosen formulation.  \n3. **Interpretability and Clinical Insight** – More evidence is needed demonstrating that regions identified by Grad‑CAM or related tools correspond to plausible prognostic biomarkers to strengthen the “biomarker discovery” claim.  \n4. **Computational Accessibility** – As the reported training demands are high (≈1.4 k GPU hours), include discussion of reduced‑scale experiments or practical guidelines to facilitate replication by groups with limited resources.  \n5. **Task‑Selection Description** – The entropy‑ranked vertex‑cover procedure for defining the 8 192 tasks should be more transparently described through concise pseudocode or a schematic clarifying how ontology structure and conditional entropy are used.\n\n---\n\n**Minor Comments**  \n- Ensure consistent figure numbering and captioning; avoid repetition across appendices.  \n- Tighten prose to remove redundant phrasing (e.g., “This choice also increase…”).  \n- Report significant figures consistently in quantitative tables.  \n- Specify data‑set license information and include file checksums for pretrained models.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe study is **significant** in framing survival‑based pretraining as a scalable foundation‑model objective for medical imaging. Its **innovation** is clear—introducing a novel survival‑learning loss integrated with CT–EHR data. The **evaluation** is generally thorough with robust metrics, confidence intervals, and ablations, though its modality scope is narrow. **Reproducibility** is supported by open‑sourced code and ethical compliance, yet the heavy computational load may hinder full replication by others. Overall, the work is technically strong and promising but requires expanded validation and clarity before meeting the journal’s publication standards.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel large-scale *time-to-event (TTE) pretraining* method for 3-D medical image encoders that integrates CT scans with longitudinal electronic health record (EHR) data. The primary methodological advancement involves defining a survival-analysis–based pretraining objective and incorporating right-censoring mechanisms into a 3-D imaging foundation model pipeline. This aligns well with the IEEE TMI's focus on imaging methodology and quantitative modeling.\n\n### Major Comments\n1. **Generalizability**: Discuss or test whether performance is applicable to non-thoracic modalities such as MR or abdominal CT, or across different institutions to strengthen claims of a widely applicable pretraining strategy.\n2. **Survival-loss sensitivity**: Include an ablation study comparing piece-wise exponential loss with Cox PH and DeepSurv losses during pretraining to evaluate the effects of modeling choices.\n3. **Interpretability/clinical insight**: Provide qualitative or quantitative evaluations to demonstrate that regions highlighted correspond to plausible prognostic biomarkers to support claims of biomarker discovery.\n4. **Computational accessibility**: Optionally include a commentary or simplified recipe enabling other groups to reproduce results using modest computational resources (e.g., scaled-down experiments).\n5. **Clarity of task-selection algorithm**: Elaborate on the entropy-ranked vertex-cover procedure by providing pseudocode or a visualization to clarify how the ontology structure and conditional entropy determine the 8,192 tasks.\n\n### Minor Comments\n- Harmonize figure numbering and captions, as some appendices repeat figure legends.\n- Refine language to eliminate redundancy, such as the phrase “This choice also increases...”.\n- Standardize significant-figure reporting in quantitative tables.\n- Add details on dataset licensing and checksums for pretrained weights.\n\n### Summary Paragraph\nThe manuscript offers a significant methodological contribution through its *Time-to-Event Pretraining* approach for 3-D medical image encoders, showcasing innovation in using survival-learning objectives at scale. While the evaluations indicate strong performance improvements and methodological rigor, the limitations regarding the generalizability of data modalities and high computational requirements could hinder reproducibility and community adoption. The findings align with TMI's criteria for significance and innovation, but effective evaluation and reproducibility aspects need further strengthening.\n\n### Decision Recommendation\n**Major Revision**. The study presents an original and promising methodological advancement within its scope. With further attention to generalizability, computational accessibility, and interpretability, it could meet IEEE TMI's methodological standards. \n\n### Confidence Level\n**4 / 5** – High confidence based on expertise in imaging deep learning and survival modeling literature.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Alejandro Lozano",
      "Curtis Langlotz",
      "Ethan Steinberg",
      "Jason Alan Fries",
      "Jeya Maria Jose Valanarasu",
      "Louis Blankemeier",
      "Nigam Shah",
      "Akshay S Chaudhari",
      "Zepeng Frazier Huo"
    ],
    "url": "pdfs/iclr.cc-2025-conference_ab67303155dc14cfbb1febaf43b5bf38ae9bd5b0.pdf",
    "remote_url": "https://openreview.net/pdf/ab67303155dc14cfbb1febaf43b5bf38ae9bd5b0.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Justin"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "Multimodal Large Language Model",
      "Biomedicine",
      "Region-Text"
    ],
    "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. \nMost current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence.\nTo mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans.\nTo achieve it, we first formulate \\textbf{Region-Centric tasks} and construct a \\textbf{large-scale dataset, MedRegInstruct,} to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a \\textbf{Region-Aware medical MLLM, MedRegA}, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. The codes and model will be made publicly available.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a new large-scale, multi-site, medical visual-question answering (VQA) dataset, called MedRegInstruct, with fine-grained region-specific annotation and multiple corresponding region-centric tasks. The proposed dataset contains multiple region-centric evaluation tasks including, region-to-text generation, text-to-region detection, and grounded medical report generation. The paper further proposed a new region-aware multi-modal large language model (MLLM), called, MedRegA, pre-trained with the proposed dataset and regional chain of thought (CoT). The proposed method outperforms multiple existing SOTA medical VQA models on the regional-centric evaluation.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n1. The proposed new region-centric medical dataset MedRegInstruct addressed the need for fine-grained, region-level, training and evaluation datasets in the medical domain. It is novel to the field and will help the development of the field as provides a more fine-grained training and evaluation annotation. It is proven that the method pre-trained with the new dataset can demonstrate more robust performance in the targeted region-specific tasks and also other related medical tasks. \n2. The paper has provided enough details about the data curation and evaluation, including the process of question-answering pair generation from medical data with region annotation, a prompt for each downstream evaluation, and detailed settings for each task. The author also seems to claim they will release the code and pre-trained model later.\n3. The paper is overall well written with nice figures and a clean presentation. It is easy to follow even if there are many details in it.\n\n### Weaknesses\n\n1. While the proposed dataset is novel and important to the field, the corresponding MedRegA model is not that novel, though I hate to say that, in comparison. The overall model design is basically the same as the base InternVL model and the major change is the training scheme and prompt formulation with the new dataset, which can be extended to other medical VQL models as well. While the author further proposed single-step regional CoT in the paper, it is still relatively straightforward. **Yet**, this is not a major drawback as the performance of this model demonstrates the superiority of training with the proposed dataset.\n2. Similarly, it seems that the baselines used in the evaluation are not all fine-tuned with the proposed dataset, which enables the model to understand regional information. Comparing the model trained on the proposed dataset with baselines that are not trained on this dataset is not very fair, to some extent. It would be interesting to see how well each baseline would improve if they were fine-tuned on the proposed dataset, even if just in a few-shot manner.\n3. Another concern about this paper is the quality of the data. Most of the regional bounding box-text pairs in the dataset were taken from the existing dataset and generated from an automated rule-based system and an LLM. The evaluation of the data quality is missing in this paper, which is very critical as the main purpose is **fine-grained regional evaluation,** where the quality of the bounding box and the correctness of the corresponding text description are very important. It would be better if some sort of data quality check could be done before releasing it to the public.\n\n### Questions\n\nOverall, this is a good paper with solid contributions and it has a significant meaning to the field. Still, the reviewer has a few questions here.\n\n1. How is the English version of the text report/caption generated for the in-house Chinese data? What kind of translation was used here?\n2. According to the “Structure Detection” section on page 5, the structure bounding box in the dataset was generated with a fine-tuned MLLM on the existing Region-Text dataset given the corresponding organ prompt. The reviewer wonders if any kind of evaluation was done on this fine-tuned MLLM, how accurate is the extract bounding box? And why not train a more straightforward detection model for this purpose? Considering that the specific detection model usually performs better than a general MLLM.\n3. Also, as mentioned above, the reviewer wonders how much will the other baseline medical MLLMs improve if they were trained with the proposed dataset. The comparison between InternVL and MedRegA in the paper is not very fair as the InternVL was only trained with natural image-text pairs. The reviewer understands it may take tons of extra time/money to conduct such an evaluation, but it would still be an interesting question to explore.\n4. According to the abstract, the code and model will be released to the public, but the reviewer wonders if the data will be released as well. This is pretty critical as the dataset is the major contribution of the paper. The answer to this question will influence the reviewer's final score for the paper.\n\n### Flag For Ethics Review\n\n- Yes, Privacy, security and safety\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe paper proposed a new medical visual question-answering dataset with in-house medical imaging data from a Chinese hospital. Though the author has an ethics statement section in the paper, further ethics evaluation may be needed to ensure there is no critical ethical issue.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript introduces a new large-scale, multi-site medical visual-question answering (VQA) dataset called MedRegInstruct, which features fine-grained region-specific annotations and several region-centric tasks, including region-to-text generation, text-to-region detection, and grounded medical report generation. Additionally, the authors present a region-aware multi-modal large language model (MLLM) named MedRegA, which is pre-trained on the MedRegInstruct dataset and a regional chain of thought (CoT). The proposed method shows superior performance compared to existing state-of-the-art medical VQA models in region-centric evaluations.\n\n### Major Comments\n1. **Model Novelty:** While the MedRegInstruct dataset is a crucial advancement for the field, the MedRegA model lacks novelty as its design closely resembles the base InternVL model, primarily differing in training scheme and prompt formulation. The introduction of a single-step regional CoT, though an improvement, remains straightforward.\n2. **Baseline Evaluation:** The evaluation comparisons may not be entirely fair, as not all baselines were fine-tuned on the proposed dataset that enables regional information comprehension. Investigating their performance after fine-tuning, even in a few-shot manner, would be valuable.\n3. **Data Quality Concerns:** A significant concern arises regarding data quality since the regional bounding box-text pairs were sourced from existing datasets and generated via an automated rule-based system and an LLM. A robust data quality evaluation is necessary before the dataset release, particularly given the emphasis on fine-grained regional evaluation.\n\n### Minor Comments\n- The paper is well-written with effective figures and clear presentation.\n- Clarification on the translation process used for the English captions generated from the in-house Chinese data would enhance understanding.\n\n### Summary Paragraph\nThe manuscript holds substantial significance with the introduction of the MedRegInstruct dataset, which innovatively focuses on region-specific medical VQA tasks. However, concerns exist regarding the novelty of the MedRegA model and the fairness of the evaluation against baseline models not trained on this dataset. Additionally, the lack of a data quality assessment poses a potential risk to the reproducibility of results derived from the dataset. Therefore, while the contributions are valuable, addressing these weaknesses is essential for the work's impact.\n\n### Decision Recommendation\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegInstruct*, a large-scale, multi-institutional medical visual question answering (VQA) dataset with fine-grained region-level annotations and multiple related region-centric tasks, including region-to-text generation, text-to-region detection, and grounded report generation. It also presents *MedRegA*, a region-aware multimodal large language model (MLLM) pre-trained on this dataset using a regional chain-of-thought mechanism. The paper is clearly written, well-structured, and easy to follow, with informative figures and a clean presentation. Overall, the work significantly contributes to the expansion of fine-grained, region-level medical VQA data and models.\n\n**Major Comments**  \n1. **Novelty of the Model:** While the dataset contribution is strong, the novelty of the proposed MedRegA model is limited. Its architecture largely follows the existing InternVL model, with the principal difference being the training scheme and regional prompts. The single-step regional chain-of-thought extension is conceptually straightforward. Although these modifications demonstrate improved performance, methodological innovation is modest.  \n2. **Fairness of Comparisons:** The evaluation may not be fully fair, as baselines were not fine-tuned on the proposed dataset. Since MedRegA benefits from this dataset’s regional annotations, the comparison potentially favors the new model. Additional experiments fine-tuning other baselines—even in a few-shot setting—would provide fairer comparisons and better contextualize gains.  \n3. **Data Quality Assessment:** The dataset combines bounding box–text pairs extracted from existing datasets and those produced via automated rule-based and LLM-based systems. However, no quantitative or qualitative evaluation of data quality is provided. Given the paper’s emphasis on fine-grained regional tasks, verifying annotation quality (bounding box accuracy and text correctness) is essential.  \n4. **Dataset Release and Translation Details:** Clarification is needed on whether the dataset will be publicly released, as this is central to the paper’s impact. Additional details are also requested on how English captions were generated from Chinese data (e.g., translation methods).  \n5. **Region Detection Method:** The “Structure Detection” step reportedly uses a fine-tuned MLLM rather than a dedicated detection model. An evaluation of this step’s accuracy or a rationale for not using a specialized detector would strengthen the technical soundness.\n\n**Minor Comments**  \n- Please clarify if ethical approval and patient data anonymization procedures are confirmed for the in-house medical data.  \n- A few notational clarifications and implementation details (e.g., prompt formats) would improve reproducibility.  \n- Ensure the paper explicitly states future release plans for data, code, and models.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \n*Significance:* The proposed dataset fills an important gap in region-level medical VQA, enabling fine-grained reasoning.  \n*Innovation:* The dataset itself is the primary innovation; the model design is comparatively incremental.  \n*Evaluation:* Experimental results show strong performance, though fairness concerns remain regarding non-fine-tuned baselines and lack of data quality assessment.  \n*Reproducibility:* The methodology and experiments are described in detail, and prospective data/code release would enhance reproducibility. Ethical considerations regarding medical data warrant additional verification.\n\n**Decision Recommendation:** **Major Revision** – Strong contribution through dataset introduction, but concerns about model novelty, fairness of evaluation, dataset quality validation, and the transparency of data release need to be addressed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this work, the authors tackle the current problem in MLLMs (multimodal large language models) of how they don't necessarily \"focus\" on particular regions, but rather take the entire image as context and then have to solve a variety of downstream tasks. To address this problem they offer two main sets of contributions. The first, is the formulation of several region specific tasks, namely region-to-text identification, text-to-region detection, and grounded report generation, which they combine into a dataset called MedRegInstruct. The second is the training strategy and modeling paradigm for a MLLM they propose, which they refer to as MedRegA. MedRegA is a bilingual, as it was trained on both English and Chinese paired datapoints, model that they apply to this larger dataset they prepare in comparison to currently available medical MLLMs.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n• The paper really introduces another method of interaction, in the bounding boxes, that can be quite useful as MedRegA is able to reason about specific regions. This is an immensely useful tool for narrowing of problems as opposed to current trained MLLMs have to do.\n\n• The proposed training strategy for learning how to incorporate the bounding boxes is simple and scalable, seems like it could be generally applied to other strategies that want to apply bounding boxes.\n\n• Regional CoT makes a lot of sense intuitively and provides a nice bias for solving region specific problems.\n\n• MedRegA has very strong performance on the chosen set of benchmarks and metrics in comparison to existing state of the art methods.\n\n### Weaknesses\n\n• The section of the report grounding dataset is quite confusing as to how its constructed, and how much it relies on the models it uses for automation for providing correct labels/groundtruth. For example, is InternLM perfect at sub-select different parts of the reports? \n\n• The paper claims to work across a broad set of modalities, but when considering the grounding dataset it seems that it only considers, as other MLLMs do, Chest Xrays?\n\n• It's unclear if the comparison to existing baselines are exactly fair as the trained MedRegA has access to more data. In fact, compared to the baseline of InternLM, it should be strictly better because MedRegA is trained with InternLM as an initialization. While it's true that the baselines don't provide a mechanism for highlighting particular regions, they still should be adapted to the new data that the authors are evaluating on.\n\n### Questions\n\n• What was the filtering criterion for reducing SA-Med2D-20M?\n\n• Is there a noticeable performance difference between English and Chinese tasks? Is there paired data that this can be evaluated on? • • • \n\n• Overall, it is unclear from the paper why exactly these two particular languages are used, other than being able to source data for both, rather than just arbitrary languages with medical report data.\n\n• Is the MedRegInstruct dataset planning to be release?\nFor the Region-Text dataset, why is the case that there needs to be this split into half for region-to-text and half for text-to-region? Can't every datapoint server the purpose of being either during training?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript addresses the challenge faced by multimodal large language models (MLLMs) regarding their lack of focus on specific regions within images, which limits their effectiveness in performing certain downstream tasks. The authors contribute a solution through the development of region-specific tasks, encapsulating these in a dataset called MedRegInstruct. Additionally, they propose a new MLLM model, MedRegA, trained on both English and Chinese data, which they evaluate against existing medical MLLMs.\n\n### Major Comments\n1. The formulation of tasks such as region-to-text identification, text-to-region detection, and grounded report generation is innovative and potentially enhances the functionality of MLLMs in medical contexts.\n2. However, the construction of the report grounding dataset lacks clarity, particularly regarding its reliance on automation for accurate labeling, raising questions about the effectiveness of the model used for labeling.\n3. The claim of multi-modality in the manuscript is questionable, as the grounding dataset appears limited to chest X-rays, similar to other MLLMs.\n4. The fairness of baseline comparisons is uncertain; MedRegA seems to have advantages due to its training data access, which raises concerns about the validity of performance claims relative to existing models like InternLM.\n\n### Minor Comments\n- Clarification is needed regarding the filtering criteria for SA-Med2D-20M.\n- Is there a notable performance difference between the tasks conducted in English and Chinese, and is there paired data to reflect this?\n- The rationale behind selecting English and Chinese for data sourcing is not adequately justified.\n- Clarification on whether the MedRegInstruct dataset will be released is necessary.\n- The decision to split the dataset into half for region-to-text and half for text-to-region tasks warrants further explanation; can’t each data point serve both purposes in training?\n\n### Summary Paragraph\nIn terms of significance, the manuscript presents a novel approach to incorporating bounding boxes in MLLMs, which enhances the focus on specific regions and offers a scalable training strategy applicable to other efforts. While its innovation lies in the proposed tasks and modeling approach, concerns remain regarding the clarity of the dataset construction and the fairness of comparative evaluations. The reproducibility of findings is also questioned given the chosen modalities and the lack of detailed performance analysis across languages. Overall, the manuscript shows promise but requires critical improvements in clarity and methodological robustness.\n\n### Decision Recommendation\nMajor revision is recommended to address the outlined concerns and enhance the overall quality and transparency of the research.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegA*, a bilingual multimodal large language model (MLLM) designed to improve region-level reasoning in medical imaging. To address the limitation of current MLLMs that process entire images globally, the authors propose region-specific tasks—region-to-text identification, text-to-region detection, and grounded report generation—compiled into a new dataset named *MedRegInstruct*. They further describe a training paradigm integrating bounding box information and a “Regional Chain of Thought” (CoT) mechanism. Overall, the paper is clearly written and presents a technically sound approach with strong empirical results on several benchmarks.\n\n**Major Comments**  \n1. **Dataset Construction Clarity:** The description of the report grounding dataset is confusing. Details about its construction, the extent of automation, and the accuracy of models (e.g., InternLM) used to generate labels are not sufficiently explained.  \n2. **Scope of Modalities:** Although the paper claims multimodal generality, it appears that only chest X-rays are considered for grounding. The extent of modality coverage should be clarified.  \n3. **Fairness of Comparisons:** The fairness of comparisons with baselines is uncertain. MedRegA benefits from more data and is initialized from InternLM, which may bias results. It is unclear whether baseline models were adapted to the same data for a fair evaluation.  \n4. **Language Choices:** The motivation for selecting English and Chinese as training languages requires elaboration beyond data availability. Clarify whether paired bilingual datasets are used and if performance varies across languages.  \n5. **Dataset Release and Design Decisions:** Specify whether *MedRegInstruct* will be publicly released. Additionally, explain the rationale for dividing region-text pairs into separate region-to-text and text-to-region subsets instead of allowing bidirectional training from all examples.\n\n**Minor Comments**  \n- Provide more detail on the filtering criterion used to reduce the SA-Med2D-20M dataset.  \n- Minor editorial suggestions include improving clarity in the dataset section and explicitly stating figure details and dataset statistics.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript is significant for introducing a novel way to incorporate spatial localization into MLLMs through bounding box–based interaction and Regional CoT reasoning. Its innovation lies in extending multimodal comprehension to region-focused learning with a straightforward, scalable training scheme. Evaluation results are strong, though the data and initialization advantages limit fairness in baseline comparisons. Reproducibility is promising given the model and dataset structure, but public release details remain uncertain.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nWhile the contribution is innovative and performance promising, issues regarding dataset construction, language rationale, and fairness of comparisons must be more clearly addressed before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work introduces a novel region-centric task and presents a large-scale Chinese medical dataset named MedRegInstruct. To address this new task, the authors developed a bilingual multimodal large language model, MedRegA, which outperforms baseline methods across most evaluation metrics.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.\tThe paper is well-organized.\n2.\tThis work introduces a novel task alongside a large-scale medical dataset.\n3.\tThe authors propose a new model that significantly outperforms other models across the majority of datasets and evaluation metrics, particularly for the newly proposed task.\n\n### Weaknesses\n\n1.\tThe article’s description of the model structure in the main text is incomplete, relying heavily on Figure 4 to convey the model’s architecture. For example, the structure of the encoder and tokenizer remains ambiguous. The authors should clarify which components utilize existing models, which are fine-tuned, and which are trained from scratch.\n2.\tThe fairness of the model comparison is questionable. In Table 1, only a subset of models has been fine-tuned based on the test dataset, leaving others unmodified. Is this due to an inability to fine-tune certain models, or for another reason? The authors should provide detailed criteria for choosing to fine-tune specific models and address the fairness of this experimental design.\n3.\tOn the newly proposed task, the authors’ model demonstrates a substantial performance advantage. They mention that existing open-source models cannot accurately capture coordinates, but it would be insightful to know if fine-tuning on the authors’ proposed dataset could improve this. If fine-tuning other models would yield better results, the authors should do so for a more persuasive comparison. Conversely, if fine-tuning has no impact, the authors should explain the model’s unique advancements to provide deeper insights.\n4.\tThe role of the CoT component is somewhat unclear, with insufficient ablation experiments to support its effectiveness. The authors should conduct a more specific ablation study to illustrate the impact of CoT rather than merely reporting improved results with its inclusion.\n5.\tWhile the authors claim that the proposed model can perform diagnostic tasks, there is no clear evaluation or user study provided. A brief evaluation or explanation of the diagnostic results would add clarity.\n6.\tSeveral prior studies focus on medical image segmentation, which aligns well with the proposed region-centric task. Have the authors considered using such methods as feature extractors, backbones, or even as comparative models?\n7.\tThe experimental results on report generation are insufficient, as the authors rely solely on descriptive analysis without quantitative evaluation or user studies. Similar to point 5, if the authors wish to validate effectiveness without explicit evaluation metrics, a user study would provide valuable support.\n\n### Questions\n\n1.\tPlease explain why certain models were fine-tuned in the comparison while others were not.\n2.\tThe authors state that adding CoT improves model performance; however, no comprehensive ablation experiments were conducted to confirm this.\n3.\tHave the authors considered comparisons using other large models as the backbone?\n4.\tDo the authors plan to open-source this dataset?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a novel region-centric task along with the introduction of a large-scale Chinese medical dataset named MedRegInstruct. The authors have developed a bilingual multimodal large language model, MedRegA, which reportedly outperforms baseline methods across most evaluation metrics. The paper is well-organized and contributes to the existing body of knowledge in medical AI by defining a new task and dataset.\n\n**Major Comments:**\n1. The description of the model structure in the main text is incomplete; it relies heavily on Figure 4, and the architecture specifics, such as the encoder and tokenizer details, are unclear. The authors should clarify which components utilize existing models versus those that are fine-tuned or trained from scratch.\n2. The fairness of the model comparison is questionable; in Table 1, only some models were fine-tuned with the test dataset, while others were left unmodified. It is unclear whether this decision was due to limitations in fine-tuning certain models or other reasons. The authors should detail criteria for model selection and address the experimental fairness.\n3. The authors mention that existing models struggle with coordinate capture; additional analysis on whether fine-tuning could enhance performance is needed. If other models might yield better outcomes with fine-tuning, such comparisons should be made.\n4. The role of the CoT component is not sufficiently explained, and there is a lack of supporting ablation studies to validate its effectiveness. The authors should conduct a specific ablation study to illustrate its impact.\n5. The manuscript lacks clear evaluations or user studies to support the claim of the proposed model's diagnostic task performance; a brief evaluation should be provided to enhance clarity.\n6. The authors should consider using prior studies on medical image segmentation as feature extractors or comparisons, given their relevance to the region-centric task.\n7. The experimental results on report generation are lacking, relying only on descriptive analysis. A quantitative evaluation or user studies would bolster the assessment of effectiveness.\n\n**Minor Comments:**\n- Clarify the experimental design and structure throughout the manuscript for improved understanding.\n- Address typos or inconsistencies in figure labeling.\n- Enhance definitions of acronyms and ensure clarity in technical terms used.\n\n**Summary Paragraph:**\nIn summary, this manuscript presents a significant contribution to the field by introducing a new task and dataset, demonstrating innovation through the development of MedRegA. However, there are concerns regarding evaluation design, particularly in terms of fairness in model comparisons and a lack of detailed evaluations. While the results show promise, reproducibility may be affected by the insufficient clarity around model structure and comparative evaluations. Overall, the manuscript requires further refinement to ensure robust and reproducible findings.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedRegA*, a bilingual multimodal large language model developed to address a newly introduced region-centric task in medical imaging. It also contributes a large-scale Chinese medical dataset, *MedRegInstruct*, as a resource for this task. The paper is well-structured and demonstrates that the proposed model outperforms baseline methods across most evaluation metrics. Overall, the work offers a novel problem formulation and dataset with solid experimental results, though several methodological and evaluation aspects require clarification.\n\n---\n\n**Major Comments**  \n1. **Model Architecture Clarity:** The description of *MedRegA*’s architecture is incomplete. Essential details such as the encoder and tokenizer structure are only implied through Figure 4. The authors should clearly specify which components are inherited from existing models, which are fine-tuned, and which are trained from scratch.  \n2. **Fairness of Model Comparison:** In Table 1, only some comparative models are fine-tuned on the test dataset, raising questions about fairness. The rationale for this selective fine-tuning should be provided, including whether technical limitations prevented tuning some models.  \n3. **Evaluation on the Proposed Task:** The paper attributes much of the performance advantage to the model’s ability to capture coordinates. It would be helpful to test whether fine-tuning existing open-source models on *MedRegInstruct* improves their performance; otherwise, a clearer explanation of what uniquely enables *MedRegA*’s success is needed.  \n4. **CoT Component Analysis:** The contribution of the Chain-of-Thought (CoT) module is unclear due to limited ablation analysis. More targeted ablation studies are needed to demonstrate its specific impact.  \n5. **Diagnostic Capability Claims:** The claim that *MedRegA* can perform diagnostic tasks is not substantiated by quantitative or user-based evaluations. A brief diagnostic evaluation or qualitative analysis would enhance credibility.  \n6. **Comparison with Segmentation-Based Methods:** Since medical segmentation aligns with the region-centric concept, the authors should discuss or test segmentation methods as feature extractors, backbones, or baselines.  \n7. **Report Generation Evaluation:** Results for report generation rely solely on descriptive analysis. Quantitative or user study–based evaluation would strengthen the evidence.\n\n---\n\n**Minor Comments**  \n- Clarify terminology and acronyms (e.g., CoT).  \n- Ensure model figures and tables are self-contained and interpretable.  \n- Minor improvements in clarity would aid reproducibility.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in defining a new region-centric vision-language task and providing an accompanying large dataset. Its **innovation** lies in the bilingual, multimodal model that integrates coordinate reasoning. However, the manuscript’s **evaluation** requires stronger baselines, clearer fine-tuning protocols, and more comprehensive ablation studies. **Reproducibility** remains limited by incomplete methodological transparency and lack of opened code or dataset (pending). Addressing these issues would greatly strengthen the contribution.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents MedRegA, a bilingual multimodal large language model designed for medical imaging tasks with enhanced regional awareness. The authors identify that existing medical MLLMs are \"region-agnostic,\" treating entire images holistically without focusing on specific anatomical regions. To address this limitation, they construct MedRegInstruct, a large-scale dataset containing 550K region-text triplets and 240K grounded reports covering three region-centric tasks: region-to-text identification, text-to-region detection, and grounded report generation. The model is built on InternVL 1.2 and trained through alignment and instruction tuning phases. Experimental results show MedRegA outperforms existing medical MLLMs across visual question answering, report generation, and classification tasks while enabling regional capabilities that previous models cannot perform. The authors also introduce Regional Chain-of-Thought (CoT) to further enhance performance by leveraging spatial knowledge during inference.\n\n## Weaknesses\n\n• **Limited technical novelty in model architecture**: The core contribution lies primarily in dataset construction rather than methodological innovation.\n  - The model directly adopts InternVL 1.2 without architectural modifications (Section 4, Page 6), limiting technical contributions to training data curation\n  - The regional representation using bounding box coordinates [x1, y1, x2, y2] with special tokens is straightforward and lacks sophistication (Figure 4, Page 6)\n  - No comparison with alternative regional encoding methods or ablation studies on representation choices\n\n• **Insufficient evaluation rigor and statistical validation**: The experimental design lacks proper statistical analysis and comprehensive baseline comparisons.\n  - No statistical significance testing reported for performance improvements across multiple datasets (Table 1, Page 7)\n  - Missing comparison with recent state-of-the-art medical vision-language models beyond the selected baselines\n  - Human validation conducted on only 50 samples with 2 experts for automatic annotation quality (Page 5), which is insufficient for robust validation\n  - Regional CoT evaluation limited to few datasets without comprehensive ablation studies (Figure 2b, Page 2)\n\n• **Questionable dataset construction methodology**: The semi-automatic labeling system raises concerns about data quality and reproducibility.\n  - Automatic structure detection achieves only 72% accuracy compared to human annotations (Page 5), potentially introducing systematic errors\n  - Report refinement using InternLM for Chinese medical reports lacks validation against expert annotations\n  - No analysis of potential biases introduced by the automatic labeling pipeline or error propagation effects\n  - Missing details on inter-annotator agreement for the manual validation subset\n\n• **Unclear mathematical formulations and evaluation metrics**: The Region-Aligned evaluation framework lacks precise mathematical definitions.\n  - Algorithm 1 (Page 32) uses informal notation without rigorous mathematical formulation\n  - Hungarian matching algorithm application not clearly specified for the object-region alignment problem\n  - IoU threshold of 0.5 for region detection appears arbitrary without justification or sensitivity analysis\n  - Missing formal problem formulations for the three region-centric tasks\n\n## Suggestions for Improvement\n\n• **Enhance architectural contributions and technical innovation**: Develop novel methods for integrating regional information beyond simple coordinate encoding.\n  - Design specialized attention mechanisms or fusion modules for better region-text alignment rather than relying solely on coordinate tokens\n  - Conduct ablation studies comparing different regional representation methods (e.g., attention maps, feature pyramids)\n  - Explore architectural modifications to InternVL that specifically support regional reasoning capabilities\n\n• **Strengthen experimental validation with rigorous statistical analysis**: Implement comprehensive evaluation protocols with proper statistical testing.\n  - Report confidence intervals and statistical significance tests (t-tests, Mann-Whitney U) for all performance comparisons\n  - Include recent medical vision-language models in baseline comparisons and expand evaluation to more diverse datasets\n  - Increase human validation sample size to at least 200+ cases with multiple expert annotators and inter-rater agreement metrics\n  - Provide comprehensive ablation studies for Regional CoT across all task categories with statistical validation\n\n• **Improve dataset construction transparency and quality control**: Establish more rigorous validation protocols for the automatic labeling system.\n  - Implement iterative refinement of automatic annotation with expert feedback loops to improve 72% accuracy\n  - Provide detailed error analysis of the automatic labeling pipeline with failure case studies\n  - Include bias analysis and mitigation strategies for the semi-automatic construction process\n  - Expand human validation to representative samples across all modalities and anatomical regions\n\n• **Formalize mathematical frameworks and evaluation protocols**: Provide rigorous mathematical definitions for all tasks and metrics.\n  - Formalize the three region-centric tasks with precise mathematical notation and problem statements\n  - Specify the Hungarian algorithm implementation details with complexity analysis for the alignment problem\n  - Justify the IoU threshold choice through sensitivity analysis and provide adaptive threshold mechanisms\n  - Present the Region-Aligned evaluation framework with formal mathematical definitions and algorithmic details",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces MedRegA, a bilingual multimodal large language model tailored for medical imaging tasks that emphasizes regional awareness. The authors argue that current medical multimodal large language models (MLLMs) are “region-agnostic,” neglecting specific anatomical regions. To overcome this challenge, they develop MedRegInstruct, a comprehensive dataset comprising 550K region-text triplets and 240K grounded reports across three region-centric tasks: region-to-text identification, text-to-region detection, and grounded report generation. Experimental results demonstrate that MedRegA surpasses existing medical MLLMs in visual question answering, report generation, and classification tasks while introducing capabilities tied to regional specificity.\n\n**Major Comments:**  \n1. **Limited Technical Novelty in Model Architecture:**\n   - The manuscript predominantly contributes a dataset rather than introducing significant methodological advancements, using InternVL 1.2 without modifications.\n   - The bounding box coordinate representation is straightforward and lacks sophistication, with no exploration of alternative encoding methods or ablation studies.\n\n2. **Insufficient Evaluation Rigor and Statistical Validation:**\n   - The experimental design misses proper statistical significance testing for performance enhancements across datasets.\n   - Comparisons are limited to selected baselines, and the human validation is insufficiently small with only 50 samples.\n\n3. **Questionable Dataset Construction Methodology:**\n   - The semi-automated labeling system’s accuracy of 72% compared to human annotations raises concerns about data quality.\n   - There are no insights into biases from the labeling pipeline or details on inter-annotator agreement.\n\n4. **Unclear Mathematical Formulations and Evaluation Metrics:**\n   - The Region-Aligned evaluation framework lacks clear mathematical definitions and precision in algorithmic implementations.\n\n**Minor Comments:**  \n- Improve clarity in Algorithm 1's notation.  \n- Specify the Hungarian algorithm application for object-region alignment.  \n- Justify the use of a 0.5 Intersection over Union (IoU) threshold without speculative justification.\n\n**Summary Paragraph:**  \nThe study meets significance with its aim to improve medical imaging through regional awareness, yet it lacks innovation, primarily relying on dataset construction rather than novel methodological approaches. The experimental validation does not fulfill adequate rigor, with limited statistical testing and insufficient baseline comparisons, undermining reproducibility. Clear and formal mathematical definitions are needed to enhance the framework’s robustness. Overall, while the contributions to regional capabilities are noteworthy, significant improvements are needed in methodology and evaluation.\n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegA*, a bilingual multimodal large language model for medical imaging tasks designed to enhance region-level understanding. The authors note that most existing medical MLLMs process entire images without regional focus and therefore propose *MedRegInstruct*, a large-scale dataset comprising 550K region–text triplets and 240K grounded reports spanning three region-centric tasks: region-to-text identification, text-to-region detection, and grounded report generation. Built upon InternVL 1.2 through alignment and instruction tuning, *MedRegA* reportedly surpasses existing medical MLLMs on multiple benchmarks and introduces a “Regional Chain-of-Thought” inference mechanism for spatial reasoning. The paper is clearly structured and ambitious in scope, but several key methodological and evaluative aspects limit its technical and scientific rigor.  \n\n**Major Comments**  \n1. **Limited technical novelty:** The work’s main innovation lies in dataset curation rather than in the model structure. The authors reuse InternVL 1.2 without modification and adopt simple bounding box coordinate tokens ([x₁, y₁, x₂, y₂]) for region encoding, without exploring more advanced mechanisms or conducting ablation studies of representation variants.  \n2. **Insufficient evaluation rigor:** The experiments omit statistical significance testing and rely on a narrow set of baselines. Human validation of the automatic annotation process is limited to 50 samples from two experts, and Regional CoT is evaluated only on a few datasets.  \n3. **Dataset quality and reproducibility concerns:** The semi-automatic labeling system achieves only 72 % accuracy relative to human annotations, risking systematic errors. The refinement of Chinese reports via InternLM lacks expert validation, and the study does not address potential label bias, inter-annotator agreement, or the reproducibility of the data pipeline.  \n4. **Unclear formulations and metrics:** The Region-Aligned evaluation framework is informally specified. Mathematical notation in Algorithm 1 is imprecise, the Hungarian matching procedure is not described, and the IoU = 0.5 threshold is arbitrary and untested.  \n\n**Minor Comments**  \n- Strengthen notation consistency, particularly in Algorithm 1 and Figure 4.  \n- Clarify terminology around “regional alignment” versus “region-aware.”  \n- Provide fuller captions and symbol definitions in figures and tables.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important gap—regional grounding in medical MLLMs—and the proposed dataset could be of lasting significance. However, its innovation is primarily data-centric rather than architectural, the evaluation lacks statistical robustness, and methodological transparency is incomplete. Reproducibility would benefit from clearer mathematical definitions, pipeline documentation, and more extensive expert validation. Overall, while the goal aligns with the journal’s focus on clinically interpretable AI, substantial revisions are required to meet standards of innovation and empirical rigor.  \n\n**Decision Recommendation:** **Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \nThe submission, *“Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks (MedRegA),”* describes a multimodal large‑language‑model framework for multi‑organ, multi‑modality image–text reasoning and report generation. It proposes region‑centric datasets (MedRegInstruct) and a “Region‑Aware” model enabling region‑to‑text, text‑to‑region, and grounded‑report generation tasks. Although the work clearly relates to medical imaging AI, its primary novelty lies in large‑scale data curation and instruction‑tuning strategies for foundation models rather than in imaging physics, reconstruction, or quantitative image analysis. Thus, it sits at the interface of computer vision/NLP and medical‑AI applications and may exceed TMI’s usual methodological focus while being highly relevant to the interpretability of imaging‑AI systems.\n\n**2. Novelty & Contribution Level**  \nCompared with prior MLLMs (e.g., RadFM, MedDr, LLaVA‑Med), the paper introduces an explicit region‑aware supervision paradigm and bilingual capability. These are incremental but non‑trivial extensions of current multimodal frameworks. The dataset construction pipeline (automatic grounding, semi‑supervision using InternLM) is original within the clinical imaging domain. Conceptual originality: moderate; engineering and system integration novelty: good; theoretical innovation: limited.\n\n**3. Technical and Experimental Rigor**  \nThe manuscript is reproducible in spirit: training datasets, task formulations, metrics, and baselines are documented extensively; quantitative comparisons cover many modalities and benchmarks. However, several aspects raise questions of rigor for TMI‑level reproducibility:  \n- Dataset creation relies on semi‑automatic organ detection with only modest validation accuracy (∼70 %). Quality control and potential bias analyses are limited.  \n- Model evaluation depends largely on language‑generation metrics rather than clinically validated endpoints.  \n- No ablation on labeling noise, no uncertainty or error analysis per modality.  \n- Training on large private datasets but unclear code and model release specifics.\n\n**4. Clarity and Presentation**  \nWriting and structure are clear for a computer‑vision audience, though dense. The paper delivers abundant quantitative tables but could shorten model‑training details and strengthen methodological rationale. Figures are readable; language quality is good.\n\n**5. Ethical and Reproducibility Compliance**  \nAn ethics statement is provided: retrospective anonymized data with IRB approval implied but not explicitly stated. The commitment to release only public‑dataset‑derived data may limit reproducibility. Explicit data‑sharing or code‑release plan should be added.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n### 1. Summary\nThe authors propose **MedRegA**, a bilingual “region‑aware” multimodal large‑language model that integrates image‑ and text‑based reasoning for multiple medical tasks—visual‑question‑answering, report generation, classification, and newly defined region‑centric tasks. They construct **MedRegInstruct**, a large bilingual dataset with region–text correspondences from public (SA‑Med2D‑20M, MIMIC‑CXR) and private hospital data using semi‑automatic grounding. A Regional Chain‑of‑Thought mechanism is introduced to enhance spatial reasoning. Extensive experiments across eight modalities show overall quantitative gains versus prior medical MLLMs.\n\n### 2. Strengths\n- Ambitious and comprehensive framework covering bilingual reporting, region grounding, and generalist MLLM capabilities.  \n- Large accompanying dataset with explicit region annotations—valuable community resource if released.  \n- Thorough benchmarking against major baselines with consistent improvements.  \n- Addresses interpretability and localizability, both important clinical concerns.  \n- Methodologically consistent use of alignment‑training and instruction‑tuning pipeline.\n\n### 3. Weaknesses\n- **Scope fit:** The main contribution is system integration and dataset engineering rather than core imaging methodology; the paper may better fit an AI‑for‑health or computer‑vision venue.  \n- **Limited methodological depth:** Little theoretical justification for “Regional CoT”; mainly empirical tuning.  \n- **Data‑quality concerns:** Semi‑automatic bounding‑box accuracy (~70 %) and potential bias not quantitatively analyzed.  \n- **Evaluation metrics:** Heavy reliance on NLP scores instead of task‑specific clinical or quantitative imaging metrics.  \n- **Reproducibility:** In‑house data likely not shareable; code availability undefined.\n\n### 4. Major Comments\n1. **Clarify dataset governance and IRB approval.** Specify ethical approval numbers and confirm that private datasets cannot identify patients.  \n2. **Quantify label quality.** Provide inter‑rater reliability and uncertainty calibration for the automatic grounding pipeline.  \n3. **Methodological explanation.** Give a mathematical description or intuition for Regional CoT—how detection outputs condition text generation—and test its statistical significance.  \n4. **Clinical validation.** Incorporate expert‑rated report consistency or lesion localization accuracy to supplement BLEU/ROUGE metrics.  \n5. **Reproducibility statement.** Explicitly plan public release (data, code, model weights). Without this, claims of generalizability are difficult to verify.\n\n### 5. Minor Comments\n- A few typographical errors (e.g., “Languange”).  \n- Condense Table 1; highlight key results with relative improvements.  \n- Provide visual examples per modality in higher resolution for readers unfamiliar with datasets.  \n- Include information on computational cost and inference time.  \n- Verify consistency of metric definitions (BLEU‑1 vs F1 etc.).\n\n### 6. Overall Recommendation\n**⚠ Reject (Out of Scope / Below TMI Novelty Threshold)**  \nWhile scientifically sound and well executed, the work is primarily **an application of existing multimodal‑LLM paradigms** with large‑scale data engineering rather than a methodological advance in medical imaging science as expected by *IEEE TMI*. It would be more suitable for a general AI‑for‑health or medical‑AI journal (e.g., MedIA, Nature Digital Medicine, IEEE JBHI).\n\nTo reach TMI standards, the authors could:  \n- Theoretically analyze region‑grounding mechanisms and their influence on image‑feature representation.  \n- Provide rigorous quantitative imaging evaluations (e.g., lesion‑localization accuracy, uncertainty maps).  \n- Explore how regional reasoning improves diagnostic reliability or radiologist performance.\n\n### 7. Confidence Level\n**4 / 5 – High confidence** in assessment based on alignment with TMI’s methodological focus and extensive comparison to current literature.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript, *“Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks (MedRegA),”* presents a bilingual “region‑aware” multimodal large‑language‑model designed for integrated image‑ and text‑based reasoning across multiple clinical imaging modalities and tasks, including report generation and visual question answering. A companion dataset, *MedRegInstruct*, is introduced to provide region–text correspondences through semi‑automatic grounding from public and private sources. The paper is clearly written and empirically detailed, though its main originality lies in system integration and dataset engineering. While the topic relates strongly to medical‑AI interpretability, the methodological focus may extend beyond *IEEE TMI’s* traditional scope.  \n\n**Major Comments**  \n1. **Dataset Ethics and Governance:** Clarify institutional review board (IRB) approval, ethical oversight, and data‑anonymization protocols for private datasets.  \n2. **Annotation Quality:** Quantitatively assess label accuracy and consistency. Include inter‑rater or grounding‑reliability analyses to validate region detection (~70 % accuracy is currently insufficiently justified).  \n3. **Methodological Explanation:** Elaborate on the *Regional Chain‑of‑Thought (CoT)* mechanism—how spatial cues influence language generation—and statistically test its contribution beyond empirical tuning.  \n4. **Evaluation Depth:** Current metrics rely mainly on linguistic scores (BLEU, ROUGE). Clinical or quantitative imaging validation (e.g., expert rating, localization accuracy) would strengthen claims.  \n5. **Reproducibility:** Clearly state data‑sharing and code‑release intentions. Without accessible models or datasets, reproducibility and generalizability remain uncertain.  \n6. **Scope and Methodological Fit:** The contribution emphasizes large‑scale engineering and multilingual extension rather than new imaging methodology; this may place the work outside TMI’s core mission.  \n\n**Minor Comments**  \n- Correct typographical errors (e.g., “Languange”).  \n- Condense Table 1, emphasizing relative improvements.  \n- Provide higher‑resolution visual examples for each modality.  \n- Report computational cost, inference time, and metric definition consistency.  \n- Streamline training details and clarify methodological rationale.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \n*Significance:* The framework ambitiously unifies bilingual and regional reasoning, addressing clinically relevant interpretability issues.  \n*Innovation:* Conceptual advances are moderate; novelty lies mainly in dataset construction and bilingual integration rather than algorithmic development.  \n*Evaluation:* Experiments cover multiple modalities and benchmarks but rely on language‑generation metrics without clinical validation.  \n*Reproducibility:* Documentation is thorough, yet data‑governance transparency and code release remain incomplete, limiting verifiability. Overall, the work is technically solid but more application‑driven than methodologically innovative at the level expected for *IEEE TMI*.  \n\n**Decision Recommendation**  \n**Reject (Out of Scope / Below TMI Novelty Threshold).**  \nThe study demonstrates strong engineering execution but limited methodological advancement in medical imaging science. A specialized AI‑for‑health venue may be more appropriate. To meet TMI standards, the authors should deepen theoretical analysis of regional reasoning, include rigorous imaging‑specific evaluations, and enhance reproducibility through open resources.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript titled *“Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks (MedRegA)”* presents a framework for multimodal large language models (MLLMs) that enhances image and text reasoning in medical contexts. It introduces region-centric datasets (MedRegInstruct) and a \"Region-Aware\" model that collectively address report generation and integrated reasoning tasks. While the work is relevant to medical imaging AI, its primary contribution lies in data curation and instruction-tuning techniques rather than in the foundational methodologies of imaging physics or analysis.\n\n**Major Comments:**  \n1. **Scope Fit:** The primary contributions relate more to system integration and dataset engineering than to core imaging methodologies, suggesting better alignment with AI-for-health or computer vision publications.\n2. **Novelty Assessment:** Although the paradigm of region-aware supervision and bilingual capability offers incremental advances, they do not represent a significant methodological shift.\n3. **Data Quality Concerns:** The dataset construction relies on semi-automatic organ detection with a modest validation accuracy of ~70%, and potential biases are inadequately analyzed.\n4. **Evaluation Metrics:** The reliance on language-generation metrics instead of clinically validated outcomes raises reproducibility issues.\n5. **Reproducibility Query:** Code and model release specifics are ambiguous, hindering reproducibility.\n\n**Minor Comments:**  \n- Correct typographical errors (e.g., “Languange”).\n- Condense Table 1 to emphasize key results.\n- Provide higher resolution examples for better understanding of datasets.\n- Include details about computational costs and inference times.\n- Ensure consistency among metric definitions (e.g., BLEU-1 vs. F1).\n\n**Summary Paragraph:**  \nThe submission provides a significant contribution in terms of introducing a comprehensive bilingual MLLM capable of handling diverse biomedical tasks. However, it primarily hinges on existing frameworks, lacking the methodological depth and theoretical analysis typically required by *IEEE TMI*. While the large dataset could be a valuable community resource, data quality concerns and a lack of clinical validation diminish its impact regarding reproducibility. The study's findings show overall improvements but are currently framed within a context that stretches beyond TMI's typical focus on innovative imaging methodologies.\n\n**Decision Recommendation:**  \n**⚠ Reject (Out of Scope / Below TMI Novelty Threshold)**  \nDespite being scientifically sound, this work primarily represents an application of existing paradigms in MLLM with an emphasis on data engineering rather than outlining a methodological advance in medical imaging science fitting for *IEEE TMI*. It may be more appropriate for journals focused on AI applications in health. To meet TMI standards, the authors should focus on theoretical analyses, rigorous imaging evaluations, and exploring the implications of their regional reasoning framework on clinical outcomes.\n\n**Confidence Level:**  \n**4 / 5 – High confidence** in the assessment based on alignment with TMI’s methodological focus and comprehensive literature comparisons.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Haonan Wang",
      "Honglong Yang",
      "Jiaji Mao",
      "Jun Shen",
      "Lehan Wang",
      "Xiaomeng Li",
      "Zehong Yang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2b1c7dbf52c032b412c014b02b84f27ac218e3ee.pdf",
    "remote_url": "https://openreview.net/pdf/2b1c7dbf52c032b412c014b02b84f27ac218e3ee.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical Foundation Model",
      "Multimodal Dataset",
      "Vision-Language Pretraining."
    ],
    "abstract": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with multigranular annotations for more than 65 diseases. These multigranular annotations encompass both global information, such as modality and organ detection, and local information like ROI analysis, lesion texture, and region-wise correlations. Unlike the existing multimodal datasets, which are limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations in the form of image-ROI-description triplets without the need for any paired text descriptions. Specifically, data from over 30 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M, achieving state-of-the-art performance on VQA-RAD, SLAKE, and PathVQA, surpassing representative SOTA multimodal large language models. Furthermore, MedTrinity-25M can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. We will make our dataset available. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces MedTrinity-25M, a comprehensive and large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with detailed annotations for more than 65 diseases. The dataset provides the most enriched annotations for various multimodal tasks, such as captioning, classification, and segmentation, and supports the pre-training of advanced multimodal medical AI models, achieving state-of-the-art performance.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe paper introduces MedTrinity-25M, the largest multimodal medical dataset to date, featuring multigranular annotations and containing over 25 million triplets (image-ROI-description). The development of an automated data annotation pipeline significantly scales up medical image-text data.\n2.\tWith the support of MedTrinity-25M, the pretrained CLIP and LLaVA models demonstrate better performance compared to previous methods.\n3.\tThe release of this dataset contributes to the medical AI community, providing researchers and practitioners with a valuable resource for advancing multimodal tasks and improving healthcare applications.\n\n### Weaknesses\n\n1.\tThe quality of the generated image-text data may not be sufficiently high. It is advisable to review the questions associated with this data. In the image-captioning constructed from MedTrinity-25M, we found that many basic imaging modalities were incorrectly identified. For instance, in the CT-RATE data used as the source, over 60,000 images were misidentified as MRI, more than 90,000 as X-ray, and even a small number were identified as endoscopy. Below, we provide some captions to illustrate this phenomenon present in MedTrinity-25M.\n“””\n{\"image_path\":\"seg_train_126_a_2-right lung.nii-34.jpg\",\"id\":\"b573995e-2d11-11ef-bbea-f02f74942466\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the thoracic region, showing the heart, part of the lung, and the spine. The heart is centrally located with the lung on either side and the spine running vertically in the background. The region of interest, located in the left-center at the bottom of the image, appears to have a different texture and intensity compared to the surrounding tissue, suggesting a possible abnormality. This region's relative position is towards the lower left side of the image, adjacent to the lower part of the lung. The content within this region may indicate a disease process, which could be related to or affecting the adjacent lung tissue. Given the proximity to the lung, it is possible that the abnormality could be influencing or being influenced by the pulmonary structures.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_126_a_2-right lung.nii-11.jpg\",\"id\":\"b57345c6-2d11-11ef-a899-f02f74942466\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the thoracic region, showing the heart, part of the lung, and the spine. The heart is centrally located with the lung on either side and the spine visible posteriorly. The region of interest, located in the lower-middle left-center of the image, shows an area with altered signal intensity, which is indicative of a pathological condition. This area is situated within the lung tissue and is characterized by a texture and signal intensity that differ from the surrounding healthy lung tissue, suggesting the presence of a disease process such as consolidation, infection, or a mass.\\n\\nThe region of interest may be related to the surrounding lung tissue in that it could represent a localized disease process affecting the lung, potentially leading to or resulting from changes in the adjacent lung parenchyma. Given the nature of MRI imaging and the appearance of the region, it is possible that this area could be associated with a process such as inflammation, demyelination, or a neoplastic growth, which may have implications for the function of the adjacent lung tissue.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_14684_a_1-covid-19 infection.nii-76.jpg\",\"id\":\"a77ee972-2d78-11ef-bdc4-f02f74942576\",\"caption\":\"The image is a radiographic scan, likely a chest X-ray, showing the thoracic region with the trachea and main bronchi appearing open, indicating no occlusive pathology. The lungs exhibit minimal emphysematous changes, and there are pleuroparenchymal sequelae changes at the apex of both lungs. Additionally, there are linear atelectasis in the middle lobe of the right lung and the lingular segment of the left lung upper lobe. The heart contour and size are normal, and there are no pleural or pericardial effusions. The mediastinal structures are not optimally evaluated due to the absence of contrast material, and the mediastinal main vascular structures are of normal width. No enlarged lymph nodes or pathological wall thickness increase are observed in the esophagus. The thoracic vertebral corpus shows normal heights, alignment, and densities, with osteophytes at the vertebral corpus corners, and the neural foramina are open.\\n\\nThe region of interest, located centrally and in the upper-middle area of the image, occupying approximately 0.3% of the area, corresponds to the lung apices where pleuroparenchymal sequelae changes are noted. These changes are characterized by alterations in lung parenchyma texture, which may appear as irregularities or areas of increased density compared to the surrounding lung tissue.\\n\\nThe pleuroparenchymal sequelae changes in the lung apices may be related to the emphysematous changes in the lungs, as both conditions can result from chronic inflammatory processes, suggesting a possible pathophysiological connection between the two findings.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_9381_a_2-covid-19 infection.nii-145.jpg\",\"id\":\"9880af92-2c1e-11ef-b9dd-f02f74942466\",\"caption\":\"The image is a chest X-ray showing a cross-sectional view of the thoracic cavity, including the lungs, heart, and part of the spine. A region of interest is located at the periphery of the thoracic cavity, likely within the lung tissue, which appears as a darker area compared to the surrounding lung parenchyma. The region of interest, which is abnormal, shows an area of increased opacity that suggests the presence of a pathological condition, possibly a mass or lesion within the lung tissue. This abnormal area is indicative of a disease process, which could be related to the surrounding lung tissue either as a primary pathology or as a secondary effect of a systemic condition affecting the lung. The abnormality's proximity to other structures within the thoracic cavity, such as the pleura or lung tissue could imply a relationship where the disease process in the region of interest may have originated from or is affecting adjacent areas, potentially impacting nearby structures due to its location within the thoracic cavity.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_7973_a_1-right lung.nii-84.jpg\",\"id\":\"88af61aa-2d11-11ef-959e-f02f74942466\",\"caption\":\"The image is a sagittal section of an endoscopic view of the thoracic region, showing the spine, part of the lung, and the surrounding thoracic structures. A region of interest is located at the lower-middle part of the image, horizontally left-center, occupying approximately 0.6% of the area. The region of interest, located in the lower-middle left-center of the image, displays an abnormality in the lung tissue, which appears to be a small, localized area with a different texture and possibly altered density compared to the surrounding lung parenchyma, suggesting a pathological change. This abnormality could be related to the adjacent lung tissue, potentially indicating a localized disease process or lesion that may be affecting or resulting from the surrounding lung tissue, given the proximity and the nature of the disease knowledge provided.\",\"source\":\"ct_rate\"}\n“””\nMoreover, there are also issues with modality misidentification in the image-captioning derived from the quilt-1m dataset. For example, pathology images were misidentified as X-ray and MRI, as follow, \n“””\n{\"image_path\":\"G-tdJ0oZxJ4_image_e74e3372-a40e-40e3-ac06-e3d53eeab845.jpg\",\"id\":\"f5ce616b-89ca-41f6-b820-b480bb3327af\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the lung, showing a cross-sectional view of the thoracic cavity. A region of interest is located at the top of the image, which appears to be in the upper part of the lung field, likely within the upper lobe, given the position of the lung's anatomy. The region of interest, which is abnormal, exhibits an unusual appearance compared to the surrounding lung tissue, possibly indicating a neoplastic process. This abnormality is characterized by a difference in texture and density, which may suggest a pathological change, such as a mass or lesion within the lung tissue. The affected area's relationship to the rest of the lung tissue could imply a localized pathological process that may be the primary site of the disease, potentially impacting or being impacted by adjacent lung structures due to its proximity to other regions, although the exact relationship depends on the nature of the pathology and its progression.\",\"source\":\"quilt_1m\"}\n{\"image_path\":\"895313503048712192_0.jpg\",\"id\":\"0b53a8c8-5661-449a-b39a-f92887fceb86\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the brain, showing a cross-sectional view that includes brain tissue with various shades of gray indicating different structures and densities. A region of interest is located at left-center part of the image, which appears to be in the cerebral hemisphere, likely within the white matter of the brain. The region of interest exhibits an abnormality that differs in texture and possibly size from the surrounding brain tissue, suggesting the presence of a pathological change. This abnormal area could be related to the surrounding brain structures, potentially affecting or being affected by them due to its proximity and the nature of brain tissue, which may indicate a pathological process such as a tumor, edema, or other brain abnormalities. The abnormality's characteristics, such as altered signal intensity, could be indicative of the disease process affecting the brain tissue's function or structure.\",\"source\":\"quilt_1m\"} \n{\"image_path\":\"962043872649011205_0.jpg\",\"id\":\"f94e8ba4-7ddf-450e-9616-d4681e9dcf02\",\"caption\":\"The image is a chest X-ray image of a 15-year-old boy's hand, showing the left side of the image is a chest X-ray showing the hand's anatomy, including the bones of the hand, with the focus on the epiphysis and possibly the bones of the hand's proximal and distal parts such as the clavicles, ribs, clavicles, and parts of the spine, which are essential for various activities like running, throwing, and punching, along with the presence of the epiphyseal plate and the development of the trapezius, which are crucial for a 15-year-old boy's hand, suggesting a focus on the skeletal structure and function of the hand, including the development of the hand's bones such as the clavicles and the development of the wrist bones, which are crucial for various activities like walking, running, and running, as well as the development of the wrist bones of the hand, which are essential for a growing hand and are indicative of a developing hand and are crucial for a growing hand and are typically found in activities such as playing with play and sports, suggesting a comprehensive evaluation of the hand's anatomy and function.\",\"source\":\"quilt_1m\"}\n{\"image_path\":\"1301506707483439105_1.jpg\",\"id\":\"7b0161c9-1451-4f27-9278-aff87726bacb\",\"caption\":\"The image is a lateral chest X-ray image of a 15-year-old boy's hand, showcasing the epiphysis of the wrist bones, which are the bones of the hand, including the radius of the wrist bones, clavicles and the bones of the hand's growth plate are visible, such as the clavicles, ribs, and vertebrae are the primary focus of the image is a close-up view of a lateral chest X-ray image, showcasing the epiphysis of the hand's anatomy, which are essential for various stages of development and growth patterns. The image is a detailed X-ray of the hand's anatomy, displaying the epiphyseal growth plate and development of the wrist bones of a 15-year-old boy's hand, with the bones of the epiphyseal plate visible in the image are the primary organ of interest, which includes the epiphyseal plate and epiphyseal plate, as well as the epiphyseal plate's development is crucial for diagnosing the stage of developmental stage of the wrist bones, typically found in the wrist bones of a 15-year-old boy's hand, which is crucial for understanding the zone of developmental development, and the image is likely to be a lateral chest X-ray image showing the epiphyseal plate's development is crucial for diagnosing the stage of developmental stage of the wrist, with the epiphyseal plate being the primary focus of the image.\",\"source\":\"quilt_1m\"}.\n2.\tThe experiments lack comprehensiveness. The comparison of multimodal large models is limited to VQA-RAD, SLAKE, and PathVQA. A broader range of specialized benchmarks, such as the health subset of MMMU, could provide a more robust comparison of the multimodal large models’ performance.\n\n### Questions\n\n1.\tThe data construction pipeline consists of two main steps: first, generating 200,000 multigranular textual descriptions via GPT-4V, and then fine-tuning LLaVA with this dataset. In my opinion, the performance of the trained model LLaVA-MedCap is significantly influenced by the quality of the data generated by GPT-4V. However, in Table 3, GPT-4V's performance falls far short of LLaVA-Tri. Why did you choose to use GPT-4V to generate the supervised fine-tuning data for the fine-tuning process? In my recent review of the publicly released MedTrinity-25M data, I found that many of the generated data instances were of average quality, with even simple modalities being incorrect.\n2.\tIn Section 4.1, it is stated that “The model is fine-tuned for three epochs on each VQA dataset and evaluated accordingly.” However, this evaluation setup is not fair when comparing with the methods presented in the table. Some results in Table 3 appear to be directly extracted from llava-med’s table, but these methods were not fine-tuned on the training set of the VQA benchmark. In contrast, the accuracy of the chosen llava-med method was achieved after fine-tuning for 15 epochs on the corresponding VQA benchmark training set, making this comparison inappropriate.\nCurrently, the evaluation paradigm for multimodal large models (MLLM) typically involves assessing the model directly on multiple benchmarks after two stages of pretraining and supervised fine-tuning. Therefore, I suggested distinguishing the results presented: there should be a comparison of the results not fine-tuned on the corresponding training set against the models that were not fine-tuned. Additionally, when comparing with methods fine-tuned on the corresponding training set, the number of fine-tuning epochs (including for the comparison methods) should be clearly indicated. If possible, please also conduct an ablation study regarding the number of fine-tuning epochs.\n3.\tIn Table 3, comparing CLIP-like models with LLaVA, a multimodal large model, seems inappropriate. It would be better to categorize them into two groups: one for CLIP models and another for MLLM.\n4.\tThe evaluation details in Table 3&4 are not clearly explained. What do \"open\" and \"close\" specifically refer to?  Please add a brief explanation of the \"open\" and \"close\" terms in the table caption or in the text describing the results.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n- Yes, Other reasons (please specify below)\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents MedTrinity-25M, a large-scale multimodal dataset for medicine, comprising over 25 million annotated images across 10 modalities, associated with more than 65 diseases. This dataset aims to enhance multimodal tasks such as captioning, classification, and segmentation while supporting the development of advanced multimodal medical AI models. Overall, the clarity and quality of the manuscript are moderate.\n\n### Major Comments\n1. **Novelty**: The contribution of MedTrinity-25M as the largest multimodal medical dataset is notable; however, concerns arise regarding the quality of the generated image-text data. Misidentifications of modalities are prevalent, e.g., CT images incorrectly labeled as MRI or X-ray, potentially impacting data utility.\n2. **Evaluation Design**: The experiments lack sufficient comprehensiveness. The comparison is limited to three benchmarks (VQA-RAD, SLAKE, and PathVQA). A wider range of specialized evaluations, such as the health subset of MMMU, could offer a more robust performance assessment.\n3. **Reproducibility**: The description of the data construction pipeline and the influence of GPT-4V raises questions. There are discrepancies in the performance and quality of generated data indicated in Table 3, necessitating a clearer rationale for the choice of GPT-4V.\n4. **Evaluation Methodology**: The evaluation set-up appears unfair when comparing with others in Table 3, as fine-tuning duration varies. A clearer distinction between results from models fine-tuned on training sets versus those not fine-tuned is essential for transparency.\n\n### Minor Comments\n1. Add clarification on the terms \"open\" and \"close\" in the evaluations presented in Tables 3 and 4.\n2. There are multiple instances of modality misidentification in data derived from the quilt-1m dataset that need addressing for accuracy.\n\n### Summary Paragraph\nThe MedTrinity-25M dataset signifies a substantial contribution to the field of medical AI through its comprehensive annotations and sizable imagery. However, there are notable weaknesses regarding data quality and evaluation comprehensiveness. The manuscript should address these weaknesses by improving data accuracy and validating model performance against a broader set of benchmarks. In terms of significance and innovation, the dataset holds promise, but the evaluation and reproducibility aspects require further attention to meet the necessary standards.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity‑25M*, a large‑scale multimodal medical dataset encompassing more than 25 million image‑ROI‑description triplets across ten imaging modalities and annotated for over 65 diseases. It aims to provide comprehensive resources for multimodal tasks such as captioning, classification, and segmentation, facilitating the pretraining of advanced AI models and achieving strong performance on standard benchmarks. Overall, the work is ambitious and of potential value to the medical AI community, though aspects of data quality, evaluation fairness, and experimental comprehensiveness warrant further improvement.  \n\n**Major Comments**  \n1. **Data Quality and Modality Misidentification:** A primary concern involves the accuracy of the automatically generated image‑text annotations. Numerous instances of modality misclassification were observed—for example, CT images labeled as MRI, X‑ray, or endoscopy, and pathology images wrongly identified as radiographic studies. These errors suggest systemic issues in the caption generation process and question the reliability of the dataset for downstream modeling.  \n2. **Limited Benchmark Coverage:** The evaluation covers only a few VQA datasets (VQA‑RAD, SLAKE, and PathVQA). Broader benchmarking, such as inclusion of the MMMU health subset, would yield a more comprehensive assessment of model generalization.  \n3. **Choice of Supervision Source:** The dataset construction pipeline relies on GPT‑4V to produce 200,000 textual descriptions later used to fine‑tune LLaVA‑MedCap. Given that GPT‑4V performs relatively poorly in Table 3 and that generated captions contain basic modality errors, justification for using GPT‑4V as the data source is needed.  \n4. **Unfair Evaluation Protocols:** Comparison fairness in Table 3 is problematic because competing methods were not equally fine‑tuned. The authors fine‑tuned LLaVA for three epochs, whereas other results were obtained with fifteen. Clearer differentiation between pre‑trained‑only and fine‑tuned results, and inclusion of ablation on fine‑tuning epochs, are recommended.  \n5. **Category Inconsistency:** CLIP‑like models and multimodal large language models (MLLMs) are compared directly in Tables 3–4. These categories should be evaluated separately for clearer interpretation.  \n6. **Ambiguity in “Open” vs “Close” Evaluation:** The meaning of these terms in Tables 3–4 is not defined and should be clarified in captions or the text.  \n\n**Minor Comments**  \n- Define all acronyms upon first use and maintain consistent table formatting.  \n- Ensure that tables explicitly describe evaluation settings, including the “open/close” terminology.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe dataset has high potential significance as one of the largest multimodal medical resources and is innovative in scalability and automated annotation. However, questions about label fidelity and modality correctness reduce confidence in reproducibility and reliability. The evaluation demonstrates promising results but is incomplete and lacks fairness in comparison. Addressing data validation, expanding benchmarks, and standardizing evaluation protocols would notably strengthen significance and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The dataset contribution is valuable, but substantial revisions are needed to ensure data quality, fair evaluation, and accurate reporting before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million images across 10 imaging modalities and covering more than 65 diseases. The dataset features multigranular annotations that include both global information and local information. The authors develop an automated pipeline that generates image-ROI-description triplets without the need for paired textual descriptions. This pipeline utilizes domain-specific expert models to identify ROIs and prompts multimodal large language models (MLLMs) to produce detailed annotations through retrieval-augmented generation (RAG).\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- Dataset Comprehensiveness: The dataset is exceptionally extensive, encompassing a wide array of imaging modalities, diseases, and anatomical structures. This diversity significantly enhances its comprehensiveness and utility for various medical AI applications.\n- Construction Pipeline through Advanced Models: The utilization of domain-specific expert models and multimodal large language models (MLLMs) for annotation substantially enriches the dataset. This approach adds multigranular details that improve the quality and depth of the annotations, making the dataset more valuable for training sophisticated models.\n\n### Weaknesses\n\n- Limited Applicability for Visual Data without ROIs: The proposed pipeline relies heavily on regions of interest (ROIs) for constructing multimodal data pairs. For visual data that lack explicit ROI annotations, it is unclear how the pipeline can be effectively applied. This limitation may restrict the dataset's usability across the full spectrum of medical images, particularly those where ROI determination is challenging or subjective.\n- Impact of Multigranular Information Not Fully Explored: While Tables 3 and 4 validate the effectiveness of the benchmark in general terms, the paper does not specifically assess how the inclusion of multigranular information, such as ROIs, influences the performance of medical MLLMs. Given that ROI is a pivotal component of the dataset, a focused evaluation comparing models trained with and without multigranular information would strengthen the claims about its benefits.\n- Insufficient Discussion of Related Benchmarks: The paper lacks a thorough discussion of existing med-MLLM benchmarks that involve multigranular information. Notably, works like \"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI\" and \"A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models\" are not cited or compared against. Including a comparative analysis would provide context and clarify how MedTrinity-25M advances the field relative to existing resources.\n- Lack of Evaluation on Multigranular-Specific Tasks: The dataset is primarily used for training and is evaluated only on VQA-RAD, SLAKE, and PathVQA. These evaluations may not fully capture the effectiveness of the dataset's multigranular annotations since they do not specifically target tasks designed to leverage such detailed information. This omission raises questions about the practical benefits of the multigranular data provided.\n- Challenges with Unique Medical Image Descriptions in RAG: The Retrieval-Augmented Generation (RAG) technique used for annotation relies on a general medical knowledge base. However, medical images often have unique features and presentations, even within the same disease category. For example, Atelectasis can manifest differently in chest X-rays depending on the affected lobe. The paper does not address how the RAG system accounts for these unique variations when the initial visual datasets typically offer only generic classification labels. This could impact the accuracy and specificity of the generated annotations.\n\n### Questions\n\n- Handling of Visual Data Without ROIs: For medical images that lack explicit ROI annotations, how does your pipeline construct the corresponding multimodal data pairs? Is there a mechanism to generate or infer ROIs in such cases, or is the pipeline limited to images where ROIs are predefined?\n- Assessing the Impact of Multigranular Information on Med-MLLMs: Considering that ROIs and multigranular annotations are central to your dataset, have you conducted experiments to evaluate how these features specifically affect the performance of medical MLLMs? Can you provide insights or results that demonstrate the advantages of including multigranular information in model training?\n- Comparison with Existing Multigranular Benchmarks: Could you elaborate on how MedTrinity-25M compares with existing benchmarks that involve multigranular information? What distinguishes your dataset from these, and how does it contribute uniquely to the advancement of general medical AI?\n- Evaluation on Multigranular-Specific Tasks: Given that the dataset is evaluated only on VQA-RAD, SLAKE, and PathVQA, which may not fully utilize multigranular annotations, do you have plans to test your dataset on tasks specifically designed for multigranular information? How can you demonstrate the effectiveness of your dataset's detailed annotations in improving model performance on such tasks?\n- Addressing Unique Medical Image Presentations in RAG: Medical images often present unique and variable features even when categorized under the same disease label. How does your RAG approach handle the specificity and variability of individual medical image descriptions? For instance, with conditions like Atelectasis that can manifest differently in imaging, how does the system ensure that the generated annotations accurately reflect these variations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents MedTrinity-25M, a comprehensive multimodal medical dataset containing over 25 million images across 10 imaging modalities and encompassing more than 65 diseases. It features multigranular annotations that provide both global and local information. The authors introduce an innovative automated pipeline that generates image-region-of-interest (ROI)-description triplets without requiring paired textual descriptions, leveraging domain-specific expert models and multimodal large language models (MLLMs) in a retrieval-augmented generation (RAG) framework.\n\n**Major Comments:**\n1. **Limited Applicability for Visual Data without ROIs:** The pipeline's reliance on ROIs raises questions about its applicability to images without explicit annotations. This limitation could hinder the dataset's effectiveness for a broader range of medical images.\n2. **Impact of Multigranular Information Not Fully Explored:** The paper does not evaluate how multigranular annotations affect MLLM performance. A focused study comparing models trained with and without these details would bolster the claims about their benefits.\n3. **Insufficient Discussion of Related Benchmarks:** The absence of a comparison with existing multigranular med-MLLM benchmarks, such as \"GMAI-MMBench\" or \"A Spectrum Evaluation Benchmark,\" limits the contextual understanding of MedTrinity-25M's advancements.\n4. **Lack of Evaluation on Multigranular-Specific Tasks:** The dataset's evaluation is centered on general tasks (VQA-RAD, SLAKE, PathVQA) that may not fully leverage the detailed annotations provided by the dataset.\n5. **Challenges with Unique Medical Image Descriptions in RAG:** The RAG technique does not address variations in medical image representations adequately, which may compromise the specificity of generated annotations for conditions such as Atelectasis.\n\n**Minor Comments:**\n- The overall clarity of the manuscript could be enhanced, as noted by the soundness rating. \n- Consider adding definitions for technical acronyms early in the document for improved accessibility.\n- Review figures for clarity and ensure that all relevant details are adequately labeled.\n- Minor typographical errors were observed and should be corrected.\n\n**Summary Paragraph:**  \nOverall, MedTrinity-25M demonstrates significant potential in terms of dataset comprehensiveness and the innovative construction of annotation pipelines. However, its significance is somewhat tempered by limitations related to the applicability of the proposed pipeline, insufficient exploration of critical performance factors, and a lack of discussion concerning existing benchmarks. While the dataset's intended contributions are clear, its reproducibility and evaluation in tasks specifically suited for multigranular data need further development.\n\n**Decision Recommendation:**  \nMajor revision is recommended to address the outlined weaknesses and enhance the manuscript's contributions to the field.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity‑25M*, a large‑scale multimodal medical dataset containing over 25 million images across 10 imaging modalities and more than 65 diseases. The dataset is annotated at multiple granularities using an automated pipeline that generates image–ROI–description triplets without requiring pre‑existing paired texts. The pipeline integrates domain‑specific expert models for region‑of‑interest (ROI) detection and employs multimodal large language models (MLLMs) via a retrieval‑augmented generation (RAG) framework for detailed textual annotation. Overall, the paper is clearly written and technically sound, though several methodological and comparative aspects require further clarification and experimental validation.  \n\n**Major Comments**  \n1. **Dependence on ROI Annotations:** The proposed pipeline presumes the availability of explicit ROI labels. Its applicability to images lacking such regions remains unclear, potentially limiting dataset generality across modalities or conditions where ROI identification is subjective or unavailable.  \n2. **Lack of Analysis on Multigranular Benefits:** While the benchmark evaluations in Tables 3–4 demonstrate overall dataset utility, they do not isolate the contribution of multigranular information. Experiments comparing models trained with and without ROI‑level annotations would substantiate claims regarding their added value.  \n3. **Insufficient Comparison with Related Benchmarks:** The manuscript omits discussion of similar multigranular medical benchmarks such as *GMAI‑MMBench* and *Spectrum Evaluation Benchmark*. Including such comparisons would contextualize the novelty and advancement of MedTrinity‑25M.  \n4. **Evaluation Tasks Not Tailored to Multigranular Data:** The dataset is primarily validated through VQA‑RAD, SLAKE, and PathVQA, which do not explicitly exploit ROI‑based annotations. Testing on multigranular‑specific or segmentation‑aware tasks would better demonstrate its strengths.  \n5. **Handling of Case‑Specific Variability in RAG:** The RAG procedure relies on a general medical knowledge base without clearly accounting for inter‑case variability in image manifestations (e.g., heterogeneous presentations of atelectasis). Further explanation of how the system ensures annotation specificity is needed.  \n\n**Minor Comments**  \n- Clarify how ROI inference operates when non‑annotated images are processed.  \n- Discuss distinctions between this dataset and existing multigranular benchmarks more explicitly.  \n- Ensure consistent terminology for “multigranular” versus “multi‑level” annotations.  \n\n**Summary Paragraph (TMI Editorial Criteria)**  \n*Significance:* MedTrinity‑25M represents a substantial contribution due to its unprecedented scale and multimodal diversity.  \n*Innovation:* The automated ROI‑based pipeline and integration of expert models with MLLMs are innovative but require clearer validation of their advantages.  \n*Evaluation:* Current experiments support general usefulness but insufficiently address the specific value of multigranular annotation or comparison to peers.  \n*Reproducibility:* The workflow appears well described; however, additional implementation details on ROI generation and RAG customization would enhance transparency.  \n\n**Decision Recommendation**  \n**Major Revision.** The dataset’s scope is impressive, but stronger empirical evaluation of multigranular benefits, explicit ROI handling, and comparisons with existing benchmarks are required for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper presents MedTrinity-25M, a large-scale multimodal medical dataset with 25 million images across 10 modalities and detailed annotations for over 65 diseases. Enhanced by automated multigranular annotations, it supports tasks like image captioning and report generation. The LLaVA-Tri model, pre-trained on MedTrinity-25M, achieved state-of-the-art results on multiple medical VQA benchmarks.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The dataset uses an automated process with medical knowledge retrieval and domain-specific models, greatly reducing manual labeling needs.\n2. With over 25 million image-ROI-description triplets, the dataset supports classification, segmentation, report generation, and spans multiple medical domains.\n3. The LLaVA-Tri model, pre-trained on MedTrinity-25M, performs exceptionally well across multiple benchmarks, showcasing the dataset’s potential to enhance medical AI applications.\n\n### Weaknesses\n\n1. Although the labeling process is automated, its reliance on domain-specific models may limit scalability when handling new modalities or emerging diseases.\n2. The accuracy of automated labeling may fall short of expert-labeled datasets, potentially impacting performance in critical medical applications. How can a high standard of automated labeling be ensured?\n3. With data from over 30 sources, potential biases in image quality, demographics, or disease distribution call for a deeper integration analysis.\n4. Performance depends on external medical knowledge bases, risking inconsistency if not updated, and the LLaVA-Tri comparison may be biased due to overlap with MedTrinity-25M's data sources.\n\n### Questions\n\n1. Given the reliance on automated processes and external knowledge sources, how is labeling consistency ensured in the data generation process? Additionally, has it been validated by human experts?\n2. Has a comparison with expert-labeled datasets been considered to further quantify the quality of automated labeling?\n3. How does the dataset address potential biases in source data? For example, is there a mechanism to prevent overrepresentation of certain demographics?\n4. What is the expandability of the labeling process for new, unrepresented modalities or diseases? Does it offer strong scalability?\n5. Although LLaVA-Tri achieved good performance on RAD, PathVQA, and SLAKE, how do the authors ensure that MedTrinity-25M does not contain data from these datasets? If these datasets were included, the model may have seen the questions and answers during training, leading to artificially high accuracy.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents MedTrinity-25M, a comprehensive multimodal medical dataset composed of 25 million images across 10 modalities, with detailed annotations for over 65 diseases. This dataset is enhanced by automated multigranular annotations, facilitating various tasks including image captioning and report generation. Additionally, the LLaVA-Tri model, pre-trained on this dataset, demonstrates state-of-the-art performance on multiple medical visual question answering (VQA) benchmarks.\n\n### Major Comments\n1. The dataset benefits from an automated labeling process utilizing medical knowledge retrieval and domain-specific models, which significantly decreases the need for manual labeling.\n2. With the extensive volume of over 25 million image-ROI-description triplets, the dataset supports classification, segmentation, and report generation across multiple medical domains.\n3. The LLaVA-Tri model, trained on MedTrinity-25M, shows exceptional performance across various benchmarks, highlighting the dataset's potential for advancing medical AI applications.\n4. There are concerns regarding the scalability of the automated labeling process, particularly its dependence on domain-specific models when integrating new modalities or addressing emerging diseases.\n5. The accuracy of automated labeling may not match that of expert-labeled datasets, raising questions about the consistency and rigor of labeling processes and their implications on critical medical applications.\n6. The dataset's compilation from over 30 sources raises potential biases in image quality, demographics, or disease distribution, which warrants a detailed integration analysis.\n7. Performance is contingent upon external medical knowledge bases, leading to risks of inconsistency if these bases are not regularly updated. Furthermore, any overlap with the MedTrinity-25M data sources could lead to biased comparisons in LLaVA-Tri's performance.\n\n### Minor Comments\n- Clarification is needed regarding the validation of the automated labeling process by human experts.\n- A comparison with expert-labeled datasets should be considered to assess the quality of automated labeling more accurately.\n- The dataset should address how it mitigates potential biases from source data, especially concerning demographic representation.\n- Additional information regarding the scalability of the labeling process for new, unrepresented modalities or diseases would be valuable.\n- Authors should explicitly confirm that MedTrinity-25M does not overlap with RAD, PathVQA, and SLAKE datasets to avoid artificially high accuracy metrics.\n\n### Summary Paragraph\nIn summary, the manuscript introduces a significant contribution to the field of medical AI through the establishment of MedTrinity-25M, characterized by its large-scale, multimodal dataset that incorporates automated annotations. While the dataset demonstrates innovation and has the potential for high significance in medical applications, concerns arise regarding evaluation consistency, the reproducibility of results, and the potential biases present in the data. Addressing these weaknesses is crucial for enhancing the reliability of the dataset and its associated model.\n\n### Decision Recommendation\nMajor revision required.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MedTrinity-25M**, a large-scale multimodal medical dataset comprising approximately 25 million images across 10 imaging modalities, annotated for over 65 diseases. Through automated multigranular annotation integrating domain-specific models and medical knowledge retrieval, the dataset supports diverse tasks such as classification, segmentation, image captioning, and report generation. A companion model, **LLaVA-Tri**, pre-trained on MedTrinity-25M, demonstrates promising results on multiple medical visual question answering (VQA) benchmarks. The manuscript is clearly presented and technically sound, though aspects of scalability and labeling quality merit closer consideration.\n\n---\n\n**Major Comments**  \n1. **Scalability and Generalization:** The automated labeling framework relies heavily on domain-specific models and knowledge retrieval. This dependence may hinder scalability to new imaging modalities or emerging diseases. The authors should clarify how the current pipeline can accommodate such future extensions.  \n2. **Labeling Accuracy and Validation:** While automation reduces manual effort, the annotation accuracy may not match that of expert-labeled datasets. Quantitative and qualitative validation against human experts would strengthen confidence in labeling reliability.  \n3. **Bias and Data Integration:** The dataset integrates data from over 30 sources, which may introduce heterogeneity or demographic imbalance. More analysis is needed on potential source biases and their implications for downstream model performance.  \n4. **External Knowledge Dependence:** LLaVA-Tri’s performance depends on external medical knowledge bases that require regular updates. The manuscript should discuss how such dependencies are managed to ensure consistent model behavior.  \n5. **Benchmark Overlap:** Clarification is requested on whether data overlap exists between MedTrinity-25M and evaluation datasets (e.g., RAD, PathVQA, SLAKE), as such overlap could inflate performance metrics.\n\n---\n\n**Minor Comments**  \n- The manuscript would benefit from explicit mention of any manual checks conducted to validate automated labels.  \n- Ensure clear descriptions of all acronyms upon first use.  \n- Minor editorial adjustments could improve readability and alignment across tables and figures.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in providing a comprehensive multimodal medical dataset that could support a wide range of AI tasks. Its **innovation** lies in large-scale automated annotation integrating medical knowledge retrieval. The **evaluation** demonstrates strong benchmark results but would be more convincing with additional external validation and bias assessment. **Reproducibility** is supported by automation, yet further detail on data sources, annotation workflows, and quality assurance would enhance transparency.\n\n---\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The dataset and model are promising, but the manuscript requires deeper analysis of labeling quality, data bias, scalability, and validation to fully support its claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \nAlthough the manuscript is positioned at the intersection of imaging and AI, its central contribution is the creation of an extremely large **dataset (MedTrinity‑25 M)** and an **automated annotation pipeline** that leverages existing segmenters and multimodal LLMs.  The work is thus primarily *data resource– and system‑construction–driven*, rather than proposing a fundamentally new reconstruction, image‑analysis, or physics‑based methodology.  IEEE TMI’s emphasis is on methodological innovation in image acquisition, reconstruction, quantitative imaging, or core learning methodology; very large dataset papers of this type usually fall outside that scope.  \n➡ ⚠ Potentially below the TMI novelty threshold; more suitable for a data‑resource or multimodal foundation‑model venue.\n\n**2. Novelty & Contribution Level**  \nThe automation of text generation without paired reports and the combination of retrieval‑augmented generation with segmentation guidance are incremental extensions of recent general‑domain RAG+LLM pipelines.  The dataset scale (25 M triplets, 10 modalities) is unprecedented, but the methodological novelty is modest—no new image‑analysis theory or algorithm is introduced.\n\n**3. Technical and Experimental Rigor**  \nThe data‑processing description is detailed, including metadata integration, ROI localization via existing expert models, and external medical‑knowledge retrieval.  However, several aspects require clarification:  \n- Reproducibility of the automated pipeline and accessibility of code/checkpoints.  \n- Verification quality: only 200‑sample expert review—small relative to 25 M records.  \n- Institutional approvals and consent status for aggregated public data.  \n- Potential propagation of bias and error from automated ROI models or text generation.  \n- Absence of uncertainty quantification or inter‑rater agreement metrics.  \n\nThe demonstration experiment (pretraining LLaVA‑Tri) is technically sound but serves only to *validate dataset usefulness*, not a methodological advance per se.\n\n**4. Clarity and Presentation**  \nWriting is clear and figures effectively illustrate the pipeline.  Some grammar and typography issues remain (“A L ARGE -SCALE,” inconsistent spacing). Minor editing would improve readability.\n\n**5. Ethical and Reproducibility Compliance**  \nThe authors claim to have used only public data and to provide open access.  Explicit confirmation of data‑use licences, anonymization status, and IRB exemption should be added.  Code release for the generation pipeline is necessary for reproducibility.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe paper presents *MedTrinity‑25 M*, a large‑scale multimodal medical image dataset containing ≈25 million image–ROI–description triplets spanning 10 modalities and >65 diseases.  An automated two‑stage pipeline integrates metadata, ROI localization from expert segmentation models, and retrieval‑augmented large‑language‑model generation to produce “multigranular” textual annotations.  Pretraining LLaVA on this dataset (LLaVA‑Tri) yields state‑of‑the‑art results on three VQA benchmarks, illustrating the dataset’s value for multimodal medical AI.\n\n**2. Strengths**  \n- Ambitious scale and broad modality coverage.  \n- Well‑engineered automated pipeline that integrates established components (RAG + ROI guidance).  \n- Demonstrated downstream performance gain of existing models.  \n- Comprehensive qualitative and quantitative dataset analysis.\n\n**3. Weaknesses**  \n- Main novelty lies in data curation; limited methodological contribution to medical imaging science.  \n- Validation of generated captions/ROIs is shallow relative to dataset size.  \n- Ethical and licensing details incomplete.  \n- Benchmarking on VQA tasks does not establish clinical or quantitative‑imaging significance.  \n- Paper fits better as a dataset/resource release rather than a TMI methods paper.\n\n**4. Major Comments**  \n1. Clarify institutional approvals, de‑identification procedure, and source‑dataset licence compliance.  \n2. Provide public access to the full annotation‑generation pipeline (code + model checkpoints) and document reproducibility.  \n3. Expand quality‑control validation: larger expert review, inter‑observer agreement, and quantitative uncertainty.  \n4. Discuss potential biases introduced by automatic ROI detectors and LLM text generators.  \n5. To align with TMI scope, articulate *methodological* innovations—for example, formal formulation of the multigranular alignment or empirical analysis of annotation fidelity on quantitative imaging metrics.\n\n**5. Minor Comments**  \n- Correct typographical inconsistencies and spacing.  \n- Ensure consistent capitalization of dataset names (e.g., “MedTrinity‑25M”).  \n- Improve figure captions for stand‑alone readability.  \n- Add a limitation paragraph acknowledging non‑verified auto‑labels.\n\n**6. Overall Recommendation**  \n**Reject (Out of Scope / Low Novelty)**  \nWell‑executed dataset paper with potential community impact but lacks sufficient methodological innovation for IEEE TMI.  Could be resubmitted after adding significant imaging‑science contributions (e.g., formal framework for multigranular annotation validation or new foundation‑model training methodology).\n\n**7. Confidence Level:** 5 (High confidence)",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedTrinity‑25M*, a large‑scale multimodal medical image dataset containing approximately 25 million image–ROI–description triplets encompassing ten imaging modalities and more than sixty‑five disease categories. The authors develop an automated two‑stage annotation pipeline that integrates metadata harmonization, ROI localization via pre‑trained expert models, and retrieval‑augmented large‑language‑model generation to create “multigranular” textual descriptions. They pretrain *LLaVA‑Tri* on the resulting dataset, demonstrating improved performance on multiple VQA benchmarks. The manuscript is clearly written, and figures effectively illustrate the workflow, though minor editorial issues remain. Overall, this is a technically solid dataset‑creation paper, but its methodological innovation is limited relative to IEEE TMI’s focus.\n\n---\n\n**Major Comments**  \n1. **Scope and Novelty** – The central contribution lies in data resource construction rather than methodological advancement in image analysis, physics‑based reconstruction, or core learning theory. The novelty threshold appears below that expected for IEEE TMI.  \n2. **Validation and Quality Control** – The verification sample of 200 cases is insufficient compared with the 25 million records. The authors should expand expert review, quantify inter‑rater agreement, and introduce uncertainty measures for automated annotations.  \n3. **Ethical Compliance** – Clarify institutional approvals, data‑use licences, anonymization procedures, and IRB exemption status.  \n4. **Reproducibility** – Public release of code, model checkpoints, and generation pipeline documentation is essential for replication.  \n5. **Bias and Reliability** – Discuss possible error propagation and bias arising from pretrained ROI detectors and LLM text generation.  \n6. **Alignment with TMI Scope** – To fit the journal’s mission, the authors should articulate methodological innovations (e.g., formal modeling of multigranular alignment or quantitative fidelity analysis).\n\n---\n\n**Minor Comments**  \n- Correct typographical inconsistencies (e.g., spacing and capitalization of “MedTrinity‑25M”).  \n- Improve figure captions so that each can be understood independently.  \n- Add a limitation section acknowledging the reliance on automatically generated labels.  \n- Minor grammar and formatting corrections are needed.\n\n---\n\n**Summary (TMI Editorial Criteria)**  \n**Significance:** The dataset’s unprecedented scale and modality coverage have potential community value but limited direct imaging‑science significance.  \n**Innovation:** The integration of existing RAG and segmentation models is incremental, offering engineering rather than conceptual novelty.  \n**Evaluation:** Demonstrations show utility for downstream tasks but do not establish quantitative or clinical relevance.  \n**Reproducibility:** Methodological transparency is partial; code‑base and licensing confirmations are required for full reproducibility.\n\n---\n\n**Decision Recommendation:** **Reject (Out of Scope / Low Novelty)**  \nWhile *MedTrinity‑25M* is an impressive data resource with potential impact for multimodal medical AI, the paper lacks the methodological innovation expected for IEEE TMI. Resubmission to a venue focused on dataset or foundation‑model resources would be more appropriate.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript introduces *MedTrinity-25 M*, a large-scale multimodal medical image dataset comprising approximately 25 million image-ROI-description triplets across 10 modalities and over 65 diseases. Its primary contributions include an automated annotation pipeline that utilizes existing segmentation frameworks and multimodal large language models (LLMs). While this work showcases significant data resource capabilities, it does not present substantial methodological innovations in reconstruction or image analysis, which are critical for the IEEE TMI scope.\n\n### Major Comments\n1. The paper potentially falls below the TMI novelty threshold, making it more suited for a data-resource or multimodal foundation-model venue.\n2. The discussed automation of text generation and integration of retrieval-augmented generation with segmentation is a modest extension of current methodologies, lacking in new theoretical contributions.\n3. Several reproducibility concerns need addressing:\n   - Clarification on the accessibility of the automated pipeline, code, and model checkpoints.\n   - Verification quality is low with only a 200-sample expert review compared to the dataset size of 25 million.\n   - The manuscript should confirm institutional approvals and consent for the public use of aggregated data.\n   - Address potential biases introduced by automated ROI models or text generation and the absence of uncertainty quantification metrics.\n4. The results from pretraining LLaVA-Tri primarily validate the dataset’s utility rather than introduce novel methodologies.\n\n### Minor Comments\n- Typographical errors and inconsistent grammar, such as “A LARGE-SCALE” and spacing issues, should be corrected for improved readability.\n- Ensure consistent capitalization and formatting of dataset names throughout the text (e.g., \"MedTrinity-25M\").\n- Figure captions should be enhanced for clarity and stand-alone readability.\n- A limitation section should be included to acknowledge non-verified automatic labels.\n\n### Summary Paragraph\nIn relation to the TMI editorial criteria, *MedTrinity-25 M* presents a significant dataset resource, offering ambitious scale and modality coverage, alongside a well-designed automated pipeline. However, the methodological innovation is limited, and the ethical considerations regarding data use are insufficiently addressed. Validation efforts do not reflect the dataset's size, reducing the perceived impact on clinical significance. Overall, while the dataset has potential value for the community, it lacks the depth of innovation expected for a TMI publication.\n\n### Decision Recommendation\n**Reject (Out of Scope / Low Novelty)**  \nThe manuscript effectively presents a dataset with potential community benefits but does not meet the novelty and methodological innovation standards required by IEEE TMI. A re-submission may be viable if significant improvements or contributions to imaging science are incorporated. \n\n### Confidence Level\n5 (High confidence)",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Ce Zhou",
      "James Zou",
      "Juncheng Wu",
      "Lang Gao",
      "Lei Xing",
      "Sheng Liu",
      "Xianhang Li",
      "Yunfei Xie",
      "Yuyin Zhou",
      "Cihang Xie",
      "Hong-Yu Zhou"
    ],
    "url": "pdfs/iclr.cc-2025-conference_d35cf31b42d0db969be38ced93f86fe655f141fa.pdf",
    "remote_url": "https://openreview.net/pdf/d35cf31b42d0db969be38ced93f86fe655f141fa.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Human-aligned models",
      "robust neural networks",
      "visual perception",
      "perceptual learning",
      "medical machine learning"
    ],
    "abstract": "The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents L-WISE, a framework leveraging adversarially robust ANNs to estimate image difficulty and apply nuanced perturbations that facilitate human learning in visual categorization tasks. By selecting challenging images and amplifying category-specific features, L-WISE improves human categorization accuracy by 33-72% and reduces training time by 20-23% across both general and clinical domains (e.g., dermoscopy and histology). The authors also discuss ethical implications, emphasizing the benefits of enhanced medical training and cautioning against potential biases that may arise from reliance on model-derived guidance.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The application of Robustified ANNs for improving human visual performance on image categorization seems like an interesting avenue. \n2. L-Wise empirically demonstrate gains in categorization accuracy and training efficiency. \n3. The paper addresses ethics concerns. Since the work mentions use of clinical data ethics discussion is of critical importance.\n\n### Weaknesses\n\n1. The paper focused on the performance of ventral stream. But we know that the human visual stream has a dorsal stream (where) that locates an object and the ventral stream (what) stream. And the interplay of these two streams forms the basis of human visual system. In this work the authors mainly focused on the ventral stream. From only quantified data, we can see the gains but it is very hard to trace this back to the nuanced perturbations the ANN produces. Hence, the suggestion is to use human gaze. The human gaze will precisely pin-point the \"where\" aspect and then will truly help us understand if at all the model perturbations are helping improve human performance. Can the authors please explain this? \n2. A robust DNN actually has worse performance on nominal data points. Data points that have not been corrupted adversarially. What was the motivation of the authors to select such a model for their experiments? \n3. The perturbations -  The perturbations if I am not mistaken are very subtle ones. For fine grained classifications, humans do follow curriculum learning but learning structures gradually from simpler to harder concepts. No experiments have shown this. It would be great if the authors can provide some empirical results/ explanation that can explain how will their method occur when you focus on structural cues rather than model perturbations that will benefit fine grained categorization. \n4. I am providing some citations related to Gaze and dual stream hypothesis that can help authors clarify my concerns \n\na. A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains, NeurIPS 2023.\nb. Literature related to papers accepted in NeurIPS Gaze Meets ML workshop. That workshop accepted papers will provide intuition of how human gaze can be used in coherence with DL models.\n\n### Questions\n\n1. For model perturbations, can the authors please provide heat maps or any qualitative results that will help us track the ANN perturbations to human visual learning? \n2. Is there a way to show if the study scales/generalizes to other ANNs as well other than Adversarially trained ones? Since human subjects have been used here, I am not sure how feasible experiments will be. This can be a general neural network or a network trained by the CutMix [a] loss that provides robustness benefits as well. \n3. I feel generating perturbations based of dual stream networks and then using human gaze to track these will be a much stronger claim to the work. Can the authors please address this question? \n4. What about adversarially trained transformers? The attention maps are different from CNN feature maps. How will the study be applicable for perturbations based of a transformer backbone? \n5. Please also address concerns raised in the weakness sections. \n\na. CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019,\n\n### Flag For Ethics Review\n\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 6\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript introduces L-WISE, a framework that utilizes adversarially robust artificial neural networks (ANNs) to evaluate image difficulty and implement targeted perturbations, thereby enhancing human performance in visual categorization tasks. The authors report significant improvements in categorization accuracy (by 33-72%) and reductions in training time (by 20-23%) across general and clinical applications, including dermoscopy and histology. Additionally, the manuscript addresses pertinent ethical considerations linked to advancing medical training while acknowledging possible biases related to model-derived guidance.\n\n### Major Comments\n1. The study primarily investigates the performance of the ventral visual stream, neglecting the dorsal stream's role in object localization. Understanding the interplay between these streams is essential. The authors should incorporate human gaze metrics to elucidate the effects of ANN perturbations on human performance.\n2. The authors' choice of a robust deep neural network (DNN) warrants clarification, as such models may perform poorly on unperturbed nominal data. What was the rationale for selecting this model for experiments?\n3. The subtlety of the applied perturbations is intriguing, yet the paper lacks empirical evidence demonstrating how these perturbations align with the principles of curriculum learning, which advocates for gradually progressing from simpler to more complex concepts. Additional empirical results supporting the methodology would strengthen the argument.\n4. Relevant citations concerning gaze and the dual stream hypothesis are suggested to clarify concerns:\n   - \"A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains,\" NeurIPS 2023.\n   - Works from the NeurIPS Gaze Meets ML workshop that provide insights into integrating human gaze with deep learning models.\n\n### Minor Comments\n- Consider providing qualitative results, such as heat maps, to illustrate how ANN perturbations correlate with human visual learning.\n- Discuss the generalizability of the findings to other neural network architectures beyond adversarially trained ones. Given the involvement of human subjects, the feasibility of such experiments may be limited.\n- It may be beneficial to explore perturbations based on dual stream networks and use human gaze tracking to support the framework's claims.\n- Address how adversarially trained transformers could yield different attention maps compared to CNNs and the implications for perturbation applications.\n\n### Summary Paragraph\nThe study's significance lies in its innovative approach to enhancing human visual categorization through adversarially robust ANNs. However, the reliance on the ventral stream and the lack of empirical support for the perturbation method raise concerns regarding reproducibility and the broader applicability of findings. Ethical implications are appropriately acknowledged, emphasizing the need for responsible research practices in human subject studies.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **L-WISE**, a framework that employs adversarially robust artificial neural networks (ANNs) to estimate image difficulty and apply targeted perturbations intended to enhance human learning in visual categorization tasks. By emphasizing challenging images and amplifying category-specific features, the method reportedly improves human categorization accuracy (33–72%) and reduces training time (20–23%) across general and clinical domains such as dermoscopy and histology. The authors also include a thoughtful discussion on ethical considerations in clinical training contexts. Overall, the paper tackles an interesting problem, though several methodological and conceptual aspects require clarification.\n\n---\n\n**Major Comments**  \n1. **Biological plausibility and dual-stream modeling:** The study focuses on the ventral (object-identification) pathway of human vision, while neglecting the dorsal (spatial-localization) stream. Since the interplay between these streams underlies human visual processing, the exclusive focus on the ventral stream limits interpretability. The reviewer suggests incorporating or discussing human gaze data to connect model perturbations with spatial attention, thereby better explaining how perturbations enhance human performance.  \n2. **Choice of adversarially trained models:** Robust DNNs are known to underperform on nominal (non-adversarial) data. The rationale for selecting such models for the presented experiments should be explicitly justified.  \n3. **Perturbation design and curriculum learning:** The perturbations appear subtle. In fine-grained categorization, humans often benefit from structured or curriculum-based learning that progresses from simple to complex concepts. It would strengthen the study to include experiments or explanations exploring how L-WISE compares or aligns with such structural learning approaches.  \n4. **Generalization and alternative architectures:** Clarification is needed on whether the method generalizes beyond adversarially trained ANNs, for example to standard networks or those trained with alternative robustness methods (e.g., CutMix). Additionally, implications for transformer-based architectures with attention maps distinct from CNN feature maps should be addressed.  \n5. **Qualitative visualization:** Please provide qualitative evidence, such as heat maps, illustrating how perturbations relate to human learning effects.  \n6. **Ethics and gaze-related literature:** The reviewer recommends consulting dual-stream and gaze-related literature (e.g., NeurIPS 2023 “Dual-Stream Neural Network” and the “Gaze Meets ML” workshop) for theoretical and empirical grounding.\n\n---\n\n**Minor Comments**  \n- Minor editorial and clarity improvements are suggested to improve presentation, such as brief explanations of technical terms and consistent use of acronyms.  \n- Ethical discussion is appreciated and appropriately placed; ensure data-handling descriptions meet responsible research standards.\n\n---\n\n**Summary (TMI Criteria)**  \nThe paper addresses a **significant** question—improving human learning through model-guided visual perturbations—but its **innovation** is somewhat constrained by limited theoretical grounding in the full visual processing model. The **evaluation** demonstrates quantitative benefits but lacks qualitative linkage to cognitive mechanisms like gaze or spatial attention. **Reproducibility** is moderately supported but would benefit from added experimental detail and visualizations. Strengths include empirical gains and ethical awareness; weaknesses concern the neurobiological interpretation and model selection rationale.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a method to enhance human visual learning by designing a model-based selection and enhancement algorithm to improve classification accuracy during testing. First, the authors select images to present to novice learners based on a model’s estimated recognition difficulty for each image. Next, they apply image perturbations intended to aid recognition for novice learners. The authors conduct experiment on the three benchmark datasets, including the natural image and the clinical image to verify the effectiveness of their proposal.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper is well-motivated, and a decent amount of technical details are given.\n2. The idea of improving the categorization performance of the novice learner by leveraging the capacity of the robustified artificial neural network is both interesting and practical.\n3. The reported improvement in novice learners' performance is notable, with gains in both test accuracy and reduced training time.\n\n### Weaknesses\n\n1. The establishment of the empirical observations is somewhat unconvincing. Do these observations hold in more complex classification tasks or when applied to medical imaging?\n2. The related work section lacks discussion of both the machine teaching and human-machine vision alignment methods.\n3. The size of the particants is somewhat small. \n4. The perception of enhanced images may be altered due to perturbations.\n\n### Questions\n\n1. The empirical observations are derived from a 16-way animal categorization task on natural images, which seems somewhat simplistic. It would be valuable to examine how these observations hold up in more complex categorization tasks or with different types of images, especially medical images. Given the typically limited availability of medical images, the proposed method could have promising applications in the medical imaging field.\n\n2. Beyond the empirical observations, is there any physiological insight or analysis on why the proposed model-based selection and enhancement method could improve novice learners’ performance in categorization tasks?\n\n3. The authors do not discuss the related machine teaching literature. An in-depth comparison with machine teaching methods, particularly with \"Teaching Categories to Human Learners with Visual Explanations\" (CVPR 2018), would be valuable. This work similarly considers image difficulty; an introduction and comparison with it are beneficial.\n\n4. The authors should also discuss the connection to human-machine vision alignment methods, such as \"Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans\" (NeurIPS 2022).\n\n5. The sample size of participants is relatively small, and expanding the participant pool is recommended to enhance the reliability of the results; also, recruiting participants from diverse backgrounds would improve the generalizability of the findings. If expanding the participant pool is impractical due to time or budget constraints, performing a power analysis or discussing effect sizes could help strengthen the reliability of the analysis.\n\n6. When using perturbations to enhance images, how does the method ensure that essential image details remain unchanged, particularly when using a large ϵ (e.g., 20)? This concern is especially pertinent for medical images, where even slight pixel changes may alter critical information. I recommend involving medical experts to review the enhanced images or using quantitative similarity measures (such as SSIM or FSIM) to verify that essential details are preserved.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a novel approach to enhancing human visual learning through a model-based selection and enhancement algorithm aimed at improving classification accuracy for novice learners. The authors detail the selection of images based on estimated recognition difficulty, followed by the application of image perturbations to facilitate recognition. Experiments are conducted on three benchmark datasets, including natural and clinical images, to validate the proposed method's effectiveness.\n\n### Major Comments\n1. The empirical observations reported are somewhat unconvincing. Further examination is needed to determine if these findings hold true in more complex classification tasks or within medical imaging contexts.\n2. The related work section is lacking in discussions surrounding both machine teaching literature and human-machine vision alignment methods, which would enrich the manuscript.\n3. The sample size of participants in the study is relatively small, potentially limiting the generalizability of the findings.\n4. There are concerns that image perturbations could alter the perception of enhanced images.\n\n### Minor Comments\n- The simplistic nature of the 16-way animal categorization task raises questions about the robustness of findings in more complex scenarios.\n- The authors should explore physiological insights that could explain improvements in novice learner performance.\n- An introduction and comparison with machine teaching methodologies, particularly \"Teaching Categories to Human Learners with Visual Explanations\" (CVPR 2018), would strengthen the discussion.\n- Connections to the study \"Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans\" (NeurIPS 2022) should be explored.\n- There should be an emphasis on the need for a larger and more diverse participant pool to enhance the reliability and generalizability of results; if impractical, the authors could conduct a power analysis or discuss effect sizes.\n- It is critical to ensure that essential details in images remain unchanged while using perturbations, especially for medical images. Consulting medical experts or utilizing quantitative similarity measures (such as SSIM or FSIM) is recommended.\n\n### Summary Paragraph\nIn conclusion, the manuscript contributes to the field of visual learning by proposing an innovative method that harnesses advanced neural network techniques. The work is significant in its potential application to medical imaging; however, concerns regarding empirical validity, literature engagement, and participant diversity may affect its evaluation and reproducibility. The strengths include technical motivation and notable performance improvements reported, while weaknesses relate to the simplicity of the empirical observations and sample size. Addressing these issues could enhance the overall impact and reliability of the study.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a method to enhance human visual learning through a model-based selection and image enhancement algorithm designed to improve classification accuracy in testing. The approach selects images for novice learners based on a model’s predicted recognition difficulty and applies targeted perturbations to aid recognition. Experiments on three benchmark datasets, including both natural and clinical images, evaluate the method’s effectiveness. The paper is well-motivated and clear in its presentation, though some claims require stronger empirical or theoretical support.\n\n---\n\n**Major Comments**  \n1. **Empirical Validity:** The presented empirical findings, primarily derived from a 16-way animal categorization task, appear limited in scope. It is unclear whether they generalize to more complex classification problems or to medical imaging tasks, where data distribution and perceptual challenges differ substantially.  \n2. **Related Work Coverage:** The related work section lacks discussion of relevant research on *machine teaching* (e.g., “Teaching Categories to Human Learners with Visual Explanations,” CVPR 2018) and *human–machine vision alignment* (e.g., “Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans,” NeurIPS 2022). These comparisons are essential to contextualize and position the contribution.  \n3. **Participant Sample Size:** The participant pool is relatively small, limiting the reliability and generalizability of the results. Expanding the sample or conducting a power/effect size analysis would help validate the findings.  \n4. **Perturbation Effects:** The enhancement process may unintentionally distort perceptual content, especially when large perturbation magnitudes (e.g., ϵ = 20) are used. Verification by domain experts or quantitative similarity metrics (SSIM, FSIM) is recommended.  \n5. **Physiological Interpretation:** The paper would benefit from discussion of possible cognitive or perceptual mechanisms explaining why the proposed method improves novice performance.\n\n---\n\n**Minor Comments**  \n- Some sections could better distinguish between natural image and medical image experiments.  \n- Minor typographical and grammatical corrections are needed.  \n- A clearer description of participant demographics would improve transparency.\n\n---\n\n**Summary (TMI Editorial Criteria)**  \n- **Significance:** The problem of improving human visual categorization is meaningful, particularly for educational and clinical applications.  \n- **Innovation:** Combining model-driven selection with perturbation-based enhancement is novel, though related literature should be more deeply examined.  \n- **Evaluation:** Promising but limited—broader datasets, larger participant groups, and stronger statistical assessment are needed.  \n- **Reproducibility:** The methodology is described in reasonable technical detail, but reproducibility could be improved with additional quantitative validation and explicit data processing steps.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe study presents an interesting and potentially impactful idea but requires broader experimental validation, deeper comparison with prior work, and clearer justification of empirical findings.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes a novel approach to augment human learning in image categorization tasks. By leveraging robustified ANNs, the study introduces model-guided image selection and enhancement strategies that increase human test-time categorization accuracy by up to 72% and reduce training duration by around 20-23%. L-WISE employs selecting images based on predicted difficulty levels and enhancing images with pixel perturbations.The proposed approach is tested on natural images, dermoscopy, and histology images. The results demonstrates efficacy of L-WISE in aiding novice learners in fine-grained categorization tasks. This research represents one of the first applications of ANNs in optimizing human visual learning in clinically relevant domains.\n\n### Soundness: 1\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- Presents an innovative use of robustified ANNs to predict task difficulty and enhance images, aiding human perception and learning.\n- Shows broad applicability by successfully testing across diverse domains, such as natural image classification, dermoscopy, and histology.\n- Achieves practical efficiency by reducing training time and improving test-time accuracy, beneficial for fields requiring rapid, accurate human image categorization training.\n\n### Weaknesses\n\n- Lacks a dedicated related work section, which would help contextualize the research.\n- Both low and high logits from ANNs show significant variation in human accuracy, making predictions less reliable in certain logit intervals.\n- Uses only the ResNet-50 architecture, limiting generalization; further testing with models like vision transformers (ViT) is needed to support broader conclusions.\n- Image enhancement may introduce biases, potentially improving accuracy only for certain major classes; additional metrics like precision and recall per class, rather than just mean accuracy, should be reported to provide a clearer assessment.\n\n### Questions\n\n1. How to choose $\\epsilon$ for different tasks and image domains?\n2. What criteria determine if a model is \"robustified\" enough for use? Have you considered specific metrics to evaluate the robustness of guide models, and how do these metrics correlate with human learning outcomes?\n3. Did you collect qualitative feedbacks from participants? Did the new curriculum and enhanced images increase mental stress of human learners? Additional learning costs beyond training time should be considered, such as cognitive load and emotional well-being.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe use of ANN-guided models in human learning may introduce unintended biases, potentially affecting participants' learning outcomes in ways that favor certain demographic groups over others. For instance, if the training of ANNs inadvertently enhances accuracy for only a subset of the population (such as specific genders, ages, or races), it could lead to biased learning outcomes. Such biases could potentially skew related job opportunities, ultimately reinforcing inequities. Ensuring that ANNs benefit all demographic groups equitably requires further investigation and ongoing evaluation to mitigate these risks.\n\nThe above concern is addressed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript presents a novel approach aimed at augmenting human learning in image categorization tasks through the use of robustified artificial neural networks (ANNs). The study introduces model-guided image selection and enhancement strategies that significantly improve the accuracy of human categorization by up to 72% while decreasing training time by approximately 20-23%. This research explores the effectiveness of the proposed L-WISE methodology across various domains, including natural images, dermoscopy, and histology, marking an important contribution to the application of ANNs in enhancing human visual learning in clinically relevant areas.\n\n**Major Comments:**\n1. The manuscript lacks a dedicated related work section, which would provide necessary context for the research.\n2. Significant variability in human accuracy for both low and high logits from the ANNs raises concerns about the reliability of predictions within certain logit ranges.\n3. The use of only the ResNet-50 architecture limits generalizability; further testing with alternative architectures, such as vision transformers (ViT), is recommended for broader applicability.\n4. Image enhancement techniques may introduce biases that could skew accuracy improvements according to specific major classes; reporting additional metrics like precision and recall per class is suggested for a more comprehensive evaluation.\n\n**Minor Comments:**\n- Consider providing clarity on how to select the parameter $\\epsilon$ for different tasks and image domains.\n- Define criteria for determining the robustness of models and suggest metrics that could evaluate this while correlating with human learning outcomes.\n- Inquiry into whether qualitative feedback from participants was gathered regarding the impact of the enhanced images and curriculum on cognitive load and emotional well-being.\n\n**Summary Paragraph:**\nIn terms of significance, the study introduces innovative methodologies that potentially advance human learning processes in image categorization. However, its generalizability is limited by the architecture choice and lack of related work discussion. The evaluation of the approach demonstrates notable efficiency in training and accuracy; yet, concerns about biases in image enhancement and the need for comprehensive assessment metrics highlight areas requiring improvement. Reproducibility aspects could benefit from broader testing and transparent methodological choices.\n\n**Decision Recommendation:**\nMajor revision is recommended to address the outlined weaknesses and substantiate findings further.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *L-WISE*, a framework that leverages robustified artificial neural networks (ANNs) to enhance human learning in image categorization tasks. The method employs model-guided image selection based on predicted difficulty levels and pixel-level image enhancement to improve novice learners’ accuracy and efficiency. The approach is evaluated across natural image, dermoscopy, and histology datasets, showing improvements of up to 72% in test-time accuracy and a 20–23% reduction in training time. Overall, the study is clearly motivated and presents one of the first attempts to optimize human visual learning in clinically relevant settings using machine-guided augmentation.\n\n**Major Comments**  \n1. The manuscript lacks a distinct *related work* section. Integrating this would help situate the study within existing literature on human–machine collaborative learning and visual training augmentation.  \n2. The reliability of ANN-predicted logits varies: both low and high confidence predictions show fluctuations in corresponding human accuracy, which limits the stability of the model’s difficulty predictions.  \n3. The experiments rely solely on the ResNet-50 architecture. Additional validation using diverse backbones, such as Vision Transformers, would strengthen claims of generality.  \n4. The potential for bias in the image-enhancement process requires attention. Enhancements may improve accuracy predominantly for major classes. Reporting class-wise metrics (e.g., precision and recall) instead of only mean accuracy would allow for a more balanced assessment.  \n5. Further clarification is needed regarding (a) how the perturbation parameter \\(\\epsilon\\) is selected across domains, (b) what robustness metrics define a \"robustified\" model, and (c) whether ANN-guided training affects cognitive load or emotional stress in human participants—factors that influence practical deployment.\n\n**Minor Comments**  \n- Improve figure and table captions for clarity.  \n- Define abbreviations (e.g., ANN) at first appearance for consistency.  \n- Minor grammatical issues should be corrected.\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is significant in proposing a machine-assisted framework for human visual learning with potential relevance to clinical training. Its innovation lies in combining robustified ANNs with adaptive image selection and enhancement. The evaluation is promising but limited by the single-network setup and lack of detailed per-class metrics. Reproducibility could be improved by elaborating the selection of \\(\\epsilon\\), the robustness metrics used, and participant feedback protocols. Overall, the study demonstrates meaningful potential but would benefit from methodological and comparative expansion.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents L-WISE (Logit-Weighted Image Selection and Enhancement), a method to augment human visual category learning using robustified artificial neural networks (ANNs). The authors establish that robustified ANNs can predict image recognition difficulty for humans via ground truth logit values (Figure 1A) and generate image perturbations that enhance human categorization accuracy (Figure 1B). L-WISE combines two strategies: (1) selecting training images based on model-estimated difficulty with gradually increasing complexity, and (2) applying image enhancements that diminish over time. The method was tested on three tasks: moth species classification, dermoscopy lesion classification, and histology tissue classification. Results show 33-72% improvements in test-time accuracy relative to control groups and 20-23% reductions in training time (Figure 4A). The approach demonstrates effectiveness across varied image domains while maintaining generalization to unmodified test images.\n\n## Weaknesses\n\n• **Limited mathematical rigor in optimization formulation**\n  - Equation 1 lacks convergence analysis or theoretical guarantees for the projected gradient ascent procedure, making it unclear when the optimization terminates or reaches optimal solutions\n  - The choice of step size η = 0.5 and number of steps ceil(2ε) in Section S1 appears ad hoc without theoretical justification or sensitivity analysis\n  - The projection operator Projε in Equation 2 is not formally defined, though it appears to constrain perturbations to an ℓ2 ball of radius ε\n\n• **Insufficient experimental controls and statistical rigor**\n  - The study lacks proper multiple comparisons corrections across the numerous statistical tests performed (Figures 1, 3, 4, Table 1), potentially inflating Type I error rates\n  - Sample sizes vary considerably across conditions (Table 1: min. 27, max. 68 participants) without power analysis justification for these differences\n  - No control for potential confounding effects of task engagement or motivation differences between groups, despite evidence of differential dropout rates (Section S9)\n\n• **Limited scope and generalizability assessment**\n  - The three experimental domains (moths, dermoscopy, histology) represent a narrow slice of visual learning tasks, with no evaluation on more common educational scenarios\n  - The fixed curriculum schedules (Figures 2E-F) are not adaptive to individual learner progress, limiting practical applicability as acknowledged in Section 5.1\n  - No comparison with other established curriculum learning or educational technology approaches beyond basic image enhancement baselines (Figure 1B)\n\n• **Methodological concerns with human subject protocols**\n  - The 17-millisecond presentation time in the ImageNet animal task (Section S3) may not reflect realistic learning conditions and could favor certain types of visual processing\n  - The use of Greek mythological names as category labels (Section S4) introduces potential cultural bias and may not generalize across diverse populations\n  - Monetary incentives tied to performance (Section 6) could create differential motivation effects between experimental conditions that confound learning outcomes\n\n## Suggestions for Improvement\n\n• **Strengthen mathematical foundation and optimization analysis**\n  - Provide theoretical analysis of convergence properties for the projected gradient ascent in Equations 1-2, including conditions under which the optimization converges to meaningful solutions\n  - Conduct systematic sensitivity analysis for hyperparameters (step size η, number of steps, α weighting) with empirical validation across different model architectures\n  - Formally define the projection operator Projε and discuss computational complexity of the enhancement procedure\n\n• **Enhance experimental design and statistical methodology**\n  - Apply appropriate multiple comparisons corrections (e.g., Bonferroni, FDR) to all statistical tests and report corrected p-values throughout the results sections\n  - Conduct formal power analysis to justify sample size choices and ensure adequate statistical power for detecting meaningful effect sizes across all experimental conditions\n  - Include control conditions that account for potential confounds such as increased engagement, attention, or motivation effects independent of the actual image modifications\n\n• **Expand experimental scope and practical applicability**\n  - Evaluate L-WISE on a broader range of visual learning tasks including more common educational scenarios (e.g., medical imaging beyond the current domains, scientific diagram interpretation)\n  - Develop and test adaptive curriculum schedules that adjust difficulty progression and enhancement levels based on individual learner performance rather than fixed schedules\n  - Compare against established curriculum learning methods and educational technologies to position L-WISE within the broader landscape of learning augmentation approaches\n\n• **Improve ecological validity and reduce potential biases**\n  - Test the approach with more realistic presentation times and learning conditions that better reflect natural educational settings rather than millisecond-duration exposures\n  - Validate the method across diverse populations and cultural contexts, moving beyond Greek mythological labels to more universally accessible category naming schemes\n  - Implement experimental designs that control for motivational effects, such as providing equivalent monetary incentives across all conditions or using non-monetary engagement strategies",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces L-WISE (Logit-Weighted Image Selection and Enhancement), a novel method aimed at improving human visual category learning through robustified artificial neural networks (ANNs). The authors demonstrate that these networks can predict image recognition difficulty using ground truth logit values and generate perturbations that enhance human categorization accuracy. The method applies a dual strategy of selecting training images based on estimated difficulty and providing temporal image enhancements. The evaluation across moth species, dermoscopy lesion, and histology tissue classification tasks shows significant improvements in accuracy and reductions in training time, showcasing effectiveness across varied domains and generalization to unmodified images. \n\n**Major Comments:**\n1. **Mathematical Rigor:** There is a lack of convergence analysis in the optimization formulation. The step size and number of steps appear arbitrary without justification.\n2. **Experimental Controls:** The study suffers from inadequate multiple comparisons corrections and varying sample sizes across conditions without justification from power analysis.\n3. **Generalizability:** The limited experimental domains restrict the applicability of findings to broader visual learning tasks, and the non-adaptive curriculum limits practical relevance.\n4. **Methodological Concerns:** The presentation time may not be realistic and could bias results. Cultural bias in category naming and potential motivation effects from performance incentives could confound outcomes.\n\n**Minor Comments:**\n- Consider adding a formal definition for the projection operator in Equation 2.\n- Improve clarity in the reporting of statistical tests and ensure corrected p-values are included.\n- Review for typographical errors and consistency in notation throughout the manuscript.\n\n**Summary Paragraph:**  \nIn relation to TMI's editorial criteria, the manuscript presents significant findings with substantial innovations in visual learning through neural networks. However, concerns arise regarding the evaluation design, particularly in terms of statistical rigor and experimental controls, limiting the findings' reproducibility and generalizability. Addressing the highlighted weaknesses will enhance the work's impact and applicability in educational contexts.\n\n**Decision Recommendation:** Major Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **L-WISE (Logit-Weighted Image Selection and Enhancement)**, a method designed to augment human visual category learning through robustified artificial neural networks (ANNs). The approach combines image selection based on model-predicted difficulty and progressively diminishing image enhancements to facilitate more efficient learning. Experiments across three domains—moth species, dermoscopy lesions, and histology tissue classification—demonstrate substantial improvements in test accuracy (33–72%) and reductions in training time (20–23%) compared with control groups. The manuscript is generally clear and well-structured, with strong empirical results, though several methodological, theoretical, and experimental limitations temper the overall rigor.  \n\n**Major Comments**  \n1. **Mathematical and Theoretical Foundations:**  \n   - Equation (1) lacks formal convergence analysis or guarantees, making it unclear whether the projected gradient ascent reliably attains optimal solutions.  \n   - Hyperparameter settings (step size η = 0.5; number of steps ceil(2ε)) appear heuristic without theoretical justification or sensitivity study.  \n   - The projection operator Projε is undefined, though implied to constrain perturbations within an ℓ₂ ball. A formal definition and discussion of computational implications are needed.  \n\n2. **Experimental Controls and Statistical Rigor:**  \n   - Absence of multiple-comparisons correction across numerous tests (Figures 1, 3, 4, Table 1) raises concerns about inflated Type I error.  \n   - Unequal sample sizes (range: 27–68) lack power analysis justification.  \n   - Potential confounds—such as variable engagement or motivation—are unaddressed despite differential dropout rates.  \n\n3. **Scope and Generalizability:**  \n   - Experimental tasks cover a narrow set of domains with limited educational relevance.  \n   - Fixed curriculum schedules cannot adapt to individual learner progression.  \n   - Baselines exclude comparisons with other curriculum-learning or instructional methods.  \n\n4. **Human Subject and Ecological Validity Concerns:**  \n   - Extremely short presentation times (17 ms) may distort ecological validity.  \n   - Use of culturally specific category labels could bias results.  \n   - Performance-based monetary incentives may introduce motivational confounds.  \n\n**Minor Comments**  \n- Provide clearer definitions for all mathematical symbols in the supplement.  \n- Explain the rationale for presentation timing and label selection.  \n- Standardize figure references and ensure statistical tables report corrected p-values.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is innovative in linking ANN-derived difficulty estimates with human training, offering potential significance for human–AI collaborative learning. However, theoretical underpinnings are underdeveloped, experimental designs lack certain statistical controls, and generalizability remains untested beyond the studied tasks. While empirical outcomes are promising, reproducibility and robustness would be strengthened with fuller methodological transparency and theoretical justification.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n⚠️ The paper, *“L‑WISE: Boosting Human Visual Category Learning through Model‑Based Image Selection and Enhancement”*, focuses on human learning enhancement and neuro‑inspired computer vision rather than methodological advances in medical image acquisition, reconstruction, or quantitative analysis. While two of the experiments use dermoscopy and histology images, the work remains centered on *human behavioral training* and *psychophysics* rather than algorithmic innovation for medical image interpretation. The work could interest education or cognitive science venues, but its translation to clinical imaging practice is indirect. Therefore, the manuscript lies partially outside the principal methodological scope of *IEEE Transactions on Medical Imaging*.\n\n2. **Novelty & Contribution Level**  \nThe methodological novelty lies in coupling “robustified” adversarially trained ANNs with image‑wise logit metrics to (i) estimate human recognition difficulty and (ii) generate subtle pixel‑space perturbations that aid human category learning. That combination is creative and fairly novel. However, the methodological advance is primarily in psychovisual learning design, not in imaging science per se. For TMI, the novelty regarding medical imaging methodology (e.g., reconstruction, quantitative modeling, learning theory) is modest.\n\n3. **Technical and Experimental Rigor**  \nThe work is carefully executed, with extensive human‑subject experiments, ablations, and statistical reporting. Ethical oversight, reproducibility, and dataset sources are clearly stated. The main concerns are (i) limited sample sizes per condition, (ii) reliance on 2‑D natural and clinical imagery with no demonstration that the perturbations preserve medically relevant structures, and (iii) the potential for dataset bias (acknowledged by the authors) in dermoscopy examples.\n\n4. **Clarity and Presentation**  \nThe manuscript is clearly written, figures are legible, and methods are described in reproducible detail. The structure follows modern ML convention (ICLR style) more than IEEE TMI format but is understandable.\n\n5. **Ethical and Reproducibility Compliance**  \nIRB approval and participant consent are documented. Source code and hyperparameters are promised, which supports reproducibility. Data used (HAM10000, MHIST) are public; de‑identification is acknowledged.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n1. **Summary**  \nThe authors propose *L‑WISE*, a human‑learning augmentation framework that uses adversarially trained (“robustified”) neural networks to guide image‑based instruction. The model’s ground‑truth logit value is used as a proxy for visual recognition difficulty and to generate mild image perturbations emphasizing class‑relevant features. Over several psychophysical experiments—on natural objects, dermoscopy images, and histology patches—the approach produced 30–70 % relative accuracy gains and 20 % reductions in training time compared with naïve learning. The authors argue this is the first demonstration that model‑based image modification can measurably accelerate *human* visual category learning.\n\n2. **Strengths**\n- Creative integration of robust‑model gradients for targeted image modification aligned with human perception.  \n- Thorough set of behavioral experiments with ablation analysis.  \n- Inclusion of clinically related image domains (dermoscopy, histology).  \n- Transparent reporting, ethical compliance, and public‑code commitment.\n\n3. **Weaknesses**\n- Primary contribution is *application‑level* (human learning enhancement), not a methodological advance in medical image formation, reconstruction, or analysis.  \n- Validation on medical images measures human learning speed, not diagnostic performance or imaging‑physics improvement.  \n- The perturbation process could alter pathology‑relevant cues—no assurance that enhanced images remain clinically faithful.  \n- The connection to quantitative imaging science (noise modeling, uncertainty, radiomics, etc.) is weak.  \n- Limited discussion of generalization to 3‑D modalities, radiology reading, or algorithmic interpretability in clinical settings.\n\n4. **Major Comments**\n- **Scope alignment:** Clarify how L‑WISE advances methodological understanding in medical imaging beyond psychovisual training. If the goal is educational technology for radiology/pathology trainees, emphasize this translational educational aspect rather than implying improvements to image analysis itself.  \n- **Clinical evaluation:** Demonstrate that perturbations preserve diagnostically critical features (e.g., lesion borders in dermoscopy, gland architecture in histology). Quantitative fidelity metrics or expert ratings could address concerns about misleading visual cues.  \n- **Comparative baseline:** Include comparisons with standard curriculum‑learning or data‑difficulty scheduling methods applied directly to *machine* classifiers trained on the same datasets, to show whether the proposed human‑oriented design yields fundamentally different structure.  \n- **Statistical power and reproducibility:** Provide number of subjects per task in the main text, not only appendices; discuss inter‑participant variability and effect sizes with confidence intervals.  \n- **Potential bias:** Expand the discussion on distributional and demographic limits of datasets, especially skin‑tone imbalance.\n\n5. **Minor Comments**\n- Adapt the manuscript style to IEEE TMI formatting and citation standards.  \n- Clarify whether adversarial fine‑tuning was 2‑D only and discuss computational cost.  \n- Suggest including supplemental material illustrating perturbations on medically relevant structures at multiple ε values.  \n- Some figure captions are lengthy; abbreviate for print readability.\n\n6. **Overall Recommendation**\n**Reject (Out of Scope / Low Novelty for TMI)**  \nThe study is methodologically sound and novel within cognitive and machine‑learning fields, but its core contribution—using robust vision models to facilitate *human category learning*—is not a methodological advancement in *medical imaging science* as defined by IEEE TMI. The work fits better in venues for computational cognitive science, human–AI interaction, or medical‑education technology (e.g., *IEEE Transactions on Human–Machine Systems*, *Medical Image Analysis* if extended to automated modeling).\n\n7. **Confidence Level:** 5 / 5  \n(High confidence in technical and scope assessment.)",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript *“L‑WISE: Boosting Human Visual Category Learning through Model‑Based Image Selection and Enhancement”* introduces a framework aimed at improving *human* visual category learning through model‑based image perturbation. Using adversarially trained neural networks, the authors estimate recognition difficulty and subtly modify images to accelerate human learning. The paper is carefully executed, with clear writing and organized presentation. However, its primary focus is psychovisual learning rather than methodological innovation in medical image analysis, placing it only partially within the traditional scope of *IEEE Transactions on Medical Imaging (TMI)*.\n\n---\n\n**Major Comments**  \n1. **Scope Alignment:** The manuscript’s contributions emphasize human training and perceptual learning rather than advances in imaging methodology. The authors should clarify how L‑WISE contributes to medical imaging science or reframe the study as an educational tool for clinical training.  \n2. **Clinical Fidelity:** Demonstrate that image perturbations preserve diagnostically critical structures (e.g., lesion borders, gland morphology). Including quantitative fidelity checks or expert ratings would strengthen confidence that modifications do not distort medical features.  \n3. **Baseline Comparison:** Add comparisons with curriculum‑learning or difficulty‑based scheduling approaches for machine classifiers to determine whether the proposed human‑oriented perturbations offer unique structural benefits.  \n4. **Statistical Power:** Report subject counts per condition in the main text and include measures of variability and confidence intervals to improve transparency of psychophysical effects.  \n5. **Dataset Bias:** Expand discussion of potential distributional bias in dermoscopy datasets (e.g., skin‑tone imbalance) and its implications for generalization.  \n6. **Relevance to 3‑D/Clinical Domains:** Discuss whether the method extends to volumetric modalities or clinical diagnostic contexts beyond 2‑D imagery.\n\n---\n\n**Minor Comments**  \n- Adjust manuscript formatting and references to follow IEEE TMI guidelines.  \n- Clarify whether adversarial fine‑tuning was restricted to 2‑D data and detail computational cost.  \n- Consider providing supplemental illustrations of perturbations on medical images at different ε values.  \n- Condense figure captions for readability.\n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant for cognitive and human‑learning research, showcasing an innovative intersection of adversarial robustness and perceptual training. Nonetheless, its innovation relative to core TMI topics—reconstruction, quantitative analysis, or diagnostic modeling—is limited. Evaluation of human performance is thorough and ethically compliant, yet lacks direct clinical performance validation. Reproducibility is well‑addressed through public datasets, ethical approval, and planned code release. Overall, while scientifically sound and creative, the work sits outside TMI’s methodological scope and provides limited advancement for imaging science.\n\n---\n\n**Decision Recommendation:** **Reject (Out of Scope / Low Methodological Novelty for TMI)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript titled *“L‑WISE: Boosting Human Visual Category Learning through Model‑Based Image Selection and Enhancement”* investigates human learning enhancement and employs neuro‑inspired computer vision techniques. Its relevance to medical imaging is indirect, primarily focusing on human behavioral training and psychophysics. While the work presents interesting ideas, it tends to deviate from the core methodological focus of *IEEE Transactions on Medical Imaging*.\n\n### Major Comments\n1. **Scope Alignment:** The manuscript should clarify how the proposed L‑WISE framework advances methodological understanding in medical imaging beyond psychovisual training. Emphasizing its educational technology aspect for radiology or pathology trainees may help align the work with TMI's scope. \n2. **Clinical Evaluation:** Include evidence that the perturbations preserve critical diagnostic features, such as lesion borders in dermoscopy and glandular architecture in histology. Metrics or expert evaluations could address potential misleading visual cues. \n3. **Comparative Baseline:** It is crucial to compare L‑WISE with standard curriculum‑learning or data‑difficulty scheduling methods applied to machine classifiers on the same datasets, illustrating whether the proposed design yields fundamentally different outcomes.\n4. **Statistical Power and Reproducibility:** Report the number of subjects per condition in the main text. Discuss inter-participant variability and include effect sizes with confidence intervals to enhance reproducibility.\n5. **Potential Bias:** Address the distributional and demographic limits of datasets, particularly regarding skin-tone imbalances acknowledged in the manuscript.\n\n### Minor Comments\n- Adjust the manuscript to conform to IEEE TMI formatting and citation standards. \n- Clarify whether adversarial fine-tuning was limited to 2-D and discuss related computational costs.\n- Consider adding supplemental material illustrating perturbations on relevant structures across multiple ε values.\n- Some figure captions are lengthy; abbreviate them for enhanced print readability.\n\n### Summary Paragraph\nIn summary, L‑WISE presents a framework aimed at enhancing human learning through the integration of adversarially trained neural networks. While the approach demonstrates significant relative accuracy gains in several experiments, its primary contributions are more aligned with educational technology than methodological advances within medical imaging as per TMI's criteria. Strengths include creative integration and rigorous experimental design, whereas weaknesses relate to limited direct applicability to medical imaging practice, insufficient validation, and a need for deeper exploration of algorithmic interpretability.\n\n### Decision Recommendation\n**Reject (Out of Scope / Low Novelty for TMI)**  \nAlthough the manuscript is methodologically robust within cognitive science and machine learning fields, its essential contributions—centered on human category learning—do not constitute a methodological advancement in medical imaging, thus positioning it better in alternative venues such as computational cognitive science or medical education technology.\n\n### Confidence Level\n5 / 5  \n(High confidence in technical and scope assessment.)",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Gabriel Kreiman",
      "Guy Gaziv",
      "James J. DiCarlo",
      "Morgan Bruce Talbot"
    ],
    "url": "pdfs/iclr.cc-2025-conference_f92d03707a21b34104857495b389885b58b8ecc6.pdf",
    "remote_url": "https://openreview.net/pdf/f92d03707a21b34104857495b389885b58b8ecc6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Diffusion based models",
      "Self-supervised MRI denoising"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a ``microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks. Code is available at https://github.com/FouierL/Di-Fusion.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a new self-supervised learning-based denoising method for diffusion MRI (dMRI). The proposed method leveraged the diffusion modeling concept, but instead of training a diffusion model with “clean images” as x_0 and noise as x_T, it utilized two diffusion weighted images (DWIs) with different diffusion encodings at both ends of a “diffusion-like” process. A denoising network was trained by predicting one DWI using a linear combination of two DWIs and an added noise term. The linear combination coefficients are time-dependent and determined via a scheduling strategy similar to training a diffusion model. The network was then used for a conditional sampling step for generating the final denoised images. The idea to utilize images acquired with different diffusion encodings to denoise one of them is interesting and the training strategy is an interesting approach to leverage the diffusion modeling concept, especially with training only latter diffusion steps to reduce hallucinations. However, several key assumptions made are questionable and the overall methodology and presentation lacks clarity. Evaluation using only dMRI signal model goodness of fit is limited and can be biased. There are a few overstatements that can mislead the readers. Detailed comments can be found below.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nA diffusion-like modeling that learns the relationship between two DWI volumes with different diffusion encodings to denoise one or each other.\n\nTraining only later step diffusion to avoid hallucination\n\nA fusion strategy that exploits linear combination of two DWIs with different contrasts with time-dependent coefficients and iterative refinement.\n\nExtensive evaluations using both simulations that exactly followed the assumptions for the proposed methodology and practical magnitude DWI data.\n\n### Weaknesses\n\nThere are statements that can be misleading in the context of MR physics (aka domain knowledge). For example, \"the noise predominantly originates from physical interferences (Fadnavis et al., 2020a)\". This statement about physical interferences is  both vague and inaccurate. This work is dealing with thermal noise or noise resulting from thermal noise in the measurements, which is not really physical interferences depending on how ones interpret them. Another example, \"Different clinical applications require varying numbers of diffusion vectors and acquisition strategies, which makes modeling the noise distribution and further implementing denoising techniques challenging\". Acquiring DWIs with varying numbers of diffusion vectors had nothing to do with the difficulty of  modeling noise distribution.\n\nMany key assumptions for the proposed method was built on do not hold which made the theoretical/mathematical foundations questionable, e.g.,\na) It seems that the authors assumed DWIs acquired with different diffusion encodings had the same underlying “clean” image and were corrupted by independent noise. This is inaccurate. In fact, two DWIs can have rather different contrasts due to the diffusion encoding effects, e.g., different diffusion encoding directions. More specifically, x and x’ cannot be simply modeled as the same y plus different noise. What are the implications of this assumption not met?\n\nb) Line 111: The authors claimed that that the proposed method does not require an explicit noise model. This is an overstatement. The J-invariance assumption, which formed the basis of the training objective in Eq. (9) implicitly requires that the noise distribution be zero-means and conditionally independent. Furthermore, additive noise model was assumed, x = y + n1 (Line 200). In dMRI, the magnitude images with higher b-values (stronger diffusion weightings) can have lower SNR for which additive noise may not hold. These need to be clarified.\n\n-  Overall, the presentation lacks clarity and there seem to be some concerning inaccuracies.\na) The linear combination relationship claimed in Section 3.1 does not seem accurate. I checked the derivation. Eq. 31 is correct which is known (so this is not a contribution of the authors), but I'm not sure about going from Eq. 31 to 32 as F_theta predicts x_0, but they are not equal, and there is also an additional term of sigma_t^2*z. Therefore, I don't think it's a correct statement to say x_(t-1) is a linear interpolation between x_out and x_t. But is this really needed for the proposed method? I really don’t see a connection between what’s argued theoretically and what’s actually being implemented.\n\nb) There are a few other inaccurate mathematical statements and notations which are confusing. For example, Eq. 7, the left side has q(x1:T |xt*) which is a joint distribution for x1 to xT, and the right side is a Gaussian distribution for xt. \nOn Line 160: {xt}1:T was described as”obtained from the reverse process.” However, in\nFigure 1 and on the right side of Equation (7) on Line 186, it appears that xt is a corrupted version of xt*.  This interpretation, along with the notation in the Fig. 1, implies that {xt}1:T would represent a forward process. It appears to this reviewer the authors had not been using a consistent definition of forward and reverse diffusion which made the overall description rather confusing. These are just examples of inconsistencies found.\n\nc) According to the J-invariance property, the noise should ideally have zero mean\nand be conditionally independent of the target output. This requirement is necessary to ensure that the expected loss for self-supervised training asymptotical approaching the supervised loss. However, the input to F(.) in Eq. (9) includes xt*, which is a linear combination of x and x’ (Eq. (6)). Given that x serves as the supervision signal for the loss, this implies a correlation between the input x∗t and the target x, which would violate the conditional independence requirement for J-invariance.\n\n### Questions\n\nIn Eq. (5) on Line 155, the authors highlighted a specific term as the ”major difference” between xt−1 and x^bar_t−1. Could the authors clarify why this particular term is considered the primary source of difference? Furthermore, can the authors elaborate on the underlying reason(s) for the “drift” in the model and how it emerges during the reverse diffusion process?\n\nAccording to the definition of the Fusion process in Eq. (6)  and the “forward process” in Eq. (7), it appears that the starting point for the forward process changes based on t, as x_t* is dependent on t. This dependence implies that the Fusion process dynamically adjusts the starting point of the forward process at each step, which is unconventional compared to typical diffusion models. Could the authors clarify the rationale behind this design?\n\nOther more recent self-supervised denoising methods should be compared, if not for all, e.g., Noise2Score and Recorrupted2Recorrupted etc.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript introduces a novel self-supervised learning-based denoising method tailored for diffusion MRI (dMRI). By utilizing two diffusion weighted images (DWIs) with varying diffusion encodings, the authors propose a method that predicts one DWI from a linear combination of the two, incorporating noise. While the concept of leveraging different diffusion encodings is intriguing and offers a fresh perspective on denoising, several key assumptions in the methodology raise questions about its clarity and validity.\n\n### Major Comments\n1. **Questionable Assumptions**: The theoretical foundations seem flawed, as the authors assume that DWIs obtained with different encodings share the same underlying “clean” image and are affected by independent noise, which is inaccurate. Such an assumption leads to potential inaccuracies in the modeling.\n   \n2. **Clarity of Noise Model**: The claim that the method doesn't require an explicit noise model is overstated; the J-invariance assumption requires specific characteristics of the noise distribution that may not hold true in dMRI applications, particularly regarding additive noise constraints.\n   \n3. **Inconsistencies in Presentation**: Mathematical derivations, particularly from Eq. 31 to Eq. 32, lack foundational accuracy and clarity. The use of inconsistent definitions regarding forward and reverse diffusion processes creates confusion in the overall description.\n\n4. **Misleading Statements**: Certain statements regarding MR physics appear vague or inaccurate, potentially misguiding readers on the nature of noise in dMRI.\n\n### Minor Comments\n- The notation used in Eq. 7 is inconsistent; the joint distribution vs. Gaussian distribution needs clarification.\n- Descriptions in Figure 1 and Equation (7) need consistency regarding the definition of the diffusion processes.\n- Typographical and mathematical inaccuracies are present throughout, particularly regarding statistical definitions and conditions.\n\n### Summary Paragraph\nThe manuscript presents significant innovation through its unique use of DWIs for denoising in dMRI. However, the soundness of its methods is compromised by questionable assumptions and inconsistencies that undermine clarity. While the contribution is noteworthy, the evaluation is limited and may lead to biased interpretations. Overall, the manuscript exhibits potential but requires substantial refinement to meet the editorial criteria of significance, innovation, evaluation, and reproducibility.\n\n### Decision Recommendation\nMajor revision is recommended to address the critical issues raised regarding assumptions, clarity, and presentation of the methodology.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a self‑supervised denoising framework for diffusion MRI (dMRI) that adapts the concept of diffusion modeling. Instead of training a standard diffusion model between clean and noisy data, the method uses two diffusion‑weighted images (DWIs) with different diffusion encodings at the ends of a “diffusion‑like” process. A neural network is trained to predict one DWI from a time‑dependent linear combination of the two DWIs with added noise, followed by a conditional sampling step for denoising. The idea of leveraging multiple diffusion encodings for denoising is interesting; however, the paper suffers from unclear formulation, questionable assumptions, and limited and potentially biased evaluation. Several statements are scientifically inaccurate or misleading, and the overall presentation lacks precision.  \n\n**Major Comments**  \n1. **Questionable Assumptions:** The framework assumes that DWIs acquired with different diffusion encodings share the same underlying “clean” image with independent noise, which is physically inaccurate as DWIs can exhibit substantially different contrasts depending on encoding direction. The implications of this violated assumption are not addressed.  \n2. **Implicit Noise Model Dependence:** Although the authors claim the method does not require a noise model, the formulation inherently assumes additive, zero‑mean, and independent noise through the J‑invariance property. These assumptions may not hold for magnitude dMRI data with low SNR and should be clarified.  \n3. **Mathematical Inconsistencies:** Several equations (e.g., Eqs. 7, 31–32) and notations appear inconsistent or incorrect. The derivation of the linear interpolation relationship and the definitions of forward vs. reverse diffusion are unclear and inconsistent between text and figures.  \n4. **Violation of J‑Invariance Conditions:** The inclusion of \\(x_t^*\\), a linear combination of x and x′, as input while using x as supervision introduces correlation that violates the conditional independence requirement central to J‑invariance.  \n5. **Evaluation Limitations:** Performance is assessed only via dMRI signal model goodness‑of‑fit, which can be biased and insufficient to support general claims. Comparisons with more recent self‑supervised denoising methods (e.g., Noise2Score, Recorrupted2Recorrupted) are missing.  \n\n**Minor Comments**  \n- Some MR physics statements are inaccurate or misleading (e.g., mischaracterizing noise origins as “physical interferences”).  \n- Theoretical derivations and notation (particularly in Sections 3.1 and 3.2) need clearer, consistent explanations.  \n- Figures and equations should align in terms of symbol definitions.  \n- Typographical and clarity issues at specific line references (e.g., Lines 111, 155, 160, 186) should be corrected.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper addresses an important problem in dMRI denoising and offers an innovative adaptation of diffusion‑model training concepts in a self‑supervised context. However, the approach rests on debatable assumptions about DWI relationships and noise properties, undermining its theoretical soundness. Presentation and derivations contain inconsistencies that hinder reproducibility. Evaluation is limited and does not convincingly demonstrate practical benefits or generalizability. Overall, while the idea is creative, the manuscript requires substantial revision for soundness and clarity before it can meet standards of significance, innovation, evaluation, and reproducibility.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces Di-Fusion, a fully self-supervised diffusion MRI (dMRI) denoising method designed to enhance the signal-to-noise ratio (SNR) of MRI data without requiring clean reference data. The authors leverage novel late diffusion steps and an adaptive sampling process to create a single-stage framework that operates without an explicit noise model. Di-Fusion demonstrates superior performance over state-of-the-art denoising methods in tasks such as microstructure modeling and tractography. The method’s efficacy is validated through extensive quantitative and qualitative evaluations on real and simulated data.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- **Flexibility with data and noise models**: Instead of relying on explicit noise models or clean training data, the method relies on an N2N training strategy and pixel shuffling to reorganize the noise, providing strong generalization potential across different noise distributions. This suggests that the method has the potential to be applied to a wider range of denoising scenarios, such as cryo-EM.\n- Compared to the current state-of-the-art method, DDM^2, this approach demonstrates comprehensive improvements. Not only does it outperform in terms of performance, but it is also simpler to implement. Notably, this method does not require additional denoiser training, significantly enhancing its practical usability.\n- As a study on dMRI denoising, this paper conducts thorough and comprehensive experiments, including extensive comparisons and analyses on downstream task performance. This renders the work methodologically and experimentally well-rounded.\n\n### Weaknesses\n\nPlease refer to the **Questions** section for details.\n\n### Questions\n\n- To my understanding, the primary goal of dMRI denoising is to reduce the number of gradients required during acquisition, thus accelerating DWI scanning. In downstream tasks based on DTI, the authors compare DTI metrics computed from noisy images with those from denoised images. Why did the authors not use more DWI data to compute a clean DTI metric as a reference for comparison?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents Di-Fusion, a fully self-supervised diffusion MRI (dMRI) denoising method that aims to improve the signal-to-noise ratio (SNR) of MRI data without the necessity of clean reference data. By utilizing innovative late diffusion steps and an adaptive sampling process, the authors propose a single-stage framework that operates without an explicit noise model. Di-Fusion shows enhanced performance compared to state-of-the-art denoising methods specifically in microstructure modeling and tractography, with extensive quantitative and qualitative evaluations substantiating its effectiveness on both real and simulated datasets.\n\n### Major Comments\n1. **Flexibility**: The method does not depend on explicit noise models or clean training data, employing an N2N training strategy and pixel shuffling to reorganize noise. This enhances its generalization potential across various noise distributions and suggests possible applications in diverse denoising scenarios, including cryo-EM.\n2. **Comparative Performance**: Di-Fusion demonstrates significant improvements over the current leading method, DDM². It not only achieves superior performance but is also easier to implement, as it does not necessitate additional denoiser training, thereby improving practical usability.\n3. **Methodological Rigor**: The study conducts thorough experiments, offering extensive comparisons and analyses regarding downstream task performance, contributing to the overall robustness of the research.\n\n### Minor Comments\n- Clarification on the choice of DTI metrics for comparison would be beneficial.\n- Some figures can be more informative with enhanced labeling or descriptions.\n\n### Summary Paragraph\nIn summary, the manuscript presents a significant advancement in dMRI denoising through the Di-Fusion method. The innovation lies in its self-supervised approach and flexibility with data and noise models, fulfilling the importance criteria of the study. While the evaluations substantiate its effectiveness, additional clarity in experimental design—particularly regarding cleanliness in DTI reference metrics—would strengthen the reproducibility and evaluation aspects. Overall, Di-Fusion exhibits a successful combination of significance and innovation, with minor concerns over methodological transparency.\n\n### Decision Recommendation\nMajor revision is recommended to address the methodological concerns and enhance clarity in the comparisons made within the study.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Di-Fusion*, a fully self-supervised diffusion MRI (dMRI) denoising framework that enhances signal-to-noise ratio without requiring clean reference data or explicit noise models. The approach integrates late diffusion steps with an adaptive sampling process to create a single-stage, self-supervised learning strategy. The paper reports notable improvements over existing state-of-the-art denoising techniques, particularly in microstructure modeling and tractography. The study is overall clear, methodologically sound, and supported by comprehensive quantitative and qualitative experiments on both real and simulated data.  \n\n**Major Comments**  \n1. **Scope and Clarity of Evaluation:** Although the work demonstrates strong performance relative to current methods (e.g., DDM²), it would be useful to better connect the denoising objective with the central application goal in dMRI—reducing the number of diffusion gradients to enable faster acquisitions.  \n2. **Reference Comparisons:** In downstream DTI analyses, the comparison between denoised and noisy data is informative. However, it remains unclear why the authors did not use additional DWI data to compute a cleaner DTI reference metric. Clarifying or justifying this choice would strengthen the evaluation and make performance claims more convincing.  \n\n**Minor Comments**  \n- The presentation and writing are generally clear, but it may help to make the relationship between N2N training, pixel shuffling, and adaptive sampling more explicit for non-specialist readers.  \n- Ensure that acronyms (e.g., DTI, DWI) are defined at first use for clarity.  \n- No ethical concerns are identified.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is **significant** in advancing self-supervised dMRI denoising by removing dependence on clean reference data. Its **innovation** lies in combining late-stage diffusion modeling with adaptive sampling in a single-stage framework. The **evaluation** is extensive and convincingly demonstrates improved performance over prior methods, though further clarification of reference comparisons would enhance credibility. In terms of **reproducibility**, the method is straightforward to implement and does not require separate denoiser training, supporting practical applicability.  \n\n**Decision Recommendation**  \n**Recommendation: Accept (Rating: 8)** — The paper presents a solid and well-executed contribution, with minor revisions suggested for improved clarity in the evaluation rationale.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes a method for denoising diffusion MRI data sets.\n\nThis is a well-studied problem with many solutions in the literature. It is an important problem, as diffusion MRI is widely used for neuroscience and for clinical medicine. Recent years have seen a trend towards using self-supervised approaches to characterise the noise distribution and separate noise from the underlying signal.  This submission falls very much in this category, but proposes a different algorithm to those that are popular in the literature.\n\nExperiments compare against five baselines and results appear competitive with other methods, sometimes surpassing them.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe algorithm appears novel, although I found it hard to tell from the literature review how novel it is - whether it takes ideas from other areas and repurposes them for this problem, or if this is an algorithm specifically designed for diffusion MRI.\n\nThe problem is an important one with widespread application.\n\nResults appear competitive on a few example images shown in the figures.\n\n### Weaknesses\n\nThe baselines chosen do not include the most widely used denoising methods.  A clear omission is the random-matrix theory approaches proposed by Veraart et al in a series of very highly cited papers starting with Neuroimage 2016.\n\nThe only quantitative results use simulations, which seem likely to be skewed towards to capabilities of the proposed algorithm.\n\nThe qualitative results on actual human data are questionable as to whether they show improvement over baselines.  Even if they do, these are single cherry-picked examples and it is not clear whether these are advantages that manifest over large collections of images/scenarios.\n\n### Questions\n\nCorresponding to weaknesses listed above.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel method aimed at denoising diffusion MRI data sets, an area of significant importance in neuroscience and clinical medicine. Given the proliferation of solutions in existing literature, the authors contribute a different algorithm that aligns with recent trends of using self-supervised approaches for noise characterization. The overall clarity of the manuscript could be improved, particularly within the literature review context.\n\n**Major Comments:**  \n1. **Novelty:**\n   - The algorithm appears novel; however, clarity regarding its uniqueness in relation to existing methods is lacking. The literature review should better articulate whether it innovatively repurposes concepts from other domains or is specifically tailored for diffusion MRI.\n   \n2. **Evaluation Design:**\n   - The chosen baselines for comparisons do not encompass the most widely adopted denoising methods. A notable omission includes the random-matrix theory approaches from Veraart et al., whose papers are highly cited in this context.\n   \n3. **Results:**\n   - The quantitative validation relies solely on simulations, which may skew the perception of the proposed algorithm's effectiveness. The qualitative results derived from human data raise questions about demonstrated improvements over baseline methods. The reliance on cherry-picked examples does not convincingly support generalizability across diverse scenarios.\n\n**Minor Comments:**  \n- Figures showcasing results should be accompanied by clearer labels and captions for improved comprehension.  \n- Additional typographical errors should be corrected for better presentation clarity.  \n- Acronyms and notation should be explicitly defined upon first use for reader clarity.\n\n**Summary Paragraph:**  \nIn terms of significance, the problem addressed by the manuscript holds considerable relevance. While the innovation aspect is demonstrated through the proposed algorithm, its true novelty remains ambiguous. The evaluation lacks comprehensive comparisons with leading methodologies, which impacts reproducibility as well. The majority of results stem from simulations and limited qualitative examples on actual human data, raising concerns about their applicability to real-world contexts.\n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a new method for denoising diffusion MRI (dMRI) data. Denoising is a crucial step in dMRI analysis, with many techniques previously developed, including recent self-supervised approaches that model noise and signal separation. The authors position their work within this emerging category but claim a distinct algorithmic design. Overall, the paper addresses an important and widely applicable problem. The presentation is somewhat unclear in explaining the degree of novelty and in contextualizing the method relative to existing literature, but experimental results indicate competitive performance.  \n\n**Major Comments**  \n1. **Literature Context and Novelty** – The review of prior work does not make it clear how the proposed algorithm differs from or extends established approaches. It is uncertain whether the method repurposes concepts from other domains or is specifically tailored for diffusion MRI.  \n2. **Baseline Selection** – The comparative study omits key baselines, particularly the random-matrix theory methods by Veraart *et al.* (Neuroimage, 2016 and subsequent works). These are highly cited and represent a major benchmark in the field.  \n3. **Evaluation Design** – Quantitative evaluations rely solely on simulations, which may favor the proposed algorithm. Real data experiments are limited to qualitative inspection of a few human examples, making it difficult to assess generalizability.  \n4. **Result Interpretation** – Visual results on real data are ambiguous, and given the small and potentially cherry-picked examples, it is unclear whether improvements over competing methods are consistent across broader datasets.  \n\n**Minor Comments**  \n- Clarify the description of the algorithm’s originality and draw clearer links to related methods.  \n- Improve presentation and figure clarity to allow more straightforward comparison of results.  \n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work targets an important denoising problem with broad relevance in neuroscience and clinical imaging. While the proposed approach might offer innovation, its novelty relative to prior literature is not sufficiently demonstrated. Evaluation lacks breadth, relying mainly on simulations and limited qualitative evidence, which restricts assessment of robustness and reproducibility. The paper’s significance would be strengthened by inclusion of standard baselines and more comprehensive real-data validation.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial improvements are needed in comparative evaluation, validation on real data, and clarification of novelty before the manuscript can be fully assessed for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces Di-Fusion, a self-supervised diffusion-based method for denoising diffusion MRI (dMRI) data. The method addresses the challenge of denoising dMRI without clean ground truth data by modifying the diffusion model framework through three key components: (1) a \"Fusion process\" that creates linear interpolations between noisy volume pairs to avoid drift during sampling (Section 3.1), (2) a \"Di-\" process that characterizes real-world noise distributions using differences between independent noisy measurements (Section 3.1), and (3) training only the latter diffusion steps to reduce generative diversity and improve stability (Section 3.2). The method also incorporates adaptive sampling with controllable refinement. Experiments on three datasets (Stanford HARDI, Sherbrooke 3-Shell, PPMI) demonstrate improved performance in microstructure modeling, tractography tracking, and quantitative metrics compared to existing methods including DDM2, Patch2Self, and traditional approaches.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and notation issues**\n  - Equation (5) introduces xt-1 and x̄t-1 without clearly defining their relationship to the standard DDPM formulation, creating confusion about which trajectory is being referenced\n  - The derivation in Appendix C.1 shows inconsistent use of conditioning variables, particularly in equations (28)-(35) where x0 and x′ are used interchangeably without clear justification\n  - Equation (8) defines ξx-x′ using a \"mess\" operation that lacks mathematical rigor and formal definition beyond \"spatial shuffling\"\n\n• **Limited theoretical justification for key design choices**\n  - The choice of Tc = 300 for training only latter diffusion steps (Section 3.2) appears arbitrary, with Figure 6 showing only empirical validation rather than theoretical reasoning\n  - The linear interpolation weights λt₁ and λt₂ in Equation (6) are defined without theoretical justification for why this specific weighting scheme prevents drift\n  - The \"Di-\" process assumption that x - x′ preserves noise variance (Appendix C.2) relies on independence assumptions that may not hold in practice for dMRI acquisitions\n\n• **Experimental evaluation limitations and missing baselines**\n  - The comparison with DDM2 is potentially unfair as Figure S32 shows DDM2's high sensitivity to hyperparameters, yet the authors use \"their best results\" without systematic hyperparameter tuning for their own method\n  - Missing comparisons with recent self-supervised denoising methods beyond those listed, particularly methods that don't require paired volumes\n  - The simulated experiments (Section 4.4) use only fastMRI data with artificially added Gaussian noise, which may not reflect the complex noise characteristics of real dMRI acquisitions\n\n• **Unclear adaptive sampling and termination criteria**\n  - The coefficient bx calculation in Equation (12) uses arbitrary thresholds ρ₁ and ρ₂ with limited justification for the specific values chosen (-0.93 and -0.95)\n  - The CSNR parameter for adaptive termination lacks principled selection guidelines, with values appearing to be empirically determined per experiment\n  - The Run-Walk acceleration strategy switches at Tr = 50 without theoretical or empirical justification for this specific threshold\n\n## Suggestions for Improvement\n\n• **Clarify mathematical formulations and provide rigorous definitions**\n  - Provide a clear mathematical definition of the \"mess\" operation in Equation (8) and justify its theoretical properties for noise preservation\n  - Reformulate the derivation in Appendix C.1 with consistent notation and clear distinctions between different trajectory types\n  - Add formal proofs or theoretical analysis showing why the proposed modifications maintain the desired properties of diffusion models\n\n• **Provide theoretical foundations for design choices**\n  - Conduct ablation studies varying Tc across a wider range and provide theoretical reasoning for the relationship between Tc and denoising performance\n  - Derive the optimal linear interpolation weights in Equation (6) from first principles or provide empirical sensitivity analysis\n  - Validate the independence assumptions underlying the \"Di-\" process with real dMRI data and provide theoretical bounds on when these assumptions hold\n\n• **Strengthen experimental validation and baseline comparisons**\n  - Perform systematic hyperparameter optimization for both the proposed method and all baselines, particularly DDM2, to ensure fair comparison\n  - Include additional recent self-supervised denoising methods and provide comparisons on datasets with known ground truth beyond simulation\n  - Validate the method on datasets with different noise characteristics and acquisition protocols to demonstrate generalizability beyond the three tested datasets\n\n• **Develop principled adaptive sampling methodology**\n  - Provide theoretical or data-driven methods for selecting ρ₁, ρ₂, and CSNR parameters rather than relying on empirical tuning\n  - Conduct systematic analysis of the Run-Walk acceleration threshold Tr and provide guidelines for setting this parameter\n  - Validate the adaptive termination criterion across different anatomical regions and noise levels to ensure consistent performance",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents Di-Fusion, a self-supervised diffusion-based method aimed at denoising diffusion MRI (dMRI) data. The method specifically addresses the challenge of denoising in the absence of clean ground truth data by implementing modifications to the diffusion model framework. Key components include a \"Fusion process,\" a \"Di-\" process that characterizes noise distributions, and selective training of diffusion steps for enhanced stability. Experimental results across three datasets (Stanford HARDI, Sherbrooke 3-Shell, PPMI) indicate improved outcomes in microstructure modeling and quantitative metrics when compared to existing denoising techniques.\n\n**Major Comments:**  \n1. **Mathematical formulation inconsistencies:** Issues with notation definitions in Equation (5) and interchangeability of variables in Appendix C.1 create confusion.\n2. **Limited theoretical justification:** The choice of Tc = 300 for training diffusion steps appears arbitrary, lacking empirical support. Similar arbitrary definitions are noted for linear interpolation weights in Equation (6).\n3. **Experimental evaluation limitations:** Fairness is compromised in baseline comparisons with DDM2 due to differences in hyperparameter tuning. Recent methods not requiring paired volumes are not included in comparisons.\n4. **Unclear adaptive sampling criteria:** The coefficient bx in Equation (12) relies on arbitrary thresholds without sufficient justification. The CSNR parameter selection lacks principled guidelines.\n\n**Minor Comments:**  \n- Improve clarity in mathematical definitions, particularly for the \"mess\" operation in Equation (8).  \n- Ensure consistent notation within Appendix C.1's derivation.  \n- Expand theoretical analysis on the proposed modifications for diffusion model properties.\n\n**Summary Paragraph:**  \nIn evaluating Di-Fusion against the TMI editorial criteria, the manuscript demonstrates significance through its innovative approach to denoising dMRI data, though it struggles with theoretical justifications and consistency in mathematical formulations. The performance evaluation is notable; however, limitations in baseline comparisons and experimental designs raise questions about reproducibility. Overall, the research contributes a potentially impactful methodology, but it requires further refinement in theoretical and experimental validation.\n\n**Decision Recommendation:**  \nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThis manuscript presents *Di-Fusion*, a self-supervised diffusion-based approach for denoising diffusion MRI (dMRI) data in the absence of clean ground truth. The method modifies the standard diffusion model through three main components: (1) a fusion process that interpolates between noisy volumes to mitigate drift, (2) a “Di-” process characterizing real-world noise differences, and (3) training restricted to latter diffusion steps for improved stability. Adaptive sampling with controllable refinement is also introduced. Experiments on multiple datasets (Stanford HARDI, Sherbrooke 3-Shell, and PPMI) report improved quantitative and qualitative results over existing baselines. The paper is ambitious and technically detailed but suffers from unclear mathematical formulation, limited theoretical grounding, and insufficiently justified experimental methodology.  \n\n**Major Comments**  \n1. **Mathematical and notation inconsistencies** – The roles of \\(x_{t-1}\\) and \\(\\bar{x}_{t-1}\\) are unclear relative to standard DDPM trajectories (Eq. 5). Appendix C.1 shows inconsistent conditioning variables, using \\(x_0\\) and \\(x'\\) interchangeably without justification. Equation (8) defines \\(\\xi_{x−x′}\\) via a “mess” operation described only informally, lacking mathematical rigor.  \n2. **Weak theoretical justification for design choices** – The truncation parameter \\(T_c=300\\) is empirically set with no theoretical analysis. Linear interpolation weights (Eq. 6) are introduced heuristically, and the assumption that \\(x−x′\\) preserves noise variance may not hold for real dMRI acquisitions.  \n3. **Experimental limitations and missing baselines** – Comparison with DDM2 may be unfair, as its results are sensitive to hyperparameters while no equivalent tuning is reported for Di-Fusion. The study omits newer self-supervised denoising methods and relies on simulated Gaussian noise experiments that inadequately reflect real noise complexity.  \n4. **Unclear adaptive sampling and stopping criteria** – Parameters (e.g., ρ₁ = −0.93, ρ₂ = −0.95) and the Run–Walk threshold (Tᵣ = 50) lack justification. The CSNR-based termination rule is mentioned but supported only by empirical tuning without principled rationale.  \n\n**Minor Comments**  \n- Clarify notation consistency throughout Appendices and figures.  \n- Improve mathematical precision of the “mess” operator definition.  \n- Some parameter choices could be better explained in figure captions or footnotes.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses an important problem in self-supervised dMRI denoising and proposes an inventive adaptation of diffusion models. Its potential impact is high if theoretical and methodological issues are resolved. Innovation lies primarily in integrating paired-noise modeling with adaptive sampling. However, the current manuscript lacks theoretical depth and comprehensive evaluation, limiting confidence in robustness and reproducibility. Explicit derivations, ablations, and clearer parameter-justification would strengthen the contribution and reliability.  \n\n**Decision Recommendation**  \n**Major Revision.** The concept is promising, but major clarifications, theoretical justification, and more rigorous experiments are required before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance**  \n✅ The submission proposes *Di-Fusion*, a self‑supervised denoising framework for diffusion MRI (dMRI) based on diffusion probabilistic models. The core of the paper centers on algorithmic and methodological innovations in image reconstruction and denoising—well inside IEEE TMI’s methodological scope.\n\n2. **Novelty & Contribution**  \nThe paper advances dMRI denoising beyond current self‑supervised and diffusion‑based methods (e.g., Patch2Self, DDM2) by:  \n- introducing a “Fusion” process to stabilize the forward diffusion trajectory;  \n- replacing additive Gaussian noise with data‑adaptive noise (“Di‑process”) without explicitly learning a noise model;  \n- training only on latter diffusion steps to restrict the generative model’s diversity;  \n- providing adaptive, controllable sampling termination.  \nThese changes constitute more than incremental improvement and achieve significant performance gains across datasets. The conceptual novelty is moderate‑to‑high and should meet TMI’s contribution threshold.\n\n3. **Technical and Experimental Rigor**  \nExperiments are broad and detailed. The authors validate on multiple public datasets, include simulated‑data studies with ground truth, ablation analyses, and downstream clinical relevance through tractography and microstructure modeling. The math appears internally consistent, though extensive. Reproducibility is supported by availability of code and parameter details.  \nPotential areas for strengthening: statistical significance analysis and runtime/computation comparisons versus baselines.\n\n4. **Clarity and Presentation**  \nThe paper is well organized (Introduction → Methods → Experiments / Discussion / Appendix) with careful mathematical exposition. However, sections could be condensed for readability in a journal format—the current text retains conference‑style density. Figures are informative but over‑numerous; several could be moved to supplemental materials. Minor grammatical polishing would improve flow.\n\n5. **Ethical and Reproducibility Compliance**  \nAll datasets are public; IRB or patient consent issues are therefore minimal. The authors provide URLs for code release and cite dataset sources appropriately. Ethical and reproducibility standards appear satisfied.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n1. **Summary**  \nThe paper presents *Di‑Fusion*, a self‑supervised diffusion‑model‑based framework for denoising diffusion MRI. By integrating a “Fusion” alignment between forward and reverse diffusion steps and a data‑driven “Di‑process” noise representation, the method enables training without clean references or explicit noise modeling. An adaptive Run‑Walk sampling strategy allows controllable refinement at inference. Experiments on multiple in‑vivo and simulated datasets show state‑of‑the‑art denoising quality and improvement in downstream diffusion metrics, outperforming Patch2Self and DDM2.\n\n2. **Strengths**  \n- Clear methodological innovation grounded in diffusion‑model theory.  \n- Fully self‑supervised, eliminating need for noise models or paired data.  \n- Extensive experimental validation, including clinical relevance (tractography, DTI/CSD fits).  \n- Reproducible experimental setup and open‑source code.  \n- Well‑reasoned ablations demonstrating the effect of each module.\n\n3. **Weaknesses**  \n- Heavy mathematical and algorithmic description may obscure intuition.  \n- Inference time and computational complexity not quantitatively benchmarked.  \n- Limited comparison on inference efficiency and memory load relative to traditional non‑deep methods.  \n- Statistical tests of improvement (e.g., p‑values across metrics) could quantify significance.  \n- Minor presentation issues: verbosity, figure overload, occasional English phrasing.\n\n4. **Major Comments**  \n   1. **Clarity of the “Fusion” and “Di‑process” mechanisms:** consider adding a simplified diagram and brief intuition before formal equations.  \n   2. **Computation and runtime:** provide quantitative inference time vs. DDM2 and Patch2Self to contextualize practical feasibility.  \n   3. **Statistical significance:** include confidence intervals or hypothesis testing on key metrics (R² scores, PSNR/SSIM).  \n   4. **Generalization across scanners/noise types:** discuss prospective validation or domain adaptation to different field strengths or vendors.  \n   5. **Condense manuscript for TMI:** streamline long appendices and repetitive derivations; focus on core algorithms and essential results.\n\n5. **Minor Comments**  \n- Correct minor typographical errors (spacing, capitalization).  \n- Improve figure captions for self‑containment; indicate units for metrics.  \n- Ensure equation numbering consistency.  \n- Some references could be updated to final journal versions.\n\n6. **Overall Recommendation**  \n**Major Revision.**  \nThe work is methodologically strong and relevant for TMI, but the manuscript requires (i) clearer presentation of algorithmic intuition, (ii) quantitative efficiency analysis, and (iii) moderate condensation for a journal article format. With these revisions, it could merit publication.\n\n7. **Confidence Level**  \n5 / 5 – High confidence in assessment.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Di‑Fusion*, a self‑supervised denoising framework for diffusion MRI (dMRI) built on diffusion probabilistic models. The approach integrates a Fusion process aligning forward and reverse diffusion trajectories and a data‑adaptive “Di‑process” noise formulation that avoids explicit noise model learning. By restricting training to latter diffusion stages and offering controllable adaptive sampling, the framework seeks to improve denoising performance and generalizability without paired training data. The paper is methodologically within IEEE TMI’s scope, generally clear, and well organized, though dense in mathematical exposition.\n\n---\n\n**Major Comments**  \n1. **Conceptual Clarity of Core Modules:** The mechanisms of “Fusion” alignment and the “Di‑process” noise adaptation are novel but technically heavy. Adding a simplified diagram and intuitive explanation preceding formal derivations would aid accessibility.  \n2. **Quantitative Efficiency Analysis:** The paper lacks benchmarking of inference runtime and computational complexity versus existing methods (e.g., DDM2, Patch2Self). Including these metrics would clarify practical feasibility.  \n3. **Statistical Significance Testing:** Results should be complemented with confidence intervals or hypothesis testing on quantitative measures (R², PSNR, SSIM) to substantiate claims of improvement.  \n4. **Generalization and Robustness:** Discussion or evidence concerning performance across different scanners, field strengths, or noise distributions would strengthen the claim of model robustness.  \n5. **Manuscript Condensation:** The text retains conference‑style verbosity; condensing mathematical detail and relocating surplus figures or derivations to supplemental materials is recommended for TMI’s journal format.\n\n---\n\n**Minor Comments**  \n- Adjust minor typographical and grammatical issues.  \n- Refine figure captions for self‑containment, including units for metrics.  \n- Ensure consistent equation numbering and formatting.  \n- Update and verify references to final published versions where possible.\n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper presents a moderately to highly novel application of diffusion probabilistic modeling to dMRI denoising, advancing beyond prior self‑supervised approaches. Its significance derives from producing state‑of‑the‑art denoising quality and improved downstream diffusion metrics. Innovation is clear in the Fusion and Di‑process mechanisms, though better conceptual framing would enhance comprehensibility. Evaluation is broad, with multiple public datasets, ablations, and relevant downstream analyses; however, significance testing and computational efficiency remain incomplete. Reproducibility is strong, supported by accessible code and parameter documentation. Ethical compliance is adequate, given exclusive use of public data.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The study is methodologically sound and potentially impactful but requires clearer presentation, efficiency benchmarking, statistical validation, and minor editorial refinement before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents *Di-Fusion*, a self-supervised denoising framework for diffusion MRI (dMRI) based on diffusion probabilistic models. It introduces significant methodological innovations in image reconstruction and denoising, which aligns well with the aims of IEEE TMI. Overall, the clarity and organization of the paper are solid, although there are areas where readability could be enhanced.\n\n**Major Comments:**  \n1. **Clarity of Algorithmic Mechanisms:** The \"Fusion\" and \"Di-process\" components could benefit from a simplified diagram and intuition prior to formal definitions.\n2. **Computation and Runtime:** Quantitative benchmarks for inference time compared to DDM2 and Patch2Self should be provided to assess practical feasibility.\n3. **Statistical Significance:** It is advisable to include statistical analyses such as confidence intervals or hypothesis tests for critical metrics (e.g., R² scores, PSNR/SSIM).\n4. **Generalization:** A discussion on potential validation or domain adaptation for various field strengths or vendors would enhance the manuscript's relevance.\n5. **Condensation for Presentation:** The manuscript should be streamlined by reducing lengthy appendices and repetitive derivations, focusing on the core algorithms and results.\n\n**Minor Comments:**  \n- Correct minor typographical errors, including spacing and capitalization inconsistencies.  \n- Enhance figure captions for clarity and ensure they are self-contained, indicating units for all metrics.  \n- Ensure consistency in equation numbering throughout the text.  \n- Consider updating some references to their final published versions.\n\n**Summary Paragraph:**  \nThe manuscript makes notable contributions in the field of dMRI denoising, emphasizing its significance and innovation through methodical advancements and extensive experimental validation with publicly available datasets. Although it fulfills the reproducibility criterion via code and data access, several aspects require improvement, particularly regarding presentation clarity and computational assessments. The work shows promise, but the weaknesses noted may hinder its accessibility and practical implementation.\n\n**Decision Recommendation:**  \n**Major Revision.**  \nWhile the methodology is strong and relevant to IEEE TMI, revisions are necessary to improve the presentation of algorithmic intuition, incorporate quantitative efficiency analysis, and condense the manuscript for journal standards. Addressing these issues could elevate the work to a publishable level.\n\n**Confidence Level:**  \n5 / 5 – High confidence in assessment.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chenxu Wu",
      "Qingpeng Kong",
      "S Kevin Zhou",
      "Zihang Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_6bd7c307edad6542cdf2945234d9af92905ebf8f.pdf",
    "remote_url": "https://openreview.net/pdf/6bd7c307edad6542cdf2945234d9af92905ebf8f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Large Language Models",
      "Large Vision Models",
      "Semantic Evaluation",
      "Computational Pathology",
      "Medical Imaging"
    ],
    "abstract": "Do Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts? Currently, only biased, qualitative approaches with limited, small-scale expert evaluations are available to answer this question. In this study, we propose the Boltzmann Semantic Score (BSS), a novel method inspired by state space modeling, to evaluate the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. Through extensive experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, we first establish a baseline of LLMs' performance in digital pathology and show that LLMs' encoding can be linked to patient outcomes. Then, we compared seven LVMs with BSS and showed that LVMs suffer from poor semantic capability when compared with encoded expert knowledge from pathology reports.\nWe also found statistically significant correlations between BSS (as a measure of structural similarity) and performance in two downstream tasks: information retrieval and survival prediction tasks. Our study also investigates the consensus among LLMs in evaluating LVMs using BSS, indicating that LLMs generally reach substantial consensus in rating LVMs, with some variation dependant on the cancer type. We believe the BSS metric proposed here holds significant potential for application in other domains with similar contexts. Data and code can be found in \\footnotesize \\url{ https://github.com/AIMLab-UBC/Boltzmann}",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a novel semantic metric called Boltzmann Semantic Score (BSS), which is inspired by state space modeling, to evaluate the semantic capability of large vision models (LVMs) in medical image processing. The authors demonstrate the effectiveness of this metric through experiments, revealing that LVMs exhibit low semantic capabilities. Additionally, BSS shows a strong correlation with the performance of LVMs on two clinical tasks: information retrieval and survival prediction.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.The paper is well-structured and clearly presented, which significantly improves its readability.\n2.The introduction of the Boltzmann Semantic Score (BSS) is an innovative approach inspired by state space modeling, providing a fresh perspective on evaluating the semantic capabilities of LVMs in medical image processing.\n3.The experiments demonstrate significant correlations between BSS and performance on the clinical tasks of information retrieval and survival prediction. Additionally, the experiments show LLMs' capabilities in these two key tasks and provide a quantitative comparison of LLM consistency. This consistency further supports BSS as an effective metric for evaluating the semantic capabilities of LVMs.\n\n### Weaknesses\n\n1.The computational complexity of BSS may be high in practical applications, particularly when applied to large-scale datasets.\n2.While the experiments show strong performance of BSS in the information retrieval task, its correlation with survival prediction is weaker. This may indicate that BSS lacks robustness across different types of tasks, especially in more complex medical applications. Therefore, its effectiveness as a general semantic metric remains to be further validated.\n3.The experiments focus on the tasks of information retrieval and survival prediction, but these tasks may differ in nature from other potential tasks. The consistency of LLMs and the effectiveness of BSS in other semantic tasks require further experimental validation across a broader range of tasks.\n4.The paper focuses on evaluating the semantic capabilities of existing LVMs, but it lacks concrete suggestions on how to improve their semantic performance. Although the limitations of LVMs are highlighted, there is little discussion on how to optimize or modify their architectures to overcome these shortcomings.\n\n### Questions\n\n1.Could the authors suggest ways to optimize BSS for large-scale datasets, or clarify if any tests on smaller subsets were conducted for comparative analysis?\n2. Since BSS performs better on information retrieval than survival prediction, could the authors elaborate on the reasons for this difference? Is there evidence BSS might generalize to other medical tasks?\n3. The paper notes limitations in LVMs' semantic capabilities. Do the authors have ideas on potential architectural or training adjustments that might address these limitations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces a novel semantic metric known as the Boltzmann Semantic Score (BSS), which is derived from state space modeling, aimed at evaluating the semantic capabilities of large vision models (LVMs) in medical image processing. The authors claim that BSS effectively highlights the limited semantic capabilities of LVMs and demonstrates a strong correlation with performance in clinical tasks, such as information retrieval and survival prediction.\n\n**Major Comments:**  \n1. The computational complexity associated with BSS may pose challenges in practical applications, particularly with large-scale datasets.  \n2. Although BSS shows strong correlation with information retrieval, its weaker correlation with survival prediction raises questions about its robustness across diverse tasks, suggesting a need for further validation.  \n3. The experimental focus is limited to information retrieval and survival prediction, potentially overlooking the performance of BSS in other semantic tasks. Comprehensive validation is essential to affirm BSS's effectiveness across a wider array of tasks.  \n4. The manuscript effectively identifies limitations in LVMs but does not offer specific strategies or recommendations for improving their semantic performance or modifying architectures to address these concerns.\n\n**Minor Comments:**  \n- The paper is well-structured and maintains clarity, enhancing overall readability.  \n- Further elaboration on the performance differences between information retrieval and survival prediction is needed, including potential generalizability of BSS to other medical tasks.  \n- Suggestions for optimizing BSS for large datasets or insights from tests on smaller subsets would improve the manuscript.  \n\n**Summary Paragraph:**  \nIn terms of significance, the introduction of the BSS presents innovative contributions to evaluating LVMs; however, methodological limitations reduce its robust application, particularly related to reproducibility and evaluation criteria. The experiments confirm some strengths of BSS in specific tasks, but weaknesses are evident in its correlation with complex medical tasks and the lack of practical guidance for improving LVMs' capabilities. Thus, while the study holds promise, further validation and expansion are crucial for comprehensive application.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThis manuscript introduces the *Boltzmann Semantic Score (BSS)*, a novel semantic metric inspired by state space modeling, designed to evaluate the semantic capabilities of large vision models (LVMs) in medical image processing. The study demonstrates BSS’s utility through experiments that reveal generally low semantic capability in current LVMs and show correlations between BSS and LVM performance on two downstream clinical tasks—information retrieval and survival prediction. Overall, the paper is clearly written and well organized, effectively communicating both the motivation and experimental findings.\n\n---\n\n**Major Comments**  \n1. **Computational Complexity:** The BSS metric may entail considerable computational cost, particularly for large-scale datasets. Discussion of scalability or algorithmic efficiency would improve practical applicability.  \n2. **Task Robustness:** While BSS shows a strong correlation with information retrieval outcomes, its weaker association with survival prediction suggests limited robustness across diverse clinical tasks. The generalizability of BSS beyond these specific settings should be further validated.  \n3. **Breadth of Experimental Evaluation:** The experiments focus on two tasks that may differ substantially from other potential medical imaging applications. Broader evaluation across additional tasks would strengthen evidence for BSS as a general semantic metric.  \n4. **Implications for LVM Development:** Although the paper identifies limitations in LVMs’ semantic capability, it provides little guidance on how model architectures or training might be improved to enhance semantic understanding. Discussion of possible optimization directions would add value.  \n\n---\n\n**Minor Comments**  \n- Clarify whether BSS was tested on subsets of large datasets to evaluate scalability.  \n- Elaborate on why BSS correlates differently across the two tasks.  \n- Ensure consistent terminology when referencing large vision models versus language–vision models.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript is significant in proposing an innovative metric (BSS) for assessing semantic understanding in LVMs, a topic of increasing importance in medical imaging AI. The methodological novelty is notable, yet the evaluation is limited in scope, and robustness across tasks remains uncertain. While experimental procedures appear reproducible, additional details on computational efficiency and cross-task validation would strengthen confidence in the approach.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes the Boltzmann Semantic Score (BSS) as a novel metric to evaluate the semantic performance of latent visual models (LVMs) by leveraging large language models (LLMs). The idea behind using BSS is to quantify how well the visual representations align with text expert annotations. The authors show that BSS could be used as a measure of semantic similarity for LVMs. This paper include applications to pathology reports and whole slide images from  The Cancer Genome Atlas (TCGA), a large publicly available cancer genome dataset. Evaluation on various tasks such as information retrieval and survival prediction is included. This paper suggests high correlations for certain cancer between BSS and performance in both survival prediction and information retrieval.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n•\tBoltzmann Semantic Score is a novel approach to evaluate the semantic perspective of LVMs.\n•\tThe work leverages a large dataset (TCGA), and experiments are performed on several benchmarks.\n•\tThe work provides interesting insights on the model performance using BSS based on observed results.\n•\tHigh correlation between BSS and two downstream tasks i.e information retrieval and survival prediction, highlighting the significance of the results.\n•\tInteresting experiments on clinical tasks showing correspondence between LLMs and patient survival.\n\n### Weaknesses\n\n•\tThe mathematics for the explanation of the Boltzmann Score and its application is rather heavy. A more concise and clearer explanation would enable to understand better the intuition behind the usefulness of BSS as an evaluation metric for LVMs.\n•\tThe authors could develop more on the clinical implication and real-word use of BSS in decision-making.\n•\tSome insights are provided to explain the differences between LVMs and LLMs performance, but the paper could investigate more thoroughly those differences and inherent variations.\n•\tA discussion of the limitations of this work in terms of generalization under different contexts is lacking.\n\n### Questions\n\n•\tDid you visualize the semantic similarity and qualitatively assess the use of BSS as evaluation metric?\n•\tHow reliable is Boltzmann Semantic Score ? \n•\tWhat preprocessing was applied to the medical reports?\n•\tCould you explain the differences observed in Table 3 a) for the two-sided Pearon's Correlation Test? \n•\tWhat is the effect of bias originating from the datasets ?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript proposes the Boltzmann Semantic Score (BSS) as an innovative metric for evaluating the semantic performance of latent visual models (LVMs) using large language models (LLMs). It aims to quantify the alignment of visual representations with expert text annotations, demonstrating potential applications in pathology reports and whole slide images from The Cancer Genome Atlas (TCGA). The authors present evaluations across various tasks such as information retrieval and survival prediction, indicating high correlations between BSS and performance outcomes.\n\n**Major Comments:**  \n1. The mathematics surrounding the Boltzmann Score requires simplification for better comprehension, enabling clearer understanding of its utility as an evaluation metric for LVMs.\n2. Further exploration of the clinical implications and real-world applications of BSS in decision-making is necessary.\n3. The discussion on performance discrepancies between LVMs and LLMs could be more robust, investigating inherent variations in detail.\n4. The paper lacks a discussion on the limitations related to generalization across different contexts.\n\n**Minor Comments:**  \n- Some figures could be presented more clearly for improved clarity.  \n- The notation used in the mathematical explanations needs standardization.  \n- Typos in the text should be addressed for better readability.  \n\n**Summary Paragraph:**  \nIn terms of significance, the introduction of the Boltzmann Semantic Score marks a noteworthy contribution to the evaluation of LVMs. The innovative approach, backed by extensive dataset utilization and experimental evaluations, offers significant insights into model performance. However, the paper could benefit from more thorough discussions on clinical implications and an exploration of the limitations of the approach. The reproducibility of results, while suggested through high correlation findings, needs further confirmation through additional datasets or different contexts.\n\n**Decision Recommendation:**  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Boltzmann Semantic Score (BSS)*, a novel metric designed to assess the semantic performance of latent visual models (LVMs) by leveraging large language models (LLMs). The approach quantifies the alignment between learned visual representations and textual expert annotations, aiming to evaluate semantic similarity in a principled manner. Experiments use The Cancer Genome Atlas (TCGA) dataset, applying BSS to tasks such as information retrieval and survival prediction. The paper reports strong correlations between BSS and task performance, suggesting the potential of BSS as a surrogate measure for semantic evaluation. Overall, the manuscript presents an intriguing idea, though some aspects of the presentation and theoretical exposition require clarification.  \n\n**Major Comments**  \n1. The mathematical formulation of the Boltzmann Semantic Score is overly detailed and difficult to interpret. A more concise and intuitive explanation would improve accessibility and clarify how BSS captures semantic alignment.  \n2. The discussion of clinical implications is limited. The manuscript would benefit from elaborating how BSS could be integrated into medical decision-making or interpretable clinical workflows.  \n3. The differences in performance across various LVMs and LLMs are mentioned but not thoroughly analyzed. A deeper investigation of these variations would strengthen the conclusions.  \n4. The paper lacks a clear discussion of the limitations, particularly regarding generalization across datasets or disease domains.  \n5. More detail on preprocessing of medical reports and bias mitigation would enhance reproducibility and transparency.  \n\n**Minor Comments**  \n- Clarify the terminology and ensure consistent presentation of mathematical notation.  \n- Improve readability by simplifying dense mathematical sections.  \n- Provide clearer explanations for statistical tests (e.g., the two-sided Pearson correlation in Table 3).  \n- Consider visualizations of semantic similarity to qualitatively demonstrate BSS behavior.  \n- Minor grammatical and formatting issues should be addressed.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe work is *significant* in proposing a new evaluation metric linking visual-linguistic alignment to downstream medical tasks. Its *innovation* lies in integrating LLM-driven semantic assessment with LVM evaluation. However, *evaluation* could be strengthened by broader validation, clearer mathematical exposition, and better clinical contextualization. In terms of *reproducibility*, dataset details are adequate, but preprocessing steps and bias control require elaboration. Overall, the concept is promising but the manuscript needs refinement for clarity and depth.  \n\n**Decision Recommendation**  \n**Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a semantic metric, BBS, to evaluate LVMs from a medically semantic perspective.\n\nThe paper also leverages LLMs and a large and collective database of medical reports across more than 30 cancer types that represent more than 9,500 patients and it also establishes a baseline of LLMs' performance in two large-scale digital pathology tasks.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n* Originality: Noval evaluation metric which evaluates the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. \n\n* Quality: Extensive experiments including experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, comparison of seven LVMs with BSS, and two correlation analyses between BSS and performance in two downstream tasks.\n\n* Clarity: Well-painted figures and clear formulas.\n\n* Significance: Well-designed metric is important for the community, especially for the evaluation of latent embedding space.\n\n### Weaknesses\n\nI cannot find significant Weaknesses in this paper.\n\n### Questions\n\nWould it be possible to make the BSS a training loss to guide and supervise vision encoder embeddings to align with the strong LLM embeddings? Will BSS have additional advantages over contrastive learning loss, such as smaller batch size requirements?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces a novel semantic metric, BBS, aimed at evaluating latent variable models (LVMs) from a medical semantic perspective. The work utilizes large language models (LLMs) alongside an extensive database of medical reports covering over 30 cancer types and representing more than 9,500 patients. Additionally, it establishes a baseline for LLMs' performance in two significant digital pathology tasks.\n\n**Major Comments:**  \n1. **Originality:** The proposed BBS metric is innovative, as it evaluates the encoding space of LVMs derived from medical images using the encoding space of LLMs from medical reports.\n2. **Quality of Experiments:** The manuscript presents comprehensive experiments, including an analysis of 32 datasets from The Cancer Genome Atlas, comparisons of seven LVMs with BSS, and two correlation analyses linking BSS to performance in downstream tasks.\n3. **Significance of the Metric:** The well-structured BBS metric is crucial for the research community, particularly in assessing latent embedding spaces in medical contexts.\n\n**Minor Comments:**  \n- The figures are well-illustrated, and the formulas are clearly presented, contributing to the overall clarity of the manuscript.  \n- The paper contains no discernible significant weaknesses.\n\n**Summary Paragraph:**  \nThis review indicates that the manuscript demonstrates high significance and originality through its novel evaluation metric and well-designed experiments. While there are no significant weaknesses noted, the integration of BBS as a training loss for guiding vision encoder embeddings could be explored further. This inquiry may elucidate whether BBS offers advantages over existing contrastive learning losses, particularly concerning batch size requirements. Overall, the evaluation and experiments offer a strong foundation that aligns well with the editorial criteria of significance, innovation, evaluation, and reproducibility.\n\n**Decision Recommendation:**  \nMinor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel semantic metric, BBS, designed to evaluate latent vision models (LVMs) from a medically semantic perspective. The approach leverages large language models (LLMs) and a comprehensive database of medical reports covering more than 30 cancer types and 9,500 patients. The study provides a baseline of LLM performance across two large-scale digital pathology tasks, with generally clear presentation and comprehensive experimental support.  \n\n**Major Comments**  \n1. The work presents substantial originality by proposing an evaluation metric (BBS) that relates visual embeddings of LVMs to semantic embeddings derived from medical reports via LLMs. The idea of measuring semantic alignment between modalities is innovative and relevant to the medical imaging community.  \n2. The evaluation is extensive, including experiments on 32 datasets from The Cancer Genome Atlas, the use of five state-of-the-art LLMs, comparison of seven LVMs with BBS, and correlation analyses between BBS scores and downstream task performance. This supports the soundness and generalizability of the method.  \n3. The paper is clearly written and visually well presented, with well-designed figures and formulas that clarify the proposed method.  \n4. While the reviewer does not identify significant weaknesses, they raise an interesting question regarding the potential of using BBS not only as an evaluation metric but also as a training loss to guide vision encoder embeddings. The reviewer further inquires whether such usage might offer advantages over contrastive learning losses, such as requiring smaller batch sizes.  \n\n**Minor Comments**  \n- The presentation could be further refined to enhance readability; currently rated moderate (Presentation: 3).  \n- No ethical concerns are identified by the reviewer.  \n\n**Summary Paragraph**  \nOverall, the paper is strong in significance and innovation, introducing an important metric that could broaden the community’s ability to assess semantic quality in vision-language systems. Evaluation is comprehensive and reproducible across a large and well-documented dataset. The main limitation lies in the absence of exploration of BBS as a training component, which could strengthen its practical impact. Clarity and presentation are satisfactory but could still be improved.  \n\n**Decision Recommendation**  \n**Recommendation: Accept (Rating: 8)** — The paper is judged to be original, methodologically sound, and significant, with no major weaknesses identified.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces the Boltzmann Semantic Score (BSS), a novel metric for evaluating Large Vision Models (LVMs) using Large Language Models (LLMs) as semantic references. The authors address whether LVMs extract medically relevant features similar to human experts by leveraging LLM encodings of pathology reports as ground truth. Using 32 TCGA datasets with ~9,500 patients, they first establish LLM baselines on information retrieval and survival prediction tasks, then propose BSS based on state space modeling and Boltzmann distribution principles. The method compares k-nearest neighbor graphs between LLM text embeddings (reference) and LVM image embeddings (observer), calculating semantic alignment through energy-based similarity measures. Key findings include: LLMs achieve strong performance on pathology tasks (Command-R: 0.871 top-1 accuracy), LVMs show poor semantic capability (BSS <0.4), and BSS correlates significantly with downstream task performance in information retrieval and survival prediction across multiple cancer types.\n\n## Weaknesses\n\n• **Mathematical Formulation Inconsistencies**: The theoretical framework contains several unclear or potentially problematic elements that undermine the method's rigor.\n  - The constant √d1 replacement for kT in Equation 2 (Page 5) lacks proper justification—why should dimensionality scaling replace thermodynamic temperature in this context?\n  - The second-order Boltzmann factor definition in Equation 3 (Page 5) appears ad-hoc without clear theoretical motivation from statistical mechanics principles.\n  - The sets A and D definitions in Equations 4-5 (Page 5) are confusing, particularly the existence quantification that doesn't clearly specify the relationship between matching states.\n\n• **Limited Experimental Validation Scope**: The evaluation framework is constrained to specific tasks and datasets that may not fully demonstrate the method's generalizability.\n  - Only two downstream tasks are evaluated (information retrieval and survival prediction) as acknowledged in Section A (Page 15), limiting evidence for broader applicability.\n  - The correlation analysis in Table 3 (Pages 9-10) shows highly variable results across cancer types, with some datasets showing weak or non-significant correlations, questioning the method's reliability.\n  - The patch number ablation study in Figure 8 (Page 18) shows BSS stability but doesn't address whether this indicates robustness or insensitivity to important visual details.\n\n• **Methodological Assumptions and Limitations**: Several core assumptions underlying the BSS approach lack sufficient justification or analysis.\n  - The assumption that LLM encodings represent \"expert knowledge\" (Section 1, Page 1) is not rigorously validated—LLMs may capture reporting patterns rather than clinical expertise.\n  - The choice of k=5 for nearest neighbors (mentioned in Section D.3, Page 17) appears arbitrary without systematic justification or sensitivity analysis across different values.\n  - The consensus analysis in Figure 7 (Page 9) shows only \"substantial\" agreement between LLMs, indicating the reference standard itself is inconsistent.\n\n• **Experimental Design and Statistical Issues**: The evaluation methodology contains potential biases and statistical concerns that may affect result interpretation.\n  - The survival prediction baseline comparison in Tables 2 and 8 (Page 8, Page 18) uses different model architectures (LLMs vs. LVMs with RSF), making direct performance comparisons potentially misleading.\n  - The correlation testing methodology in Algorithms 2-3 (Pages 20-21) may introduce multiple testing issues without appropriate correction procedures.\n  - The qualitative examples in Figure 9 (Page 21) are cherry-picked and insufficient to demonstrate systematic semantic alignment capabilities.\n\n## Suggestions for Improvement\n\n• **Strengthen Mathematical Foundation**: Provide rigorous theoretical justification for the core mathematical components to enhance method credibility.\n  - Derive or cite established theoretical basis for using √d1 as the kT equivalent in the Boltzmann factor, or explore alternative normalization approaches.\n  - Provide clear statistical mechanics or information-theoretic motivation for the second-order Boltzmann factor formulation.\n  - Rewrite the set definitions in Equations 4-5 with clearer notation and explicit algorithmic procedures for identifying matching/non-matching state pairs.\n\n• **Expand Experimental Validation**: Broaden the evaluation scope to provide more comprehensive evidence of the method's effectiveness and limitations.\n  - Include additional downstream tasks such as disease classification, tumor grading, or image segmentation to demonstrate broader applicability.\n  - Conduct systematic analysis of correlation variability across cancer types, potentially identifying dataset characteristics that predict BSS reliability.\n  - Perform comprehensive ablation studies examining patch sampling strategies, aggregation methods, and their impact on semantic representation quality.\n\n• **Address Core Methodological Assumptions**: Provide more rigorous validation of fundamental assumptions and design choices.\n  - Validate LLM encodings against expert annotations through direct comparison studies or established clinical benchmarks.\n  - Conduct systematic hyperparameter analysis for k-nearest neighbors, providing principled selection criteria based on dataset characteristics.\n  - Develop methods to handle or quantify uncertainty arising from LLM disagreement in the reference standard.\n\n• **Improve Experimental Rigor**: Enhance the statistical and experimental design to provide more reliable and interpretable results.\n  - Implement fair comparison frameworks ensuring equivalent model architectures and training procedures across LLM and LVM evaluations.\n  - Apply appropriate multiple testing corrections and provide confidence intervals for correlation estimates across different experimental conditions.\n  - Expand qualitative analysis with systematic evaluation criteria and larger sample sizes to better demonstrate semantic alignment patterns.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents the Boltzmann Semantic Score (BSS), a novel metric for assessing Large Vision Models (LVMs) using Large Language Models (LLMs) as semantic references. The study aims to evaluate whether LVMs extract medically relevant features comparable to human experts by utilizing LLM encodings of pathology reports. With analysis based on 32 TCGA datasets comprising approximately 9,500 patients, the authors establish baselines for information retrieval and survival prediction tasks, ultimately proposing BSS grounded in state space modeling and Boltzmann distribution principles. The clarity of the manuscript is adequate, but the presentation would benefit from improved rigor in certain areas.\n\n### Major Comments\n1. **Mathematical Formulation Inconsistencies**: Several elements within the theoretical framework lack clarity, undermining the method's rigor.\n   - Justification is insufficient for replacing kT with √d1 in Equation 2 (Page 5).\n   - The definition of the second-order Boltzmann factor in Equation 3 lacks clear theoretical grounding.\n   - Definitions for sets A and D in Equations 4-5 (Page 5) are confusing, particularly regarding the relationship between matching states.\n\n2. **Limited Experimental Validation Scope**: The evaluation is confined to specific tasks and datasets, which may not showcase the method's generalizability.\n   - Only two downstream tasks are assessed (information retrieval and survival prediction), which limits broader applicability.\n   - Results in Table 3 (Pages 9-10) exhibit variable correlation across cancer types, raising concerns about reliability.\n   - The patch number ablation study in Figure 8 (Page 18) does not clarify the implications for robustness or insensitivity.\n\n3. **Methodological Assumptions and Limitations**: Key assumptions underlying BSS require further scrutiny.\n   - Validation of LLM encodings as \"expert knowledge\" is lacking, as LLMs may reflect mere reporting patterns.\n   - Selection of k=5 for nearest neighbors appears arbitrary without validation.\n   - In Figure 7 (Page 9), \"substantial\" agreement indicates inconsistencies in the reference standard itself.\n\n4. **Experimental Design and Statistical Issues**: Potential biases and statistical concerns affect result interpretation.\n   - Comparisons in Tables 2 and 8 (Pages 8, 18) utilize different model architectures, leading to potentially misleading results.\n   - The correlation testing methodology in Algorithms 2-3 (Pages 20-21) may overlook necessary multiple testing adjustments.\n   - The qualitative data presented in Figure 9 (Page 21) are insufficient and may be selectively presented.\n\n### Minor Comments\n- Clarify the theoretical basis for replacing kT with √d1 in the mathematical framework.\n- Address confusing set definitions in Equations 4-5 with clearer notation.\n- Revise the presentation of qualitative examples to include a broader range of cases for robustness.\n\n### Summary Paragraph\nIn evaluating the manuscript against the TMI editorial criteria, notable strengths include the innovative approach in applying LLMs as references for semantic evaluation of LVMs and the comprehensive dataset. However, weaknesses in methodological rigor, limited experimental validation, and inconsistent assumption validation are significant concerns. This work presents a novel metric but requires improvements in theoretical grounding and broader evaluations to enhance its significance and reproducibility.\n\n### Decision Recommendation\nThe recommendation is for **major revision** to address the identified weaknesses and enhance the overall rigor of the study.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the *Boltzmann Semantic Score (BSS)*, a novel metric for evaluating Large Vision Models (LVMs) by using Large Language Models (LLMs) as semantic references. The approach seeks to determine whether LVMs capture clinically meaningful information in medical images comparable to human expertise, employing LLM-based encodings of pathology reports as proxies for expert understanding. Utilizing 32 TCGA datasets (~9,500 patients), the authors benchmark LLM performance on information retrieval and survival prediction tasks, introduce the BSS derived from state-space modeling and Boltzmann distribution principles, and assess the semantic alignment between LVM and LLM embeddings. The manuscript is clearly written and addresses an important and timely question at the intersection of computational pathology and multimodal learning, though several theoretical and experimental aspects require substantial refinement.\n\n---\n\n**Major Comments**  \n1. **Mathematical Formulation Issues**:  \n   - The replacement of the thermodynamic constant \\(kT\\) by \\(\\sqrt{d_1}\\) in Equation 2 is insufficiently justified.  \n   - The “second-order Boltzmann factor” in Equation 3 seems ad hoc without grounding in statistical mechanics.  \n   - The definitions of sets \\(A\\) and \\(D\\) (Equations 4–5) are ambiguous regarding the matching-state relationships.\n\n2. **Limited Experimental Validation**:  \n   - Evaluation covers only two downstream tasks (retrieval and survival prediction), restricting generalizability.  \n   - Correlations across cancer types in Table 3 are inconsistent, raising questions about robustness.  \n   - The patch ablation study (Figure 8) demonstrates stability but not necessarily sensitivity or robustness to visual detail.\n\n3. **Questionable Methodological Assumptions**:  \n   - Treating LLM embeddings as “expert knowledge” is not empirically validated.  \n   - The selection of \\(k = 5\\) neighbors lacks sensitivity analysis.  \n   - Consensus among LLMs (Figure 7) shows moderate agreement, suggesting unreliability of the reference standard.\n\n4. **Experimental and Statistical Concerns**:  \n   - Comparisons use non-equivalent model architectures, complicating interpretation of performance differences.  \n   - Correlation tests may suffer from multiple-testing bias without correction.  \n   - Qualitative figures (Figure 9) are limited and potentially cherry-picked.\n\n---\n\n**Minor Comments**  \n- Clarify notation and variable definitions in Section 2 to improve readability.  \n- Ensure consistent citation of equations and tables.  \n- Verify figure labeling and placement (e.g., Figures 7–9) for clarity.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study advances an innovative concept—quantifying semantic alignment between LLM- and LVM-derived embeddings via an energy-based formulation. Its potential significance lies in enabling principled evaluation of vision-language coherence in medical AI. However, unclear theoretical formulation and restrictive experimental design limit both interpretability and reproducibility. The work’s innovation is evident, but the evaluation lacks sufficient breadth and statistical rigor, and assumptions about LLM “expert” validity remain weakly supported. Strengthening mathematical justification, expanding tasks, and enforcing tighter experimental controls would substantially enhance its contribution.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** – The paper presents an original and potentially impactful idea, but substantial clarification of mathematical foundations and a broader, statistically sound evaluation are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to IEEE TMI**  \nThe manuscript introduces a new, theoretically motivated metric—the **Boltzmann Semantic Score (BSS)**—for quantitatively assessing whether *large vision models* (LVMs) encode medically relevant semantics, as inferred from paired *large language model* (LLM) embeddings of pathology reports.  The work is situated squarely in medical imaging methodology and model evaluation, rather than purely clinical application.  The use of paired text–image embeddings and a physics‑inspired energy‑based formulation is clearly methodological and extends quantitative imaging science.  \n✅ **Within scope for TMI.**\n\n**2. Novelty & Contribution Level**  \nThe paper’s novelty lies in:  \n- defining cross‑modal “semantic similarity” between vision and language embeddings using a Boltzmann‑style energy model;  \n- establishing quantitative correlations between this metric and downstream imaging tasks;  \n- demonstrating consensus measures across LLMs.  \nWhile the use of embedding similarity is not new, the *formalism* and derivation of BSS, and its application as a modality‑independent evaluative metric, are distinct.  The originality is methodological rather than incremental.  \n→ Moderate‑to‑strong novelty.\n\n**3. Technical & Experimental Rigor**  \nTheoretical derivation is carefully presented; experiments use 32 TCGA datasets, five LLMs, and seven LVMs with reproducible protocols.  Statistical tests, cross‑validation, and code release are well documented.  Limitations and bias sources are acknowledged.  \nHowever, clarity on hyper‑parameter sensitivity (e.g., choice of k, normalization, computational stability) and on whether BSS can be computed reproducibly across institutions could be expanded.  \n→ Technically solid but could benefit from additional validation beyond TCGA.\n\n**4. Clarity and Presentation**  \nThe manuscript is generally clear, mathematically consistent, and well illustrated.  A few sections are dense and might be condensed for a journal audience. Figures 5–6 and Table 3 are informative but could be streamlined. Minor English and formatting issues remain from conference style.  \n\n**5. Ethical and Reproducibility Compliance**  \nWork uses de‑identified TCGA and TCGA‑Reports datasets with public access and cites data‑use statements. Code availability is declared.  No ethical concerns noted.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe authors propose the **Boltzmann Semantic Score**, a theoretically derived metric to evaluate the semantic fidelity of large vision models in digital pathology. They link image‑embedding neighborhoods with corresponding text‑embedding neighborhoods from LLM‑encoded pathology reports, treating similarity as an energy distribution in a Boltzmann framework. BSS quantifies how well visual representations align with expert textual semantics. Experiments across 32 TCGA cancers compare multiple LLMs and LVMs, assess BSS correlations with retrieval accuracy and survival prediction, and analyze inter‑LLM consensus.\n\n**2. Strengths**  \n- Clearly methodological innovation with formal mathematical grounding.  \n- Large‑scale evaluation using publicly available datasets and multiple foundational models.  \n- Demonstration that BSS correlates with clinically relevant downstream tasks.  \n- Reproducibility and open‑code commitment.  \n- Provides an interpretable, quantitative alternative to subjective attention‑map evaluations.\n\n**3. Weaknesses**  \n- Validation confined to TCGA; cross‑institutional or non‑pathology imaging not tested.  \n- The physical analogy (Boltzmann energy) may be conceptually appealing but empirically could be simplified; ablation on functional form lacking.  \n- Limited discussion of computational cost and sensitivity to hyper‑parameters (k, distance metric).  \n- Figures and tables are dense; readability should be improved for journal format.  \n- Clinical interpretability of absolute BSS values is still qualitative.\n\n**4. Major Comments**  \n1. **External generalization:** Demonstrate BSS on at least one additional dataset or imaging modality (radiology, microscopy from another source) to show robustness.  \n2. **Parameter sensitivity:** Add analysis of how k and embedding dimensionality affect BSS stability.  \n3. **Method comparison:** Benchmark BSS against simpler alignment metrics (e.g., CCA, mutual information, cosine similarity) to justify the Boltzmann formulation.  \n4. **Implementation detail:** Clarify normalization constants, graph construction, and how differences in embedding dimension (d₁ ≠ d₂) are handled.  \n5. **Interpretability:** Provide examples linking high/low BSS values to concrete histologic semantics to aid clinical readers.\n\n**5. Minor Comments**  \n- Edit for grammar and consistent figure notation (e.g., use uniform LLM/LVM abbreviations).  \n- Include units/axes labels on all plots.  \n- Merge or summarize Tables 1a–d to reduce redundancy.  \n- Adapt manuscript from conference style (sections 1–8) to TMI format.  \n- Verify persistent GitHub link and dataset license.\n\n**6. Overall Recommendation**  \n🟡 **Major Revision** – The paper presents a promising, novel methodology suitable for IEEE TMI but requires broader validation and refinement of presentation and justification before acceptance.\n\n**7. Confidence Level:** 4 / 5 – High confidence in technical evaluation and field relevance.\n\n---\n\n**Summary Assessment:**  \nInnovative cross‑modal metric bridging medical imaging and language models; within TMI scope; theoretically sound and well executed. With additional validation and editorial refinement, it could make a valuable contribution to quantitative methodology in medical imaging.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a new theoretically grounded metric, the *Boltzmann Semantic Score (BSS)*, designed to quantify how effectively large vision models (LVMs) capture medically relevant semantics derived from large language model (LLM) embeddings of pathology reports. The work is methodological, focused on model evaluation rather than clinical application, and it combines text–image embeddings through a physics‑inspired Boltzmann formulation. The paper is clearly written, mathematically consistent, and aligned with the scientific scope of *IEEE Transactions on Medical Imaging*.\n\n---\n\n**Major Comments**  \n1. **External Generalization:** Validation is currently limited to TCGA datasets. Demonstrating BSS on additional datasets or imaging modalities (e.g., radiology, independent pathology collections) would strengthen claims of generality and robustness.  \n2. **Parameter Sensitivity:** The paper should include analyses of how hyper‑parameters such as *k*, distance metrics, and embedding dimensionality affect BSS stability and reproducibility across institutions.  \n3. **Comparison with Simpler Metrics:** Benchmark BSS against more conventional alignment measures (e.g., CCA, cosine similarity, mutual information) to justify the Boltzmann energy formulation and highlight its added value.  \n4. **Implementation Details:** Clarify normalization procedures, graph construction steps, and how mismatched embedding dimensions (*d₁ ≠ d₂*) are reconciled to ensure full reproducibility.  \n5. **Interpretability:** Add examples illustrating high versus low BSS values in relation to identifiable histologic semantics to improve clinical interpretability.  \n6. **Presentation and Density:** Figures 5–6 and Table 3 are informative but complex; consolidating or simplifying them could improve readability.  \n\n---\n\n**Minor Comments**  \n- Revise grammar and formatting for journal style; harmonize use of LVM/LLM abbreviations.  \n- Ensure all plots include units and axis labels.  \n- Combine redundant tables (e.g., Tables 1a–d).  \n- Verify the functionality of the GitHub repository and dataset license statements.  \n- Moderate overly dense sections to improve flow and accessibility.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is methodologically significant, offering a quantitative and interpretable metric for cross‑modal evaluation of medical imaging models. Innovation derives from applying a Boltzmann energy formalism to semantic alignment between image and language embeddings. Evaluation is comprehensive within the TCGA setting but requires broader validation and sensitivity analyses for stronger empirical grounding. Reproducibility is commendable, supported by public datasets and declared code release. Minor editorial improvements would enhance clarity and accessibility.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The work is well within *IEEE TMI*’s scope and demonstrates solid theoretical contributions, but broader validation, comparative analysis, and improved presentation are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents the **Boltzmann Semantic Score (BSS)**, a theoretically motivated metric aimed at quantitatively assessing whether large vision models (LVMs) effectively encode medically relevant semantics based on paired embeddings from large language models (LLMs) of pathology reports. This work contributes to the field of medical imaging methodology and model evaluation, offering a novel approach rather than purely clinical application. Its methodological advancements, particularly regarding the use of paired text-image embeddings and a physics-inspired energy-based formulation, represent a meaningful extension to quantitative imaging science.\n\n**Major Comments:**  \n1. **External Generalization:** Additional validation of BSS on at least one dataset or imaging modality (e.g., radiology or microscopy from another source) is necessary to demonstrate robustness.  \n2. **Parameter Sensitivity:** An analysis of how the parameter 'k' and embedding dimensionality influence the stability of BSS should be included.  \n3. **Method Comparison:** A comparison of BSS against simpler alignment metrics (e.g., CCA, mutual information, cosine similarity) is essential to justify the application of the Boltzmann formulation.  \n4. **Implementation Details:** Clarity is needed regarding normalization constants, the construction of graphs, and the handling of differences in embedding dimensions (d₁ ≠ d₂).  \n5. **Interpretability:** Provide examples that correlate high/low BSS values with specific histologic semantics to enhance understanding for clinical readers.\n\n**Minor Comments:**  \n- Correct grammar and ensure consistent figure notation (e.g., uniform abbreviations for LLM/LVM).  \n- Make sure all plots include units/axes labels.  \n- Consider merging or summarizing Tables 1a–d to reduce redundancy.  \n- Adapt the manuscript from its conference style to align with TMI formatting.  \n- Verify the functionality of the persistent GitHub link and the dataset license.\n\n**Summary Paragraph:**  \nThe manuscript introduces a novel metric bridging medical imaging and language models, demonstrating significant methodological innovations and potential in quantitative evaluation. While the work is technically sound and replicable, further validation beyond the TCGA dataset and enhancements in presentation clarity are needed. Specifically, attention to external validation and interpretability are required to address some limitations.\n\n**Decision Recommendation:**  \n🟡 **Major Revision** – The manuscript effectively proposes a promising methodology suitable for IEEE TMI but requires more extensive validation and refinement of its presentation and justification for acceptance.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Ali Bashashati",
      "Ali Khajegili Mirabadi",
      "Hossein Farahani",
      "Katherine Rich"
    ],
    "url": "pdfs/iclr.cc-2025-conference_4a62e78f3be27e86a9498d083dc2dcb4d2136f2f.pdf",
    "remote_url": "https://openreview.net/pdf/4a62e78f3be27e86a9498d083dc2dcb4d2136f2f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Vision-language model",
      "fine-grained alignment",
      "large-scale pre-training",
      "CT image"
    ],
    "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.",
    "decision": "Accept (Spotlight)",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes a fine-grained vision-language model (fVLM) to improve CT image interpretation by associating specific anatomical regions in CT scans with corresponding descriptions in radiology reports, addressing limitations of global image-report contrastive methods. It introduces a large dataset, MedVL-CT69K, encompassing 272,124 CT scans and achieves state-of-the-art performance. Key contributions include anatomy-level contrastive learning and a method to handle false negatives.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. Fine-Grained Anatomy-Level Alignment: The paper introduces a novel, fine-grained vision-language model (fVLM) that aligns anatomical regions in CT images with corresponding report descriptions.\n2. Large and Comprehensive Dataset: Experiments were conducted on a range of datasets including the largest CT dataset to date (MedVL-CT69K)\n3. Effectively tackling shortcomings of their method by introducing the dual false-negative reduction approach.\n\n### Weaknesses\n\n1. Experiments are incomplete: Table 1 doesn't include a performance evaluation of the methods from Table 3, namely CT-CLIP, CT-VocabFine, CT-LiPro. Table 3 doesn't include performance evaluation of the methods from Table 1. In both cases it's not argued why those experiments were not conducted. For Table 1, and Table 2 it's also unclear how the 2D approaches were adapted for 3D and how their evaluation was then done. Additionally, the evaluation metrics in Table 1 and Table 3 differ. Why? Furthermore, the performance comparison in Table 2 should also include comparisons to other CT-CLIP approaches, not only natural image counterparts (he it's again not clear how the 2D models were adapted to 3D).  \n2. Report generation uses only a single NLU metric. Other NLU metrics such as ROUGE would be interesting. Furthermore, using the GREEN metric[1] or RadFact[2] for evaluation would allow assessing clinical relevance.\n3. T-SNE visualization: An (additional) comparison of the embeddings to CT-CLIP would be be interesting to see if they have the same clustering behavior. \n4. Ablations study on masking missing: The masking approach is not well investigate and no ablation studies are done. One intuitive baseline would be the anatomical cropping of the image. What's the advantage of the masking approach? \n5. The authors approach only works for CT scans, since it relies on anatomy segmentations from TotalSegmentator. Furthermore, only on the classes which are available in TotalSegmentator.\n\n[1] Ostmeier, S., Xu, J., Chen, Z., Varma, M., Blankemeier, L., Bluethgen, C., ... & Delbrouck, J. B. (2024). GREEN: Generative Radiology Report Evaluation and Error Notation. arXiv preprint arXiv:2405.03595.\n\n[2] Bannur, S., Bouzid, K., Castro, D. C., Schwaighofer, A., Bond-Taylor, S., Ilse, M., ... & Hyland, S. L. (2024). MAIRA-2: Grounded Radiology Report Generation. arXiv preprint arXiv:2406.04449.\n\n### Questions\n\n1. What's the authors reproducibility statement? Will the dataset, code and weights be released? Especially publishing the dataset would be of great value for the community.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a fine-grained vision-language model (fVLM) aimed at enhancing CT image interpretation by associating specific anatomical regions in CT scans with corresponding descriptions in radiology reports. This approach seeks to overcome the limitations of existing global image-report contrastive methods. Notably, the work utilizes a substantial dataset, MedVL-CT69K, which includes 272,124 CT scans, achieving state-of-the-art performance. The main contributions consist of anatomy-level contrastive learning and a strategy to address false negatives.\n\n**Major Comments:**\n1. **Incomplete Experiments:** The performance evaluation in Table 1 omits results for methods listed in Table 3, particularly CT-CLIP, CT-VocabFine, and CT-LiPro. Conversely, Table 3 fails to report on the methods in Table 1. The rationale for these omissions is unclear. Additionally, the adaptations of 2D approaches for 3D evaluation are not well-explained. There are inconsistencies in evaluation metrics used in Tables 1 and 3.\n2. **Report Generation Metrics:** The evaluation relies solely on one NLU metric. Incorporating additional metrics such as ROUGE, GREEN, or RadFact could enhance the assessment of clinical relevance.\n3. **T-SNE Visualizations:** It would be beneficial to compare the embeddings with CT-CLIP to examine clustering behavior.\n4. **Ablation Study on Masking:** The masking technique lacks thorough investigation, and no ablation studies are performed. A baseline using an anatomical cropping approach could clarify the advantages of the proposed masking method.\n5. **Model Limitations:** The model is restricted to CT scans due to its reliance on anatomical segmentations from TotalSegmentator, limiting its applicability to available classes.\n\n**Minor Comments:**\n- Clarification of the adaptation of 2D models to 3D would improve the manuscript’s clarity.\n- A clearer explanation of evaluation metrics across tables would enhance the understanding.\n- Minor typographical revisions are needed throughout the document for better presentation.\n\n**Summary Paragraph:**  \nThis study addresses significant challenges in CT image interpretation through the innovative fVLM and a large-scale dataset. While the contributions are noteworthy, particularly in contrastive learning and addressing false negatives, the work suffers from incomplete experiments and limitations in the robustness of evaluation metrics. The reproducibility and broader applicability of the model warrants further exploration, especially considering its constraints on anatomical segmentations. The review indicates that while there are promising aspects, several improvements could enhance the manuscript's overall contribution to the field.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a fine-grained vision–language model (fVLM) designed to enhance CT image interpretation by linking specific anatomical regions to corresponding textual descriptions from radiology reports. To support this approach, the authors introduce *MedVL-CT69K*, a large dataset comprising 272,124 CT scans. The proposed model incorporates anatomy-level contrastive learning and a dual false-negative reduction strategy to overcome limitations of previous global contrastive methods. The paper is generally well written and clear in its exposition, with promising performance results suggesting state-of-the-art achievements in CT image–report alignment.\n\n**Major Comments**  \n1. **Incomplete Experimental Evaluation:** The experimental design is inconsistent across Tables 1–3. Some key methods (CT-CLIP, CT-VocabFine, CT-LiPro) are not evaluated across all tables, and the authors do not explain why. It is also unclear how 2D methods were adapted for 3D data, and why evaluation metrics differ between tables. The comparison in Table 2 should include other CT-CLIP variants, not only models trained on natural images.  \n2. **Limited Report Generation Metrics:** The evaluation of the report generation task uses only a single NLU metric. Incorporating additional metrics such as ROUGE, GREEN, or RadFact would provide a more comprehensive assessment, particularly regarding clinical relevance.  \n3. **Embedding Visualization:** The t-SNE visualization could be strengthened by including comparisons with CT-CLIP to verify whether similar clustering patterns are observed.  \n4. **Ablation on Masking Strategy:** The masking-based approach is insufficiently investigated. An ablation comparing this to a simpler anatomical cropping baseline would clarify its benefit.  \n5. **Scope Limitation:** The method depends on anatomical segmentation from TotalSegmentator, restricting applicability to CT data and to the anatomical classes provided by that tool.  \n\n**Minor Comments**  \n- Clarify the reproducibility plan, specifying whether code, model weights, and particularly the dataset will be released, as this would be valuable for the community.  \n- Minor inconsistencies in table labeling and explanations could be revised for clarity.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper offers a significant and innovative contribution by introducing fine-grained anatomy-level vision–language alignment and a large CT dataset. However, evaluation completeness and breadth are limited, leaving some uncertainty about the robustness of the results. The work is technically sound but would benefit from more extensive ablation studies and comparative analyses. Reproducibility is uncertain pending clarity on dataset and code release plans.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The study introduces a promising and potentially impactful approach, but it requires more thorough experimental comparisons, expanded evaluation metrics, and clearer reproducibility assurances before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors present an vision-language model for  anatomy-aware medical report generation, adressing the task of false negative reduction.  A large (private) dataset is curated and good performances on public datasets are presented.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n* The study addresses an interesting multi-modal learning problem from the medical domain\n* The presented false negative reduction is a meaningful integration to vision-language model report generation\n* The results are promising indicating the relevance of this addition\n\n### Weaknesses\n\n* The methodology is not quite clear\n* There is no codebase, preproduction may be a challenge.\n\n### Questions\n\n* Image Encoder Setup: The authors mention using a ViT base model with MAE pretraining on ImageNet-1K and a patch size of 16 x 16 x 32 (referenced in A.2). However, it’s unclear how they process 3D volumes with 3D patch embeddings through a 2D ViT model. Although they provide details on the training procedure, the overall model architecture remains vague. Given the lack of source code, a clearer description of the model's structure is crucial. Can the authors improve?\n* Comparison with 2D CLIP Methods: The comparisons made with 2D CLIP methods in Table 1 are not fully explained. Specifically, it's unclear how they adapted these methods to a 3D context and how the training was conducted. Please clarify.\n* Evaluation with CT-CLIP: In Table 3, the comparison with CT-CLIP models is ambiguous. Did the authors fine-tune a pre-trained model (based on their in-house data) on CT-RATE, or was there a different approach taken?\n* t-SNE Consistency: To enhance the consistency of the embedding spaces shown in Fig. 7, the authors could consider setting a random seed before calculating the t-SNE embeddings.\n* HU Value Clipping: The authors chose to clip HU values between -300 and 400, which raises a question: Does this range cover all abnormalities? For instance, lung-related abnormalities are typically closer to -1000 (representing air), while bone-related abnormalities can reach values close to 1000. Since they utilized both non-contrast and contrast-enhanced CT scans (arterial, venous, and delayed phases), did they apply this same HU preprocessing across all scans? Additionally, did the authors use this range for the CT-RATE training ablations (which is different than the original CT-RATE paper)? Please clarify.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nnone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a vision-language model aimed at generating anatomy-aware medical reports, specifically targeting the reduction of false negatives. The authors have curated a substantial private dataset and demonstrate promising performances on public datasets, contributing to the field of multi-modal learning in medical applications.\n\n### Major Comments\n1. **Methodology Clarity**: The methodology lacks clarity, particularly regarding the model architecture and processing of 3D volumes with a 2D ViT model.\n2. **Codebase Availability**: The absence of a codebase may pose challenges for reproducibility.\n3. **Model Architecture**: The overall model architecture is vaguely described, necessitating a more comprehensive explanation.\n4. **Comparative Analysis**: The adaptation of 2D CLIP methods to a 3D context is not fully elaborated, particularly in the training details.\n5. **CT-CLIP Evaluation**: The evaluation method involving CT-CLIP models is ambiguous, and further clarification on fine-tuning practices is required.\n6. **HU Value Clipping**: Questions arise regarding the clipping of HU values between -300 and 400, particularly in relation to covering all potential abnormalities. Clarification on the uniform application of this clipping across different CT scans is needed.\n\n### Minor Comments\n- The embedding spaces presented in Fig. 7 could benefit from consistency improvements by setting a random seed before calculating t-SNE embeddings.\n\n### Summary Paragraph\nOverall, the study presents significant innovations in the intersection of vision and language models for medical report generation, showcasing an important contribution towards improving accuracy in this domain. However, the methodological clarity and reproducibility aspects need considerable attention. Specifically, the lack of detailed explanations regarding the model architecture and processing approaches undermine the evaluation's robustness. While the study exhibits promising findings, further clarifications and improvements are essential to meet the rigor required for reproducibility and significance in the field.\n\n### Decision Recommendation\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a vision–language model for anatomy-aware medical report generation, targeting the reduction of false negatives. The authors curate a large private dataset and report promising results on several public benchmarks. The topic is timely and relevant to multimodal learning in medical imaging, and the overall presentation demonstrates potential, though methodological clarity and reproducibility are limited.\n\n---\n\n**Major Comments**  \n1. **Methodological Clarity:** The architecture and implementation details are insufficiently described. The model reportedly uses a ViT base with MAE pretraining on ImageNet-1K and a patch size of 16×16×32, but it remains unclear how 3D volumes are handled through a 2D ViT. Without code, this ambiguity limits reproducibility. A more transparent description of the full architecture and data flow is recommended.  \n2. **Comparison with Baselines:** The adaptation and training procedures used for 2D CLIP methods (Table 1) and CT-CLIP models (Table 3) are insufficiently explained. It is unclear how these 2D models were adjusted for 3D inputs or whether CT-CLIP was fine-tuned on in-house data before evaluation. These details are essential for fair comparison.  \n3. **Evaluation Reproducibility:** The study does not release its codebase, which poses challenges for verification and reuse. Availability of code or detailed pseudocode would enhance reproducibility.  \n4. **Preprocessing Clarification:** The HU clipping range (−300 to 400) may not encompass all relevant abnormalities (e.g., air at −1000, bone near 1000). Clarification is needed on whether this range was uniformly applied across all scan types, including contrast-enhanced variants, and whether it matches the setup used in CT-RATE experiments.  \n\n---\n\n**Minor Comments**  \n- To improve consistency of t-SNE visualizations (Fig. 7), a fixed random seed should be applied.  \n- The paper’s methodological exposition could be refined for greater clarity and accessibility.  \n\n---\n\n**Summary (TMI Criteria)**  \nThe work is significant in addressing false negative reduction within multimodal report generation and introduces a meaningful addition to the field. Innovation lies in integrating anatomy awareness, though novelty is somewhat constrained by limited architectural transparency. Evaluation results are encouraging but lack full interpretability due to incomplete methodological details. Reproducibility remains a concern given the absence of public code and insufficient procedural documentation.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** — The study demonstrates promise and relevance but requires clearer methodological explanations, better-documented comparisons, and enhanced reproducibility to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work discusses the fine-grained alignment issue for vision-language clip-like pretraining for CT and reports. To improve the granularity of alignment the authors propose to pre-segment key anatomical structures in CT using a public segmentation model (Totalsegmenter), and to pre-segment descriptions corresponding to individual anatomical structures in the free-text report using an open-access LLM (Qwen). This allows fine-grained contrastive learning at the anatomical structure level. Considerations are made to avoid pushing representations of the same organ/condition away. Improved performances are shown for zero-shot abnormality detection compared with several existing approaches.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe idea of increasing the granularity of contrastive learning by data pre-segmentation using public tools is neat, simple, and straightforward. \n\nThe paper is written with sufficient clarity, where the insights behind each design and the implementations details are well-presented. \n\nImproved performance on zero-shot abnormality detection is shown compared with some existing works.\n\n### Weaknesses\n\nFor a comprehensive assessment of report generation some essential scores are missing, such as BLEU 1-3, ROUGE-L, and METEOR. Also, comparisons with vision encoders from peer vision-language pre-training for CT works may be needed. This is my major concern as the authors have claimed improved performance on report generation task.\n\nDespite improved granularity, attributing sentences of reports `mentioning` a structure to the image feature of that structure may sometimes be misleading especially when manual verification/correction is not accessible. E.g., in practice pancreatitis is often associated with the entire abdomen instead of the pancreas alone and neighboring structures. The authors are encouraged to comment on this.\n\n### Questions\n\nA high-level question: The capability of open-source image segmentation tools (totalsegmenter and/or SAM family) and open-source LLM advance fiercely -- does it imply that the utility of (un-/weakly-supervised) vision-language pretraining for CT will gradually phase out? One may directly construct semantic labels from text reports using strong enough LLM tools and turn the problem back to large scale supervised learning. I am curious to hear the authors’ insight.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nA separate ICLR ethics reviewer might not be needed. However, as the proposed study involves curating and analyzing human CT images and reports, the authors need to make sure that essential ethical approval is obtained, and pertinent privacy laws/regulations are complied with.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript addresses the fine-grained alignment issue in vision-language clip-like pretraining for CT and its corresponding reports. The authors propose an innovative approach to enhance granularity by pre-segmenting key anatomical structures in CT images using the public segmentation model Totalsegmenter, and pre-segmenting descriptions related to these structures in free-text reports with the open-access LLM Qwen. The study demonstrates enhanced performance in zero-shot abnormality detection compared to existing methods, showcasing both clarity in writing and the significance of its contributions.\n\n**Major Comments:**  \n1. Essential evaluation metrics for report generation such as BLEU 1-3, ROUGE-L, and METEOR are omitted, impacting the comprehensive assessment of the proposed method. Comparisons with vision encoders from other peer works should also be included for robustness, as the authors claim improved performance on the report generation task.\n2. While the granularity is improved, linking report sentences that mention a structure to the respective image feature may be misleading. Manual verification is often challenging; for instance, conditions like pancreatitis are associated more with broader regions than singular anatomical features. The authors are encouraged to discuss this limitation.\n\n**Minor Comments:**  \n- The paper presents its ideas clearly, with well-defined insights behind the design and implementation details.  \n- Improvement in zero-shot abnormality detection over existing works is commendable.\n\n**Summary Paragraph:**  \nThis manuscript presents a significant contribution to the field by enhancing the granularity of contrastive learning through effective data pre-segmentation. However, it lacks adequate evaluation metrics and comprehensive comparisons to substantiate claims about its contributions to report generation. Furthermore, attention should be given to the interpretation of report structures in relation to image features. Overall, the work is innovative but requires more extensive evaluation and discussion to strengthen its claims.\n\n**Decision Recommendation:**  \nMinor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the fine-grained alignment problem in vision–language pretraining for CT images and corresponding reports. The authors propose a strategy that first segments anatomical structures in CT scans using a public segmentation model (TotalSegmentator) and then segments related descriptions in radiology reports using an open-access large language model (Qwen). This enables contrastive learning at the anatomical-structure level. The paper is clearly written, with transparent explanations of design choices and implementation details. Reported experiments indicate improved zero-shot abnormality detection performance compared with several existing methods.\n\n---\n\n**Major Comments**  \n1. **Evaluation metrics for report generation:** The study lacks key quantitative metrics for the report generation task (e.g., BLEU 1–3, ROUGE-L, METEOR). These are essential for a comprehensive assessment, especially since improved report generation performance is claimed.  \n2. **Comparative experiments:** Additional comparisons are needed with vision encoders from other CT-specific vision–language pretraining studies to substantiate the generality and competitiveness of the proposed model.  \n3. **Attribution accuracy:** Although finer granularity is beneficial, automatically linking report sentences to image features of the mentioned structure could be misleading without manual verification. For instance, pancreatitis may involve neighboring abdominal structures beyond the pancreas itself. Clarification or discussion of such attribution limitations would strengthen the paper.  \n4. **Conceptual question on future directions:** The reviewer raises a high-level question: given rapid improvements in open-source segmentation and LLM tools, will un-/weakly-supervised vision–language pretraining for CT gradually lose relevance as direct large-scale supervised learning becomes feasible? The authors’ perspective on this would be valuable.\n\n---\n\n**Minor Comments**  \n- The paper is generally clear and well-organized.  \n- The reviewer notes no specific typographical or formatting issues.  \n- Ethical compliance should be ensured given the use of human CT data and reports.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The work addresses a relevant challenge in fine-grained CT-text alignment for medical imaging.  \n**Innovation:** The approach of using public segmentation models and open LLMs for pre-segmentation is simple yet conceptually interesting.  \n**Evaluation:** Experimental validation is focused on abnormality detection but lacks essential report-generation metrics and broader comparative studies.  \n**Reproducibility:** The use of open tools supports reproducibility, though details on data use and labeling could be clarified. Overall, the contribution is promising but requires stronger evaluation and discussion of limitations.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes fVLM (fine-grained Vision-Language Model) for CT image understanding, addressing limitations of existing vision-language models that perform global image-text alignment. The method decomposes CT scans into 36 anatomical regions using TotalSegmentator (Figure 2) and radiology reports into anatomy-specific descriptions using LLMs (Section 3.1). Fine-grained contrastive learning aligns visual and textual embeddings at the anatomy level rather than globally (Equation 2). To handle false negatives arising from abundant normal samples and similar abnormalities, the authors introduce dual false negative reduction with co-teaching strategy (Section 3.3, Equation 3). The approach is evaluated on MedVL-CT69K dataset comprising 272,124 CT scans from 69,086 patients. Results show 81.3% average AUC across 54 disease diagnosis tasks, outperforming CLIP by 12.9% and supervised methods by 8.0% (Table 1). On public benchmarks CT-RATE and Rad-ChestCT, fVLM achieves 7.4% and 4.8% absolute AUC improvements respectively (Table 2).\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies**: The contrastive learning framework contains notation and computational concerns that affect clarity and correctness.\n  - Equation 1 uses Nj as both batch size and number of structurally complete samples, creating ambiguity about the actual denominator in softmax normalization and potential computational errors\n  - The ground-truth label construction for yi2t and yt2i in Equation 2 lacks explicit mathematical definition, making the loss computation unclear\n  - Equation 3 shows label correction but doesn't specify how α=0.5 was determined or provide sensitivity analysis to this hyperparameter choice\n\n• **Limited experimental validation scope**: The evaluation lacks comprehensive baselines and statistical rigor expected for medical AI claims.\n  - Table 1 shows missing specificity and sensitivity values for supervised baselines, preventing complete performance comparison across all methods\n  - No statistical significance testing reported for the claimed improvements across 54 disease tasks, making it unclear if differences are statistically meaningful\n  - Reader study mentioned in Section 4.2 is relegated to appendix without main paper discussion, despite being crucial for clinical validation\n\n• **Questionable technical design choices**: Several methodological decisions appear suboptimal or inadequately justified.\n  - Figure 2 shows anatomical grouping from 104 to 36 regions but lacks quantitative analysis of how this grouping affects performance versus computational efficiency\n  - Section 3.1 describes using \"null\" strings and default descriptions for missing anatomies, potentially introducing noise without systematic evaluation of this preprocessing step\n  - The co-teaching strategy in Section 3.3 requires training two models simultaneously, doubling computational cost without thorough cost-benefit analysis\n\n• **Dataset and reproducibility concerns**: The large-scale dataset claims are not sufficiently substantiated with proper validation.\n  - MedVL-CT69K dataset details are sparse with disease distribution relegated to appendix tables, making it difficult to assess data quality and annotation reliability\n  - No inter-annotator agreement scores provided for the expert radiologist annotations used in validation and test sets\n  - Code availability mentioned on page 1 but implementation details in Appendix A.2 are insufficient for reproduction of the complex anatomy parsing and report decomposition pipeline\n\n## Suggestions for Improvement\n\n• **Clarify mathematical formulations**: Provide precise mathematical definitions and validation for all computational components.\n  - Define Nj explicitly in each context and provide clear algorithmic description of softmax normalization when samples are truncated by RandomCrop\n  - Add explicit mathematical definition for ground-truth label construction yi2t and yt2i with worked examples\n  - Include hyperparameter sensitivity analysis for α in Equation 3 and justify the choice of α=0.5 with ablation studies\n\n• **Strengthen experimental validation**: Expand evaluation methodology to meet medical AI standards for clinical applicability.\n  - Complete all baseline comparisons in Table 1 with missing metrics and provide statistical significance testing using appropriate medical statistics\n  - Include the reader study results in main paper with detailed methodology and statistical analysis of radiologist agreement\n  - Add cross-dataset validation beyond CT-RATE and Rad-ChestCT to demonstrate generalization capability\n\n• **Justify and optimize technical decisions**: Provide systematic evaluation of key methodological choices with quantitative support.\n  - Conduct ablation study on anatomical grouping strategies showing performance-efficiency trade-offs across different grouping granularities\n  - Systematically evaluate the impact of \"null\" string handling and default descriptions on model performance with controlled experiments\n  - Provide computational cost analysis comparing co-teaching approach with single model alternatives and demonstrate when the additional cost is justified\n\n• **Enhance dataset documentation and reproducibility**: Provide comprehensive dataset validation and implementation details for scientific reproducibility.\n  - Include detailed dataset statistics, annotation protocols, and inter-annotator agreement scores in main paper with quality validation metrics\n  - Provide complete implementation details including anatomy parsing pipeline, report decomposition algorithms, and training procedures in main paper or detailed supplementary material\n  - Release standardized evaluation protocols and baseline implementations to enable fair comparison with future methods",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents the fine-grained Vision-Language Model (fVLM) aimed at enhancing CT image understanding by addressing the shortcomings of existing global image-text alignment methods. By segmenting CT scans into 36 anatomical regions and generating anatomy-specific descriptions from radiology reports, the authors propose a novel contrastive learning framework that aligns visual and textual embeddings at a more granular level. The evaluation of fVLM on the MedVL-CT69K dataset demonstrates significant performance improvements over competing methods, marking a notable contribution in the domain of medical imaging.\n\n**Major Comments:**  \n1. **Mathematical Formulation Inconsistencies:** \n   - Ambiguities arise in Equation 1, where Nj is used to denote both batch size and the number of structurally complete samples, leading to potential computational errors in softmax normalization.\n   - The construction of ground-truth labels for yi2t and yt2i in Equation 2 lacks clarity, which undermines the understanding of loss computation.\n   - Equation 3 introduces label correction but fails to justify the choice of α=0.5 or provide a sensitivity analysis for this hyperparameter.\n\n2. **Limited Experimental Validation Scope:** \n   - Table 1 lacks specificity and sensitivity values for supervised baselines, which hinders a comprehensive performance comparison.\n   - The absence of statistical significance testing for reported improvements leaves uncertainty regarding their meaningfulness.\n   - The reader study referenced in Section 4.2 is included in the appendix, minimizing its importance in clinical validation.\n\n3. **Questionable Technical Design Choices:** \n   - The anatomical grouping from 104 to 36 regions in Figure 2 lacks a quantitative performance versus computational cost analysis.\n   - The use of \"null\" strings for missing anatomies as described in Section 3.1 could introduce unwanted noise without adequate justification.\n   - The co-teaching strategy in Section 3.3 increases computational expenses without a thorough analysis of its benefits.\n\n4. **Dataset and Reproducibility Concerns:** \n   - Sparse details on the MedVL-CT69K dataset and relegated disease distribution data hinder quality assessment.\n   - No inter-annotator agreement scores are presented for the expert annotations used in validation.\n   - While code availability is mentioned, the implementation details in Appendix A.2 are insufficient for reproducing the processes described.\n\n**Minor Comments:**  \n- Improve mathematical clarity by explicitly defining Nj in every context and clarifying the algorithmic structure of softmax normalization.\n- Provide definitions for ground-truth label construction with illustrative examples.\n- Offer a sensitivity analysis for α in Equation 3, justifying the choice based on results of ablation studies.\n- Enhance the completeness of baseline comparisons and include statistical significance testing for robustness.\n- Introduce reader study results in the main text with detailed methodology.\n- Ensure documentation of dataset statistics, annotation protocols, and agreement scores are directly available in the main body of the paper.\n  \n**Summary Paragraph:**  \nIn terms of significance, fVLM presents a noteworthy advancement in CT image understanding by achieving significant AUC improvements over existing methods. However, the manuscript faces challenges in innovation due to inconsistencies in mathematical formulations and the limited breadth of experimental validation. While the evaluation demonstrates promising results, the lack of rigorous baselines and formal statistical analysis raises concerns regarding reproducibility and credibility. Addressing these weaknesses will be crucial for enhancing the impact and reliability of the fVLM framework.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **fVLM (fine-grained Vision-Language Model)** for CT image understanding. Unlike prior global vision-language alignment methods, fVLM performs fine-grained alignment by decomposing CT scans into 36 anatomical regions (via TotalSegmentator) and radiology reports into anatomy-specific descriptions (via LLMs). A dual false negative reduction with co-teaching strategy addresses the challenge of similar normal and abnormal cases. The model is evaluated on the MedVL-CT69K dataset and shows notable AUC improvements over CLIP and supervised baselines, as well as state-of-the-art performance on CT-RATE and Rad-ChestCT benchmarks. While the method is conceptually strong and results are promising, several methodological, experimental, and reproducibility concerns limit its current clarity and reliability.  \n\n**Major Comments**  \n1. **Mathematical formulation inconsistencies** – The presentation of the contrastive learning framework is unclear and potentially inconsistent.  \n   - Equation 1 ambiguously defines *Nⱼ*, used both as batch size and number of structurally complete samples, creating confusion in normalization.  \n   - The ground-truth label terms *yᵢ₂ₜ* and *yₜ₂ᵢ* in Equation 2 are not formally defined, leaving the loss computation ambiguous.  \n   - Equation 3 introduces label correction but provides no rationale for the choice of α = 0.5 or sensitivity analysis thereof.  \n\n2. **Limited experimental validation** – The evaluation does not fully meet expected standards for clinical AI research.  \n   - Table 1 omits specificity and sensitivity for several supervised baselines, hindering full comparison.  \n   - Statistical significance testing across 54 disease tasks is absent, undermining claims of improvement.  \n   - The reader study is confined to the appendix despite its importance for clinical validation.  \n\n3. **Questionable technical design choices** – Several design decisions are insufficiently justified.  \n   - The grouping from 104 to 36 regions (Figure 2) is not supported by quantitative analysis of trade-offs.  \n   - The use of “null” strings for missing anatomies may introduce noise and is not evaluated.  \n   - The co-teaching approach doubles computational cost without cost-benefit justification.  \n\n4. **Dataset and reproducibility concerns** – Dataset description and implementation transparency are inadequate.  \n   - Limited reporting of MedVL-CT69K distribution and annotation details restricts assessment of data quality.  \n   - No inter-annotator agreement scores are provided.  \n   - Code availability is mentioned but Appendix A.2 lacks sufficient technical detail for reproducibility.  \n\n**Minor Comments**  \n- Clarify all notation in equations to ensure mathematical consistency.  \n- Improve figure labeling and equation referencing for readability.  \n- Expand Table captions to explain metrics and abbreviations.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper presents a significant and innovative attempt to achieve fine-grained visual-textual alignment in CT interpretation. Its conceptual contribution is noteworthy, and empirical results suggest potential clinical applicability. However, the current version falls short in evaluation rigor and transparency. The mathematical definitions require refinement to ensure correctness; the absence of statistical testing weakens claims of significance; and insufficient dataset documentation limits reproducibility. Strengthening these areas would substantially enhance the manuscript’s credibility and scientific impact.  \n\n**Decision Recommendation:** **Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI – ✅**  \nThe manuscript presents a new *fine-grained vision–language pre-training framework (fVLM)* for CT image understanding, targeting a fundamental methodological problem in multimodal medical imaging: cross-modal alignment between image regions and radiology report text. The work explicitly tackles the widespread “global alignment” limitation of prior medical CLIP-like models and introduces anatomy-level decomposition, mask-guided visual embedding, and a dual false-negative correction mechanism.  \nThis is clearly a methods paper — not simply an application — and lies solidly within IEEE TMI’s scope of imaging methodology and machine learning for medical imaging.\n\n**2. Novelty & Contribution Level – High**  \n- Introduces explicit anatomy-level contrastive learning within a pretraining framework.  \n- Proposes a *dual false-negative reduction* and *co-teaching* scheme tailored for CT-based contrastive learning.  \n- Curates and analyzes a very large paired CT–report dataset (69 k patients, >270 k scans).  \n- Demonstrates generalization across different datasets and benchmarks.  \nWhile grounded on CLIP-like foundations, the explicit localization-driven, anatomy-conditioned approach and pragmatic correction of false negatives represent genuine methodological innovation rather than an incremental extension.\n\n**3. Technical and Experimental Rigor – Good/Very Good**  \nMethods are mathematically described and ablated (including details of anatomy parsing, report decomposition, and training strategies).  \nExtensive experiments: a large in-house dataset plus two public benchmarks with strong baselines (CLIP, BIUD, Merlin, etc.). Performance metrics (AUC, accuracy, F1, etc.) align with accepted standards.  \nThe weakest aspect is reproducibility: while code is said to be public, segmentation and LLM-based report decomposition are complex, and practical reproducibility could be challenging. Uncertainty quantification and statistical significance testing of AUC gains are also not discussed.\n\n**4. Clarity and Presentation – Good**  \nThe paper is well-structured, with clear motivation, schematic figures, and comparative tables. However, certain subsections (especially Sec. 3.1–3.3) are dense and could benefit from clearer notation and flow. Minor grammatical and typographical errors exist but do not obscure meaning.\n\n**5. Ethical and Reproducibility Compliance – Acceptable**  \nThe paper mentions patient data from multiple institutions and anonymization, though explicit IRB/ethics approval statements are not visible in the excerpt. Code release is noted. The dataset itself is likely private, which limits external reproducibility but is acceptable for TMI provided ethical approvals are properly documented at revision.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n### 1. Summary\nThe paper proposes **fVLM**, a fine-grained vision–language pre-training framework for CT imaging. Unlike traditional CLIP-based medical VLMs that align whole scans with entire radiology reports, fVLM performs explicit **anatomy-level alignment** using segmentation-based visual decomposition and LLM-assisted report parsing. It further introduces a **dual false-negative reduction** and **co-teaching** scheme to mitigate mispaired samples in contrastive learning.  \nEvaluations on an internal large-scale dataset (MedVL‑CT69K) and external benchmarks (CT‑RATE, Rad‑ChestCT) show consistent improvements (~7–13 pp AUC gains) over current state‑of‑the‑art methods. Additional report‑generation and scaling‑law analyses demonstrate effective transfer.\n\n### 2. Strengths\n- Clearly methodological contribution addressing a known bottleneck (global misalignment in medical CLIP).  \n- Well-motivated, technically detailed novel framework extending beyond simple application.  \n- Massive dataset and comprehensive experimental validation on multiple benchmarks.  \n- Quantitative improvements are large and consistent.  \n- Solid ablation and scaling studies supporting design choices.  \n- Code availability enhances transparency.\n\n### 3. Weaknesses\n- Reproducibility of the report‑parsing pipeline depends heavily on proprietary tools (LLM prompts, anatomical grouping), which may hinder independent verification.  \n- Lack of statistical testing across disease‑wise AUC results.  \n- No detailed ethical or IRB statement; necessary for multi‑institutional patient data.  \n- Discussion of clinical interpretability, uncertainty analysis, and potential limitations of segmentation accuracy could be strengthened.  \n- Some dense methodological description and heavy reliance on appendix for critical implementation details.\n\n### 4. Major Comments\n1. **Clarify data governance:** Provide explicit IRB or ethics approval statements and confirm that all CT data and reports were properly de‑identified.  \n2. **Reproducibility:** Supply more procedural detail on anatomy grouping logic and LLM prompts (perhaps as supplementary material or open scripts).  \n3. **Statistical significance:** Report confidence intervals or significance levels for AUC improvements over baselines.  \n4. **Segmentation dependency:** Discuss how segmentation errors or variations in TotalSegmentator performance affect alignment robustness.  \n5. **Clinical interpretability:** Expand on whether anatomy‑wise attention maps correlate with radiologist interpretation and how fVLM could aid explainability or clinical feedback cycles.\n\n### 5. Minor Comments\n- Several typographical inconsistencies (spacing, capitalization, hyphenation) and minor grammatical issues (e.g., “findings and impression sections independently” can be clarified).  \n- Consider summarizing the fine‑grained alignment and dual FNR mechanisms in pseudocode form for clarity.  \n- Figures 2–5 could be enlarged and labeled for better readability in print.  \n- Update references to include corresponding IEEE TMI or related works on medical VLMs where relevant.\n\n### 6. Overall Recommendation\n**Major Revision**  \nThe work is methodologically strong and well within IEEE TMI scope. However, before publication, it requires (1) clearer documentation of ethical approvals and dataset handling, and (2) enhancements to reproducibility and statistical reporting to meet TMI standards.\n\n### 7. Confidence Level\n**5 / 5 (High)** — Assessment based on extensive familiarity with medical imaging machine learning and VLM literature.\n\n---\n\n**Summary Statement:**  \nThis submission presents a substantial methodological advancement in medical vision–language pre‑training via anatomy‑aware contrastive learning. The innovations are significant and likely to influence future multimodal imaging research. With strengthened documentation on ethics, reproducibility, and quantitative rigor, the manuscript would meet the high methodological standards expected by IEEE TMI.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *fVLM*, a fine‑grained vision–language pre‑training framework for CT image understanding, designed to address the challenge of cross‑modal alignment between image regions and radiology report text. By implementing anatomy‑based segmentation, report decomposition via large language models (LLMs), and dual false‑negative correction within a contrastive learning framework, the paper targets a key methodological limitation of prior medical CLIP‑like models. The study is clearly methodological in nature and fits well within IEEE TMI’s scope. Overall, the paper is clearly written, technically substantial, and generally well‑organized, though some sections are dense and could better emphasize implementation transparency and ethical compliance.\n\n---\n\n**Major Comments**  \n1. **Ethics and Data Governance:** Explicitly state institutional review board (IRB) or ethics approvals for all data sources, and confirm anonymization of CT scans and reports used from multiple institutions.  \n2. **Reproducibility of Pipeline:** Provide procedural details for anatomy grouping logic, LLM prompts, and segmentation dependencies. Consider releasing supplementary scripts or pseudocode to facilitate replication.  \n3. **Statistical Rigor:** Include confidence intervals or significance testing for reported AUC improvements to substantiate claims of superiority over baselines.  \n4. **Segmentation Sensitivity:** Analyze robustness to segmentation errors (e.g., TotalSegmentator performance) and discuss their effect on alignment quality.  \n5. **Clinical Interpretability:** Strengthen discussion of how anatomy‑wise attention maps or localization features correspond to radiologist insights and potential for explainable AI applications.\n\n---\n\n**Minor Comments**  \n- Minor grammatical and typographical errors should be corrected for consistency (e.g., capitalization, spacing, hyphenation).  \n- Summarize fine‑grained alignment and dual false‑negative reduction modules in pseudocode for clearer understanding.  \n- Figures could be enlarged and annotated for better readability.  \n- Update references with recent IEEE TMI or comparable medical VLM works.  \n- Clarify several dense sections (notably Secs. 3.1–3.3) to improve narrative flow.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study offers a substantial methodological advancement through anatomy‑level alignment and dual false‑negative correction, addressing a recognized limitation in vision–language modeling for medical imaging. Its innovation is noteworthy and empirically supported by large‑scale experiments demonstrating considerable performance gains across datasets. The evaluation is comprehensive and convincing but would be strengthened by statistical significance testing. Reproducibility remains the principal weakness due to reliance on LLM‑based report parsing and segmentation pipelines, which are complex to replicate. Ethical documentation and interpretability analysis also require elaboration. Overall, the contribution is significant and potentially influential once these issues are addressed.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** — The paper is methodologically strong and well within TMI’s scope, but publication should await improvements in ethical transparency, reproducibility documentation, and statistical validation to satisfy the journal’s rigor standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces a novel fine-grained vision–language pre-training framework (fVLM) aimed at improving cross-modal alignment between CT images and radiology report text, thereby addressing a critical limitation of existing medical CLIP-like models related to global alignment. This methods-focused work presents significant contributions, including anatomy-level decomposition and dual false-negative correction mechanisms, aligning well with IEEE TMI’s focus on imaging methodology and machine learning for medical applications.\n\n**Major Comments:**\n1. **Scope & Relevance to TMI:** The study is a clear methodological contribution, not a mere application, and fits within the journal's domain.\n2. **Novelty & Contribution Level:** Focusing on anatomy-level contrastive learning and the implementation of a dual false-negative reduction strategy shows high novelty, which, alongside a large dataset of 69k patients and comprehensive benchmarking, signifies strong contribution.\n3. **Technical and Experimental Rigor:** While methods are mathematically robust and experiments are extensive, reproducibility remains a concern due to complex segmentation and report decomposition processes. Uncertainty quantification and statistical significance across AUC metrics need to be more thoroughly addressed.\n4. **Clarity and Presentation:** The manuscript is generally well-structured but contains dense sections that could benefit from clearer notation and organization to enhance readability.\n5. **Ethical Compliance:** Anonymization is mentioned, but IRB/ethics approval statements should be included to align with TMI standards. Although code is available, the dataset's private nature may affect reproducibility.\n\n**Minor Comments:**\n- Typographical inconsistencies and minor grammatical issues need addressing for improved clarity.\n- Consider using pseudocode to summarize complex mechanisms like the fine-grained alignment and dual false-negative reduction.\n- Figures should be enlarged and appropriately labeled for better visibility in print.\n- Update references to include relevant works from IEEE TMI or related fields.\n\n**Summary Paragraph:**  \nThis work presents a significant methodological advancement in vision–language pre-training for medical imaging through an anatomy-aware approach. It raises's relevance by addressing known alignment issues, showcasing innovation in methodology, but necessitates improvements in reproducibility and ethical documentation. While the experiments indicate robust findings, further statistical rigor and clarity would enhance its evaluation criteria within TMI.\n\n**Decision Recommendation:**  \n**Major Revision**  \nThe manuscript demonstrates considerable methodological strength and fits the scope of IEEE TMI. It requires clearer ethical documentation, improved reproducibility protocols, and enhanced reports on statistical significance prior to being published.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Le Lu",
      "Lin Yang",
      "Ling Zhang",
      "Qi Zhang",
      "Ruizhe Guo",
      "Sinuo Wang",
      "Tingbo Liang",
      "Weiwei Cao",
      "Xianghua Ye",
      "Zhongyi Shui",
      "Jianpeng Zhang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2d4fcaeb7d709760b3ac4de319e56760ca9ed11b.pdf",
    "remote_url": "https://openreview.net/pdf/2d4fcaeb7d709760b3ac4de319e56760ca9ed11b.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Vision-language model;  zero-shot enhancement; Structural Representation"
    ],
    "abstract": "Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as prompts for the target disease name during the inference phase.  \nHowever, these methods typically treat prompts as equivalent context to the target name, making it difficult to assign specific disease knowledge based on visual information, leading to a coarse alignment between images and target descriptions. In this paper, we propose StructuralGLIP, which introduces an auxiliary branch to encode prompts into a latent knowledge bank layer-by-layer, enabling more context-aware and fine-grained alignment. Specifically, in each layer, we select highly similar features from both the image representation and the knowledge bank, forming structural representations that capture nuanced relationships between image patches and target descriptions. These features are then fused across modalities to further enhance detection performance.\nExtensive experiments demonstrate that StructuralGLIP achieves a +4.1\\% AP improvement over prior state-of-the-art methods across seven zero-shot medical detection benchmarks, and consistently improves fine-tuned models by +3.2\\% AP on endoscopy image datasets.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a zero-shot object detection model, StructrualGLIP, for medical image-based object detection tasks. The method uses a knowledge bank that stores encoded text prompts, which are later used to select and match relevant image features to form fine-grained structural representations to allow better alignment of image features with the prompt information, achieving accurate, context-aware detection.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. the paper demonstrates a comprehensive evaluation of four medical image modalities.\n2. the paper's representation and structure are mostly clear and easy to follow (although with some language and word choice issues, which will be discussed below).\n\n### Weaknesses\n\n1. The claimed novelty appears to be on the latent knowledge bank and its function to store encoded prompt tokens and later be used at each encoder layer as a vision token selector and vice versa. This is coined as a mutual selection process. The selected information is then merged into the original image and text encoder back at each layer. The entire process is different to a standard contextual prompt method (Fig. 1(a)) but it feels like a quite incremental difference which I don't see much novelty.\n2. The word choice of \"medical detection\" bugs me, in medical science detection refers to \"detecting diseases\" whereas here this is object detection in medical images so this may cause confusion to certain readers.\n3. Line 234 looks unfinished.\n4. Eq. 6&7, perhaps the Top-P/Q^{max} function can be simplified by using argmax/argsort function?\n5. L201/534: \"... fine-grained alignment between target descriptions and medical images\", L:256 \"forming fine-grained structural representations\", can the authors clarify what \"fine-grained\" refers to in those places?\n6. L294: \"...like BLIP Li et al. (2022a)\", is the VQA model BLIP or not? Have you considered other VQA models and would the performance of other VQA models fluctuate your detection results?\n7. L177: I don't entirely agree that zero-shot detection has a real-world clinical need as the clinicians I've encountered would not trust zero-shot settings, in the medical domain, accurate detection/segmentation/diagnosis is the the most important thing.\n\n### Questions\n\nPlease address Weaknesses #1,5,6.\n\n\n------------------\n04/12/2024:\nI thank the authors' detailed responses. Since the reviewer's post deadline has passed so I am writing my final comments here. \n\nMy conclusion is that I will retain my original score. I believe the paper should improve clarity as many new arguments and technical details surfaced during the discussion period which should have been included in the main manuscript. The novelty and settings should be explored in more depth.\n\nMy comments are:\n\n1. Novelty. \n\nI agree with the authors' argument the image and language encoders do not need to be fine-tuned in your zero-shot detection setting. This I already mentioned in my last comment: \"I can understand if the auxiliary branch's language encoder is untrained\". My question was whether the modules in Eq. 8 to 10 need training or not? Especially the MHA module in Eq. 8, GLIP has the X-MHA module, did the author reuse the X-MHA weights for the MHA without retraining? The authors mentioned in L263 \"we employ a multi-head attention (MHA) mechanism\" which reads like it is a new module by the authors' own design. Given an MHA module (as far as I know) should have learnable weights but the authors claim the zero-shot detection setting is training-free, then I find the logic here is contradicted. Furthermore, the authors quoted L210 to demonstrate the training-free paradigm, but that is immediately after Eq.4 and the sentence only describes the encoders. This is an imprecise response to my question so I'm not making a judgment here, but this does make me think the paper should improve clarity. \n\nThe above was not the main point of my original novelty concern, but a spin-off. My original point was that if the authors wanted to demonstrate the \"preventing/addressing domain shifting\" argument for the novelty, you should consider measuring the actual feature distribution difference between GLIP and StructureGLIP.  The authors revealed in the last comments that the domain referred to the features before RPN, and that is what I think you should demonstrate. To recap the authors' comments, the authors claimed the other methods would concatenate prompt so the feature to RPN becomes CLS token of [target_name, prompt] which causes the domain shift, whereas StructualGLIP did the prompts integration in early layers so StructuralGLIP only has CLS of [target_name] to the RPN, and that addresses the domain shift. Conceptually, maybe it is true but please provide empirical evidence. The final performance is a surrogate measure, which does not directly support your domain shift argument. Finally, StructrualGLIP also fuses additional information in the process, so the CLS token should exhibit some domain shift as you are trying to do domain adaptation, these two arguments also have contradictions to each other. \n\nRegarding the \"addressing noise prompt\" argument, I'd imagine the top-P/Q has a certain capability of limiting noise prompts as you only choose the top tokens, so even when you add more prompts, they won't be selected if the network was already trained to have higher attention for the target classes tokens, but that on the other hand, make it trivial argument rather than a novelty.\n\n7. Zero-shot enhancement. After reading the authors' most recent comments, I strongly feel the setting should not be called zero-shot. When the target dataset's labelled data are already used to fine-tune your GLIP base model, then this is weakly supervised. In L175-176, the authors state \"We propose a zero-shot enhancement setting. This involves fine-tuning the model on medical datasets first, and then using prompts to further improve performance on unseen medical images\", initially I considered this as the model was trained on relevant datasets such as same/similar imaging modalities but different tasks or different datasets. However, if the target dataset itself was used for finetuning for training, then the \"further improve performance on unseen medical images\" does not hold.\n\nOther comments:\nThe experiments I requested were for the understanding of how much performance gain was attributable to the design/novelty of StructrualGLIP, i.e., the introduction of the auxiliary branch. The zero-shot enhancement setting (i.e., fine-tuned GLIP) has boosted the performance by a larger margin than the StructualGLIP can bring to GLIP. Also, the choice of the VQA can sometimes affect the AP/AP50 by a noticeable margin (i.e., that 75 AP) but it appears not stable. Then, at least for me, the viability of zero-shot medical detection unfortunately remains questionable.\n\nOnce again I appreciate the authors' effort to address my comments but I'm sorry I will retain my original score based on my thoughts above. The authors are encouraged to delve into the details of the novelty and concepts as well as improve clarity of the manuscript.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents StructrualGLIP, a zero-shot object detection model aimed at enhancing medical image-based object detection tasks through a knowledge bank of encoded text prompts. The authors claim that this approach allows for better alignment of image features with prompts, facilitating context-aware detection. Overall, the clarity and quality of the presentation are mixed, with clear structural organization but notable language issues.\n\n### Major Comments\n1. **Novelty**: The method's novelty relies on the use of a latent knowledge bank for prompt token selection. However, this incremental difference lacks substantial innovation over standard contextual prompt methods, which diminishes its perceived uniqueness.\n2. **Terminology Clarity**: The term \"medical detection\" is misleading as it may imply detecting diseases rather than objects in medical images, potentially causing confusion among readers.\n3. **Technical Clarity**: Line 234 appears incomplete, requiring revision for coherence.\n4. **Equation Simplification**: Equations 6 and 7 might benefit from simplification by utilizing argmax/argsort functions.\n5. **Clarification Needed**: The use of \"fine-grained\" in lines 201/534 and 256 needs clarification regarding what it specifically refers to in context.\n6. **VQA Model Context**: The mention of BLIP as a VQA model raises questions regarding its inclusion. Have the authors considered performance fluctuations with other VQA models?\n7. **Clinical Applicability**: The assertion that zero-shot detection meets real-world clinical needs is questionable; clinicians may not trust zero-shot settings, given that accuracy is paramount in medical diagnosis.\n\n### Minor Comments\n- The manuscript contains several language choices that require revision for clarity.\n- The notation of certain terms and acronyms could use standardization to enhance understanding.\n- There are minor typographical errors that should be corrected for professionalism.\n\n### Summary Paragraph\nIn evaluating the manuscript against the TMI criteria, its significance is diminished by concerns around novelty, which appears incremental rather than groundbreaking. The evaluation design is complex but lacks empirical evidence supporting claims of addressing domain shift. Reproducibility is questionable due to potential misunderstandings around zero-shot settings, and the clarity of the manuscript needs improvement to effectively communicate its contributions.\n\n### Decision Recommendation\nMajor revision is necessary to address clarity issues, refine the novelty claims, and provide more comprehensive evaluations and empirical support related to the methods proposed.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a zero-shot object detection model tailored for medical image analysis. The approach introduces a “knowledge bank” storing encoded text prompts that interact with image features to generate fine-grained structural representations for improved image–prompt alignment. The manuscript is generally clear and well organized, with comprehensive experiments across four medical imaging modalities. Nevertheless, several aspects of novelty, clarity, and the definition of the claimed “zero-shot” setting require further refinement.\n\n---\n\n**Major Comments**  \n1. **Novelty and Contribution** – The proposed latent knowledge bank and mutual selection mechanism appear conceptually similar to existing contextual prompt methods, rendering the contribution incremental rather than fundamentally novel. The claimed innovations (e.g., domain-shift mitigation and noise-prompt suppression) are not convincingly supported by empirical evidence. The reviewer encourages quantitative analysis of feature distribution differences between GLIP and StructuralGLIP to substantiate the domain shift claim.  \n2. **Training-Free Logic** – Clarification is needed regarding whether modules in Eqs. 8–10, particularly the Multi-Head Attention (MHA), possess learnable parameters or reuse pretrained weights. This point appears inconsistent with the claim of a training-free zero-shot paradigm.  \n3. **Definition of Zero-Shot Enhancement** – The so-called “zero-shot enhancement” involves fine-tuning on the target dataset, which contradicts the zero-shot premise and aligns more closely with a weakly supervised setting. This requires clearer articulation.  \n4. **Evaluation and Comparisons** – The experiments show performance gains, yet the magnitude of improvement due to the new architecture versus fine-tuning remains unclear. Variability introduced by the choice of VQA model also affects stability, questioning reliability.  \n5. **Conceptual Clarity** – Several statements use vague terminology, such as “fine-grained alignment.” The meaning of such terms should be explicitly defined. Additionally, some lines (e.g., L234) appear incomplete or unclear.  \n\n---\n\n**Minor Comments**  \n- The phrase “medical detection” could be confusing; “object detection in medical images” would be clearer.  \n- Typographical and stylistic issues appear in line references and equations (e.g., simplification of Top‑P/Q^{max} using argmax/argsort).  \n- Figures and equation references—especially Fig. 1(a)—should be checked for consistency.  \n- Clarify whether the model compared to BLIP refers specifically to the VQA model version used.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the problem of medical zero-shot detection is relevant, but the proposed novelty remains modest. Regarding **innovation**, the introduction of a knowledge bank and mutual selection process is potentially interesting yet not convincingly distinct from prior methods. The **evaluation** is broad across modalities but leaves unresolved questions about the attribution of performance gains and stability of comparisons. For **reproducibility**, the paper is mostly transparent yet requires clearer explanation of training-free claims and fine-tuning protocols to ensure the results can be independently verified.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial clarifications on novelty, experimental evidence, and consistency of the zero-shot definition are needed before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduce a new zero-shot method for medical Vision-Language Models(VLMs) for detecting unknown targets. To address the prompt umatched with the variations in the medcial images, the authors propose a StructualGLIP desgin with a main and an auxiliary branch encoders for text input and introduce a mutual selcetion mechanism. \nThe author explain that the auxiliary branch would work as a knowledge bank where the main branch can extract latent prompt tokens, while the tokens in the knowledge banked are filtered by the mutual selection process.\nOverall, the motivation, method, and performance of this work is good enough, but I still need some explaination for some detail, please refer to the weakness and question section. I will consider adjust my rating based on the authors response.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. This work is aiming for a vital issue in medical image understanding field, which is the generalization capability of foundation models with limit data access. The motivation of improving the existing work is clear and strong, which is the lack of object level prompt and fail to capture the various feature of images during prompt desgin.\n2. This work present a novel but efficient method, called StructualGLIP, to increase the model's zero-shot/few-shot performance on various datasets. The desgin of StructualGLIP introduce the knowledge bank and mutual selection process to help prompt design process. This method address several shortcomings of current method and is novela and effective.\n3. This method largely increased the zero-shot performance on different medical image datasets across different modalities.\n\n### Weaknesses\n\n1. Line 234, the sentence seems not finished. \n2. One of the major problem of the proposed method is not trainable as the Top-K selection operation is non-differentiable, while previous work is differentiable and thus finetuning would result in better performance. I would suggest include Reparameterization Trick for Gumbel-Softmax to improve your method. Though this work is good enough as a stand-alone method for zero-shot detection. But I see a potential to achieve better performance.\n\n### Questions\n\n1. How to evaluate the quality of the generated prompts by VQA method. As some previous work pointed out, the VLM without medical domain adaptation perform poorly on some medical datasets, especially for radiology datasets.\n2. For Prompt as Knowledge Bank ablation study seciton, I would like to see an experiment on the whether adding noisy knowledge (for example, knowledge for another target) would sharply downgrade the StructureGLIP performance. This experiment would the the robustness of the mutual-selection process.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nNA",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a new zero-shot method for medical Vision-Language Models (VLMs), aimed at detecting unknown targets in medical imagery. The authors introduce a StructuralGLIP design featuring main and auxiliary branch encoders tailored for text input, as well as a mutual selection mechanism. While the motivation and approach are commendable, further clarification on specific details is needed for complete understanding.\n\n**Major Comments:**\n1. The study addresses a significant challenge in the medical image understanding field: the generalization capabilities of foundational models with limited data access. The motivation for improving upon existing methods is clear, particularly regarding the lack of object-level prompts and the failure to capture diverse image features during prompt design.\n2. The proposed StructuralGLIP method is both novel and efficient, enhancing zero-shot and few-shot performance across various datasets through the incorporation of a knowledge bank and mutual selection process, addressing several limitations of current approaches.\n3. The method demonstrably improves zero-shot performance across different medical image datasets and modalities.\n\n**Weaknesses:**\n1. Line 234 contains an incomplete sentence.\n2. A significant drawback of the proposed method is its non-trainability, as the Top-K selection operation is non-differentiable. Previous differentiable approaches may yield better performance through fine-tuning. Including a Reparameterization Trick for Gumbel-Softmax could potentially enhance the method; however, it is adequate as a stand-alone solution for zero-shot detection.\n\n**Questions:**\n1. How is the quality of the generated prompts evaluated using the VQA method, given that prior studies indicate VLMs without specific medical domain adaptation can underperform on medical datasets, especially in radiology?\n2. In the ablation study for the Prompt as Knowledge Bank, an experiment assessing the impact of integrating noisy knowledge (e.g., knowledge for differing targets) on StructuralGLIP performance is recommended to evaluate the robustness of the mutual-selection process.\n\n**Summary Paragraph:**\nThe manuscript addresses relevance, innovation, and evaluation by proposing a new approach to enhance medical VLMs under zero-shot conditions, showing promise in advancing the field. However, concerns regarding non-trainability and prompt evaluation methods need to be addressed. With improvements in these areas, the study would significantly contribute to the reproducibility and practical application of VLMs in medical imaging.\n\n**Decision Recommendation:**\nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a zero-shot framework, *StructuralGLIP*, aimed at improving medical vision-language models (VLMs) for detecting previously unseen targets. The approach employs a dual-branch encoder structure—main and auxiliary branches—for text prompts, incorporating a mutual selection mechanism. The auxiliary branch functions as a knowledge bank from which the main branch extracts latent prompt tokens filtered via this mutual selection. The study addresses the challenge of mismatched textual prompts and image variations in medical data. Overall, the paper presents a clearly motivated and reasonably explained contribution, though several clarifications are required to strengthen its presentation and technical soundness.  \n\n**Major Comments**  \n1. **Trainability Limitation:** The proposed method’s Top-K selection operation is non-differentiable, which prevents fine-tuning. In contrast, comparable methods are trainable and tend to achieve better performance with supervised adaptation. The reviewer suggests investigating a reparameterization strategy such as the Gumbel-Softmax trick to mitigate this limitation.  \n2. **Evaluation of Prompt Quality:** It remains unclear how the authors assess the quality of the generated prompts. Given prior evidence that general-purpose VLMs perform suboptimally on certain medical datasets, a clearer evaluation strategy—particularly for radiology—is needed.  \n3. **Ablation Robustness Test:** In the “Prompt as Knowledge Bank” ablation, an additional experiment could explore robustness by introducing noisy or unrelated knowledge into the bank to assess how sensitively performance degrades.  \n4. **Minor Incomplete Content:** The sentence at Line 234 appears unfinished and should be revised for completeness.  \n\n**Minor Comments**  \n- Several typos and grammatical errors appear (e.g., “StructualGLIP,” “selcetion,” “medcial”); these should be corrected.  \n- The authors may wish to improve explanation clarity for the mutual selection mechanism and the auxiliary branch functioning.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript addresses an important issue—the generalization of medical VLMs with limited data—and introduces an innovative architectural concept combining structural prompt design and knowledge retrieval. It demonstrates consistent performance improvements across multiple datasets, indicating potential significance. However, the lack of trainability and limited evaluation details slightly weaken the methodological rigor and reproducibility. With clarification of technical details and further robustness evaluations, the work could form a solid contribution to the field.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper is promising and reasonably innovative, but improvements are needed in methodological clarity, evaluation completeness, and wording accuracy before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors introduce StructuralGLIP, a novel approach to zero-shot medical detection using vision-language models (VLMs). This method leverages structured representations within a dual-branch architecture that enables nuanced alignment between images and textual prompts, significantly enhancing the model's adaptability to new medical scenarios without needing annotated data. StructuralGLIP uses category-level prompts, maintained in a latent knowledge bank, and a mutual selection mechanism for precise cross-modal fusion, thus improving accuracy across diverse medical imaging datasets.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n(1)\tThe paper introduces an effective structural representation by encoding prompts into a knowledge bank and utilizing a dual-branch structure. This approach enables adaptive and context-aware alignment, which is particularly advantageous for complex medical detection tasks.\n(2)\tStructuralGLIP outperforms traditional zero-shot models by effectively handling both instance-level and category-level prompts, achieving significant improvements across various benchmarks in endoscopy, microscopy, radiology, and more.\n(3)\tBy allowing for zero-shot enhancement, the model can be fine-tuned and then further improved with category-level prompts, a feature well-suited for dynamic medical settings where data annotation is scarce.\n\n### Weaknesses\n\n(1)\tThe proposed dual-branch structure with a knowledge bank requires complex engineering and computational resources, potentially limiting its accessibility for practitioners in less resource-rich environments.\n(2)\tThe paper may not adequately address the potential data imbalance present in the datasets used for evaluation. Some diseases or conditions may have significantly fewer examples, which could impact the model's performance and generalizability.\n(3)\tThe model's inner workings, particularly regarding how it selects and utilizes prompts, may be difficult for practitioners to interpret, limiting trust in its decisions and making it harder to diagnose potential failures.\n(4)\tDespite improvements in alignment, there may still be instances of misalignment between visual features and prompts, especially in cases of atypical presentations, which could lead to missed detections.\n\n### Questions\n\n(1)\tTo what extent can the findings be generalized to other medical imaging modalities or less common diseases? Are there plans to evaluate the model on broader datasets?\n(2)\tBesides Average Precision, what other metrics were considered for evaluating model performance? Are there plans to incorporate user feedback or clinical outcomes in future evaluations?\n(3)\tThis paper focuses on zero-shot medical detection, whereas GLIP was initially developed for natural images. Can the proposed method also be applied effectively to natural image datasets?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents StructuralGLIP, an innovative methodology for zero-shot medical detection utilizing vision-language models (VLMs). The authors propose a dual-branch architecture that effectively aligns images with textual prompts, enhancing adaptability to new medical scenarios without the need for annotated data. The clarity and overall quality of the presentation could benefit from improvements, as noted in the evaluation.\n\n### Major Comments\n1. The introduction of a structural representation through a latent knowledge bank and dual-branch architecture demonstrates adaptability and context-awareness, beneficial for complex medical tasks.\n2. StructuralGLIP yields superior performance compared to traditional zero-shot models, adeptly managing both instance-level and category-level prompts, with notable advancements in various medical imaging benchmarks including endoscopy, microscopy, and radiology.\n3. The model's ability to incorporate zero-shot enhancement is advantageous for rapidly evolving medical environments with limited annotated data.\n4. The dual-branch structure and knowledge bank methodology may impose significant engineering demands and computational resources, potentially limiting accessibility in resource-constrained healthcare settings.\n5. The evaluation may not comprehensively address the data imbalance inherent in the utilized datasets, potentially affecting the model's performance and generalizability for less represented diseases.\n6. The inner workings of the model, particularly prompt selection and usage, lack clarity, risking trust and understandability for practitioners and complicating the diagnosis of failures.\n7. Despite advancements in alignment, misalignment issues might still arise, particularly in atypical cases, potentially resulting in missed detections.\n\n### Minor Comments\n- Clarification on the methodology and rationale for performance metrics beyond Average Precision would enhance the paper's robustness.\n- The writing could be refined for clearer communication, specifically in complex sections that may confuse readers.\n\n### Summary Paragraph\nThis study offers a significant contribution to zero-shot medical detection through innovation in methodology and evaluation. While the proposed system shows strength in adaptability and effectiveness, it raises concerns regarding accessibility, data imbalance, and interpretability. These weaknesses impact the overall reproducibility and practical applicability, suggesting a need for further refinement and evaluation across diverse datasets and medical conditions.\n\n### Decision Recommendation\nMinor revision is recommended to address the identified weaknesses and enhance clarity, ultimately increasing the paper's impact and applicability in clinical settings.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a new vision–language model designed for zero-shot medical detection. It introduces a dual-branch architecture incorporating structured prompts through a knowledge bank and mutual selection mechanism to enhance cross-modal alignment between medical images and textual descriptions. The approach aims to improve adaptability to unseen medical scenarios without requiring additional annotation. The paper is generally clear and technically sound, though parts of the explanation could be more accessible.\n\n---\n\n**Major Comments**  \n1. **Complexity and Resource Requirements:** The dual-branch design and maintenance of a structured knowledge bank appear computationally demanding, which may hinder adoption in resource-limited environments. The authors should comment on efficiency optimization or model scalability.  \n2. **Dataset Imbalance and Generalizability:** The evaluation datasets may contain class imbalance across disease types, potentially influencing model generalization. Further clarification or analysis regarding dataset composition and its impact on performance would strengthen the work.  \n3. **Interpretability of the Prompt Mechanism:** The internal logic governing prompt selection and utilization is insufficiently explained. Greater transparency or visualization of this process would improve user trust and facilitate error diagnosis.  \n4. **Residual Misalignment:** Despite improvements in alignment, occasional mismatches between textual prompts and visual features may persist, particularly for atypical or visually subtle cases. Discussion of mitigation strategies is encouraged.  \n5. **Scope of Evaluation:** It is unclear whether the approach can extend effectively to other modalities or even non-medical images. Evaluation on broader datasets would help establish general applicability.  \n6. **Evaluation Metrics:** Only average precision metrics appear discussed; consideration of complementary metrics (e.g., sensitivity, specificity) or downstream clinical implications could provide a more comprehensive performance assessment.\n\n---\n\n**Minor Comments**  \n- Some sections could benefit from refined figures or examples to clarify the dual-branch mechanism.  \n- Ensure consistent use of acronyms and technical terms throughout the paper.  \n- A few textual explanations of prompt fusion are overly condensed and would benefit from clearer stepwise presentation.\n\n---\n\n**Summary Paragraph (TMI Editorial Criteria)**  \nIn terms of **significance**, the method addresses an important problem—reducing annotation needs in medical detection. Regarding **innovation**, the structural prompt mechanism represents a meaningful extension beyond conventional vision–language pairing. However, the paper’s **evaluation** could be broadened to account for dataset balance and additional metrics, and the **reproducibility** of the model would be improved by clearer disclosure of computational resources, parameter tuning, and internal prompt logic. Overall, the contribution is promising and well-motivated but requires additional clarification and validation.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents StructuralGLIP, a zero-shot medical detection framework that addresses limitations in existing vision-language models for medical imaging. The core problem is that current methods suffer from coarse alignment between images and target descriptions when using prompts. The proposed solution employs a dual-branch architecture where the main branch processes target names and images, while an auxiliary branch encodes prompts into a latent knowledge bank. A mutual selection mechanism dynamically matches relevant visual and linguistic features across layers. The method supports both instance-level and category-level prompts, with category-level prompts remaining fixed across instances of the same class. Experiments across eight medical datasets (endoscopy, microscopy, photography, radiology) demonstrate superior performance in both zero-shot detection and zero-shot enhancement settings, achieving average improvements of +4.2% AP and +4% AP respectively over baseline methods.\n\n## Weaknesses\n\n• **Mathematical formulation clarity and consistency issues**\n  - Equation (6) uses inconsistent notation where the summation over j appears outside the Top-Pmax function but the operation inside involves all j values, making the selection mechanism unclear (Page 5, Equation 6)\n  - The dot product operation in Equations (6) and (7) between individual tokens and entire matrices (oi_j and Bi_q) lacks dimensional consistency explanation (Page 5, Equations 6-7)\n  - The relationship between Ki_v selected in Equation (6) and its subsequent use in Equation (7) creates circular dependency that is not mathematically well-defined (Page 5, Equations 6-7)\n\n• **Insufficient experimental rigor and baseline comparisons**\n  - Limited comparison with recent medical detection methods beyond GLIP-based approaches, missing evaluation against domain-specific medical detection frameworks (Table 1, Page 7)\n  - Inconsistent experimental settings across datasets with varying train/test splits that make cross-dataset performance comparisons unreliable (Section 4.1, Page 7; Appendix C, Pages 16-17)\n  - Ablation studies focus primarily on hyperparameters P and Q but lack comprehensive analysis of key architectural components like the dual-branch design versus single-branch alternatives (Table 5, Page 9; Figure 4, Page 10)\n\n• **Limited technical novelty in core components**\n  - The mutual selection mechanism essentially implements standard attention-based token selection without significant algorithmic innovation beyond existing prompt-based methods (Section 3.2, Pages 4-6)\n  - The dual-branch architecture follows conventional auxiliary branch designs used in continual learning literature, as acknowledged by the authors (Section 2, Page 3)\n  - Category-level prompt generation relies on standard VQA models (BLIP) and LLMs (GPT-4) without novel prompt engineering techniques (Section 3.3, Page 6; Figure 3b)\n\n• **Evaluation limitations and result interpretation concerns**\n  - Zero-shot enhancement experiments show inconsistent improvements across methods, with some baseline methods experiencing performance degradation that may indicate experimental setup issues rather than method superiority (Table 4, Page 9)\n  - Missing statistical significance testing and confidence intervals across the extensive experimental results, making it difficult to assess the reliability of reported improvements (Tables 1, 4, Pages 7, 9)\n  - Category-level prompt evaluation lacks comparison of computational efficiency gains versus instance-level prompts despite efficiency being a claimed advantage (Tables 2, 3, Pages 8)\n\n## Suggestions for Improvement\n\n• **Clarify and correct mathematical formulations**\n  - Rewrite Equations (6) and (7) with explicit dimensional analysis and clear definition of the selection operation, specifying whether selection occurs before or after similarity computation\n  - Provide a more rigorous mathematical definition of the mutual selection mechanism, potentially using formal notation for top-k selection operations\n  - Add a mathematical proof or detailed explanation showing how the circular dependency between Ki_v and Ki_l is resolved in practice\n\n• **Strengthen experimental validation and comparisons**\n  - Include comparisons with recent domain-specific medical detection methods beyond GLIP variants, such as specialized medical object detection frameworks\n  - Standardize experimental protocols across datasets with consistent evaluation metrics and provide detailed statistical analysis including confidence intervals and significance tests\n  - Conduct comprehensive ablation studies comparing dual-branch versus single-branch architectures and analyzing the contribution of each major component\n\n• **Enhance technical contributions and innovation**\n  - Develop novel algorithmic improvements to the mutual selection mechanism beyond standard attention-based approaches, potentially incorporating medical domain knowledge\n  - Propose innovative architectural designs that go beyond conventional auxiliary branch methods, drawing insights specific to medical image analysis\n  - Create more sophisticated prompt engineering techniques tailored to medical terminology and disease characteristics rather than relying solely on existing VQA/LLM tools\n\n• **Improve evaluation methodology and result presentation**\n  - Investigate and explain the performance degradation of baseline methods in zero-shot enhancement experiments, ensuring fair experimental conditions\n  - Provide comprehensive statistical analysis including error bars, confidence intervals, and significance tests for all reported performance improvements\n  - Conduct detailed efficiency analysis comparing computational costs and inference times between category-level and instance-level prompt approaches with quantitative measurements",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces StructuralGLIP, a zero-shot medical detection framework designed to overcome limitations observed in current vision-language models for medical imaging. The main issue addressed is the coarse alignment found between images and target descriptions when using prompts. The proposed model incorporates a dual-branch architecture that processes target names and images in the main branch while encoding prompts in a latent knowledge bank through an auxiliary branch. The framework demonstrates robust performance across diverse medical datasets, achieving notable improvements in both zero-shot detection and enhancement settings.\n\n**Major Comments:**  \n1. **Clarity and Consistency of Mathematical Formulation:**  \n   - Equation (6) presents inconsistent notation, resulting in unclear operations regarding the Top-Pmax function (Page 5, Equation 6).  \n   - The dot product operations in Equations (6) and (7) lack documented dimensional consistency (Page 5, Equations 6-7).  \n   - There exists a circular dependency concerning Ki_v in Equation (6) and its role in Equation (7), which is not well-defined (Page 5, Equations 6-7).  \n\n2. **Experimental Design and Baseline Comparisons:**  \n   - The comparison primarily rests on GLIP-related methods without adequate evaluation against recent, domain-specific medical detection frameworks (Table 1, Page 7).  \n   - Variability in train/test splits among datasets raises concerns about the reliability of cross-dataset comparisons (Section 4.1, Page 7; Appendix C, Pages 16-17).  \n   - Ablation studies inadequately explore key architectural components, focusing mainly on hyperparameters rather than design comparisons (Table 5, Page 9; Figure 4, Page 10).  \n\n3. **Technical Novelty:**  \n   - The mutual selection mechanism appears to replicate standard attention-based token selection with negligible algorithmic innovation (Section 3.2, Pages 4-6).  \n   - The dual-branch architecture aligns with established auxiliary designs, as recognized by the authors (Section 2, Page 3).  \n   - The prompt generation leverages standard models (BLIP, GPT-4) without original engineering techniques (Section 3.3, Page 6; Figure 3b).  \n\n4. **Evaluation and Result Interpretation:**  \n   - Inconsistent performance improvements in zero-shot enhancement may indicate flaws in the experimental setup rather than demonstrating method superiority (Table 4, Page 9).  \n   - A lack of statistical significance testing hampers the assessment of the reported improvements (Tables 1, 4, Pages 7, 9).  \n   - Category-level evaluation fails to compare computational efficiencies with instance-level prompts despite claiming advantages (Tables 2, 3, Pages 8).  \n\n**Minor Comments:**  \n- Clear mathematical definitions and proof of circular dependencies in Equations (6) and (7) should be provided.  \n- Further include comparisons with specialized medical detection frameworks and standardize evaluation protocols across datasets.  \n- Investigate the efficiency of computational costs between category-level and instance-level approaches.  \n\n**Summary Paragraph:**  \nThe manuscript makes a significant contribution to medical imaging by presenting a novel framework that offers improvements in zero-shot detection. However, there are substantial weaknesses regarding the clarity of math formulations, experimental comparisons, and technical novelties. While the significance and potential of the proposed method are commendable, the evaluation methodology and detail in results are lacking, which raises concerns about reproducibility and reliability.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a zero‑shot medical object detection framework designed to overcome the coarse image–text alignment seen in existing vision–language models. The proposed dual‑branch architecture processes target names and images while maintaining a latent knowledge bank of prompts, leveraging a mutual selection mechanism to align visual and linguistic features. The method supports both instance‑level and category‑level prompts and is evaluated on eight medical imaging datasets spanning endoscopy, microscopy, photography, and radiology. Reported results show consistent gains in zero‑shot detection and enhancement tasks over baseline models. Overall, the paper is ambitious and well‑motivated but suffers from mathematical ambiguities, limited experimental rigor, and questions regarding novelty and evaluation reliability.\n\n---\n\n**Major Comments**  \n1. **Clarity and Consistency of Mathematical Formulation**  \n   - Equations (6)–(7) present dimensional and definitional inconsistencies. The summation term in Equation (6) and the interaction between tokens and matrices (oi_j and Bi_q) are unclear, as is the dependency between Ki_v and Ki_l.  \n   - The selection mechanism’s operation order (before or after similarity computation) is poorly defined, generating uncertainty about the mutual selection process.  \n\n2. **Experimental Design and Comparisons**  \n   - Comparisons focus primarily on GLIP‑based baselines, omitting recent domain‑specific medical detection frameworks.  \n   - Dataset splits differ across experiments, undermining cross‑dataset consistency.  \n   - Ablation analyses address only hyperparameters (P, Q) without evaluating architectural variants such as single‑branch alternatives.\n\n3. **Technical Novelty**  \n   - The mutual selection mechanism appears analogous to established attention‑based selection.  \n   - The dual‑branch and auxiliary components draw heavily from existing architectures, and prompt generation uses standard BLIP and GPT‑4 models without distinctive engineering innovations.\n\n4. **Evaluation and Interpretation**  \n   - Zero‑shot enhancement results are inconsistent, and some baselines degrade notably, suggesting potential experiment‑setup artifacts.  \n   - Statistical significance and confidence analysis are absent.  \n   - Claimed computational advantages of category‑level prompts are not quantitatively validated.\n\n---\n\n**Minor Comments**  \n- Clarify formula notation and ensure algebraic consistency in Section 3.2.  \n- Standardize evaluation settings and metrics across datasets for comparability.  \n- Include clearer figure captions for Tables 1–5 and Figures 3–4 describing experimental setups.\n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses a relevant gap in zero‑shot medical detection but exhibits limited methodological innovation beyond existing vision–language alignment strategies. While the experimental results are promising, inconsistencies and missing statistical analyses weaken claims of significance. The paper’s conceptual framework is potentially impactful if mathematical definitions are clarified and experiments standardized, which would also enhance reproducibility.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \nThe paper introduces *StructuralGLIP*, a vision‑language framework for **zero‑shot medical object detection**. The focus is on methodological innovation—particularly, structural prompt encoding via a dual‑branch architecture and “mutual selection” for cross‑modal alignment—and not on clinical application per se. This is indeed imaging methodology (multimodal representation learning, vision‑language modeling, reconstruction of spatial–semantic correspondences).  \n✅ Clearly within TMI’s methodological scope.\n\n**2. Novelty & Contribution Level**  \nCompared with prior GLIP‑based detection and prompt‑pool methods, the study proposes a new *structural prompt* formulation that converts text prompts into a latent “knowledge bank” and performs layer‑wise dynamic matching with visual tokens. This extends continual‑learning prompt pools to detection and applies it to medical images. The addition of *zero‑shot enhancement* (post‑fine‑tuning improvements without new labels) is also novel.  Overall the concept is meaningfully new and generalizable, not a mere data re‑run.  \n\n**3. Technical and Experimental Rigor**  \nFormulation and ablations are carefully described; equations are consistent. Multiple datasets (endoscopy, microscopy, radiology, dermatology) are used. However:\n- Training and inference parameters are described mainly at a high level; reproducibility relies on appendices.  \n- Ground‑truth annotation sources are cited but details on license/IRB are not emphasized.  \n- Statistical testing or uncertainty intervals are absent—improvements given as means only.  \nOtherwise, comparisons to strong baselines are comprehensive.\n\n**4. Clarity & Presentation**  \nThe manuscript is well written for a conference paper but would require reformatting to TMI standards. Figures are dense yet legible. Terminology sometimes mixes “target” and “prompt;” more consistent notation would aid readers. English usage is largely clear.  \n\n**5. Ethical / Reproducibility Compliance**  \nDatasets used are public benchmark sets; patient protection therefore acceptable, although explicit IRB language or statement of anonymization should appear. The authors claim code release, which aligns with reproducibility norms.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n### 1. Summary\nThe authors propose **StructuralGLIP**, a dual‑branch vision‑language network for zero‑shot medical detection. Traditional grounded language–image pre‑training (GLIP) methods treat prompts as concatenated text, yielding coarse alignment between descriptions and image features. StructuralGLIP encodes prompts into a *latent knowledge bank* within an auxiliary branch. Through a *mutual‑selection mechanism* at every layer, the model retrieves the most relevant semantic tokens and fuses them with vision features by cross‑modal attention, refining structural representations. It further supports *category‑level prompts*—general attribute sets generated by large language models—and introduces a *zero‑shot enhancement* scenario, where a fine‑tuned detector can still improve via prompts without new labels. Experiments on eight datasets (endoscopy, microscopy, radiology, dermatology) show consistent AP gains over GLIP, AutoPrompter, MIU‑VL, and MPT baselines.\n\n### 2. Strengths\n- Methodologically new prompt‑selection and latent‑bank design with clear architectural justification.  \n- Broad experimental coverage across imaging modalities and tasks.  \n- Demonstrates both zero‑shot and post‑fine‑tuning (enhancement) benefits.  \n- Well‑executed ablations clarifying the effects of bank size, fusion layer, and prompt generation.  \n- Code release planned, supporting reproducibility.\n\n### 3. Weaknesses\n- Statistical significance and uncertainty of reported AP improvements not provided.  \n- Dataset use and ethical statements minimal—should confirm IRB or license status explicitly.  \n- Heavy dependence on large language models (BLIP, GPT‑4) may hinder reproducibility if prompts change.  \n- Complexity and computation cost of dual‑branch fusion not quantified.  \n- Figures occasionally self‑referential (to supplementary material) and should be reconstructed for journal readability.\n\n### 4. Major Comments\n1. **Quantitative rigor.**  Provide standard deviations or confidence intervals across runs; include significance testing versus baselines.  \n2. **Computational analysis.**  Report training/inference cost, parameter count, and speed relative to GLIP to justify practicality.  \n3. **Prompt‑generation transparency.**  Make the automatically generated prompts and LLM queries fully available; consider auditing their medical accuracy.  \n4. **Clarify data governance.**  Explain dataset licensing and ethical approvals to meet TMI reproducibility policies.  \n5. **Broader applicability.**  Discuss how the approach generalizes beyond GLIP—could the structural‑prompt mechanism attach to other backbones (e.g., SAM, MedSAM)?  \n6. **Theory and intuition.**  A concise mathematical rationale describing why mutual‑selection stabilizes cross‑modal alignment would strengthen the methodological contribution.\n\n### 5. Minor Comments\n- Unify acronym usage (“AP@50” vs. “AP50”).  \n- Improve figure captions; many rely on color difference not visible in grayscale.  \n- Ensure consistent spelling (e.g., “mutual‑selection,” “fine‑tuned”).  \n- Verify all references conform to IEEE citation style.  \n- Expand conclusion to explicitly state clinical translation potential.  \n\n### 6. Overall Recommendation\n**Major Revision.**  \nThe paper offers substantive and original methodology likely suitable for TMI but needs strengthened reproducibility, statistical assurance, and ethical transparency before journal publication.\n\n### 7. Confidence Level\n**4 / 5.**  The reviewer is confident in assessing vision‑language medical imaging methodology.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *StructuralGLIP*, a dual‑branch vision–language framework for zero‑shot medical object detection. The approach focuses on structural prompt encoding and a layer‑wise “mutual‑selection” mechanism to improve cross‑modal alignment. Prompts are transformed into a latent knowledge bank, allowing semantic retrieval and fusion with visual tokens. Experiments on eight diverse medical imaging datasets demonstrate consistent performance gains over existing GLIP‑based and prompt‑pool baselines. The paper is clearly written, methodologically oriented, and within the scope of *IEEE TMI*, though its presentation requires adaptation to journal format.\n\n---\n\n**Major Comments**  \n1. **Quantitative rigor:** Provide statistical significance or uncertainty measures for AP improvements; current results are reported as means only.  \n2. **Computational complexity:** Quantify resource usage—parameter count, training/inference time, and computational cost relative to GLIP—to evaluate practicality.  \n3. **Prompt transparency and reproducibility:** Release the generated prompts, LLM queries, and configuration details to ensure replicability and allow assessment of medical accuracy.  \n4. **Ethical and data governance clarity:** Explicitly state dataset licensing status, consent/IRB information, and anonymization to align with *TMI* reproducibility policy.  \n5. **Theoretical explanation:** Include a concise formal analysis explaining why the mutual‑selection mechanism enhances cross‑modal alignment stability.  \n6. **Generality of approach:** Discuss potential extension of the structural‑prompt mechanism to other model backbones (e.g., SAM, MedSAM).  \n\n---\n\n**Minor Comments**  \n- Harmonize terminology and acronyms (e.g., “AP@50” vs. “AP50”).  \n- Improve figure captions and avoid reliance on color for interpretation; reformat for journal readability.  \n- Maintain consistent spelling and notation (“mutual‑selection,” “fine‑tuned”).  \n- Ensure references conform to IEEE citation format.  \n- Expand conclusion to mention prospective clinical translation.  \n\n---\n\n**Summary (against TMI criteria)**  \n*Significance:* The study tackles an important methodological problem in zero‑shot medical detection with broad modality coverage.  \n*Innovation:* The structural prompt and latent knowledge bank are methodologically novel and substantively extend prior GLIP frameworks.  \n*Evaluation:* Empirical study is wide‑ranging and includes informative ablations, but lacks statistical validation and computational analysis.  \n*Reproducibility:* Code release is planned, yet reproducibility and ethical compliance require clearer documentation of datasets, prompt generation, and governance.\n\n---\n\n**Decision Recommendation:** **Major Revision** – The paper presents an original and promising methodology appropriate for *TMI*, but requires stronger statistical evaluation, reproducibility evidence, and ethical transparency before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript presents **StructuralGLIP**, a vision-language framework aimed at advancing zero-shot medical object detection. The primary focus lies in methodological innovation, notably through structural prompt encoding with a dual-branch architecture and mutual selection for cross-modal alignment. The work is relevant within the context of imaging methodology, contributing to multimodal representation learning and vision-language modeling.\n\n### Major Comments\n1. **Quantitative Rigor:** Include standard deviations or confidence intervals across runs and significance testing relative to established baselines.\n2. **Computational Analysis:** Report training and inference costs, parameter counts, and processing speeds compared to GLIP to validate the model's practicality.\n3. **Prompt-Generation Transparency:** Make automatically generated prompts and LLM queries available, and consider auditing them for medical accuracy.\n4. **Clarify Data Governance:** Provide explicit details on dataset licensing and ethical approvals to align with TMI’s reproducibility policies.\n5. **Broader Applicability:** Discuss the potential for generalization beyond GLIP; could the structural-prompt mechanism be applied to other models like SAM or MedSAM?\n6. **Theory and Intuition:** A concise mathematical explanation for the effectiveness of mutual selection in stabilizing cross-modal alignment would enhance the methodological contribution.\n\n### Minor Comments\n- Ensure consistent use of acronyms (e.g., “AP@50” vs. “AP50”).\n- Enhance figure captions, as many rely on color differences that may not be visible in grayscale.\n- Check for consistent spelling (e.g., “mutual-selection,” “fine-tuned”).\n- Verify all references adhere to IEEE citation standards.\n- Expand the conclusion to explicitly articulate the potential for clinical translation.\n\n### Summary Paragraph\nIn summary, the study offers an innovative approach to zero-shot medical object detection through **StructuralGLIP**. While the methodology shows promise in terms of significance and innovation, certain aspects related to evaluation and reproducibility require attention. The paper's strengths lie in its novel architectural design and comprehensive experimental testing across various datasets. However, the absence of statistical significance testing and clarity on ethical considerations diminishes its robustness. \n\n### Decision Recommendation\n**Major Revision.**  \nThe manuscript showcases substantial and original methodologies with potential suitability for TMI, yet it necessitates improvements in terms of reproducibility, statistical rigor, and ethical transparency prior to publication. \n\n### Confidence Level\n**4 / 5.** The reviewer expresses confidence in the assessment of the vision-language medical imaging methodology presented in this work.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Baochang Zhang",
      "Chunyu Xie",
      "Haoyu Huang",
      "Linlin Yang",
      "Tongfei Chen",
      "Xianbin Cao",
      "Yuguang Yang",
      "Dawei Leng"
    ],
    "url": "pdfs/iclr.cc-2025-conference_42ae230c934ff880d86ebd78533b2cdcd90fd1b8.pdf",
    "remote_url": "https://openreview.net/pdf/42ae230c934ff880d86ebd78533b2cdcd90fd1b8.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Multimodal Learning",
      "Medical Imaging"
    ],
    "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, medical imaging domains introduce two key challenges: dynamic modality fusion and modality-task dependence. The quality and amount of task-related information from different modalities could vary significantly across patient samples, due to biological and demographic factors. Traditional fusion methods apply fixed combination strategies that fail to capture this dynamic relationship, potentially underutilizing modalities that carry stronger diagnostic signals for specific patients. Additionally, different clinical tasks may require dynamic feature selection and combination from various modalities, a phenomenon we term “modality-task dependence.” To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions. We evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate our method's superiority over state-of-the-art methods under different metrics of classification and segmentation tasks like Accuracy, AUROC, AUPRC, and DICE.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a multi-modal, multi-task, mixture of experts for various medical diagnoses to address the challenges of sample-dynamic modality fusion (and modality-task dependence (selecting the right modalities for a task). Concretely, this is done by using a combination of modality-specific experts and experts shared between modalities and tasks. M4OE shows promising initial results in terms of both absolute performance and enforcing modality utilization.\n\nBased on the weaknesses and questions outlined my score indicates a rejection for now, but I generally like the motivation of the paper, especially the aspect on modality utilization. I am willing to increase my score if my concerns are addressed and questions clarified.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The M4OE is highly effective at enforcing modality utilization - this is a meaningful contribution that many multimodal models suffer from, although I do have some questions about this.\n- The overall performance of the model is outperforming the baseline, even if the results are missing crucial information to validate the statistical significance of the results.\n- Strong visuals that are additive to the understanding of the paper.\n- Good conceptual motivation of the paper, although I believe that the motivation would further benefit from some concrete examples of sample dynamism and clinical examples of tasks that are modality-dependent.\n\n### Weaknesses\n\n- Abstract: the one-liner for sample-dynamic modality fusion is unclear as the specific and shared information always varies per sample unless they are identical. To my knowledge, sample-dynamic spans a much wider field of problems like missingness, robustness to noise, which the manuscript does not consider.\n- Abstract: “Results demonstrate superiority over state-of-the-art methods” is extremely vague. Along which metric?\n- You claim an expansive space by saying that the method is “multi-modal multi-task”, but your experiments only look at multi-view settings of a single modality (images). I would encourage you to narrow the scope/claim of the paper as the paper does not consider heterogeneous modalities (images, text, tabular, etc.).\n- Experimental setup: I would encourage you to provide more detail in this section to aid reproducibility. For example, it is unclear whether cross-validation is used. No confidence intervals or standard deviation of results are reported to judge the statistical significance of the results. Additionally, no code was provided in the supplementary materials that would help with the clarification of the experimental setup.\n- Literature: missing out on the largest corpus of literature (intermediate fusion), which many latent variable models for multimodal fusion fall under, many of which are using a mix of modality-specific and shared spaces.\n\n### Questions\n\n- How do you determine which expert sees which task? The connection between Figure 1 and the method section is not very clear.\n- The manuscript talks a lot about sample adaptivity, but how does your experimental setup show that the model handles sample adaptivity effectively? Which aspects of sample adaptivity?\n- Figure 3c suggests that the modality utilization is forced towards the same mean in your method. What about cases where modality dominance/competition is good? For example, if I have one very noisy modality, wouldn’t it be desirable to have the modality that contains all the signal to get all the model’s attention? Isn’t this graph showing that we enforce equal utilisation of all modalities regardless of the signal? Additionally, does this finding not contradict your claim in Figure 1, which is that only some experts are used (as opposed to all experts with a more balanced contribution).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a multi-modal, multi-task mixture of experts (M4OE) framework aimed at improving medical diagnoses by addressing challenges associated with sample-dynamic modality fusion and task-dependent modality selection. The approach combines modality-specific experts with shared experts across modalities and tasks, demonstrating promising initial results in terms of performance and modality utilization. While I appreciate the paper's motivation, I currently recommend rejection based on identified weaknesses and concerns, but I am open to reconsidering if these issues are addressed satisfactorily.\n\n**Major Comments:**\n1. The abstract's description of sample-dynamic modality fusion is vague, as the nature of shared information varies with each sample. It overlooks broader issues such as missingness and robustness to noise.\n2. Assertion of superiority over state-of-the-art methods is unclear; specific metrics need to be provided for clarity.\n3. The claim of being \"multi-modal multi-task\" is misleading as experiments focus solely on multi-view settings of a single modality (images). Consider narrowing this claim to reflect the actual scope.\n4. The experimental setup lacks detail necessary for reproducibility. Information on whether cross-validation was used is missing, and statistical significance metrics (confidence intervals, standard deviations) are not reported. Additionally, no supplementary code has been provided.\n5. The literature review overlooks key works in intermediate fusion, particularly those involving latent variable models that integrate modality-specific and shared spaces.\n\n**Minor Comments:**\n- Provide concrete examples of sample dynamism and modality-dependent clinical tasks to strengthen the motivation.\n- Clarify the connection between Figure 1 and the expert task assignment in the methodology.\n- Address issues regarding how the model effectively captures sample adaptivity in the experimental setup.\n- Figure 3c raises concerns that modality utilization may lead to equal weighting of modalities; this should be examined further, particularly regarding cases of modality dominance or competition.\n\n**Summary Paragraph:**\nThe manuscript exhibits significance in its focus on modality utilization and initial performance improvements. However, it falls short in terms of innovation by not fully addressing the multi-modal aspects as claimed. The evaluation lacks thoroughness, significantly impacting reproducibility due to insufficient experimental details. Overall, while the model demonstrates potential, critical issues remain that hinder its contribution to the field.\n\n**Decision Recommendation:**\nReject.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *M4OE*, a multi-modal, multi-task mixture-of-experts model designed for medical diagnosis applications. The approach aims to address the problem of sample-dynamic modality fusion and task-modality dependence through a combination of modality-specific and shared experts. The paper presents promising initial performance results and meaningful contributions in enforcing modality utilization. The overall motivation and presentation are clear, though several conceptual, methodological, and experimental aspects require clarification and refinement before the work can be accepted.\n\n---\n\n**Major Comments**  \n1. **Scope and Claims:** The manuscript asserts a “multi-modal, multi-task” framework, yet the experiments are limited to multi-view imaging data. This narrow empirical scope does not support claims about handling heterogeneous modalities (e.g., images, text, tabular data). The claims should be narrowed or additional experiments added.  \n2. **Conceptual Clarity:** The definition of “sample-dynamic modality fusion” in the abstract is ambiguous. The description does not align with the broader meaning of sample dynamics, which can include missing modalities and robustness to noise.  \n3. **Experimental Design and Reproducibility:** The evaluation lacks sufficient methodological details. It is unclear whether cross-validation is used, and there are no confidence intervals or standard deviations to assess statistical significance. The absence of code or supplementary materials further limits reproducibility.  \n4. **Evidence and Metrics:** The statement “results demonstrate superiority over state-of-the-art” is vague and should specify the metrics of comparison.  \n5. **Literature Coverage:** The work omits discussion of relevant literature in intermediate fusion, particularly latent-variable models combining modality-specific and shared spaces.  \n6. **Methodological Clarifications:** The relationship between Figure 1 and the description of expert-task assignments is unclear. Additional explanation is needed about how experts are matched to tasks and how sample adaptivity is demonstrated experimentally.  \n7. **Analysis of Modality Utilization:** Figure 3c suggests equalized modality contributions, which may not always be desirable if certain modalities are noisy. The authors should clarify how the method handles modality dominance and whether this behavior contradicts the claim of selective expert usage.\n\n---\n\n**Minor Comments**  \n- The abstract could be refined for clarity and precision.  \n- The manuscript would benefit from more concrete clinical examples illustrating sample dynamism and task-specific modality dependence.  \n- Figures are visually strong and aid understanding but require clearer correspondence with the text.  \n- Typographical and stylistic polishing is recommended.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n- **Significance:** The topic of dynamic multimodal fusion is important and highly relevant to medical AI.  \n- **Innovation:** The proposed mixture-of-experts formulation is conceptually interesting, particularly regarding modality utilization.  \n- **Evaluation:** Experimental validation is incomplete, lacking statistical rigor and sufficient evidence to support the claimed superiority.  \n- **Reproducibility:** Missing methodological details and absent code limit reproducibility. With clarified methods and expanded experiments, the paper’s contribution could become significantly stronger.\n\n---\n\n**Decision Recommendation:** **Reject (encouraged to resubmit after major revision).**  \nThe work presents a promising idea but requires substantial clarification, expanded experiments, and refinement of claims to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this paper, the author present a new framework for training multi-modal networks, called the Multi-modal Multi-task Mixture of Experts. The framework consists of two components:\n- MSoE: Modality specific mixture of experts --> for each modality, they learn a function g that applies:\n column-wise softmax (D) on X times a learnable matrix, multiplied by X, followed by row-wise softmax (C) on the output and a linear combination to compute the prediction.\n- MToE: Modality shared modality task mixture of experts --> connects tasks to input modalities by learning a task embedding shared across experts.\n\nThey also propose a mutual information loss and evaluate the approach on four publicly available medical imaging datasets for breast cancer and OCT.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nThe framework presented is original and interesting. It outperforms existing baseline models. The authors run experiments on multiple datasets and conduct an ablation study.\n\n### Weaknesses\n\n- The paper presentation requires improvement. For example, there is unnecessary use of ; and there is incorrect use of opening quotations \". The authors also repeatedly introduce the abbreviations - this should be done once.\n- I found it difficult to parse through Figure 2 (Can you relate it with the textual explanation of the functions?)\n- The authors only compare to a few baselines, can you incorporate more? There is a lot of literature on multimodal learning now.\n- Are the performance improvements significant? Can you conduct significance testing and provide confidence intervals?\n- The experiments are conducted on medical imaging datasets. How does this apply to other non-imaging modalities where modality competition may be more pronounced. For example, this could be applicable to MIMIC CXR (chest X-rays) and MIMIC EHR where downstream tasks are more dependent on the EHR modality.\n- The main results section in the text should also discuss the quantitative results.\n- What was your hyperparameter tuning strategy? It is unclear if these baselines have been best optimized.\n- Can you also compute AUROC and AUPRC for the classification tasks? Accuracy is not sufficient.\n\n### Questions\n\n- Can the authors discuss the scalability of the framework? What is the computational complexity?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript introduces a novel framework for training multi-modal networks, termed the Multi-modal Multi-task Mixture of Experts (M^3oE). This framework is composed of two main components: the Modality-Specific Mixture of Experts (MSoE) and the Modality-Shared Task Mixture of Experts (MToE). The authors aim to improve learning functions across various modalities, particularly in the context of medical imaging for breast cancer and OCT, supported by evaluation on four publicly available datasets. However, while the conceptual contributions are promising, the presentation and depth of analysis require enhancements for clarity and rigor.\n\n**Major Comments:**\n1. The overall presentation of the paper needs improvements; for instance, there is inconsistent punctuation, specifically the use of semicolons and quotation marks.\n2. Figure 2 is confusing; it would benefit from a clearer relationship to the textual explanation of the functions.\n3. The authors have compared their method to only a limited number of baselines; incorporating a wider range of comparisons from existing literature on multimodal learning is necessary.\n4. The significance of the performance improvements is not adequately tested; conducting significance testing and providing confidence intervals would strengthen the findings.\n5. The applicability of the proposed framework to non-imaging modalities requires discussion, especially considering datasets like MIMIC CXR (chest X-rays) and MIMIC EHR.\n6. The results section should include a discussion of quantitative results alongside qualitative ones.\n7. The hyperparameter tuning strategy is not explicitly stated; clarification on optimization of the baselines is needed.\n8. Additional metrics such as AUROC and AUPRC should be computed for classification tasks, as accuracy alone is insufficient.\n\n**Minor Comments:**\n- The introduction of abbreviations should occur only once to avoid redundancy.\n- There are minor typographical errors to correct throughout the manuscript.\n\n**Summary Paragraph:**\nIn relation to the TMI editorial criteria, the manuscript presents a significant advancement in multi-modal network training (significance), demonstrating originality and outperforming existing baselines (innovation). However, the limited comparison to existing methodologies and lack of robust statistical analysis detract from its overall evaluation quality. The framework's potential relevance beyond medical imaging is also insufficiently addressed, impacting its reproducibility. Overall, while there are noteworthy contributions, several critical issues require attention to meet the necessary standards for publication.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a new framework for training multimodal networks, termed the *Multi‑modal Multi‑task Mixture of Experts* (M3oE). The model comprises two main components: (1) modality‑specific mixtures of experts (MSoE) that learn functions per modality through a softmax‑based transformation and linear combination; and (2) modality‑shared mixtures of experts for tasks (MToE) that link task embeddings across modalities. The authors also introduce a mutual information loss and evaluate their method on four publicly available medical imaging datasets related to breast cancer and OCT. Overall, the concept is original and promising, though the clarity and presentation of the manuscript need improvement.\n\n**Major Comments**  \n1. **Presentation and Clarity** – The paper’s readability is hindered by grammatical issues and inconsistent abbreviation usage. Unnecessary punctuation and inconsistent notation should be corrected. Figure 2 is difficult to interpret and should be better aligned with the corresponding textual explanation.  \n2. **Comparative Evaluation** – Only a few baseline methods are included. Given the extensive literature on multimodal learning, additional comparisons are needed to better contextualize performance gains.  \n3. **Statistical Significance** – The manuscript reports improvements but does not assess their significance. Statistical significance testing and confidence intervals should be added.  \n4. **Generalization Beyond Imaging** – All experiments focus on medical imaging datasets. Discussion of applicability to non‑imaging modalities (e.g., MIMIC‑CXR and EHR data) would strengthen the paper.  \n5. **Experimental and Reporting Details** – The main results section should explicitly discuss quantitative outcomes. The hyperparameter tuning strategy is unclear, and it is uncertain if baselines were fairly optimized.  \n6. **Evaluation Metrics** – For classification tasks, the inclusion of AUROC and AUPRC would provide a more comprehensive performance assessment.  \n7. **Scalability** – Clarify the computational complexity and scalability of the proposed framework.\n\n**Minor Comments**  \n- Correct inconsistent punctuation and misuse of quotation marks.  \n- Introduce each abbreviation only once.  \n- Improve figure captions for clarity.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is innovative in proposing a multimodal, multitask mixture‑of‑experts framework combining shared and modality‑specific structures with a mutual information loss. Its significance is supported by demonstrated empirical improvements; however, the evaluation lacks statistical rigor, broader comparisons, and discussion on generalizability. Reproducibility would benefit from clearer experimental description and explicit tuning details. Presentation issues and limited scope currently reduce clarity and impact.\n\n**Decision Recommendation**  \n**Major Revision.** The technical contribution is original and potentially significant, but additional analyses, improved clarity, and stronger experimental validation are needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper mainly addresses two challenges in clinical tasks: patient-level and task-level dynamic fusion. For the patient-level fusion, a modality-specific MoE is employed. For the task-level fusion, a modality-task MoE with conditional MI regularization between experts and modalities given tasks is adopted. The experiments using EMBED, RSNA, VinDR, and GAMMA datasets outperform existing methods in both single-task and multi-task settings.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The paper is well-structured and easy to follow. \n- The motivation is clearly stated and convincing. \n- The experiments show promising results over many baselines both in stand-alone and add-on manners.  \n- The paper adopted PID to make a fair comparison of synergy information.\n\n### Weaknesses\n\n- As far as I understand, there have been works leveraging the shared and specific information across modalities and should be included in discussions, see [1-3]. \n- Is there an ablation study for a reduced number of experts? How sensitive is this method when the number of experts decreases compared to other MoE methods? What is the procedure for choosing the number of experts? \n- Please discuss the computational cost compared to the baselines. \n\n\n[1]Wang, Hu, et al. \"Multi-modal learning with missing modality via shared-specific feature modelling.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2]Yao, Wenfang, et al. \"DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.\n\n[3]Chen, Cheng, et al. \"Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion.\" Medical Image Computing and Computer Assisted Intervention–MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part III 22. Springer International Publishing, 2019.\n\n### Questions\n\n- Are there any ablation studies on datasets other than EMBED?\n- Are there any theoretical explanations on why the method mitigates gradient conflict?\n- Is there any clinical interpretation of the results in Figure 5? For example, the difference in modality contribution across different tasks.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**:  \nThe manuscript addresses two significant challenges in clinical tasks: patient-level and task-level dynamic fusion. It employs a modality-specific mixture of experts (MoE) for patient-level fusion and a modality-task MoE with conditional mutual information regularization for task-level fusion. The experiments conducted on datasets such as EMBED, RSNA, VinDR, and GAMMA demonstrate improvements over existing methods in both single-task and multi-task settings.\n\n**Major Comments**:  \n1. The manuscript acknowledges the employment of a modality-specific MoE; however, it should reference existing literature that explores shared and specific information across modalities (see references [1-3]).  \n2. There is no ablation study demonstrated for a reduced number of experts. The authors should clarify the method's sensitivity to changes in the number of experts compared to other MoE approaches. Additionally, the criteria used to select the number of experts should be discussed.  \n3. A discussion regarding the computational cost of the proposed method in comparison to the baseline methods is warranted.\n\n**Minor Comments**:  \n- The paper is well-structured and clear, contributing to reader understanding.  \n- The motivation for the study is articulated and convincing.  \n- The experimental results significantly surpass various baselines, which are evaluated in both standalone and synergistic manners.  \n- The inclusion of PID enables a fair comparison of synergy information.  \n\n**Summary Paragraph**:  \nThe manuscript is significant for its innovation in addressing clinical task challenges using dynamic fusion techniques. It shows potential in evaluation through various datasets and achieves reproducibility in its findings. However, it requires improvements in theoretical discussions, particularly concerning existing literature, ablation studies, and computational costs. These weaknesses may impact the manuscript's overall contributions.\n\n**Decision Recommendation**:  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses two central challenges in clinical multi‑task learning: patient‑level and task‑level dynamic fusion. The authors propose a modality‑specific Mixture of Experts (MoE) for patient‑level fusion and a modality‑task MoE with conditional mutual information regularization to promote disentanglement between experts and modalities. Experiments conducted on four datasets (EMBED, RSNA, VinDR, and GAMMA) demonstrate superior performance to existing baselines under both single‑task and multi‑task settings. Overall, the paper is clearly structured, well motivated, and presents its results in an accessible manner.  \n\n**Major Comments**  \n1. **Related Work Coverage:** The discussion should include prior works that explore shared and modality‑specific feature modeling across modalities, such as Wang et al. (2023), Yao et al. (2024), and Chen et al. (2019). These are directly relevant and would clarify the novelty and positioning of the proposed method.  \n2. **Ablation on Number of Experts:** The review requests an ablation study examining how performance changes with fewer experts. The selection criteria for the number of experts should be explained, and the sensitivity of the model to this parameter should be analyzed relative to other MoE‑based methods.  \n3. **Computational Efficiency:** A quantitative comparison of computational cost versus baselines would strengthen claims of practicality and scalability.  \n4. **Additional Analyses:** The reviewer asks whether ablation studies were conducted on datasets beyond EMBED, and seeks theoretical reasoning for how the method mitigates gradient conflict.  \n5. **Clinical Interpretation:** The reviewer suggests expanding discussion of Figure 5 to include potential clinical interpretation—specifically, how modality contributions vary across different tasks.  \n\n**Minor Comments**  \n- None noted beyond requests for expanded discussion and clarification.  \n- No ethical concerns identified.  \n\n**Summary Paragraph (TMI Criteria)**  \nFrom the standpoint of significance, the paper tackles clinically relevant challenges in multimodal fusion with promising quantitative results. Innovation lies in the dual‑level MoE design and the use of conditional MI regularization. The evaluation is extensive across multiple datasets but could be strengthened by additional ablations, cost analysis, and theoretical justification. Reproducibility appears reasonable given the reported methodology, though further detail on hyperparameter choices (e.g., number of experts) would assist reproducibility.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision — The paper shows strong potential and competitive performance but requires deeper comparative discussion, additional analysis on expert sensitivity and computational cost, and clearer theoretical and interpretive explanations.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThe authors propose M4oE, a Multi-modal Multi-task Mixture of Experts framework for medical imaging that addresses two key challenges: dynamic modality fusion and modality-task dependence. The method consists of Modality-Specific MoE (MSoE) modules that retain modality-specific features and a Modality-shared Modality-Task MoE (MToE) module that models joint probability distributions of modalities, experts, and tasks. The framework uses conditional mutual information loss to encourage experts to learn diverse patterns of task-specific modality-shared information. The authors evaluate M4oE on four public datasets (EMBED, RSNA, VinDr, GAMMA) covering breast cancer screening and retinal disease diagnosis, demonstrating superior performance over state-of-the-art methods across classification and segmentation tasks. The framework provides both sample-level and population-level interpretability of modality contributions and shows effectiveness in mitigating gradient conflicts and modality competition issues.\n\n## Weaknesses\n\n• **Mathematical formulation clarity and consistency issues**\n  - The conditional probability P(Mi,Ej|Tk) definition in Section 3.3 appears inconsistent with its normalization, as the denominator sums over all modalities, experts, and tasks rather than normalizing properly for the specific task Tk\n  - The mutual information objective I(M;E|T) derivation lacks clear connection to how it addresses the stated hypothesis of dynamic modality-task dependence\n  - Notation inconsistencies appear between the preliminary section describing soft MoE with matrices D and C versus the MToE implementation with different dimensionalities and task slots\n\n• **Experimental validation limitations**\n  - Limited baseline comparisons within the medical AI domain, with only 2 methods per application area (mammography: MIRAI, Asymmirai; ophthalmology: Eyemost, Eyestar) as shown in Table 1\n  - Cross-validation methodology unclear, as Section 4.1 mentions \"5-fold cross validation\" but provides insufficient detail about data splitting and statistical significance testing\n  - Missing computational complexity analysis and training time comparisons, despite claims of improved efficiency through dynamic expert selection\n\n• **Interpretability claims insufficiently validated**\n  - The synergy analysis in Figure 4 using partial information decomposition lacks statistical significance testing and confidence intervals for the claimed modality-task dependence patterns\n  - Sample-level interpretability examples in Figure 5 show only two patient cases, insufficient to demonstrate generalizability of the interpretation approach\n  - Population-level modality contribution analysis lacks quantitative metrics to validate the clinical sensibility claims made in Section 4.3\n\n• **Technical soundness concerns in core components**\n  - The MToE module's task slot mechanism lacks theoretical justification for why this specific architecture would encourage modality-task dependence learning\n  - Gradient conflict mitigation analysis in Figure 6 shows correlation but not causation between expert sparsity and reduced conflicts\n  - The conditional mutual information loss weight α=0.05 appears arbitrarily chosen without systematic hyperparameter optimization across datasets\n\n## Suggestions for Improvement\n\n• **Clarify and validate mathematical formulations**\n  - Provide rigorous mathematical derivation showing how the conditional probability definitions properly normalize and connect to the mutual information objective\n  - Include formal theoretical analysis demonstrating why maximizing I(M;E|T) encourages the desired modality-task dependence behavior\n  - Standardize notation throughout the paper and provide clear dimensional analysis for all matrix operations in the MToE architecture\n\n• **Strengthen experimental validation methodology**\n  - Include comprehensive comparisons with additional medical AI baselines and provide statistical significance testing (t-tests, confidence intervals) for all performance claims\n  - Provide detailed cross-validation methodology including data splitting strategies, evaluation protocols, and computational resource requirements\n  - Add ablation studies on computational complexity, inference time, and scalability to larger expert numbers and modality combinations\n\n• **Provide rigorous interpretability validation**\n  - Conduct statistical analysis of synergy patterns across larger patient cohorts with confidence intervals and significance testing for modality-task dependence claims\n  - Include quantitative evaluation of sample-level interpretability against expert annotations or established clinical knowledge\n  - Develop systematic metrics for evaluating population-level modality contribution patterns and validate against clinical literature or expert assessment\n\n• **Enhance technical component validation**\n  - Provide theoretical justification or empirical evidence for why the task slot architecture specifically improves modality-task dependence learning compared to alternative designs\n  - Include controlled experiments isolating the causal relationship between expert sparsity and gradient conflict reduction\n  - Conduct systematic hyperparameter optimization for the mutual information loss weight α across all datasets and provide sensitivity analysis",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe authors of this manuscript present M4oE, a Multi-modal Multi-task Mixture of Experts framework aimed at improving medical imaging applications by addressing dynamic modality fusion and modality-task dependence. The framework incorporates Modality-Specific MoE (MSoE) modules for retaining modality-specific features and a Modality-Task MoE (MToE) module for modeling joint distributions of modalities, tasks, and experts. The evaluation of M4oE on four public datasets—EMBED, RSNA, VinDr, and GAMMA—demonstrates its advanced performance in classification and segmentation tasks, along with interpretability capabilities for modality contributions.\n\n**Major Comments:**  \n1. **Mathematical formulation clarity and consistency issues:**\n   - The definition of conditional probability P(Mi,Ej|Tk) in Section 3.3 is inconsistent regarding its normalization. \n   - The derivation of the mutual information objective I(M;E|T) lacks clarity in its relevance to the dynamic modality-task dependence hypothesis.\n   - Notation inconsistencies between the preliminary descriptions of soft MoE and the MToE implementation hinder comprehension.\n\n2. **Experimental validation limitations:**\n   - The manuscript presents limited baseline comparisons in the medical AI field, utilizing only two comparative methods for each application area.\n   - Details regarding cross-validation methodology are insufficient, particularly in data splitting and statistical significance testing.\n   - There is a lack of analysis on computational complexity and training time despite claims of improved efficiency.\n\n3. **Interpretability claims insufficiently validated:**\n   - The analysis in Figure 4 lacks statistical significance testing and confidence intervals for claims regarding modality-task dependence.\n   - Two patient cases in Figure 5 do not provide sufficient generalizability for sample-level interpretability.\n   - The analysis in Section 4.3 does not include quantitative metrics to support claims made regarding population-level contributions.\n\n4. **Technical soundness concerns:**\n   - The task slot mechanism within the MToE module lacks theoretical justification for its role in modality-task dependence.\n   - Correlation shown in Figure 6 does not establish causation between expert sparsity and conflict reduction.\n   - The choice of the conditional mutual information loss weight α=0.05 appears arbitrary without systematic optimization.\n\n**Minor Comments:**  \n- Standardize notation throughout the manuscript.  \n- Include additional details on evaluation protocols and computational resource requirements.  \n- Consider providing sensitivity analysis for hyperparameters in the MToE architecture.  \n\n**Summary Paragraph:**  \nIn summary, the manuscript presents significant contributions to medical imaging through the M4oE framework. However, concerns regarding the clarity of mathematical formulations, the robustness of experimental validation, and the validation of interpretability claims must be addressed to enhance its impact. While the innovative aspects of the framework are commendable, further work is necessary to ensure thorough evaluation and reproducibility in clinical contexts.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **M4oE (Multi‑modal Multi‑task Mixture of Experts)**, a framework designed to address dynamic modality fusion and modality–task dependence in medical imaging. The approach combines modality‑specific MoE (MSoE) components with a modality‑shared Modality‑Task MoE (MToE) module, employing conditional mutual information loss to encourage diverse, task‑specific shared representations. Experiments on four public breast and retinal imaging datasets (EMBED, RSNA, VinDr, GAMMA) demonstrate superior performance over selected state‑of‑the‑art baselines across classification and segmentation tasks. The paper also discusses interpretability, gradient conflict mitigation, and computational efficiency. Overall, the proposed idea is promising, but several aspects require clarification and deeper validation.  \n\n---\n\n**Major Comments**  \n1. **Mathematical formulation clarity** – The definition of conditional probability \\( P(M_i,E_j|T_k) \\) in Section 3.3 appears inconsistent with its normalization, summing over all modalities and tasks rather than within a fixed \\(T_k\\). The derivation of the mutual‑information objective \\(I(M;E|T)\\) is insufficiently connected to the stated hypothesis on modality–task dependence. Notation and dimensionality differ between preliminary soft‑MoE formulations and the MToE implementation.  \n2. **Experimental validation** – Baseline coverage is narrow, with at most two methods per application area. The 5‑fold cross‑validation procedure is under‑described, lacking detail on data partitioning and significance testing. Claims of improved efficiency are not supported by runtime or complexity analyses.  \n3. **Interpretability evidence** – The synergy analysis (Fig. 4) lacks statistical testing; sample‑level interpretability (Fig. 5) relies on two cases only; and population‑level modality contributions are not quantitatively validated against clinical knowledge.  \n4. **Technical soundness** – The rationale for the “task slot” mechanism in MToE is not theoretically justified. The connection between expert sparsity and gradient conflict reduction is correlational rather than causal. The mutual‑information loss weight (\\(\\alpha=0.05\\)) appears ad hoc and unoptimized.\n\n---\n\n**Minor Comments**  \n- Ensure consistent notation and dimensional labeling throughout the mathematical sections.  \n- Clarify figure captions, particularly for Figs. 4–6, to improve interpretability.  \n- Explain all acronyms upon first use and correct small formatting inconsistencies in tables.\n\n---\n\n**Summary (TMI Editorial Criteria)**  \n**Significance:** The topic of multi‑modal, multi‑task learning in medical imaging is important, and the proposed architecture could have broad relevance.  \n**Innovation:** The combination of modality‑specific and modality‑task experts guided by mutual‑information objectives is potentially novel, but theoretical justification is incomplete.  \n**Evaluation:** Empirical evaluation spans multiple datasets but lacks comprehensive baselines, statistical testing, and computational analysis, limiting confidence in performance claims.  \n**Reproducibility:** Implementation details, cross‑validation procedures, and hyperparameter settings require fuller disclosure for reproducibility.\n\n---\n\n**Decision Recommendation:** **Major Revision** — The paper shows promise but needs substantial clarification of mathematical formulation, strengthened experimental design, and more rigorous validation of interpretability and efficiency claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## Phase 1 — Analytical and Scope Audit\n\n**1. Scope & Relevance to TMI**  \n✅ The manuscript focuses on a new *multi‑modal, multi‑task learning framework*—“M4oE”—that integrates Mixture‑of‑Experts modeling into medical imaging workflows. The contribution centers on methodological innovation in multimodal fusion, dynamic modality‑task dependence, and interpretable probabilistic modeling rather than merely applying existing models. The work fits squarely within IEEE *Transactions on Medical Imaging* (TMI)’s methodological scope.\n\n**2. Novelty & Contribution Level**  \nThe authors propose:  \n- a **Modality‑Specific MoE (MSoE)** and **Modality‑Shared Modality‑Task MoE (MToE)**,  \n- a novel *conditional mutual‑information regularization* term to encourage dynamic coupling between tasks and modalities, and  \n- a probabilistic interpretation connecting experts, modalities, and tasks.  \n\nRelative to existing fusion or MoE‑based architectures, the joint modeling of modality–expert–task dependencies is conceptually meaningful and appears new. The work is a clear extension of MoE principles with coherent methodological innovations explicitly aimed at medical imaging.\n\n**3. Technical and Experimental Rigor**  \nExperiments span four public datasets (mammography and ophthalmology) covering classification and segmentation tasks across multiple institutions and modalities. Comparisons against strong baselines (Mirai, Asym‑Mirai, Eyemost, Eyestar, and modern multi‑task/multi‑modal methods) are comprehensive.  \nHowever:\n- Limited methodological detail on hyperparameter search, statistical analysis, and how datasets were partitioned or balanced.  \n- The conditional mutual‑information computation could use more mathematical justification (approximation stability, computational complexity).  \n- Reproducibility indicators (e.g., code link, random seed control) are lightly specified.\n\n**4. Clarity and Presentation**  \nThe manuscript is generally well organized and technically dense. Figures are appropriately referenced and conceptually clear. Some language polishing is still needed (“modality-shared modality-task MoE” phrasing is hard to parse), and equations could be typeset more compactly. The narrative occasionally mixes motivation, assumption, and result within the same paragraph, which can obscure clarity.\n\n**5. Ethical and Reproducibility Compliance**  \nAll datasets mentioned are public and appropriately cited. There are statements on hardware, training setup, and code availability. The authors do not explicitly mention IRB, but since only public de‑identified datasets are used, this is acceptable.\n\n---\n\n## Phase 2 — IEEE TMI Review Report\n\n### 1. Summary\nThe manuscript presents **M4oE**—a *Multi‑modal Multi‑task Mixture of Experts* framework for medical imaging. The model introduces (i) **Modality‑Specific experts** to preserve modality‑unique information, and (ii) a **Modality‑Task‑shared Mixture‑of‑Experts** to capture dynamic dependencies between modalities and clinical tasks. A conditional mutual‑information regularization guides experts to learn diverse, task‑dependent modality interactions. The approach is validated on four multi‑modal imaging datasets (mammography and retinal imaging), showing consistent gains over single‑task and multi‑modal baselines in classification and segmentation accuracy, and offering interpretable analysis of modality contributions.\n\n### 2. Strengths\n- **Methodological Novelty:** Joint modeling of modality, expert, and task dependence through probabilistic MoE design.  \n- **Solid Empirical Support:** Multi‑dataset validation covering both imaging domains and institutions.  \n- **Interpretability:** Provides sample‑ and population‑level modality attribution.  \n- **Potential Clinical Relevance:** Addresses real‑world multimodal diagnostic workflows (e.g., combining FFDM and 2DS, fundus and OCT).  \n- **Rigor of Experiments:** Ablation and gradient‑conflict analyses show understanding of the learning dynamics.\n\n### 3. Weaknesses\n- **Limited Theoretical Rigor:** Conditional mutual‑information objective is motivated intuitively; a stronger formulation or proof of convergence/generalization would help.  \n- **Implementation Transparency:** Details of task weighting, expert architecture, and statistical confidence intervals are missing.  \n- **Clarity Issues:** Technical explanations could be streamlined; notation and abbreviations occasionally impede readability.  \n- **Generalization Validation:** Although datasets differ by country, external prospective data or ablation on unseen modalities would further strengthen claims.\n\n### 4. Major Comments\n1. **Clarify the mutual‑information regularization:** Derive or cite how it is estimated in practice (discrete vs. continuous, stability). Discuss computational burden and whether gradients are stable with large numbers of experts.  \n2. **Provide statistical rigor:** Include standard deviations or confidence intervals across folds for Table 1 results and state significance testing.  \n3. **Broaden methodological context:** Explicitly contrast with existing dynamic fusion MoE or expert‑routing works (e.g., LIMOE, AdaMV‑MoE) to delineate novelty in formulation.  \n4. **Reproducibility:** Include training/validation/test splits, random seed info, data preprocessing steps, and confirm that code repository is public and functional.  \n5. **Improve clarity of framework description:** Consider a concise schematic summarizing data flow, inputs → MSoE → MToE → task heads, with consistent notation.  \n6. **Discuss potential clinical translation:** What aspects of interpretability can be practically exploited by radiologists or ophthalmologists? Is there quantitative user study or qualitative feedback?\n\n### 5. Minor Comments\n- Correct typographical issues (spacing around “M4oE”, consistent capitalization).  \n- Streamline abstract: currently repetitive across introductory paragraphs.  \n- Add legends and clearer color scales to Figures 3–6 for readability.  \n- Cite latest TMI and MedIA works on multimodal fusion (2024) to reinforce positioning.  \n- Ensure all symbols in equations are defined (e.g., clarify Φ dimension changes).\n\n### 6. Overall Recommendation\n**Major Revision**  \nThe manuscript presents a promising and original methodological advance that fits well within TMI’s scope. However, several clarifications regarding theoretical formulation, reproducibility, and presentation need to be addressed before it meets TMI’s publication standard. With stronger explanation and validation, the work could become a solid contribution.\n\n### 7. Confidence Level\n**4 / 5** – High. The methodological topic and literature base are well understood; judgments rely on clear evidence from the manuscript.\n\n---\n\n**Summary Judgment:**  \n*Methodologically innovative and aligned with TMI focus; requires deeper theoretical exposition, more transparent experiments, and refined presentation before acceptance.*",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **M4oE**, a *Multi‑modal Multi‑task Mixture of Experts* framework designed to improve multimodal medical imaging analysis. This model integrates modality‑specific and modality‑task–shared expert modules and employs a conditional mutual‑information regularization term to address dynamic interactions between modalities and tasks. The approach is tested on four public datasets in mammography and retinal imaging for both classification and segmentation tasks. Overall, the work is methodologically innovative and aligns well with the *IEEE Transactions on Medical Imaging* (TMI) scope, though certain theoretical and presentation aspects require clarification to reach publication quality.\n\n**Major Comments**  \n1. **Clarification of Mutual‑Information Regularization:** The conditional mutual‑information objective is only conceptually described. Provide mathematical details or references explaining its practical estimation (discrete vs. continuous variables, stability, computational load) and discuss convergence and gradient stability for many experts.  \n2. **Statistical and Experimental Rigor:** Include explicit details of dataset partitioning, hyperparameter search, and statistical testing. Tables should report standard deviations or confidence intervals, and the significance of performance gains.  \n3. **Contextualization with Related Work:** Strengthen comparisons with recent adaptive fusion and expert‑routing architectures (e.g., LIMOE, AdaMV‑MoE) to establish the precise novelty of the proposed formulation.  \n4. **Reproducibility Transparency:** Specify task weighting schemes, modeling parameters, random seed settings, and ensure that the announced code repository is accessible and functional.  \n5. **Clarity of Framework Description:** The text occasionally mixes motivation and results. A concise schematic illustrating the pipeline (data → MSoE → MToE → task heads) and streamlined notation would improve comprehension.  \n6. **Clinical Interpretation and Impact:** Clarify how the model’s interpretability aspects could be utilized by radiologists or ophthalmologists, and whether user feedback or pilot clinical validation was considered.\n\n**Minor Comments**  \n- Correct typographical issues and maintain consistent use of “M4oE.”  \n- Simplify and de‑duplicate abstract statements.  \n- Improve figure readability with clearer legends and color schemes.  \n- Define all symbols in equations and make notation consistent.  \n- Cite the latest 2024 multimodal fusion studies in TMI and *Medical Image Analysis* to strengthen context.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript demonstrates clear **significance** by addressing multimodal, multitask learning—a timely topic in medical imaging. Its **innovation** lies in the probabilistic Mixture‑of‑Experts formulation linking modalities and tasks through conditional mutual information. The **evaluation** is relatively extensive across datasets and institutions, though it lacks full statistical rigor. **Reproducibility** remains partial: code availability and training details are mentioned but insufficiently specified. With theoretical sharpening, transparent methodology, and improved clarity, the paper could become a strong contribution to TMI.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "## Opening Paragraph\nThe manuscript presents a novel multi-modal, multi-task learning framework known as “M4oE” that integrates Mixture-of-Experts (MoE) modeling into medical imaging applications. The key contributions include significant methodological advancements in multimodal fusion, dynamic modality-task dependencies, and interpretable probabilistic modeling. This work aligns well with the scope of IEEE *Transactions on Medical Imaging* (TMI), showcasing innovation and relevance in tackling complex medical imaging challenges.\n\n## Major Comments\n1. **Novelty and Contribution**: The authors introduce Modality-Specific MoE (MSoE) and Modality-Shared Modality-Task MoE (MToE), alongside a novel conditional mutual-information regularization term. This approach facilitates dynamic coupling among tasks and modalities, presenting a conceptually meaningful extension to existing fusion architectures.\n2. **Technical Rigor**: The experiments are comprehensive, covering four public datasets (mammography and ophthalmology) with comparisons to various robust baselines. However, there is limited methodological detail regarding hyperparameter searches, statistical analyses, and dataset partition strategies.\n3. **Evaluation Clarity**: Additional mathematical justification for the conditional mutual-information computation is necessary, including aspects like approximation stability. Moreover, details regarding reproducibility indicators such as code availability and random seed control are not sufficiently clear.\n\n## Minor Comments\n- Polishing of language is needed for clarity (e.g., the phrase “modality-shared modality-task MoE” is confusing).\n- Figures are well-referenced, though clearer legends and color scales should be provided for Figures 3–6.\n- Ensure equitable definitions for all symbols in equations.\n- Correct typographical inconsistencies (e.g., spacing around “M4oE” and capitalization).\n- Streamline the abstract to reduce redundancy with the introduction.\n\n## Summary Paragraph\nThis manuscript holds significant promise in providing innovative solutions for medical imaging via the M4oE framework. Regarding TMI's editorial criteria: it demonstrates notable significance and innovation while also presenting solid empirical evaluations. However, clarity and reproducibility aspects need attention to meet rigorous standards. The manuscript, with revisions for theoretical depth and transparency, offers the potential for substantial contributions to the field.\n\n## Decision Recommendation\n**Major Revision**  \nWhile the manuscript features a strong methodological contribution, improvements are essential in theoretical exposition, reproducibility, and presentation clarity to meet TMI's publication standards effectively. \n\n## Confidence Level\n**4 / 5** – High. The concepts presented are well understood within the context of related literature, making the reviewer's assessments reliable based on the manuscript's content.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Liyue Shen",
      "Luning Wang",
      "Zhengxu Tang",
      "Zitao Shuai",
      "Chenwei Wu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_59d8664cbd1f44ea7da5f36cba0de2bec3d71759.pdf",
    "remote_url": "https://openreview.net/pdf/59d8664cbd1f44ea7da5f36cba0de2bec3d71759.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Synthesizing Realistic fMRI: A Physiological Dynamics-Driven Hierarchical Diffusion Model for Efficient fMRI Acquisition",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Tolga"
    ],
    "primary_area": [
      "learning on time series and dynamical systems"
    ],
    "keywords": [
      "Time Series",
      "Diffusion"
    ],
    "abstract": "Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications. While generative models such as diffusion models can synthesize fMRI signals to alleviate these issues, they often underperform due to neglecting the brain's complex structural and dynamic properties.\nTo address these limitations, we propose the Physiological Dynamics-Driven Hierarchical Diffusion Model, a novel framework integrating two key brain physiological properties into the diffusion process: brain hierarchical regional interactions and multifractal dynamics. \nTo model complex interactions among brain regions, we construct hypergraphs based on the prior knowledge of brain functional parcellation reflected by resting-state functional connectivity (rsFC). This enables the aggregation of fMRI signals across multiple scales and generates hierarchical signals. \nAdditionally, by incorporating the prediction of two key dynamics properties of fMRI—the multifractal spectrum and generalized Hurst exponent—our framework effectively guides the diffusion process, ensuring the preservation of the scale-invariant characteristics inherent in real fMRI data.\nOur framework employs progressive diffusion generation, with signals representing broader brain region information conditioning those that capture localized details, and unifies multiple inputs during denoising for balanced integration.\nExperiments demonstrate that our model generates physiologically realistic fMRI signals, potentially reducing acquisition time and enhancing data quality, benefiting clinical diagnostics and machine learning in neuroscience.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nIn this paper, the authors proposed a novel framework named the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) for fMRI analytics. The PDH-Diffusion framework integrates two essential brain physiological properties, hierarchical regional interactions and multifractal dynamics, into the diffusion process. The primary goal is to improve diffusion models’ capability to generate realistic fMRI time series signals by accurately capturing these physiological characteristics.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nOverall, the major strength of this work lies in its novelty. The authors have developed an innovative framework that captures complex interdependencies and multifractal dynamics within synthetic fMRI signals.\n\nSpecifically, their contribution includes integrating three key components into the diffusion process: a hypergraph-based signal generator, a dynamics properties guiding module, and a cross-brain region progressive diffusion model. This integration enhances the realism of the generated signals. The authors provide a robust theoretical foundation for their methods and perform extensive quantitative analysis, demonstrating the framework’s accuracy and effectiveness in time series forecasting. The paper is well-organized and includes relevant background information. Results from the proposed method outperform multiple peer models in time-series forecasting and diffusion models, as evidenced by superior MAE, MAPE, and RMSE scores, highlighting the model’s effectiveness.\n\n### Weaknesses\n\nThe reviewers have multiple concerns about the framework and potential impact in this work. \n\n1). The confusion about physiological fMRI. Usually, fMRI are categorized into resting-state and task-based fMRI. The resting-state fMRI is commonly scanned without specific stimulus, whereas task-based fMRI is acquired based on external stimulus, such as 7 tasks in HCP. Is physiological fMRI is either resting-state or task-based signal? The authors do not clarify the concept even in Introdcution section.\n\n2). Limited motivation and impact. In Abstract, the authors mentioned \"Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications.\" It seems that the authros' work can advance the fMRI for clincial application, but the authors do not generate some neurological or psychiatric fMRI to validate. From reviewers' perspective, using the innovative PDH-Diffusion model, it can assist physician to provide lengthy fMRI signal which will denfinitely reduce the inconvenience of patients. Only generating healthy fMRI can impair the motivation and impact of this work.\n\n3). Lacking of qualitative comparison. The authors have provided an extensive quantitative validation of PDH-Diffusion model with other peer methods using MAE, MAPE, and RMSE. Unfortunately, the autors do not provide any qualitative results, such as Functional Connectivity Map or Brain Connectivity Maps, of PDH-Diffusion. That is, although averaging metrics such as MAE, RMSE, MAPE across 10 runs may demonstrate robustness, these metrics  cannot fully capture the quality or realism of the synthesized fMRI signals. Notably, the qualitative results is also vital in clinics, since these results showcase which brain regions are severly impaired by neurological disorders.  Importantly, there is no visual representation given of the generated fMRI signal, which would be valuable for assessing their plausibility. \n\n4). Several technical issues. The variance schedule (parameters $\\alpha_n$ and $\\beta_n$) in the diffusion process may not be fully optimized for different regions or scales, potentially leading to inappropriate noise levels in certain hierarchical levels. This could result in over-smoothing or overfitting at certain levels. Additionally, conditioning on historical data could lead to overfitting if the model becomes too dependent on past values, especially if the training data does not represent the full spectrum of brain dynamics. Without explicit mitigation measures, such as adaptive variance schedules or regularization techniques, these issues may limit the model’s ability to generalize to new or varied patterns, impacting its robustness and effectiveness.\n\n5). Multiple typographical mistakes. In Section 4.3 (ABLATION ANALYSIS) there are many typographical errors. The term “share radio” is used instead of ‘share ratio’ in “The influence of share radio” this section. Similar typographical errors appear in the caption and text of Figure 3(b), as well as in Section 3.4. These typographical mistakes impair the redability of this paper.\n\n### Questions\n\nThe reviewers have raised several questions regarding the weaknesses of this work:\n\n1). Qualitative Comparisons: The authors are strongly encouraged to provide qualitative comparisons between the synthesized and real fMRI signals to allow for a qualitative assessment of the model’s performance.\n\n2). Optimal Range of Brain Regions: The results suggest an “optimal” range of brain regions that enhances performance. How is this range determined, and is it manually set? Reviewers are concerned about the reliance on manual design for determining this optimal range.\n\n3). Validation of Multifractal Properties: How do the authors validate that the generated signals preserve multifractal properties?\n\n4). Risk of Overfitting: Do the variance schedule parameters ($\\alpha_n$ and $\\beta_n$) and the historical data used in training the PDH-Diffusion model lead to overfitting?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a novel framework called the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion), aimed at enhancing fMRI analytics. The framework integrates hierarchical regional interactions and multifractal dynamics into the diffusion process, with the primary objective of improving the generation of realistic fMRI time series signals. While the work exhibits novelty, the overall clarity and impact could benefit from further elaboration and qualitative evaluations.\n\n### Major Comments\n1. **Confusion Around Physiological fMRI**: The manuscript lacks clarity on whether the physiological fMRI is categorized as resting-state or task-based. This distinction is important and should be addressed in the introduction.\n   \n2. **Limited Motivation and Impact**: The paper suggests potential advancements for clinical applications of fMRI; however, it does not validate the framework with neurological or psychiatric fMRI data, potentially diminishing its impact.\n   \n3. **Lack of Qualitative Comparison**: While extensive quantitative validations using metrics like MAE, MAPE, and RMSE are provided, the absence of qualitative assessments (e.g., Functional Connectivity Maps) hinders a full evaluation of the synthesized fMRI signals' realism and clinical relevance. Visual representations of generated signals are also lacking.\n   \n4. **Technical Issues**: There may be optimization issues with the variance schedule parameters ($\\alpha_n$ and $\\beta_n$), potentially affecting noise levels and leading to over-smoothing or overfitting. Additionally, reliance on historical data could contribute to overfitting if not managed with regularization techniques.\n   \n5. **Typographical Mistakes**: Various typographical errors are present, particularly in Section 4.3 and Figure 3(b), which detract from the readability and professionalism of the manuscript.\n\n### Minor Comments\n- Figures and captions contain typographical issues that need correction.\n- The clarity and professionalism of the presentation could be enhanced through careful proofreading.\n\n### Summary Paragraph\nIn summary, the manuscript demonstrates significant potential in relation to the TMI editorial criteria, particularly in its innovative approach to fMRI signal generation. However, the absence of qualitative evaluations limits its significance and robustness. The evaluation design, though quantitatively sound, requires qualitative assessments to enhance credibility. While the reproducibility of results is noted, there are concerns regarding overfitting and the optimization of model parameters.\n\n### Decision Recommendation\nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Physiological Dynamics–Driven Hierarchical Diffusion Model* (PDH‑Diffusion) for fMRI signal generation. The framework combines two key physiological characteristics—hierarchical regional interactions and multifractal dynamics—into the diffusion process, aiming to produce more realistic synthetic fMRI time series. The paper is generally well organized and theoretically grounded, presenting comprehensive quantitative evaluations that indicate improvements over existing diffusion and time‑series forecasting models.  \n\n**Major Comments**  \n1. **Conceptual Ambiguity of Physiological fMRI:** The term “physiological fMRI” is not clearly defined. It remains uncertain whether this refers to resting‑state or task‑based fMRI. The introduction should clarify this concept to avoid confusion.  \n2. **Limited Motivation and Impact:** The motivation described in the abstract suggests potential clinical relevance (e.g., reducing scan time or patient inconvenience). However, experiments are conducted only on healthy fMRI data without neurological or psychiatric examples, reducing the potential medical significance.  \n3. **Lack of Qualitative Comparisons:** Although extensive quantitative results (MAE, MAPE, RMSE) are provided, no qualitative analyses such as functional or brain connectivity maps are shown. Visual representations of generated fMRI activity are crucial to assess realism and clinical relevance.  \n4. **Technical Concerns in Diffusion Parameters:** The variance schedule parameters (αₙ, βₙ) may not be properly optimized across scales or brain regions, possibly introducing over‑smoothing or overfitting. Conditioning on historical data could further limit generalization if the training set lacks sufficient variability. Mitigation strategies such as adaptive scheduling or regularization should be discussed.  \n5. **Typographical Errors:** Multiple typographical issues (e.g., “share radio” instead of “share ratio” in Section 4.3 and Figure 3(b)) detract from readability.  \n\n**Minor Comments**  \n- Clarify how the “optimal” range of brain regions is determined—manual selection versus learned parameterization.  \n- Expand discussion on how multifractal properties are validated in the generated signals.  \n- Ensure consistent terminology and correct remaining minor typographical errors throughout.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThis work is notable for its innovative integration of hierarchical and multifractal physiological dynamics within a diffusion framework, offering potential conceptual advances for fMRI signal modeling. Its significance would increase with clearer definitions, clinical validation, and qualitative visualization. Evaluation is rigorous quantitatively but incomplete qualitatively. The paper is reproducible in structure and presentation but would benefit from additional methodological clarifications and corrected typographical issues.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial clarification, qualitative validation, and correction of technical and typographical issues are needed before the manuscript can be reconsidered.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors proposed a novel algorithm for synthesizing realistic functional MRI (fMRI) via physiological dynamics-driven hierarchical diffusion model. Then, the authors validated the feasibility of synthesized fMRI data by comparing it to other generative mechanisms. The scientific merit of this work mainly comes from the conceptual advances in their proposed algorithm. It is indeed challenging to synthesize realistic fMRI data while preserving unique aspects of the brain system. The authors combined three different modules, each serving different roles, to synthesize brain dynamics with preserved network-like structure and fractal components. Given results of the extensive validation experiment on the large fMRI cohort, the validity of the proposed algorithm is clear; yet, the scientific significance of this work is somewhat limited as there was no experiment demonstrating the practical usefulness of the proposed algorithm.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe major originality of this study comes from its conceptual advances embedded in the proposed algorithm. It is indeed challenging to synthesize realistic fMRI data while preserving unique aspects of the brain system. The quality and clarity of the models in the main text are reasonably strong as well.\n\n### Weaknesses\n\nThe major concern comes from the unclear significance of this work. As the authors argued in the Introduction, acquisition of fMRI is expensive. Thus, synthesizing fMRI signal can be tempting. Although expensive, real fMRI data reflects unique information of individuals. This study, however, was not able to demonstrate the synthesized fMRI still convey unique information of subjects. Slight improvement in forecasting future timepoints of fMRI signal does not suggest the significance or practical usefulness of the model. This works needs additional analysis or applications highlighting the unique advantage of the synthesized fMRI data from the proposed model.\n\n### Questions\n\n1. In table 1, does “T_Pred=32 or 64” mean predicting 32 or 64 time points of fMRI data?\n2. If my speculation in the former question is right, it is counter-intuitive that forecasting more time points (96 vs. 32) did not lead to an increase in errors. Please clarify this. \n3. According to Fig 1, it looks like the resolution of synthesized data is bounded to the level of ROI. It is more desirable to synthesize fMRI dynamics at the level of vertex. Is the proposed method able to be applied at the vertex level as well?     \n4. In line 451, please double check the definition of MAE.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel algorithm for synthesizing realistic functional MRI (fMRI) data using a physiological dynamics-driven hierarchical diffusion model. The authors validate this algorithm by comparing the synthesized fMRI data against other generative mechanisms. Although the study showcases significant conceptual advancements and demonstrates the feasibility of the proposed method, the practical significance of the work is lacking as no experiments are conducted to highlight the utility of the synthesized data in real-world scenarios.\n\n### Major Comments\n1. **Novelty and Significance**: The originality of the proposed algorithm is commendable; however, the manuscript does not sufficiently demonstrate the practical usefulness or unique advantages of the synthesized fMRI data.\n2. **Evaluation Design**: The validation experiments on the large fMRI cohort establish the algorithm's feasibility, but additional analyses are required to confirm that the synthesized fMRI retains unique individual information.\n3. **Practical Application**: The improvement in forecasting future timepoints of fMRI signals does not convincingly support the significance of the model's practical applications.\n\n### Minor Comments\n- Clarify the meaning of “T_Pred=32 or 64” in Table 1 regarding the prediction of time points.\n- Explain why forecasting more time points (96 vs. 32) does not result in increased errors, as this is counter-intuitive.\n- In Figure 1, it appears that the synthesized data's resolution is limited to the ROI level. Can the proposed method also apply at the vertex level?\n- Verify the definition of MAE in line 451 for accuracy.\n\n### Summary Paragraph\nIn summary, this study makes valuable conceptual contributions to the synthesis of fMRI data using a novel algorithm demonstrating feasible results through extensive validation. However, the practical significance and innovation are called into question, as the authors do not provide evidence to support the utility of the synthesized data in preserving unique individual information. The evaluation design is solid, but further analysis is needed to enhance the findings' reproducibility and significance regarding real-world applications.\n\n### Decision Recommendation \nMajor revision is recommended to address the identified weaknesses, particularly regarding the demonstration of the synthesized data's practical usefulness and unique advantages.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a physiological dynamics–driven hierarchical diffusion model to synthesize realistic functional MRI (fMRI) data. The work integrates three modules designed to preserve network-like brain structure and fractal characteristics during data synthesis. The study’s main contribution lies in the conceptual novelty of the proposed generative framework. Validation experiments using a large fMRI cohort provide evidence that the algorithm is technically sound and capable of generating plausible synthetic data. However, despite the methodological sophistication, the manuscript’s scientific significance appears limited due to the absence of experiments demonstrating the practical usefulness or interpretive value of the synthesized data. Overall, the presentation is clear and the modeling is well explained.  \n\n**Major Comments**  \n1. **Significance and Practical Utility:** The main weakness lies in the unclear real-world significance. Although the method synthesizes realistic fMRI signals, it does not show that these synthetic data capture subject-specific or biologically meaningful information. Demonstrating that the generated fMRI data can serve practical research or clinical purposes would considerably strengthen the contribution.  \n2. **Evaluation Design:** The reported improvement in forecasting future time points is modest and does not convincingly convey the advantage of the proposed model. Additional analyses illustrating unique benefits of the synthesized data are needed.  \n3. **Clarification of Prediction Setting:** In Table 1, it is unclear whether “T_Pred = 32 or 64” refers to the number of predicted time points. If so, the finding that forecasting more time points (96 vs. 32) does not increase errors is counterintuitive and should be explained.  \n4. **Spatial Resolution Limitation:** Figure 1 suggests that the synthetic data are generated at the ROI level. It would be valuable to clarify whether the method can be extended to vertex-level synthesis to better capture local dynamics.  \n\n**Minor Comments**  \n- Please check the definition of MAE in line 451 for correctness.  \n- Ensure all figures and tables clearly state experimental parameters and units.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is methodologically innovative but of moderate overall significance. The conceptual approach demonstrates originality (innovation) and is evaluated on a large dataset (evaluation), yet it falls short in establishing application value or broader scientific impact (significance). The experiments are reproducible in principle, though more details on implementation and parameters would facilitate replication.  \n\n**Decision Recommendation**  \n**Major revision.** The manuscript requires additional analyses or demonstrations that highlight the practical advantages and scientific relevance of the synthesized fMRI data.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a novel approach for generating realistic fMRI data using diffusion models, specifically designed to account for regional interactions and spectral features of the brain. The model captures regional connectivity in a hierarchical structure, where fine-scale signals are conditioned on larger-scale signals. To learn spectral features, it incorporates loss functions that capture fractal characteristics. Results demonstrate improved performance over existing time-series forecasting and diffusion-based models. Additionally, ablation studies validate the effectiveness of each model component.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The proposed method, which captures connectivity and spectral features, is a novel approach. \n2. The method is rigorously validated using multiple benchmarks and ablation studies.\n\n### Weaknesses\n\n1. The paper has limited reproducibility due to missing details about data preparation and experimental setup. Additional information is needed on the dataset used, including whether it involved resting-state or task-based fMRI, whether subjects were healthy or under specific conditions, and the rationale for selecting regions of interest (ROI), which were reduced from 268 to 82. Clarification on data split (e.g., train/test division, sample counts) is also required. If the codebase will not be provided, the paper should include a detailed description of the network architecture (such as layer specifications and activation functions) and the training setup for benchmark methods in an appendix.\n2. The practical implications, particularly the clinical applications of the proposed method, are somewhat unclear and could benefit from further exploration and discussion.\n\n### Questions\n\n1. What is the sampling frequency for the fMRI?\n2. It would be beneficial to analyze the reconstructed signal to determine if the observed patterns align with expectations. Calculating the spectrum and fractal characteristics would provide an important validation of the model’s effectiveness. Additionally, some neurological features can also be checked. For instance, if the data is from resting-state fMRI, does it reveal the default mode network?  \n3. Line 465 typo 69->96\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a novel methodology for generating realistic fMRI data through diffusion models, focusing on the integration of regional interactions and spectral features of the brain. The proposed model effectively captures hierarchical regional connectivity, linking fine-scale signals to larger-scale signals. It employs specific loss functions to encapsulate fractal characteristics. The findings indicate enhanced performance compared to existing time-series forecasting and diffusion models. Furthermore, ablation studies substantiate the contributions of individual model components.\n\n**Major Comments:**\n1. The reproducibility of the study is limited due to insufficient details regarding data preparation and the experimental setup. Specific information is necessary about the dataset, including whether it encompasses resting-state or task-based fMRI, the health conditions of subjects, and the rationale behind the reduction of regions of interest (ROI) from 268 to 82. Clarification on the data split, including train/test division and sample counts, is also essential. If the codebase is not shared, an appendix should provide comprehensive details about the network architecture, including layer specifications and activation functions, along with the training setup for benchmark methods.\n2. The clinical implications of the proposed method are not clearly articulated and would benefit from further discussion and exploration.\n\n**Minor Comments:**\n- The sample frequency for the fMRI data should be specified.\n- An analysis of the reconstructed signal is recommended to verify alignment with expected patterns. This should include calculating the spectrum and fractal characteristics for additional validation of the model's efficacy. It may also be pertinent to investigate neurological features, such as whether the resting-state data reflects the default mode network.\n- There is a typo noted on line 465, where \"69\" should be corrected to \"96.\"\n\n**Summary Paragraph:**\nIn evaluating the manuscript against the TMI editorial criteria, it demonstrates significance through its innovative approach to fMRI data generation. While the contributions are noteworthy and backed by rigorous validation, significant concerns regarding reproducibility and clarity of clinical applications remain. The manuscript would benefit from the inclusion of crucial experimental details and further exploration of practical implications. \n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a novel diffusion-based method for generating realistic fMRI time-series data. The approach explicitly models regional interactions through a hierarchical network structure, where fine-scale signals are conditioned on broader regional dynamics, and introduces loss functions to capture spectral and fractal characteristics. The work is clearly motivated and demonstrates improved performance over baseline forecasting and diffusion models, supported by comprehensive ablation analyses. Overall, the study is conceptually interesting and technically well executed, though certain aspects require clarification to ensure reproducibility and stronger contextual grounding.  \n\n**Major Comments**  \n1. **Reproducibility and Experimental Details:** The reproducibility of the study is limited by insufficient detail about the dataset and experimental setup. The manuscript should specify whether the data are resting-state or task-based fMRI, the subject population (e.g., healthy or clinical), and the rationale for reducing the number of regions of interest from 268 to 82. Details on data splitting (training/testing proportions and sample counts) should also be provided. If the source code will not be released, a more explicit description of the network architecture, layer types, activations, and training parameters for both the proposed and baseline models should be included, ideally in an appendix.  \n2. **Practical and Clinical Relevance:** The paper does not sufficiently discuss potential clinical or neuroscientific implications. Expanding on how the generated data may support real-world neuroimaging analyses or clinical applications would enhance the significance and impact of the work.  \n\n**Minor Comments**  \n- Clarify the fMRI sampling frequency used in experiments.  \n- Consider analyzing reconstructed signals to verify whether spectral and fractal metrics match expectations and whether relevant brain networks (e.g., default mode network for resting-state fMRI) can be identified.  \n- Typographical correction: line 465, “69” should be “96.”  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is significant in proposing a structured diffusion model that integrates spatial hierarchy and spectral learning for fMRI data synthesis. Its methodological innovation is clear and supported by empirically validated results. However, the evaluation would benefit from greater transparency regarding data and methods to ensure reproducibility. The clinical significance remains to be fully articulated. With more detailed reporting and expanded discussion, the work could make a meaningful contribution to generative modeling in neuroimaging.  \n\n**Decision Recommendation**  \n**Major Revision.** Clarification of data handling, methodological details, and application relevance is required before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes a Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) for generating realistic fMRI time series signals to address challenges of lengthy acquisition times and motion sensitivity in fMRI scanning. The method integrates two key physiological properties: brain hierarchical regional interactions modeled through hypergraphs based on resting-state functional connectivity (rsFC), and multifractal dynamics characterized by multifractal spectrum and generalized Hurst exponent. The framework consists of three components: a Hypergraph-based Hierarchical Signals Generator that aggregates fMRI signals across multiple spatial scales, a Dynamics Properties Guiding Module that predicts multifractal characteristics to condition the diffusion process, and a Cross-brain Region Guiding Progressive Diffusion Model that uses broader regional signals to guide localized signal generation. Experiments on the Human Connectome Project dataset demonstrate superior performance compared to baseline time series forecasting and diffusion methods across prediction lengths of 32, 64, and 96 timesteps, with MAE improvements from 29.36 to 28.56 for 32-step prediction.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and notation issues**\n  - The transition from Equation (2) to Equation (3) lacks clear mathematical justification for the self-similarity property derivation (Page 3, Section 2.2)\n  - Notation inconsistency where Y(t) is defined as integral of X(s) but then used as increment process without clear definition (Page 3-4, Equations 2-4)\n  - The multifractal spectrum calculation in Equation (8) uses standard formulations but lacks connection to the specific fMRI context and how it relates to the predictor design (Page 7, Section 3.3)\n\n• **Limited experimental validation and baseline comparisons**\n  - Only one dataset (HCP) is used for evaluation, limiting generalizability claims across different populations and scanning protocols (Table 1, Page 9)\n  - Missing comparisons with recent fMRI-specific generation methods or brain signal synthesis approaches that could be more relevant than general time series models (Section 4.1, Page 8)\n  - Ablation studies show substantial performance drops but lack analysis of why each component contributes differently (Table 2, Page 10)\n\n• **Insufficient technical detail and reproducibility concerns**\n  - The hypergraph construction using k-hop neighbors method lacks specific implementation details and parameter selection criteria (Section 3.2, Page 5-6, Figure 2)\n  - The multi-condition integration mechanism is described vaguely without clear mathematical formulation for how conditions are weighted and combined (Section 3.4, Page 7-8)\n  - Training procedure details are sparse, including how the multifractal properties are computed during training and inference phases (Section 3.5, Page 8)\n\n• **Questionable physiological relevance and validation**\n  - No validation that generated signals actually preserve meaningful brain network properties beyond multifractal characteristics (No direct evidence found in the manuscript)\n  - The claim that broader regional signals provide \"general activity trends\" lacks empirical support or neurobiological justification (Section 3.4, Page 7)\n  - Missing analysis of whether the generated signals maintain clinically relevant features for diagnostic applications mentioned in the introduction (Page 1, Abstract)\n\n## Suggestions for Improvement\n\n• **Strengthen mathematical rigor and notation consistency**\n  - Provide detailed mathematical derivation connecting the power law relationship in Equation (1) to the self-similarity properties in Equations (3)-(4)\n  - Clearly define the relationship between the increment process Y(t) and the original signal X(t) with consistent notation throughout\n  - Include detailed mathematical formulation showing how the predicted multifractal spectrum guides the diffusion process conditioning\n\n• **Expand experimental validation comprehensively**\n  - Evaluate on additional fMRI datasets from different populations, scanning protocols, and clinical conditions to demonstrate generalizability\n  - Include comparisons with fMRI-specific generation methods and recent brain signal synthesis approaches rather than only general time series models\n  - Provide detailed analysis explaining why each ablated component contributes to performance, including visualizations of generated signals with and without each module\n\n• **Enhance technical transparency and implementation details**\n  - Specify exact parameters, thresholds, and selection criteria for hypergraph construction including how k-hop neighbors are determined for different brain regions\n  - Provide explicit mathematical formulation for the multi-condition integration mechanism including weighting schemes and combination strategies\n  - Include comprehensive training algorithm with detailed procedures for multifractal property computation, loss function optimization, and inference-time prediction\n\n• **Validate physiological meaningfulness rigorously**\n  - Conduct quantitative analysis showing that generated signals preserve essential brain network properties such as functional connectivity patterns, network topology, and clinical biomarkers\n  - Provide neurobiological evidence or empirical validation supporting the claim that broader regional signals capture meaningful activity trends\n  - Include evaluation of generated signals on downstream clinical tasks such as disease classification or cognitive state prediction to demonstrate practical utility",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) aimed at producing realistic fMRI time series signals to overcome challenges such as lengthy acquisition times and motion sensitivity inherent in fMRI scanning. This method combines key physiological properties, including brain regional interactions modeled through hypergraphs based on resting-state functional connectivity and multifractal dynamics characterized by a multifractal spectrum and generalized Hurst exponent. The methodology is comprised of a Hypergraph-based Hierarchical Signals Generator, a Dynamics Properties Guiding Module, and a Cross-brain Region Guiding Progressive Diffusion Model. Experimental results indicate improvements in prediction accuracy compared to baseline methods.\n\n### Major Comments\n1. **Mathematical formulation inconsistencies and notation issues**: \n   - Lack of clear justification for self-similarity in the transition from Equation (2) to (3) (Page 3, Section 2.2).\n   - Notation inconsistency: Y(t) defined as an integral of X(s) is later treated as an increment process without adequate clarification (Pages 3-4, Equations 2-4).\n   - The multifractal spectrum calculation in Equation (8) uses standard formulas but fails to connect with the fMRI context.\n\n2. **Limited experimental validation and baseline comparisons**: \n   - Evaluation relies on a single dataset (HCP), limiting the generalizability of results (Table 1, Page 9).\n   - Missing comparisons with relevant fMRI generation methods more aligned with the study's aims (Section 4.1, Page 8).\n   - Ablation studies show significant performance drops without an analysis of component contributions (Table 2, Page 10).\n\n3. **Insufficient technical detail and reproducibility concerns**: \n   - The hypergraph construction lacks detailed parameters and selection criteria (Section 3.2, Pages 5-6, Figure 2).\n   - Vague description of the multi-condition integration mechanism without clear mathematical formulation (Section 3.4, Pages 7-8).\n   - Sparse details on training procedures, particularly regarding the computation of multifractal properties (Section 3.5, Page 8).\n\n4. **Questionable physiological relevance and validation**: \n   - Absence of evidence confirming that generated signals retain meaningful brain network properties (No direct validation in the manuscript).\n   - The assertion regarding broader regional signals providing \"general activity trends\" lacks empirical support (Section 3.4, Page 7).\n   - Evaluation of whether generated signals maintain features relevant for diagnostics is missing (Page 1, Abstract).\n\n### Minor Comments\n- Strengthen the mathematical rigor and consistency in notation throughout the manuscript.\n- Improve clarity in Figures and Tables for better comprehension.\n- Correct typographical errors identified within the document.\n\n### Summary Paragraph\nThis manuscript exhibits several strengths including the exploration of innovative methodology within fMRI signal generation. However, it struggles with issues relating to the significance of findings, limited experimental validation, and concerns regarding reproducibility. While the innovation in integrating hierarchical and multifractal dynamics is noteworthy, the lack of comprehensive comparisons and detailed methodological descriptions inhibits the evaluation of its practical utility. Overall, empirical validation of the physiological relevance of the generated signals remains a critical gap.\n\n### Decision Recommendation\nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a *Physiological Dynamics‑Driven Hierarchical Diffusion Model (PDH‑Diffusion)* aimed at generating realistic fMRI time‑series signals to mitigate lengthy acquisition times and motion sensitivity issues. The approach leverages hierarchical brain interactions modeled via hypergraphs based on resting‑state functional connectivity (rsFC) and integrates multifractal dynamics characterized by the multifractal spectrum and generalized Hurst exponent. The framework combines three modules—a hypergraph‑based hierarchical signal generator, a dynamics‑guiding module using multifractal characteristics, and a cross‑region diffusion model—to simulate physiologically plausible signals. Experiments on the Human Connectome Project dataset show improved forecasting accuracy over baseline diffusion and time‑series models. Overall, the concept is original and of potential impact, but the paper suffers from mathematical inconsistencies, limited validation, and insufficient methodological detail.\n\n---\n\n**Major Comments**  \n1. **Mathematical formulation and notation.** The derivation of the self‑similarity property from Equations (2) to (3) lacks rigorous justification, and notational inconsistencies appear where *Y(t)* alternates between integral and increment definitions. The multifractal spectrum (Eq. 8) employs standard formulas but its adaptation to fMRI context and linkage to the predictor design are unclear.  \n2. **Experimental scope.** Evaluation on a single dataset (HCP) constrains generalizability. Comparisons omit recent fMRI‑specific generation and brain signal synthesis approaches. Ablation results are presented but without interpretive discussion of component contributions.  \n3. **Reproducibility and technical transparency.** The description of hypergraph construction, including *k*‑hop neighborhood parameters, and the multi‑condition integration mechanism are vague. Details of multifractal computation during training and inference are insufficient.  \n4. **Physiological relevance.** There is no evidence that generated signals preserve meaningful brain network properties or clinically relevant information. The assumption that broader regional signals convey general activity trends lacks empirical or neurobiological support.\n\n---\n\n**Minor Comments**  \n- Clarify notation consistency across Equations (2–4).  \n- Expand methodological descriptions in Sections 3.2–3.5 for reproducibility.  \n- Ensure all acronyms (e.g., rsFC, HCP) are defined at first mention.  \n- Improve figure captions for clarity, especially Figure 2 illustrating hypergraph structure.  \n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important and challenging topic with an innovative diffusion‑based generative model grounded in physiological dynamics. However, its significance is weakened by limited validation and uncertainty about physiological interpretability. While the conceptual innovation is notable, the evaluation lacks breadth, and the reproducibility is hindered by incomplete methodological disclosure. Enhanced mathematical clarity, broader experiments, and physiological validation are necessary to substantiate the claims and practical utility.  \n\n---\n\n**Decision Recommendation:** **Major Revision.** The manuscript has potential but requires substantial clarification, expanded experimental validation, and stronger physiological justification before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI – ✅**  \nThe paper presents a *methodological* contribution rather than an application-level one. It proposes a new **physiology-informed diffusion model for fMRI signal generation**, incorporating hierarchical hypergraph modeling of functional connectivity and multifractal temporal dynamics — both of which directly address the physics and statistical characteristics of fMRI data. This methodological focus aligns with IEEE TMI’s interest in medical imaging modeling, reconstruction, and signal characterization.\n\n**2. Novelty & Contribution Level – Moderate to High**  \nDiffusion models for temporal or medical imaging data are increasingly explored, but this work claims originality through:\n- Incorporation of **hypergraph-based functional connectivity priors** to encode hierarchical spatial structure;  \n- Integration of **multifractal dynamic properties (Hurst exponents, spectra)** as physiological guidance within the diffusion process;  \n- A **progressive cross-region conditioning strategy** that links multi-scale brain activity modeling.  \n\nThese constitute distinct and non-trivial extensions of existing diffusion-based fMRI synthesis or imputation approaches. The combination of neurophysiological priors with generative diffusion modeling represents an emerging methodological trend.\n\n**3. Technical and Experimental Rigor – Generally Solid**  \n- Methodology is well formulated, equations clearly derived; training objectives and module interplay are transparent.  \n- Experiments use the **HCP dataset** with reasonable baselines (both standard forecasting models and other time-series diffusion models).  \n- Ablations substantiate each design component.  \nHowever, more detail on *data preprocessing*, *subject-wise validation*, and *evaluation of physiological realism* (beyond MAE/RMSE/MAPE) would strengthen the rigor. Generalizability to other datasets or acquisition conditions is not demonstrated.\n\n**4. Clarity and Presentation – Good**  \n- The manuscript reads fluently, with good technical organization and clear figures.  \n- Some grammatical inconsistencies and typographical errors (\"business\" instead of \"basis\", variable notation inconsistencies) should be corrected.  \n- Figure labeling and captions could better articulate the diffusion process timelines and data flow.  \n\n**5. Ethical and Reproducibility Compliance – Adequate**  \n- Uses publicly available HCP dataset, appropriately referenced.  \n- Code availability is promised via GitHub.  \n- No concerns identified around human data or IRB usage given that HCP is anonymized open data.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThis paper introduces the **Physiological Dynamics-Driven Hierarchical Diffusion (PDH-Diffusion) model** for synthesizing realistic fMRI time-series data. The model incorporates (1) hierarchical spatial priors represented as hypergraphs derived from resting-state functional connectivity, and (2) temporal scale-free characteristics via a multifractal dynamics guiding module. A progressive diffusion process leverages inter-region conditioning to generate increasingly localized brain signals. Evaluation on the HCP dataset demonstrates improved time-series prediction performance (lower MAE/RMSE compared with existing diffusion and transformer baselines) and ablation studies support the design.\n\n**2. Strengths**\n- Conceptually novel integration of **neural physiology priors** with a **hierarchical generative diffusion model**.  \n- Careful mathematical exposition linking multifractal modeling to physiological interpretation of fMRI dynamics.  \n- Strong comparative and ablation experiments that show consistent quantitative gains.  \n- Clearly scoped toward *synthetic fMRI generation* with potential downstream impact on data augmentation and accelerated acquisition.\n\n**3. Weaknesses**\n- Evaluation remains largely **signal-level (forecast accuracy)**; lacks physiological plausibility metrics (e.g., power spectral slopes, fractal spectrum fidelity, or functional network preservation).  \n- Limited discussion of **validation strategy**—whether the training/testing splits are subject-independent is unclear.  \n- The claimed reduction in acquisition time is **speculative**, as no empirical or physics-based simulation supports it.  \n- The **methodological generalization to imaging modalities or multi-subject variability** is not explored.  \n- Minor clarity issues and somewhat heavy mathematical exposition may reduce accessibility for imaging readers.\n\n**4. Major Comments**\n1. **Validation depth:** Augment current evaluations with physiological realism metrics (fractal similarity, rsFC preservation, sample quality assessed by clinicians).  \n2. **Overclaim mitigation:** Rephrase the suggested impact on \"reducing acquisition time\" and \"enhancing clinical diagnostics\" to hypotheses rather than demonstrated outcomes.  \n3. **Reproducibility details:** Provide precise training/validation partitioning protocol, hyperparameter selection, and computational resources.  \n4. **Discussion & limitations:** Add explicit discussion on potential bias from using only HCP data and possible extension to task-based or pathological datasets.  \n5. **Model interpretation:** Clarify the mapping between hypergraph levels and known neuroanatomical scales (network-module-node) to enhance biological interpretability.\n\n**5. Minor Comments**\n- Typographical corrections: “rational business” → “rational basis”, “dimentional” → “dimensional”, etc.  \n- Improve figure fonts and resolutions (Figures 1–2).  \n- Standardize variable notation between text and equations.  \n- Some reference formats inconsistent (e.g., capitalization).  \n- A concise visual pseudocode diagram for the training pipeline would aid reader comprehension.\n\n**6. Overall Recommendation: MAJOR REVISION**  \nThe paper presents **methodological novelty of high potential relevance to TMI**, particularly its physiologically grounded perspective on generative modeling of fMRI. However, **experimental validation is limited to accuracy-based metrics**, and several claims regarding clinical utility and acquisition efficiency are unsupported. With enhanced experimental validation, tighter presentation, and clarified discussion of scope, the work could reach TMI publishable standards.\n\n**7. Confidence Level:** 4/5  \n(High confidence in methodological assessment; medium in physiological evaluation pending additional results.)\n\n---\n\n**Reviewer Summary:**  \nThis submission fits within IEEE TMI’s methodological scope and shows promising innovation in physiologically informed generative modeling for fMRI. It would benefit substantially from expanded validation demonstrating biological and clinical relevance, along with clearer exposition of assumptions and limitations.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a *Physiology‑informed Hierarchical Diffusion (PDH‑Diffusion) model* designed to synthesize realistic fMRI time‑series data. It integrates hierarchical hypergraph representations of functional connectivity and multifractal temporal dynamics to capture physiological properties of brain activity. The work is methodologically focused, relevant to IEEE TMI’s interests in signal characterization and model‑based image analysis. Overall writing is clear and technically organized; figures are informative though labeling could be improved. The study demonstrates quantitative gains on the HCP dataset, but evaluation remains limited in physiological validation and generalizability.\n\n---\n\n**Major Comments**  \n1. **Evaluation depth:** The experiments emphasize forecast accuracy (MAE/RMSE) but lack physiological plausibility metrics such as fractal spectrum fidelity or resting‑state network preservation. Inclusion of such measures would strengthen claims of realism.  \n2. **Validation protocol:** Clarify whether training and testing partitions are subject‑independent, and provide full details on data preprocessing, hyperparameter selection, and computational setup to ensure reproducibility.  \n3. **Claim moderation:** Statements about reducing acquisition time or enhancing diagnostics should be reframed as potential implications rather than verified outcomes, since no supporting empirical evidence or simulation is provided.  \n4. **Generalization and bias:** Discuss limitations arising from exclusive reliance on HCP data and suggest how the approach could extend to task‑based or pathological datasets.  \n5. **Model interpretation:** Elaborate on how hypergraph levels correspond to neuroanatomical hierarchies (network–module–node) to clarify the biological meaning of the representations.\n\n---\n\n**Minor Comments**  \n- Correct typographical and grammatical errors (e.g., “rational business” → “rational basis,” “dimentional” → “dimensional”).  \n- Improve figure font size, resolution, and captions to better depict diffusion process timelines.  \n- Standardize variable notation throughout text and equations; ensure consistent reference formatting.  \n- Including a concise pseudocode or workflow diagram would aid comprehension of the training pipeline.\n\n---\n\n**Summary (TMI Criteria)**  \n*Significance:* Addresses modeling of fMRI signal generation from physiological principles, a topic of considerable relevance.  \n*Innovation:* Integrates hypergraph‑based spatial priors and multifractal guidance—conceptually novel for diffusion modeling in neuroimaging.  \n*Evaluation:* Technically solid but limited to numerical prediction accuracy; lacks assessment of physiological realism or cross‑dataset validation.  \n*Reproducibility:* Use of public HCP data and promised code release are positive, yet experimental details need fuller specification.  \n\nStrengths include theoretical clarity, well‑defined ablation studies, and strong alignment with TMI’s methodological scope. Weaknesses center on restricted validation breadth, some overstatements of impact, and presentation polish.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe study presents a promising and technically robust methodological contribution but requires expanded validation, moderated claims, and clearer reproducibility details before it meets TMI publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a significant methodological contribution by proposing a novel **physiology-informed diffusion model for fMRI signal generation**. This model integrates hierarchical hypergraph modeling of functional connectivity with multifractal temporal dynamics, directly addressing the physics and statistical characteristics of fMRI data. The approach aligns well with IEEE TMI’s focus on advances in medical imaging modeling, reconstruction, and signal characterization.\n\n**Major Comments:**\n1. **Validation Depth:** Expand current evaluations to include physiological realism metrics, such as fractal similarity, resting-state functional connectivity preservation, and clinician-assessed sample quality.\n2. **Overclaim Mitigation:** Adjust the language regarding the model's impact on reducing acquisition time and enhancing clinical diagnostics, framing these as hypotheses rather than definitive outcomes.\n3. **Reproducibility Details:** Clarify the training/validation partitioning protocol, hyperparameter selection process, and details regarding computational resources used.\n4. **Discussion and Limitations:** Include a focused discussion on potential biases from relying solely on HCP data and the model's applicability to task-based or pathological datasets.\n5. **Model Interpretation:** Provide clearer mapping between hypergraph levels and established neuroanatomical scales (e.g., network-module-node) to enhance biological interpretability.\n\n**Minor Comments:**\n- Correct typographical errors such as “rational business” to “rational basis” and “dimentional” to “dimensional.”\n- Improve figure readability by enhancing font sizes and resolutions (specifically for Figures 1–2).\n- Standardize variable notation between the text and corresponding equations for consistency.\n- Ensure reference formats are consistent, particularly with regard to capitalization.\n- Including a concise visual pseudocode diagram of the training pipeline would improve reader comprehension.\n\n**Summary Paragraph:**  \nThe study introduces the **Physiological Dynamics-Driven Hierarchical Diffusion (PDH-Diffusion) model** aimed at synthesizing realistic fMRI time-series data. It demonstrates significant innovation through the integration of neural physiology priors and hierarchical generative methodologies. While the manuscript shows potential with improved time-series prediction performance in evaluations on the HCP dataset and solid comparative experiments, there are notable areas for improvement in the validation of physiological plausibility and discussions surrounding its experimental claims. These factors affect the work's overall significance, innovation, and reproducibility.\n\n**Decision Recommendation:**  \nMAJOR REVISION is recommended. The methodological novelty and relevance to TMI are evident, yet the manuscript requires enhanced experimental validation, more rigorous support for claims of clinical utility and acquisition efficiency, as well as clearer exposition of potential biases and limitations. \n\n**Confidence Level:** 4/5  \nThis assessment rests on high confidence in the methodological evaluation and medium confidence regarding physiological implications, pending additional validation results.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Wuyang Li",
      "Yixuan Yuan",
      "Yujiang",
      "Yufan Hu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_4e63299434922414da63f4df9505c42a3d79bc3b.pdf",
    "remote_url": "https://openreview.net/pdf/4e63299434922414da63f4df9505c42a3d79bc3b.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Tolga"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "data synthesis",
      "diffusion models",
      "cardiac MRI",
      "lung nodule CT",
      "segmentation"
    ],
    "abstract": "Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR.",
    "decision": "Accept (Spotlight)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a novel 3D lesion inpainting method, LeFusion, which uses diffusion models to address data scarcity in medical imaging. Its primary aim is to generate synthetic lesions in lung CT and cardiac MRI scans for augmenting training data in lesion segmentation tasks. The approach is validated through both visual quality assessments and data augmentation derived segmentation \n\nperformance improvement. Three key contributions can be summarised below: \nLeFusion Model: The authors identify that existing lesion inpainting methods struggle to preserve anatomically accurate backgrounds alongside the inpainted lesion, remarking that modelling the former is both hard and unnecessary. LeFusion is introduced to address this challenge incorporating two distinct features: (a) Training on a lesion focused diffusion loss, which only considers the lesion region. (b) Preserving the background at inference time with RePaint [1] by generating the lesion separately, while integrating forward-diffused background contexts into the reverse diffusion process. This design yields realistic lesions, better preserved backgrounds and improves data augmentation outcomes in both CT and MRI compared to non-lesion-specific models (Cond-Diffusion) both with and without RePaint based sampling. \n\nModality-Specific Variants: Two specialized variants are introduced to address modality-specific challenges. LeFusion-H uses histogram-based conditioning to capture diverse lesion textures in CT, succesfully solving the texture mode collapse observed for the baseline LeFusion. LeFusion-J models multiple tissue subtypes in MRI via multi-channel decomposition, which enables the joint generation of different lesion tissue types typically observed in cardiac lesions. Both variants demonstrate superior data augmentation effectiveness in their respective modalities. \n\nDiffMask for Mask Generation: All variants of LeFusion rely on either existing real masks or handcrafted ones as priors for generating lesions in healthy scans. As a more flexible alternative, DiffMask is a diffusion model that generates synthetic lesion masks from basic spatial constraints, defined as a sphere with user specified location and size. Using the generated masks for data augmentation leads to the largest improvement in segmentation performance relative to the baseline in both CT and MRI.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nLesion generating models are tools with significant potential for mitigating bias in medical vision AI algorithms concerning lesion detection, segmentation and quantification. Advancements in this topic should be highlighted in venues like this. \nThe manuscript is sufficiently well written, all the provided Figures/Tables are insightful and adequately formatted. \nThe choice of a 3D method for this inpainting problem is most adequate for CT and MRI. In these modalities, clinical lesion analysis workflows depend on the visualisation of multiple affected slices and 2D slice-wise inpainting methods would lead to slice-wise discontinuities. \n\nThe proposed method is sufficiently contextualised in the Introduction and Related work sections, where the reseach gap is clearly defined. Beyond that, this gap is empirically demonstrated by experimenting with state-of-the-art approaches (Cond-Diffusion variants and RePaint). \n\nThe proposed methodologies are thoroughly evaluated through comparisons with multiple other approaches, focusing on visual inspection of inpainted lesions (including comparison with real lesions) and their their downstream usability for training segmentation models. The latter evaluation used two different segmentation models, which contributes to the robustness of the findings across different segmentation training strategies. In addition, evaluating the approach on both MRI and CT datasets, ensures that the findings are not only applicable to one imaging domain. \n\nThis paper provides multiple key contributions which not only address the research gap but also deal with modality specific challenges related to lesion texture and shape heterogeneity. The corresponding claims are well supported by the results.\n\n### Weaknesses\n\nWhile S4, the Introduction and Background sections seem to imply that the proposed lesion focused loss is a novel contribution proposed for the first time by the authors. This might not be necessarily true considering that there have been other works that employ similar approaches [2, 3]. While few and perhaps not as thoroughly evaluated, mentioning them could further strengthen the contextualisation of the approach. \n\nThe description of the RePaint method in the experimental section implicitly suggests it consists of Cond-Diffusion using the RePaint [1] inference scheme. If that is the case it should be mentioned explicitly, if not then it should be better described. \nIn the segmentation experiments, it is understood that masks priors for generating lesions in healthy scans (N’) are either derived from real masks, handcrafted or generated by DiffMask. However, additional information should be provided on how exactly the conditioning histograms in this N’ setting are selected when using LeFusion-H variants. \n\nRegarding DiffMask, the definition and role of boundary mask is not very clear. From Figure 4, it is presumed that it corresponds to the bounding box defining the volume crop centred on the lesion. However, the statement “The boundary mask removes areas outside the boundary at each diffusion step” challenges this concept. Further clarity on this point would be appreciated. Furthermore, it is only implicit, that the DiffMask takes the CT/MRI volume crop as an input in addition to the conditioning control sphere. Section 3.3. should be updated to enhance clarity on all these aspects. \n\nAdding supplementary details on how the model training and checkpoint selection was conducted for the RePaint, Cond-Diffusion, Cond-Diffusion (L) would improve transparency. \n \n[Minor]\t \nMore detail on the dataset preprocessing would be beneficial for further reproducibility. A mention to the volume resolution is particularly lacking. \n\nThe choice of the specific crop-size could be further supported on previous work, for instance [4]. In addition, while not critical for acceptance, it would be interesting to study its effect over the results and would maybe answer the question: “How much local context is it necessary to generate realistic lesion?” \n\nWhile the purpose of the inpainted lesions is for downstream model training, further validating them using a radiologist would safeguard from potential biases that the generative model might be introducing the lesions. \n\nWhile describing Tables 1 and 2 it would be useful to clarify what is considered as “significant”. Since no standard deviations were provided, it is implied that these results were obtained for a single fold, so the concept of significance here is vague. In addition, while S5, the robustness of these findings to the specific data split could still be reinforced by adopting some sort of cross validation strategy. \nThe authors left unclear whether the segmentation model was trained on the volume crops centred on the lesion or on the entire scans. From using the Copy-Paste method in the evaluation, the latter is presumed but it is not explicitly mentioned. \n\nIn the cardiac MRI experiments, the LeFusion baseline of modelling the two lesion tissue types with separate models is mentioned as LeFusion in Table 2 but as LeFusion-S in Figure 5 and in the Appendix. It is suggested that the authors stick to one terminology. \nAs a work mainly focusing on specific diffusion model mechanics for improved lesion inpainting, it makes sense that the evaluation focus on comparing different diffusion based methods. That said, it would still be interesting to see how GAN based approaches like [4, 5] would fair in this comparison. \n \nReferences:  \n[1] Lugmayr, Andreas, et al. \"Repaint: Inpainting using denoising diffusion probabilistic models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. \n[2] Hansen, Colin, et al. \"Inpainting Pathology in Lumbar Spine MRI with Latent Diffusion.\" arXiv preprint arXiv:2406.02477 (2024). \n[3] Rouzrokh, Pouria, et al. \"Multitask brain tumor inpainting with diffusion models: A methodological report.\" arXiv preprint arXiv:2210.12113 (2022). \n[4] Yang, Jie, et al. \"Class-aware adversarial lung nodule synthesis in CT images.\" 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE, 2019. \n[5] Wu, Linshan, et al. \"FreeTumor: Advance Tumor Segmentation via Large-Scale Tumor Synthesis.\" arXiv preprint arXiv:2406.01264 (2024)\n\n### Questions\n\nFor specific questions please refer to the points made in weaknesses.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph:\nThis manuscript presents a novel 3D lesion inpainting method called LeFusion, utilizing diffusion models to combat data scarcity in medical imaging. The primary objective is to synthesize lesions in lung CT and cardiac MRI scans to enhance training datasets for lesion segmentation tasks. The study is supported by both visual quality assessments and improvements in segmentation performance derived from data augmentation. \n\n### Major Comments:\n1. **LeFusion Model**: The authors note that existing methods fail to preserve anatomically accurate backgrounds while inpainting lesions, introducing LeFusion to tackle this issue through:\n   - Training on a lesion-specific diffusion loss.\n   - Integrating background context during inference via RePaint.\n   This model yields improved realism in lesions and better retention of backgrounds compared to existing methods.\n   \n2. **Modality-Specific Variants**: The introduction of LeFusion-H and LeFusion-J effectively addresses challenges specific to lung CT and cardiac MRI, respectively, enhancing texture diversity and accommodating multiple tissue types.\n\n3. **DiffMask for Mask Generation**: DiffMask provides a flexible alternative for generating synthetic lesion masks; however, specificity on the selection of conditioning histograms for LeFusion-H is needed.\n\n### Minor Comments:\n- The manuscript could benefit from additional details on dataset preprocessing and volume resolution for reproducibility. \n- It would be interesting to explore the effect of crop size on outcomes, as well as to validate inpainted lesions with radiologist input.\n- Clarification on the definitions of \"significant\" results in Tables 1 and 2, as well as the role of the boundary mask in DiffMask, is warranted.\n\n### Summary Paragraph:\nIn relation to significance, the manuscript addresses a critical gap in lesion segmentation, contributing innovative methods that mitigate texture and shape variability. The evaluation is robust through comparisons with established methods across imaging modalities, enhancing reproducibility. Nonetheless, while the core contributions are well-supported, there are areas lacking clarity and context, particularly regarding novel claims and methodological specifics. \n\n### Decision Recommendation:\nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a novel 3D lesion inpainting framework based on diffusion models designed to mitigate data scarcity in medical imaging. The method aims to synthesize realistic lesions in lung CT and cardiac MRI scans for use in lesion segmentation data augmentation. The paper further introduces modality-specific extensions (LeFusion-H for CT and LeFusion-J for MRI) and an auxiliary *DiffMask* model for generating lesion masks from spatial constraints. The manuscript is clearly written, well contextualized in prior work, and supported by comprehensive experiments encompassing both qualitative and quantitative evaluations in two imaging modalities.\n\n---\n\n**Major Comments**  \n1. **Novelty of Lesion-Focused Loss:** The paper suggests that the lesion-focused diffusion loss is novel, though similar concepts have appeared in earlier works. The authors should acknowledge and discuss these related efforts to strengthen the contextual positioning of their contribution.  \n2. **Clarification of RePaint Implementation:** The description of the RePaint method is ambiguous regarding its integration with Cond-Diffusion. The authors should explicitly specify whether it uses the standard RePaint inference scheme or a modified variant.  \n3. **Conditioning Histogram in LeFusion-H:** Additional detail is needed on how histograms are selected for generating lesions in healthy scans (N′) when applying the LeFusion-H variant.  \n4. **DiffMask Explanation:** The role of the boundary mask and whether DiffMask uses image volume crops during inference require clarification. Section 3.3 should clearly define the inputs and processing pipeline.  \n5. **Training and Checkpointing:** Supplementary methodological details on training procedures and checkpoint selection for RePaint, Cond-Diffusion, and Cond-Diffusion(L) would enhance transparency.  \n6. **Experimental Protocols:** Explicitly clarify if segmentation models were trained on lesion-centered crops or entire scans, and justify the crop size selection with reference to prior work.  \n7. **Evaluation Robustness:** Since results lack standard deviations and appear to be from a single fold, claims of “significance” need better grounding. Performing cross-validation or providing variance measures would strengthen confidence in performance gains.  \n8. **Terminology Consistency:** In cardiac MRI results, maintain consistent naming for the baseline method (LeFusion vs. LeFusion-S).  \n9. **Comparative Scope:** Although the focus on diffusion models is logical, comparisons to GAN-based lesion synthesis methods (e.g., [4], [5]) would broaden the context and situate the proposed framework more fully.\n\n---\n\n**Minor Comments**  \n- Provide more details on dataset preprocessing, including volume resolution.  \n- Clarify the description of the boundary mask in Figure 4.  \n- Consider radiologist-based evaluation to assess realism and guard against generative bias.  \n- Ensure consistent format of Tables 1–2 and specify the meaning of “significant.”  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper addresses an important and timely problem—synthetic lesion generation for improving medical image segmentation—with a technically well-motivated diffusion-based approach. It demonstrates solid innovation through lesion-focused modeling and modality-specific adaptations, with convincing empirical validation showing improvements over baselines in both CT and MRI. The study’s strengths lie in clarity, methodological soundness, and robust two-modality evaluation. However, reproducibility and transparency could be improved with deeper clarifications on training protocols, dataset preprocessing, and evaluation significance. Overall, this work constitutes a meaningful advance in lesion inpainting methodology.\n\n---\n\n**Decision Recommendation:** **Accept (minor revision)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors introduce a latent diffusion model-based method for inserting lesions into healthy medical images while also providing an accompanying mask. They utilize a number of additions to their model to address limitations of prior work or naïve approaches to this task (both pre-existing and seemingly novel), such as combining forward-diffused backgrounds with reverse-diffused foregrounds, introducing intensity histogram-conditioning to the diffusion model to control lesion texture, as well as techniques for further control of the shape, size etc. of the generated lesion. They evaluate their method for a variety of experimental scenarios on 3D cardiac MRI lesion and CT lung nodule generation, showing that their technique results in noticeable improvements to existing approaches with respect to using their generated data to train downstream task segmentation models.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\nMajor\n1. The paper is polished, well-written and well-presented. Topics and concepts are organized and presented in a digestible fashion.\n2. Overall, decent technical novelty. This incorporates many techniques which all come together to result in a strongly-performing methods, some pre-existing (such as combined noised backgrounds with denoised foregrounds), and some seemingly novel (such as histogram-based textural control). Also, despite the many components, the approach still seems relatively watertight because these additions are all pretty lightweight/simple (a good thing). No requirement for an additional network or something of that sort.\n3. Overall, results are strong. Clear improvements over baseline methods is basically all cases, using reasonable metrics. They also study a range of training settings, which is good. Clear improvements over Cond-Diffusion, which would be the naïve approach that many would think of first trying for this task; the limitations of it as discussed in the introduction are clear from the experiments.\n4. They also have fairly extensive ablation studies for their method, which is important given the number of components that they propose using. There are still a few related questions that I have, but they are minor.\n5. In general, the evaluation is fair and appropriate. The datasets are challenging benchmarks, and I think two is sufficient given the wide range of experiments completed on them. There is also a good number of baseline models, especially considering that this task is relatively niche, so the methodological baselines that they compare to seem strong.\n\nMinor\n1. The motivation for this problem is clear: pathological subjects are indeed rare, especially for screening populations. Your survey of the limitations of existing lesion synthesis approaches also supports the motivation; for example, they result in low quality backgrounds, they lack precise control over generated lesions, etc.\n2. The use of a histogram representation to condition the model on may seem too reductive for some applications, but it seems to work well here (makes sense given the clear correspondence between histogram shape/number of peaks and generated lesion morphology shown in Fig. 3), supported by the clear improvement to your method that including the -H module produced.\n\n### Weaknesses\n\nMajor\n1. Some limitations of impact/scope: This task is clinically important but still fairly niche in medical image analysis, which itself is fairly niche within general machine learning and computer vision. The method (and task itself) also requires that dataset used needs the required annotations, which many medical datasets may not possess, and can be expensive/time-consuming to acquire. Overall, these limit the impact of the work somewhat, in the context of an ML conference at the level of ICLR, compared to a venue a bit more niche like MICCAI.\n\nMinor\n1. The benefits from using multi-channel decomposition (comparing the \"-J\" to no \"-J\" variants of your model in Table 2) are quite small. Can you provide some analysis or discussion of why this is the case, even if just hypothesizing? (However, I am guessing that the computational requirement to adding this component is practically negligible, so there is not really any harm in including it even if it results in only a very small performance improvement.)\n2. You state in the abstract that synthesizing multi-peak and multi-class lesions is a \"major challenge\" I agree with the multi-peak case given how much your histogram-conditioning improved the generation of such lesions, but based on your channel decomposition module's only very small improvements to performance, I'm unsure if generating multi-class lesions could not already be done well by prior methods. Could you clarify this/point to your results that support this, and/or provide quantitative evidence that multi-class synthesis is challenging for prior approaches?\n\nTo summarize, the paper is methodologically solid, with some technical novelty, and demonstrates clear improvements to prior techniques for lesion generation tasks in medical images via well-designed experiments and baselines. However, the main limitation is just that the task is relatively niche within medical image ML, which makes it more niche within general ML, and so may be less impactful at a venue like ICLR as opposed to a medical imaging-focused venue such as MICCAI or MIDL. Still, these limitations do not take away the good things about the paper (of which there are many), so I vote for a marginal accept.\n\n### Questions\n\n1. In the tables (e.g. table 1), what do you mean by the significantly adverse/positive effects denoted by red/blue? Could you please clarify this in the text as well via a small note in the table caption(s)?\n2. My suggestion: move image quality assessment quantitative results in the appendix (Table A2) to the main text if you have room. These are important metrics. You can shorten the related works to make space, that section doesn't need to be quite so extensive (or some of it could be moved to the supplementary).\n    - Also, why didn't you evaluate unpaired perceptual metrics like FID, KID (https://arxiv.org/abs/1801.01401), SWD (https://arxiv.org/abs/1710.10196) etc.? the first two may have limitations for this task given that they use pretrained natural image features, but despite this they are still commonly used metrics for generative medical image models. I would consider adding these for future work, and also explaining why they are not used (particularly for the wider ICLR audience).\n3. For the multiclass lesion case/-J model, did you study how performance/generation quality scales with adding more classes? This point may be a bit moot given how small the changes in performance were measured after adding the channel decomposition module to the base model, but I'm still curious.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel method based on latent diffusion models for inserting lesions into healthy medical images, alongside appropriate masks. The authors introduce several enhancements to their model to overcome the limitations of existing methodologies. Notable innovations include the integration of forward-diffused backgrounds with reverse-diffused foregrounds and an intensity histogram-conditioning technique for controlling lesion texture. The approach is evaluated across various experimental scenarios, specifically focusing on the generation of 3D cardiac MRI lesions and CT lung nodules. Overall, the authors demonstrate significant improvements over existing techniques when using their synthetic data for training segmentation models.\n\n**Major Comments:**  \n1. The manuscript is well-structured and clearly written, presenting topics and concepts in an accessible manner.\n2. The technical novelty is commendable; it successfully integrates a mix of existing and new techniques for a robust model without necessitating complex additional networks.\n3. Results show substantial improvements over baseline methods across various metrics, benefitting from a robust analysis of multiple training settings. The paper effectively highlights the limitations of naive approaches through experimental evidence.\n4. Comprehensive ablation studies lend credence to the proposed method, although some related questions remain minor in nature.\n5. The evaluation framework is appropriate, utilizing challenging benchmark datasets and including a solid range of baseline models, which enhances the findings' reliability.\n\n**Minor Comments:**  \n1. The study's motivation is evident, particularly regarding the scarcity of pathological subjects in screening populations. The authors highlight the limitations of previous synthesis approaches effectively.\n2. The histogram representation approach, while potentially reductive for some applications, demonstrates efficacy within this context, as evidenced by visual results in Figure 3.\n\n**Weaknesses:**  \n1. The clinical relevance of the task may be limited, as it targets a niche area within medical image analysis, which is itself a subset of broader machine learning and computer vision. Furthermore, the requirement for annotated datasets could pose challenges regarding accessibility.\n2. The performance benefits from multi-channel decomposition appear minimal. A discussion or analysis of this observation would enhance clarity, despite negligible computational costs of this addition.\n3. Clarification is needed regarding the complexity of synthesizing multi-class lesions as stated in the abstract, especially in light of the limited performance improvements observed with the channel decomposition module.\n\n**Summary Paragraph:**  \nIn summary, this manuscript is methodologically robust with a notable degree of technical innovation, showcasing clear advancements in lesion generation in medical imaging via well-conducted experiments and thorough baseline comparisons. However, its clinical impact is constrained by the niche nature of the task within medical imaging, which may affect its appeal to broader machine learning conferences such as ICLR. Despite these limitations, the strengths of the work are significant, warranting positive consideration.\n\n**Decision Recommendation:**  \nI recommend a minor revision for the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a diffusion model–based approach for realistic lesion insertion into healthy medical images, along with an accompanying lesion mask. The method integrates several architectural and conditioning improvements to overcome the limitations of previous lesion synthesis techniques, including a strategy combining forward-diffused backgrounds with reverse-diffused foregrounds, histogram-based intensity conditioning for texture control, and mechanisms to adjust lesion geometry. The work is evaluated on 3D cardiac MRI and CT lung nodule data, demonstrating consistent gains in generating data that enhance downstream segmentation performance. The paper is clearly written, technically sound, and methodologically coherent.\n\n**Major Comments**  \n1. The method exhibits solid technical novelty, combining known and novel elements (e.g., histogram conditioning) into a cohesive pipeline without added network complexity. The design remains efficient and conceptually clear.  \n2. Experimental results are strong, with consistent improvements across datasets and baselines, supported by appropriate metrics and well-conducted ablation studies.  \n3. The evaluation is comprehensive, covering two challenging benchmarks and multiple baselines, suitable for this task’s scope.  \n4. A main limitation lies in the work’s domain specificity. While lesion synthesis is clinically valuable, it represents a relatively narrow niche within medical imaging, limiting impact in broader ML venues. The method also depends on datasets with specific lesion annotations, which may restrict applicability.  \n5. Some clarifications would strengthen the paper’s presentation: specify the meaning of color-coded significance markers in tables, and explain minor benefits of the multi-channel decomposition (“–J” model) and the claim about difficulty of multi-class lesion generation.\n\n**Minor Comments**  \n- Consider moving quantitative image quality scores (Table A2) into the main text, potentially condensing the related works instead.  \n- Briefly justify the non-use of metrics such as FID, KID, or SWD, since readers from broader ML communities may expect them.  \n- Provide a short analysis of how multi-class lesion generation scales with added classes.  \n- Clarify table caption notes regarding red/blue significance indicators.  \n- The histogram-based control module appears effective; the discussion aligns well with visual evidence in Fig. 3.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant for medical image synthesis, presenting an effective diffusion-based lesion generation framework with practical control mechanisms. Innovation lies in the integration of histogram conditioning and the efficient combination of diffusion stages. Evaluation is thorough and demonstrates reproducible improvements across datasets and configurations. While the work’s impact is tempered by its niche application scope, methodological rigor and reproducibility are strong.\n\n**Decision Recommendation**  \n**Recommendation: Minor Revision (Marginal Accept).**  \nThe paper is technically solid and clearly presented, with convincing results. Minor clarifications and modest reorganization would improve accessibility and completeness for a broader readership.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper focuses on generating lesion-containing images from healthy images to address challenges in downstream segmentation tasks, such as real-world data scarcity and long-tail distribution issues. Previous research on medical image synthesis has primarily concentrated on lesion generation design, often overlooking high-fidelity background preservation. The authors propose a lesion-focused diffusion model, LeFusion, which maintains high-fidelity background by integrating the background from forward diffusion into the reverse diffusion process, thus simplifying the learning process and improving output control. Additionally, two effective strategies are introduced: histogram-based texture control and multi-channel decomposition to address the two main challenges in lesion texture synthesis: 1) multimodal and 2) multiclass lesions. The paper is well-written, with comprehensive experimental comparisons.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The overall paper structure is clear and well-expressed.\n2. A novel diffusion model is redesigned from the perspective of high-fidelity background preservation, with two texture generation control techniques developed to address multimodal and multiclass issues.\n3. The comparative methods are recent benchmarks from the past two years, making the results highly convincing.\n\n### Weaknesses\n\n1.There is a lack of detail on implementation specifics (such as the sampling process) and theoretical support for the method.\n2. Analysis and discussion on the continuity at the fusion boundaries between lesion and background are missing, as well as the impact on downstream tasks.\n\n### Questions\n\n1. The reverse diffusion sampling process is not clearly defined; it appears to rely solely on the transformation in Equation (1), without detailing the sampling process or providing theoretical justification for omitting it.\n2. Although the background from forward diffusion is used as the background in the reverse sampling process, and the loss constraint is applied only to the lesion area, how is continuity and smoothness ensured in the intersecting regions between the lesion and background?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript addresses the generation of lesion-containing images from healthy images, targeting key challenges in segmentation tasks such as data scarcity and long-tail distribution issues. It introduces a novel lesion-focused diffusion model, LeFusion, which integrates high-fidelity background preservation into the diffusion process to enhance output control. The authors further propose two strategies—histogram-based texture control and multi-channel decomposition—to tackle the main challenges in lesion texture synthesis, specifically concerning multimodal and multiclass lesions. The paper is well-structured and includes comprehensive experimental comparisons.\n\n**Major Comments:**  \n1. The paper demonstrates a clear and well-organized structure.\n2. It presents a novel diffusion model with an emphasis on high-fidelity background preservation, which is a fresh perspective in the field.\n3. The comparative analysis employs recent benchmarks from the last two years, supporting the credibility of the results.\n4. There is insufficient detail regarding implementation specifics, such as the sampling process, and a lack of theoretical justification for the method's approach.\n5. The discussion regarding continuity at the boundaries between the lesion and the background is inadequate, particularly concerning its implications for downstream tasks.\n\n**Minor Comments:**  \n- The reverse diffusion sampling process needs clearer definition; it seems to depend on Equation (1) without providing detailed descriptions or theoretical foundation for its omission.  \n- The manuscript does not sufficiently address how continuity and smoothness are maintained in the intersecting areas between the lesion and the background, especially given the loss constraint applied only to the lesion region.  \n\n**Summary Paragraph:**  \nThis review indicates that while the manuscript is significant due to its innovative approach in lesion generation and control techniques, it lacks depth in the evaluation of its methods and reproducibility due to missing implementation details. The strengths, particularly the clarity and recent benchmarking, are notable, but weaknesses regarding theoretical grounding and continuity analysis need addressing to enhance the paper's robustness.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of generating lesion-containing medical images from healthy ones to support downstream segmentation tasks, particularly in situations of limited real-world data and long-tail class distributions. The authors present *LeFusion*, a lesion-focused diffusion model that emphasizes high-fidelity background preservation. By integrating background information from forward diffusion into the reverse diffusion process, the approach simplifies learning and enhances controllability. Two additional strategies—histogram-based texture control and multi-channel decomposition—are implemented to manage multimodal and multiclass lesion texture synthesis. The manuscript is clearly written and provides thorough experimental comparisons.  \n\n**Major Comments**  \n1. **Methodological Detail:** The paper lacks sufficient explanation of implementation details, including the precise formulation of the reverse diffusion sampling process. The method appears to depend primarily on a transformation defined in Equation (1), but theoretical justification or description of the sampling steps is not provided.  \n2. **Theoretical Support:** There is limited discussion supporting the theoretical rationale of the approach. Greater elaboration on how background integration mathematically contributes to fidelity and controllability would strengthen the work.  \n3. **Boundary Continuity:** The continuity and smoothness at the interface between generated lesions and preserved background are not analyzed. The absence of discussion or experimental evidence regarding fusion boundaries raises questions about visual consistency and downstream task performance.  \n4. **Impact on Downstream Tasks:** While lesion generation quality is emphasized, the influence of boundary artifacts or synthesis quality on segmentation outcomes is not deeply discussed.  \n\n**Minor Comments**  \n- Implementation specifics such as sampling settings and parameter values should be clarified for reproducibility.  \n- Minor clarifications on figures and equations would improve comprehensibility.  \n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The work contributes a novel approach to lesion synthesis with potential benefit for medical image segmentation.  \n*Innovation:* The concept of background-preserving diffusion and texture control is original and well-motivated.  \n*Evaluation:* Experiments include recent benchmark comparisons, but discussion of boundary artifacts and downstream implications remains limited.  \n*Reproducibility:* External reproducibility is hindered by incomplete methodological detail and insufficient theoretical clarity.  \n\n**Decision Recommendation:** **Major Revision** – The core idea is promising and experimentally well-supported, but improved methodological transparency and a clearer theoretical foundation are needed to ensure scientific rigor and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents LeFusion, a lesion-focused diffusion model for generating synthetic medical images containing pathological lesions from lesion-free images. The method addresses data scarcity and long-tail imbalances in medical AI by redesigning diffusion objectives to focus solely on lesion regions while preserving high-fidelity backgrounds through forward-diffused background integration (Section 3.1, Figure 2). Key contributions include histogram-based texture control for multi-peak lesions, multi-channel decomposition for multi-class lesions, and lesion mask diffusion (DiffMask) for controlling lesion size, location, and boundary (Section 3.2-3.3). Validation on LIDC lung nodule CT and Emidec cardiac MRI datasets demonstrates improvements in downstream segmentation tasks using nnUNet and SwinUNETR, with Dice score improvements up to 5.18% for lung nodules and significant enhancements for cardiac lesion segmentation (Tables 1-2).\n\n## Weaknesses\n\n• **Mathematical formulation lacks rigor and clarity**\n  - Equation (4) uses element-wise multiplication notation Mf ∥ϵ − pθ (ˆxt, t) ∥² without clearly defining whether this applies the mask before or after computing the L2 norm, creating ambiguity in the loss computation\n  - The multi-channel decomposition objective in Equation (6) sums losses across channels but doesn't specify how gradients flow between channels or whether shared parameters exist, limiting reproducibility\n  - The histogram control mechanism in Equation (5) introduces h as a condition but provides no mathematical definition of how histograms are computed, normalized, or integrated into the cross-attention mechanism\n\n• **Experimental evaluation contains significant methodological gaps**\n  - Table 1 shows baseline comparisons but lacks statistical significance testing or confidence intervals across the 520 test ROIs, making it unclear whether reported improvements are statistically meaningful\n  - The histogram control analysis (Figure 6, Appendix C) uses only 100 subsets and PSNR/SSIM metrics that may not capture clinically relevant lesion characteristics, limiting the validity of diversity claims  \n  - Table 2 reports results on only 10 test cases for cardiac lesions, which is insufficient for robust evaluation given the complexity of multi-class lesion generation and the reported performance variations\n\n• **Technical approach lacks theoretical justification for key design choices**\n  - The lesion-focused loss design (Equation 4) restricts learning to lesion regions but provides no theoretical analysis of how this affects the model's ability to maintain realistic lesion-background transitions at mask boundaries\n  - The multi-channel decomposition strategy assumes lesion classes can be modeled independently then combined, but offers no justification for why this preserves inter-class spatial relationships observed in real pathology\n  - The recurrent mechanism for inpainting (Appendix F) introduces additional hyperparameters (recurrent length, sampling frequency) without principled selection criteria or sensitivity analysis\n\n## Suggestions for Improvement\n\n• **Enhance mathematical rigor and notation clarity**\n  - Rewrite Equation (4) to explicitly specify whether the mask is applied element-wise to the difference before norm computation: ∥Mf ⊙ (ϵ − pθ (ˆxt, t)) ∥², and provide dimensional analysis to ensure consistency\n  - Add detailed mathematical specifications for the multi-channel training objective including parameter sharing schemes, gradient flow mechanisms, and convergence properties across channels\n  - Define the histogram computation, normalization, and cross-attention integration procedures with explicit equations showing how histogram features h are embedded and conditioned in the diffusion process\n\n• **Strengthen experimental validation and statistical analysis**  \n  - Conduct statistical significance testing (e.g., paired t-tests, Wilcoxon signed-rank tests) across all reported metrics with appropriate multiple comparison corrections, and report confidence intervals for all performance improvements\n  - Expand the histogram control evaluation to include clinically relevant metrics beyond PSNR/SSIM, such as radiologist assessment scores or lesion characteristic preservation measures, using larger sample sizes (≥500 cases)\n  - Increase the cardiac lesion test set size through cross-validation or additional data acquisition to ensure robust evaluation, and provide per-class confusion matrices to analyze failure modes\n\n• **Provide theoretical foundations and principled design justification**\n  - Derive theoretical bounds on reconstruction error at lesion-background boundaries when using lesion-focused loss, and empirically validate boundary quality using edge-preserving metrics or radiologist evaluation\n  - Justify the multi-channel decomposition approach through ablation studies comparing joint vs. independent modeling, and analyze how spatial correlations between lesion classes are preserved during synthesis\n  - Establish principled selection criteria for recurrent mechanism hyperparameters through systematic ablation studies, and provide computational complexity analysis comparing different recurrent configurations",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces LeFusion, a lesion-focused diffusion model designed to generate synthetic medical images containing pathological lesions from lesion-free images. By addressing data scarcity and long-tail imbalances in medical AI, the method redefines diffusion objectives to prioritize lesion regions while maintaining high-fidelity backgrounds through forward-diffused background integration. Notable contributions include advanced texture control for multi-peak lesions, multi-channel decomposition for multi-class lesions, and the novel lesion mask diffusion (DiffMask), which allows for precise control over lesion size, location, and boundary. Validation via LIDC lung nodule CT and Emidec cardiac MRI datasets indicates enhancements in downstream segmentation tasks, with reported Dice score improvements of up to 5.18% in lung nodules and marked gains in cardiac lesion segmentation.\n\n**Major Comments:**  \n1. **Mathematical formulation lacks rigor and clarity:**\n   - Equation (4) is ambiguous regarding the application of the mask relative to the L2 norm computation.\n   - Equation (6) lacks detail on gradient flow and parameter sharing in the multi-channel decomposition objective.\n   - The histogram control mechanism in Equation (5) is unspecified in terms of computation and integration.\n\n2. **Experimental evaluation contains significant methodological gaps:**\n   - Table 1 lacks statistical significance testing or confidence intervals, raising concerns about the validity of reported improvements.\n   - The histogram control analysis employs insufficient subsets and metrics that may not adequately represent clinical relevance.\n   - Table 2's limited cardiac lesion test cases compromise robust evaluation.\n\n3. **Technical approach lacks theoretical justification:**\n   - The lesion-focused loss design is unaccompanied by a theoretical analysis of its impact on lesion-background transitions.\n   - Independence in multi-channel decomposition is not justified in terms of inter-class spatial relationships.\n   - The recurrent inpainting mechanism requires principled selection criteria for hyperparameters without adequate analysis.\n\n**Minor Comments:**  \n- Improve the clarity of Equation (4) by explicitly detailing mask application.  \n- Provide a comprehensive explanation of multi-channel training objectives, including parameter sharing and gradient flow.  \n- Define procedures related to histogram computation explicitly in the manuscript.  \n\n**Summary Paragraph:**  \nThe study presents a significant contribution to the fields of medical imaging and AI by proposing an innovative approach to generating synthetic pathological images. However, there are major deficiencies in the mathematical rigor and experimental validation, including clarity of key equations and the adequacy of sample sizes in evaluations. The lack of theoretical justification for design choices and hyperparameter selections diminishes the reproducibility and significance of the findings. Thus, while the manuscript showcases innovative potential, substantial improvements in methodology, statistical analysis, and theoretical support are necessary to meet TMI editorial criteria of significance, innovation, evaluation, and reproducibility.\n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **LeFusion**, a lesion-focused diffusion model designed to synthesize medical images containing pathological lesions from healthy images. The objective is to mitigate data scarcity and class imbalance in medical AI training by localizing diffusion objectives to lesion regions while maintaining realistic background structures through forward-diffused background integration. The approach includes histogram-based texture control, multi-channel lesion decomposition, and lesion mask diffusion mechanisms. The method is evaluated on LIDC lung nodule CT and Emidec cardiac MRI datasets, showing improvements in segmentation performance. Overall, the paper presents a technically ambitious framework with clear relevance, but certain aspects of the mathematical formulation, evaluation design, and theoretical grounding require refinement to ensure rigor and reproducibility.  \n\n**Major Comments**  \n1. **Mathematical Rigor and Clarity** – Several equations lack precise definitions. Equation (4) is ambiguous regarding the application of lesion masks within the loss function. Equation (5) introduces histogram conditioning without specifying how histograms are computed or normalized. Equation (6) omits details on gradient flow and parameter sharing between channels, reducing transparency and reproducibility.  \n2. **Experimental Evaluation Gaps** – Reported results (Tables 1–2) lack statistical significance tests or confidence intervals, casting doubt on the robustness of claimed improvements. The histogram control analysis relies on a small subset (100 samples) and PSNR/SSIM metrics that may not capture clinically relevant lesion diversity. The cardiac lesion evaluation uses only 10 test cases, insufficient for reliable quantitative assessment.  \n3. **Insufficient Theoretical Justification** – The lesion-focused loss does not analyze its effect on lesion-background boundaries. The multi-channel decomposition assumes independent modeling of lesion classes without justification for preserving spatial relationships. The recurrent inpainting mechanism introduces several hyperparameters but lacks a principled selection rationale or sensitivity study.  \n\n**Minor Comments**  \n- Clarify ambiguous notation in equations and ensure all symbols are defined.  \n- Explicitly link figures (e.g., Figure 2, Figure 6) to corresponding methodological descriptions for readability.  \n- Include dimensional consistency checks in equations for reproducibility.  \n\n**Summary Paragraph (TMI Editorial Criteria)**  \nThe submission addresses a **significant** problem—data imbalance and lesion realism in synthetic medical imaging—but its **innovation** requires stronger theoretical substantiation, particularly in loss design and decomposition strategy. The **evaluation** demonstrates promising performance gains but falls short on statistical robustness and dataset scale. Regarding **reproducibility**, incomplete definitions and unclear gradient flows reduce transparency. Strengthening mathematical rigor, providing thorough statistical analysis, and offering theoretical justification would substantially improve the paper’s scientific soundness.  \n\n**Decision Recommendation**  \n**Major Revision.** Strengthen the mathematical formulation, expand statistical evaluation, and add theoretical justification to support the methodological design.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n   ✅ The manuscript introduces a new generative‑modeling framework—**LeFusion**, a lesion‑focused 3‑D diffusion architecture for controllable pathology synthesis in CT and MRI.  It centers on imaging‐methodology rather than pure application: redesign of diffusion objectives, histogram‑based texture control, multi‑channel generative decomposition, and mask diffusion for 3‑D data augmentation.  These are methodological advances directly pertinent to medical‑image generation, image reconstruction, and quantitative data modeling—well within *IEEE TMI* scope.\n\n2. **Novelty & Contribution Level**  \n   Moderate‑to‑High.  The lesion‑focused diffusion objective and the explicit disentanglement of foreground lesion and background represent conceptual novelty relative to prior inpainting‑based or conditional‑diffusion works.  Histogram control and multi‑channel decomposition are incremental refinements but nicely engineered.  The work advances lesion synthesis by improving control and fidelity in 3‑D settings—above typical “apply Stable‑Diffusion” papers.  Novelty is methodological rather than theoretical.\n\n3. **Technical and Experimental Rigor**  \n   The technical exposition is sound and equations consistent.  Experiments are broad (lung CT, cardiac MRI) with multiple baselines (GAN, conditional diffusion, RePaint).  Quantitative comparison uses Dice, NSD, and perceptual indices, and code availability is declared.  However:  \n   * No statistical significance testing for segmentation improvement.  \n   * Limited ablation on diffusion parameters vs. output fidelity.  \n   * Generalization across modalities tested on small 3‑D volumes—unclear robustness to other anatomies.  \n   * “Histogram‑control” and “DiffMask” parameters partially heuristic; unlike a theoretical treatment, they rely on empirical tuning.  \n   * The reproducibility statement is good, but details of dataset licensing and preprocessing pipelines should be aligned with TMI reproducibility standards.\n\n4. **Clarity and Presentation**  \n   The paper is clearly structured, with figures clarifying architectural differences and generation results.  Some long paragraphs and typographic issues (perhaps from LaTeX extraction) obscure readability.  Editing for flow and conciseness would help a TMI audience less familiar with diffusion‑model notation.\n\n5. **Ethical and Reproducibility Compliance**  \n   Public datasets (LIDC‑IDRI, Emidec) are used; no private clinical data.  Ethics statement acknowledges potential misuse.  Code release promised.  Compliance appears adequate.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n**1. Summary**  \nThe manuscript presents *LeFusion*, a lesion‑focused diffusion framework for medical‑image synthesis.  By restricting diffusion learning to masked lesion regions while incorporating forward‑diffused background context, the model preserves anatomical backgrounds and allows controllable insertion of synthetic pathologies.  It further introduces (i) histogram‑based texture conditioning for multi‑peak lesions, (ii) multi‑channel decomposition for multi‑class lesion generation, and (iii) a mask‑diffusion module (*DiffMask*) to generate diverse lesion shapes.  Experiments on LIDC lung CT and Emidec cardiac MRI show improved lesion‑segmentation accuracy (up to ≈5 % Dice increase) for nnUNet and Swin‑UNETR trained with synthetic data.\n\n**2. Strengths**\n- Clear methodological contribution focused on core imaging science (diffusion‑based data generation).  \n- Addresses a significant bottleneck—data scarcity and imbalance—for supervised imaging AI.  \n- Demonstrates technical rigor in comparing multiple generative baselines on 3‑D data.  \n- Well‑documented, code to be released, and quantitative downstream validation.  \n- Integrates controllability (texture, size, class) seldom seen in prior medical‑diffusion works.\n\n**3. Weaknesses**\n- Limited theoretical justification and ablation to isolate each proposed component’s gain.  \n- Validation is confined to two public datasets; lacks broader clinical or multi‑institutional generalization.  \n- No statistical significance or uncertainty analysis on performance gains.  \n- Some implementation details (variance schedule, hyper‑parameter sensitivity) are buried in appendix and could be summarized for reproducibility.  \n- Writing could be condensed; figures occasionally over‑crowded for print format.\n\n**4. Major Comments**\n1. **Quantitative validation** – Provide statistical testing (e.g., paired t‑test or Wilcoxon) for Dice/NSD improvements to confirm significance.  \n2. **Ablation depth** – Add explicit ablations isolating lesion‑focused loss, histogram conditioning, multi‑channel decomposition, and DiffMask.  \n3. **Generalizability** – Discuss adaptation to other modalities (e.g., brain MRI, ultrasound) and computational scalability to larger 3‑D volumes.  \n4. **Reproducibility alignment** – Clarify detailed preprocessing pipeline, normalization steps, and diffusion schedule so an independent group can replicate results.  \n5. **Clinical interpretability** – Include expert visual evaluation or realism scoring from radiologists if feasible; this would strengthen claims of “realistic” lesion synthesis.  \n\n**5. Minor Comments**\n- Edit for grammar and consistent notation (e.g., spaces around equations, lowercase Greek letters).  \n- Figures 1–5 could use higher‑contrast overlays for readability in grayscale print.  \n- Ensure references to concurrent 2024‑24 works are up‑to‑date and formatted per IEEE TMI style.  \n- Clarify acronyms (NSD, PSNR, SSIM) at first mention.  \n- Consider shortening introduction paragraphs on data scarcity and fairness—they duplicate known background.\n\n**6. Overall Recommendation**  \n**Major Revision** – The paper presents a promising, methodologically solid diffusion‑based framework with substantial potential for *TMI*.  To reach publication quality, the authors should strengthen experimental analysis, clarify component contributions, and ensure reproducibility and clinical realism are thoroughly demonstrated.\n\n**7. Confidence Level:** 4 / 5 – High confidence in evaluation; subject‑matter (medical image synthesis & diffusion modeling) is within reviewer expertise.\n\n---\n\n*In summary:* *LeFusion* is a creative and technically competent advancement in controllable lesion synthesis.  With deeper analysis and improved presentation, it could become a valuable contribution to the methodological literature of medical‑image generation and augmentation in *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *LeFusion*, a lesion‑focused 3D diffusion framework for controllable pathology synthesis in CT and MRI. The approach redesigns diffusion objectives to confine learning to lesion regions while preserving background anatomy and introduces histogram‑based texture control, multi‑channel generative decomposition, and a mask‑diffusion module (*DiffMask*). The work targets methodological advancement in medical image generation with applications to data augmentation and reconstruction. Overall the paper is clearly written, technically competent, and relevant to *IEEE Transactions on Medical Imaging* (TMI), though several analyses and clarifications are needed to fully meet TMI standards.\n\n---\n\n**Major Comments**  \n1. **Quantitative validation:** Statistical significance or uncertainty analysis (e.g., paired t‑test, Wilcoxon) should be provided to confirm reported Dice/NSD improvements in segmentation performance.  \n2. **Ablation depth:** The contribution of each module—lesion‑focused loss, histogram control, multi‑channel decomposition, and DiffMask—should be explicitly isolated through ablation experiments.  \n3. **Generalizability:** Validation is limited to LIDC lung CT and Emidec cardiac MRI. Discussion of scalability to larger 3D volumes and adaptation to other anatomies or modalities (e.g., brain MRI, ultrasound) would broaden impact.  \n4. **Reproducibility alignment:** While code release is promised, more explicit details are needed on preprocessing, normalization, diffusion scheduling, and dataset licensing to meet TMI reproducibility standards.  \n5. **Clinical interpretability:** Incorporating expert visual assessment or realism scoring by radiologists could substantiate the claim of realistic lesion generation.  \n6. **Presentation and clarity:** Some sections are lengthy and figures crowded; editing for conciseness and visual clarity would improve readability for a broad TMI audience.\n\n---\n\n**Minor Comments**  \n- Check grammar and consistent notation (use lowercase Greek letters, spacing around equations).  \n- Enhance figure contrast to ensure readability in grayscale printing.  \n- Define acronyms (NSD, PSNR, SSIM) at first use and update references to recent 2024 works per IEEE style.  \n- Shorten introductory material on data scarcity to avoid redundancy.  \n- Correct minor typographic artifacts likely from LaTeX extraction.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \n*LeFusion* addresses a significant challenge in medical imaging—data scarcity and lesion imbalance—through a controllable diffusion‑based synthesis framework. Its methodological innovation lies in lesion‑focused diffusion with auxiliary conditioning mechanisms that improve realism and control. Evaluation is comprehensive but limited to two datasets without statistical confirmation or detailed ablation; broader and deeper assessment would strengthen evidence. Reproducibility is promising given the declared code release yet requires clearer procedural documentation. Overall, the study is significant and innovative but needs more rigorous validation and refined presentation to reach TMI publication standards.\n\n---\n\n**Decision Recommendation:** **Major Revision** – The work is promising and well aligned with TMI scope, but requires additional experimental analysis, transparency, and editorial refinements before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces *LeFusion*, a novel generative modeling framework designed for lesion-focused 3-D diffusion, aimed at enabling controllable pathology synthesis in CT and MRI images. The work emphasizes advancements in imaging methodology, particularly in diffusion objectives, multi-channel generative decomposition, and texture control, making significant contributions in the realms of medical image generation and quantitative data modeling. The manuscript is well-structured, although improvements in writing clarity could enhance its accessibility for the *IEEE TMI* readership.\n\n**Major Comments:**  \n1. **Quantitative Validation**: It is necessary to provide statistical testing (e.g., paired t-test or Wilcoxon) for Dice and NSD improvements to confirm significance.  \n2. **Ablation Depth**: Conduct explicit ablations to isolate the contributions of the lesion-focused loss, histogram conditioning, multi-channel decomposition, and the DiffMask module.  \n3. **Generalizability**: Discuss potential adaptations of the framework to other modalities such as brain MRI and ultrasound, along with considerations for scaling to larger 3-D volumes.  \n4. **Reproducibility Alignment**: Clarify preprocessing details, normalization steps, and diffusion parameters to ensure independent replicability.  \n5. **Clinical Interpretability**: If possible, include evaluations from radiologists regarding the realism of synthesized lesions to strengthen claims about the quality of outputs.  \n\n**Minor Comments:**  \n- Edit for grammar and ensure consistent notation, including proper spacing around equations and correct casing for Greek letters.  \n- Enhance Figures 1–5 with high-contrast overlays for better visibility in grayscale print.  \n- Confirm that references to contemporary works from 2024 are current and formatted in accordance with IEEE TMI style.  \n- Clarify acronyms (NSD, PSNR, SSIM) upon first mention for better comprehension.  \n- Consider condensing introductory paragraphs discussing data scarcity and fairness to avoid duplication of known background.  \n\n**Summary Paragraph:**  \nWhile *LeFusion* presents a promising approach to controllable lesion synthesis and showcases strong methodological contributions, it requires further development in terms of experimental validation, theoretical justification, and clarity in presentation. The work is innovative and tackles significant limitations in data scarcity and imbalance in supervised imaging AI. Nevertheless, the evaluation lacks broader generalization and reproducibility considerations, which are essential criteria for publication in *IEEE TMI*.\n\n**Decision Recommendation:**  \n**Major Revision** – The manuscript demonstrates a solid methodological foundation but needs improvements in experimental rigor, clarity, and alignment with reproducibility standards for successful publication. \n\n**Confidence Level:** 4 / 5 – I have high confidence in this evaluation, given my expertise in medical image synthesis and diffusion modeling. \n\nIn summary, *LeFusion* stands as a technically competent advancement with substantial potential impact on medical-image generation and augmentation robust enough for *IEEE Transactions on Medical Imaging*.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Hantao Zhang",
      "Jiancheng Yang",
      "Pascal Fua",
      "Shouhong Wan",
      "Wei Peng",
      "Xinyuan Wang",
      "Yuhe Liu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_c4c499d3163801b947e70b673f9eadca4be34844.pdf",
    "remote_url": "https://openreview.net/pdf/c4c499d3163801b947e70b673f9eadca4be34844.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Medical Image Segmentation",
      "Vision-Language Pre-Training",
      "Zero-Shot Segmentation"
    ],
    "abstract": "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces Malenia, a framework for zero-shot lesion segmentation in 3D medical images, aimed at addressing the challenge of transferring image-level knowledge to pixel-level segmentation tasks. Building on advancements in medical vision-language pre-training, Malenia integrates a multi-scale mask-attribute alignment framework and a Cross-Modal Knowledge Injection module to link visual and textual features, enabling the model to handle previously unseen lesions. The authors evaluate Malenia's performance through experiments on three datasets (two public and one private) across 12 lesion categories.\n\n### Soundness: 2\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\n1. The topic on zero-shot lesion segmentation is valuable, as clinical settings often involve diverse, emerging anomalies, and data collection is challenging, increasing the need for models that handle unseen diseases in an open-set context.\n2. The authors strengthen their claims with both qualitative and quantitative results, enhancing the credibility of their findings and providing a well-rounded evaluation of the proposed approach.\n3. The paper is well-organized, with a clear and logical structure that makes complex concepts accessible and easy to follow for readers.\n\n### Weaknesses\n\n1. The Zero-Shot Inference section lacks sufficient detail on how the model handles test CT images containing unseen tumor types. For instance, if a test image includes both a kidney tumor and a gallbladder tumor, it is unclear how the model predicts these different tumors within the same image. Additionally, the phrase “we obtain the class information for each predicted lesion mask by referencing a clinical knowledge table” is ambiguous. Further clarification is needed on what this clinical knowledge table entails and how it is used in the inference process.\n2. The comparisons in Table 1 and Table 2 may not be entirely fair. The authors introduce additional ATTRIBUTE DESCRIPTIONS annotations, which provide extra information not available to baseline methods like TransUNet, nnUNet, and Swin UNETR. These baselines rely solely on image data, while the proposed approach leverages attribute annotations, giving it an advantage in performance comparisons.\n3. The fully-supervised performance of baseline methods reported in Table 2 appears unusually low. For instance, in the official nnUNet paper, liver tumor segmentation achieved a Dice score of 76 on the test set, whereas the presented result here is only 61.33. Similarly, lung tumor segmentation originally reported a Dice score of 74, but this paper reports 54.44. This discrepancy raises concerns about the reproducibility and fairness of the comparisons.\n4. In Table 7, the authors compare their method to existing vision-language pretraining strategies, but it is unclear how these methods were reproduced. Most vision-language pretraining approaches require access to diagnostic reports, which are not included in the public MSD and KiTS23 datasets. It would be helpful to understand how the authors conducted these comparisons without complete diagnostic reports. Additionally, the absence of comparisons with important pretraining models like CT-CLIP weakens the evaluation.\n5. Scalability of the proposed method is questionable due to its reliance on ATTRIBUTE DESCRIPTIONS. For practical clinical applications, this approach would require physicians to provide additional lesion descriptions across eight visual attribute aspects, which may not be feasible in all settings. This dependency could limit the method’s applicability in diverse clinical environments.\n\n### Questions\n\nplease refer to Weaknesses\n\n### Flag For Ethics Review\n\n- Yes, Privacy, security and safety\n\n### Rating: 5\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nThis paper utilizes private clinical data, which may require an ethics review to ensure compliance with privacy, security, and safety standards.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents Malenia, a novel framework for zero-shot lesion segmentation in 3D medical images, addressing the complexities of transferring image-level knowledge to pixel-level tasks. By leveraging advancements in medical vision-language pre-training, Malenia incorporates a multi-scale mask-attribute alignment framework and a Cross-Modal Knowledge Injection module to connect visual and textual features, thus enabling effective segmentation of previously unseen lesions. The authors systematically evaluate Malenia's performance on three datasets, comprising two public and one private, across twelve lesion categories.\n\n### Major Comments\n1. The Zero-Shot Inference section lacks detailed explanations on how the model manages test CT images with different unseen tumor types. For example, the method for segmenting both a kidney and gallbladder tumor in a single image is unclear. Additionally, the reference to a \"clinical knowledge table\" needs further elaboration to clarify its contents and its role in the inference process.\n2. The evaluations in Tables 1 and 2 may not be equitable. The introduction of additional ATTRIBUTE DESCRIPTIONS provides extra insight that baseline methods such as TransUNet, nnUNet, and Swin UNETR lack, as they rely solely on image data.\n3. The reported fully-supervised performance of baseline methods in Table 2 appears to be lower than expected based on previous literature, which raises concerns regarding reproducibility and fairness in these comparisons. For instance, the nnUNet reported a Dice score of 76 for liver tumor segmentation, while this manuscript indicates only 61.33.\n4. In Table 7, comparisons to existing vision-language pretraining strategies are presented without clear information on how these methods were reproduced, especially considering that most require access to diagnostic reports not available in public datasets. Additionally, not comparing against key models like CT-CLIP weakens the evaluation.\n5. The scalability of the method is questionable, given its reliance on ATTRIBUTE DESCRIPTIONS. Requiring physicians to provide detailed lesion descriptions may not be practical in all clinical settings.\n\n### Minor Comments\n- Clarification is needed regarding the terminology and acronyms utilized throughout the paper.\n- Consider proofreading for typographical errors to enhance overall quality.\n\n### Summary Paragraph\nIn summary, while Malenia offers significant innovation in zero-shot lesion segmentation and presents credible evaluations, there are notable concerns related to reproducibility and fairness in comparisons. The reliance on additional annotations, as well as potential challenges in practical scalability and the need for further clarity in technical explanations, may hinder its applicability. Addressing these critiques related to significance, innovation, evaluation, and reproducibility could strengthen the manuscript considerably.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a framework for zero-shot lesion segmentation in 3D medical images. The method aims to bridge the gap between image-level and pixel-level knowledge transfer by integrating a multi-scale mask–attribute alignment framework and a Cross-Modal Knowledge Injection module that align visual and textual representations. Experiments spanning three datasets (two public, one private) and twelve lesion categories support the evaluation. Overall, the topic is timely and relevant for open-set medical imaging; the paper is clearly organized and easy to follow.\n\n**Major Comments**  \n1. **Zero-shot inference details:** The description of how the model processes test CT images containing multiple unseen tumor types is insufficient. It remains unclear how predictions are generated when several lesion classes (e.g., kidney and gallbladder tumors) appear together. The statement about obtaining “class information … by referencing a clinical knowledge table” lacks clarity—further explanation of the table’s contents and role in inference is required.  \n2. **Fairness of comparisons:** In Tables 1–2, the method benefits from additional ATTRIBUTE DESCRIPTIONS annotations not accessible to baseline models (TransUNet, nnUNet, Swin UNETR). This added information may bias results, compromising fairness of comparisons.  \n3. **Baseline performance discrepancies:** Reported Dice scores for supervised baselines appear lower than expected (e.g., nnUNet liver tumor Dice = 61 vs 76 in the original paper; lung tumor = 54 vs 74). These inconsistencies raise concerns about reproducibility and evaluation fairness.  \n4. **Vision–language pretraining comparisons:** Table 7 includes comparisons to other vision–language pretraining methods, but reproducibility is uncertain because those methods usually rely on diagnostic reports absent in the public datasets used. Clarification is needed on how such comparisons were performed, and the omission of relevant approaches (e.g., CT‑CLIP) weakens completeness.  \n5. **Scalability and practicality:** Dependence on ATTRIBUTE DESCRIPTIONS, requiring physicians to specify multiple lesion attributes, may limit clinical feasibility and scalability across diverse settings.\n\n**Minor Comments**  \n- None beyond clarification issues noted above.  \n- Ensure all tables clearly specify data sources and annotation types.\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the topic is important for handling unseen clinical anomalies. Regarding **innovation**, the integration of visual–text alignment and cross-modal knowledge components is conceptually sound but may depend heavily on manual attribute annotations. The **evaluation** presents quantitative and qualitative results but lacks clarity on fairness and reproducibility. Concerning **reproducibility**, baseline discrepancies and limited description of pretraining comparisons reduce confidence in the reported performance.\n\n**Decision Recommendation**  \n**Major Revision.** The paper is promising and well presented but requires clarifications on zero-shot inference design, fair comparison methodology, and reproducibility to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces Malenia, a novel framework specifically designed for 3D zero-shot lesion segmentation in medical images. The authors aim to address the challenges of transferring image-level knowledge to pixel-level tasks—such as lesion segmentation—by proposing a multi-scale lesion-level mask-attribute alignment approach. Malenia improves the alignment between the visual features of unseen lesions with textual representations, leveraging Cross-Modal Knowledge Injection (CMKI) to enhance both visual and textual embeddings. Experimental results across three datasets demonstrate that Malenia outperforms state-of-the-art (SOTA) methods in zero-shot lesion segmentation.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed Malenia framework is novel, particularly in its use of multi-scale mask-attribute alignment for 3D lesion segmentation. This is a meaningful extension of zero-shot segmentation methods to the medical domain, where unseen lesion categories are common (e.g., \"hepatocellular carcinoma,\" mentioned in Sec. 1).\n2. The CMKI module is a strong contribution because it combines visual and textual embeddings, enriching the segmentation prediction with complementary information from both modalities. This is a novel approach that enhances the zero-shot capability of the model.\n3. The paper introduces a multi-positive contrastive loss to fine-tune the alignment between lesion masks and textual attributes. This approach allows the model to generalize better to unseen lesions by learning from shared visual attributes, which distinguishes it from prior works like SAM (Kirillov et al., 2023).\n\n### Weaknesses\n\n1. The method relies on transforming patient reports into structured descriptions of eight visual attributes, which requires expert radiologist involvement. This process may not be scalable or practical in real-world applications where such annotations are not readily available.\n2. As noted in Appendix D, the model performs less effectively on attributes requiring complex visual semantic understanding, such as \"Surface Characteristics\" and \"Specific Features\". This suggests a potential limitation in capturing intricate visual patterns.\n3. While the model shows strong performance on 12 lesion categories, the diversity of lesions in clinical practice is vast. The ability of Malenia to generalize to a wider range of unseen lesions without additional training remains uncertain.\n\n### Questions\n\n1. In Section 3.1, the authors employ the Multi-Positive InfoNCE (MP-NCE) loss for aligning mask embeddings with multiple attribute embeddings. Could the authors provide more details on how the MP-NCE loss is computed and how it facilitates learning with multiple positive pairs? Specifically, how are the positive and negative pairs defined in the context of multiple attributes?\n   Could the authors explain how this formulation effectively handles multiple positive pairs for each mask token?\n2. The model uses a fixed number of mask tokens ($N=16$) and defines $R=8$ attribute aspects. Have the authors conducted any experiments to assess how varying $N$ or the number of attributes affects the model's performance? An ablation study on these hyperparameters could provide insights into the model's sensitivity and optimal settings.\n3. As noted in Appendix D, the model performs less well on attributes like \"Surface Characteristics\" and \"Specific Features\". What are the potential reasons for this lower performance? Could incorporating additional contextual information or advanced features help the model better capture complex visual semantics?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents Malenia, a novel framework aimed at advancing 3D zero-shot lesion segmentation in medical imaging. It addresses the challenge of transferring image-level knowledge to pixel-level tasks through a multi-scale mask-attribute alignment approach. The authors introduce a Cross-Modal Knowledge Injection (CMKI) mechanism to enhance the congruence between visual features of unseen lesions and their textual representations. Experimental results from three datasets indicate that Malenia outperforms existing state-of-the-art methods in zero-shot lesion segmentation.\n\n### Major Comments\n1. **Novelty**: The framework introduces a unique multi-scale mask-attribute alignment for 3D lesion segmentation, marking a significant addition to zero-shot segmentation methods in the medical field, particularly for common unseen lesion types (e.g., \"hepatocellular carcinoma\").\n2. **CMKI Contribution**: The integration of visual and textual embeddings via the CMKI module is a strong contribution that augments the segmentation prediction with complementary data, enhancing the model's zero-shot capabilities.\n3. **Multi-Positive Contrastive Loss**: The introduction of a multi-positive contrastive loss fine-tunes the mask-attribute alignment, helping the model generalize to unseen lesions utilizing shared visual attributes, setting it apart from prior models like SAM (Kirillov et al., 2023).\n\n### Minor Comments\n- The reliance on structured descriptions of visual attributes necessitates expert input, which may limit the practical application of the method.\n- The model's performance diminishes on complex visual attributes (e.g., \"Surface Characteristics\"), indicating possible limitations in understanding intricate visual cues.\n- Although the model excels in 12 lesion categories, its generalizability to a wider array of lesions in clinical practice remains questionable without additional training.\n\n### Summary Paragraph\nThe manuscript highlights several strengths against the TMI editorial criteria. It demonstrates significance through its innovative framework for lesion segmentation and shows promise in evaluation via experimental results. However, concerns about reproducibility arise due to reliance on expert radiologist input and potential limitations in performance with complex visual attributes. The generalizability of Malenia beyond the tested categories also poses a challenge.\n\n### Decision Recommendation\nMinor revision suggested, addressing clarity in the methodology and enhancing insights on performance generalizability.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a novel framework for 3D zero‑shot lesion segmentation in medical imaging. The method aims to bridge image‑level knowledge and pixel‑level segmentation tasks through a multi‑scale lesion‑level mask–attribute alignment strategy. Central to the approach is the Cross‑Modal Knowledge Injection (CMKI) module, which jointly optimizes visual and textual embeddings to enhance feature alignment. Experiments on three datasets show that Malenia consistently outperforms state‑of‑the‑art zero‑shot segmentation methods. Overall, the paper is clearly written, the motivation is well articulated, and the contributions are substantial for the medical imaging community.  \n\n**Major Comments**  \n1. **Annotation scalability:** The method depends on converting patient reports into structured descriptions of eight visual attributes. This step requires expert radiologist input, which may limit scalability and real‑world applicability where such detailed textual annotations are scarce.  \n2. **Attribute‑level performance:** According to Appendix D, the approach struggles on attributes that involve complex visual semantics (e.g., “Surface Characteristics” and “Specific Features”). This limitation indicates that the model may not fully capture intricate morphological patterns.  \n3. **Generalization scope:** Although validated on 12 lesion categories, the diversity of clinical lesions is far greater. The generalization potential of Malenia to unseen lesion types without retraining remains unclear.  \n4. **Technical clarification:** Section 3.1 introduces the Multi‑Positive InfoNCE (MP‑NCE) loss for aligning mask and attribute embeddings. Further explanation is needed regarding the computation of MP‑NCE, definition of positive and negative pairs, and how multiple positives per mask token are managed.  \n5. **Hyperparameter sensitivity:** The model uses a fixed number of mask tokens (N = 16) and attributes (R = 8). It would be valuable to include an ablation examining how varying these parameters influences performance.\n\n**Minor Comments**  \n- Clarify the lower performance for certain attributes—could additional contextual or semantic cues help?  \n- Ensure consistent terminology and notation when referring to attributes and mask tokens.  \n- Figures and appendices could better highlight where CMKI provides the largest gains.\n\n**Summary Paragraph (TMI Criteria)**  \nThe work is **significant** in addressing zero‑shot segmentation for 3D medical images, a challenging and under‑explored area. Its **innovation** lies in combining multi‑scale alignment and cross‑modal knowledge injection. The **evaluation** is convincing, covering multiple datasets and comparisons with SOTA models, though further ablations would strengthen the analysis. Regarding **reproducibility**, methods and datasets are described clearly, but the reliance on expert‑derived textual attributes may hinder full replicability in practical settings.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nThe paper is innovative and promising but requires additional clarification on loss formulation, ablation analyses, and discussion of attribute scalability before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work presents a zero-shot medical image semantic segmentation framework that is based on maskformer and attribute-based visual-language feature fusion. To achieve fine-grain alignment between visual and textual concepts, the authors propose to decompose radiological reports into attributes describing lesions of interest, assisted by LLMs and human annotators. The obtained attributes allows fine-grain fusion and alignment between lesion features and textual concepts, yielding improved generalization performance upon unseen lesions. The proposed framework is evaluated on tumor segmentation tasks, and demonstrates improved segmentation results compared with existing zero-shot segmentation approaches on unseen lesions.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe idea of decomposing free-text reports into attributes for fine-grain training and inference is intuitive and feasible. It can significantly reduce the noise and ambiguity associated with unstructured free-text reports and yields improved performance on unseen lesions.\n\nThe key components and the detailed implementations are described in good detail. \n\nThe proposed approach demonstrates performance improvements compared with previous zero-shot image segmentation methods.\n\nDetailed ablation studies are presented to verify the effectiveness of key components.\n\n### Weaknesses\n\nDespite stronger segmentation performance, the major framework looks similar to ZePT (Jiang et al., 2024): maskformer backbone + interacting visual and textual features that are obtained from detailed description of the lesions. Therefore, the key take-home message for readers may be a bit unclear: should the readers interpret the key technical contribution as explicitly decomposing textual descriptions/reports into categorized attributes for the ease of learning and inference?\n\nReaders may argue that using GPT-4 for attribute construction from radiological reports is often infeasible in practice due to the privacy and legal concerns. The authors are encouraged to argue if switching to local open-source LLMs would yield a similar level of performance gain. \n\nThe authors may want to decompose some of technical details in Figure 1 into separate figures and move them closer to corresponding paragraphs.\n\n### Questions\n\nConsidering the practicality issue, would replacing GPT-4 with an open-source LLM for attribute construction (probably also without human annotators?) reaching a similar level of performance gain (as mentioned above)?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript introduces a zero-shot medical image semantic segmentation framework that utilizes MaskFormer and integrates attribute-based visual-language feature fusion. The authors aim to enhance fine-grain alignment between visual and textual concepts by decomposing radiological reports into attributes that describe lesions, leveraging the support of large language models (LLMs) and human annotators. Their approach achieves improved segmentation performance on unseen lesions in tumor segmentation tasks compared to existing zero-shot methods.\n\n### Major Comments\n1. **Novelty and Contribution**: While the framework shows improved segmentation results, it bears similarities to the ZePT model by Jiang et al. (2024), which also combines a MaskFormer backbone with interacting visual and textual features. This resemblance raises questions about the distinct contributions of the current work, particularly regarding how the decomposition of textual descriptions into categorized attributes serves the framework.\n2. **Evaluation Design**: The authors should clarify whether using GPT-4 for attribute construction is practical due to privacy and legal concerns. An exploration of the potential use of local open-source LLMs as an alternative and its impact on performance would strengthen the evaluation.\n3. **Technical Clarity**: Some details in Figure 1 could be better presented by decomposing them into separate figures that are positioned closer to their respective discussion sections to enhance clarity.\n\n### Minor Comments\n- Consider addressing minor typos throughout the manuscript for improved clarity.\n- The title could benefit from additional specificity to reflect the innovative aspects of the framework.\n\n### Summary Paragraph\nThis study makes a notable contribution to medical image segmentation by presenting an innovative framework that enhances performance on unseen lesions through attribute-based feature fusion. However, the overlap in approach with existing models raises questions regarding the distinctiveness of its contributions. While the representation of results is sound and the implementation details are adequately described, the practical implications of utilizing GPT-4 warrant further discussion. Overall, the manuscript exhibits strengths in its methodologies but requires clarifications that address the novelty and practicality of its methods.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a zero-shot medical image semantic segmentation framework based on a MaskFormer architecture combined with attribute-based visual–language feature fusion. Radiology reports are decomposed into attribute sets describing lesions, assisted by large language models (LLMs) and human annotators, to enable fine-grained alignment between visual and textual concepts. The method is evaluated on tumor segmentation tasks, demonstrating improved performance on unseen lesions compared with existing zero-shot segmentation approaches. The paper is clearly written, with sufficient technical detail and a convincing set of experiments.\n\n**Major Comments**  \n1. **Novelty and Relation to Prior Work:** While performance improvements are shown, the proposed framework appears conceptually similar to ZePT (Jiang et al., 2024), which also combines MaskFormer with interacting visual–textual features derived from detailed lesion descriptions. The distinctive technical contribution relative to ZePT—namely, the explicit decomposition of textual descriptions into categorized attributes—should be further clarified to strengthen the paper’s novelty.  \n2. **Practicality and Feasibility:** The use of GPT‑4 for attribute extraction from radiological reports raises feasibility and privacy concerns in real-world clinical deployment. The authors should discuss whether local or open-source LLMs could replicate comparable performance and how privacy issues might be mitigated.  \n3. **Evaluation and Presentation:** Figure 1 condenses several important technical components, and decomposition into multiple figures aligned with the text would improve clarity. More explicit discussion of how attribute extraction contributes quantitatively to zero-shot generalization would also enhance the evaluation section.  \n4. **Open Question:** Could an open-source LLM—possibly without human annotators—achieve similar performance in attribute extraction? Addressing this experimentally or conceptually would improve completeness.\n\n**Minor Comments**  \n- The manuscript’s structure and writing are clear, though figure placement could better correspond to the narrative flow.  \n- Verify consistent terminology for “attributes,” “features,” and “descriptors” to avoid confusion.  \n- Minor typographical issues are minimal.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses a significant challenge in zero-shot medical image segmentation, offering a method that leverages textual attribute decomposition for better generalization. Its innovation lies in aligning decomposed textual features with visual representations. The evaluation is convincing with strong ablations, though differentiation from prior work could be clearer. Reproducibility is supported through detailed implementation, but reliance on GPT‑4 introduces practical limitations that may hinder adoption.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents Malenia, a novel framework for 3D zero-shot lesion segmentation that leverages vision-language pre-training through mask-attribute alignment. The work addresses the challenge of segmenting unseen lesion types in CT scans by decomposing medical reports into structured descriptions of eight fundamental disease attributes (location, shape, density, etc.) and aligning these with multi-scale mask representations. The framework incorporates a Cross-Modal Knowledge Injection (CMKI) module that fuses visual and textual features to generate segmentation predictions. Experiments across three datasets (MSD, KiTS23, and in-house data) with 12 lesion categories demonstrate superior performance compared to existing zero-shot methods, achieving 6.40% improvement in DSC on MSD and 8.14% on KiTS23 over the best competing method.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and unclear notation**\n  - Equation (2) uses inconsistent indexing where m_j^(i) appears in the loss function but the superscript notation is not consistently applied throughout (Section 3.1)\n  - The bipartite matching process for selecting S foreground mask tokens is mentioned but the mathematical formulation for this matching is not provided (Page 5)\n  - The scaling factor √C in Equation (5) lacks justification for why this specific normalization is chosen over alternatives like temperature scaling (Section 3.2)\n  - The ensemble weights β1 and β2 in the final prediction are set to 0.5 without mathematical justification or sensitivity analysis (Page 6)\n\n• **Limited experimental validation and evaluation gaps**\n  - The private dataset contains only 30 cases per lesion type, which is insufficient for robust statistical validation of zero-shot performance claims (Table 5, Page 16)\n  - Cross-validation results are only reported for seen lesions (Table 2) but not for unseen lesions, making it difficult to assess statistical significance of zero-shot improvements (Page 9)\n  - The clinical knowledge table used for disease identification (Step-IV in Figure 2) is not validated against expert annotations or inter-rater agreement (Page 6)\n  - Computational efficiency comparison lacks inference time measurements, only providing FLOPs which may not reflect real-world performance (Table 10, Page 23)\n\n• **Methodological concerns regarding attribute construction and generalizability**\n  - The semi-automatic pipeline using GPT-4 for attribute extraction introduces potential bias and inconsistency, as different LLMs may produce varying attribute descriptions (Section B.2, Page 16)\n  - The eight disease attributes are derived from expert consultation but lack validation against established medical taxonomies or inter-expert agreement studies (Table 6, Page 17)\n  - The multi-positive contrastive learning assumes independence between attributes, but medical attributes often have complex interdependencies that are not modeled (Equation 2, Page 5)\n  - The framework's reliance on structured reports limits applicability to institutions without comprehensive reporting standards (Section 3.1, Page 4)\n\n## Suggestions for Improvement\n\n• **Strengthen mathematical rigor and provide clearer formulations**\n  - Provide explicit mathematical formulation for the bipartite matching process used to select foreground mask tokens, including the matching cost function and Hungarian algorithm application\n  - Include sensitivity analysis for the scaling factor √C and ensemble weights β1, β2, with ablation studies showing performance across different parameter values\n  - Standardize notation throughout the paper, ensuring consistent use of superscripts and subscripts across all equations\n  - Add theoretical justification or empirical validation for the choice of MP-NCE loss over other multi-positive contrastive learning approaches\n\n• **Expand experimental validation with more rigorous statistical analysis**\n  - Increase the private dataset size to at least 100 cases per lesion type or provide confidence intervals and statistical significance tests for current results\n  - Conduct cross-validation experiments for zero-shot performance evaluation, reporting mean and standard deviation across multiple splits\n  - Validate the clinical knowledge table against expert annotations with inter-rater agreement metrics (Cohen's kappa or similar)\n  - Include actual inference time measurements alongside FLOPs, testing on multiple hardware configurations to assess real-world computational efficiency\n\n• **Address methodological limitations and improve generalizability**\n  - Conduct inter-expert agreement studies for the eight disease attributes and compare against established medical ontologies like RadLex or SNOMED CT\n  - Perform ablation studies using different LLMs for attribute extraction and quantify the impact of potential biases in the semi-automatic pipeline\n  - Model attribute interdependencies explicitly, potentially through conditional probability distributions or graph-based representations of attribute relationships\n  - Evaluate framework performance on datasets with varying report completeness and quality to assess robustness to real-world clinical variations",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces Malenia, a novel framework for 3D zero-shot lesion segmentation that effectively utilizes vision-language pre-training through mask-attribute alignment. The study aims to tackle the complex challenge of segmenting previously unseen lesion types in CT scans by structuring medical reports into descriptions of eight essential disease attributes, which are then aligned with multi-scale mask representations. The integration of a Cross-Modal Knowledge Injection (CMKI) module facilitates the fusion of visual and textual features to enhance segmentation predictions. The reported experiments, conducted across three datasets (MSD, KiTS23, and in-house data) covering 12 lesion categories, indicate significant improvements over existing zero-shot methods.\n\n**Major Comments:**  \n1. **Mathematical formulation inconsistencies and unclear notation:**\n   - Equation (2) exhibits inconsistent indexing, particularly with m_j^(i) where the superscript notation is not uniformly applied (Section 3.1).\n   - The lack of a mathematical description for the bipartite matching process in selecting S foreground mask tokens is noted (Page 5).\n   - The scaling factor √C in Equation (5) should be justified relative to alternatives like temperature scaling (Section 3.2).\n   - The parameters β1 and β2 are set to 0.5 without adequate justification or sensitivity analysis (Page 6).\n\n2. **Limited experimental validation and evaluation gaps:**\n   - The private dataset consists of merely 30 cases per lesion type, which raises concerns regarding the robustness of the zero-shot performance claims (Table 5, Page 16).\n   - Cross-validation results are only available for seen lesions, obscuring insights into the statistical significance of zero-shot advancements (Page 9).\n   - The clinical knowledge table for disease identification lacks validation against expert annotations or assessments of inter-rater agreement (Page 6).\n   - The computational efficiency analysis lacks inference time metrics, offering only FLOPs that may not accurately represent real-world performance (Table 10, Page 23).\n\n3. **Methodological issues concerning attribute construction and generalizability:**\n   - The use of GPT-4 in attribute extraction may introduce variances and biases, as different language models could produce inconsistent descriptions (Section B.2, Page 16).\n   - The derived eight disease attributes have not been validated against authoritative medical taxonomies or studies of inter-expert consensus (Table 6, Page 17).\n   - The independence assumption in multi-positive contrastive learning doesn’t account for the complex interdependencies of medical attributes (Equation 2, Page 5).\n   - The reliance on structured reporting might limit the utility of the framework in institutions lacking standardized reporting methods (Section 3.1, Page 4).\n\n**Minor Comments:**  \n- Standardize the notation utilized throughout the manuscript for consistency.  \n- Clarify the bipartite matching formulation and elucidate the associated mathematical derivations.  \n- Enhance clarity in the description of the scaling factor and the rationale behind specific design choices.\n\n**Summary Paragraph:**  \nIn terms of significance, the paper addresses a crucial gap in medical imaging by proposing a zero-shot segmentation approach. The novelty lies in the framework's integration of vision-language features for improved accuracy. However, the evaluation of rigorous statistical analysis and validation raises concerns regarding reproducibility due to the limited dataset size and absence of thorough cross-validation for unseen lesions. Additionally, the manuscript lacks methodological clarity in aspects such as mathematical formulations and the construction of disease attributes, impacting the overall robustness of its findings.\n\n**Decision Recommendation:**  \nMajor revision required.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a novel framework for 3D zero-shot lesion segmentation that leverages vision–language pretraining through mask–attribute alignment. The approach decomposes structured radiology reports into eight disease attributes (e.g., location, shape, density) and aligns these with multi-scale visual mask representations via a Cross-Modal Knowledge Injection (CMKI) module. Evaluations on MSD, KiTS23, and an in-house dataset covering 12 lesion types demonstrate improved Dice Similarity Coefficient (DSC) scores—6.40% on MSD and 8.14% on KiTS23—over existing zero-shot segmentation methods. The paper is well-motivated and clearly written, but there are significant issues regarding mathematical formulation, evaluation rigor, and generalizability that need to be addressed.\n\n---\n\n**Major Comments**  \n1. **Mathematical Formulation and Notation**  \n   - Equation (2) uses inconsistent indexing; terms such as \\(m_j^{(i)}\\) are not applied uniformly.  \n   - The bipartite matching process for selecting foreground mask tokens is referenced but lacks an explicit mathematical description.  \n   - The scaling factor \\( \\sqrt{C} \\) in Equation (5) is introduced without justification or comparison to alternatives like temperature scaling.  \n   - Ensemble weights (\\( \\beta_1, \\beta_2 = 0.5 \\)) are fixed arbitrarily without sensitivity analysis.  \n\n2. **Experimental Validation and Evaluation Gaps**  \n   - The private dataset includes only 30 cases per lesion type, limiting statistical power.  \n   - Cross-validation is performed only on seen lesions; zero-shot results lack significance testing.  \n   - The clinical knowledge table used for disease identification is not validated against expert or inter-rater annotations.  \n   - Computational efficiency is reported only in FLOPs, with no measurement of actual inference times.  \n\n3. **Methodological and Generalizability Concerns**  \n   - The semi-automatic attribute extraction using GPT‑4 may introduce bias or inconsistency across LLMs.  \n   - The eight attributes were derived via expert consultation but not validated against existing medical ontologies or inter-expert agreement.  \n   - Multi-positive contrastive learning assumes independence among attributes, neglecting known interdependencies.  \n   - Dependence on structured reports may limit generalizability to less standardized clinical settings.  \n\n---\n\n**Minor Comments**  \n- Ensure consistent notation of indices and superscripts.  \n- Clarify terms in equations and standardize cross-referencing of sections and figures.  \n- Provide clear definitions of all abbreviations when first introduced.  \n- Improve figure/table captions for standalone clarity.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in addressing zero-shot lesion segmentation, a challenging and clinically relevant problem. Its innovation lies in aligning structured textual attributes with visual mask representations through vision–language pretraining. However, weaknesses in mathematical rigor, insufficient statistical evaluation, and limited dataset diversity reduce the strength of the experimental evidence. Reproducibility is hindered by incomplete details on parameter settings, dataset construction, and lack of open validation of the attribute-extraction process. Strengthening empirical and theoretical justification will substantially enhance both credibility and generalizability.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The paper presents a promising idea with clear potential but requires substantial clarification, more rigorous experiments, and validation of methodological assumptions before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to IEEE TMI**  \n✅ The manuscript presents a clear methodological advance in medical imaging. It introduces a new vision‑language framework (Malenia) for **3‑D zero‑shot lesion segmentation**, focusing on a novel *multi‑scale lesion‑level mask–attribute alignment* strategy and a *cross‑modal knowledge‑injection* (CMKI) module. The topic—transfer of image‑text pre‑training to volumetric segmentation—is well aligned with TMI’s focus on imaging methodology and machine learning foundations.\n\n2. **Novelty & Contribution Level**  \nThe paper clearly situates itself beyond prior vision‑language or SAM adaptations by:\n- extending alignment from image‑ to voxel‑scale,  \n- decomposing textual reports into eight fine‑grained radiologic attributes, and  \n- introducing multi‑positive contrastive learning plus a cross‑modal fusion block.  \nThese constitute substantive algorithmic innovations rather than incremental dataset application. Novelty is high for the zero‑shot segmentation sub‑field.\n\n3. **Technical and Experimental Rigor**  \nThe method is extensively described with mathematical formulation, ablation studies, and comparisons on MSD, KiTS23, and a private dataset. Experiments include both “seen” and “unseen” category evaluation, measure Dice/NSD, and analyze compute cost. Overall rigor is good; nevertheless, reproducibility would benefit from code availability, more statistical reporting (variance or confidence intervals for Table 1), and additional cross‑institution validation to confirm real‑world robustness.\n\n4. **Clarity and Presentation**  \nThe manuscript is long but clearly structured, with good figures and tables. English grammar is acceptable though some sentences could be shortened for readability. The method description is dense and assumes reader familiarity with transformer architectures; a brief schematic for CMKI’s data flow would aid comprehension.\n\n5. **Ethical & Reproducibility Compliance**  \nPublic datasets and a private anonymized dataset with IRB waiver are properly documented. Dataset usage appears compliant. The authors promise to release code and attribute annotations, aligning with reproducibility guidelines.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n### 1. Summary  \nThis paper proposes **Malenia**, a 3‑D vision‑language framework that enables zero‑shot lesion segmentation in CT images. The key idea is to align multi‑scale lesion mask tokens with structured textual descriptions of radiologic attributes (e.g., shape, density, surface characteristics). Contrastive multi‑positive learning trains shared embeddings linking lesion features to language‑based attributes. A **Cross‑Modal Knowledge Injection** module further fuses textual and visual information for prediction. Evaluated on MSD, KiTS23, and an in‑house dataset (12 lesion categories), Malenia consistently outperforms SAM‑derived and other recent zero‑shot methods and maintains competitive accuracy for seen classes.\n\n### 2. Strengths  \n- Methodologically novel extension of vision‑language pre‑training to volumetric segmentation.  \n- Comprehensive experiments with strong quantitative gains (≈6–9 % Dice improvement on unseen lesions).  \n- Explicit use of radiologist‑curated attribute ontology bridges medical semantics and computational models.  \n- Careful ablation analyses supporting component contributions.  \n- Efficiency advantage by removing text encoder at inference.\n\n### 3. Weaknesses  \n- Statistical robustness across institutions is limited; confidence intervals for cross‑patient Dice are missing.  \n- The use of GPT‑4 for attribute extraction raises practical concerns about accessibility and reproducibility; clarity on publicly shareable tools would help.  \n- CMKI design and parameter choices are mathematically described but lack intuition or theoretical justification.  \n- Writing could be tightened—method section sometimes repeats implementation details.  \n- Discussion of clinical impact or potential pitfalls (e.g., handling ambiguous margins) could be expanded in the main text rather than appendix.  \n- Limited exploration beyond CT modality or single‑phase contrast conditions.\n\n### 4. Major Comments  \n1. **Quantify Uncertainty:** Provide standard deviation or 95 % confidence intervals for Dice/NSD across test volumes; this is required to judge statistical significance of improvements.  \n2. **Reproducibility Clarification:** Specify which components (code, structured attributes, trained weights) will be released and under what license. Describe preprocessing details sufficient for replication.  \n3. **Generalization Analysis:** Include cross‑institution or modality transfer study—e.g., training on MSD only and testing on KiTS23—to better demonstrate zero‑shot robustness.  \n4. **CMKI Discussion:** Offer conceptual or empirical justification for dual‑branch aggregation coefficients (β1 = β2 = 0.5). A small sensitivity analysis would strengthen confidence.  \n5. **Comparison Scope:** Include comparison with more conventional foundation‑model fine‑tuning (e.g., Swin‑UNETR pretrained on RadImageNet) to show improvement is not solely due to more training epochs.  \n6. **Limitations:** Move part of Appendix I (limitations and future work) into the main discussion for transparency per TMI guidelines.\n\n### 5. Minor Comments  \n- Improve figure readability (increase font size in Figs 1–2).  \n- Correct minor typographical issues (“tate‑of‑the‑art” → “state‑of‑the‑art”).  \n- Define all abbreviations (LLM, CMKI) at first appearance in the main manuscript.  \n- Consider shortening or summarizing Dataset Section B; move granular IRB details to Supplementary Materials.  \n- Reference formatting: ensure consistent capitalization per IEEE style.  \n\n### 6. Overall Recommendation  \n**Major Revision** – The work is methodologically strong and highly relevant to TMI, but needs clearer experimental uncertainties, improved reproducibility documentation, and condensed presentation before acceptance.\n\n### 7. Confidence Level  \n**4 / 5** – High confidence in the technical assessment; moderate uncertainty regarding dataset generalization pending additional validation.\n\n---\n\n**Summary Recommendation:**  \nMalenia represents a significant step toward zero‑shot 3‑D medical segmentation via vision‑language alignment—appropriate for TMI after addressing rigor and reproducibility clarifications.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Malenia*, a three‑dimensional vision–language framework for zero‑shot lesion segmentation in CT volumes. It aligns multi‑scale lesion‑mask tokens with structured textual descriptions of radiologic attributes and incorporates a Cross‑Modal Knowledge Injection (CMKI) module to fuse image and text representations. The paper aims to extend image–text pretraining to volumetric segmentation and demonstrates improvements over existing zero‑shot and segment‑anything methods. The manuscript is well structured and technically detailed, though somewhat dense in its methodological exposition.\n\n---\n\n**Major Comments**  \n1. **Quantify Uncertainty:** Add measures of variability (standard deviation or 95 % confidence intervals) for Dice and NSD metrics to support statistical credibility of the reported improvements.  \n2. **Reproducibility:** Explicitly state which resources (code, trained weights, and attribute ontology) will be made public, including licensing and preprocessing details necessary for replication.  \n3. **Generalization and Robustness:** Incorporate cross‑institution or modality‑transfer evaluation—e.g., training on MSD and testing on KiTS23—to substantiate claims of zero‑shot generalization.  \n4. **CMKI Analysis:** Provide conceptual or sensitivity analysis of CMKI weighting parameters (β₁, β₂ = 0.5) to justify design decisions.  \n5. **Comparison Scope:** Include baseline comparisons with conventional pretrained medical models (e.g., Swin‑UNETR) to rule out performance gains due merely to extended training.  \n6. **Transparency and Limitations:** Move a concise limitations discussion from the appendix into the main text, covering clinical applicability and possible failure cases.\n\n---\n\n**Minor Comments**  \n- Increase legibility of Figures 1–2 (font size, resolution).  \n- Correct typographical errors (e.g., “tate‑of‑the‑art” → “state‑of‑the‑art”).  \n- Define abbreviations such as LLM and CMKI on first use.  \n- Condense Dataset Section B and relocate IRB details to the supplement.  \n- Standardize reference capitalization following IEEE style.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper presents a significant and innovative approach, extending vision–language modeling to 3‑D medical segmentation with clear algorithmic novelty through multi‑scale mask–attribute alignment and CMKI integration. Experimental evaluation is thorough and shows consistent Dice improvements (≈6–9 %) on unseen classes; however, statistical variability and domain generalization remain insufficiently analyzed. Ethical and dataset documentation are compliant, and promised code release supports reproducibility, though precise dissemination details are still needed. Overall, the work is promising but requires additional quantitative validation and clearer implementation transparency.\n\n---\n\n**Decision Recommendation:** **Major Revision** – Technically strong and relevant to TMI, but the manuscript should address uncertainty quantification, reproducibility details, and presentation clarity before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces **Malenia**, a novel vision-language framework aimed at enhancing 3-D zero-shot lesion segmentation in medical imaging. It proposes a multi-scale lesion-level mask-attribute alignment strategy coupled with a cross-modal knowledge-injection module. The study’s topic aligns well with the IEEE TMI's focus on innovative imaging methodologies and the application of machine learning techniques, showcasing a clear methodological advance.\n\n**Major Comments:**  \n1. **Quantification of Uncertainty:** The manuscript should include standard deviations or 95% confidence intervals for Dice/NSD across test volumes to assess statistical significance of the reported improvements.  \n2. **Reproducibility Clarity:** Specify which components (code, structured attributes, trained weights) will be released and under what licensing terms. Detailed preprocessing procedures should be described to facilitate replication.  \n3. **Generalization Analysis:** It would be beneficial to incorporate a cross-institution or modality transfer study, e.g., training exclusively on MSD and testing on KiTS23, to demonstrate robustness in zero-shot segmentation.  \n4. **CMKI Justification:** The design of the cross-modal knowledge injection and the choice of dual-branch aggregation coefficients (β1 = β2 = 0.5) require clearer justification, either conceptually or empirically. A sensitivity analysis could enhance confidence in the model's performance.  \n5. **Comparison Scope:** A comparison with conventional foundation-model fine-tuning approaches (e.g., Swin-UNETR pretrained on RadImageNet) would help illustrate that improvements are not solely due to increased training epochs.  \n6. **Limitations Discussion:** Consider incorporating the discussion on limitations and future work currently placed in Appendix I into the main text for transparency according to TMI guidelines.\n\n**Minor Comments:**  \n- Enhance figure readability by increasing the font size in Figures 1 and 2.  \n- Address minor typographical errors (e.g., correct \"tate‑of‑the‑art\" to \"state‑of‑the‑art\").  \n- Define all acronyms (e.g., LLM and CMKI) upon first mention in the manuscript.  \n- Summarize or condense Dataset Section B; detailed IRB information could be placed in the Supplementary Materials.  \n- Ensure consistent reference formatting in accordance with IEEE style guidelines.\n\n**Summary Paragraph:**  \nThis manuscript presents a significant contribution to the field of medical imaging by introducing Malenia, which shows high novelty through the adaptation of vision-language frameworks for volumetric segmentation. While the technical rigor is commendable, increased transparency regarding statistical uncertainties and reproducibility measures is recommended to enhance the work's validity and reliability. Overall, the manuscript effectively addresses innovative methodologies but should refine its presentation and discussion on clinical implications.\n\n**Decision Recommendation:**  \n**Major Revision** – The manuscript demonstrates strong methodological quality and is pertinent to IEEE TMI. However, clarifications regarding experimental uncertainties, enhanced reproducibility documentation, and a more concise presentation are necessary prior to acceptance. \n\n**Confidence Level:**  \n**4 / 5** – High confidence in the technical assessment, but moderate uncertainty exists regarding dataset generalization awaiting further validation.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Shaoting Zhang",
      "Wenhui Lei",
      "Xiaofan Zhang",
      "Yankai Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_743b761d4a84ae02565015701e9c3f189a2f413f.pdf",
    "remote_url": "https://openreview.net/pdf/743b761d4a84ae02565015701e9c3f189a2f413f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "MONICA: Benchmarking on Long-tailed Medical Image Classification",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Justin"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Long-tailed Learning",
      "Benchmark",
      "Medical Image Classification"
    ],
    "abstract": "Long-tailed learning is considered to be an extremely challenging problem in data imbalance learning. It aims to train well-generalized models from a large number of images that follow a long-tailed class distribution. In the medical field, many diagnostic imaging exams such as dermoscopy and chest radiography yield a long-tailed distribution of complex clinical findings. Recently, long-tailed learning in medical image analysis has garnered significant attention. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often leads to unfair comparisons and inconclusive results. To help the community improve the evaluation and advance, we build a unified, well-structured codebase called Medical OpeN-source Long-taIled ClassifiCAtion (MONICA), which implements over 30 methods developed in relevant fields and evaluated on 12 long-tailed medical datasets covering 6 medical domains. Our work provides valuable practical guidance and insights for the field, offering detailed analysis and discussion on the effectiveness of individual components within the inbuilt state-of-the-art methodologies. We hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning. The codebase will be publicly available on GitHub.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a framework and codebase for structured benchmarking of long-tailed (LT) learning methods on various medical image classification tasks. The benchmark, MONICA, implements over 30 LT learning methods, with comprehensive experiments assessing performance across 12 LT medical image classification datasets spanning 6 different modalities. The experiments analyze which methods, and categories of methods, provide the most benefit to LT medical image classification across tasks in a controlled environment.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- This work addresses an important problem, namely the variability in dataset/hyperparameters/etc. when evaluating LT learning methods for medical image classification tasks. These variations in setting make head-to-head comparisons difficult, so MONICA serves to provide a “fair playing ground” for these LT learning methods.\n- The framework will become publicly available and should serve as an extensible resource going forward for LT medical image classification research.\n- The organization and presentation quality of the paper is strong, with helpful use of formatting (typesetting, color, etc.) and high-quality figures.\n- Experiments are very thorough, spanning many relevant methods, datasets, and tasks.\n- Discussion is thoughtful, going beyond simply displaying all benchmark results. The authors try to synthesize takeaways, provide caveats/limitations, assess out-of-distribution performance, and more.\n\n### Weaknesses\n\n- Writing can be improved throughout. See specific comments below for examples of awkward wording, inconsistent naming, grammatical errors, etc.\n- It is possible that choosing a fixed set of hyperparameters across methods unintentionally advantages certain methods. Ideally, one could argue that each method should be individually tuned on each task; however, I am aware that this would require a vast amount of resources and time, so I do not consider this a major limitation. More practical solutions to enhance the benchmark would be the following: (i) uncertainty estimates should be provided (e.g., bootstrapped confidence intervals or standard deviations over multiple runs), and (ii) multiple performance metrics should be provided (e.g., AUROC).\n\n### Questions\n\n- Is it possible that the chosen hyperparameters used across all methods happen to be more advantageous for certain methods and suboptimal for others? In one sense, using the same set of hyperparameters across methods appears “fair”; however, it may actually be more fair to individually tune each method on each task. I recognize the difficulty of conducting fair comparisons in such a large-scale experimental setting, where it is costly to, e.g., run multiple trials of all experiments. I am not asking the authors to necessarily perform such experiments, but rather to consider this point and perhaps comment on it as a limitation/consideration.\n- Can the authors provide a summary of practical suggestions for which methods to use in a few sentences near the Conclusion?\n- I might suggest including the **rank** of each method on a given task in all tables. This would also enable you to *quantitatively* assess method performance across tasks (which method has the lowest average/median rank overall?). To make this work logistically (fit all columns in the table), you may need to reduce the precision to one decimal place, e.g.\n- The two paragraphs “LTMIC improves out-of-distribution detection” and “Using imbalanced validation dataset for checkpoint selection” are not properly set up. For the former, what does it mean to use “ImageNet as OOD samples, with 1,000 randomly selected images”? What exactly is the task, how is it formulated, and how are experiments conducted? Further, why do we care about this model behavior? For the former, Figure 4 and its findings are confusing – why exactly does this demonstrate “stable convergence”? My general advice: **Use the methods section to describe and prepare the reader to understand everything that appears in the results**. When I come to these results sections, I should already have an idea of what experiments you have performed.\n\n**Minor comments/questions:**\n- Avoid editorializing with value judgments: “benchmark is **meticulously** designed”; “we… develop a… **well-structured** codebase”; “our work provides **valuable** practical guidance”. Simply present your work and let the reader make these judgments!\n- “data imbalance learning” is not a phrase I have heard. Perhaps “imbalanced learning”?\n- “unified, strictly formulated, and comprehensive benchmark”. Unsure what “strictly formulated” means. Could simply say “unified, comprehensive benchmark”\n- “we build a… codebase…, which implements over 30 methods… and evaluated on 12… datasets”. It seems that “evaluated on” is the wrong tense; also, what is being evaluated?\n- This does not belong in an abstract: “We hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning.”\n- Often unnecessary inclusion of “the” before concepts: “The deep learning techniques”; “the collected image datasets”; “the long-tailed imbalance”\n- “The deep learning techniques have proven effective for most computer vision tasks benefiting from the grown-up dataset scale.” Remove “The”; what does “grown-up dataset scale” mean? “Grown-up” is not the right adjective – be more concrete.\n- Refrain from claims like “always result” (line 57) – soften to “usually” or similar\n- Confused by this justification: “it is vital to recognize these rare diseases in real-world practice, as they are relatively rare for doctors and may also lack diagnostic capacity.” This reads as “it is vital to recognize rare diseases because they are rare”.\n- Line 65: can change “contributions, i.e.,” -> “contributions:”\n- Be consistent with capitalization/presentation of terms: “Re-sampling” vs “re-sampling”; “Module improvement” vs. “Module Improvement”; “mnist” vs. “MNIST”; “mixup” vs. “MixUp”; etc.\n- Line 82: “we are still curious to explore”. Perhaps just “we aim to explore”?\n- “The partition schemes are vita important”. What does “vita” mean?\n- The last two paragraphs of the introduction are probably better off being formatted as bulleted or numbered lists. Also, it is unclear why these numbered lists are formatted differently: **1) xxx.** vs. (1) xxx.\n- “of class $k$ where $\\rho$ denoted as imbalance ratio”. The phrase “denoted as” is awkward + need a comma after $k$\n- “a common assumption in long-tailed learning is when the classes are sorted by cardinality in decreasing order” I’m not sure what this means or why this represents an “assumption”. I would just remove this sentence since it does not seem to be used later.\n- Line 144: “is a long-tailed version constructed from”. Need to say it is a version “of” something; alternatively, use a word other than “version” like “dataset”\n- Inconsistent spacing/use of commas in numbers. “10, 015” -> “10,015”; “3200 fundus images” -> “3,200 fundus images”\n- Inconsistent spacing around commas and colons: “training/off-site testing / on-site testin”; “7 : 1: 2”; etc.\n- “Liver Tumor Segmentation Benchmark” -> “the Liver Tumor Segmentation Benchmark”\n- I realize it is hard to categorize some methods into one bin but GCL loss going in Information Augmentation is interesting, particularly since all other losses fall under re-sampling. It seems to also have module improvement as well.\n- “Causal classifier (Tang et al., 2020) resorted to causal inference for keeping the good and removing the bad momentum causal effects in long-tailed learning.” The phrase “resorted to” is strange and has a negative connotation; also, what do “good” and “bad” mean?\n- “All these designs are for the fairness and the practicality of the comparison on the benchmark.” Too vague – in what specific way do these support fairness?\n- Table 3: Inconsistent “Avg” vs “Avg.” vs “avg”\n- Table 4: Consider using a line break occasionally (so one loss function occupies two rows). This would allow you to use a larger font size. Also, be consistent “CrossEntropy” vs “CE”?\n- “assessing MixUp based solely on performance is not fair”. Soften to “may not be fair”\n- “led to a significant performance decline, e.g,”. Refrain from saying “significant” without statistical significance test + change “e.g,” -> “e.g.,”\n- “Use two-stage training as a general paradigm” sounds like a command. Perhaps “Using”?\n- Define “SSL” acronym at first use\n- “Modify classifier to reduce prediction bias” -> “Classifier modification to reduce prediction bias”\n- “In Fig. 2, We visualize” -> “In Fig. 2, we visualize”\n- Table 5 indicates the meaning of asterisk, which is never used in the table.\n- “models with larger parameters”. The parameters are not “larger” – could say “more parameters” or “a larger parameter count” perhaps.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a framework and codebase, named MONICA, for structured benchmarking of long-tailed (LT) learning methods applied to various medical image classification tasks. With its implementation of over 30 LT learning methods, the paper provides extensive experiments assessing performance across 12 datasets covering 6 different modalities. Overall, the manuscript is well-organized and the presentation quality is commendable, enabling readers to access a “fair playing ground” for evaluating LT learning methods.\n\n**Major Comments:**  \n1. The writing throughout the manuscript needs improvement; specific examples of awkward wording, inconsistent naming, and grammatical errors are provided below.  \n2. The choice of a fixed set of hyperparameters may unintentionally favor certain methods. While individual tuning per method and task would be ideal, practical solutions such as providing uncertainty estimates and multiple performance metrics should be considered enhancements.  \n3. The results evaluation could benefit from clarity regarding the experiments and methodology, especially in sections discussing \"LTMIC improves out-of-distribution detection\" and “Using imbalanced validation dataset for checkpoint selection.\" Further detail on task formulation and experimental procedures is needed. \n\n**Minor Comments:**  \n- Avoid editorializing language in the presentation of work (e.g., \"meticulously designed\").  \n- Consider using \"imbalanced learning\" instead of “data imbalance learning.”   \n- The term \"strictly formulated\" is vague; simplify to \"unified, comprehensive benchmark.\"  \n- Correct grammatical inaccuracies, such as “evaluated on” in the context used.  \n- Remove non-essential phrases from the abstract, e.g., “We hope this codebase serves…”  \n- Ensure consistency in terms, such as capitalizations and phrasing, throughout the manuscript.  \n\n**Summary Paragraph:**  \nThe manuscript addresses the critical issue of variability in evaluating LT learning methods for medical image classification, representing an important contribution to the field. However, writing clarity and methodological rigor need attention to enhance reproducibility and the significance of results. While MONICA serves as a robust resource for ongoing research, the need for diverse performance metrics and better-defined hyperparameter settings remains. \n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MONICA**, a benchmarking framework and accompanying codebase designed for structured evaluation of long-tailed (LT) learning methods applied to medical image classification. It integrates over 30 LT learning algorithms and reports results across 12 datasets spanning six modalities. The study aims to enable fair and reproducible comparisons by standardizing experimental conditions. The paper is generally well-organized, clearly presented, and supported by comprehensive experiments and well-designed figures.\n\n---\n\n**Major Comments**  \n1. **Evaluation Design and Fairness:** The use of fixed hyperparameters across all methods, while intended to ensure fairness, may unintentionally favor certain models. An alternative would be per-method tuning per task, though this may be computationally infeasible. The authors should at least acknowledge this as a potential limitation.  \n2. **Statistical Reporting and Metrics:** The benchmark would be strengthened by including uncertainty estimates (e.g., standard deviations or bootstrapped confidence intervals) and a broader selection of performance metrics such as AUROC.  \n3. **Result Presentation:** Consider presenting method **ranks** within tables to facilitate quantitative comparison of relative performance across tasks.  \n4. **Experimental Clarity:** The sections describing “LTMIC improves out-of-distribution detection” and “Using imbalanced validation dataset for checkpoint selection” require clearer setup in the Methods section. The OOD task definition, dataset composition, and rationale should be explicitly stated to help readers interpret these results.  \n5. **Writing and Style:** The manuscript would benefit from systematic language editing to correct awkward phrasing, inconsistent naming, and minor grammatical issues.\n\n---\n\n**Minor Comments**  \n- Avoid subjective qualifiers (e.g., “meticulously designed,” “valuable guidance”).  \n- Replace uncommon or unclear terms (“data imbalance learning,” “grown-up dataset scale”) with standard wording.  \n- Ensure consistent terminology, formatting, capitalization, number spacing, and table labeling (e.g., “Avg” vs. “Avg.”).  \n- Revise ambiguous expressions such as “resorted to causal inference” or “larger parameters.”  \n- Define acronyms on first use and maintain uniform style in figure and table captions.  \n- Simplify or reformat the introduction’s final sections for readability and structural consistency.\n\n---\n\n**Summary Paragraph (TMI Editorial Criteria)**  \nThe work’s **significance** lies in providing the first extensive, standardized benchmark addressing reproducibility challenges in LT medical image classification. The **innovation** is primarily infrastructural—establishing a unified evaluation framework rather than proposing new algorithms. The **evaluation** is broad and well-controlled but would benefit from adding uncertainty measures and clearer methodological exposition. Regarding **reproducibility**, code release and dataset coverage are strong positives, though clearer documentation and improved writing would further enhance transparency.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** — The study is technically solid and potentially impactful, but revisions are needed to strengthen methodological transparency, statistical reporting, and textual clarity before it is ready for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents a unified benchmark for long-tailed learning in the medical domain by integrating several existing datasets and implementing a complete pipeline from data loading to model training and evaluation. The authors claim that the benchmark supports over 30 methods for comparison and provides an analysis of their performances.\n\n- Update after the discussion phase:\n\nThank you for the detailed responses! I've raised my score. While my concerns are not entirely resolved, I believe with careful revisions, its future version has the potential to be accepted.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe paper attempts to provide a comprehensive benchmark for long-tailed medical image classification. The idea of integrating multiple existing methods and datasets into a unified platform could potentially be useful for researchers who want to compare various methodologies under a standardized framework.\n\n### Weaknesses\n\n1. Motivation. The paper lacks sufficient justification for evaluating long-tailed problems specifically in medical imaging tasks. While the authors mention some motivations at the beginning, these arguments are not convincing. Is there a fundamental difference between long-tailed problems in medical imaging and those in conventional tasks? Would this difference necessitate different methodologies? Even if the data modalities and evaluation methods are distinct (e.g., balanced vs. imbalanced test sets), would this lead to fundamentally different approaches? The paper analyzes multiple methods based on this premise but fails to provide insightful conclusions, which further deepens my skepticism about the motivation.\n\n2. Dataset Contribution. Although the paper claims to use 12 datasets, 7 of these come from MedMNIST, and several of them are derived from previous work. This reduces the originality of the dataset contribution. Furthermore, the split between multi-class and multi-label datasets is 9/3, respectively. It is worth noting that many existing studies have already utilized MedMNIST for long-tailed learning (https://scholar.google.com/scholar?cites=11226954386823169312&scipsc=1&q=long+tail). Given that 7 out of the 12 datasets in this paper are from MedMNIST, why should users choose MONICA over MedMNIST, which already has extensive use and coverage in the medical imaging field? Additionally, the experimental methods used for multi-class and multi-label datasets are almost entirely different, and the analysis of multi-label results is limited to a single vague statement that multi-label classification is more challenging. This gives the impression that multi-label datasets were included just for the sake of completeness, rather than being a key focus.\n\n3. Code Contribution. The code is not provided in the appendix, nor is there an anonymous GitHub link, which means the authors' claims about the code cannot be verified. By comparison, the NeurIPS D&B track (single-blind review) usually includes dataset or code links, along with information about author affiliations, licenses, and ethics. Although such links may be added after acceptance, this suggests that such work may not be well-suited for ICLR's double-blind review process.\n\n    Additionally, the description of the code structure in Section 3.1 is not particularly informative. The modular design described is basic and lacks novel insights. A more impactful modular design, like in mmdetection, which breaks down components into backbone, neck, and bbox head, would have been more meaningful. As it stands, the description feels unnecessary.\n\n4. Experimental Analysis Lacks of Insights. Comments below:\n\n    - Despite using multiple datasets, the authors only provide a generic / systematic comparison of the methods without analyzing differences across domains. For example, there is no discussion about which methods are better suited for dermatology versus ophthalmology. Almost all discussion is very general, without any specific insights related to medical applications. This diminishes the value of using 12 datasets, as the conclusions drawn are not substantially different from what could be obtained from a single dataset.\n\n    - The analysis in Section 4.2 is poorly organized. There is no clear structure, with the discussion jumping from evaluation metrics (e.g., \"Curse of shot-based group evaluation\") to re-sampling methods, MixUp, two-stage training, and even self-supervised learning in a seemingly random fashion. Many claims are also not supported by data. The overall takeaway from the experimental section is unclear, and I did not gain any insights on how to design better models.\n\n    - In Section 4.1, there is inconsistency in the training strategies used: some methods use a unified training strategy, while others use the one specified in the original paper (e.g., SAM, Line 306), with no explanation for this discrepancy.\n\n    - There are issues with the tables, such as Table 2, where it is unclear what methods like ERM, cRT, and LWS represent, as they are not referenced properly. Additionally, Section 3.2.3 does not fully align with the table.\n\n    - The categorization of methods is confusing. The authors categorize methods into three types—class re-sampling, information augmentation, and module improvement—but later mention that re-sampling and MixUp are used in many methods, making the classification in Tables 2/3 somewhat meaningless.\n\n    - The discussion on self-supervised learning (Line 398) appears out of place, as it is not introduced earlier.\n\n    - Similarly, the mention of OOD detection (Line 421) is abrupt and lacks context.\n\n    - The section on using an imbalanced validation dataset for checkpoint selection is unclear about its purpose. The conclusion seems to be that GCL exhibits lower fluctuations, but the reasoning and implications are not well explained. Additionally, Figure 4 lacks labels for the x and y axes, making interpretation difficult.\n\n    - Line 475 suddenly states that multi-label classification is more challenging without providing adequate context or analysis.\n\n    - Line 504 claims that \"the most advanced long-tailed learning methods no longer focus on improving a single strategy,\" but this claim is not well-supported by the preceding analysis.\n\n### Questions\n\nSee weakness sections. Some more questions below:\n\nCould you elaborate on why the results of self-supervised learning and OOD detection are relevant in this paper? They seem out of place given the main focus on long-tailed classification.\n\nWhy did the authors not include a domain-specific analysis (e.g., which methods work better for certain medical fields)? It seems like an important missed opportunity.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript proposes a unified benchmark for long-tailed learning specifically in the medical domain by consolidating several existing datasets and detailing a comprehensive pipeline for data loading, model training, and evaluation. The authors assert that this benchmark supports over 30 methods for comparative analysis. While the manuscript demonstrates potential contributions, overall clarity and justification for specific methodologies are lacking.\n\n**Major Comments:**\n1. **Motivation:** The reasons for focusing on long-tailed challenges in medical imaging are inadequately articulated. The manuscript fails to convincingly differentiate between long-tailed issues in medical imaging versus conventional contexts. More insight is necessary to justify the need for distinct methodologies in medical circumstances.\n   \n2. **Dataset Contribution:** The claim of utilizing 12 datasets is weakened by the reliance on MedMNIST, from which 7 datasets are derived. This raises questions about originality and why users should prefer this benchmark over already established datasets like MedMNIST. The uneven experimental methods between multi-class and multi-label datasets are concerning, compounded by a lack of depth in the analysis of multi-label results.\n\n3. **Code Contribution:** There is no code provided for verification, nor is there a link to an anonymous GitHub repository. The description of the code structure appears basic, lacking innovative details. This absence of practical accessibility contradicts expectations for transparency in this research community.\n\n4. **Experimental Analysis:** The analysis reveals several critical shortcomings:\n   - A lack of domain-specific insights, as no discussions delineate method performance distinctions in different medical fields.\n   - Poor organization in Section 4.2, making it challenging to grasp the analysis’s aim and conclusions.\n   - Inconsistent training strategies being employed without justification.\n   - Tables lack adequate context and clarity regarding methodologies.\n   - Sudden assertions regarding multi-label challenges and self-supervised learning seem irrelevant and poorly integrated into the overall narrative.\n\n**Minor Comments:**\n- The acronym definitions should be clarified upon first use.\n- Figures necessitate axis labels for better comprehension.\n- Several claims are presented without sufficient empirical support.\n\n**Summary Paragraph:**\nIn evaluating the manuscript against the four TMI editorial criteria, its significance is recognized through the attempt to standardize long-tailed learning evaluation in medical imaging, yet its innovation is hindered by the reliance on previously established datasets. Evaluation methods lack rigor and depth, especially in addressing domain-specific nuances, which undermines the paper's reproducibility potential. While the paper has strengths in proposing a benchmarking framework, it falls short in delivering comprehensive insights and justifications for its approaches.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a unified benchmark for long-tailed learning in medical image classification, combining multiple existing datasets and providing an end-to-end pipeline for data loading, model training, and evaluation. The benchmark reportedly supports more than thirty methods and analyses their performance. The paper aims to facilitate standardized comparisons in medical long-tailed learning. The overall presentation is adequate, though the motivation, dataset originality, and experimental insight are limited.  \n\n---\n\n**Major Comments**  \n1. **Motivation and Scope:** The justification for studying long-tailed problems specifically in medical imaging is weak. The paper does not convincingly demonstrate how long-tailed learning in medical tasks fundamentally differs from conventional domains or warrants specialized methodologies. The analyses performed do not yield domain-specific insights, reducing the perceived relevance of the medical focus.  \n\n2. **Dataset Contribution:** The dataset composition limits originality. Of the twelve datasets, seven derive from MedMNIST, and the rationale for creating a new benchmark instead of using existing ones is not well articulated. The imbalance between multi-class and multi-label datasets (9/3) is noted, and the handling of multi-label data appears superficial, giving the impression of inclusion for completeness rather than substantive study.  \n\n3. **Code Availability and Design:** Absence of code or repository link undermines reproducibility. The textual description of the code structure is generic and lacks meaningful modular innovations. Compared to other frameworks, the design appears simplistic and of limited technical contribution.  \n\n4. **Experimental Analysis and Interpretation:** The evaluation lacks depth and organization. Cross-domain insights (e.g., dermatology vs. ophthalmology) are missing. Section 4.2 mixes topics such as metrics, re-sampling, and MixUp without clear logical flow. Key tables and figures lack clarity, and several claims are presented without empirical support. Method categorization is inconsistent, and sections on self-supervised learning, OOD detection, and imbalanced validation appear disconnected or unexplained.  \n\n---\n\n**Minor Comments**  \n- Clarify notations for methods such as ERM, cRT, and LWS in tables.  \n- Ensure alignment between text descriptions (Section 3.2.3) and tables.  \n- Improve labeling in Figure 4 (axes missing).  \n- Maintain consistent training strategies across compared methods.  \n- Several abrupt statements (e.g., on multi-label difficulty, advanced methods) should be substantiated.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The paper addresses an important problem—benchmarking long-tailed learning in medical imaging—but its added value over existing efforts remains uncertain.  \n**Innovation:** The integration of multiple datasets is practical but insufficiently novel. Limited methodological or architectural contribution is evident.  \n**Evaluation:** The experimental design is extensive but weak in interpretive depth and cross-domain analysis. Organization issues reduce clarity.  \n**Reproducibility:** Currently low due to missing code and incomplete methodological transparency.  \n\n---\n\n**Decision Recommendation:** **Major Revision** — The benchmark concept is potentially valuable, but stronger motivation, improved organization, domain-specific analysis, clearer presentation, and verified code release are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors introduce the problem of long-tailed medical image classification and challenges in the field. Then they develop MONICA which is a package to benchmark various methods ranging from different loss functions, augmentations, etc on the benchmark datasets across medical image classification tasks. They provide an overview of datasets and methods and experiment results on the datasets. The authors additionally share learnings and observations.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- A good overview of the datasets curated for this work\n- important contribution of decoupling the codebase\n- A good overview of the method approaches\n- practically useful to AI researchers in medical imaging\n\n### Weaknesses\n\n- It would help to expand the benchmark datasets and bring in a canonical set for a field such as Camlyon for Pathology, etc. WILDS (medical subset) is a great example of a dataset to bring in to this benchmarking codebase\n- Resnet-50 is used as a backbone but the community has generally moved on to more complex backbones such as ConvNext / Swin or foundation model backbones for different datasets. \n- Generally the community uses pretrained backbones rather than training the backbones from the scratch.\n- The same backbone is used for every task for fairness but generally a sweep over backbones would help since different modalities and tasks require different approaches\n- Top-1 accuracy is an in appropriate metric for model selection in imbalances settings and AUROC, AUPRC, F1 should be used\n- error bars are missing in experiments\n- More thorough error analysis\n- Clearer articulation of novel insights\n- Better connection to clinical relevance\n- More detailed ablation studies\n\n### Questions\n\n- Are their any key trends that you'll observed across the board to narrow down the design space for the future across the general task space? The results are not convincing in any one direction across the board on tasks and methods\n- Do you'll think stronger backbones can help learn better features?\n- Did you'll consider trying complex augmentation techniques such as AugMix or even learned augmentations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript addresses the challenges of long-tailed medical image classification and introduces MONICA, a benchmarking package designed to evaluate various methodologies, including different loss functions and augmentations, on established medical image classification datasets. The authors present overviews of the datasets, methods, and experimental results, along with valuable observations and insights gained throughout their research.\n\n**Major Comments:**\n1. The benchmark datasets could be expanded to include canonical sets, such as Camelyon for pathology and the medical subset of WILDS, to enhance the benchmarking codebase.\n2. Although ResNet-50 is utilized as a backbone, the field has largely transitioned to more sophisticated options such as ConvNext or Swin models. Consideration of these alternatives is warranted.\n3. The use of pretrained backbones is prevalent in the community; therefore, relying solely on training from scratch may not align with current best practices.\n4. Employing the same backbone across all tasks for fairness may overlook the benefits of exploring a range of backbones, as various modalities and tasks might require tailored approaches.\n5. The reliance on top-1 accuracy as the model selection metric is inappropriate in imbalanced settings; metrics such as AUROC, AUPRC, and F1-score are recommended.\n6. Missing error bars in the experiments reduce the interpretability of results.\n7. The manuscript could benefit from a more thorough error analysis to elucidate areas for improvement.\n8. There is a need for clearer articulation of novel insights derived from the study.\n9. A stronger connection to clinical relevance would enhance the practical implications of the research.\n10. More comprehensive ablation studies would provide better insight into the contributions of individual components of the proposed methods.\n\n**Minor Comments:**  \n- The manuscript would benefit from clearer structure and presentation of figures.\n- Review language for typographical errors and clarify notation and acronyms.\n\n**Summary Paragraph:**  \nThe study presents a significant effort in benchmarking medical image classification methods, showcasing innovation through the development of MONICA. However, there are weaknesses regarding dataset comprehensiveness, appropriate metrics for model evaluation, and the clarity of insights. The evaluation design could benefit from broader applicability and relevance to contemporary practices in model selection. Although the manuscript contributes valuable resources to the AI medical imaging community, there are areas where reproducibility and clarity could be improved.\n\n**Decision Recommendation:**  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of long-tailed medical image classification and introduces *MONICA*, a benchmarking framework designed to evaluate various methods—including different loss functions and data augmentations—across multiple medical image classification datasets. The authors provide a structured overview of datasets, methods, and experimental results, offering insights and observations from their findings. Overall, the paper presents a practically useful tool for researchers, though aspects of experimental design and evaluation could be strengthened to enhance the work’s rigor and clarity.\n\n---\n\n**Major Comments**  \n1. **Benchmark Scope:** The benchmark could be expanded to include more canonical datasets widely recognized in the field, such as Camelyon for pathology or the medical subset of WILDS. This would improve generalizability and visibility.  \n2. **Model Backbone:** The study relies on ResNet-50 as the backbone, but more recent architectures (e.g., ConvNeXt, Swin, or foundation models) are now prevalent. Using stronger, pretrained backbones could lead to more representative and competitive results.  \n3. **Task-Specific Variation:** Employing the same backbone for all tasks ensures fairness but may not suit diverse modalities. A systematic exploration of alternative backbones would provide more insight into task-specific performance differences.  \n4. **Evaluation Metrics:** Top-1 accuracy is suboptimal for imbalanced datasets. Metrics such as AUROC, AUPRC, or F1 should be preferred for a more reliable assessment.  \n5. **Statistical Reporting:** Experimental results lack error bars, which limits assessment of robustness. A more thorough error analysis and detailed ablation studies would enhance credibility.  \n6. **Interpretation and Relevance:** The discussion could articulate novel insights more clearly and strengthen connections to clinical relevance.  \n7. **Trends and Design Insight:** The results do not show convincing trends across tasks and methods; further synthesis could help delineate design guidance for future work.\n\n---\n\n**Minor Comments**  \n- Clarify observed cross-task trends and limitations.  \n- Consider evaluating learned augmentations (e.g., AugMix) to test augmentation sensitivity.  \n- Improve presentation of numerical results for clarity.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in that it offers a unified benchmarking framework for long-tailed medical imaging tasks and a decoupled codebase beneficial for the research community. Its innovative contribution lies in integrating and standardizing evaluation approaches, though novelty in methodology is modest. Evaluation design could be improved through stronger baselines, additional metrics, and proper uncertainty quantification. Reproducibility is facilitated by open benchmarking resources but would be enhanced by including more datasets and detailed experimental documentation.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MONICA (Medical OpeN-source Long-taIled ClassifiCAtion), a comprehensive benchmark for long-tailed medical image classification. The authors address the problem of class imbalance in medical datasets where rare diseases are underrepresented, leading to poor model performance on tail classes. MONICA implements over 30 existing methods across three categories: re-sampling, information augmentation, and module improvement. The benchmark evaluates these methods on 12 medical datasets spanning 6 domains (dermatology, ophthalmology, radiology, pathology, hematology, histology, gastroenterology) with varying imbalance ratios. Key findings include: (1) methods combining multiple strategies (like GCL) outperform single-approach methods, (2) two-stage training paradigms show effectiveness, (3) self-supervised learning performs poorly on limited medical data, and (4) classifier modifications help reduce prediction bias. The authors provide a modular codebase and extensive empirical analysis to guide future research in long-tailed medical image classification.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and clarity issues**\n  - The problem definition in Section 2.1 contains notation inconsistencies, where π is defined as both a vector and individual frequencies (Page 3, lines 115-120), making the mathematical framework unclear\n  - The imbalance ratio definition ρ = n₁/nₖ lacks clear specification of whether classes are pre-sorted by cardinality, creating ambiguity in experimental setup interpretation\n  - Multi-label formulation yi ∈ {0,1}ᵏ appears disconnected from the subsequent single-label focused analysis, suggesting insufficient mathematical integration of both problem variants\n\n• **Experimental design and evaluation methodology limitations**\n  - Dataset modification approach using Pareto distribution sampling (Table 1, Section 2.2) lacks theoretical justification for why this specific distribution appropriately models medical data imbalance compared to naturally occurring long-tailed distributions\n  - Inconsistent evaluation protocols across datasets where some use original validation/test splits while others use artificially created balanced subsets, compromising fair comparison validity\n  - Missing statistical significance testing and confidence intervals across the extensive results in Tables 2-3, making it difficult to assess whether observed performance differences are meaningful\n  - Hyperparameter selection methodology not clearly described, potentially introducing optimization bias favoring certain methods over others\n\n• **Limited technical novelty and contribution scope**\n  - The work primarily represents an empirical study implementing existing methods without proposing novel algorithmic contributions to long-tailed learning (Section 3.2, Page 5)\n  - Medical domain-specific insights are superficially discussed (Page 10, lines 534-539) without deep integration of clinical knowledge that could distinguish this work from general computer vision benchmarks\n  - Codebase contribution, while valuable, lacks technical innovation beyond modular organization of existing implementations\n\n• **Insufficient analysis depth and incomplete experimental coverage**\n  - Self-supervised learning dismissal (Page 8, lines 408-427) appears premature with limited hyperparameter exploration, contradicting the comprehensive evaluation claims\n  - Multi-label classification analysis (Page 9, lines 486-510) lacks depth despite being identified as more challenging, with insufficient investigation of label co-occurrence effects\n  - Missing ablation studies on key components like data augmentation strategies and loss function combinations that could provide actionable insights for practitioners\n\n## Suggestions for Improvement\n\n• **Enhance mathematical rigor and notation consistency**\n  - Clearly define the ordering assumption for classes and standardize notation throughout Section 2.1, ensuring π is consistently defined as either vector or scalar quantities\n  - Provide theoretical justification for the imbalance ratio metric and its relationship to medical diagnosis difficulty gradients\n  - Develop unified mathematical framework that seamlessly integrates both multi-class and multi-label formulations with clear problem boundaries\n\n• **Strengthen experimental methodology and statistical rigor**\n  - Justify the choice of Pareto distribution for dataset modification through comparison with real-world medical data distribution analysis or provide alternative sampling strategies\n  - Implement consistent evaluation protocols across all datasets using the same train/validation/test splitting methodology to ensure fair comparisons\n  - Add statistical significance testing with confidence intervals and multiple random seeds to validate the robustness of performance claims across Tables 2-3\n  - Provide detailed hyperparameter selection methodology including search spaces and optimization procedures for reproducibility\n\n• **Develop novel contributions beyond empirical evaluation**\n  - Propose new evaluation metrics specifically designed for medical long-tailed scenarios that account for clinical importance of different disease classes\n  - Integrate medical domain knowledge through hierarchical learning architectures or clinically-informed loss functions rather than treating this as generic computer vision\n  - Design novel algorithmic components that address medical-specific challenges like rare disease detection and diagnostic confidence calibration\n\n• **Deepen analysis and expand experimental comprehensiveness**\n  - Conduct thorough ablation studies on self-supervised learning with extensive hyperparameter tuning and medical-specific augmentation strategies before dismissal\n  - Provide comprehensive multi-label classification analysis including label co-occurrence pattern effects and specialized evaluation metrics beyond mean average precision\n  - Include detailed component-wise ablation studies showing the individual contribution of different technical components (augmentation, loss functions, sampling strategies) to overall performance",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThe manuscript presents MONICA (Medical OpeN-source Long-taIled ClassifiCAtion), a comprehensive benchmark aimed at addressing the challenges of class imbalance in medical image classification for rare diseases. By evaluating over 30 methods across three categories—re-sampling, information augmentation, and module improvement—MONICA assesses performance on 12 medical datasets across six domains, revealing insights into effective strategies for improving model performance on tail classes. The clarity of the mathematical formulations and the depth of experimental analysis, however, raise several concerns.\n\n## Major Comments\n1. **Mathematical formulation inconsistencies and clarity issues**\n   - Notation inconsistencies in Section 2.1, particularly with π being defined in conflicting ways, result in unclear mathematical frameworks.\n   - Ambiguity in the imbalance ratio definition ρ = n₁/nₖ due to lack of specification regarding class sorting by cardinality.\n   - The multi-label classification formulation appears disconnected from the single-label analysis, indicating insufficient integration of both approaches.\n\n2. **Experimental design and evaluation methodology limitations**\n   - The rationale for using Pareto distribution sampling for dataset modification lacks theoretical support.\n   - Inconsistent evaluation protocols across datasets compromise the validity of comparisons due to varying test splits.\n   - The absence of statistical significance testing in results makes it challenging to assess the meaningfulness of observed performance differences.\n   - The hyperparameter selection methodology needs clearer documentation to prevent potential optimization biases.\n\n3. **Limited technical novelty and contribution scope**\n   - The study mainly implements existing methods rather than proposing novel contributions to long-tailed learning.\n   - Insights pertaining to the medical domain appear superficial and could benefit from deeper clinical integration.\n   - The codebase, while useful, does not present significant technical innovations beyond organization.\n\n4. **Insufficient analysis depth and incomplete experimental coverage**\n   - Dismissing self-supervised learning based on limited hyperparameter exploration seems premature.\n   - The analysis of multi-label classification challenges lacks depth and does not explore label co-occurrence effects.\n   - Key components such as data augmentation strategies and loss function combinations lack thorough ablation studies.\n\n## Minor Comments\n- Ensure notation consistency across Section 2.1.\n- Provide clearer definitions related to medical diagnosis difficulty gradients.\n- Include detailed hyperparameter selection strategies for reproducibility.\n- Clarify statistical testing methods in comparison tables.\n\n## Summary Paragraph\nThis manuscript presents significant work by introducing MONICA as a benchmark for long-tailed medical image classification, addressing an important challenge in the field. However, issues regarding the clarity of mathematical formulations, experimental design, lack of novel contributions, and insufficient analysis depth hinder its overall significance, innovation, evaluation, and reproducibility. Improvements in these areas are essential to elevate the contribution of this work beyond empirical evaluation.\n\n## Decision Recommendation\nMajor revision is recommended to address the outlined concerns and enhance the overall quality of the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MONICA* (Medical OpeN-source Long-taIled ClassifiCAtion), a comprehensive benchmark designed to address class imbalance in medical image classification tasks. The authors evaluate over 30 existing long-tailed learning methods—spanning re-sampling, information augmentation, and module improvement—across 12 datasets from six medical domains. The study reports that combination strategies and two-stage training paradigms outperform single methods, while self-supervised approaches perform weakly on limited medical data. The paper includes an open-source modular codebase and extensive results. Overall, the manuscript is ambitious and well-organized but exhibits issues concerning methodological rigor, clarity in mathematical formulation, and limited technical novelty.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity** – Notation inconsistencies in Section 2.1 (e.g., π defined as both vector and scalar) obscure the problem’s formal definition. The imbalance ratio ρ lacks clear ordering assumptions, and the integration between multi-label and single-label formulations is incomplete.  \n2. **Experimental design and evaluation** – The use of Pareto-distributed sampling for dataset modification is insufficiently justified and may not reflect real-world data. Evaluation protocols vary across datasets, impairing fair comparison. Statistical significance, confidence intervals, and detailed hyperparameter selection procedures are absent, raising concerns about robustness and reproducibility.  \n3. **Technical novelty and contribution scope** – The study is primarily empirical, without proposing new algorithmic methods. Discussion of medical domain–specific implications remains superficial, and the codebase represents a compilation of existing methods rather than an innovation.  \n4. **Depth of analysis and coverage** – The dismissal of self-supervised learning is based on limited exploration, and the multi-label analysis lacks depth. Key component ablations (e.g., augmentation, loss design) are missing, reducing actionable insight.  \n\n**Minor Comments**  \n- Clarify mathematical notation and define all variables consistently.  \n- Explain the rationale for dataset sampling choices.  \n- Ensure uniform data splitting across tasks.  \n- Expand hyperparameter documentation for transparency.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe benchmark has potential significance as a unified testing ground for long-tailed medical image classification, promoting reproducibility through publicly released code. However, its innovation is limited because it synthesizes rather than extends existing methods. Evaluation design and reporting lack statistical rigor, and reproducibility is hindered by incomplete methodological details. The study’s strength lies in breadth and accessibility, while weaknesses involve insufficient theoretical depth and missing clinical contextualization.  \n\n**Decision Recommendation**  \n**Recommendation:** *Major Revision.* The paper contributes valuable benchmarking infrastructure, but substantial revisions are needed to improve mathematical clarity, methodological justification, and analytical completeness.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \nThe manuscript, *MONICA: Benchmarking on Long‑Tailed Medical Image Classification*, introduces a large‑scale, standardized benchmark and modular codebase integrating over 30 methods for long‑tailed medical image classification across 12 datasets spanning six medical specialties.  \nWhile the topic concerns medical image analysis, the core contribution is the establishment of a *benchmarking framework*—not a new imaging methodology or algorithm. IEEE TMI typically prioritizes methodological innovations in imaging science (e.g., new reconstruction techniques, modeling, or learning theory), whereas benchmark papers are better aligned with venues emphasizing reproducibility or empirical evaluation (e.g., MIA, MedIA, Computer Vision–focused conferences).  \n**→ ⚠️ Potentially below the TMI novelty threshold; somewhat outside the core scope of IEEE TMI.**\n\n**2. Novelty & Contribution Level**  \nThe novelty lies in integrating existing long‑tailed learning strategies into a unified codebase rather than proposing a new algorithmic formulation or theoretical insight. The design and analysis are useful for the community but not scientifically novel at the methodological level expected by TMI.\n\n**3. Technical and Experimental Rigor**  \nThe technical execution appears careful:\n- Broad inclusion of 30 methods with explicit taxonomy (re‑sampling, augmentation, module improvement).  \n- Experimental consistency (shared backbones, identical hyperparameters).  \n- Reasonably described datasets, imbalance construction, and metrics.  \nHowever, reproducibility and data governance could be more explicit—e.g., IRB or license details for reused datasets, public code availability timeline, and statistical uncertainty estimates (no standard deviations or confidence intervals are provided).\n\n**4. Clarity and Presentation**  \nThe manuscript is clearly structured, grammatically sound, and includes detailed tables of dataset statistics and results. Figures are informative, but an abundance of tables and acronym‑heavy text reduce readability for non‑specialists.\n\n**5. Ethical and Reproducibility Compliance**  \nPublicly available datasets are used, but the paper should explicitly mention adherence to data privacy, anonymization, and IRB considerations. Code release is claimed but not yet verified, which is essential for a benchmark paper.\n\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe paper presents MONICA, a unified benchmark and modular codebase for long‑tailed medical image classification (LTMIC). It consolidates 30 existing long‑tailed learning strategies, evaluates them across 12 datasets from six medical domains, and provides comparative analyses regarding resampling, augmentation, and classifier calibration effects. The authors aim to standardize evaluation and guide future methodological development in LTMIC.\n\n**2. Strengths**\n- Comprehensive coverage of existing long‑tailed approaches under consistent experimental settings.  \n- Valuable empirical insights into the behavior of existing methods on diverse medical datasets.  \n- Modular, open‑source codebase encouraging reproducible research.  \n- Clear documentation of dataset properties and benchmark design.\n\n**3. Weaknesses**\n- Limited methodological novelty; primarily an engineering and benchmarking effort.  \n- No new algorithm, model, or theoretical framework proposed.  \n- Limited statistical rigor—no uncertainty quantification across repeated runs.  \n- Sparse discussion of clinical translation or task‑specific implications for medical imaging practice.  \n- Incomplete treatment of ethical/IRB and data‑governance details.\n\n**4. Major Comments**\n1. **Scope alignment:** Clarify how this benchmark advances *methodological* understanding rather than purely empirical evaluation. For TMI, the authors could formalize new evaluation metrics, calibration theory, or analytical findings that generalize beyond the benchmark itself.  \n2. **Novelty enhancement:** Consider introducing a principled methodological contribution—e.g., a new metric or model calibration theory for long‑tailed medical learning—to elevate beyond a resource paper.  \n3. **Experimental rigor:** Add variance/standard deviation across runs to assess statistical significance.  \n4. **Ethical compliance:** Explicitly confirm dataset licenses, patient anonymization, and IRB approvals for all public datasets.  \n5. **Reproducibility:** Provide a DOI‑linked, version‑controlled release of the MONICA codebase and pre‑defined data splits.  \n6. **Clinical relevance:** Discuss how long‑tailed imbalance impacts diagnostic performance and how MONICA supports clinical translation (e.g., rare‑disease detection).  \n\n**5. Minor Comments**\n- Improve figure readability with larger fonts and clearer legends.  \n- Standardize terminology (e.g., “Multi‑class (MC)” vs. “Single‑label”).  \n- Proofread for minor grammatical errors (“vita important” → “vitally important”).  \n- Include references in consistent IEEE style.  \n- Provide ablation or runtime comparison for reproducibility.\n\n**6. Overall Recommendation**  \n**Reject (Out of Scope / Low Novelty)** – While technically competent and potentially valuable as a community benchmark, the work does not introduce methodological innovations or theoretical insights that meet the novelty criteria of *IEEE Transactions on Medical Imaging*. The paper may be better suited to a conference or journal focused on computational benchmarking or applied medical AI.\n\n**7. Confidence Level**  \n**4 / 5** – High confidence in the assessment based on familiarity with long‑tailed learning and IEEE TMI standards.\n\n---\n\n**Suggestions for Elevation to TMI Standards**\nTo reach TMI publishability, the authors could:\n- Introduce a formal mathematical framework for quantifying imbalance effects or classifier bias specific to medical imaging.  \n- Propose a new training objective or generalization bound tailored to long‑tailed medical datasets.  \n- Demonstrate domain‑adaptive extensions to imaging modalities (e.g., 3‑D reconstruction, quantitative imaging biomarkers).  \n- Incorporate clinical validation demonstrating improved detection of rare diseases in real‑world settings.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript *MONICA: Benchmarking on Long‑Tailed Medical Image Classification* presents a large‑scale benchmark and modular codebase that unifies over 30 existing long‑tailed learning methods across 12 datasets spanning six medical domains. Its goal is to standardize evaluation in long‑tailed medical image classification and guide future research. The paper is well written, clearly structured, and technically comprehensive. However, it focuses on benchmarking rather than proposing a new imaging methodology, which may place it outside the core scope and novelty expectations of *IEEE Transactions on Medical Imaging (TMI)*.  \n\n---\n\n**Major Comments**  \n1. **Scope Alignment:** The paper’s primary contribution is empirical benchmarking rather than methodological advancement. The authors should clarify how MONICA supports deeper methodological understanding relevant to imaging science, such as through novel evaluation criteria or analytical findings.  \n2. **Novelty Enhancement:** The work integrates existing algorithms but lacks new theoretical or algorithmic insight. Introducing a new metric, calibration analysis, or theoretical model of class imbalance effects could raise the contribution to TMI’s standards.  \n3. **Experimental Rigor:** Add statistical measures (e.g., variance, standard deviation) across runs to establish the significance of performance differences.  \n4. **Ethical and Governance Details:** Explicitly state dataset licenses, anonymization procedures, and IRB compliance for all reused datasets.  \n5. **Reproducibility:** Ensure DOI‑linked, version‑controlled release of the MONICA codebase and predefined dataset splits to verify claims of openness and replicability.  \n6. **Clinical Relevance:** Discuss how long‑tailed learning influences diagnostic reliability, particularly for rare conditions, and how the benchmark might translate to clinical use.  \n\n---\n\n**Minor Comments**  \n- Enlarge figure fonts and clarify legends for readability.  \n- Standardize terminology (e.g., “Multi‑class (MC)” vs. “Single‑label”).  \n- Correct minor grammatical errors and typographical issues.  \n- Ensure consistent IEEE reference formatting.  \n- Consider reporting ablation or runtime comparisons for completeness.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe benchmark offers significant practical value by unifying diverse methods and datasets, promoting standardized evaluation and open science in medical image classification. However, its innovation resides in integration and design, not in new imaging methodology. The evaluation is broad but lacks statistical uncertainty measures, and reproducibility depends on the timely public release of the code and data splits. While the manuscript’s clarity and organization are strong, its limited methodological novelty and partial ethical documentation constrain its alignment with TMI’s focus on innovation in imaging modeling and theory.  \n\n---\n\n**Decision Recommendation**  \n**Reject (Out of Scope / Low Novelty)** – Despite solid engineering and community utility, the paper does not meet the methodological innovation threshold typical for *IEEE TMI*. It may be better suited for venues emphasizing reproducible benchmarking or applied computer vision in medicine.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript titled *MONICA: Benchmarking on Long-Tailed Medical Image Classification* presents a large-scale, standardized benchmark and a modular codebase that integrates over 30 methods for long-tailed medical image classification across 12 datasets representing six medical specialties. While the work addresses an important area of medical image analysis, its primary contribution lies in establishing a benchmarking framework rather than presenting a new imaging methodology or algorithm, which is typically the focus of submissions to IEEE TMI.\n\n**Major Comments:**\n1. **Scope alignment:** Clarify how this benchmark contributes to methodological understanding rather than solely serving empirical evaluation purposes. The authors should consider formalizing new evaluation metrics or analytical findings that extend beyond the benchmark framework.\n2. **Novelty enhancement:** Introduce a methodological contribution, such as a new metric or model calibration theory, to enhance the manuscript's novelty and elevate it beyond a resource-oriented paper.\n3. **Experimental rigor:** Include statistical measures such as variance or standard deviation across runs to assess significance.\n4. **Ethical compliance:** Confirm adherence to dataset licenses, patient anonymization, and obtain IRB approvals for all utilized datasets.\n5. **Reproducibility:** Provide a DOI-linked, version-controlled release of the MONICA codebase alongside predefined data splits for transparency.\n6. **Clinical relevance:** Discuss how long-tailed class imbalance affects diagnostic performance and how MONICA facilitates clinical applications, particularly in detecting rare diseases.\n\n**Minor Comments:**\n- Improve the readability of figures with larger fonts and clearer legends.\n- Standardize terminology, particularly differentiating between “Multi-class (MC)” and “Single-label”.\n- Correct minor grammatical errors (e.g., “vita important” should be “vitally important”).\n- Ensure references follow a consistent IEEE style format.\n- Consider adding ablation or runtime comparisons to support reproducibility.\n\n**Summary Paragraph:**  \nThe manuscript presents a comprehensive benchmarking framework and modular codebase for long-tailed medical image classification, achieving significant empirical insights. However, it lacks methodological novelty, falling short of the innovation criteria typically upheld by IEEE TMI. While the technical execution is sound, concerns regarding reproducibility, statistical rigor, and ethical considerations warrant attention. Addressing these areas could enhance the paper's significance and impact.\n\n**Decision Recommendation:**  \n**Reject (Out of Scope / Low Novelty)** – Although the manuscript is technically competent and offers valuable contributions as a benchmark, it does not fulfill the novelty expectations for *IEEE Transactions on Medical Imaging*. The work would be more suited to a venue focused on computational benchmarking or applied medical AI.\n\n**Confidence Level:**  \n**4 / 5** – I express high confidence in this assessment based on expertise in long-tailed learning and familiarity with IEEE TMI standards. \n\n**Suggestions for Elevation to TMI Standards:**  \nTo enhance publishability, the authors could:\n- Develop a formal mathematical framework for quantifying imbalance effects or classifier bias specific to medical imaging.\n- Propose a new training objective or generalization bounds applicable to long-tailed medical datasets.\n- Explore domain-adaptive extensions for various imaging modalities, such as 3-D reconstruction and quantitative imaging biomarkers.\n- Incorporate clinical validation to demonstrate improved rare disease detection in practical settings.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Huimin Lu",
      "Lie Ju",
      "Peibo Duan",
      "Siyuan Yan",
      "Xiaodan Xing",
      "Yang Nan",
      "Yukun Zhou",
      "Zongyuan Ge"
    ],
    "url": "pdfs/iclr.cc-2025-conference_42d1ec064be79233cea3b21fed2023b04f20381e.pdf",
    "remote_url": "https://openreview.net/pdf/42d1ec064be79233cea3b21fed2023b04f20381e.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Precision Medicine; Patient-Specific Segmentation; Out-of-Distribution Patient Adaptation"
    ],
    "abstract": "Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}ersonalized}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the selected part-aware prompt, we propose a distribution-similarity-based retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^{\\text{2}}\\text{SAM}$ improves the performance by $\\texttt{+} 8.0$% and $\\texttt{+} 2.0$% mean Dice score within two patient-specific segmentation tasks, and exhibits impressive generality across different domains, $\\textit{e.g.}$, $\\texttt{+} 6.4$% mIoU on the PerSeg benchmark. Code will be released upon acceptance.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe article addresses challenges in personalized treatment within modern precision medicine, particularly in the context of medical image segmentation. The key issues it aims to solve include:\n\n1.\tPatient Variability: There is considerable variability among different patients, which complicates the segmentation of tumors and critical organs in medical images.\n2.\tLimited Annotated Data: Many existing segmentation algorithms rely on large amounts of annotated training data. However, personalized treatment often encounters a shortage of such data for individual patients, making it difficult to train models effectively.\n\nTo overcome these obstacles, the authors propose a new approach formulated as an in-context segmentation problem, leveraging the promptable segmentation mechanism of the Segment Anything Model (SAM). Their method, named P2SAM (Part-aware Personalized Segment Anything Model), allows for seamless adaptation to new, out-of-distribution patients using only one-shot patient-specific data.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The manuscript is clearly expressed and presents the research in a logical and structured manner, although some sentences are lengthy and could benefit from simplification to enhance readability.\n2. The paper presents substantial qualitative results and includes experiments across multiple datasets, demonstrating a considerable amount of work.\n\n### Weaknesses\n\n1. Representation: The overall logic of the paper appears problematic to me. The title and introduction emphasize \"Precision Medicine,\" yet the writing primarily focuses on highlighting the general applicability of the proposed method. Numerous examples and results are presented from the natural image domain, while the content related to \"Precision Medicine\" is notably limited, which may cause confusion for readers.\n\n2. The motivation for the \"Part-aware Prompt Mechanism\" is unclear: what is the reasoning behind this approach, and how does it address challenges in medical tasks? The architecture diagram is also based on natural image applications, leaving it unclear how the proposed method tackles issues specific to precision medicine. Additionally, there is no discussion on how this approach handles different modalities in medical imaging, which the paper should address.\n\n3. Method: Several aspects require further clarification and enhancement. \n\n     a. Firstly, the decision to use only a single negative point per cluster raises concerns regarding its sufficiency. A more robust approach would involve utilizing multiple negative points to enhance model generalization.\n\n     b. Secondly, while the method is designed for patient-specific segmentation, it raises concerns about its application to multi-segmentation tasks. A discussion on how the part-aware prompt mechanism could adapt to scenarios involving multiple segmentations would improve the methodology's applicability.\n\n     c. Additionally, the manuscript relies on 2D segmentation, which requires numerous points for effective performance. This issue remains unaddressed, and the choice not to utilize native 3D models is not sufficiently justified. Given the advantages of 3D models in capturing spatial relationships and providing more comprehensive context in medical imaging, exploring their potential application in this study would strengthen the methodology.\n\n4. Experiments: Several areas require clarification and enhancement.\n\n     a. Firstly, it is unclear whether the number of clusters set for K-means is consistent across different datasets. A detailed explanation of how the clustering parameters are determined for each dataset would improve the robustness of the results and ensure comparability.\n\n     b. Secondly, the comparison methods lack the inclusion of the latest medical-related benchmarks. Integrating more recent studies or state-of-the-art approaches would provide a more comprehensive evaluation of the proposed method's performance. This would help contextualize the results within the current landscape of medical imaging segmentation research.\n\n5. The paper lacks comparisons with state-of-the-art (SOTA) methods. Numerous improvements to SAM tailored for medical imaging have been proposed, yet many of them are omitted in this work’s comparative experiments. Furthermore, several segmentation baselines widely used in the medical domain are not referenced, which casts doubt on the demonstrated effectiveness of the proposed method.\n\n### Questions\n\n1. Is the proposed method truly aimed at addressing \"Precision Medicine,\" as suggested in the title and introduction, or is it intended as a more general approach applicable to both natural image and medical imaging domains? These are fundamentally different focuses, and clarity on this point is essential.\n\n2. What is the motivation behind the proposed method, and how does it address some of the fundamental challenges in the medical imaging field, such as unclear lesion boundaries and significant variations across different imaging modalities? Clarity on how these issues are tackled is crucial.\n\n3. Since the method claims to address patient-specific segmentation, it must consider the significant variability in characteristics across different diseases, such as liver cancer, pancreatic cancer, and rectal cancer. How does the proposed method ensure effectiveness when dealing with patients suffering from various diseases with distinct features?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript addresses challenges in personalized treatment within modern precision medicine, specifically focusing on medical image segmentation. It aims to tackle patient variability and the scarcity of annotated data, proposing a novel approach called P2SAM (Part-aware Personalized Segment Anything Model). This approach utilizes the Segment Anything Model (SAM) to facilitate adaptation to new patients with minimal data. Although the manuscript is well-structured, it presents substantial issues that impact its clarity and overall contribution.\n\n### Major Comments\n1. **Representation Issues**: The paper emphasizes \"Precision Medicine\" in the title and introduction, but much of the content relates to general applications of the method, primarily featuring examples from the natural image domain. This inconsistency may confuse readers regarding the paper's focus.\n  \n2. **Clarity of Motivation**: The rationale for the \"Part-aware Prompt Mechanism\" remains unclear. The architecture is based on natural images, and there is a lack of discussion on how this approach addresses challenges in medical tasks or different imaging modalities.\n\n3. **Methodological Concerns**: \n   - The decision to use a single negative point per cluster is insufficient; utilizing multiple negative points could enhance generalization.\n   - The application to multi-segmentation tasks needs elaboration, particularly concerning the part-aware prompt mechanism.\n   - The reliance on 2D segmentation without justification for not utilizing 3D methods diminishes the study’s methodological robustness.\n\n4. **Experimental Clarity**: \n   - Consistency in the number of K-means clusters across datasets is unclear, and an explanation of parameter selection is necessary.\n   - Inclusion of recent medical benchmarks in comparison methods is lacking, which affects the comprehensiveness of the evaluation.\n\n5. **Comparative Analysis**: The manuscript omits comparisons with numerous state-of-the-art methods tailored for medical imaging, raising concerns about the demonstrated effectiveness of the proposed approach.\n\n### Minor Comments\n- Sentence length could be simplified in several areas for better readability.\n- There are ambiguities in figures and notation that should be clarified.\n- Minor typographical errors need correction.\n\n### Summary Paragraph\nIn summary, the manuscript’s significance lies in its attempt to address key challenges in personalized medical image segmentation. However, the lack of clarity regarding its focus—precision medicine versus general applicability—diminishes its innovative potential. While the methodological design introduces a new approach, the issues surrounding reproducibility and insufficient comparative analysis with state-of-the-art benchmarks present considerable weaknesses. \n\n### Decision Recommendation\nMajor revision is recommended to enhance clarity, address methodological concerns, and improve the overall rigor of the analysis presented in the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses challenges in personalized treatment within precision medicine, focusing on patient-specific medical image segmentation. The proposed method, P2SAM (Part-aware Personalized Segment Anything Model), is presented as an in-context segmentation approach leveraging the promptable mechanism of the Segment Anything Model (SAM). It aims to adapt efficiently to new patients using only one-shot data. The paper is clearly structured and presents substantial experimental work across several datasets, though the writing could be more concise.\n\n---\n\n**Major Comments**  \n1. **Focus and Scope Misalignment:** The title and introduction emphasize “Precision Medicine,” but much of the paper concentrates on the general adaptability of the proposed approach, including examples and figures from natural image applications. The relevance to precision medicine is insufficiently demonstrated and may confuse readers.  \n2. **Motivation and Relevance of the Part-aware Prompt Mechanism:** The rationale behind this mechanism and its specific advantages for medical imaging tasks are unclear. The architecture diagram draws from natural image examples, providing little insight into adaptation across medical imaging modalities.  \n3. **Methodological Clarifications:**  \n   - The use of a single negative point per cluster may be inadequate; multiple negative points could support better generalization.  \n   - The applicability of the part-aware mechanism to multi-segmentation tasks is not discussed.  \n   - The restriction to 2D segmentation is insufficiently justified; exploring or explaining the exclusion of native 3D models would improve rigor.  \n4. **Experimental Design:**  \n   - It remains unclear whether clustering parameters (e.g., number of K-means clusters) are consistent across datasets.  \n   - Comparisons omit several recent and relevant state-of-the-art (SOTA) methods in medical imaging. Inclusion of these would better contextualize the reported performance.  \n5. **Comparative Evaluation:** The experimental results lack benchmarking against prominent medical imaging baselines and recent SAM-based improvements for clinical applications, limiting the strength of the performance claims.\n\n---\n\n**Minor Comments**  \n- Some sentences are long and could be simplified for better readability.  \n- Clarify terms and symbols in figures; ensure consistent terminology between text and diagrams.  \n- Include a brief discussion on the handling of different imaging modalities.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper proposes an interesting adaptation of SAM for patient-specific segmentation, potentially contributing to personalized treatment workflows. However, its *significance* is reduced by unclear alignment with precision medicine goals. The *innovation* is moderate but underexplained due to limited motivation for the part-aware mechanism. The *evaluation* lacks comprehensive and up-to-date comparisons, and several critical methodological details are missing, which also weakens *reproducibility*. Overall, while promising in direction, the work requires substantial clarification and strengthening of its medical relevance.\n\n---\n\n**Decision Recommendation:** **Reject** (Rating: 1, Confidence: 5)",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors target patient-specific segmentation by proposing a new segmentation pipeline: (1) obtaining multiple point-prompts from the part-aware prompt mechanism, and (2) feeding these point-prompts to a prompt-based segmentation network, such as SAM. The authors also propose a similarity-based refinement to control the number of prompts found during step 1. Main experiments were conducted on the NSCLC and CVC-ClinicDB datasets where the proposed method achieves the state-of-the-art performances. Further experiments were conducted to show the method's performance against tracking algorithms and on the one-shot segmentation task.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The paper is well-written and easy to follow.\n2. The experiments are thorough, covering various important and related topics. \n3. The proposed refinement strategy on the number of prompts is conceptually easy and seems effective.\n\n### Weaknesses\n\n1. Unclearness in writing: (a) The motivation for Table 3 is unclear. It overlaps with Table 1 and 2 and brings little additional information. (b) For the baseline methods: direct-transfer and fine-tune, looks like no prompts are provided during the evaluation stage. Can the authors verify this is true?\n2. The contribution on the Part-aware Prompt Mechanism is unclear. Although the authors have demonstrated the effectiveness of the overall method, it is unclear if the improvements are from the part-based sampling strategy or merely because of having multiple prompts.\n3. (Minor) The application range of the method is limited to patients with multi-exams.\n\n### Questions\n\n1. To demonstrate the effectiveness of the Part-aware Prompt Mechanism, could the author compared to a modified version of PerSAM where instead of selecting \"two points with the highest and lowest confidence values\", the authors may select the top K and bot K' points where K and K' matches P^2SAM. In this way, the authors can demonstrate that points found by the Part-aware Prompt Mechanism are more effective. \n2. Since authors have shown better one-shot segmentation performance, I am curious to see if the authors can show their method's performance beyond patients with multi-exams. For example, for some medical segmentation datasets without patients with multi-exams, can the author randomly select one image-mask pair as the prior and test the model's performance on other pairs.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel segmentation pipeline aimed at patient-specific segmentation. This pipeline consists of obtaining multiple point-prompts through a part-aware prompt mechanism and utilizing these prompts in a prompt-based segmentation network, such as SAM. The authors conduct comprehensive experiments on the NSCLC and CVC-ClinicDB datasets and report state-of-the-art performance. Additionally, the manuscript explores the method's efficacy in comparison to tracking algorithms and its application in a one-shot segmentation task.\n\n**Major Comments:**  \n1. **Clarity of Writing:** \n   - The motivation for Table 3 is not well-defined, as it appears to overlap with Tables 1 and 2, offering minimal additional insight.\n   - There is ambiguity regarding baseline methods such as direct-transfer and fine-tune, particularly relating to prompt usage during evaluation. Verification from the authors is needed on this aspect.\n   \n2. **Contribution Clarity:** \n   - The impact of the Part-aware Prompt Mechanism remains unclear. While the overall method's effectiveness is demonstrated, it is uncertain whether improvements arise from the part-based sampling strategy or simply from employing multiple prompts.\n\n3. **Application Range:** \n   - The methodology's applicability is predominantly restricted to patients with multiple exams, indicating a limitation in generalizability.\n\n**Minor Comments:**  \n- The proposed refinements regarding prompt quantity are conceptually straightforward and appear effective, although the practical application is limited. \n\n**Summary Paragraph:**  \nIn relation to TMI editorial criteria, the manuscript showcases significant potential in terms of innovation and evaluation. It presents a method that pushes the boundaries of current capabilities in segmentation, albeit with limitations in clarity and the scope of application. While the experiments are rigorously conducted, the rationale for certain choices, such as in Table 3 and the baseline evaluations, detracts from full reproducibility. The need for a clearer demonstration of the Part-aware Prompt Mechanism's effectiveness and potential performance beyond multi-exam patients is noted as a key area for improvement.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a patient-specific medical image segmentation approach using a two-step pipeline. First, multiple point-prompts are derived via a part-aware prompt mechanism; second, these prompts are input to a prompt-based segmentation model such as SAM. A similarity-based refinement is introduced to control the number of generated prompts. The method is evaluated primarily on the NSCLC and CVC-ClinicDB datasets, where it achieves state-of-the-art performance. Additional experiments assess its tracking-related and one-shot segmentation capabilities. Overall, the paper is well written and organized, with clearly described experiments and motivated objectives.\n\n**Major Comments**  \n1. **Clarity and Redundancy in Reporting:** The motivation and distinct contribution of Table 3 are unclear, as its content appears to overlap with Tables 1 and 2 and adds limited new insight.  \n2. **Baseline Configuration:** For the baseline methods (“direct-transfer” and “fine-tune”), it seems that no prompts are used during evaluation. The authors should confirm whether this is accurate, as it affects the validity of the comparisons.  \n3. **Attribution of Improvements:** The individual contribution of the *Part-aware Prompt Mechanism* is not fully disentangled. The observed performance gains could stem from multiple prompts rather than the part-aware strategy itself.  \n4. **Experimental Scope:** The method’s applicability appears confined to patients with multiple examinations. Exploring performance in datasets without multi-exam patients would strengthen generalizability.\n\n**Minor Comments**  \n- The description related to Table 3 could be revised for conciseness and clearer motivation.  \n- Clarify notation and ensure all acronyms are defined at first use.  \n- Minor comment: explicitly state any datasets or patient conditions assumed in the experiments.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe proposed work addresses a relevant problem and provides an intuitively effective refinement strategy. Its main strength lies in a practical framework and careful evaluation design. However, aspects of methodological novelty and attribution of improvements require further clarification. While the experiments are comprehensive and reproducible in principle, certain ablation studies (e.g., isolating the part-aware mechanism) would enhance the evaluation’s strength. The work is significant for patient-specific segmentation but somewhat narrow in scope due to dependency on multi-exam data.\n\n**Decision Recommendation**  \n**Recommendation:** **Major Revision** — The paper has solid potential but requires clearer justification of methodological novelty, improved presentation of overlapping tables, and clarification of baseline settings.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis study introduces a segmentation model called P2SAM which enables efficient, personalized medical image segmentation based on one-shot patient-specific data, improving segmentation performance across different patients and domains.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n1. The work introduces a \"part-aware prompt mechanism\" that leverages one-shot, patient-specific data for segmentation without additional fine-tuning. This feature is particularly suited to precision medicine applications where data annotation is minimal.\n\n2. Unlike standard SAM applications, P2SAM integrates a distribution-similarity-based retrieval approach that optimizes the selection of prompts, enhancing segmentation performance and reducing ambiguities.\n\n3. P2SAM is demonstrated to be effective across a variety of medical and natural image datasets, making it versatile for both patient-specific and general segmentation tasks.\n\n4. Quantitative Improvements: The model shows notable improvement over existing methods (PerSAM and Matcher), with significant gains in Dice scores and mIoU in various datasets, proving the robustness of the approach in handling out-of-distribution medical images.\n\n5. The paper presents extensive comparisons across multiple datasets and includes qualitative results that illustrate the model’s adaptability and performance in challenging segmentation tasks.\n\n### Weaknesses\n\n1. Complexity and Computational Cost: The proposed method, while innovative, involves complex modules (e.g., retrieval approach for optimal prompt selection) that may increase computational demands, potentially impacting usability in real-time clinical applications.\n\n2. Limited Generalization to Other Modalities: Although versatile within specific settings, the method's performance in more diverse medical imaging modalities (e.g., MRI or ultrasound) remains unexplored.\n\n3. Ambiguity in Module Contributions: The study could benefit from additional ablation studies to further clarify each component's role, such as the part-aware prompt mechanism versus the distribution-similarity-based retrieval.\n\n4. Dependence on SAM Backbone: While P2SAM leverages SAM effectively, it is still heavily reliant on SAM’s architecture. Any significant updates to SAM or its successors may require substantial adaptation of P2SAM.\n\n### Questions\n\n1. Scalability to Other Imaging Tasks: Could P2SAM be effectively adapted to other domains, such as pathological imaging, or expanded to handle 3D volumetric data with higher efficiency?\n\n2. Integration with SAM 2: The authors briefly mention SAM 2. How would the proposed retrieval mechanism and prompt system adapt to SAM 2's expanded capabilities in video and sequential data processing?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a segmentation model named P2SAM aimed at enhancing personalized medical image segmentation using one-shot patient-specific data. This model improves segmentation outcomes across diverse patients and domains. Overall, the study is clear and informative, with a solid contribution to the field of precision medicine.\n\n**Major Comments:**  \n1. **Innovation and Contribution:** P2SAM introduces a “part-aware prompt mechanism” that utilizes one-shot patient-specific data without requiring additional fine-tuning. This is essential for applications in precision medicine where data annotation is often limited.  \n2. **Performance Evaluation:** The model employs a distribution-similarity-based retrieval approach that optimizes prompt selection, leading to improved segmentation performance and reduced ambiguities compared to standard SAM applications.  \n3. **Versatility:** P2SAM is validated on various medical and natural image datasets, showing efficacy in both patient-specific and general segmentation tasks.  \n4. **Quantitative Results:** Significant improvements over existing methods (PerSAM and Matcher) are reported, with notable gains in Dice scores and mIoU across datasets, demonstrating robustness with out-of-distribution medical images.  \n5. **Comparative Analysis:** Extensive comparisons and qualitative results included in the paper further highlight the model’s adaptability and effectiveness in challenging tasks.\n\n**Minor Comments:**  \n- The complexity and computational cost associated with P2SAM could impact usability in real-time clinical settings due to the sophisticated modules involved.  \n- The method’s generalization to other imaging modalities (e.g., MRI, ultrasound) requires further exploration.  \n- Additional ablation studies are necessary to clarify the contributions of various components, such as the part-aware prompt mechanism and distribution-similarity retrieval.  \n- The dependency on the SAM architecture warrants caution, as any significant changes to SAM could necessitate modifications to P2SAM.\n\n**Summary Paragraph:**  \nThe study meets the TMI editorial criteria with significant innovation presented through P2SAM. The evaluation demonstrates notable improvements over existing methods, enhancing reproducibility across different datasets. However, challenges such as computational demands and the need for broader modality generalization are factors that could affect the study's real-world applicability and scalability. \n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *P2SAM*, a segmentation framework designed for efficient and personalized medical image segmentation using one-shot, patient-specific data. The study aims to improve cross-patient and cross-domain segmentation performance by integrating a part-aware prompt mechanism and a distribution-similarity-based retrieval strategy. Overall, the paper is clearly written, and the proposed method appears technically sound with convincing experiments and comparisons.\n\n**Major Comments**  \n1. **Methodological Complexity and Computational Cost:** While innovative, the proposed architecture incorporates several complex components—particularly the retrieval mechanism for optimal prompt selection—which may impose substantial computational overhead and limit practical use in time-sensitive clinical contexts.  \n2. **Generalization to Diverse Modalities:** The study demonstrates broad applicability but does not explore performance across truly heterogeneous medical modalities such as MRI or ultrasound, leaving generalizability uncertain.  \n3. **Clarity on Module Contributions:** The paper would benefit from deeper ablation studies to clarify the relative impact of the part-aware prompt module and the distribution-similarity-based retrieval component.  \n4. **Dependence on SAM Backbone:** P2SAM’s reliance on the SAM architecture may constrain adaptability; any major revisions in SAM or newer versions could require significant reengineering, raising concerns about long-term sustainability.  \n\n**Minor Comments**  \n- It would be useful to discuss computational efficiency metrics, such as inference time per case, to contextualize the practical cost.  \n- The authors briefly mention SAM 2 but do not explore how the proposed mechanisms could transfer to sequential or video-based segmentation; elaboration would improve completeness.  \n- Clarify whether patient-specific adaptation requires additional memory or storage resources beyond standard SAM implementations.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in advancing personalized segmentation with minimal annotation. Its innovation lies in combining prompt-based adaptation with distribution-aware retrieval, which is demonstrated through extensive evaluations on both medical and natural image datasets. The results show strong quantitative improvements over comparable methods, indicating solid evaluation rigor. However, reproducibility could be strengthened with clearer documentation of module contributions and computational constraints. Despite some open questions about generality and practicality, the study offers a meaningful step toward personalized medical image analysis.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial clarifications regarding computational efficiency, component contributions, and generalization are needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes Part-aware Personalized Segment Anything Model (P2SAM) for patient-specific medical image segmentation. The method addresses challenges of large patient variability and limited annotated data by formulating patient-specific segmentation as an in-context learning problem. P2SAM introduces a part-aware prompt mechanism that clusters foreground features from one-shot prior data into multiple parts using k-means++, then selects multiple-point prompts based on cosine similarity between part-level features and target image features (Equations 1-2, Figure 3). A distribution-similarity-based retrieval approach using Wasserstein distance determines the optimal number of parts for each case (Figure 5, Section 3.2). The method can be integrated into promptable segmentation models like SAM and SAM 2 without fine-tuning. Experiments on two patient-specific tasks show improvements of +8.0% and +2.0% mean Dice score, with +6.4% mIoU on PerSeg benchmark (Tables 1-2, Table 4).\n\n## Weaknesses\n\n• **Mathematical formulation lacks rigor and clarity**\n  - Equation 1 uses imprecise notation where ◦ represents \"mask selection\" but the dimensionality change from FR ∈ Rh×w×d to Ff_R ∈ Rnf×d is not clearly defined, making the operation ambiguous\n  - The clustering procedure description states \"cluster Ff_R with k-mean++ into n parts\" but provides no mathematical formulation for how centroids {Pc_R}n_c=1 are computed from clustered features\n  - Cosine similarity computation in Equation 2 lacks proper indexing notation and the relationship between similarity maps and prompt selection is not mathematically specified\n\n• **Insufficient experimental validation and comparison scope**\n  - Limited baseline comparisons exclude recent medical SAM variants like MedSAM-2, UniverSeg, and other domain-specific methods that are more relevant than general few-shot segmentation methods like PANet\n  - Cross-domain evaluation lacks statistical significance testing and confidence intervals across all reported improvements in Tables 1-2, undermining the reliability of claimed performance gains\n  - Ablation studies in Tables 6-8 test only limited hyperparameter ranges and fail to explore sensitivity to clustering initialization, similarity threshold selection, and retrieval range boundaries\n\n• **Methodological limitations and unclear technical choices**\n  - The retrieval approach assumes foreground features follow the same distribution (Section 3.2, Figure 5) but provides no theoretical justification or empirical validation of this assumption across different medical imaging modalities\n  - Part-aware clustering relies on SAM's encoder features without analyzing what semantic information these features capture or why k-means++ clustering would produce meaningful anatomical parts\n  - The method for selecting negative prompts differs between natural and medical images (Section 3.2) but lacks principled justification for this domain-specific treatment\n\n• **Limited scope and generalizability concerns**\n  - Evaluation focuses primarily on two specific medical tasks (NSCLC and polyp segmentation) with limited diversity in anatomical structures, imaging modalities, and pathological conditions\n  - The approach assumes availability of high-quality prior annotations but doesn't address robustness to annotation errors or suboptimal reference masks that are common in clinical settings\n  - No analysis of computational overhead or clinical workflow integration compared to existing methods, limiting practical applicability assessment\n\n## Suggestions for Improvement\n\n• **Enhance mathematical rigor and notation clarity**\n  - Provide explicit mathematical definition of the mask selection operation in Equation 1, clearly specifying how spatial coordinates map to feature indices and how dimensionality reduction occurs\n  - Add formal algorithmic description of the k-means++ clustering procedure and centroid computation, including convergence criteria and initialization strategy\n  - Include precise mathematical formulation linking similarity scores in Equation 2 to prompt coordinate selection, with proper indexing and coordinate system definitions\n\n• **Expand experimental validation comprehensively**\n  - Include comparisons with recent medical foundation models (MedSAM-2, UniverSeg, ScribblePrompt) and provide fair comparison protocols with consistent evaluation metrics and datasets\n  - Report statistical significance tests, confidence intervals, and effect sizes for all performance comparisons, along with multiple random seed evaluation to establish reliability\n  - Conduct extensive ablation studies covering clustering algorithm variants, similarity metrics, retrieval ranges, and sensitivity analysis across different hyperparameter settings\n\n• **Strengthen methodological foundation with theoretical justification**\n  - Provide theoretical analysis or empirical validation of the distribution similarity assumption, including experiments across diverse medical imaging modalities and pathological conditions\n  - Analyze what semantic/anatomical information SAM encoder features capture through visualization studies and correlation analysis with known anatomical structures\n  - Develop principled criteria for domain-specific prompt selection strategies, supported by theoretical framework or extensive empirical analysis across multiple domains\n\n• **Broaden evaluation scope and practical considerations**\n  - Evaluate on diverse medical tasks spanning multiple anatomical regions, imaging modalities (CT, MRI, ultrasound), and pathological conditions to demonstrate true generalizability\n  - Test robustness to annotation quality variations, including experiments with deliberately corrupted reference masks and inter-annotator variability analysis\n  - Include computational cost analysis, memory requirements, and integration complexity comparison with existing clinical workflows to support practical deployment claims",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents the Part-aware Personalized Segment Anything Model (P2SAM) aimed at enhancing patient-specific medical image segmentation. The proposed method tackles challenges related to patient variability and scarcity of annotated data by structuring the task as an in-context learning problem. Key contributions include a part-aware prompt mechanism that clusters features from prior data and a retrieval approach based on distribution similarity using Wasserstein distance. The experiments demonstrate competitive performance improvements on two specific patient tasks, marking significant advances in the domain.\n\n**Major Comments:**\n1. **Mathematical Rigor and Clarity:**\n   - The notation in Equation 1 is imprecise; the change in dimensionality is not clearly defined.\n   - Lacks a formal mathematical description of centroid computation in the clustering process.\n   - Equation 2's cosine similarity computation requires better indexing notation and a clearer link to prompt selection.\n\n2. **Experimental Validation and Comparison:**\n   - Limited baseline comparisons omit recent relevant medical SAM variants, focusing too heavily on general few-shot segmentation methods.\n   - Statistical significance testing for cross-domain evaluation is lacking, undermining performance claims.\n   - Ablation studies do not comprehensively cover hyperparameter variations nor sensitivity to initial conditions.\n\n3. **Methodological Limitations:**\n   - Assumptions regarding the distribution of foreground features across modalities lack justification or validation.\n   - Clustering relies on SAM's encoder features without adequate analysis of captured semantic information.\n   - Differences in the selection of negative prompts for natural versus medical images need principled justification.\n\n4. **Generalizability Concerns:**\n   - The evaluation is narrowly focused on specific tasks, restricting the generalizability of the findings.\n   - There is no consideration of the robustness of the method against annotation errors in clinical contexts.\n   - Computational overhead and integration into clinical workflows are not assessed, limiting practical relevance.\n\n**Minor Comments:**\n- The mathematical formulation in Equation 1 needs clarification on spatial coordinate mapping.\n- Explicit algorithmic details for k-means++ clustering should be included.\n- The text should cite statistical analysis procedures for performance metrics.\n\n**Summary Paragraph:**\nThe P2SAM manuscript shows promise in addressing significant challenges in medical image segmentation through innovation in prompt mechanisms and retrieval strategies. However, it falls short in terms of rigorous evaluation and methodological justification. The significance of the findings remains uncertain due to limited experimental validation and a narrow focus, impacting reproducibility and broader applicability in clinical settings.\n\n**Decision Recommendation:**\nMajor revision required.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Part-aware Personalized Segment Anything Model (P2SAM)* for patient-specific medical image segmentation. The proposed approach formulates patient-specific segmentation as an in-context learning problem and introduces a part-aware prompt mechanism that clusters foreground features into multiple parts using k-means++. A distribution-similarity-based retrieval method with Wasserstein distance is employed to adaptively determine the number of parts per case. The method integrates into general prompt-based architectures like SAM or SAM 2 without additional fine-tuning. Experimental results on two tasks (NSCLC and polyp segmentation) and the PerSeg benchmark demonstrate performance gains of up to +8.0% Dice and +6.4% mIoU. The manuscript is clearly motivated and technically detailed, though certain methodological and experimental aspects require clarification and strengthening.  \n\n**Major Comments**  \n1. **Mathematical clarity and rigor** – Equations and notation (particularly Equations 1–2) are imprecise. The mask selection operation and related dimensional transformations are ambiguously defined. The clustering stage lacks mathematical formulation of centroid computation, and the cosine similarity formulation does not specify how similarity maps guide prompt selection.  \n2. **Experimental validation and comparisons** – Baseline coverage is limited and omits several recent medical SAM variants (e.g., MedSAM-2, UniverSeg). Absence of statistical significance testing weakens claims of improvement, and ablation studies explore narrow parameter ranges without sensitivity analysis to initialization or retrieval settings.  \n3. **Methodological assumptions** – The distribution-similarity retrieval assumes homogeneity of foreground feature distributions without justification. Dependence on SAM encoder features and k-means++ grouping lacks theoretical or empirical analysis. The negative prompt selection strategy is inconsistently defined across domains.  \n4. **Generalizability and practicality** – Evaluation is restricted to two tasks, limiting generalization. The approach assumes perfect prior annotations without testing robustness to noisy masks. Computational efficiency and clinical integration are not evaluated, constraining assessment of real-world applicability.  \n\n**Minor Comments**  \n- Improve equation notation consistency and indexing.  \n- Clarify Figure references and cross-link equations to algorithm steps.  \n- Ensure consistent reporting of metrics and confidence intervals.  \n- Expand acronym definitions at first appearance.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is significant in targeting personalized medical image segmentation and proposes an innovative part-aware prompting framework. However, innovation is undercut by incomplete mathematical specification and limited theoretical grounding. Evaluation is insufficiently comprehensive to confirm significance, and reproducibility is hindered by missing methodological details, statistical reporting, and analysis of computational load. Addressing these issues through clearer mathematical exposition, broader benchmarks, and stronger empirical validation would greatly enhance impact and reliability.  \n\n**Decision Recommendation**  \n**Major Revision.** The concept is promising, but substantial improvement in technical rigor, experimental breadth, and reproducibility documentation is needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n✅ The manuscript presents a methodological innovation relevant to medical imaging: a new *Part‑aware Personalized Segment Anything Model (P2SAM)* for patient-specific segmentation. It focuses on algorithmic adaptation of promptable foundation models to individualized segmentation tasks in medical imaging, which is well within TMI’s methodological scope (image analysis, reconstruction, machine learning applied to imaging).\n\n2. **Novelty & Contribution Level**  \nThe paper introduces (i) a *part-aware multiple-point prompt selection* strategy and (ii) a *distribution-similarity-based retrieval* mechanism to automatically determine the optimal number of feature parts. These are nontrivial algorithmic modifications that extend SAM-type models for one‑shot, patient‑specific segmentation. While the approach leverages existing SAM backbones, the methodological framing—*in-context segmentation* for personalized medicine—is novel for TMI. The contribution surpasses mere application-level use of pre-existing networks.\n\n3. **Technical and Experimental Rigor**  \nThe manuscript is technically consistent and fairly detailed. Four medical datasets are described, with clear train/test splits, use of in‑ and out‑of‑distribution scenarios, and evaluation against multiple baselines (PerSAM, Matcher, etc.). Reproducibility appears reasonable, with code release promised upon acceptance. However, the study’s test datasets are primarily 2D slices instead of volumetric segmentation, and uncertainty or statistical significance analysis of metric improvements is missing. Ethical approval for patient imaging data reuse is not discussed explicitly.\n\n4. **Clarity and Presentation**  \nThe paper is well structured, clearly written, and follows typical IEEE TMI scientific style. Figures are illustrative, though figures adapted from natural images (to show part-level features) somewhat dilute the medical imaging focus. Some copy-editing would help with English phrasing and formatting.\n\n5. **Ethical and Reproducibility Compliance**  \nPublic datasets are used (Aerts et al., Jha et al.), but explicit mention of IRB or patient consent exceptions is absent. Reproducibility is fair but would benefit from more implementation details and open-source code confirmation.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n**1. Summary**  \nThe authors propose P2SAM, a *Part‑aware Personalized Segment Anything Model* for data‑efficient patient-specific segmentation. Building on promptable foundation models (SAM/SAM2), they cluster foreground features from a single annotated reference image into part-level representations, select multiple point prompts based on feature similarity, and determine the optimal number of parts using a distribution‑similarity (Wasserstein distance) retrieval criterion. The method requires no fine‑tuning to adapt to a new patient and can optionally use LoRA or full fine-tuning for domain adaptation. Experiments on CT (NSCLC, 4D-Lung) and endoscopy datasets demonstrate 2–8 % Dice score gains over prior SAM‑based personalization methods. Ablation studies confirm the influence of part number and retrieval scheme.\n\n**2. Strengths**\n- Addresses clinically significant challenge: data‑efficient, patient‑adaptive segmentation.  \n- Introduces a clear algorithmic advance over prompt‑based SAM approaches.  \n- Validates across multiple datasets and with different SAM backbone sizes.  \n- Potentially generalizable—works also on natural-image benchmarks.  \n- Good methodological exposition and comparison with strong baselines.\n\n**3. Weaknesses**\n- Novelty is moderate; relies heavily on existing SAM foundation models.  \n- Experiments are limited to 2D cases; volumetric and multi‑modal tests are lacking.  \n- Absence of uncertainty quantification and statistical testing.  \n- No discussion of runtime, memory, or prompt‑selection computational burden.  \n- Ethical/IRB statement missing.  \n- Some presentation issues (dense text, excessive figures from nonmedical examples).\n\n**4. Major Comments**\n1. **Scope and Validation:** Extend quantitative evaluation to fully 3D volumetric segmentations or multi‑modality tasks (e.g., MRI, PET‑CT) to confirm broader medical applicability.  \n2. **Uncertainty and Significance:** Provide statistical analysis (standard deviations or confidence intervals) and possibly case‑level comparisons to establish reliability.  \n3. **Complexity Assessment:** Discuss computational overhead introduced by part clustering and retrieval compared with single‑prompt methods, especially for clinical deployment scenarios.  \n4. **Ethics & Data Use:** Explicitly state that all datasets are publicly available and comply with institutional ethics/IRB requirements.  \n5. **Clarify Limitations:** The limitation with multiple similar objects should be emphasized in the main text (not only Appendix E), with possible mitigation strategies.  \n6. **Reproducibility:** A clear indication of code/public repository and parameter initialization details would enhance reproducibility.\n\n**5. Minor Comments**\n- Improve grammar and consistency (e.g., spacing around mathematical symbols, variable notation).  \n- Replace figures using natural images with additional medical examples to reinforce TMI relevance.  \n- Ensure tables are consistent in units and precision.  \n- Verify reference formatting (e.g., remove duplicated semicolons in list citations).  \n- Condense Appendix A SAM review; it restates known material.\n\n**6. Overall Recommendation**  \n**Major Revision** – The paper demonstrates promising methodological innovation suitable for TMI, but requires stronger medical imaging validation, explicit ethical statements, and additional quantitative rigor to meet journal standards.\n\n**7. Confidence Level**  \n4 / 5 – High confidence in evaluation and understanding of the work, though final judgment depends on additional experiments and maturity of the methodology.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *P2SAM*—a Part‑aware Personalized Segment Anything Model—for patient‑specific medical image segmentation. It adapts foundation models (SAM/SAM2) to enable individualized segmentation from minimal annotation by using part‑based feature clustering and prompt selection. The work falls squarely within TMI’s methodological scope, addressing patient‑adaptive, data‑efficient segmentation. The manuscript is technically detailed, clearly organized, and generally well written, though ethical compliance and some presentation aspects require improvement.\n\n---\n\n**Major Comments**  \n1. **Scope and Validation:** The experimental validation is limited to 2D segmentation. To substantiate clinical value and generalizability, the authors should extend experiments to full 3D volumes or additional modalities (e.g., MRI, PET‑CT).  \n2. **Statistical Reliability:** Report uncertainty estimates (e.g., standard deviations or confidence intervals) for Dice metrics and include statistical significance analysis to confirm the robustness of observed gains.  \n3. **Computational Complexity:** Quantify runtime and memory costs associated with part clustering and retrieval, and compare these against single‑prompt or simpler personalization baselines to assess feasibility for clinical deployment.  \n4. **Ethics and Data Governance:** Explicitly confirm that all datasets are publicly available and used in line with IRB or patient consent policies. An ethical statement should be added to the manuscript.  \n5. **Limitations and Applicability:** Emphasize existing limitations—particularly the handling of multiple similar objects—within the main text rather than relegating them to the appendix, and discuss possible future mitigation.  \n6. **Reproducibility:** Strengthen reproducibility by providing implementation details (parameter initialization, backbone variants) and a verifiable link or commitment for public code release.\n\n---\n\n**Minor Comments**  \n- Check grammar, spacing around equations, and unify variable notation.  \n- Replace illustrative figures drawn from natural images with medically relevant examples.  \n- Standardize table precision and units; ensure consistent reference formatting.  \n- Condense the SAM background section in Appendix A to avoid redundancy.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study tackles a clinically meaningful challenge by extending foundation models for efficient patient‑specific segmentation. Methodologically, it offers a distinct part‑aware prompting mechanism and retrieval criterion, representing a genuine though incremental innovation. Validation across multiple datasets supports the approach, but limitations in 3D evaluation and lack of statistical testing reduce evidential strength. Reproducibility is reasonable with clear dataset details, yet confirmation of open‑source code and ethical compliance remains necessary. Overall, the work is significant and well executed but requires further experimental and procedural refinement to meet TMI standards.\n\n---\n\n**Decision Recommendation:** **Major Revision.** The contribution is promising and methodologically sound but needs expanded validation, statistical rigor, explicit ethics statements, and enhanced reporting for full acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a methodological innovation relevant to medical imaging: the *Part-aware Personalized Segment Anything Model (P2SAM)*, designed for patient-specific segmentation. It adapts promptable foundation models to individualized medical imaging tasks, aligning well with IEEE TMI’s methodological focus on image analysis and machine learning applications in imaging.\n\n**Major Comments:**\n1. **Scope and Validation:** The evaluation should include fully 3D volumetric segmentations and multi-modality tasks (e.g., MRI, PET-CT) to confirm the broader applicability in medical settings.\n2. **Uncertainty and Significance:** Statistical analysis, including standard deviations or confidence intervals, along with case-level comparisons, is necessary to establish reliability of the results.\n3. **Complexity Assessment:** The manuscript should discuss the computational overhead associated with part clustering and retrieval in comparison to single-prompt methods, especially in clinical settings.\n4. **Ethics & Data Use:** An explicit statement confirming public availability of datasets and compliance with institutional ethics/IRB is required.\n5. **Clarify Limitations:** The limitations regarding multiple similar objects should be highlighted in the main text rather than just the appendix, accompanied by potential mitigation strategies.\n6. **Reproducibility:** Clear information regarding code availability and parameter initialization would significantly enhance reproducibility.\n\n**Minor Comments:**\n- The manuscript would benefit from improved grammar and consistency, particularly in mathematical symbols and variable notation.\n- Figures based on natural images should be replaced with additional medical examples to strengthen the paper's relevance to TMI.\n- Ensure tables display consistent units and precision.\n- Reference formatting should be double-checked to eliminate any duplicate semicolons in citations.\n- Condense the content in Appendix A regarding the SAM review, as it reiterates known material.\n\n**Summary Paragraph:**  \nThe proposed P2SAM model addresses a significant clinical challenge by enabling data-efficient, patient-adaptive segmentation. While the methodological advancements are noteworthy, the reliance on existing SAM frameworks implies moderate novelty. The experiments, primarily focusing on 2D data, lack the necessary breadth in terms of volumetric analysis and multimodal testing. Though the study exhibits reasonable reproducibility, it requires explicit ethical considerations and thorough statistical validation to fulfill IEEE TMI’s editorial criteria.\n\n**Decision Recommendation:**  \n**Major Revision** - The manuscript exhibits promising methodological innovation suitable for TMI but necessitates enhanced medical imaging validation, precise ethical statements, and increased quantitative rigor to meet the standards of the journal.\n\n**Confidence Level:**  \n4 / 5 - There is high confidence in the evaluation and understanding of the work; however, the final judgment is contingent on the execution of additional experiments and maturation of the methodology.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ],
    "url": "pdfs/iclr.cc-2025-conference_10d7d4d4e6a581994367bb8103dcf41b5a52cd66.pdf",
    "remote_url": "https://openreview.net/pdf/10d7d4d4e6a581994367bb8103dcf41b5a52cd66.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Single Domain Generalization for Rare Event Detection in Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Deep Learning",
      "Knowledge",
      "Rare Event Detection",
      "Out-of-distribution detection"
    ],
    "abstract": "Single Domain Generalization (SDG) addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset. In the medical diagnosis and disease detection domain, where data is often limited and events of interest are rare, deep learning (DL) models frequently exhibit suboptimal performance, leading to poor generalization across datasets. In multi-center studies, disparate data sources, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare event characteristics. This paper addresses this challenge by first leveraging a pre-trained large vision model to rank classes based on their similarity to the rare event class, allowing focused handling of the most similar class, and then integrates domain-invariant knowledge on rare event with DL to accurately classify the rare event class. By carefully incorporating expert knowledge with data-driven DL, our technique effectively regularizes the model, enhancing robustness and performance even with limited data availability. We present a case study on seizure onset zone detection using fMRI data, demonstrating that our approach significantly outperforms state-of-the-art vision transformers, large vision models, and knowledge-based systems, achieving an average F1 score of 90.2% while maintaining an overall F1 score of 85.0% across multi-center datasets.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors introduce an algorithm for generalizing to unknown domains using only training data from a single known domain, \"RareSaGE\", which proposes to integrate domain-invariant expert knowledge with neural networks for the problem of rare class classification. Their method achieves improved performance for this task compared to several neural network- and expert knowledge-based techniques.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper focuses on a lesser known, but still seemingly important problem which deserves attention. They test on a realistic dataset comprised of medical data sampled from real centers, demonstrating real life applicability.\n2. The method is seemingly fairly technically novel. The rarity quantification doesn't seem novel (they cite Li et al), but overall, the combination of expert knowledge (an old field) with modern neural network models is interesting and unusual.\n3. Improvements over the compared baselines are significant for all metrics (except maybe precision) (Table 1)\n4. The intuition behind the method is reasonable, and the formalism discussed does help with understanding this. The reasoning behind \"rationale for definition 3.1\" makes sense, i.e., explaining the challenges in working with rare classes with respect to class entropy.\n5. I think the four characterizations of a rare event make sense (not sure if these are novel or not?), although discrimination and significance may be a bit redundant.\n\n### Weaknesses\n\n**Major:**\n1. **Generally limited experiments (don't even begin until page 9):** I would suggest moving many of the less-important dataset, algorithm and method details into the supplementary, and filling the missing space with more experiments, especially those considering ablation studies, failure cases for your method, computational analyses, and others. The biggest issues are as follows.\n    1. For a method defined in such generality, the experiments are quite limited: just a case study based on one fMRI datasets. This is a big negative, because it is unclear if the method could generalize to other problems, or if it was bespoke to (or developed specifically for) this case study. This hurts the paper's suitability for ICLR, as the larger impact on machine learning is not clear to me, which would require testing on other datasets/problem settings/etc.\n    2. Moreover, while the improvements over the compared baselines are significant for all metrics (except maybe precision) the comparison/baseline models may not be sufficient:\n        - I'm not an expert in domain generalization, but are the comparison models in Table 1 really proper baselines (maybe another reviewer can chime in)? Simple training or finetuning on one domain and testing on another domain (as well as the knowledge-based systems) doesn't seem like a strong, appropriate baseline. Are there better unknown domain generalization techniques which should have been tested?\n        - Alternatively, what about comparing to one-class-classification methods, i.e. OOD/anomaly-detection methods? This is also a large body of work which seems suitable. At the very least, why weren't these discussed in the related work?\n        - why wasn't AUC presented if sensitivity and precision were also presented? This would be a better general performance metric than accuracy for accounting for class imbalance.\n\t\n2. **Lack of ablation/hyperparameter sensitivity studies:**\n    1. this method has quite a few moving parts, which each could be brittle. The more of these moving parts that there are, the more ways that the method could fail when extended to new datasets/problems. It is important to consider how changing or removing one component would affect the performance, in order to judge how reliant to algorithm is on that component. However, the paper critically lacks any ablation/sensitivity studies for such hyperparameters/settings. I will discuss potential ablation studies which could be done, but I challenge the authors to think of more. Some examples:\n        1. On pg. 4, a rare class is defined by a 2-sigma distance from the mean class entropy. Why/how was 2 sigma chosen? This is seemingly an important parameter, yet you do no ablation/sensitivity studies on the effect of different values for this (number of sigmas used to define a rare class)\n        2. Algorithm 1 is quite complex, additionally with certain steps not quantitatively/explicitly defined.\n        3. The use of PCA on clip features may have limitations, as its linearity may be too much of a restriction. Why wasn't some form of nonlinear component/degrees-of-freedom analysis tested as well?\n        3. The form of the expert knowledge in the tested scenario (Eq. 6) is quite specific. Are there viable alternatives to this formulation that could have been tested on this dataset? Also, in general, what is the feasibility for converting expert knowledge into this format?\n\t\t\t\n**Minor:**\n1. More clear, technical method details are needed in the abstract and introduction to be clear what these contributions are. For example:\n    1. In the abstract: \"This paper addresses … even with limited data availability.\" This really lacks in explicit details on how the method actually works. how is this ranking done, specifically? is it via a novel algorithm? also, what does \"focused handling\" mean? and how is domain-invariant knowledge \"integrated\"? These questions could be gleaned from the main text of the paper, but a better level of detail could still be provided in the abstract with less vague wording.\n    2. Similarly in the introduction, more technical details are needed. Your repeatedly describe the \"integration\" of domain invariant expert knowledge with deep learning, but I'm unclear from reading this what this actually explicitly describes.\n2. Misleading claims/vague wording:\n    1. In the introduction, you say \"This concept is particularly crucial in the field of artificial intelligence (AI) for medicine, where the aim is to accurately diagnose new patient cases across centers\". This is misleading. AI for medicine extends far beyond diagnosis, including tasks such as segmentation, registration, harmonization, etc… and that's just for images, not including AI for medicine beyond images.\n    2. \"In medical imaging, there are various factors …  acute disorders\" also in the introduction; what are the \"classes\" being described here? I'm not sure if this is really intra-class variability (at least since you don't explain what the classes are here), its really just various factors that can contribute to domain shift problems in medical datasets.\n    3. \"In the medical diagnosis and disease detection domain, where data is often limited … variability in rare event characteristics. \" in the abstract is phrased confusingly. in the first sentence, is performance poor on the data because of limited data, rare cases of interest, or both? Maybe say that the second sentence describes a problem which exacerbates the problem described in the first sentence.\n3. Other missing technical details:\n    1. how is feature overlap/resemblance (described in the first paragraph of Sec. 3.2 actually computed? cosine similarity of the feature vectors?\n    2. Algorithm 1 shouldn't really be labeled as an \"algorithm\" in my opinion, because many of the steps are not defined explicitly, and left up to interpretation. More details are needed.\n    3. They provided anonymous code link in Sec. 4.1 is broken. It would have been helpful to see how such a novel and relatively complex algorithm was implemented in practice.\n4. Writing/clarity issues: the paper suffers from issues in writing quality, including clarity and typos, discussed next. For example:\n    1. The paper can be a bit challenging to read, not because of technical details, but because redundant information is often provided (see for example, \"solution overview\" and \"solution details\" could certainly be shortened and combined in 3.2)\n    2. Many sentences are terse and relatively telegraphic. The paper is still understandable, but could have better flow and general writing.\n    3. There is a too-long run-on sentence in \"To improve SDG for rare events, leveraging expert knowledge such as clinical opinion on …, has immense potential\" in the introduction.\n5. Typos/formatting issues:\n    1. In the abstract: \"Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset\"\n        1. also, this is phrased confusingly. maybe say extensively studied in classification of balanced datasets?  But is having an imbalanced dataset not just kind of a triviality?\n    2. also in abstract \"integrates domain-invariant knowledge on rare event\" another typo.\n    3. Also in the intro, page 2 \"with DL using pre-trained large vision model (LVM) \"\n    4. There seems to be an issue in how in-text citations are formatted: no spacing between them, not parenthetical, etc. Makes it a bit harder to read the paper.\n    5. Why are the equations written with such tiny text? They require zooming in just to read. I'm not sure if this formatting modification is allowed for ICLR.\n\n### Questions\n\n1. Is \"single domain generalization\" a common term when describing your problem scenario? it sounds like it describes generalizing TO a single domain, not training on a single one and being able to generalize to unknown ones.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents an algorithm named \"RareSaGE\" aimed at generalizing to unknown domains using training data exclusively from a known domain, integrating domain-invariant expert knowledge with neural networks for rare class classification. The authors report improved performance compared to existing neural network and expert knowledge-based methods. Overall, the paper addresses a significant issue, though there are limitations concerning the breadth and depth of experimentation.\n\n### Major Comments\n1. **Limited Experiments**: The experiments do not begin until page 9, limiting the manuscript’s depth. Important details about the dataset, algorithm, and method could be relocated to supplementary materials to include more experiments, such as ablation studies and analyses of failure cases.\n   - The single case study based on fMRI datasets raises concerns about the generalizability of the method to other problems, impacting the relevance of the study for ICLR.\n   - The chosen comparison models in Table 1 may not establish appropriate baselines. More robust unknown domain generalization techniques and one-class classification methods could enhance the rigor of comparisons.\n   - The absence of AUC metrics is notable; presenting this would provide a more comprehensive assessment considering class imbalance.\n  \n2. **Lack of Ablation/Hypothetical Sensitivity Studies**: The algorithm’s complexity introduces several moving parts that may affect reliability. The absence of ablation studies inhibits understanding of how the method's performance is dependent on its components.\n   - The definition of a rare class based on a 2-sigma distance lacks sensitivity analysis for alternative values.\n   - The paper does not fully define the complex steps in Algorithm 1, and there may be limitations in linear techniques like PCA applied to clip features.\n   - The specific formulation of expert knowledge presented raises questions about the testability of alternative approaches.\n\n### Minor Comments\n1. The abstract and introduction require clearer, more technical details about the method's contributions.\n2. Certain claims and terminology, particularly regarding the scope of AI in medicine, are misleading and require clarification.\n3. Technical details such as feature overlap computation are not defined clearly, and the anonymous code link provided is broken.\n4. The writing quality, including clarity and flow, can be improved; redundancy and typos abound, affecting readability.\n\n### Summary Paragraph\nThe manuscript presents a method that shows potential in addressing a recognized problem within the machine learning community. However, its significance is tempered by limited experimentation and questionable baselines, affecting its reproducibility. There is some innovation in combining expert knowledge with neural networks, yet the lack of rigorous evaluation and clarity detracts from its overall contribution. Improvements in method presentation and a deeper exploration of variants would benefit the manuscript.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RareSaGE*, an algorithm designed to generalize to unseen domains when only a single known domain is available for training. It targets the issue of rare class classification by combining domain-invariant expert knowledge with neural network modeling. Experiments on real medical imaging data indicate improved results compared to both neural and expert knowledge–based baselines. The paper addresses a relevant and underexplored topic, is technically interesting, and demonstrates real-world applicability, though the overall experimental scope and clarity of presentation remain limited.\n\n---\n\n**Major Comments**  \n1. **Limited Experimental Evaluation:** The experimental section begins relatively late (page 9) and focuses only on one fMRI dataset. This narrow evaluation raises concerns about generalizability to other domains or modalities and limits broader impact. The dataset, algorithmic, and methodological details could be moved to supplementary material to allow space for deeper experimentation, including ablation, computational, and failure analyses.  \n   - The choice of baselines may be insufficient: it is unclear whether the compared methods in Table 1 represent strong domain-generalization candidates. Additional baselines such as one-class or anomaly detection approaches should be considered or at least discussed.  \n   - Reporting of performance metrics could be improved: including AUC would better represent class-imbalanced performance than accuracy alone.  \n\n2. **Missing Ablation and Sensitivity Studies:** The approach includes several components and hyperparameters that might affect robustness, yet none have been evaluated systematically. Examples include:  \n   - The use of a 2‑sigma rule to define rare classes—its selection and influence remain unexplored.  \n   - Algorithm 1 contains vaguely defined steps; specific quantitative definitions are required.  \n   - Reliance on linear PCA for feature dimensionality might constrain performance; nonlinear alternatives could be examined.  \n   - The formulation of expert knowledge (Eq. 6) seems specific; its adaptability to other forms should be discussed.\n\n---\n\n**Minor Comments**  \n- **Abstract and Introduction:** Provide clearer descriptions of the method’s technical details (how ranking, “focused handling,” and integration of expert knowledge operate).  \n- **Terminology and Clarity:** Some phrases are misleading or overly broad (e.g., “AI for medicine” limited to diagnosis). Clarify references to “classes” in discussions of intra‑class variability.  \n- **Missing Details:** Specify how feature resemblance is computed, and note that the anonymous code link is broken.  \n- **Writing and Structure:** Merge redundant sections (“solution overview/details”), improve sentence flow, and correct run‑on sentences and typos. Ensure readable equation formatting and proper citation spacing.\n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study tackles an important and relatively underexplored problem—rare‑class generalization from a single domain—and introduces a conceptually appealing synthesis of expert knowledge with neural models. The approach appears innovative, and the presented results show promise. However, limited experimental breadth, lack of ablation and sensitivity analysis, and incomplete methodological clarity restrict confidence in the claims and reproducibility. Improving the evaluation scope and technical precision would strengthen the paper’s overall impact.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents RareSaGe, a novel approach for SGD in detecting rare events in medical imaging. First, this method handles the most similar classes by using a pre-trained large vision model (LVM) to rank classes based on their similarity to the rare event class, which aims to effectively classify the rare event. Then, it further integrates expert knowledge with deep learning to enhance robustness and address challenges related to limited data. Experiments conducted on multi-center datasets for seizure onset zone (SOZ) detection in fMRI data demonstrate that RareSaGe achieves high generalization performance, significantly outperforming other models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The manuscript is written in clear English and is relatively easy to follow.\n2. The experimental results demonstrate the effectiveness of the proposed framework.\n3. The motivation for developing the method makes sense\n\n### Weaknesses\n\n1. The title is focused on MEDICAL IMAGING, but the data is too singular, validated only on fMRI data and limited to two categories (two centers).\n2. The introduction to the dataset in Section 4.1 and the Supplementary Material is relatively limited.\n2. There are missing many important ablation studies, such as validating the effectiveness of the two types of expert knowledge on the method.\n\n### Questions\n\n1. The title is focused on MEDICAL IMAGING, but the data is too singular, validated only on fMRI data and limited to two categories (two centers).\n2. In Section 4.1, the link to the dataset is unavailable; it was not previously a publicly available dataset. The introduction to the dataset in Section 4.1 and the Supplementary Material is relatively limited. \n3. Additionally, there are no visual results of the dataset presented in the manuscript, leading to insufficient feasibility of the results.\n4. In Section 3.4.2 EXPERT KNOWLEDGE ON RS-fMRI, it is mentioned that \"These locations can be extracted employing established image processing algorithms.\" The visual results obtained from these algorithms for specific locations should be presented.\n5. There are many important ablation studies missing, such as validating the effectiveness of the two types of expert knowledge on the method.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript introduces RareSaGe, an innovative approach designed for stochastic gradient descent (SGD) to detect rare events in medical imaging. By utilizing a pre-trained large vision model (LVM) to rank class similarities and integrating expert knowledge with deep learning, this method aims to enhance classification accuracy despite the challenge of limited data. The experiments conducted on multi-center datasets for seizure onset zone (SOZ) detection in fMRI show that RareSaGe notably outperforms other existing models, indicating its potential contribution to the field.\n\n**Major Comments:**\n1. The manuscript's title emphasizes MEDICAL IMAGING, yet the dataset is narrowly focused, validated only on fMRI data and limited to two categories from two centers.\n2. The introduction to the dataset in Section 4.1 and Supplementary Material is insufficiently detailed.\n3. Important ablation studies are missing, particularly regarding the validation of the two types of expert knowledge utilized in the method.\n\n**Minor Comments:**\n- The title should reflect the limited scope of the dataset to avoid misleading readers.\n- Section 4.1 lacks a link to the dataset; it was not previously available publicly.\n- The absence of visual results from the dataset in the manuscript limits the feasibility assertions.\n- In Section 3.4.2, visual results from established image processing algorithms should be included to illustrate the extracted locations.\n  \n**Summary Paragraph:**\nIn reviewing the manuscript against TMI editorial criteria, the significance of RareSaGe in the context of rare event detection is evident, particularly due to its novel integration of expert knowledge. However, its innovation is somewhat constrained by the dataset's limited size and the lack of comprehensive evaluation, particularly in ablation studies. While the reproducibility of results based on the current dataset is promising, it cannot be fully assessed without broader validation.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RareSaGe*, a novel strategy for stochastic gradient descent (SGD) aimed at detecting rare events in medical imaging. The method leverages a pre-trained large vision model (LVM) to rank classes by similarity to a rare event class and integrates expert knowledge to strengthen model robustness under limited data conditions. Experiments on multi-center datasets for seizure onset zone (SOZ) detection in fMRI demonstrate promising generalization and superior performance compared with other approaches. Overall, the paper is clearly written, logically organized, and presents a well-motivated problem.  \n\n**Major Comments**  \n1. **Scope of Validation** – The application focus suggested by the title (“medical imaging”) is broad, yet the evaluation is restricted to fMRI data from only two centers, which limits generalizability across other imaging modalities.  \n2. **Dataset Description and Availability** – The dataset used in Section 4.1 is insufficiently described. The provided link is unavailable, and details in the Supplementary Material are minimal. More transparent dataset access and clearer documentation are needed.  \n3. **Ablation and Expert Knowledge Validation** – Key ablation analyses are missing, particularly to confirm the contribution of the two forms of expert knowledge incorporated in the model. Without these, it is difficult to assess how much each component contributes to the reported gains.  \n4. **Visualization and Algorithmic Outputs** – The paper mentions extraction of anatomical locations using “established image processing algorithms” (Section 3.4.2), yet no visual examples or representative outputs are shown. Including visualizations would substantiate the feasibility of the approach.  \n\n**Minor Comments**  \n- Supplement Section 4.1 could include basic dataset statistics and representative images.  \n- Figures or qualitative examples would enhance clarity and interpretability.  \n- The text is generally clear, though a more balanced clarification of data sources and centers would benefit readers.  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of *significance*, the study addresses an important challenge—rare event detection in medical data—but its impact is constrained by limited dataset diversity. The proposed integration of LVM-based similarity ranking and expert knowledge is *innovative*, yet the lack of ablation analyses limits evaluation of novelty strength. The *evaluation* is solid within the given dataset but would need broader validation to confirm robustness. Regarding *reproducibility*, incomplete dataset accessibility and missing implementation details reduce transparency.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper discusses Single Domain Generalization (SDG) in the context of rare event classification, particularly in medical diagnosis, where limited data and imbalanced datasets pose significant challenges. The authors propose a method that utilizes a pre-trained large vision model to rank classes by their similarity to rare events, enabling focused classification. By integrating domain-invariant expert knowledge with data-driven deep learning, the approach improves model robustness and performance, even with scarce data. A case study on detecting seizure onset zones using fMRI data shows that this method achieves an average F1 score of 90.2%.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis paper addresses an interesting and important problem: how to leverage large vision models (LVMs) to tackle rare diseases, which involve imbalanced, infrequent cases and domain shifts. While there are many SDG methods available, the authors claim that none have been specifically applied to rare cases in medical data. It appears that the authors are addressing a new problem.\n\n### Weaknesses\n\nProblematic Settings\n\n1. While there are currently no methods addressing SDG in the context of limited data and rare diseases, this setting appears to combine elements of SDG with few-shot imbalanced classification. If a SDG method is sufficiently robust, it could potentially be integrated with existing imbalanced learning or rare disease fine-tuning techniques to effectively address this issue. However, the authors lack experiments to validate this point, which raises questions about the relevance of this particular setting in the field. The authors have the opportunity to tackle a significant challenge: how to integrate large vision models (LVMs) for rare disease classification, taking into account that rare diseases can vary in size, may be in-domain or out-of-domain, and may be either imbalanced or balanced, thereby providing a more comprehensive framework.\n\nLimited Discussions and Comparisons of Results\n\n2. The authors have not compared their approach with existing SDG and imbalanced learning methods, which diminishes the convincing nature of their experiments and calls into question their claim of being state-of-the-art (SOTA).\n\n### Questions\n\nPlease see my comments above\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript investigates Single Domain Generalization (SDG) as applied to rare event classification in medical diagnosis, addressing the significant challenges posed by limited and imbalanced datasets. The authors propose a novel method that leverages a pre-trained large vision model to rank classes based on their similarity to rare events, thereby enhancing focused classification. By merging domain-invariant expert knowledge with data-driven deep learning, the approach aims to improve model robustness and performance, even under data scarcity. A case study analyzing seizure onset zones using fMRI data demonstrates an impressive average F1 score of 90.2%.\n\n**Major Comments:**\n1. **Problematic Settings:** Although no methods exist that address SDG specifically in the context of limited data and rare diseases, the proposed setting appears to blend elements of SDG and few-shot imbalanced classification. A robust SDG method could incorporate existing imbalanced learning or rare disease fine-tuning techniques to effectively address these challenges. However, the lack of experimental validation raises concerns about the relevance of the current approach.\n   \n2. **Limited Discussions and Comparisons of Results:** The authors do not compare their proposed method to existing SDG and imbalanced learning approaches. This absence of comparisons undermines the credibility of their findings and raises doubts about their claim to being state-of-the-art (SOTA).\n\n**Minor Comments:**\n- Ensure clarity in defining terms related to imbalanced datasets and rare diseases.\n- Consider including a table summarizing experimental results for better clarity.\n- Review grammar for typos and ensure consistent notation across the manuscript.\n\n**Summary Paragraph:**  \nIn terms of significance, the study tackles an important issue by exploring the integration of large vision models for rare disease classification, an innovative angle in the realm of SDG. However, the evaluation lacks a robust comparison to existing methods, limiting its reproducibility and generalizability. While the proposed method shows promise in performance metrics, the foundational experiments and discussions require strengthening to present a more comprehensive framework that acknowledges the variability inherent in rare diseases.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of Single Domain Generalization (SDG) in rare event classification, particularly within medical diagnosis tasks where data scarcity and class imbalance are major challenges. The authors propose leveraging a pre-trained large vision model (LVM) to rank classes according to their similarity to rare events, thereby enabling targeted classification. By integrating domain-invariant expert knowledge with data-driven learning, the method aims to enhance robustness and performance in low-data regimes. A case study on seizure onset zone detection using fMRI data reportedly yields an average F1 score of 90.2%. Overall, the paper presents an interesting attempt to combine SDG and LVMs for rare disease detection, though certain aspects of motivation, validation, and comparison require strengthening.  \n\n**Major Comments**  \n1. **Problem Setting and Validation:** While the problem—SDG under limited and imbalanced data—is compelling, it appears to merge aspects of SDG and few-shot imbalanced classification. The paper does not sufficiently justify this combined setting or validate whether the proposed approach meaningfully differs from or improves upon existing techniques in these adjacent areas. Further experiments examining integration with standard imbalanced-learning or rare-disease fine-tuning methods would better establish the necessity and relevance of the proposed framework.  \n2. **Comparative Analysis:** The manuscript lacks quantitative or qualitative comparisons with existing SDG and imbalanced learning baselines. Without such benchmarks, the claims of state-of-the-art performance remain unsubstantiated. Including comparisons with leading SDG and related rare-event methods would notably improve the empirical rigor and credibility of the results.  \n\n**Minor Comments**  \n- The manuscript would benefit from a clearer discussion delineating SDG, few-shot, and imbalanced learning settings to aid contextual understanding.  \n- Clarify whether “rare diseases” in the experiments correspond to in-domain or out-of-domain distributions.  \n- Ensure consistent terminology when describing large vision models and normalization terms throughout the text.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses a timely and significant medical AI problem, proposing an innovative integration of pre-trained visual models for SDG in rare conditions. The conceptual direction is promising, but current evidence is limited by insufficient comparison and unclear framing of novelty. Evaluation lacks breadth, and reproducibility could be improved through more detailed experimental validation and discussion of generalization boundaries. With revisions targeting these issues, the work could make a more convincing contribution.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses Single Domain Generalization (SDG) for rare event detection in medical imaging, specifically targeting seizure onset zone (SOZ) identification in fMRI data. The authors propose RareSaGe, an algorithm that integrates domain-invariant expert knowledge with deep learning to handle challenges of limited data, class imbalance, and domain shift. The method first uses CLIP to identify the \"overlap class\" most similar to the rare event class, then applies deep learning for overlap/non-overlap classification and knowledge-based quadratic optimization for rare/non-rare distinction. Expert knowledge is encoded as anatomical propositions and logical formulas specific to SOZ characteristics. Experiments on two-center fMRI datasets demonstrate average F1 scores of 90.2% for cross-center validation and 94.7% for aggregate validation, outperforming state-of-the-art vision transformers and large vision models.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and unclear definitions**\n  - Definition 3.1 for rare class identification uses class entropy thresholds (θr > θM + σθ or θr < θM - σθ) but lacks justification for the 2σ criterion (Section 3.1)\n  - The connection between Equations 4-5 and the Fischer information relationship is asserted without proper derivation (Page 5)\n  - Equation 6's logical formula κSOZ combines atomic propositions but the mapping from continuous proposition values to binary logic is not explained (Section 3.4.2)\n\n• **Limited experimental validation and evaluation concerns**\n  - Only two medical centers are used for validation, which is insufficient to demonstrate true generalizability across diverse clinical settings (Section 4.1)\n  - The \"confidence threshold tc = 0.9\" for classifier integration is mentioned but no sensitivity analysis or justification is provided (Section 3.4.3)\n  - Comparison baselines lack recent domain generalization methods specifically designed for medical imaging, limiting the strength of performance claims (Table 1)\n\n• **Unclear algorithmic details and implementation gaps**\n  - Algorithm 1 references a \"significant change in validation accuracy\" stopping criterion without defining the threshold or measurement method (Line 2)\n  - The machine selection strategy for choosing between MDL and MQO classifiers is mentioned but the actual selection process is not clearly specified (Section 3.2)\n  - The integration of DL and expert knowledge in the \"Label Predictor\" step lacks algorithmic precision regarding how conflicting predictions are resolved (Section 3.4.3)\n\n• **Insufficient analysis of expert knowledge generalizability**\n  - The claim that \"expert knowledge is inherently domain-invariant\" is not empirically validated across the different scanner types and patient populations (Section 5)\n  - The atomic propositions (p1, pg, ps, pa, pw, pv) are derived from specific literature but their stability across different imaging protocols is not demonstrated (Section 3.4.2)\n  - No analysis is provided on how variations in imaging acquisition parameters affect the expert knowledge features (Section 3.4.2)\n\n## Suggestions for Improvement\n\n• **Strengthen mathematical foundation and provide rigorous derivations**\n  - Provide theoretical justification for the 2σ threshold in Definition 3.1 through statistical analysis or cite appropriate literature\n  - Include complete derivation of the Fischer information relationship in Equations 4-5 or move detailed proof to appendix\n  - Clarify the mapping mechanism from continuous proposition values in Section 3.4.2 to binary logical operators in Equation 6\n\n• **Expand experimental validation with comprehensive baselines**\n  - Include additional medical centers (minimum 4-5) from different geographical regions and scanner manufacturers to strengthen generalizability claims\n  - Conduct systematic sensitivity analysis for the confidence threshold tc with values ranging from 0.5 to 0.95\n  - Add comparisons with recent medical domain generalization methods such as domain-adversarial training and meta-learning approaches\n\n• **Provide detailed algorithmic specifications and implementation details**\n  - Define specific numerical criteria for \"significant change in validation accuracy\" in Algorithm 1 with concrete threshold values\n  - Present a detailed flowchart or pseudocode for the machine selection strategy between MDL and MQO classifiers\n  - Specify exact conflict resolution mechanisms when DL and expert knowledge predictions disagree in the Label Predictor component\n\n• **Validate expert knowledge robustness across imaging conditions**\n  - Conduct ablation studies removing individual atomic propositions to demonstrate their individual contributions to cross-domain performance\n  - Analyze the stability of proposition values (p1, pg, ps, pa, pw, pv) across different scanner manufacturers, field strengths, and acquisition protocols\n  - Include quantitative analysis of how imaging parameter variations (TR, TE, resolution) affect expert knowledge feature extraction reliability",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript explores Single Domain Generalization (SDG) for rare event detection in medical imaging, specifically focusing on the identification of seizure onset zones (SOZ) from fMRI data. The authors introduce RareSaGe, a novel algorithm that merges domain-invariant expert knowledge with deep learning techniques to address challenges such as limited data, class imbalance, and domain shift. Empirical results on fMRI datasets demonstrate promising F1 scores of 90.2% for cross-center validation and 94.7% for aggregate validation, indicating superior performance over existing state-of-the-art methods.\n\n### Major Comments\n1. **Mathematical Formulation Inconsistencies**:\n   - Definition 3.1 introduces entropy thresholds for rare class identification but does not justify the 2σ parameter.\n   - The connection between Equations 4-5 and the Fischer information relationship is claimed but lacks derivation.\n   - The mapping from continuous proposition values to binary logic in Equation 6 is not adequately explained.\n\n2. **Limited Experimental Validation**:\n   - Validation is restricted to two medical centers, which raises questions regarding the generalizability of findings.\n   - The confidence threshold of 0.9 for classifier integration lacks sensitivity analysis and justification.\n   - Comparisons with recent medical domain generalization methods are absent, weakening performance claims.\n\n3. **Unclear Algorithmic Details**:\n   - Algorithm 1 refers to a stopping criterion based on validation accuracy changes without defining what constitutes a \"significant change.\"\n   - The process for selecting between MDL and MQO classifiers is vaguely articulated.\n   - The integration strategy for resolving conflicts in the \"Label Predictor\" step is not precisely outlined.\n\n4. **Insufficient Analysis of Expert Knowledge Generalizability**:\n   - The assumption that expert knowledge is universally valid across different scanners and patient populations lacks empirical support.\n   - Stability of derived atomic propositions across protocols is not demonstrated.\n   - There is no analysis of how variations in imaging parameters may influence expert knowledge features.\n\n### Minor Comments\n- Improve clarity by providing rigorous definitions and justifications for the criteria used within the methodology.\n- Enhance the manuscript by adding visual aids such as flowcharts or pseudocode to illustrate algorithm procedures effectively.\n- Proofread for typographical errors and ensure consistent notation throughout the manuscript.\n\n### Summary Paragraph\nThe manuscript exhibits strengths in proposing an innovative approach to a critical area in medical imaging, as highlighted by high F1 scores. However, concerns arise regarding the mathematical rigor, experimental scope, and clarity of algorithmic implementation, which hinder reproducibility. By addressing these weaknesses and enhancing the validation framework, the overall significance and impact of the work could be substantially improved.\n\n### Decision Recommendation\nMajor revision is recommended to address the aforementioned concerns related to mathematical clarity, experimental validation, algorithmic detail, and generalizability of findings.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RareSaGe*, a framework for Single Domain Generalization (SDG) in rare event detection within medical imaging, focusing on seizure onset zone (SOZ) localization in fMRI data. The method combines domain-invariant expert knowledge with deep learning to address challenges such as data scarcity, class imbalance, and domain shift. The approach leverages CLIP to identify an overlap class, performs overlap/non-overlap classification, then applies knowledge-based quadratic optimization for rare/non-rare discrimination. Expert knowledge is expressed through anatomical propositions in logical form. Experiments on two-center fMRI datasets show promising performance, with cross-center and aggregate F1 scores of 90.2% and 94.7%, respectively, outperforming strong baselines.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity** – Key definitions and equations require stronger grounding. The 2σ threshold in Definition 3.1 lacks justification; the link between Equations 4–5 and the Fisher information is asserted without derivation; and the translation of continuous proposition values to binary logic in Equation 6 is unspecified.  \n2. **Experimental validation and generalizability** – Validation on only two centers is insufficient to demonstrate SDG capability across diverse clinical domains. The fixed confidence threshold (tc = 0.9) lacks sensitivity analysis, and comparison baselines omit recent domain generalization methods tailored for medical imaging.  \n3. **Algorithmic specification and implementation details** – Algorithm 1 references an undefined “significant change in validation accuracy.” The machine selection strategy between MDL and MQO classifiers is unclear, and the integration of deep learning with expert knowledge in the Label Predictor lacks explanation regarding conflict resolution.  \n4. **Expert knowledge robustness** – The claim that expert knowledge is inherently domain-invariant is not empirically validated. The stability of atomic propositions (p1, pg, ps, pa, pw, pv) under varied imaging conditions and protocols remains untested.  \n\n**Minor Comments**  \n- Provide derivations or append references supporting Equation formulations.  \n- Add numerical thresholds or pseudocode for reproducibility.  \n- Clarify notation consistency (e.g., θr, θM) and ensure figure/table captions specify experimental conditions.  \n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses a relevant problem and introduces an innovative hybrid knowledge–deep learning model. Its conceptual novelty is moderate to high, but the methodological rigor is hampered by unclear derivations and incomplete algorithmic specification. Experimental results are promising yet limited in scope, affecting claims of generalization. Reproducibility is restricted due to missing implementation and sensitivity details.  \n\n**Decision Recommendation:** **Major Revision** — The work has potential significance but requires stronger theoretical justification, expanded experiments, and clearer methodological exposition to support its claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to IEEE TMI**  \n✅ The manuscript presents a methodological framework (“RareSaGe”) combining single-domain generalization (SDG), deep learning (DL), and symbolic expert knowledge for rare event detection in medical imaging, demonstrated on seizure onset zone (SOZ) localization from resting-state fMRI. The contribution lies at the intersection of machine learning methodology and neuroimaging – directly within the scope of *IEEE Transactions on Medical Imaging*. While the study uses a specific application (epilepsy fMRI), the proposed method (knowledge-augmented SDG) is generalizable to rare pathologies across imaging modalities.\n\n2. **Novelty & Contribution Level**  \nThe work claims *first application of SDG to rare event detection* in medical imaging and proposes a novel integration of large vision models, entropy-based classifier selection, and symbolic expert knowledge. The mathematical formulation of “rare class” via entropy and the use of hybrid machine orchestration (DL + knowledge) go beyond standard data-driven domain generalization. This is a moderately strong methodological innovation. However, novelty relative to prior “human–AI collaboration” and “knowledge-guided DL” work (e.g., physics-informed or knowledge-enhanced models) may need clearer delineation.\n\n3. **Technical and Experimental Rigor**  \nThe paper outlines a well-defined algorithmic pipeline, including formal definitions, a quantitative entropy-based rarity criterion, and experiments across independent sites. Cross-center and aggregate analyses illustrate robustness, and performance comparisons include SOTA baselines. However, clarity is occasionally compromised by overly broad theoretical exposition; certain methodological details (e.g., SVM kernel, parameter tuning, statistical testing, uncertainty quantification) should be expanded. The dataset is small but realistic for rare-condition imaging. Public code and anonymized data improve reproducibility.\n\n4. **Clarity and Presentation**  \nThe paper is generally well written, but language could be more concise. Mathematical definitions and algorithmic steps are explicit, yet Figures 1–2 are difficult to interpret without expanded captions. Some references are repetitive, and citation formatting occasionally inconsistent (e.g., missing spaces before parentheses). Section 3.2–3.4 mixes methodological and implementation details in long paragraphs; clearer separation would enhance readability.\n\n5. **Ethical and Reproducibility Compliance**  \nIRB approval and de-identification are stated. Reproducibility is strong—code availability is declared, and full parameter information is reportedly in supplementary material. Ethical standards seem fully met.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n### 1. Summary\nThe manuscript proposes **RareSaGe**, a hybrid framework for *Single Domain Generalization (SDG)* in rare event medical image classification. Using vision transformer and CLIP-based feature embeddings, class entropy defines the rare class and an “overlap class.” A dual-model orchestration—deep learning for overlap discrimination and symbolic knowledge-based quadratic optimization for rare event classification—integrates data-driven and expert-derived cues. The approach is validated on multi-center fMRI data for seizure onset zone detection, demonstrating higher F1-scores (≈90%) than domain adaptation and knowledge-only baselines, and generalizing across scanner types and age distributions.\n\n### 2. Strengths\n- Addresses a **critical gap**: generalization from a single source domain for rare pathological events.  \n- Introduces a **formally defined rarity/entropy metric** underpinning a principled algorithm.  \n- Integrates **expert anatomical and functional knowledge** in a quantifiable and interpretable manner.  \n- Demonstrates **multi-center and cross-demographic validation**, suggesting substantial translational potential.  \n- Ensures **ethical compliance and public reproducibility**—commendable for TMI standards.\n\n### 3. Weaknesses\n- **Novelty boundaries**: differentiation from existing knowledge-guided or physics-informed DL frameworks needs sharper positioning.  \n- **Limited dataset scope**: only fMRI epilepsy data; evidence of generalizability to other imaging modalities remains speculative.  \n- **Experimental detail gaps**: missing kernel specification, statistical significance tests, and uncertainty reporting (confidence intervals).  \n- **Clarity issues**: algorithmic description is verbose; clearer pseudocode/diagram structure would aid readers.  \n- **Comparisons**: Baselines mostly DL or knowledge-only; inclusion of recent hybrid or federated generalization techniques would strengthen claims.\n\n### 4. Major Comments\n1. **Clarify the methodological novelty versus prior “knowledge-enhanced” systems** such as KENN, PGNN, or expert-in-the-loop frameworks. Explain what is unique about RareSaGe’s entropy-based orchestration.  \n2. **Provide more quantitative statistical analysis** (e.g., standard deviations across folds, p-values) to assess whether observed F1 improvements are statistically significant.  \n3. **Expand reproducibility detail** in the main paper—summarize key hyperparameters, SVM configuration, and training epochs rather than deferring entirely to supplement.  \n4. **Discuss limitations and generalizability**—how would this method perform on different modalities (CT, MRI structural, histopathology)?  \n5. **Improve figure clarity**—redraw Fig. 1 with process flow labeled (feature extraction → DL → knowledge module → final fusion).  \n\n### 5. Minor Comments\n- Standardize notation and typography (e.g., “class-wise entropy,” “RareSaGe”).  \n- Correct citation formatting (inconsistent brackets, some missing commas).  \n- Ensure all acronyms are defined at first mention (e.g., QO, EKE).  \n- Simplify overly long sentences in Section 3.2–3.4 for readability.  \n- Consider a stylistic pass to align with IEEE TMI manuscript guidelines.\n\n### 6. Overall Recommendation\n**Major Revision.**  \nThe manuscript presents a *technically interesting and methodologically credible* approach addressing a meaningful challenge in medical imaging generalization. The concept of entropy-based rare class definition and hybrid knowledge–DL fusion has potential TMI-level impact, but the current version requires stronger contextualization, expanded methodological transparency, and tighter presentation before it meets TMI’s rigor and generalizability expectations.\n\n### 7. Confidence Level\n**4 / 5** – High confidence; reviewer is familiar with domain generalization and knowledge-informed imaging ML methods.\n\n---\n\n**Constructive Outlook for Authors:**  \nTo elevate the work to TMI standards, consider: (i) expanding experiments beyond a single application, (ii) articulating the theoretical novelty relative to existing human–AI and physics-informed paradigms, and (iii) tightening experimental documentation and visual presentation. These refinements would position *RareSaGe* as a generalizable methodological contribution to robust medical image analysis.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RareSaGe*, a hybrid framework designed for single-domain generalization (SDG) in rare event detection from medical images. It integrates deep learning–based feature extraction with symbolic expert knowledge to localize seizure onset zones (SOZ) from resting-state fMRI. The study aims to enhance generalization from limited domains and rare pathologies, a topic well aligned with *IEEE Transactions on Medical Imaging*. The paper is generally clear and technically solid, though the presentation could be streamlined for readability.\n\n---\n\n**Major Comments**  \n1. **Novelty Clarification:** The authors should better distinguish *RareSaGe* from existing knowledge-guided or physics-informed deep learning methods (e.g., KENN, PGNN). Explicitly articulate how the entropy-based orchestration offers conceptual or methodological advances.  \n2. **Experimental Rigor:** The statistical analysis is insufficient. Include performance variability (e.g., standard deviations, p-values) to determine whether reported F1-score improvements are statistically significant.  \n3. **Reproducibility Detail:** Key implementation details—SVM kernel choice, parameter tuning procedures, training epochs—should appear in the main text rather than only in the supplementary material.  \n4. **Generality Discussion:** The framework is demonstrated solely on fMRI epilepsy data. Discuss potential adaptation to other imaging modalities (CT, structural MRI, histopathology) and identify any anticipated challenges.  \n5. **Figure and Structure Improvements:** Revise Figure 1 to clearly depict the pipeline (feature extraction → deep model → knowledge module → fusion). Reorganize Sections 3.2–3.4 to separate methodological and implementation content.  \n\n---\n\n**Minor Comments**  \n- Standardize notation (e.g., “class-wise entropy”) and consistently capitalize *RareSaGe*.  \n- Correct citation formatting and typographical inconsistencies.  \n- Define all acronyms at first use (e.g., QO, EKE).  \n- Shorten lengthy sentences and ensure language conciseness.  \n- Align presentation with IEEE TMI formatting conventions.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe paper addresses a **significant** problem—robust generalization for rare pathological events—from a **methodologically innovative** standpoint by defining an entropy-based rarity measure and combining data-driven and symbolic reasoning. The **evaluation** demonstrates strong within- and cross-site accuracy but lacks comprehensive statistical validation. **Reproducibility** is promising due to code and data sharing, yet full transparency on hyperparameters and training settings is required. Overall, the submission is technically credible and potentially impactful but demands refinement in presentation, validation, and contextualization of novelty.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The approach is promising and compliant with ethical and reproducibility standards, but additional experimental detail, clearer comparative positioning, and improved clarity are necessary before it meets *IEEE TMI* standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces **RareSaGe**, a hybrid framework for *Single Domain Generalization (SDG)* aimed at detecting rare events in medical imaging, specifically focusing on seizure onset zone localization using resting-state fMRI. The study bridges machine learning methodology and neuroimaging, making a relevant contribution to *IEEE Transactions on Medical Imaging*. Despite its application to epilepsy fMRI, the proposed knowledge-augmented SDG method has the potential for broader applicability across various rare pathologies and imaging modalities.\n\n**Major Comments:**  \n1. **Novelty & Contribution:** The manuscript claims to be the first to apply SDG to rare event detection; however, the uniqueness in relation to prior knowledge-guided frameworks needs clearer articulation, particularly in entropy-based orchestration.\n2. **Technical Rigor:** A well-defined algorithmic pipeline is presented, but important methodological details (e.g., SVM kernel specifics, statistical testing, and uncertainty quantification) should be expanded for enhanced clarity.\n3. **Dataset Scope:** While the dataset used is appropriate for the subject matter, generalizability to other imaging modalities (CT, MRI, histopathology) remains to be established.\n4. **Clarity and Presentation:** The manuscript, although generally well written, could benefit from more concise language, and figures, particularly Figures 1–2, require clearer captions and structure.\n5. **Comparative Analysis:** Baselines are primarily focused on deep learning or knowledge-only approaches; incorporating recent hybrid or federated techniques would bolster the presented claims.\n\n**Minor Comments:**  \n- Ensure standardization in notation and typography throughout the manuscript.  \n- Adjust citation formatting for consistency (e.g., correct missing spaces and commas).  \n- Define all acronyms upon first use (e.g., QO, EKE).  \n- Simplify sentences in Sections 3.2–3.4 to enhance readability.  \n- Conduct a stylistic revision to align with IEEE TMI standards.\n\n**Summary Paragraph:**  \nIn terms of **significance**, the manuscript addresses a pressing challenge within medical imaging by providing a novel framework for generalization in rare pathological contexts. Although the **innovation** involved in integrating expert knowledge and entropy-based metrics is commendable, differentiation from existing methods must be emphasized. **Evaluation** is robust, highlighted by multi-center validation and thorough testing, yet gaps in statistical analytics persist. **Reproducibility** is strong with public code availability, yet further detail on hyperparameters and methodology is needed. Overall, while the study shows promise, enhancements in clarity, context, and methodological rigor are required.\n\n**Decision Recommendation:**  \n**Major Revision.**  \nThe manuscript presents an intriguing approach to a significant issue in medical imaging but requires refinement in contextualization, methodological transparency, and presentation to fully align with TMI standards.\n\n**Confidence Level:**  \n**4 / 5** – High confidence; the reviewer possesses a solid understanding of the domain and applicable methodologies. \n\n**Constructive Outlook for Authors:**  \nTo improve the manuscript's standing, authors should consider diversifying the experimental scope, clarifying the novelty concerning existing frameworks, and tightening the presentation and documentation of experiments. These changes will significantly enhance the contribution of *RareSaGe* to the field of medical image analysis.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Ayan Banerjee",
      "BIN XU",
      "Payal Kamboj",
      "Sandeep Gupta"
    ],
    "url": "pdfs/iclr.cc-2025-conference_6b804922fe464f4b49ffe111b1d8d47e1d6592be.pdf",
    "remote_url": "https://openreview.net/pdf/6b804922fe464f4b49ffe111b1d8d47e1d6592be.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Training Physics-Driven Deep Learning Reconstruction without Raw Data Access for Equitable Fast MRI",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Computational Imaging",
      "Fast MRI",
      "Unsupervised Learning",
      "Compressed Sensing",
      "Deep Learning",
      "Equity"
    ],
    "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Even though PD-DL offers higher acceleration rates compared to existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. One impediment for their deployment is the difficulties with generalization to pathologies or population groups that are not well-represented in training sets. This has been noted in several studies, and fine-tuning on target populations to improve reconstruction has been suggested. However, current training approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and underserved areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose CUPID for high-quality PD-DL training, using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates the goodness of the output with a compressibility-based approach, while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show that CUPID achieves similar quality compared to well-established PD-DL training strategies that require raw k-space data access, while outperforming conventional compressed sensing (CS) and state-of-the-art generative methods. We also demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden. As an approach that radically deviates from existing strategies, CUPID presents an opportunity to provide equitable access to fast MRI for underserved populations in an attempt to reduce the inequalities associated with this expensive imaging modality.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe article proposes a deep-learning reconstruction method without having access to multi-coil $k$-space data. For the full-column-rank forward operator $E_{\\Omega}$ and the under-sampled $k$-space $\\textbf{y}$, the authors assume access to $E_{\\Omega}^+ \\textbf{y}$ where $+$ denotes the Moore-Penrose inverse. Compared to having access to fully sampled $k$-space as in fully supervised reconstruction, or undersampled $k$-space as in self-supervised reconstruction, this assumption is less restrictive. The proposed method was shown to perform on par with PD-DL methods requiring $k$-space data at relatively low acceleration rates ($4\\times$).\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The data access assumption is less restrictive compared to fully- and self-supervised reconstruction methods, and the article focuses on a real-life problem, as raw k-space access is a known limitation in this field.\n2. The method is physics-based; thus, it is expected to be less prone to hallucinations compared to methods that do not utilize the forward operator.\n3. The method was tested in various settings, focusing on both retrospective and prospective undersampling.\n\n### Weaknesses\n\n1. The quantitative results table is missing the standard error of the mean, which is crucial for a fair comparison.\n2. It is incorrect to say that equispaced undersampling patterns were not explored in score-based models. In fact, the authors have already cited at least one article (\"Robust Compressed Sensing MRI with Deep Generative Priors\") that explored equispaced undersampling, discussing it not only in the appendix but also in the main text (e.g., Fig. 2, 9, 10). \n3. The generative method baseline is weak and outdated, given how rapidly state-of-the-art methods are advancing.  Several approaches published in the last 12 months can achieve better results. Including this particular method is fine, but labeling it as state-of-the-art is misleading.\n\n### Questions\n\n1. Does the clinical PI reconstruction readily allow saving the complex-valued image without any research agreements, or does it only allow the magnitude image to be saved? I am asking because it is rare to see a DICOM dataset containing anything other than magnitude images.\n2. Regardless of whether ScoreMRI is state-of-the-art, its performance appears unexpectedly low. Is it possible that it was significantly under-trained? In our experience, these models require much longer training times compared to unrolled networks, which might explain why it underperforms even at $4\\times$.\n3. The choice of perturbations seems somewhat arbitrary. Could you elaborate on this choice? Learning the perturbations simultaneously with reinforcement learning could also be a potential direction for future work.\n4. In Figure $2$, loss function, the left hand side was written as $\\mathcal{L}(\\mathbf{x}^{(k)}, \\mathbf{x_{PI}})$ but the $\\mathcal{L}_{\\text{comp}}$ on the right hand side has $\\mathbf{x}^{(m)}$ rather than $\\mathbf{x}^{(k)}$. Is this intentional or a typo? \n5. Line $151-152$, while unrolled methods achieved top positions in reconstruction challenges three years ago, it is unknown if they remain the highest performers. To the best of my knowledge, no recent, fair comparison between state-of-the-art diffusion and unrolled methods has been published, including a reader study to compare their clinical effectiveness.\n\n**Minor comments** \n\n* The fastMRI knee matrix size is $320\\times 320$, not $320\\times 368$. Please see Table 1 in DOI: 10.1148/ryai.2020190007\n* Line $42-43$, demand have shown -> demand has shown\n* Line $269$, shapes that has different intensity values -> shapes that have different intensity values\n* Line $46$, Fig.$1$ -> Fig. $1$, the space was omitted in most, if not all, references to figures, tables, and sections.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a deep-learning reconstruction method that operates without access to multi-coil \\(k\\)-space data. The authors propose an approach utilizing the Moore-Penrose inverse of the full-column-rank forward operator \\(E_{\\Omega}\\) and the under-sampled \\(k\\)-space data \\(\\textbf{y}\\). This assumption is less restrictive than those found in fully supervised or self-supervised reconstruction methods. The results indicate that the proposed method performs comparably to PD-DL techniques that require \\(k\\)-space data at low acceleration rates of \\(4\\times\\).\n\n**Major Comments:**  \n1. The assumption of data access is notably less restrictive, addressing a critical limitation in the field.\n2. The physics-based approach is expected to reduce the likelihood of hallucinations, distinguishing it from non-physics-based methods.\n3. The method underwent testing in diverse settings, assessing both retrospective and prospective undersampling.\n4. **Weaknesses:**\n   - The quantitative results table lacks the standard error of the mean, critical for comparisons.\n   - Claims regarding equispaced undersampling patterns in score-based models are incorrect as previous literature, including their own citations, has explored these.\n   - The generative method baseline is outdated, as recent advances in the field could render it insufficient; labeling it as state-of-the-art misrepresents its relevance.\n\n**Minor Comments:**  \n- The fastMRI knee matrix should be referenced as \\(320\\times 320\\), not \\(320\\times 368\\) (see Table 1 in DOI: 10.1148/ryai.2020190007).\n- Correct grammatical errors such as \"demand have shown\" to \"demand has shown\" (Lines 42-43).\n- Adjust \"shapes that has different intensity values\" to \"shapes that have different intensity values\" (Line 269).\n- Ensure consistent formatting, e.g., \"Fig. 1\" rather than \"Fig.$1$\" (Line 46).\n\n**Summary Paragraph:**  \nIn relation to the TMI editorial criteria, the manuscript demonstrates significance by addressing a practical issue in data accessibility for deep-learning reconstruction. Its innovation lies in utilizing a less restrictive assumption regarding data access. Evaluation could be improved with more detailed quantitative results, including standard errors, and by adopting a more representative baseline for comparison. Reproducibility may be questioned due to potential training inadequacies in the comparison model, ScoreMRI, which could affect findings.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a deep-learning–based MRI reconstruction framework designed to operate without requiring access to multi-coil \\( k \\)-space data. The approach assumes access to the Moore–Penrose inverse of the under-sampled acquisition, which is a less restrictive condition than those typically imposed in fully or self-supervised methods. The algorithm integrates the physical forward operator into the learning process and demonstrates comparable reconstruction performance to PD-DL approaches on low-acceleration (\\( 4\\times \\)) data. The paper is clearly written and technically sound, addressing a practically relevant problem where raw \\( k \\)-space availability is often limited.\n\n**Major Comments**  \n1. **Quantitative Evaluation:** The reported results lack standard error of the mean values, which are necessary to assess statistical significance and ensure fair comparison across methods.  \n2. **Literature Coverage:** The claim that equispaced undersampling has not been studied in score-based models is inaccurate. The cited paper *“Robust Compressed Sensing MRI with Deep Generative Priors”* explicitly discusses this sampling strategy in the main text and figures, not only in the appendix.  \n3. **Baselines:** The generative model baseline used appears out of date; several stronger methods introduced in the past year outperform it. While inclusion of this baseline is acceptable, describing it as state-of-the-art is misleading.  \n4. **Methodological Details and Clarifications:**  \n   - Clarify whether the clinical PI reconstruction allows saving complex-valued images directly or only magnitude reconstructions.  \n   - Provide details on the training duration and optimization of ScoreMRI, as its relatively low performance may be due to under-training.  \n   - Explain how perturbations were chosen and whether joint or learned perturbation strategies could improve results.  \n   - In Figure 2, verify consistency between the left-hand loss term \\(\\mathcal{L}(\\mathbf{x}^{(k)}, \\mathbf{x}_{PI})\\) and its right-hand counterpart using \\(\\mathbf{x}^{(m)}\\).  \n   - Update Lines 151–152 to accurately reflect the current landscape of unrolled versus diffusion-based methods, as recent comparative studies are lacking.\n\n**Minor Comments**  \n- Correct “fastMRI knee matrix size” to \\(320 \\times 320\\) (see DOI: 10.1148/ryai.2020190007).  \n- Line 42–43: “demand have shown” → “demand has shown.”  \n- Line 269: “shapes that has” → “shapes that have.”  \n- Add missing spaces before figure, table, and section references (e.g., “Fig.$1$” → “Fig. 1”).  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe manuscript addresses an important practical limitation—restricted access to \\( k \\)-space—through a physics-informed deep-learning reconstruction approach. Its innovation lies in relaxing data-access assumptions while maintaining competitive performance. The method is well evaluated on multiple sampling scenarios, though the absence of uncertainty measures and limited comparison breadth reduce the robustness of conclusions. Reproducibility appears feasible but would benefit from additional training details and clarified implementation aspects. Overall, the work is methodologically sound and of moderate significance, with clear room for improvement in evaluation completeness.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper present an novel MRI reconstruction method that only requires dicom data (even undersampled data), with the motivation that rural regions won't have the raw data access. The key idea is through using random perturbations.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\n1. I like the idea of using random image domain perturbations, sounds like a good way to capture pior distribution.\n2. I appreciate the authors for the experiments on the prospective undersampled MRI reconstruction, which is a very important experiment for all MRI reconstruction tasks.\n\n### Weaknesses\n\nOverall, there are a few weaknesses that concerns me:\n1. I didn't really buy the idea of this rural region raw-data access. why we need raw-data? its for the training purpose! we already have a great number of raw k-space data (from fastMRI, mridata.org and others), as long as we have a good model trained, the rural regions can just use it without any training. In the end of the day, all MRI scanner will sample raw k-space data, so the whole motivation doesn't sound solid to me. \nMeanwhile, from what i understand from your method, the input is CG-SENSE of undersampled MRI, is it realistic? the input quality is not really diagnoistic.\n\n2. The presentation is not very clear and the paper is not well-writtened, all the method section for your method is condensed to one *single* page with a figure (figure 2), i have to go back and forth to understand whats R, whats DF, and how you do the perturbations, thats supposed to be the main part of the paper.\n\n3. The score-MRI results look kinda weird to me, try to look at other generative models. It shouldn't be that bad.\n\n4. Figure 3, the result doesn't look good to me, over smoothing textures.\n\n### Questions\n\n1. how do your method compared to single-coil DL-based reconstruction.\n2. how sensitive is your method to different input? (different vendors tend to have different reconstruction pipelines.)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph:\nThe manuscript presents a novel MRI reconstruction method that leverages DICOM data, including undersampled datasets, addressing the challenges faced in rural regions with limited access to raw data. The central innovation lies in employing random perturbations to enhance reconstruction quality.\n\n### Major Comments:\n1. The justification for focusing on rural regions and raw data access appears weak. The reliance on raw data seems unnecessary for training, as there are numerous available datasets (e.g., fastMRI, mridata.org). A well-trained model should suffice for rural applications without requiring additional training. This undermines the paper's motivation.\n2. The method's input, CG-SENSE of undersampled MRI data, raises concerns about realism and diagnostic quality.\n3. Presentation clarity is lacking; the methods section is condensed into a single page accompanied by a figure (Figure 2). This leads to confusion regarding key terms (e.g., R, DF) and the perturbation process, which are fundamental to the paper.\n4. The score-MRI results need further scrutiny, as they present inconsistencies compared to existing generative models—this raises questions about validity.\n5. The results shown in Figure 3 indicate over-smoothing of textures, further questioning the effectiveness of the proposed method.\n\n### Minor Comments:\n- Consider revising the clarity of figures and terminology used throughout the manuscript to enhance reader comprehension.\n- There are some grammatical issues that need to be addressed, particularly “well-writtened.”\n\n### Summary Paragraph:\nIn summary, the manuscript demonstrates potential significance in addressing MRI reconstruction within rural contexts, showing innovation through random perturbations. However, evaluation appears flawed due to weak justification regarding the need for raw data and real-world applicability. Reproducibility is also a concern, given issues with clarity and presentation. Overall, while the paper captures important issues in MRI technology, revisions are required to strengthen its contributions and delivery.\n\n### Decision Recommendation:\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a novel MRI reconstruction method that operates directly on DICOM data, including undersampled images, addressing the scenario where raw k-space data may not be accessible—particularly in resource-limited settings. The approach introduces random perturbations in the image domain to capture data priors and improve reconstruction quality. While the idea is interesting and potentially useful, the paper’s presentation and clarity are limited, and the motivation for the proposed setting is not fully convincing.\n\n---\n\n**Major Comments**  \n1. **Motivation and Practicality:** The rationale regarding rural regions lacking raw data access is unconvincing. In practice, raw k-space data are widely available through public datasets (e.g., fastMRI, mridata.org) for training purposes. Once trained, models can be deployed broadly without requiring local raw data. Furthermore, MRI scanners inherently collect raw data for reconstruction, which challenges the stated motivation.  \n2. **Input Data and Realism:** The method appears to use CG-SENSE reconstructions of undersampled MRI as input, but the diagnostic realism of such inputs is questionable. The manuscript should clarify the realism and feasibility of this assumption.  \n3. **Method Description and Clarity:** The methods section is condensed into roughly one page and lacks sufficient detail. It is difficult to follow key definitions (e.g., “R,” “DF”) and the perturbation process. This section should be expanded and more clearly explained.  \n4. **Experimental Results:** The Score-MRI reconstructions appear unusual compared with other generative reconstruction methods and should be critically analyzed. Figure 3 results exhibit oversmoothing and loss of texture detail.  \n5. **Comparative Evaluation:** A comparison with single-coil deep learning–based reconstructions would provide valuable context. Sensitivity analysis to input variations from different vendors or reconstruction pipelines should also be discussed.\n\n---\n\n**Minor Comments**  \n- The overall writing and presentation need significant improvement for readability.  \n- Figures could be labeled more clearly, and figure captions should provide sufficient explanatory detail.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study introduces an interesting concept of DICOM-based reconstruction via random perturbations, which could broaden the accessibility of advanced MRI reconstruction. However, its significance is diminished by a weak practical motivation and unclear methodological explanations. While innovative in concept, the evaluation is insufficient and comparative analyses are limited. The paper’s current presentation also hinders reproducibility.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis manuscript proposed an unsupervised approach to improve MR DICOM image quality by introducing the loss term $l_{comp}+\\lambda l_{pif}$, where $l_{comp}$ is to suppress noise and $l_{pif}$ is to keep fidelity to the input.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThis manuscript is well-written, and the figures are well-drafted.\n\n### Weaknesses\n\n1. The motivation is weak. Starting reconstruction from DICOM output rather than raw k-space is unusual for MRI practitioners, as it does not allow us to construct the encoding operator, $\\mathrm{E}$.\n\n2. The title is misleading: claiming a \"physics-driven\" approach while dismissing the need for raw k-space data is contradictory. In MRI, k-space data represents the underlying physical imaging process.\n\n3. The reported results for the baseline method, scoreMRI, are questionable, with performance significantly below that in the original publication.\n\n4. The novelty and contribution of this work are limited. The author attempted to improve the image quality with the proposed loss term $l_{comp}+\\lambda l_{pif}$, where $l_{comp}$ is to suppress noise and $l_{pif}$ is to keep fidelity to the input. In my eyes, this is a simple combination of existing approaches for MRI image quality enhancement.\n\n### Questions\n\n1. Refer to Weakness 1, how the coil sensitivity and sampling pattern are handled in the $\\mathrm{E}$?\n\n2. Refer to Weakness 2, how to construct the $\\mathrm{E}$ for scoreMRI?\n\n3. The proposed method resembles SSDU, yet its performance is reportedly better to CUPID. Could you clarify the reason for this?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents an unsupervised approach aimed at improving the quality of MR DICOM images by introducing the loss term \\(l_{comp} + \\lambda l_{pif}\\), where \\(l_{comp}\\) targets noise suppression and \\(l_{pif}\\) maintains fidelity to the input data. While the writing quality is commendable and the figures are well-prepared, the overall contribution of the manuscript to the field appears limited.\n\n**Major Comments:**  \n1. The motivation for the proposed approach is weak. Initiating reconstruction from DICOM output instead of raw k-space is atypical for MRI practitioners, hindering the ability to construct the encoding operator, \\(\\mathrm{E}\\).  \n2. The title claims a \"physics-driven\" approach, which is contradictory given the dismissal of raw k-space data, a critical component of the MRI imaging process.  \n3. Results reported for the baseline method, scoreMRI, raise concerns, as performance is significantly lower than in the original publication.  \n4. The novelty of this work is questionable. The proposed loss term \\(l_{comp} + \\lambda l_{pif}\\) appears to be a basic combination of existing methods aimed at enhancing MRI image quality.\n\n**Minor Comments:**  \n- Consider revising the title for clarity to accurately reflect the manuscript’s content.  \n- Ensure all figures are properly labeled and referenced within the text.  \n- Clarify notation for terms used in the proposed methodology to enhance understanding.  \n\n**Summary Paragraph:**  \nIn terms of significance, the manuscript presents a potentially relevant area of research in MRI image quality enhancement; however, the innovation is limited as it builds on existing methodologies without significant advancement. The evaluation design lacks clarity, particularly regarding the handling of the encoding operator, \\(\\mathrm{E}\\), and baseline performance metrics appear unreliable. Reproducibility remains uncertain given the methodology employed. \n\n**Decision Recommendation:**  \nMajor revision is recommended to address the identified weaknesses, particularly in motivation, novelty, and methodological clarity before reconsideration for publication.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents an unsupervised approach aimed at improving MR DICOM image quality through a composite loss function, \\( l_{comp} + \\lambda l_{pif} \\), where \\( l_{comp} \\) suppresses noise and \\( l_{pif} \\) preserves fidelity to the input data. The paper is clearly written and accompanied by well-prepared figures. However, concerns arise regarding the motivation, physical grounding, and the level of novelty in the proposed approach.  \n\n**Major Comments**  \n1. **Motivation and Methodological Basis:** The choice to perform reconstruction from the DICOM output rather than raw k-space data is unconventional in the MRI field. This prevents proper construction of the encoding operator \\( \\mathrm{E} \\), which is essential in modeling the imaging process. The rationale behind this design choice requires stronger justification.  \n2. **Conceptual Consistency:** The title’s reference to a “physics-driven” approach is misleading, as the method ignores raw k-space information—the core representation of the physical imaging process in MRI. This inconsistency undermines the claimed scientific framing.  \n3. **Baseline Comparisons:** The performance of the baseline method, scoreMRI, appears significantly lower than reported in its original publication, raising concerns about the correctness or fairness of the comparison.  \n4. **Novelty and Contribution:** The proposed loss function appears to be a straightforward combination of existing terms for noise suppression and fidelity preservation, which limits its originality and impact relative to existing MRI image enhancement approaches.  \n\n**Minor Comments**  \n- Clarify how coil sensitivity maps and sampling patterns are handled within \\( \\mathrm{E} \\).  \n- Explain how the encoding operator is constructed for scoreMRI.  \n- Provide insight into why the proposed method outperforms CUPID while resembling SSDU.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe manuscript addresses a relevant problem in improving MR image quality but offers limited methodological innovation and weak physical justification. The evaluation lacks strong baselines and sufficient explanation of comparative methods, affecting the work’s significance and reproducibility. While the presentation quality is acceptable, the overall contribution does not substantively advance the field under TMI’s criteria of significance, innovation, evaluation, and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision** – Substantial methodological clarification, strengthened motivation, and more rigorous experimental validation are required before this work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses the challenge of training physics-driven deep learning (PD-DL) reconstruction models for accelerated MRI without access to raw k-space data, which limits deployment in underserved areas. The authors propose CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), which trains unrolled networks using only clinical DICOM images reconstructed via parallel imaging. The method combines two loss terms: a compressibility-based term using reweighted ℓ1 norm on wavelet coefficients (Equation 8), and a parallel imaging fidelity term that ensures consistency through carefully designed perturbations (Equation 9). The approach is evaluated on retrospective fastMRI knee and brain datasets at R=4 acceleration and prospective 7T brain data at R=9. Results show CUPID achieves comparable performance to supervised and self-supervised methods requiring raw k-space access (Table 1), while outperforming compressed sensing and generative approaches like ScoreMRI.\n\n## Weaknesses\n\n• **Mathematical formulation lacks rigor and clarity**\n  - The parallel imaging fidelity term in Equation 9 lacks theoretical justification for why perturbations that avoid R-fold aliasing should stabilize training; no formal analysis of convergence properties or optimality conditions is provided\n  - The expectation over perturbations p in Equation 9 is approximated using only K=6 samples (Section B.1, Figure 7), but no theoretical or empirical analysis of convergence rate or variance of this Monte Carlo estimate is provided\n  - The connection between the fold-over constraint design and the effectiveness of the fidelity term is stated conceptually (page 5, lines 225-230) but lacks mathematical rigor in establishing why this specific constraint prevents zero solutions\n\n• **Limited experimental validation scope**\n  - Only uniform/equidistant undersampling patterns are evaluated (page 6, lines 289-291), which represents a narrow subset of clinically relevant acceleration patterns, limiting generalizability claims\n  - Prospective evaluation is limited to a single 7T scanner with one subject (Section 4.4), providing insufficient evidence for real-world deployment effectiveness\n  - No comparison with other unsupervised methods that could potentially work without raw k-space data, such as deep image prior variants or other self-supervised approaches adapted for DICOM-only training\n\n• **Perturbation design lacks systematic analysis**\n  - The choice of perturbation patterns (letters, shapes, card suits) appears ad-hoc (Figure 9, Appendix B.2) with no principled framework for optimal perturbation selection\n  - The statement that \"intensity of perturbations was empirically found to be more important than shapes/orientations\" (lines 917-920) lacks quantitative analysis or theoretical justification\n  - The fold-over constraint criterion is described qualitatively (page 5, lines 270-275) but lacks precise mathematical definition or validation that all chosen perturbations satisfy this constraint\n\n• **Insufficient analysis of method limitations**\n  - The discussion of filtering operations in clinical DICOM images (Section 4.6, lines 486-495) acknowledges but does not address how common vendor-specific processing affects the fundamental assumption that x_PI = (E^H_Ω E_Ω)^(-1) E^H_Ω y_Ω\n  - Limited analysis of computational requirements for deployment in resource-constrained settings, despite this being a primary motivation (lines 508-517 acknowledge hardware limitations but provide no concrete solutions)\n  - The magnitude-only DICOM limitation (Section 4.6, lines 518-530) is identified but not addressed, potentially limiting applicability in many clinical scenarios\n\n## Suggestions for Improvement\n\n• **Strengthen mathematical foundations**\n  - Provide theoretical analysis of the parallel imaging fidelity term, including convergence guarantees and conditions under which the method avoids trivial solutions\n  - Analyze the Monte Carlo approximation error in Equation 9 and provide guidelines for selecting K based on desired approximation accuracy\n  - Formalize the fold-over constraint mathematically and prove that perturbations satisfying this constraint provide effective regularization\n\n• **Expand experimental validation comprehensively**\n  - Include evaluation on random and other clinically relevant undersampling patterns beyond uniform/equidistant to demonstrate broader applicability\n  - Conduct multi-site, multi-scanner prospective validation with sufficient sample sizes to establish statistical significance of real-world performance\n  - Compare against adapted versions of other unsupervised methods that could potentially work with DICOM-only data to establish competitive advantage\n\n• **Develop systematic perturbation framework**\n  - Establish a principled framework for perturbation design based on theoretical analysis of what properties make perturbations effective for training stabilization\n  - Provide quantitative analysis of how perturbation characteristics (intensity, shape, frequency content) affect reconstruction quality\n  - Develop and validate precise mathematical criteria for the fold-over constraint and provide automated methods for perturbation validation\n\n• **Address practical deployment challenges**\n  - Investigate and propose solutions for handling vendor-specific filtering operations, potentially through robust loss formulations or preprocessing methods\n  - Provide concrete analysis of computational requirements and propose lightweight variants for resource-constrained environments\n  - Develop methods for handling magnitude-only DICOMs and unknown coil sensitivities, which are common in clinical practice",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript addresses the challenge of training physics-driven deep learning (PD-DL) reconstruction models for accelerated MRI using only clinical DICOM images, without access to raw k-space data. The authors propose CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), which utilizes unrolled networks incorporating a compressibility-based loss term and a parallel imaging fidelity term. The evaluation of CUPID on various datasets demonstrates performance comparable to both supervised and self-supervised methods, alongside improved outcomes compared to compressed sensing and generative techniques.\n\n### Major Comments\n1. **Mathematical Formulation**: \n   - The formulation of the parallel imaging fidelity term lacks rigor and theoretical justification regarding perturbations avoiding R-fold aliasing.\n   - No formal analysis on convergence properties of the method or empirical evidence analyzing the convergence rate of the Monte Carlo estimate is provided.\n   - The conceptual connection between the fold-over constraint and fidelity effectiveness needs mathematical backing.\n\n2. **Experimental Validation Scope**: \n   - Evaluation is limited to uniform undersampling patterns, reducing generalizability.\n   - Prospective validations are confined to a single subject on one 7T scanner, providing inadequate evidence for real-world application.\n   - There is a lack of comparison to other unsupervised techniques that may effectively leverage DICOM data alone.\n\n3. **Perturbation Design Analysis**: \n   - The selection of perturbation patterns appears arbitrary without a theoretical framework.\n   - Claims regarding perturbation intensity versus shape require further quantitative analysis.\n   - The fold-over constraint requires a clearer mathematical definition with validation for all selected perturbations.\n\n4. **Method Limitations Analysis**: \n   - The discussion on vendor-specific filtering is not addressed concerning its impact on the method's assumptions.\n   - Computational requirements for practical deployment need further exploration despite being highlighted as a primary motivation.\n   - Limitations pertaining to magnitude-only DICOM data are noted but not sufficiently addressed.\n\n### Minor Comments\n- Clarify the notation and ensure consistency in variable definitions throughout the manuscript.\n- Revise figures to enhance clarity and provide additional context in captions where necessary.\n- Check for typographical errors and representational consistency, particularly in equations.\n\n### Summary Paragraph\nIn regard to **significance**, the work addresses a notable gap in accelerated MRI reconstruction methods. **Innovation** is demonstrated through the proposed method, although the theoretical foundations may benefit from strengthening. The **evaluation** design is limited in variety, which constrains the breadth of applicability claims. Finally, the work raises concerns about **reproducibility** due to restrictions in validation methodology and computational analysis. While the manuscript presents a promising approach, addressing these weaknesses is essential for enhancing its impact and reliability.\n\n### Decision Recommendation\nMajor revision is recommended to address the highlighted conceptual, theoretical, and experimental shortcomings.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a physics-driven deep learning (PD-DL) framework, CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), aimed at training MRI reconstruction models when raw k-space data are unavailable—a scenario common in resource-limited or clinical environments. CUPID combines a compressibility-based loss on wavelet coefficients with a parallel imaging fidelity term that enforces data consistency through designed perturbations. The approach is validated on retrospective fastMRI knee and brain datasets (R=4) and a prospective 7T brain dataset (R=9), demonstrating performance comparable to methods requiring raw k-space. The manuscript addresses an important problem and is generally well written, but several methodological and validation aspects require substantial strengthening.  \n\n**Major Comments**  \n1. **Mathematical formulation and theoretical rigor**  \n   - The parallel imaging fidelity term (Eq. 9) lacks justification regarding how perturbations that avoid R-fold aliasing stabilize training; convergence or optimality analyses are absent.  \n   - The Monte Carlo approximation using only six perturbation samples (K=6) is not supported with empirical or theoretical discussion of variance or convergence.  \n   - The fold-over constraint is described conceptually but lacks formalization showing why it prevents trivial zero solutions.  \n\n2. **Experimental validation scope**  \n   - Experiments are limited to uniform undersampling, restricting clinical generalizability.  \n   - Prospective results are based on a single subject at one 7T scanner, offering limited evidence of robustness.  \n   - The comparison set omits other potential unsupervised DICOM-based approaches, affecting the completeness of evaluation.  \n\n3. **Perturbation design clarity**  \n   - The ad-hoc selection of perturbation patterns (letters, shapes, card suits) lacks a principled selection rationale.  \n   - The claim that perturbation intensity matters more than shape is not quantitatively supported.  \n   - The fold-over constraint itself is qualitatively described but lacks quantitative verification.  \n\n4. **Analysis of limitations and practical deployment**  \n   - Vendor-specific filtering of DICOM images may violate model assumptions but is not analyzed in depth.  \n   - Computational and hardware considerations for deployment are acknowledged without practical evaluation.  \n   - Magnitude-only DICOM limitations and unknown coil sensitivities remain unresolved.  \n\n**Minor Comments**  \n- Clarify notation and explicitly define the fold-over constraint mathematically.  \n- Provide details on how perturbations are generated and validated.  \n- Improve figure captions for readability (e.g., Figures 7 and 9).  \n- Include brief justification for parameter choices such as wavelet transform and loss weights.  \n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe proposed CUPID framework addresses an important barrier to broad MRI reconstruction deployment—training without raw k-space data. Its conceptual innovation lies in combining compressibility-based and parallel imaging–inspired losses. However, the theoretical underpinnings of the fidelity term are underdeveloped, and experimental validation is limited in scope. Evaluation lacks comprehensive testing across sampling patterns, scanners, and comparative unsupervised baselines. Reproducibility is hindered by incomplete mathematical formalization and limited implementation transparency. With stronger theoretical justification and broader experimental validation, the work could make a meaningful contribution.  \n\n**Decision Recommendation:** **Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI – ✅**  \n   The manuscript clearly fits within the scope of *IEEE Transactions on Medical Imaging* (TMI). It introduces a new framework, CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), for physics-driven deep learning (PD-DL) MRI reconstruction without raw k-space access. The work addresses a key methodological challenge in MRI reconstruction — how to train PD-DL models in settings where only DICOM images are available — and includes a complete mathematical formulation, validation, and theoretical grounding.  \n\n2. **Novelty & Contribution Level – High**  \n   The CUPID framework replaces the standard data-fidelity–based supervision with a hybrid unsupervised loss incorporating a reweighted ℓ1 compressibility measure and a novel parallel-imaging fidelity term. This enables PD-DL model training solely from reconstructed clinical images. This is a meaningful conceptual advance beyond established approaches such as SSDU or generative priors (e.g., ScoreMRI). The potential impact for equitable MRI access is notable.\n\n3. **Technical and Experimental Rigor – Strong but could use more depth in mechanisms**  \n   The loss formulation is mathematically consistent, and experimental setups are well-documented. Comparisons against a wide set of methods (supervised PD-DL, SSDU, EI, ScoreMRI, CS, CG-SENSE) are credible. Retrospective and prospective validations are provided. However, more explicit analysis of computational cost, bias from vendor postprocessing, and performance under varying noise levels would strengthen the paper.\n\n4. **Clarity and Presentation – Very good**  \n   The manuscript is well structured, clear, and readable. Equations and figures are well labelled. Minor stylistic and typographical errors exist, but nothing that inhibits comprehension.\n\n5. **Ethical and Reproducibility Compliance – Acceptable**  \n   The paper uses publicly available datasets (fastMRI) and prospective institutional acquisition with proper anonymization assumptions. Code availability is promised. The study adheres to ethical imaging research standards.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n### 1. Summary\nThe paper proposes CUPID, a new framework for training physics-driven deep learning MRI reconstruction networks without access to raw k-space data. CUPID introduces a compressibility-based unsupervised loss combined with a parallel-imaging fidelity mechanism that stabilizes training and prevents degenerate solutions. The method can be applied in both database and zero-shot (subject-specific) regimes. Retrospective and prospective studies demonstrate that CUPID achieves image quality comparable to supervised and self-supervised PD-DL methods that require raw data, outperforming compressed sensing and diffusion-based generative models.\n\n### 2. Strengths\n- **Methodological novelty:** First reported framework enabling unsupervised PD-DL reconstruction training using only DICOM images.  \n- **Physics consistency:** Integrates theoretical insights from parallel imaging and compressibility-based regularization.  \n- **Experimental breadth:** Includes retrospective, prospective, and zero-shot validations across multiple body regions and acquisition protocols.  \n- **Practical impact:** Addresses data-access inequities by enabling model adaptation at clinical sites without raw data access.  \n- **Clarity and reproducibility:** Detailed mathematics, ablation analyses, and anticipated code release.\n\n### 3. Weaknesses\n- **Limited theoretical analysis of convergence and stability** of the proposed unsupervised loss; more formal justification or empirical robustness checks would benefit scientific depth.  \n- **Vendor postprocessing effects** (filtering, partial Fourier, coil-combination bias) are discussed qualitatively but not quantitatively validated.  \n- **Resource and computational requirements** for zero-shot fine-tuning are not benchmarked.  \n- **Clinical evaluation** (radiologist scoring, diagnostic equivalence) is absent, though understandably early-stage.  \n- Minor clarity issues in notation and typographical consistency.\n\n### 4. Major Comments\n1. **Clarify the mathematical behavior of the CUPID loss.**  \n   Provide further intuition or analysis on how the trade‑off parameter λ balances the two terms and under what conditions convergence is guaranteed. An energy-landscape illustration or gradient analysis could help.  \n\n2. **Quantify vendor and acquisition variability.**  \n   Since DICOM preprocessing varies by scanner, include an experiment quantifying robustness to differences in filtering or noise scaling between SENSE and GRAPPA vendor pipelines.  \n\n3. **Training resource assessment.**  \n   Report memory, GPU runtime, and fine‑tuning duration to establish feasibility in limited-resource settings emphasized in the motivation.  \n\n4. **Uncertainty characterization.**  \n   Consider adding uncertainty metrics (e.g., Monte‑Carlo dropout or ensemble variation) to assess reliability compared to existing PD‑DL models.  \n\n5. **Clinical validation prospects.**  \n   While outside this submission’s immediate scope, discuss how clinical readers might evaluate reconstruction fidelity to original diagnostic contrast.\n\n### 5. Minor Comments\n- Typographical inconsistencies: “Fi delity”, “target acceleration rate” (spacing) in line 70.  \n- Ensure consistent use of mathematical symbols (e.g., Ω vs. Ω).  \n- Figures could include zoomed‑in reference regions for all dataset types.  \n- Reference list formatting needs harmonization per IEEE style (some capitalization inconsistencies).  \n- Clarify whether λ was selected empirically or through cross‑validation.\n\n### 6. Overall Recommendation  \n**Recommendation:** **Major Revision**  \n**Rationale:** The paper presents a highly novel and timely methodological contribution with strong alignment to *IEEE Transactions on Medical Imaging*. However, additional quantitative evidence of robustness across vendors, stronger convergence justification, and expanded discussion of computational feasibility are required to reach TMI’s methodological rigor standard.\n\n### 7. Confidence Level  \n**5 – Expert**  \nThe reviewer has strong familiarity with physics‑driven deep learning and MRI reconstruction literature (SSDU, MoDL, ScoreMRI, etc.), and confidence in assessing novelty and methodological soundness.\n\n---\n\n**Summary Decision:**  \nThis is a promising, innovative study directly relevant to medical imaging methodology. With additional analyses and minor clarifications, CUPID could become a high‑impact contribution to the field and is worthy of revision and eventual publication in *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *CUPID* (Compressibility‑inspired Unsupervised Learning via Parallel Imaging Fidelity), a physics‑driven deep learning (PD‑DL) MRI reconstruction framework designed to train without access to raw k‑space data. The paper targets a key challenge in PD‑DL — enabling robust model development from DICOM‑only data — and presents a mathematically grounded formulation integrating compressibility and parallel‑imaging fidelity terms. The work is clearly written, well organized, and demonstrates substantial methodological innovation and potential clinical relevance.  \n\n---\n\n**Major Comments**  \n1. **Mathematical characterization of the CUPID loss:** The balance between the compressibility term and the parallel‑imaging fidelity term (parameter λ) requires more analytical or empirical justification. Discussion of convergence behavior, stability, or gradient properties would strengthen theoretical claims.  \n2. **Vendor preprocessing variability:** Because DICOM data undergo vendor‑specific filtering, coil‑combination, and partial‑Fourier corrections, a quantitative robustness analysis across vendors is essential to confirm generalizability.  \n3. **Computational/resource efficiency:** Benchmark GPU memory use, runtime, and fine‑tuning cost to substantiate claims of clinical feasibility, particularly for zero‑shot adaptation scenarios.  \n4. **Uncertainty quantification:** Include or discuss methods (e.g., dropout, ensembles) to assess reconstruction reliability relative to conventional PD‑DL approaches.  \n5. **Prospects for clinical validation:** Although early‑stage, outlining how radiologists or clinicians might assess diagnostic fidelity would clarify the framework’s translational relevance.  \n\n---\n\n**Minor Comments**  \n- Correct typographical issues (“Fi delity”, spacing in “target acceleration rate”).  \n- Ensure uniform mathematical notation (e.g., Ω vs. Ω).  \n- Include zoomed‑in reference crops in all figure comparisons.  \n- Harmonize reference formatting in accordance with IEEE style.  \n- Specify how the trade‑off parameter λ was chosen (empirical or cross‑validated).  \n\n---\n\n**Summary Paragraph (TMI Editorial Criteria)**  \n**Significance:** The approach directly addresses data‑access inequities in MRI reconstruction by removing dependence on raw k‑space input.  \n**Innovation:** CUPID is the first reported unsupervised PD‑DL reconstruction framework based solely on DICOM data, integrating compressibility priors with parallel‑imaging consistency.  \n**Evaluation:** The experiments cover retrospective, prospective, and zero‑shot settings across body regions but need additional vendor and resource analyses for completeness.  \n**Reproducibility:** Methodological details, datasets, and planned code release indicate good reproducibility, though convergence and stability analyses are still limited.  \n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe paper represents a strong and original contribution aligned with *IEEE Transactions on Medical Imaging*. To meet TMI’s standards, it requires quantitative robustness verification, clearer theoretical justification of the objective function, and detailed computational profiling before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript presents CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), a new framework for physics-driven deep learning (PD-DL) MRI reconstruction that operates without access to raw k-space data. This work addresses a significant challenge in MRI reconstruction, focusing on training PD-DL models where only DICOM images are available, and provides a comprehensive mathematical formulation supported by validation and theoretical grounding.\n\n### Major Comments\n1. **Scope & Relevance:** The manuscript aligns well with the aims of *IEEE Transactions on Medical Imaging* and contributes a meaningful advancement in methodology.\n2. **Novelty & Contribution:** CUPID leverages an innovative unsupervised loss based on a reweighted ℓ1 compressibility measure and a parallel-imaging fidelity term, representing a conceptual leap from established methods such as SSDU and ScoreMRI.\n3. **Technical Rigor:** While the loss formulation is mathematically sound and experiments are well-documented, additional analysis on computational cost, vendor biases, and performance under varying noise levels would enhance the depth of the paper.\n4. **Clarity and Presentation:** The manuscript is organized and clear, with well-labelled equations and figures; however, minor typographical errors are present that do not significantly impact understanding.\n5. **Ethical Compliance:** The use of publicly available datasets and proper ethical considerations is commendable, and the promise of code availability fosters reproducibility.\n\n### Minor Comments\n- Correct typographical errors in the text, including “Fi delity” and inconsistencies with mathematical symbols (e.g., Ω vs. Ω).\n- Enhance figures by including zoomed-in reference regions and ensure consistent formatting in the reference list per IEEE style.\n- Clarify the method of selection for the trade-off parameter λ.\n\n### Summary Paragraph\nIn summary, the paper offers significant advancements in MRI reconstruction through the CUPID framework, satisfying the **significance** and **innovation** criteria of TMI. The methodological strength shows promise in practical applications, particularly addressing data-access inequities. However, the **evaluation** and **reproducibility** aspects could be strengthened with more robust quantitative data and clearer assessments of vendor and acquisition variability.\n\n### Decision Recommendation\n**Recommendation:** Major Revision  \n**Rationale:** The manuscript showcases a highly innovative and relevant contribution to medical imaging. Essential improvements include providing additional quantitative evidence for robustness, stronger justification for convergence properties, and a more detailed discussion of computational requirements to meet TMI’s standards.\n\n### Confidence Level\n**5 – Expert**  \nThe reviewer possesses extensive familiarity with PD-DL and MRI reconstruction literature, ensuring confidence in evaluating the novelty and methodological rigor of the work provided.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Mehmet Akcakaya",
      "Merve Gulle",
      "Yasar Utku Alcalar"
    ],
    "url": "pdfs/iclr.cc-2025-conference_058122326c88eb7ecc35a0739e0ce1f6c8ad76ad.pdf",
    "remote_url": "https://openreview.net/pdf/058122326c88eb7ecc35a0739e0ce1f6c8ad76ad.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Lecture Learning",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Justin"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "Multimodal assistant",
      "surgical",
      "multimodal instruction-following data",
      "dataset"
    ],
    "abstract": "Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images. Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos. One major contributing factor is the absence of datasets in the surgical field. In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far. To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. We will release our code, model, and the instruction-tuning dataset.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal large language model designed as a conversational assistant for surgical applications. To support this, the authors developed Surg-QA, a large-scale dataset containing 102,000 surgical video-instruction pairs, generated through a structured two-stage question-answer pipeline. This pipeline helps extract structured knowledge from surgical lecture videos, enabling the LLaVA-Surg model to understand complex surgical procedures and answer open-ended questions in a zero-shot setting. The model leverages CLIP for visual encoding and is fine-tuned on Surg-QA to specialize in surgical video question-answering, achieving superior performance compared to existing general-domain models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe authors provide a novel dataset, Surg-QA, which is a significant resource for training multimodal surgical models, covering diverse surgical procedures and question-answer pairs.\n2.\tThe two-stage pipeline for question-answer generation mitigates hallucinations in LLM outputs, resulting in higher quality and reliability of generated data.\n3.\tLLaVA-Surg demonstrates notable improvements over general multimodal models in zero-shot surgical video question-answering tasks, showcasing its efficacy in understanding surgical context.\n\n### Weaknesses\n\n1. The paper should compare its model with recent multimodal LLM approaches, specifically ReAct (Yao et al., 2023), which combines reasoning and action for complex tasks.\n[1] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023, January). ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations (ICLR).\n2. Using CLIP for frame-by-frame encoding lacks temporal modeling and increases processing costs and redundancy, burdening the LLM as frame count grows.\n3. The paper lacks an in-depth error analysis, especially regarding potential hallucinations or misunderstandings in complex surgical scenarios. Although the authors claim to reduce hallucinations, achieving perfect performance seems challenging.\n4. The model’s adaptability to other medical or clinical fields is unclear, as broader evaluations on datasets like RAD, SLAKE, and PathVQA are missing, which may limit its wider applicability.\n\n### Questions\n\n1. Does splitting video into frames for CLIP’s visual encoder lead to a loss of spatiotemporal information, and wouldn’t a video encoder like Video Swin Transformer [2] better capture temporal dynamics?\n[2] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., & Hu, H. (2022). Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 3202-3211).\n2. How does LLaVA-Surg perform compared to other state-of-the-art multimodal methods? In addition to general multimodal models, a detailed comparison with models like ReAct would provide a more comprehensive evaluation. Has comparison with other two-stage methods [3] in VQA task been overlooked?\n[3] Gai, X., Zhou, C., Liu, J., Feng, Y., Wu, J., & Liu, Z. (2024). MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale. arXiv preprint arXiv:2404.12372.\n3. Is the two-stage question-answer generation process applicable to other medical fields, and if so, what adjustments would be required? Additionally, validating the method’s performance on public datasets like RAD, SLAKE, and PathVQA would strengthen its generalizability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents LLaVA-Surg, a multimodal large language model intended to function as a conversational assistant for surgical applications. The authors introduce Surg-QA, a comprehensive dataset comprising 102,000 surgical video-instruction pairs generated through a structured two-stage question-answer pipeline. This approach allows for the extraction of structured knowledge from surgical lectures, enabling LLaVA-Surg to comprehend complex procedures and respond to open-ended questions in a zero-shot scenario. The model employs CLIP for visual encoding and is fine-tuned on Surg-QA, demonstrating superior performance relative to existing general-domain models.\n\n**Major Comments:**  \n1. The authors should compare LLaVA-Surg with recent multimodal LLM approaches, particularly ReAct (Yao et al., 2023), which integrates reasoning and action for complex tasks.  \n2. The use of CLIP for frame-by-frame encoding may lack temporal modeling capabilities, resulting in higher processing costs and redundancy as frame count increases.  \n3. There is insufficient in-depth error analysis regarding potential hallucinations or misunderstandings in complex surgical contexts, despite claims of reducing such occurrences.  \n4. The adaptability of LLaVA-Surg to other medical or clinical fields requires clarification, as broader evaluations on datasets like RAD, SLAKE, and PathVQA are absent, which limits its potential applications.\n\n**Minor Comments:**  \n- The manuscript would benefit from improved clarity in its presentation, particularly in illustrating the two-stage pipeline methodology.  \n- Clarifications are needed for acronyms and terminology used throughout the paper.  \n- A few typographical errors were noted that should be corrected for improved readability.\n\n**Summary Paragraph:**  \nIn evaluating the manuscript against TMI editorial criteria, LLaVA-Surg demonstrates significance through its novel dataset and contributions to surgical procedures. While it showcases innovation in multimodal question-answering, the evaluation lacks comparisons with state-of-the-art methods, and the reproducibility of its findings may be questioned due to limited testing on broader datasets. Overall, the strengths lie in its unique resource and improved performance, while the weaknesses include a need for additional comparisons and evaluations.\n\n**Decision Recommendation:**  \nMinor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **LLaVA-Surg**, a multimodal large language model designed as a conversational assistant for surgical applications. It introduces a supporting dataset, **Surg-QA**, containing 102,000 surgical video–instruction pairs generated through a structured two-stage question–answer pipeline. This process aims to extract meaningful knowledge from surgical lecture videos, enhancing the model’s capability to interpret and respond to complex surgical procedures in a zero-shot setting. The model integrates CLIP for visual encoding and fine-tunes it on Surg-QA, demonstrating improved performance compared with general multimodal models. Overall, the paper is clearly written and introduces a valuable resource for surgical AI, though some methodological and comparative aspects warrant further clarification.  \n\n**Major Comments**  \n1. **Comparative Evaluation** – The model should be compared with recent multimodal LLMs such as *ReAct* (Yao et al., 2023), which integrates reasoning and action for complex tasks. Including such baselines would strengthen the assessment of novelty and performance.  \n2. **Lack of Temporal Modeling** – Using CLIP for frame-by-frame visual encoding omits temporal dependencies and results in higher computational costs and redundancy, posing scalability concerns.  \n3. **Error Analysis and Hallucination Assessment** – The paper does not provide a detailed error analysis. Although the two-stage pipeline aims to reduce hallucinations, the model’s behavior in complex or ambiguous surgical scenarios remains insufficiently analyzed.  \n4. **Generalizability** – The model’s applicability to other medical domains is unclear. Broader testing on publicly available datasets (e.g., RAD, SLAKE, PathVQA) would help evaluate transferability and robustness.  \n\n**Minor Comments**  \n- Clarify whether the two-stage question–answer generation process could generalize to non-surgical domains, and specify what modifications may be needed.  \n- Consider evaluating temporal visual encoding using methods like the Video Swin Transformer (Liu et al., 2022).  \n- Provide quantitative comparisons with two-stage medical VQA approaches such as *MedThink* (Gai et al., 2024).  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in proposing a domain-specialized multimodal assistant and introducing the large-scale Surg-QA dataset. Its innovation lies in the structured two-stage Q&A generation pipeline, which helps reduce hallucinations. However, evaluation could be more comprehensive—particularly with inclusion of recent multimodal baselines and temporal modeling analyses. Reproducibility appears feasible with well-documented datasets and methods, though more detailed error and generalization studies would enhance rigor.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a novel surgical multimodal dataset, which consists of over 102,000 video-instruction pairs generated through a two-stage pipeline, aimed at enhancing the understanding and conversational capabilities of surgical videos.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n1. With over 102,000 video-instruction pairs, this dataset is the largest in the surgical field.\n2. Structured data annotation pipeline using LLMs minimizes the risk of generating inaccurate or nonsensical content, improving dataset reliability.\n3. Releasing the dataset, model, and code publicly fosters further research and development in the surgical AI domain.\n4. The dataset can be a valuable resource for training and education, helping surgical trainees learn through interactive Q&A about real procedures.\n\n### Weaknesses\n\n1. The paper does not address how the data's quality is maintained as the videos are obtained from the web. The clinicians have reviewed the output of their MLLM model, but the paper does not confirm whether clinicians or domain experts have reviewed the raw data to ensure accuracy and reliability.\n2. Concerns regarding the release, privacy, and permission risks associated with using sensitive surgical videos are not adequately discussed.\n3. The paper lacks comprehensive validation across essential surgical downstream tasks and other surgical QA datasets, which are crucial for demonstrating clinical usability. There is also a need for more rigorous benchmarking against a broader range of state-of-the-art video MLLM architectures to establish the dataset's utility and the model's performance more robustly.\n4. The comparison of the proposed methods with SOTA methods is limited and does not include the latest works. The manuscript also lacks evaluations with models trained on other surgical datasets, limiting the assessment of the proposed model's generalizability across different surgical scenarios.\n5. The paper may need to evaluate the visual quality of the surgical videos.\n\n### Questions\n\n1. How can the quality of the data be ensured? The data collected may already contain a lot of noise and has been reprocessed by an LLM. Is there any person or clinician reviewing these raw data?\n2. Can the data be released? Are there privacy and permission risks associated with the collected data?\n3. The authors need to conduct more zero-shot evaluations on downstream tasks relevant to the surgical field, such as phase recognition, action/instrument classification, and other surgical domain VQA data to demonstrate the clinical usability of their method.\n4. The authors need to compare with more state-of-the-art methods. The comparison methods in Table 3 were all first released in 2023.\n5. The authors may verify their dataset on more benchmarks of SOTA Video MLLM architectures.\n6. Also, the authors need more zero-shot comparisons with the same VLM trained on other surgical datasets, to showcase the generalizability of their proposed dataset.\n7. The authors may evaluate the visual quality of the surgical videos themselves, as they are obtained from the website.\n\n### Flag For Ethics Review\n\n- Yes, Legal compliance (e.g., GDPR, copyright, terms of use)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPotential copyright problem for online data.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel surgical multimodal dataset consisting of over 102,000 video-instruction pairs, developed through a two-stage pipeline. This dataset aims to enhance the understanding and conversational capabilities within surgical videos, proposing significant contributions to the field of surgical AI.\n\n**Major Comments:**\n1. The paper does not sufficiently address the quality maintenance of the data sourced from the web. While clinician review of the MLLM model output is mentioned, the manuscript fails to confirm if domain experts have assessed the raw data for accuracy.\n2. Privacy and permission risks related to the use of sensitive surgical videos are not adequately considered in the paper.\n3. There is a lack of comprehensive validation across essential surgical downstream tasks and other surgical QA datasets, which is critical for establishing clinical usability. Additionally, more rigorous benchmarking against a wider array of state-of-the-art (SOTA) video MLLM architectures is necessary to validate the dataset's utility.\n4. Comparisons between proposed methods and SOTA techniques are limited and exclude the most recent studies. The lack of evaluations involving models trained on other surgical datasets restricts the assessment of generalizability across various surgical scenarios.\n5. An evaluation of the visual quality of the surgical videos would enhance the overall rigor of the work.\n\n**Minor Comments:**\n- Clarify how the quality of the raw data is ensured, considering potential noise from online extraction and LLM processing.\n- Discuss potential privacy and permission risks involved in data release.\n- Conduct further zero-shot evaluations on downstream tasks relevant to surgery, including phase recognition and action/instrument classification.\n- Expand the comparison with state-of-the-art methods, especially those released beyond 2023.\n- Verify dataset efficacy against more benchmarks of SOTA video MLLM architectures.\n- Include additional zero-shot comparisons with the same VLM on other surgical datasets to demonstrate generalizability.\n- Evaluate the visual quality of surgical videos as they originate from external sources.\n\n**Summary Paragraph:**  \nThe manuscript demonstrates notable strengths, including the generation of the largest dataset in the surgical field and the structured annotation pipeline leveraging LLMs. However, it also has significant weaknesses: insufficient quality controls, privacy concerns, and inadequate validation and benchmarking hinder the overall impact of the work. In terms of significance, innovation, evaluation, and reproducibility, the dataset offers potential contributions but falls short in ensuring quality and demonstrating clinical applicability.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a large-scale multimodal surgical dataset consisting of over 102,000 video–instruction pairs created through a two-stage pipeline. The primary goal is to enhance understanding and conversational capabilities for surgical video analysis. The paper describes the dataset generation process, model design, and intended use in surgical education and AI research. The overall presentation is understandable, but the manuscript lacks depth in validation and ethical discussion.  \n\n**Major Comments**  \n1. **Data Quality Assurance:** The provenance and quality of web-sourced videos are insufficiently discussed. The paper confirms clinician review of the generated model outputs but not of the raw data itself, leaving uncertainty about dataset accuracy and reliability.  \n2. **Privacy and Ethical Considerations:** There are potential risks regarding the release of sensitive surgical videos without confirmed consent or appropriate anonymization. Privacy and permission compliance (e.g., GDPR, copyright) need thorough clarification.  \n3. **Insufficient Evaluation:** The dataset’s utility is not fully demonstrated through downstream surgical tasks such as phase recognition, instrument classification, or surgical VQA evaluation. Broader and more rigorous benchmarking is necessary.  \n4. **Limited Comparison to State of the Art:** The methods compared in the paper are mostly from 2023, omitting several recent SOTA models. Comparisons with existing surgical datasets and newer multimodal architectures are required to assess generalizability and performance robustness.  \n5. **Visual Quality Assessment:** The study does not evaluate the visual clarity or suitability of the web-collected videos, which may affect model training quality and usability.  \n\n**Minor Comments**  \n- Some figures and tables could benefit from clearer labeling of comparison methods.  \n- The manuscript would be strengthened by specifying the pre-processing steps used before model training.  \n- Terminology should be standardized—for example, clearly distinguishing “LLM,” “VLM,” and “MLLM.”  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the dataset size and open release represent a valuable contribution to surgical AI research. Regarding **innovation**, the structured two-stage annotation pipeline shows methodological novelty. However, weaknesses in **evaluation**—especially lacking downstream task validation and broader comparisons—limit the evidence supporting clinical utility. **Reproducibility** is strong due to planned public release but weakened by unclear data sourcing and ethical compliance.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper contributes an important resource but requires stronger evaluation, clearer ethical transparency, and enhanced benchmarking to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal conversational assistant based on surgical videos. Additionally, they introduce a new dataset with 102,000 question-answer pairs for training multimodal LLMs. The authors provide details of their data generation procedure, which is carefully designed to avoid hallucinations. The paper provides detailed comparisons with existing general-purpose and surgical-purpose datasets. Lastly, the authors provide a human and LLM evaluation of the dataset, showing consistent scores.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- **Clarity**: The paper is well-written and easy to follow. \n- **Contributions**: This work makes a significant contribution to the development of surgical chat assistants. The dataset contains a wider range of surgical QAs compared to previous works. The proposed model and dataset may be valuable resources for researchers in this area.\n\n### Weaknesses\n\n- **Dataset Availability**: The surgical videos are available on WebSurg and are not a contribution of the authors. Therefore, the data availability may be subject to license changes from the content owners and WebSurg.\n- **Hallucinations and Data Quality**: As the authors mentioned, there may be hallucinations in the dataset, since it is automatically generated. The authors provide chatGPT and human evaluations, but that is not enough to infer the data quality.\n- **Model Availability**: It is not possible to reproduce the results since the model is not available yet, but enough details are provided to support the paper.\n\n### Questions\n\nThe paper is very well written and addresses its objectives. It also supports its claims and provides adequate experiments. Therefore, I am leaning toward accepting this paper, but I have some minor concerns regarding the legality of using WebSurg's surgical videos. I also have some questions:\n1. The authors mention that the model is limited by hallucinations, which is a serious concern for a surgical chatbot. Could you please provide more details, and types of hallucinations, and give some examples?\n2. Would it be possible to evaluate LLaVA-Surg on the SSG-VQA dataset? I am interested in knowing more about the breadth of your dataset and if it contains enough information for cross-dataset generalization.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nI am wondering about the WebSurg's policies on using their videos to train deep learning models, but I could not find any information about this in their terms of use.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents LLaVA-Surg, a multimodal conversational assistant designed for surgical videos, along with a new dataset comprising 102,000 question-answer pairs specifically tailored for training multimodal large language models (LLMs). The authors outline a thorough data generation process aimed at mitigating hallucinations, and they provide comprehensive comparisons with both general-purpose and surgical-focused datasets. In addition, the paper includes evaluations conducted by humans and LLMs, revealing consistent scoring across assessments.\n\n**Major Comments**  \n1. **Dataset Availability**: The surgical videos utilized are sourced from WebSurg and should not be considered a novel contribution by the authors. The stability of data availability might be impacted by licensing agreements with content owners.\n2. **Hallucinations and Data Quality**: The authors acknowledge the potential for hallucinations within the automatically generated dataset. While evaluations by ChatGPT and human reviewers are presented, these alone do not sufficiently ensure high data quality.\n3. **Model Availability**: The results cannot be reproduced as the model is currently unavailable, although the manuscript provides adequate details to support its claims.\n\n**Minor Comments**  \n- The manuscript is well-written and clear.\n- The study makes a notable contribution to the field of surgical chat assistants, presenting a more extensive collection of surgical QAs than previous works.\n- Concerns exist regarding the legality of utilizing WebSurg’s surgical videos for model training.\n\n**Summary Paragraph**  \nThe review of LLaVA-Surg highlights significant innovation in developing a conversational assistant for surgical contexts, coupled with a valuable dataset. However, concerns regarding the legal status of video usage and potential data quality issues due to hallucinations warrant attention. While the manuscript meets basic evaluation criteria, the lack of model reproducibility remains a notable limitation.\n\n**Decision Recommendation**  \nMinor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *LLaVA‑Surg*, a multimodal conversational assistant for surgical videos, along with a new dataset of approximately 102,000 question–answer pairs designed for training multimodal large language models (LLMs). The work details the data generation pipeline, emphasizing safeguards against hallucinations, and presents comparative analyses with existing general‑purpose and surgical‑specific datasets. Overall, the paper is clearly written, well‑organized, and presents a coherent narrative that can be followed easily.\n\n**Major Comments**  \n1. **Dataset Availability and Licensing:** The dataset relies on surgical videos sourced from WebSurg, which are not the authors’ own contribution. As data access depends on WebSurg’s licensing terms, availability and long‑term reproducibility could be affected by changes in external permissions. Clarification on the legality and sustainability of using these videos is warranted.  \n2. **Data Quality and Hallucinations:** Although the authors attempt to minimize hallucinations during data generation, the automatic nature of the process inherently introduces potential errors. The provided evaluations (ChatGPT and human review) are helpful but insufficient to fully assess dataset reliability for safety‑critical applications such as surgical chatbots.  \n3. **Reproducibility and Model Availability:** The model itself is not publicly released, which currently limits reproducibility of results. Nevertheless, the paper includes sufficient methodological details for conceptual understanding and partial replication of the described pipeline.  \n4. **Further Evaluation:** Extension of experiments to other surgical datasets—such as SSG‑VQA—could strengthen claims about dataset comprehensiveness and cross‑dataset generalization performance.\n\n**Minor Comments**  \n- The manuscript is written clearly and maintains consistent terminology.  \n- The authors could expand discussion on typical hallucination types with specific examples.  \n- Ensure that any mention of data sources aligns precisely with WebSurg’s official usage policies.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe submission is significant for advancing multimodal surgical AI dialogs and provides a valuable dataset with broader coverage than existing resources. Its innovation lies in adapting large multimodal language modeling to the surgical domain. Evaluation is reasonable but could be enhanced through independent quality assessments and external dataset testing. Reproducibility remains limited until both dataset and model access are formally resolved and clarified.  \n\n**Decision Recommendation**  \n**Recommendation: Minor Revision.** The work is strong and nearly publishable but requires clarification of data licensing, expanded discussion on hallucination types, and potentially additional evaluations to reinforce data validity and compliance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces LLaVA-Surg, a multimodal large language model for surgical video understanding, trained on Surg-QA, a novel dataset of 102,000 surgical video-instruction pairs. The authors propose a two-stage question-answer generation pipeline that extracts structured information from surgical lecture videos using WhisperX for transcription and Llama-3-70B for content generation. The pipeline categorizes surgical knowledge into four levels: object identification, action recognition, reasoning, and expert-level deduction/planning (Figure 1, Section 3). The dataset comprises 100K pairs from structured surgical video learning and 2K pairs for visual concept alignment, derived from 44K video clips across 2,201 surgical procedures (Table 2). LLaVA-Surg achieves superior performance compared to general-domain models (Video-LLaVA, Video-ChatGPT) on zero-shot surgical video question-answering tasks, with a score of 2.45 versus 1.32 and 1.04 respectively (Table 3).\n\n## Weaknesses\n\n• **Mathematical Formulation Clarity**\n  - Equation 2 provides an unclear definition of instruction construction, with inconsistent notation where Xt_q appears both as a conditional expression and standalone variable (Page 4, lines 195-200)\n  - The video-level feature derivation f ∈ R^(N+h×w)×D lacks explanation of how temporal and spatial features are actually concatenated given their different dimensionalities (Section 4, Page 7, lines 324-332)\n  - The multi-turn conversation construction in Equation 3 doesn't clearly specify how video tokens are handled in subsequent turns beyond the first (Page 4, lines 203-208)\n\n• **Dataset Quality and Validation Concerns**\n  - The two-stage pipeline claims to reduce hallucinations but provides only one cherry-picked example comparison against end-to-end approaches (Figure 3, Page 5), insufficient to validate systematic improvement\n  - No quantitative analysis of extraction accuracy from the information extraction stage is provided, making it impossible to assess pipeline reliability (Section 3, Page 4)\n  - The surgical visual concept alignment data uses only 2K samples from CholecT50, representing a tiny fraction that may not adequately cover surgical visual concepts (Page 5, lines 259-263)\n  - GPT-3.5-turbo evaluation framework lacks validation beyond correlation with two clinicians on 60 samples, raising questions about evaluation reliability across the full 4,359 test samples (Figure 5, Section 5.2)\n\n• **Experimental Rigor and Baseline Limitations**\n  - Comparison baselines are inappropriate as Video-LLaVA and Video-ChatGPT are general-domain models not designed for surgical content, making performance gaps expected rather than meaningful (Table 3, Page 7)\n  - Training details lack crucial information about data splits, potential overlap between training and test surgical procedures, and cross-contamination prevention (Section 5.1, Page 7)\n  - Qualitative evaluation relies heavily on cherry-picked examples without systematic analysis of failure modes or performance across different surgical types (Tables 4-8, Pages 9, 14-15)\n\n• **Technical Architecture Limitations**\n  - The temporal fusion mechanism using simple average pooling may lose important sequential information crucial for understanding surgical procedures (Section 4, Page 7, lines 324-332)\n  - No ablation studies demonstrate the contribution of individual components like the two-stage pipeline versus architectural choices (No direct evidence found in the manuscript)\n  - Limited analysis of how the model handles multi-turn conversations beyond the provided examples (Section 4, Page 7)\n\n## Suggestions for Improvement\n\n• **Enhance Mathematical Precision**\n  - Rewrite Equation 2 with consistent notation and clear conditional logic for instruction construction across different turns\n  - Provide detailed mathematical formulation of the temporal-spatial feature concatenation process, including dimensionality matching steps\n  - Clarify how video representations are maintained and accessed in multi-turn conversations beyond the initial turn\n\n• **Strengthen Dataset Validation**\n  - Conduct systematic comparison of the two-stage versus end-to-end pipeline across multiple metrics and examples, not just one cherry-picked case\n  - Implement quantitative evaluation of information extraction accuracy using expert annotation on a validation subset\n  - Expand surgical visual concept alignment data significantly or demonstrate that 2K samples provide adequate coverage through diversity analysis\n  - Validate GPT evaluation framework against expert annotations across broader sample sizes and different surgical procedure types\n\n• **Improve Experimental Design**\n  - Include surgical-domain specific baselines or adapt general models with surgical data for more meaningful comparisons\n  - Provide detailed dataset splitting methodology ensuring no procedure-level contamination between train and test sets\n  - Conduct comprehensive failure analysis across different surgical categories and question types rather than relying on selected examples\n\n• **Address Technical Architecture Concerns**\n  - Implement and evaluate more sophisticated temporal modeling approaches beyond average pooling for surgical sequence understanding\n  - Conduct ablation studies isolating the contributions of the two-stage pipeline, architectural components, and training strategies\n  - Provide systematic evaluation of multi-turn conversation capabilities with quantitative metrics rather than anecdotal examples",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents LLaVA-Surg, a multimodal large language model designed for surgical video understanding. This model is trained on Surg-QA, a novel dataset consisting of 102,000 surgical video-instruction pairs. The authors propose a two-stage question-answer generation pipeline that leverages WhisperX for transcription and Llama-3-70B for content generation. The structured surgical knowledge is categorized into four levels: object identification, action recognition, reasoning, and expert-level deduction/planning. LLaVA-Surg demonstrates improved performance over general-domain models on zero-shot surgical video question-answering tasks.\n\n**Major Comments:**  \n1. **Mathematical Formulation Clarity:** \n   - Equation 2’s definition of instruction construction is unclear and contains inconsistent notation.\n   - The derivation of video-level features lacks clarity regarding the concatenation of temporal and spatial features.  \n   - Construction of multi-turn conversations in Equation 3 is not explicitly defined for subsequent turns.\n   \n2. **Dataset Quality and Validation Concerns:** \n   - The claim that the two-stage pipeline reduces hallucinations is unverified, relying on a single example.\n   - There is no quantitative analysis of extraction accuracy provided.\n   - Surgical visual concept alignment data is based on a small sample size, raising concerns about its comprehensiveness.\n   - The evaluation framework’s validation is limited to correlation with two clinicians on a very small sample size.\n\n3. **Experimental Rigor and Baseline Limitations:** \n   - Comparison baselines used are not appropriate for assessing surgical content.\n   - Important details regarding data splits and potential overlap between training and test sets are omitted.\n   - The qualitative evaluation lacks systematic performance analysis across surgical types.\n\n4. **Technical Architecture Limitations:** \n   - The temporal fusion mechanism may overlook important sequential information.\n   - There are no ablation studies presented to show the contributions of the model's components.\n   - The analysis of multi-turn conversation handling is limited and needs further elaboration.\n\n**Minor Comments:**  \n- Improve the clarity of Equation 2 and ensure consistent notation.\n- Expand on the explanation of the video feature derivation process.\n- Clarify multi-turn video representation management.  \n- Include systematic evaluations and diverse samples for dataset validation.\n- Ensure dataset splitting methodology is detailed to avoid contamination in training and testing.\n\n**Summary Paragraph:**  \nThe manuscript shows significance as it addresses surgical video understanding with a novel dataset and improved model performance; however, concerns regarding its innovation are present in the unclear mathematical formulations and validation issues. Evaluation of the proposed method is limited, lacking robust comparisons and metrics. Reproducibility is questioned due to inadequate methodological transparency and lack of detailed validation. Overall, while the contribution is noteworthy, substantial improvements are necessary in terms of clarity, validation, and experimental rigor.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LLaVA-Surg*, a multimodal large language model designed for surgical video understanding. Built upon the newly introduced *Surg-QA* dataset comprising approximately 102,000 video–instruction pairs, the work proposes a two-stage question–answer generation framework combining WhisperX transcription and Llama‑3‑70B for content generation. The approach aims to extract structured knowledge at multiple reasoning levels, from object identification to expert-level surgical planning. The paper reports that LLaVA‑Surg outperforms general-purpose video–language models in zero‑shot surgical video QA tasks. Overall, the manuscript is ambitious in scope and introduces an interesting dataset–model combination, though several aspects of mathematical formulation, dataset validation, and experimental design require clarification and stronger empirical support.  \n\n**Major Comments**  \n1. **Mathematical Formulation Clarity** – Equation 2 lacks consistent notation and clear conditional definitions; Equation 3 does not specify how video tokens are maintained across dialogue turns. The derivation of video‑level features \\( f \\in \\mathbb{R}^{(N+h\\times w)\\times D} \\) is insufficiently explained, particularly regarding temporal–spatial concatenation.  \n2. **Dataset Quality and Validation** – The claim that the two‑stage pipeline reduces hallucination relies on a single illustrative example without quantitative support. No accuracy metrics are provided for the information extraction stage. The visual concept alignment subset (2K samples) may be too small to ensure adequate coverage. The GPT‑based evaluation framework is only lightly validated (60 samples), limiting confidence in large‑scale results.  \n3. **Experimental Rigor and Baselines** – Reported comparisons against general‑domain models (Video‑LLaVA, Video‑ChatGPT) yield unsurprising gaps due to domain mismatch. Details about training–testing splits and prevention of procedure overlap are missing. Qualitative analyses include cherry‑picked examples without systematic failure analyses across surgical types.  \n4. **Technical Architecture and Ablations** – The use of average pooling for temporal fusion may omit essential sequential information. The paper lacks ablation studies to assess contributions of the two‑stage design or architecture components, and does not quantitatively evaluate multi‑turn conversation handling.  \n\n**Minor Comments**  \n- Improve clarity and consistency of notation in equations and variable definitions.  \n- Ensure figure labeling and table references (e.g., Figures 3, 5; Tables 4–8) directly correspond to described content.  \n- Provide clear definitions for acronyms at first use and correct minor typographical inconsistencies.  \n\n**Summary Paragraph (TMI Criteria)**  \n*Significance*: The work targets an important and emerging direction—surgical video understanding—with potentially high clinical and educational impact.  \n*Innovation*: Introduction of the *Surg‑QA* dataset and a two‑stage pipeline are novel, but methodological innovations need clearer articulation.  \n*Evaluation*: Current evaluation is limited by small validation subsets, inadequate baselines, and lack of quantitative ablations, reducing evidential strength.  \n*Reproducibility*: Missing details on data splits and processing pipelines hinder reproducibility. Addressing these aspects would substantially strengthen the manuscript.  \n\n**Decision Recommendation**  \n**Major Revision** — The paper presents promising ideas and data resources but requires significant clarification, more rigorous validation, and comprehensive experiments before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n   ⚠️ The paper, *LLAVA-Surg: Towards a Multimodal Surgical Assistant via Structured Lecture Learning*, primarily addresses data generation and multi-modal LLM training for surgical video QA. The focus is on building and fine-tuning large pre-trained vision-language models (based on CLIP + LLaMA) with a new dataset (Surg-QA). While relevant to medical applications, it does not contribute new imaging physics, reconstruction, or quantitative imaging methodology. Rather, it falls in the category of applied AI for video understanding. Therefore, it may be considered peripheral to IEEE TMI’s core scope and methodological novelty expectations.\n\n2. **Novelty & Contribution Level**  \n   The work is strongest in dataset creation and a proposed *two-stage QA generation pipeline* that reduces hallucinations when generating multimodal training data. This structured approach to automatic dataset curation is novel within the surgical video domain, yet largely relies on existing LLM and VLM architectures (CLIP, LLaVA, WhisperX, LLaMA-3). The methodological innovation is incremental compared with prior multimodal instruction-tuning frameworks. The claim of being the “first multimodal conversational assistant for surgery videos” is reasonable but mostly application-oriented.\n\n3. **Technical and Experimental Rigor**  \n   The data pipeline and training protocol are adequately described. Experimental evaluation across 4,359 QA pairs, plus limited human and GPT-based scoring, show internal consistency, though the evaluation is qualitative and proxy-based. Quantitative metrics for visual understanding (e.g., phase recognition, spatial localization, temporal coherence) are lacking. Reproducibility appears reasonable since the authors will release code and data. Clinical claims are limited to conversation quality and not clinical safety or utility.\n\n4. **Clarity and Presentation**  \n   The manuscript reads clearly with logical flow and well-labeled figures. Minor language irregularities (“LLaV A-Surg” spacing, etc.) could be corrected in editing. Figures are informative, and reference coverage is adequate.\n\n5. **Ethical and Reproducibility Compliance**  \n   The dataset is derived from publicly available WebSurg videos; no patient-identifiable data appear involved. Use conditions and open-source licensing are clearly stated (CC BY NC 4.0). No IRB concerns arise. Code and data release are promised, supporting reproducibility.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n1. **Summary**  \n   The paper introduces *Surg-QA*, a large instruction-tuning dataset of 102 k surgical video–QA pairs generated from lecture videos via a two-stage LLM-driven extraction and generation process. Using this dataset, the authors fine-tune a multimodal conversational model, *LLaVA‑Surg*, for surgical video question answering and compare it against existing VLMs (Video‑LLaVA, Video‑ChatGPT, LLaVA‑Med). GPT-based and clinician assessments show improved relevance and reduced hallucination in responses.\n\n2. **Strengths**  \n   - Creation of a large surgical video QA dataset filling a gap in multimodal surgical informatics.  \n   - Structured two-step QA generation approach that reduces hallucination versus direct prompting.  \n   - Clear documentation of dataset diversity and strong reproducibility practices.  \n   - Promising performance improvements for conversational understanding of surgical videos.  \n\n3. **Weaknesses**  \n   - Methodological contributions lie in data engineering and model fine-tuning, not novel medical imaging algorithms per se.  \n   - Evaluation relies heavily on GPT scoring; limited direct human validation or objective accuracy metrics.  \n   - No analysis of model uncertainty, bias, or generalization to unseen surgical domains.  \n   - The study focuses on model conversation quality rather than interpretable or quantitative imaging understanding, limiting its relevance to TMI’s core readership.\n\n4. **Major Comments**  \n   1. **Scope Alignment:** The work’s focus on multimodal LLM fine-tuning for conversations, rather than imaging methodology or quantitative reconstruction theory, suggests it may be below TMI’s novelty threshold. The authors should clarify methodological advances that directly benefit *medical imaging science* (e.g., structured representation learning from videos, temporal attention mechanisms).  \n   2. **Evaluation Depth:** Replace or complement GPT-based evaluation with standardized visual QA metrics or expert scoring across multiple raters, including inter-rater consistency.  \n   3. **Model Transparency:** Discuss mechanisms for error analysis, hallucination quantification, and interpretability.  \n   4. **Generalizability:** Provide cross-dataset validation (e.g., other surgical video datasets) to support claims of broad applicability.  \n   5. **Ethical Reporting:** Explicitly state that videos were de-identified and publicly licensed for research use.\n\n5. **Minor Comments**  \n   - Correct repeated typography (“LLaV A-Surg” → “LLaVA‑Surg”).  \n   - Streamline sections to reduce overlap between Related Work and Introduction.  \n   - Clarify figure captions for Figures 1–4 (legibility of small text).  \n   - Include dataset statistics in tabular supplement rather than verbose inline description.  \n   - Update reference formatting to IEEE style.\n\n6. **Overall Recommendation**  \n   **Reject (Out of Scope / Low Novelty)** – While technically solid and valuable for the surgical computing community, the contributions are primarily in dataset construction and LLM fine-tuning for conversational understanding. These advances are more appropriate for venues focused on medical AI applications or multimodal learning (e.g., *MedIA*, *IEEE JBHI*, or domain-specific conferences). The paper does not meet TMI’s threshold for fundamental methodological innovation in medical imaging science.\n\n7. **Confidence Level:** 5 / 5",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript *LLaVA‑Surg: Towards a Multimodal Surgical Assistant via Structured Lecture Learning* presents the development of *Surg‑QA*, a large dataset of surgical video–question‑answer pairs derived through a two‑stage large language model (LLM) pipeline, and the training of a multimodal conversational model, *LLaVA‑Surg*. The study aims to enable more accurate and context‑aware question answering for surgical videos. The paper is clearly written, logically structured, and supported by informative figures and appropriate referencing. However, its focus lies primarily in applied multimodal AI and conversational modeling rather than core medical imaging methodology, placing it somewhat outside the traditional scope of IEEE TMI.  \n\n**Major Comments**  \n1. **Scope Alignment:** The work emphasizes multimodal LLM fine‑tuning for dialogue rather than methodological innovations in medical imaging, rendering it potentially outside TMI’s core focus. The authors should clarify methodological contributions that advance imaging science, such as structured video representation or temporal modeling techniques.  \n2. **Evaluation Depth:** The assessment relies heavily on GPT‑based scoring with limited expert validation. Incorporating standardized visual QA metrics, multiple human raters, and inter‑rater agreement would strengthen the evaluation.  \n3. **Model Transparency:** Further analysis of hallucination mechanisms, interpretability, and quantitative error characterization is needed to ensure scientific rigor.  \n4. **Generalizability:** Validation on additional surgical video datasets would demonstrate robustness and support claims of broad applicability.  \n5. **Ethical Reporting:** Although the videos are publicly sourced and anonymized, an explicit statement confirming de‑identification and permissible research use is recommended.  \n\n**Minor Comments**  \n- Standardize terminology and correct typographic errors (e.g., “LLaV A‑Surg” → “LLaVA‑Surg”).  \n- Reduce redundancy between Introduction and Related Work.  \n- Improve figure captions for readability and add dataset statistics in tabular form.  \n- Ensure references comply with IEEE formatting.  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the work makes a useful contribution to surgical informatics through large‑scale dataset release and code sharing, but its impact on imaging methodology is limited. **Innovation** lies in the structured two‑stage QA generation pipeline, yet model components largely reuse existing architectures. **Evaluation** demonstrates qualitative improvements using internal and GPT‑based metrics but lacks quantitative or clinically validated measures. **Reproducibility** is commendable, supported by open data, code, and transparent procedures. Overall, the study is technically solid but methodologically incremental and outside TMI’s core research domain.  \n\n**Decision Recommendation:** **Reject (Out of Scope / Low Novelty)** — The submission offers valuable applied multimodal AI contributions but does not meet TMI’s threshold for fundamental methodological innovation in medical imaging.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript titled *LLAVA-Surg: Towards a Multimodal Surgical Assistant via Structured Lecture Learning* presents a study focused on data generation and multi-modal LLM training specifically for surgical video question answering (QA). The authors develop and fine-tune large pre-trained vision-language models using a new dataset named Surg-QA. Although the study contributes to medical applications, it primarily pertains to applied AI rather than advancing fundamental methodologies in imaging physics or quantitative imaging, thus positioning it on the periphery of IEEE TMI’s core scope.\n\n**Major Comments:**  \n1. **Scope Alignment:** The focus on multimodal LLM fine-tuning for conversational tasks rather than quantitative imaging or reconstruction methods indicates it may fall short of TMI’s novelty expectations. The authors are encouraged to clarify any methodological advancements that benefit the field of medical imaging science, such as structured representation learning.  \n2. **Evaluation Depth:** The evaluation heavily relies on GPT-based scoring. It is recommended to supplement or replace this with standardized visual QA metrics or expert assessments to enhance rigor.  \n3. **Model Transparency:** There is a need for a discussion on error analysis mechanisms, quantification of hallucinations, and overall interpretability of the models.  \n4. **Generalizability:** To substantiate claims of broad applicability, cross-dataset validation with other surgical video datasets should be included.  \n5. **Ethical Reporting:** It is crucial to explicitly note that the videos used are de-identified and publicly licensed for research purposes.\n\n**Minor Comments:**  \n- Correct typographic inconsistencies (e.g., “LLaV A-Surg” to “LLaVA‑Surg”).  \n- Streamline content to reduce redundancy between the Related Work and Introduction sections.  \n- Enhance the clarity of figure captions for Figures 1-4, particularly regarding legibility of smaller text.  \n- Move dataset statistics to a tabular supplement instead of verbose inline descriptions.  \n- Update references to conform to IEEE formatting.\n\n**Summary Paragraph:**  \nThe study introduces *Surg-QA*, a significant dataset of 102k surgical video QA pairs, employing a novel two-stage extraction process for instruction tuning. While the creation of this dataset and the structured QA generation pipeline represent valuable contributions to surgical informatics, the methodological innovations are mainly incremental in context to existing frameworks in multimodal instruction-tuning. Evaluation methods raise concerns regarding reliance on GPT-based scoring without sufficient human validation or quantitative measures, thus affecting the overall assessment of reproducibility and significance within the TMI framework.\n\n**Decision Recommendation:**  \n**Reject (Out of Scope / Low Novelty)** – Although the technical aspects and dataset construction are commendable and serve the surgical computing community, the contributions lean more toward dataset generation and LLM enhancement for conversational interfaces. These developments are more suitably aligned with journals focused on medical AI or multimodal applications, such as *MedIA* or *IEEE JBHI*, rather than meeting TMI’s threshold for significant contributions in medical imaging science. \n\n**Confidence Level:** 5 / 5",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Brian R Quaranto",
      "Garrett Skinner",
      "Gene Yang",
      "Jiajie Li",
      "Jinjun Xiong",
      "Peter C W Kim",
      "Steven D Schwaitzberg"
    ],
    "url": "pdfs/iclr.cc-2025-conference_04d73daf100581d96e3a971dd358d0aad68ebdd1.pdf",
    "remote_url": "https://openreview.net/pdf/04d73daf100581d96e3a971dd358d0aad68ebdd1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Hallucination Benchamrk",
      "Hallucination Evaluation Method",
      "Medical Large Vision Language Model"
    ],
    "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations—a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MedihallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MedihallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work have been released.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a hallucination detection and evaluation benchmark in the medical multimodal domain. Recognizing that large vision-language models often inherit hallucination tendencies from foundational large-language models, the authors propose a framework for more accurate and detailed hallucination assessment in high-stakes medical applications. \nThe paper makes three primary contributions: (i) Med-HallMark, the benchmark dedicated to medical hallucination detection (ii) MediHallDetector, a hallucination detection model tailored for the medical domain; and (iii) MediHall Score, an evaluative metric designed for hallucinations of different severity and types. \nThrough extensive experiments, the paper claims that MediHall Score provides more nuanced insights than traditional metrics and that MediHallDetector enhances hallucination detection performance in medical LVLMs.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well-motivated in that it addresses a critical and underexplored area in medical LVLMs: detecting and evaluating hallucinations, which is crucial for ensuring safety in clinical applications. The work frames hallucination detection specifically for the medical domain. The MediHall Score introduces a nuanced metric that prioritizes clinical impact by differentiating hallucinations based on their severity and type. The ablation study of different SFT methods for training the detector also adds robustness to the study.\n\n### Weaknesses\n\nThe paper raises several questions about primarily the soundness of evaluation and the reliability of the proposed metric (detailed in the **Questions** section). For example, the paper limits its IRG evaluation to only one domain-specific model, XrayGPT, which is problematic given that IRG tasks are where hallucination metrics are particularly valuable. Additionally, results for XrayGPT in IRG appear inconsistent with those of Minigpt-4, which was shown to underperform in traditional metrics but achieved the highest MediHall Score among all evaluated models, calling into question the reliability of the MediHall Score. The paper could have strengthened its findings by evaluating other state-of-the-art LVLMs, such as open-source, domain-specific models like BiomedGPT or Med-Flamingo, or proprietary MLLMs like GPT-4 and Gemini Pro, to validate the metric’s robustness. Finally, the experimental design lacks a thorough investigation into the role of image inputs in tuning MediHallDetector. The study could benefit from ablation studies that assess performance with and without image inputs, especially given that the image encoder and connector taken from LLaVA are frozen during training and could have introduced extra errors due to its lack of visual understanding ability. These design choices suggest that the current benchmark version may be limited in scope and could benefit from further refinement and validation.\n\n### Questions\n\n**Construction of the benchmark:**\n1. Could you elaborate on the motivation behind designing confidence-weakening and counterfactual questions beyond the \"conventional\" questions in established test sets like SLAKE and RAD-VQA? Regarding counterfactual question generation, considering the GPT model is provided with limited information (text-only, single question with its ground-truth answer), how is the quality of GPT-generated counterfactual questions and answers ensured?\n\n2. Robust evaluation of hallucinations in open-ended IRG tasks is inherently challenging, particularly due to synonymous terms (in frameworks like ICD-10, multiple expressions might refer to the same condition). Since the ground truth for open-ended IRG scenarios in this paper derives from medical reports in MIMIC and OpenI test sets, how does the evaluation framework account for potential synonyms in model responses given the limited set of 1800 images and their reports seen by MediHallDetector?\n\n3. In using LLaVA-Med to generate GT responses for Med-VQA, why not utilize the GT already provided in the dataset, given that the questions originate from an established source? For the IRG scenario, are model-generated responses also exclusively from LLaVA-Med? If so, might this narrow distribution affect the generalizability of MediHallDetector, as it is fine-tuned specifically on LLaVA-Med responses?\n\n**Experimental Design:**\n1. How much does the image come into play in tuning the MedihallDetector model? It could be necessary to ablate w./w.o. image input, especially when you freeze the image encoder and connector during training.\n\n**Results**\n1. For medical VQA tasks (Table 1), why introduce a MediHall Score when accuracy already exists as a metric for Med-VQA? Notably, XrayGPT, with an accuracy of only 0.02, has a MediHall Score of 0.36. Could this indicate potential inflation in the MediHall Score?\n\n2. For medical IRG tasks (Table 2), the results for XrayGPT as measured by MediHallDetector seem counter-intuitive compared to Minigpt-4. In the original XrayGPT study [1], Minigpt-4 was a baseline against which improvements were demonstrated using ROUGE (Table 1 in [1]). Here, XrayGPT does outperform Minigpt-4 in all conventional metrics, but in the MediHall Score, with Minigpt-4 obtaining the highest MediHall Score among evaluated models. Could this raise concerns regarding the reliability of the MediHall Score metric?\n\n3. Could you clarify why models like BLIP2, LLaVA-Med, and RadFM do not receive MediHall Scores for medical IRG tasks? Open-ended IRG tasks are particularly significant since accuracy is already used as a metric for VQA. Presently, only XrayGPT is evaluated in IRG tasks using the proposed model and metric; incorporating additional state-of-the-art models would enhance the proposed metric's robustness. For instance, open-source domain-specific models such as BiomedGPT [2] and Med-Flamingo [3], which perform well on established tasks, or widely used proprietary MLLMs like GPT-4, Claude 3.5, and Gemini Pro could be valuable benchmarks.\n\n**Typo**\n1. In Figure 2(a), “pairs” is misspelled.\n2. Could you clarify whether XrayGPT is used in constructing the benchmark? It appears in Figure 1(b) but is not referenced in section 3.4.\n\n[1] https://aclanthology.org/2024.bionlp-1.35.pdf\n[2] https://arxiv.org/abs/2305.17100\n[3] https://arxiv.org/abs/2307.15189\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nn/a",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a benchmark for hallucination detection and evaluation within the medical multimodal domain. Recognizing the propensity for hallucination in large vision-language models (LVLMs) derived from foundational large-language models, the authors propose a framework aimed at improving the accuracy of hallucination assessments in critical medical applications. The contributions include Med-HallMark, a benchmark tailored for medical hallucination detection; MediHallDetector, a model for detecting hallucinations in medical contexts; and the MediHall Score, a metric designed to evaluate hallucinations of varying severity and types. Through comprehensive experiments, the authors claim that their proposed metrics and models yield improved insights compared to existing methodologies.\n\n### Major Comments\n1. The evaluation of the proposed methodology primarily relies on a single domain-specific model, XrayGPT. This limitation raises concerns regarding the robustness of the evaluation framework.\n2. Results indicate discrepancies in performance where XrayGPT's results diverge from those of Minigpt-4, suggesting potential issues with the reliability of the MediHall Score.\n3. The study lacks evaluation of additional state-of-the-art LVLMs, which could provide a more comprehensive validation of the proposed metrics.\n4. The experimental design does not sufficiently investigate the impact of image inputs on the performance of MediHallDetector, which should be addressed through further ablation studies.\n\n### Minor Comments\n- Figure 2(a) contains a typographical error; “pairs” is misspelled.\n- Clarification is needed on XrayGPT's role in constructing the benchmark, as it appears in Figure 1(b) but is not discussed in section 3.4.\n\n### Summary Paragraph\nThe manuscript aligns with TMI editorial criteria by highlighting a significant issue—hallucinations in medical LVLMs—making its contribution noteworthy for future clinical applications (significance). The innovation is evident in the introduction of Med-HallMark and the MediHall Score, as they provide tailored evaluation metrics for the medical domain. However, the limitations in experimental scope, particularly regarding the evaluation across various models and the role of image inputs, raise questions about reproducibility and overall robustness.\n\n### Decision Recommendation\nMajor revision is recommended to address the methodological concerns raised, specifically regarding the evaluation framework's scope and robustness.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a hallucination detection and evaluation benchmark tailored to the medical multimodal domain, addressing the challenge that large vision-language models (LVLMs) often inherit hallucination tendencies from general-purpose language models. The work’s main contributions include: (i) *Med-HallMark*, a benchmark for detecting medical hallucinations; (ii) *MediHallDetector*, a model for domain-specific hallucination detection; and (iii) *MediHall Score*, a novel evaluation metric differentiating hallucination severity and type. Overall, the study is well-motivated and emphasizes clinical safety, though issues in the evaluation design and reliability of results limit its soundness.\n\n**Major Comments**  \n1. **Evaluation Scope and Reliability:** The study evaluates primarily on XrayGPT, limiting the generalizability of MediHall Score and MediHallDetector. Broader benchmarking with additional state-of-the-art or domain-specific LVLMs (e.g., BiomedGPT, Med-Flamingo, GPT‑4, Gemini Pro) is necessary to validate robustness.  \n2. **Metric Consistency:** Discrepancies between conventional metrics and MediHall Score raise questions about the latter’s reliability. For instance, XrayGPT’s high MediHall Score despite low accuracy suggests possible inflation or inconsistency in metric calibration.  \n3. **Benchmark and Data Construction:** The motivation for generating “confidence-weakening” and “counterfactual” questions beyond those in established datasets (e.g., SLAKE, RAD-VQA) should be clarified, as should the quality control of GPT‑generated counterfactuals.  \n4. **Handling Synonyms in IRG Evaluation:** Since open-ended image–report generation (IRG) tasks involve synonymous or equivalent medical terms, the evaluation framework’s strategy for handling lexical variation needs to be specified.  \n5. **Experimental Design and Ablation:** The contribution of image input during MediHallDetector training is unclear. Given that the image encoder and connector are frozen, an ablation study with and without image input would clarify their role and potential limitations.  \n6. **Use of LLaVA-Med Outputs:** The rationale for using LLaVA-Med to generate ground truth responses instead of dataset-provided ones requires explanation, as dependence on a single generator may reduce generalizability.  \n7. **Incomplete Reporting:** Some models (BLIP2, LLaVA-Med, RadFM) lack MediHall Scores for IRG tasks, constraining comparative analysis.\n\n**Minor Comments**  \n- Clarify whether XrayGPT contributes to benchmark construction, as implied by Figure 1(b).  \n- Correct the typo in Figure 2(a) (“pairs” misspelled).  \n- Some figures and results tables could benefit from clearer cross-referencing within the text.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe topic is significant and understudied, addressing hallucination detection in medical LVLMs—a high-impact area for safety and reliability. The benchmark and metric are conceptually innovative, introducing domain-specific constructs. However, evaluation suffers from limited scope, inconsistent metric behavior, and insufficient validation across diverse models. The reproducibility of results would improve with more transparent dataset construction and ablation analyses. Overall, the paper’s novelty is balanced by methodological weaknesses that currently hinder confidence in its findings.\n\n**Decision Recommendation**  \n**Major Revision.** The work presents an important and timely contribution but requires broader evaluation, clearer methodological justification, and additional validation to establish soundness and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes an LLM/LVLM evaluation benchmark, especially for the hallucination in the medical domain tasks. It also proposes a metric to evaluate the severity of hallucination in LVLMs, and a fine-tuned evaluator model is released to perform the universal evaluation for given set information based on (input image I, the original prompt P, the LVLM answer A and the ground truth GT).\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper proposes a novel benchmark for evaluation the LLMs specially in the Medical domain, and provides a comprehensive framework from dataset, metric, and the fine-tuned evaluation model, which is a completed work through the evaluation pipeline.  \n●\tThe paper is writing in a smooth way, easy to follow and simple to understand.  \n●\tThe work is of good significance, should be a meaningful angle to advance the LLM/LVLMs applications in real-world healthcare domain.\n\n### Weaknesses\n\n●\tThe paper did not emphasize the special challenges in Healthcare domain, after the reading, except for the first part of the dataset that involves the `Medical` multi-modality data such as CT, the evaluation process is on the common hallucination challenges from any Vision-language model, it raises the concerns that whether the proposed model stands out from common LVM hallucination evaluation, is other existing work able to solve the same question by simply adapting to the medical data? What is the advantage of the proposed fine-tuned model over other baseline methods?  \n●\tThe hierarchical definition of the MediHALL score seems a bit intuitive, which is a simple evenly scaled value on 5 categories.  \n●\tThe paper lacks discussion on the relevant work that conduct similar hallucination detection in LLMs at Healthcare domain.  \n●\tIt is suggested that the paper could make further clarification on certain questions as in the Question section, if most critical concerns can be addressed, it is plausible to raise the score for the paper’s quality evaluation.\n\n### Questions\n\n1. The authors explicitly defined 5 types of hallucination levels, among which: Catastrophic Hallucination, Critical Hallucinations, Minor Hallucination are severity levels, but the attribute, prompt-induced ones is indeed the `cause` of the hallucinations. There raise doubts of the rationale of such empirical classifications including attribute, prompt-induced to severity levels. \n \n2. As in line 215, the conventional Q_{conv} questions are generated by the GPT3.5, how is the quality guaranteed? How is the initial conv question obtained?  \n \n3. LLaVA-Med was used to infer answers, what is the size of the whole dataset? Even though the authors claim that the data has been examined, how is the quality guaranteed?   \n \n4. Also, for the dataset part that is relevant to the IRG scenario, the authors use a sampling method to draw 1,800 images and their corresponding medical reports from existing datasets: MIMIC-test and OpenI datasets. This weakens the contribution towards the data as proposed in line 065.  \n \n5. In line 255, “This fine-grained metric xxx”, starts abruptly, since the previous paragraph discusses the existing metrics drawbacks. Then it directly refers to “This metric”, which is not transitioning naturally.  \n \n6. The MediHALL score is relatively simple, which is built upon the pre-categorized types of hallucination levels and assigns different values in a hierarchical way.  Based on this, the latter human-annotation and fine-tuning are conducted, are they able to maintain the objectiveness?  \n \n7. In line 318, which directs the training data details to Figure, it explicitly shows the categories, and the types of training data covered for MEDIHALLDETECTOR, but what about the amount of the data? And how is the instruction pair data derived?  \n \n8. While the paper is positioned as the first benchmark for medical LM hallucination evaluation, there is relevant work worth referencing:  \n\n[1] MEDHALU: Hallucinations in Responses to Healthcare Queries by Large Language Models \n[2] MEDIC: TOWARDS A COMPREHENSIVE FRAMEWORK FOR EVALUATING LLMS IN CLINICAL APPLICATIONS \n[3] Faithfulness Hallucination Detection in Healthcare AI\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph:\nThe manuscript presents a framework for evaluating Large Language Models (LLMs) and Large Vision-Language Models (LVLMs), specifically addressing hallucination challenges within the medical domain. It introduces a novel benchmark along with a metric to assess the severity of hallucinations, complemented by a fine-tuned evaluator model designed for universal evaluation based on a combination of input images, prompts, LVLM outputs, and ground truths. The paper is generally well-structured, making it accessible and understandable for readers.\n\n### Major Comments:\n1. The manuscript does not sufficiently emphasize the unique challenges associated with the healthcare domain. Most of the evaluation appears to address typical hallucination challenges present in vision-language models, raising questions about the distinctiveness and advantages of the proposed evaluation compared to existing methods.\n2. The hierarchical definition of the MediHALL score may lack depth, appearing rather intuitive and based on simple scaling across five categories.\n3. A discussion surrounding existing work in hallucination detection relevant to LLMs in the healthcare domain is noticeably absent.\n4. Further clarification is needed on several aspects raised in the \"Questions\" section; addressing critical concerns could improve the manuscript's overall evaluation score.\n\n### Minor Comments:\n- The classification of hallucination levels raises doubts about the rationale behind including 'prompt-induced' as a cause of hallucinations.\n- The quality control of questions generated by GPT-3.5 should be elaborated upon, specifically regarding their sourcing.\n- Information regarding the size and quality assurance of the dataset utilized with LLaVA-Med is required.\n- The sampling method of drawing from MIMIC-test and OpenI datasets appears to diminish the proposed contribution towards the dataset.\n- A smoother transition is needed in the discussion of the fine-grained metric, as it jumps from metrics' drawbacks to the proposed solution abruptly.\n- The objectiveness of the human-annotation and fine-tuning process following the MediHALL score framework should be questioned.\n- Details regarding the amount of training data for MEDIHALLDETECTOR are lacking, as is a description of how the instruction pair data was derived.\n- Relevant references to prior work should be included to substantiate claims in the field.\n\n### Summary Paragraph:\nIn summary, the manuscript addresses a significant issue in evaluating hallucinations in LLMs, enhancing innovation in the medical field. However, its evaluation design lacks clear distinctiveness from existing frameworks, which affects its reproducibility. While it provides useful contributions in terms of metrics and a novel evaluation model, more rigorous discourse on existing literature and unique challenges in healthcare is necessary to fulfill the evaluation criteria thoroughly.\n\n### Decision Recommendation:\nMinor revision is recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a benchmark and evaluation framework for assessing hallucinations in large language and vision-language models (LLMs/LVLMs) within the medical domain. It further proposes a new metric, MediHALL, intended to quantify hallucination severity across multiple levels, and presents a fine-tuned evaluator model for universal scoring based on multimodal inputs (image, prompt, answer, and ground truth). The paper is clearly written, conceptually complete, and potentially significant for advancing reliability assessment in medical AI applications.\n\n---\n\n**Major Comments**  \n1. **Domain-Specificity and Novelty:** The study does not sufficiently highlight the unique challenges of hallucination detection specific to the healthcare domain. Much of the evaluation appears transferable from general LVLM hallucination work, raising questions about the distinct advantage of this medical-focused benchmark and fine-tuned model.  \n2. **Metric Design:** The hierarchical definition of the MediHALL score appears somewhat ad hoc, based on evenly scaled categories rather than a justified empirical framework. This simplification may limit interpretability and validity.  \n3. **Related Work:** The discussion overlooks existing work in medical hallucination benchmarking (e.g., MEDHALU, MEDIC, and Faithfulness Hallucination Detection in Healthcare AI). These should be explicitly compared to clarify novelty.  \n4. **Classification Rationale:** The inclusion of “attribute” and “prompt-induced” hallucinations as severity levels seems conceptually inconsistent, as they represent causes rather than degrees of severity.  \n5. **Dataset Quality and Scope:** More details are needed on dataset size, sampling rationale, and quality control, especially for MIMIC-test and OpenI subsets. The reliance on 1,800 sampled images may reduce the dataset’s originality.  \n6. **Evaluation and Clarity Issues:** Transitions between sections (e.g., line 255) are abrupt. The explanation of training data composition and instruction pairs for MEDIHALLDETECTOR (line 318) lacks quantitative clarity.  \n7. **Quality Assurance:** The generation of conventional questions via GPT‑3.5 (line 215) requires justification regarding quality assurance and reproducibility of initial prompts.\n\n---\n\n**Minor Comments**  \n- The paper is generally clear and well written, though minor stylistic improvements could enhance flow.  \n- Ensure objective phrasing when describing annotation and fine-tuning procedures; clarify how human evaluation consistency is maintained.  \n- Minor inconsistencies in figure referencing and paragraph transitions should be addressed.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work offers a timely attempt to formalize hallucination evaluation in medical LVLMs, an area of high significance. Its strength lies in integrating dataset, metric, and evaluator components into one framework. However, innovation is limited by conceptual overlap with general hallucination benchmarks, underdeveloped metric justification, and insufficient discussion of prior art. Evaluation design and dataset description need greater transparency to enhance reproducibility. With stronger empirical grounding, clearer distinctions from non-medical benchmarks, and inclusion of comparative baselines, the contribution could become substantially more robust.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a new benchmark, Med-HallMark, and proposes evaluation tools specifically tailored to medical contexts. Med-HallMark provides a multi-dimensional framework to identify and assess hallucinations generated by LVLMs. The benchmark includes multi-tasking hallucination support, diverse data for hallucination detection, and a hierarchical categorization of hallucinations based on clinical impact. Additionally, the paper presents MediHall Score, a metric designed to evaluate hallucinations in medical text output, and MediHallDetector, a specialized LVLM aimed at improved hallucination detection through multi-task training.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The introduction of Med-HallMark and MediHall Score fills a significant gap by addressing hallucination detection in medical LVLMs.\n- The proposed categorization of hallucinations is innovative and highly relevant, allowing for analysis of potential model impacts on medical decision-making.\n- The authors conduct extensive experiments comparing popular LVLM models on Med-HallMark.\n\n### Weaknesses\n\n- Is this very different from the accuracy? After reading the paper, I just feel that this paper just makes a more fine-grained classification of errors, and the classification standard needs to be discussed and re-designed. For example, in the report generation task, the model outputs a nonsense sentence for the chest X-ray, \"This is a chest X-ray of a person\", which is obviously correct, but it is not what we want. How should this be judged?\nSo this makes me wonder whether such a benchmark is necessary.\n- Some recent related work [1,2,3,4,5,6] is missing. \n- The overall data scale is relatively small, and the medical image modalities involved are limited to radiology.\n- How to ensure the accuracy of annotation? \n- Since doctors are hired to do the annotation, have the possible ethical risks been resolved? For example, IRB approval, etc.\n\n\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Yan Q, He X, Wang X E. Med-HVL: Automatic Medical Domain Hallucination Evaluation for Large Vision-Language Models[C]//AAAI 2024 Spring Symposium on Clinical Foundation Models. 2024.\n\n[3] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[4] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n[5] Nan Y, Zhou H, Xing X, et al. Beyond the Hype: A dispassionate look at vision-language models in medical scenario[J]. arXiv preprint arXiv:2408.08704, 2024.\n\n[6] Yan Q, He X, Yue X, et al. Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA[J]. arXiv preprint arXiv:2405.20421, 2024.\n\n### Questions\n\n- The format of the reference is weird. Please check it.\n- The figure 2 (d) is vague.\n\n### Flag For Ethics Review\n\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nSince doctors are hired to do the annotation, have the possible ethical risks been resolved? For example, IRB approval, etc.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents Med-HallMark, a new benchmark aimed at improving hallucination detection in medical Large Vision Language Models (LVLMs). It introduces evaluation tools designed specifically for medical contexts, offering a multi-dimensional framework for identifying and assessing hallucinations. Notably, the paper details the MediHall Score—a metric for evaluating hallucinations in medical text—and MediHallDetector, a specialized LVLM that enhances hallucination detection via multi-task training.\n\n**Major Comments:**  \n1. The distinction between Med-HallMark and accuracy is unclear; it appears to refine classification of errors without sufficient discussion on standardization. For instance, in the report generation task, the model may produce valid yet irrelevant sentences, raising questions about assessment criteria.\n2. Several relevant recent works are omitted from the literature review, including references [1] to [6].\n3. The dataset appears limited in size and primarily focuses on radiological images, potentially restricting generalizability.\n4. Concerns regarding the accuracy of annotations need addressing, particularly in how they are verified.\n5. The ethical implications of using medical professionals for annotation should be clarified, specifically regarding Institutional Review Board (IRB) approval and associated risks.\n\n**Minor Comments:**  \n- The formatting of the references needs correction for consistency.  \n- Figure 2(d) lacks clarity and could benefit from improved exposition.\n\n**Summary Paragraph:**  \nThe manuscript's strengths lie in its innovative approach to hallucination detection through Med-HallMark and MediHall Score, addressing a critical gap within medical LVLM evaluation frameworks. However, potential weaknesses include the need for a clearer distinction from existing accuracy measures, the omission of recent literature, and concerns about the limited dataset and annotation practices. In terms of significance, the work presents meaningful contributions; on innovation, it introduces relevant categorization; however, the evaluation and reproducibility aspects require further enhancement to ensure robustness and ethical compliance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Med-HallMark*, a new benchmark designed to evaluate hallucinations produced by large vision–language models (LVLMs) in medical contexts. The work also presents two associated components: *MediHall Score*, a metric quantifying hallucination in text outputs, and *MediHallDetector*, an LVLM trained via multi-task learning to enhance hallucination detection. The benchmark aims to provide a multi-dimensional and clinically oriented framework for hallucination identification and classification. Overall, the manuscript is clear and well-organized, addressing a timely problem in medical AI evaluation.\n\n**Major Comments**  \n1. **Definition and necessity of the benchmark:** While the paper distinguishes different types of hallucination, the conceptual difference between hallucination detection and generic accuracy/error analysis is not fully convincing. The current categorization may largely resemble fine-grained error classification, and the criteria for classifying hallucinations could be refined.  \n2. **Benchmark validity:** The practical necessity of introducing a new benchmark is questioned, as the example provided (“This is a chest X-ray of a person”) illustrates challenges in definition and assessment. This raises doubts about the benchmark’s distinct contribution.  \n3. **Related work coverage:** Several recent and highly relevant studies ([1]–[6]) are missing and should be discussed to position the contribution within current literature.  \n4. **Dataset limitations:** The dataset size is relatively small and restricted to radiology images, which may limit generalizability.  \n5. **Annotation reliability and ethics:** The review queries how annotation accuracy is ensured and whether ethical clearance (e.g., IRB approval) has been obtained, given that physicians were involved in labeling.\n\n**Minor Comments**  \n- The references should follow a consistent citation format.  \n- Figure 2(d) lacks clarity and should be improved for readability.\n\n**Summary Paragraph (TMI Criteria)**  \nThe manuscript addresses a significant issue concerning the trustworthiness of medical LVLMs (significance), and its hierarchical categorization of hallucination types represents an innovative element. However, questions remain about the conceptual rigor and necessity of the benchmark (innovation), the sufficiency of experimental validation given limited data scope (evaluation), and reproducibility of results due to annotation details and ethics processes not being fully described (reproducibility).\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses hallucination detection and evaluation in Large Vision Language Models (LVLMs) for medical applications. The authors introduce Med-HallMark, the first benchmark specifically designed for medical multimodal hallucination detection, featuring multi-task support (Med-VQA and IRG), multifaceted data, and hierarchical hallucination categorization with five severity levels (Catastrophic, Critical, Attribute, Prompt-induced, Minor). They propose MediHall Score, a hierarchical evaluation metric that considers clinical impact severity, and MediHallDetector, a specialized LVLM for hallucination detection. The benchmark evaluation covers popular LVLMs including BLIP2, InstructBLIP, LLaVA variants, mPLUG-Owl2, XrayGPT, and others across traditional metrics (BertScore, METEOR, ROUGE, BLEU) and the proposed MediHall Score. Results demonstrate varying performance across models, with MediHallDetector achieving superior hallucination detection compared to GPT-3.5 and GPT-4.\n\n## Weaknesses\n\n• **Mathematical formulation clarity and consistency issues**\n  - The MediHall Score calculation in Section 4 lacks mathematical rigor with inconsistent notation (Hi vs Hj, pages 270-277)\n  - Score assignment rationale (Hc=0.0, Hcr=0.2, etc.) appears arbitrary without theoretical justification or empirical validation (page 270)\n  - The averaging formula for IRG tasks oversimplifies complex hallucination interactions within reports (equation on page 277)\n\n• **Benchmark construction methodology concerns**\n  - Heavy reliance on GPT-3.5/GPT-4 for question generation and rewriting introduces potential biases and circular dependencies (Section 3.4, pages 216-225)\n  - Manual annotation process lacks detailed inter-annotator agreement statistics beyond basic description of \"three experienced doctors\" (pages 262-269)\n  - Data sampling methodology for IRG tasks inadequately described with vague criteria like \"high-quality data\" (page 267)\n\n• **Evaluation framework limitations**\n  - Hierarchical hallucination categorization lacks systematic validation against clinical expert consensus (Section 3.3, pages 183-215)\n  - MediHall Score comparison with traditional metrics only provides selective examples rather than comprehensive statistical analysis (Table 4, Figure 3)\n  - Baseline model evaluation focuses primarily on English medical data, limiting generalizability claims (Tables 1-2, pages 324-377)\n\n• **Technical implementation gaps**\n  - MediHallDetector architecture description lacks sufficient technical detail about the \"dual-layer fully connected network\" connector (Figure 2a, page 306)\n  - Training methodology combines heterogeneous data sources without addressing potential domain mismatch issues (Section 5, pages 324-336)\n  - Ablation study design incompletely explores architectural choices and hyperparameter sensitivity (Table 5, page 510)\n\n• **Experimental rigor deficiencies**\n  - Limited statistical significance testing across model comparisons despite claims of superiority (Tables 1-3, pages 324-428)\n  - Evaluation consistency metrics for MediHallDetector based on small sample size of 300 inferences (page 486)\n  - Missing analysis of failure cases and edge conditions that could reveal systematic weaknesses (no direct evidence found in the manuscript)\n\n## Suggestions for Improvement\n\n• **Strengthen mathematical foundation and notation**\n  - Provide theoretical justification for MediHall Score weight assignments through clinical impact studies or expert validation\n  - Standardize notation throughout Section 4 and include formal mathematical definitions for all variables\n  - Develop more sophisticated aggregation methods for IRG tasks that account for semantic relationships between sentences\n\n• **Enhance benchmark construction rigor**\n  - Reduce dependency on generative AI by incorporating more human-generated questions and implementing systematic bias detection procedures\n  - Report comprehensive inter-annotator agreement statistics including Cohen's kappa and detailed disagreement resolution protocols\n  - Specify quantitative criteria for data quality assessment and provide detailed sampling methodology with reproducible procedures\n\n• **Strengthen evaluation methodology**\n  - Conduct systematic validation of hierarchical categorization through large-scale expert surveys and clinical impact assessments\n  - Provide comprehensive statistical analysis comparing MediHall Score with traditional metrics across diverse medical scenarios\n  - Expand evaluation to include multilingual medical data and cross-cultural validation studies\n\n• **Improve technical documentation and analysis**\n  - Provide detailed architectural specifications including layer dimensions, activation functions, and connection patterns for MediHallDetector\n  - Address domain adaptation strategies for combining heterogeneous training data and analyze potential negative transfer effects\n  - Conduct comprehensive ablation studies covering architectural choices, training strategies, and key hyperparameters\n\n• **Enhance experimental validation**\n  - Include statistical significance testing with appropriate corrections for multiple comparisons across all model evaluations\n  - Expand evaluation dataset size and conduct multiple independent validation rounds to establish robust consistency metrics\n  - Analyze systematic failure patterns and edge cases to identify model limitations and provide actionable insights for improvement",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThis manuscript focuses on hallucination detection and evaluation in Large Vision Language Models (LVLMs) specifically within medical applications. The authors present Med-HallMark, the inaugural benchmark tailored for medical multimodal hallucination detection, which includes multi-task support (Med-VQA and IRG), diverse data, and a hierarchical categorization of hallucinations across five severity levels. Additionally, they propose the MediHall Score, a new evaluation metric that emphasizes the severity of clinical impacts, along with MediHallDetector, an LVLM designed for hallucination detection. The evaluation encompasses several well-known LVLMs and employs both traditional metrics and the MediHall Score, revealing that MediHallDetector outperforms GPT-3.5 and GPT-4 in detecting hallucinations.\n\n## Major Comments\n1. **Mathematical formulation clarity and consistency**: \n   - The calculation of the MediHall Score in Section 4 exhibits inconsistent notation and lacks mathematical rigor.\n   - The rationale for score assignments appears arbitrary without strong theoretical or empirical backing.\n   - The averaging formula used for IRG tasks oversimplifies complex relationships in hallucination reports.\n\n2. **Benchmark construction methodology**: \n   - The heavy reliance on GPT-3.5/GPT-4 raises concerns about potential biases.\n   - The manual annotation process presents insufficient details on inter-annotator agreement.\n   - The data sampling methodology is inadequately described, especially regarding \"high-quality data.\"\n\n3. **Evaluation framework limitations**: \n   - Hierarchical categorization lacks systematic validation against expert consensus.\n   - The comparison of MediHall Score with traditional metrics fails to offer comprehensive statistical analysis.\n   - The evaluation focuses primarily on English medical data, limiting its generalizability.\n\n4. **Technical implementation gaps**: \n   - Insufficient description of the MediHallDetector architecture.\n   - The training methodology does not appropriately address domain mismatch challenges.\n   - The ablation study inadequately explores architectural choices.\n\n5. **Experimental rigor deficiencies**:\n   - The manuscript does not provide adequate statistical significance testing across model comparisons.\n   - Evaluation metrics for MediHallDetector rely on a limited sample size.\n   - There is a lack of analysis concerning failure cases.\n\n## Minor Comments\n- Standardize mathematical notation in Section 4 and clarify definitions.\n- Add inter-annotator agreement statistics and protocols.\n- Enhance descriptions of architectural details for MediHallDetector.\n\n## Summary Paragraph\nIn terms of significance, the manuscript addresses an important issue in medical AI. However, the innovation is somewhat diminished by inconsistencies in mathematical formulation and the potential bias in benchmark construction. The evaluation methods are not fully robust, primarily due to limited validation against clinical consensus. While reproducibility is attempted through established LVLMs, the heavy reliance on specific models raises concerns. The overall strength of results is hindered by the methodological weaknesses identified, particularly in evaluation rigor and generalizability.\n\n## Decision Recommendation\nMajor revision is recommended to address the identified weaknesses in mathematical clarity, benchmark methodology, evaluation framework, technical documentation, and experimental validation.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *Med‑HallMark*, a benchmark for hallucination detection and evaluation in Large Vision–Language Models (LVLMs) for medical imaging. It introduces a hierarchical hallucination categorization (five severity levels), a unified evaluation metric termed *MediHall Score* that incorporates clinical impact severity, and a dedicated detection model, *MediHallDetector*. The dataset integrates multiple tasks (Med‑VQA and IRG) and is evaluated across several existing LVLMs (BLIP2, InstructBLIP, LLaVA variants, mPLUG‑Owl2, XrayGPT, etc.) using both traditional metrics and the proposed score. The study is ambitious and addresses an important gap, but the mathematical rigor, benchmark construction methodology, and evaluation validation require substantial strengthening.  \n\n---\n\n**Major Comments**  \n1. **Mathematical Formulation and Rigor:**  \n   - The *MediHall Score* lacks clear mathematical justification; notation is inconsistent (e.g., Hi vs. Hj).  \n   - Weight assignments (Hc = 0.0, Hcr = 0.2, etc.) appear arbitrary with no theoretical or empirical grounding.  \n   - The averaging formula for IRG tasks oversimplifies interactions among hallucination types.  \n\n2. **Benchmark Construction Methodology:**  \n   - Heavy dependence on GPT‑3.5/GPT‑4 for question generation introduces potential biases and circularity.  \n   - Annotation protocol provides insufficient inter‑annotator agreement data beyond noting “three experienced doctors.”  \n   - Sampling criteria for IRG tasks are ill‑defined, with vague references to “high‑quality data.”  \n\n3. **Evaluation Framework Limitations:**  \n   - The hierarchical hallucination categorizations have not been validated via expert consensus.  \n   - Comparisons with traditional metrics are illustrative rather than statistically comprehensive.  \n   - Benchmark is limited largely to English data, raising generalizability concerns.  \n\n4. **Technical and Implementation Gaps:**  \n   - Architecture details for *MediHallDetector* are incomplete, especially regarding the “dual‑layer fully‑connected” connector.  \n   - Mixing heterogeneous training sources without addressing domain mismatch issues undermines reliability.  \n   - Ablation studies only partially explore model design and hyperparameter sensitivity.  \n\n5. **Experimental Rigor Deficiencies:**  \n   - No statistical significance testing supports the performance claims.  \n   - Consistency metrics rely on a small sample of 300 inferences.  \n   - Absence of failure‑case analysis prevents understanding of systematic weaknesses.  \n\n---\n\n**Minor Comments**  \n- Clarify notation consistency throughout Section 4.  \n- Improve figure and table cross‑referencing for clarity.  \n- Define all acronyms upon first use and proofread for typographical uniformity.  \n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant in addressing medical hallucination evaluation, and the benchmark concept is innovative. However, the contribution’s impact is limited by weak mathematical grounding and insufficiently validated benchmark methodology. Evaluation lacks statistical robustness, compromising reproducibility and confidence in conclusions. Strengthening theoretical justification, annotation rigor, and quantitative analysis would make the work more credible and reproducible.  \n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI – ✅**\n   The manuscript addresses *hallucination detection and evaluation in Large Vision-Language Models (LVLMs) applied to medical imaging tasks*. While the paper is under ICLR review and is motivated by AI/ML evaluation rather than pure imaging physics, it is relevant to *medical imaging informatics and AI safety*—both being within the extended methodological remit of *IEEE TMI*, provided the focus is on methodological contributions rather than application performance. Its primary contributions (a medical hallucination benchmark, a new evaluation metric, and a detection model) constitute a computational methodology in medical imaging analysis.\n\n2. **Novelty & Contribution Level – Moderate to High**\n   The introduction of **Med-HallMark**, a benchmark tailored to detecting multimodal hallucinations in medical LVLMs, appears to be the first of its kind. The **MediHall Score** provides a structured, clinically oriented scoring framework, and the **MediHallDetector** model is a novel fine-tuned LVLM variant optimized for hallucination detection. Together, these offer a *conceptual and empirical advance*. However, the methodological novelty lies mainly in systematic dataset design and metrics adaptation, rather than a new theoretical model for image formation or reconstruction. Novelty is strong within the subfield of *medical multimodal model evaluation* but relatively applied compared to core TMI methodological expectations.  \n\n3. **Technical and Experimental Rigor – Generally Solid, but Some Gaps**\n   The manuscript describes dataset curation, data sources (Slake, VQA-RAD, MIMIC, OpenI), annotation protocols, and hierarchical labeling by clinicians. Training and evaluation pipelines are transparent, and performance comparisons are comprehensive. That said:\n   - Statistical uncertainty or inter-rater agreement analysis is limited.\n   - Validation is confined to public datasets; clinical diversity or generalizability analysis is missing.\n   - Reproducibility details (scripts, annotation schema) are promised but not independently verified.\n\n4. **Clarity and Presentation – Good**\n   The structure is logical, figures are clear, and tables are thorough. Writing is occasionally verbose and could be tightened. Some inconsistent grammar and formatting occur but do not obscure meaning.\n\n5. **Ethical and Reproducibility Compliance – Adequate**\n   The paper properly addresses dataset licensing, anonymization, and non-use in real clinical decision support. Ethical statements are included, citing CC0/MIT/CC BY-NC data sources with IRB exemption. Code/data release is planned, meeting reproducibility expectations.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n1. **Summary**  \n   The paper introduces **Med-HallMark**, a benchmark for detecting and evaluating hallucinations in large vision–language models (LVLMs) applied to medical imaging tasks such as visual question answering and imaging report generation. It defines a *hierarchical taxonomy* of hallucination severity, proposes the **MediHall Score** metric for quantitative evaluation, and fine-tunes a model named **MediHallDetector** to classify hallucination types in multimodal inputs. Extensive experiments establish baseline performance for existing LVLMs and demonstrate improved alignment with human expert judgment.\n\n2. **Strengths**\n   - Presents the *first comprehensive medical benchmark* targeting hallucination detection in LVLM-generated clinical text.  \n   - Introduces a *clinically meaningful hierarchical scoring system* (MediHall Score).  \n   - Incorporates *expert medical annotation* and multi-task design (VQA + report generation).  \n   - Provides strong baseline experiments and comparative analyses.  \n   - Clearly motivated by patient safety and interpretability concerns.\n\n3. **Weaknesses**\n   - The methodological novelty primarily lies in *benchmark and metric creation*, not in algorithmic or theoretical innovation expected for TMI’s core audience.  \n   - Annotation details and interobserver reliability metrics are not quantitatively documented.  \n   - Limited discussion on *generalization* beyond radiographic domains or real diagnostic settings.  \n   - Model training and architecture largely follow existing LVLM schemes with modest adaptation.  \n   - Lacks uncertainty analysis and potential clinical validation case studies.\n\n4. **Major Comments**\n   1. **Scope Alignment:** To meet *TMI*’s methodological standard, the authors should emphasize how the benchmark or detection model generalizes to broader imaging modalities and contributes to imaging science, not only evaluation research. Including examples on MRI, CT, and ultrasound could strengthen scope relevance.\n   2. **Annotation Reliability:** Provide quantitative inter-rater statistics (κ or ICC) for clinician labeling and show how hierarchical categories achieve consistency.\n   3. **Statistical Analysis:** Expand evaluation with confidence intervals or hypothesis testing for score differences among models.\n   4. **MediHall Score Validation:** Include correlation analyses between MediHall Score and human expert judgment to demonstrate construct validity versus existing metrics.\n   5. **Model Transparency:** Clarify how MediHallDetector decisions are influenced by visual versus textual features—e.g., through attention visualization or ablation on image context.\n\n5. **Minor Comments**\n   - Consolidate writing to reduce redundancy in describing benchmark design.  \n   - Correct minor grammar and typographical issues (e.g., spacing, inconsistent capitalization).  \n   - Cite related medical imaging QA/robustness studies (e.g., segmentation uncertainty, label noise modeling).  \n   - Suggest inclusion of supplementary qualitative examples showing borderline hallucination cases.  \n   - Ensure dataset and code release links are functional upon submission.\n\n6. **Overall Recommendation: ⚠️ Major Revision**\n   The work is technically sound and contributes a needed evaluation resource for safe medical multimodal AI, but it sits near the boundary of *IEEE TMI’s methodological threshold*. Strengthening analysis of annotation reliability, metric validation, and model interpretability would elevate the paper from an application-oriented study to a methodologically generalizable contribution suitable for TMI.\n\n7. **Confidence Level: 4/5**  \n   The reviewer has high confidence in assessing novelty, rigor, and scope alignment given familiarity with medical AI benchmarking and multimodal image–language models.\n\n---\n\n**Summary Judgment:**  \n**Potentially impactful but currently below TMI’s full methodological bar.** With deeper validation, analytical rigor, and broader imaging relevance, the paper could be a meaningful contribution to the journal’s emerging subdomain of *trustworthy medical imaging AI.*",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Med-HallMark*, a benchmark for detecting and evaluating hallucinations in Large Vision–Language Models (LVLMs) applied to medical imaging tasks such as visual question answering and report generation. It further introduces the *MediHall Score*, a hierarchical metric for quantifying hallucination severity, and *MediHallDetector*, a fine-tuned LVLM for hallucination classification. The paper is well organized, generally clear, and relevant to the *IEEE TMI* readership through its focus on methodological evaluation and AI safety, though its contributions are primarily dataset- and metric-oriented rather than algorithmically theoretical.\n\n---\n\n**Major Comments**  \n1. **Scope Alignment:** The manuscript would benefit from clearer justification of how the benchmark and model generalize across imaging modalities (e.g., MRI, CT, ultrasound) to align with TMI’s methodological scope rather than remaining confined to evaluation research.  \n2. **Annotation Reliability:** Quantitative measures of inter-rater agreement (e.g., Cohen’s κ, ICC) should be provided to validate annotation consistency and strengthen confidence in the hierarchical labeling process.  \n3. **Statistical Rigor:** Performance comparisons lack uncertainty quantification. Including confidence intervals or hypothesis testing for observed score differences would enhance interpretability.  \n4. **Metric Validation:** Correlation analyses between MediHall Score and expert clinical ratings would demonstrate the metric’s construct validity relative to existing evaluation frameworks.  \n5. **Model Transparency:** Explain how visual and textual components contribute to MediHallDetector decisions. Visualization or ablation analyses could clarify interpretability and the true multimodal nature of the detector.\n\n---\n\n**Minor Comments**  \n- Reduce redundancy in the benchmark description and improve conciseness throughout.  \n- Correct minor grammatical inconsistencies and formatting issues.  \n- Expand citations to include related work on robustness, label noise modeling, and medical imaging quality assessment.  \n- Provide supplementary examples showing borderline hallucination cases.  \n- Ensure dataset and code release links are active and accessible upon submission.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* Addresses a growing concern in trustworthy medical imaging AI by creating a benchmark for hallucination detection, with clear clinical motivation around safety and interpretability.  \n*Innovation:* Novelty lies mainly in benchmark and metric development rather than algorithmic innovation, positioning the work as applied computational methodology.  \n*Evaluation:* Experiments are broad and systematic but would benefit from statistical validation, inter-rater reliability analysis, and multi-domain generalization.  \n*Reproducibility:* Dataset sources, ethical use, and code-release plans are clearly stated but should be verified upon publication to ensure transparency and usability.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe study provides an important foundation for hallucination evaluation in medical LVLMs but currently falls below *TMI*’s methodological expectations. Strengthening reliability analyses, quantitative validation, and interpretability will be necessary to elevate it to journal standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents **Med-HallMark**, a benchmark focused on detecting and evaluating hallucinations in Large Vision-Language Models (LVLMs) applied to medical imaging tasks. It aims to provide a systematic methodology for assessing hallucinations in clinical settings, contributing to the domains of medical imaging informatics and AI safety. The overall clarity of the manuscript is generally good, with a logical structure and clear figures, although some areas could benefit from more concise writing.\n\n**Major Comments:**\n1. **Scope Alignment:** To align with IEEE TMI’s methodological expectations, the authors should elaborate on the benchmark’s generalizability to diverse imaging modalities, including MRI, CT, and ultrasound. This would enhance the relevance of the contribution to imaging science beyond just evaluation.\n2. **Annotation Reliability:** Quantitative measures of inter-rater agreement (such as κ or ICC) for clinician labeling are necessary to affirm the reliability of the hierarchical categories.\n3. **Statistical Analysis:** An expansion of the evaluation to include confidence intervals or hypothesis testing for score differences among models is warranted.\n4. **MediHall Score Validation:** Correlation analyses between the MediHall Score and human expert judgment should be included to demonstrate the construct validity of the metric against existing measures.\n5. **Model Transparency:** Clarification is needed regarding how the MediHallDetector’s decisions are influenced by visual and textual features, potentially illustrated through attention visualization or ablation studies on image context.\n\n**Minor Comments:**\n- The writing could be streamlined to avoid redundancy in describing the benchmark design.\n- Minor grammatical and typographical errors should be corrected, including inconsistencies in capitalization and spacing.\n- References to related studies on medical imaging quality assurance and robustness should be cited, particularly concerning segmentation uncertainty and label noise modeling.\n- It is suggested to include supplementary qualitative examples of borderline hallucination cases.\n- Ensure that links to datasets and code releases are functional upon submission.\n\n**Summary Paragraph:**  \nThis manuscript offers a significant contribution to the field by introducing Med-HallMark, a much-needed benchmark for evaluating hallucination detection in medical LVLMs. While it demonstrates some innovative elements in the area of medical multimodal model evaluation, the overall methodological novelty is modest, primarily centered on benchmark and metric establishment rather than theoretical or algorithmic advancements. The strengths lie in its strong baseline performance and clinical relevance, while the weaknesses pertain to the limited generalizability analysis, lack of detailed statistical validation, and insufficient documentation of annotation reliability.\n\n**Decision Recommendation:**  \n⚠️ Major Revision  \nThe manuscript contains solid technical foundations and addresses an important area within medical AI, yet it needs enhancements in terms of analytical rigor, metric validation, and methodological applicability to reach publication standards appropriate for IEEE TMI. \n\n**Confidence Level: 4/5**  \nThe reviewer expresses a high level of confidence in evaluating the manuscript due to prior experience with medical AI benchmarking and multimodal image-language model assessments.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Dingkang Yang",
      "Dongling Xiao",
      "Jiawei Chen",
      "Ke Li",
      "Lihua Zhang",
      "Mingcheng Li",
      "Shunli Wang",
      "Tong Wu",
      "Xiaolu Hou",
      "Yue Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_e3a42a649db419e8ec7cf4e6173f60905540569e.pdf",
    "remote_url": "https://openreview.net/pdf/e3a42a649db419e8ec7cf4e6173f60905540569e.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning to Teach: Improving Mean Teacher in Semi-supervised Medical Image Segmentation with Dynamic Decay  Modulation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "transfer learning, meta learning, and lifelong learning"
    ],
    "keywords": [
      "Meta learning",
      "Medical image segmentation",
      "semi-supervised learning"
    ],
    "abstract": "Medical image segmentation is essential in medical diagnostics but is hindered by the scarcity of labeled three-dimensional imaging data, which requires costly expert annotations. Semi-supervised learning (SSL) addresses this limitation by utilizing large amounts of unlabeled data alongside limited labeled samples. The Mean Teacher model, a prominent SSL method, enhances performance by employing an Exponential Moving Average (EMA) of the student model to form a teacher model, where the EMA decay coefficient is critical. However, using a fixed coefficient fails to adapt to the evolving training dynamics, potentially restricting the model's effectiveness. In this paper,\nwe propose Meta MeanTeacher, a novel framework that integrates meta-learning to dynamically adjust the EMA decay coefficient during training. We approach proposed Dynamic Decay Modulation (DDM) module in our Meta MeanTeacher framework, which captures the representational capacities of both student and teacher models. DDM heuristically learns the optimal EMA decay coefficient by taking the losses of the student and teacher networks as inputs and updating it through pseudo-gradient descent on a meta-objective. This dynamic adjustment allows the teacher model to more effectively guide the student as training progresses.\nExperiments on two datasets with different modalities, i.e., CT and MRI, show that Meta MeanTeacher consistently outperforms traditional Mean Teacher methods with fixed EMA coefficients. Furthermore, integrating Meta MeanTeacher into state-of-the-art frameworks like UA-MT, AD-MT, and PMT leads to significant performance enhancements, achieving new state-of-the-art results in semi-supervised medical image segmentation.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces the Meta Mean Teacher framework, a novel approach to improve semi-supervised medical image segmentation. Traditional Mean Teacher models use a fixed Exponential Moving Average (EMA) decay coefficient to update the teacher model, but this fixed value often limits model effectiveness. Meta Mean Teacher addresses this limitation by introducing a Dynamic Decay Modulation (DDM) module that adaptively adjusts the EMA decay coefficient based on training dynamics. This dynamic adjustment optimizes the student-teacher learning process, enabling better performance in tasks with limited labeled data.\n\nKey contributions of this work include:\n\n1. Adaptive EMA Decay: The DDM module optimizes the EMA decay coefficient, enhancing the model's adaptability and enabling it to capture richer training representations.\n2. Plug-and-Play Architecture: Meta Mean Teacher is designed to integrate seamlessly into existing models, improving performance across various Mean Teacher-based methods.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. Dynamic Adaptability: By incorporating the Dynamic Decay Modulation (DDM) module, the framework dynamically adjusts the EMA decay coefficient (α) during training. This adaptability ensures that the teacher model evolves effectively with the student model, allowing more precise guidance as training progresses. This approach addresses a common limitation in fixed-coefficient Mean Teacher models, which often fail to account for varying training dynamics.\n\n2. Plug-and-Play Module: The Meta Mean Teacher framework is designed as a modular system, making it highly compatible with existing models based on the Mean Teacher architecture. This modularity allows easy integration into various semi-supervised frameworks like UA-MT, AD-MT, and PMT.\n\n3. Enhanced Stability and Robustness: The framework benefits from the Mean Teacher method’s inherent stability due to EMA but improves upon it by learning an optimal decay coefficient through meta-learning techniques.\n\n### Weaknesses\n\n1. High Computational Overhead: The adaptive EMA adjustment via DDM introduces complexity and requires more computational resources. The dynamic adjustment process, which includes cloning and iterative updates of both teacher and student models, may not be feasible for real-time or resource-limited applications, especially when processing large 3D medical imaging data.\n\n2. Limited Exploration of Other Adaptive Techniques: While the paper focuses on dynamically adjusting the EMA decay coefficient, other hyperparameters (like learning rates or loss weight factors) could also impact model performance in semi-supervised learning. The focus on only one parameter might restrict the overall optimization potential, as additional adjustments could further enhance the segmentation quality.\n\n### Questions\n\n1. On the impact of α=0.01: Why does the model show improvement when α is set to 0.01? This result seems to contradict the explanation provided in Section 3.1.\n\n2. The use of fixed α in the ablation experiments in Section 4.3: In Section 4.3, why was the average fixed α method chosen for comparison? From Figure 1, we can see that lower α values ​​(such as 0.03, 0.05, and 0.1) significantly degrade the performance. In contrast, α of 0.97 achieves performance higher than 0.84. Doesn't this general average comparison seem a bit biased?\n\n3. The impact of α greater than 0.5: From Table 1, we can see that when α is greater than 0.5, its impact on performance becomes less significant. Test whether randomly selected α values ​​greater than 0.5 are beneficial, thereby potentially improving the results?\n\n4. Suspicion about the data in Section 4.4 compared with other methods: Why is your experimental setting different from that in \"Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation\", but most of the state-of-the-art data (including LA and Pancreas-NIH datasets) are the same as the data in that paper? Does this indicate that the data is directly borrowed?\n\n5. The m symbol in Figure 2: What does \"m\" mean in Figure 2? Is this symbol redundant?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents the Meta Mean Teacher framework, a novel method aimed at enhancing semi-supervised medical image segmentation. Unlike traditional Mean Teacher models, which rely on a fixed Exponential Moving Average (EMA) decay coefficient, this approach introduces a Dynamic Decay Modulation (DDM) module that adaptively adjusts the EMA decay coefficient according to training dynamics. This innovation aims to optimize the student-teacher learning process, particularly in scenarios with limited labeled data.\n\n### Major Comments\n1. **Adaptive EMA Decay**: The DDM module successfully enhances the adaptability of the EMA decay coefficient, allowing the model to capture richer training representations.\n2. **Plug-and-Play Architecture**: The Meta Mean Teacher framework can be integrated seamlessly with existing models, thus improving performance across various Mean Teacher-based approaches.\n3. **High Computational Overhead**: The adaptive mechanism introduces complexity and demands substantial computational resources, potentially limiting feasibility for real-time or resource-constrained scenarios, especially with large 3D medical images.\n4. **Limited Exploration of Other Adaptive Techniques**: The focus on the dynamic adjustment of the EMA decay coefficient neglects other hyperparameters that could significantly affect model performance. This singular focus may restrict overall optimization and segmentation quality.\n\n### Minor Comments\n- Clarify the impact of setting α = 0.01, as this contradicts the explanation in Section 3.1.\n- Explain the choice of using average fixed α in the ablation studies of Section 4.3, given the significant performance variation observed with different α values.\n- Investigate whether selecting random α values greater than 0.5 could yield better results, as indicated by Table 1.\n- Address the variation in experimental settings compared to previous studies, such as the paper \"Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation.”\n- Clarify the meaning of \"m\" in Figure 2 and whether it is necessary.\n\n### Summary Paragraph\nIn terms of significance, the Meta Mean Teacher framework offers a promising direction in enhancing semi-supervised medical image segmentation by mitigating limitations found in fixed-decay Mean Teacher models. Its innovative use of a DDM module presents a notable advancement, although the dependency on adaptive techniques introduces considerable computational demands that may hinder practical applications. While the evaluation demonstrates engagement with model adaptability and performance, there is room for further exploration of additional hyperparameters. Overall, the work shows potential but requires further validation to address its computational demands and optimization scope.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the *Meta Mean Teacher* framework, designed to enhance semi-supervised medical image segmentation by overcoming limitations of conventional Mean Teacher models that rely on a fixed Exponential Moving Average (EMA) decay coefficient. The proposed model introduces a *Dynamic Decay Modulation* (DDM) module that adaptively adjusts the EMA decay parameter based on training dynamics, aiming to improve student-teacher co-evolution and segmentation performance in low-label scenarios. The work is clearly written, introducing a modular approach that can be integrated into various existing Mean Teacher-based methods.\n\n---\n\n**Major Comments**  \n1. **Computational Overhead:** The adaptive DDM process significantly increases computational complexity. The requirement to clone and iteratively update both teacher and student models may hinder scalability and practicality, particularly for large 3D datasets or resource-constrained environments.  \n2. **Limited Scope of Adaptivity:** The study only explores adaptation of the EMA decay coefficient. Extending the approach to other hyperparameters (e.g., learning rate, loss weighting) could yield further performance improvements and broader generalization.  \n3. **Experimental Clarifications:**  \n   - Please clarify why the model achieves improvement specifically at α = 0.01, which appears inconsistent with the discussion in Section 3.1.  \n   - In Section 4.3, justify the use of a fixed α average for comparison, as performance differences across α values (e.g., lower values degrading results, α = 0.97 performing better) suggest that the averaging strategy may bias outcomes.  \n   - Table 1 indicates that performance stabilizes when α > 0.5. Would random selection of α in this range still maintain or enhance results?  \n4. **Data and Experimental Consistency:** There is uncertainty regarding the dataset usage in Section 4.4, as it closely mirrors that used in the paper *Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation*. Please clarify whether data or configurations were directly reused.  \n\n---\n\n**Minor Comments**  \n- The meaning of the symbol “m” in Figure 2 should be clarified; if redundant, consider removal.  \n- Presentation and figures are generally clear but could benefit from additional annotations aiding reader interpretation.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe *Meta Mean Teacher* framework is an innovative extension of the Mean Teacher paradigm, introducing dynamic adaptation of the EMA coefficient through meta-learning. It offers conceptual novelty and potential performance improvements in semi-supervised medical image segmentation. However, its elevated computational cost and narrow focus on a single adaptive parameter limit both practical applicability and generalizability. The evaluation is thorough but raises concerns about experiment consistency and clarity of comparative baselines. Reproducibility would benefit from further disclosure on data usage and training configurations. Overall, the contribution is promising but requires clarification and further validation.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper explores the EMA decay coefficient within the MT semi-supervised framework, fully tapping into the potential of the MT framework. Additionally, it introduces a novel meta-learning strategy to dynamically find the optimal EMA decay coefficient during the training process. Experiments conducted on two medical image datasets demonstrate that this method achieves superior performance.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper introduces a novel meta-learning strategy to adjust the EMA decay coefficient, fully tapping into the potential of the MT semi-supervised framework.\n2. This paper introduces a strategy to adjust the EMA decay coefficient to improve semi-supervised segmentation performance, which could be a meaningful contribution to this field.\n3. The extensive experimental results show the effectiveness of the proposed method.\n\n### Weaknesses\n\n1. I have not observed many innovative aspects in the application of meta-learning to the field of semi-supervised medical image segmentation. Part of the reason for this is the clarity of the writing; it is currently unclear what significant differences exist between the proposed DDM and previous meta-learning strategies. If there are no substantial differences, then the methodological contribution of this approach appears to be quite limited.\n2. Could the authors explain what potential drawbacks a fixed EMA decay coefficient might have on the MT framework, particularly in the context of medical image processing?\n3. The motivation is unclear. I do not understand why a dynamic change in $\\alpha$ would have a greater advantage compared to a fixed value. $\\alpha$ can be understood as the weight distribution between the teacher model’s parameters and the student model’s parameters during the iterative update process, with the teacher model’s weight being overwhelmingly dominant. I question the assumption that dynamically varying $\\alpha$ between 0.95 and 0.99 is necessarily better than a fixed value of 0.97. Could you provide a plot showing how $\\alpha$ changes dynamically over training iterations in the experiments?\n4. In Equation 2, what criteria does DDM use to derive αm? Is there a relationship between $\\alpha_m$ and these two losses? For example, if the teacher model has a lower loss, should $\\alpha_m$ be larger? Please explain.\n5. The notation in the paper is somewhat confusing. In Equation 1, what are the differences between $\\Theta_s^*$ and $\\Theta_s$, and between $\\Theta_s$ and $\\theta_s$? Additionally, what is meant by meta data $\\mathcal{D}_m$, and how does it differ from labeled data and unlabeled data? Furthermore, the $\\mathcal{L}_m$ formula is missing; I suggest adding it.\n6. What is the initial value of $\\alpha$? Has an ablation study been conducted to verify the impact of $\\alpha$?\n7. The authors mention that DDM can be promoted as a plug-and-play component for different models. However, I think DDM has limitations. For instance, how would DDM be applied to commonly used pseudo-labeling methods based on CPS (Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision)?\n\n### Questions\n\nPlease refer to the weakness.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript investigates the Exponential Moving Average (EMA) decay coefficient within the Multi-Teacher (MT) semi-supervised framework, leveraging the capabilities of the MT architecture. It presents a novel meta-learning strategy that dynamically determines the optimal EMA decay coefficient throughout training. The experiments conducted on two medical image datasets indicate that the proposed method achieves superior performance.\n\n**Major Comments:**\n1. Limited innovation: The application of meta-learning in semi-supervised medical image segmentation lacks significant novelty. The writing clarity hampers understanding of the differences between the proposed Dynamic Decay Mechanism (DDM) and previous meta-learning approaches, suggesting limited methodological contributions.\n2. Fixed EMA decay coefficient drawbacks: Please elucidate potential drawbacks of employing a fixed EMA decay coefficient within the MT framework, especially in medical image processing contexts.\n3. Motivation clarity: The rationale for a dynamic EMA decay coefficient (α) versus a fixed value remains unclear. Given the dominance of the teacher model's weights during parameter updates, please clarify why varying α between 0.95 and 0.99 is preferable over a constant value of 0.97. A plot showing the changes in α throughout training iterations would be beneficial.\n4. Derivation criteria for α_m: In Equation 2, please specify the criteria used by DDM to derive α_m, and clarify the relationship between α_m and the two losses. Should α_m increase if the teacher model exhibits lower loss?\n5. Notation confusion: Clarify distinctions between notations such as Θ_s*, Θ_s, and θ_s in Equation 1. Define meta data (𝓓_m) and its differentiation from labeled and unlabeled data. Also, consider including the missing formula for 𝓛_m.\n6. Initial α value and ablation study: What is the initial value for α, and has an ablation study been performed to assess the impact of varying α?\n7. Limitations of DDM: The authors state that DDM can serve as a plug-and-play component for various models. However, please explain how DDM could be integrated with commonly used pseudo-labeling methodologies based on Cross Pseudo Supervision (CPS).\n\n**Minor Comments:**  \n- Clarify unclear terms and concepts throughout.\n- Ensure consistency in notation and provide missing equations.\n\n**Summary Paragraph:**  \nThe manuscript exhibits significant strengths in its experimental validation and novel contribution towards optimizing the EMA decay coefficient using a meta-learning strategy. However, concerns regarding the innovative aspects of the approach, clarity of motivation, and notation consistency are notable weaknesses. In terms of significance, innovation, evaluation, and reproducibility, the work shows promise but requires revisions to enhance clarity and justification of its contributions to the field.\n\n**Decision Recommendation:**  \nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the role of the exponential moving average (EMA) decay coefficient in the Mean Teacher (MT) semi-supervised learning framework. It proposes a new meta-learning approach to dynamically optimize the EMA decay coefficient during training, aiming to enhance segmentation performance. Experiments on two medical image datasets demonstrate improved results. Overall, the paper is clearly structured and scientifically sound, though some aspects of novelty and clarity in the methodological description could be strengthened.  \n\n**Major Comments**  \n1. The claimed innovation—using meta-learning to dynamically adjust the EMA decay coefficient—appears incremental. The manuscript should clarify how the proposed Dynamic Decay Module (DDM) differs from existing meta-learning strategies used in semi-supervised medical image segmentation. Without this distinction, the methodological contribution seems limited.  \n2. The motivation for dynamically updating the EMA decay coefficient is not fully convincing. The authors should explain why varying α during training is expected to outperform a fixed coefficient (e.g., 0.97) and provide empirical visualization such as a curve showing α’s evolution across iterations.  \n3. The paper should clarify what potential drawbacks a fixed EMA decay coefficient introduces within the MT framework, particularly in medical image analysis contexts.  \n4. Equation (2) lacks sufficient explanation: how is αₘ derived, and what relationship exists between αₘ and the loss terms? If the teacher model yields a lower loss, should αₘ be larger? This reasoning should be provided.  \n5. Notation is confusing in places. The differences between Θₛ*, Θₛ, and θₛ are unclear. The definition and role of metadata 𝒟ₘ, and how it differs from labeled and unlabeled data, require clarification. The formula for 𝓛ₘ should also be included.  \n6. The initial value of α should be specified, and an ablation study assessing its impact is needed.  \n7. While the authors claim DDM is plug-and-play, its applicability beyond the MT framework—e.g., to methods like Cross Pseudo Supervision (CPS)—remains uncertain and should be discussed.  \n\n**Minor Comments**  \n- Improve notation consistency and define all variables upon first use.  \n- Ensure equations include all referenced terms.  \n- Strengthen clarity in the description of DDM’s workflow.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses an important challenge in semi-supervised medical image segmentation and provides a well-motivated empirical evaluation. Its significance lies in improving the adaptability of the MT framework through dynamic parameter adjustment. However, the degree of methodological innovation over prior meta-learning approaches remains unclear. The experiments appear solid, but the lack of ablation details and missing formula definitions weaken interpretability and reproducibility. More comprehensive analyses and clearer exposition would enhance the paper’s credibility.  \n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The study shows potential but requires additional justification of novelty, clearer mathematical explanations, and more complete empirical validation to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents the 'Meta Mean Teacher', an approach for semi-supervised medical image segmentation. Building on the Mean Teacher model, which leverages exponential moving average (EMA) to create a stable teacher model from a student model, this framework introduces the Dynamic Decay Modulation (DDM) module. DDM dynamically adjusts the EMA decay coefficient based on both the student and teacher losses, improving the model's adaptability during training.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nThe paper addresses semi-supervised learning in medical image segmentation with a novel meta-learning approach, introducing the Dynamic Decay Modulation (DDM) module to adjust the EMA decay coefficient dynamically. \n\nThe paper strengthens its empirical evaluation by testing on three datasets, covering different imaging modalities.\n\n### Weaknesses\n\nWhile the paper builds on the Mean Teacher model, which is well-established in semi-supervised learning, it may lack substantial novelty as the framework mainly modifies an existing approach. Although the Dynamic Decay Modulation (DDM) module adds a new layer of adaptability, many similar extensions to Mean Teacher already exist, potentially limiting the paper's contribution to novel methodology.\n\n\nThe experimental scope appears limited as it only includes limited number of baseline methods, i.e. Mean Teacher variations like UAMT with UNet and VNet, models that have already been well-explored in this context. The paper’s experiments may be restricted by a limited range of labeled-to-unlabeled data ratios, which does not fully capture the model’s performance across different semi-supervised settings. Testing with a wider variety of label-scarcity scenarios would offer more robust insights into the framework's adaptability and practical applicability in real-world cases where data availability varies.\n\n### Questions\n\n(1) How do you ensure that comparisons are fair in semi-supervised learning scenarios? For example, I understand that in some cases, we can control the percentage of labeled and unlabeled data, such as using 5% or 10% labeled data. However, the feature distribution of labeled and unlabeled data cannot be guaranteed to be the same.\n\n\n(2) The exclusive use of VNet as the backbone may limit the generalizability of the results, as it does not reflect performance across more commonly used architectures like UNet or newer ViT-based UNets.\n\n(3) In Table 2, I observe that VNet’s performance is significantly lower than others when only 5% of data is labeled, but it is only slightly lower when 10% is labeled. Could you explain why this discrepancy occurs? Additionally, could you provide more results for cases with 20%, 50%, 80%, and 90% labeled data, if available?\n\n(4) In table 3, why VNet outperforms UA-MT when 20% are labeled.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript introduces the 'Meta Mean Teacher', an innovative approach designed for semi-supervised medical image segmentation. It builds upon the established Mean Teacher model, incorporating an adaptive mechanism through the Dynamic Decay Modulation (DDM) module, which dynamically adjusts the exponential moving average (EMA) decay coefficient based on both student and teacher losses. The clarity of the presentation is adequate, but improvements could enhance understanding.\n\n**Major Comments:**\n1. **Novelty:** The framework modifies an existing Mean Teacher approach, which may limit its novelty. Although DDM enhances adaptability, similar extensions to the Mean Teacher have been previously proposed, challenging the paper’s contribution to innovative methodology.\n2. **Experimental Scope:** The experimentation relies on a narrow selection of baseline methods, including variations of Mean Teacher such as UAMT with UNet and VNet, which may have been extensively explored. Expanding the experimental design to include additional baseline models would enhance the robustness of the results.\n3. **Generalization:** The exclusive use of VNet as the backbone architecture may hinder generalizability. Including more commonly adopted architectures such as UNet or ViT-based UNets would provide a broader performance perspective.\n4. **Label Ratios:** The limited range of labeled-to-unlabeled data ratios in the experiments might not showcase the model's performance comprehensively across various semi-supervised settings. Evaluating a wider spectrum of label scarcity would yield greater insights into adaptability and real-world applicability.\n\n**Minor Comments:**\n- Clarification needed on how to ensure fair comparisons in semi-supervised learning, particularly regarding feature distribution discrepancies between labeled and unlabeled data.\n- The performance discrepancy in VNet across varying labeled data percentages requires further explanation (e.g., the significant drop at 5% labeled data but a small drop at 10%).\n- Additional results for greater labeled data percentages (20%, 50%, 80%, and 90%) would be beneficial.\n  \n**Summary Paragraph:**\nIn relation to TMI criteria, the manuscript presents significant contributions in addressing semi-supervised learning in medical image segmentation; however, it struggles with innovation due to its reliance on an existing model. The evaluation could be strengthened by a broader experimental design, allowing for more robust comparisons, and enhancing reproducibility across various scenarios. Overall, the work offers a sound basis but raises concerns regarding its novelty and evaluation scope.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Meta Mean Teacher* framework for semi-supervised medical image segmentation. Building on the established Mean Teacher paradigm, the paper proposes a *Dynamic Decay Modulation (DDM)* module that dynamically adjusts the exponential moving average (EMA) decay coefficient based on both student and teacher losses, aiming to enhance adaptability during training. The authors evaluate their method across three datasets encompassing different imaging modalities. Overall, the paper is clearly written and addresses a relevant problem in semi-supervised segmentation, though its methodological novelty and evaluation breadth appear limited.\n\n---\n\n**Major Comments**  \n1. **Novelty and Contribution:** The approach largely extends the traditional Mean Teacher framework rather than introducing a fundamentally new semi-supervised learning paradigm. Although DDM offers an adaptive mechanism, similar modifications to EMA-based methods are already present in prior work, reducing the level of methodological innovation.  \n2. **Experimental Scope:** The evaluation includes only a narrow set of baseline comparisons—mainly Mean Teacher variants such as UAMT with UNet and VNet. A broader selection of methods would better position the proposed approach within the current landscape.  \n3. **Data Regime Coverage:** The experiments explore only a limited range of labeled-to-unlabeled ratios, restricting insights into how the model performs under varying levels of supervision. Additional tests (e.g., 20%, 50%, 80%, 90% labeled data) would strengthen claims of robustness and adaptability.  \n4. **Backbone Generalization:** The exclusive use of VNet as the backbone limits assessment of architectural generalizability. Benchmarking with more common or recent architectures, such as UNet or ViT-based models, would improve confidence in the results.  \n5. **Fairness of Comparisons:** Clarification is needed on how fairness is ensured in semi-supervised setups, particularly regarding differences in labeled/unlabeled data distributions.\n\n---\n\n**Minor Comments**  \n- Clarify performance discrepancies observed in Tables 2 and 3, such as why VNet performs worse at 5% labeled data but improves at higher proportions, and why it surpasses UA-MT at 20%.  \n- Ensure tables include all relevant experimental details for reproducibility.  \n- English presentation and figure quality are adequate; only minor polishing may be required.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important topic—improving semi-supervised segmentation using adaptive teacher-student consistency. Its significance lies in potential enhancements to model stability under limited annotations. However, innovation is modest since it mainly adapts an existing framework. The evaluation demonstrates cross-modality testing but remains narrow in baseline diversity and data regime coverage. Reproducibility is acceptable given methodological clarity, though more architectural and experimental variation would strengthen generalizability.\n\n---\n\n**Decision Recommendation:** **Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes Meta Mean Teacher, a framework that dynamically adjusts the Exponential Moving Average (EMA) decay coefficient in Mean Teacher-based semi-supervised medical image segmentation. The authors introduce a Dynamic Decay Modulation (DDM) module that uses meta-learning to optimize the EMA coefficient α during training, rather than using a fixed value. The DDM takes losses from both student and teacher models as inputs and updates through pseudo-gradient descent on meta-data. The method employs a student-guided approach where DDM parameters are updated based on student model performance. Experiments on LA Heart (MRI) and Pancreas-NIH (CT) datasets show improvements when DDM is integrated into existing Mean Teacher variants (UA-MT, AD-MT, PMT), with the best results achieving new state-of-the-art performance on both datasets across multiple metrics including Dice, Jaccard, 95HD, and ASD.\n\n## Weaknesses\n\n• **Theoretical justification lacks rigor**: The mathematical analysis supporting the student-guided approach contains significant gaps and questionable assumptions.\n  - Equation (3) claims P(θ* ∈ span(θt, θs)) ≈ 0 without formal proof or dimensionality analysis (Page 6, lines 270-273)\n  - The Universal Approximation Theorem application in Equation (5) ignores practical constraints of finite networks and training time (Page 6, lines 286-289)\n  - The function space analysis assumes the existence of optimal parameters θ* without establishing convergence properties (Page 5, lines 251-253)\n\n• **Experimental design limitations compromise validity**: The evaluation protocol has several methodological concerns that affect the reliability of the results.\n  - Limited dataset diversity with only two medical imaging datasets, both relatively small (100 and 82 volumes respectively) as stated in Section 4.2 (Page 7, lines 329-337)\n  - Inconsistent baseline comparisons where some methods show missing results (MCF entries marked as \"-\" in Tables 2-3)\n  - No statistical significance testing or confidence intervals reported across multiple runs (Tables 2-3, Pages 7-9)\n  - Training limited to only 6000 iterations which may not ensure convergence for the meta-learning components (Page 6, lines 318-321)\n\n• **Technical implementation details are insufficient**: Critical aspects of the DDM module design and meta-learning process lack adequate specification.\n  - DDM network architecture described only as \"typically a MLP\" without layer specifications, activation functions, or parameter counts (Page 4, lines 212-214)\n  - Meta-data splitting strategy not clearly defined - unclear how meta-data Dm differs from training data Ds (Algorithm 1, line 1)\n  - Hyperparameter selection process for meta-learning rate ηD and meta-iteration counts not discussed (Algorithm 1, lines 10-16)\n  - No computational overhead analysis comparing training time and memory requirements against baseline methods\n\n• **Algorithmic presentation contains inconsistencies**: The meta-learning algorithm and loss formulations have notation and logical gaps.\n  - Algorithm 1 uses θs_clone and θt_clone but the relationship to original parameters is unclear in the meta-phase (lines 8-17)\n  - Equation (2) defines αm using current timestep losses but Algorithm 1 shows different loss computation sequences (Page 5, line 237 vs. Algorithm 1 lines 10-14)\n  - Bilevel optimization formulation in Equation (1) doesn't clearly specify how Θt depends on ΘD through the EMA update (Page 5, lines 229-231)\n\n## Suggestions for Improvement\n\n• **Strengthen theoretical foundations**: Provide rigorous mathematical analysis with formal proofs and clear assumptions.\n  - Derive the probability bound in Equation (3) using explicit dimensionality arguments or provide empirical validation through synthetic experiments\n  - Replace the Universal Approximation Theorem argument with finite-sample complexity analysis showing when student-guided updates converge faster than teacher-guided ones\n  - Establish convergence properties of the meta-learning objective and prove existence of optimal DDM parameters under realistic constraints\n\n• **Enhance experimental rigor**: Expand the evaluation to include more comprehensive datasets and statistical validation.\n  - Include additional medical imaging datasets from different anatomical regions and imaging modalities to demonstrate broader generalizability\n  - Provide complete baseline results for all compared methods or justify exclusions with clear explanations\n  - Report confidence intervals and statistical significance tests across multiple independent runs (minimum 5 runs recommended)\n  - Extend training duration or provide convergence analysis to ensure meta-learning components reach stable performance\n\n• **Complete technical specifications**: Provide comprehensive implementation details to enable reproducibility and fair comparison.\n  - Specify complete DDM architecture including layer dimensions, activation functions, initialization schemes, and total parameter counts\n  - Define meta-data construction process with clear separation criteria from training and validation sets\n  - Document all hyperparameter selection procedures including grid search ranges and validation strategies\n  - Include computational complexity analysis with training time comparisons and memory usage measurements against baseline methods\n\n• **Clarify algorithmic formulation**: Resolve notation inconsistencies and provide clearer algorithmic descriptions.\n  - Revise Algorithm 1 to clearly distinguish between cloned model updates and original model states throughout the meta-learning phase\n  - Align DDM input formulation in Equation (2) with the loss computation sequence shown in Algorithm 1\n  - Reformulate the bilevel optimization problem to explicitly show how teacher parameters depend on DDM outputs through the EMA mechanism",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents Meta Mean Teacher, a novel framework that adaptively modifies the Exponential Moving Average (EMA) decay coefficient within Mean Teacher-based semi-supervised medical image segmentation. The authors introduce a Dynamic Decay Modulation (DDM) module, which utilizes meta-learning to optimize the EMA coefficient α during training instead of relying on a fixed value. Experimental results on the LA Heart (MRI) and Pancreas-NIH (CT) datasets demonstrate that integrating DDM enhances existing Mean Teacher variants, achieving state-of-the-art performance across several metrics.\n\n### Major Comments\n1. **Theoretical Justification**: The manuscript lacks rigor in its theoretical analysis:\n   - The claim in Equation (3) regarding P(θ* ∈ span(θt, θs) ≈ 0) is presented without a formal proof (Page 6, lines 270-273).\n   - The application of the Universal Approximation Theorem in Equation (5) overlooks constraints related to network finiteness and training duration (Page 6, lines 286-289).\n   - The function space analysis assumes the existence of optimal parameters θ* while not establishing convergence properties (Page 5, lines 251-253).\n\n2. **Experimental Design Limitations**: Several methodological flaws affect result validity:\n   - The datasets are limited in diversity and size (100 and 82 volumes) (Page 7, lines 329-337).\n   - Baseline comparisons are inconsistent, with some methods lacking results (MCF entries marked as \"-\" in Tables 2-3).\n   - There is an absence of statistical significance testing or confidence intervals (Tables 2-3, Pages 7-9).\n   - Limited training iterations (6000) may not ensure convergence for the meta-learning aspects (Page 6, lines 318-321).\n\n3. **Implementation Details**: Specifications for DDM and meta-learning are insufficient:\n   - The DDM architecture is vaguely described, lacking details like layer specifications or activation functions (Page 4, lines 212-214).\n   - The meta-data splitting strategy is not clearly outlined (Algorithm 1, line 1).\n   - There is no discussion of hyperparameter selection for meta-learning (Algorithm 1, lines 10-16).\n   - A computational overhead analysis is absent.\n\n4. **Algorithm Presentation**: Notation inconsistencies and logical gaps are evident:\n   - The relationship of θs_clone and θt_clone to original parameters is unclear in Algorithm 1 (lines 8-17).\n   - Equation (2) and Algorithm 1 show discrepancies in loss computation (Page 5, line 237 vs. Algorithm 1, lines 10-14).\n   - The formulation in Equation (1) fails to elucidate how Θt depends on ΘD (Page 5, lines 229-231).\n\n### Minor Comments\n- Improve clarity regarding the DDM architecture and ensure all technical elements are defined.\n- Adjust notations in the algorithm for consistency and clarity.\n- Correct typographical errors and inconsistencies in formatting throughout the text.\n\n### Summary Paragraph\nIn evaluating the manuscript against the TMI editorial criteria, significant strengths are recognized in the innovation of the Meta Mean Teacher framework. However, the theoretical foundations are weak, lacking rigorous analysis, which impacts its perceived significance. Additionally, the experimental design raises concerns regarding reproducibility due to limited datasets and lack of statistical validations. Although the framework demonstrates promising performance, foundational improvements are essential for ensuring robust evaluation and reproducibility.\n\n### Decision Recommendation\n**Major Revision** - The authors are encouraged to address the highlighted weaknesses and enhance the manuscript's overall rigor and clarity.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Meta Mean Teacher*, a semi-supervised medical image segmentation framework that dynamically adjusts the Exponential Moving Average (EMA) decay coefficient in Mean Teacher models via a meta-learning strategy. The proposed Dynamic Decay Modulation (DDM) module optimizes the EMA coefficient during training based on student–teacher loss feedback, aiming to improve consistency and adaptability. Experiments conducted on LA Heart (MRI) and Pancreas-NIH (CT) datasets demonstrate superior performance compared to existing Mean Teacher variants across multiple segmentation metrics. The manuscript is clearly written and well-structured, though some theoretical and methodological aspects require further reinforcement for credibility and reproducibility.\n\n---\n\n**Major Comments**  \n1. **Insufficient Theoretical Justification** – The mathematical rationale for the student-guided meta-learning approach is incomplete. Key claims, such as the probability assumption in Equation (3) and the use of the Universal Approximation Theorem in Equation (5), lack rigorous proof or dimensional reasoning. The convergence and existence of optimal parameters are also not established.  \n2. **Experimental Design Limitations** – The empirical evaluation employs only two small datasets, limiting generalizability. Baseline comparisons are inconsistent, with missing results for some methods. No statistical significance analysis or confidence intervals are provided, and training is restricted to 6000 iterations, potentially hindering convergence assessment.  \n3. **Incomplete Technical Specification** – The architecture of the DDM module, meta-data construction, and hyperparameter tuning procedures are insufficiently described, impeding reproducibility. Computational overhead relative to baselines is not analyzed.  \n4. **Algorithmic and Notational Inconsistencies** – Algorithm 1 and the associated equations show ambiguities regarding cloned parameter updates, sequence of loss computation, and the explicit dependency between teacher parameters and DDM outputs in the bilevel optimization formulation.\n\n---\n\n**Minor Comments**  \n- Clarify the distinction between dataset partitions used for meta-data and training data.  \n- Specify missing architectural details (e.g., MLP layer sizes, activation functions).  \n- Ensure symbols and equations are consistent between the text and pseudocode.  \n- Include missing entries in tables or justify omissions.  \n\n---\n\n**Summary (TMI Criteria)**  \nThe work is **significant** in addressing adaptive EMA parameterization within semi-supervised segmentation and shows promising empirical gains. However, the **innovation** is diminished by the lack of solid theoretical grounding. The **evaluation** is limited in scope and statistical robustness, and the **reproducibility** suffers due to missing implementation specifics. Strengthening mathematical proofs, expanding experiments, and detailing methods would substantially improve the work’s overall reliability.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## Phase 1 – Analytical and Scope Audit\n\n**1. Scope & Relevance to TMI**  \n✅ The manuscript presents a new *methodological* contribution to semi-supervised *medical image segmentation*. It introduces a meta-learned mechanism for dynamically modulating the Exponential Moving Average (EMA) decay rate in Mean Teacher frameworks, verified on MRI and CT datasets. The focus is on learning methodology rather than purely application performance, so the scope is aligned with *IEEE TMI’s interest in methodological innovation for imaging analysis*.\n\n**2. Novelty & Contribution Level**  \nThe key novelty lies in the *Dynamic Decay Modulation (DDM)* module that adaptively learns the EMA decay coefficient through a meta-learning loop, as well as the proposed *student-guided* update scheme for the meta-learner. While the concept builds on preexisting Mean Teacher and meta-learning frameworks, dynamically optimizing EMA coefficients in this manner is not widely explored in medical imaging SSL. The contribution is incremental but technically distinct and could have cross-task generalization potential if presented with deeper theoretical or analytical depth.\n\n**3. Technical and Experimental Rigor**  \nThe paper provides detailed algorithmic formulation, pseudo-code, and mathematical argument for the superiority of the student-guided strategy. Reproducibility information is reasonably clear (datasets, hyperparameters, optimizer setup). Experiments span two standard datasets (LA MRI, NIH Pancreas CT) with comparisons to strong baselines. However, the performance gains are modest in magnitude (~0.3–1% improvements) and not statistically analyzed. No uncertainty measures or confidence intervals are reported. Additional datasets or sensitivity analysis (e.g., variation in labeling ratio, impact of DDM architecture) would strengthen the evidence.\n\n**4. Clarity and Presentation**  \nThe paper is generally well structured and written in standard machine learning style. Mathematical notation is clear, though dense. Figures are referenced appropriately; minor typographical inconsistencies persist (spacing, subscript alignment, variable font encoding). The exposition could benefit from clearer conceptual illustrations of the meta-learning loop and theoretical assumptions.\n\n**5. Ethical & Reproducibility Compliance**  \nBoth datasets (LA, NIH Pancreas) are public, with proper citations. No ethical concerns detected. The authors promise code release upon acceptance; however, there is no discussion of reproducibility artifacts or random seed control, which TMI now expects for machine-learning submissions.\n\n---\n\n## Phase 2 – IEEE TMI Review Report\n\n### 1. Summary\nThis paper proposes **Meta Mean Teacher**, a semi-supervised medical image segmentation framework that integrates a *Dynamic Decay Modulation (DDM)* module into the Mean Teacher paradigm. DDM uses meta-learning to dynamically adjust the EMA decay coefficient (α) between teacher and student models. The authors argue that a static α limits teacher adaptivity. In their proposed student-guided meta-update strategy, loss signals from the student drive the optimization of DDM parameters on a held-out meta set. Experiments on cardiac MRI (LA dataset) and abdominal CT (NIH Pancreas) demonstrate consistent, if modest, performance improvements over fixed-decay Mean Teacher and recent variants (UA-MT, AD-MT, PMT).\n\n### 2. Strengths\n- Addresses a well-motivated problem—the rigidity of Mean Teacher EMA updates.  \n- Introduces a generalizable, modular, plug-and-play meta-learning component.  \n- Provides solid algorithmic description and clear pseudo-code.  \n- Demonstrates cross-modality (MRI, CT) generalization.  \n- Compatible with various Mean Teacher–based frameworks and improves their performance.  \n\n### 3. Weaknesses\n- Conceptual novelty is moderate—extending existing SSL-MT frameworks rather than introducing a fundamentally new architecture.  \n- Marginal quantitative improvements (0.3–1% Dice gain) without statistical analysis.  \n- Limited exploration of DDM stability, computational cost, or ablation on DDM network structure.  \n- Theoretical analysis is mostly qualitative; derivations remain heuristic.  \n- Experiments restricted to two standardized datasets; lacks broad validation or clinical-readiness perspective.  \n\n### 4. Major Comments\n1. **Novelty Clarification:** Emphasize and clearly delineate how the proposed dynamic decay differs from simple adaptive EMA heuristics (e.g., scheduled or learnable α). Current presentation risks appearing as an incremental hyperparameter-tuning scheme.  \n2. **Experimental Validation:** Include statistical significance testing or confidence intervals to support claims of improvement. Conduct at least one additional dataset (e.g., brain tumor, liver segmentation) to demonstrate broader applicability.  \n3. **Reproducibility:** Provide code or pseudo-code for the DDM architecture (input features, dimensionality) and report computational overhead.  \n4. **Ablation and Sensitivity:** Explore the impact of (a) the DDM network size, (b) meta-learning rate η_D, and (c) number of meta iterations.  \n5. **Practical Implications:** Discuss how dynamic EMA affects training stability, convergence, and inference efficiency, which are critical for reproducibility and potential clinical integration.  \n\n### 5. Minor Comments\n- Fix typographical inconsistencies (e.g., “αm” spacing, duplicated variable symbols, inconsistent capitalization).  \n- Clarify Figure 2 legend and describe the meta-learning update process schematically.  \n- Cite recent related work on adaptive consistency and hyperparameter meta-learning in medical segmentation beyond 2023.  \n- Explicitly mention random seed settings and average results over multiple runs.  \n- Minor language editing for grammar and readability (“Teacher can provide an unbiased estimate only if…”).  \n\n### 6. Overall Recommendation\n**Recommendation:** **Major Revision**  \nThe paper is **in-scope** and **methodologically sound**, but its originality and evidential strength fall slightly below TMI acceptance threshold without further experimental substantiation. Expanded validation, statistical rigor, and deeper methodological insight would elevate it to TMI publication standards.  \n\n### 7. Confidence Level\n**4 / 5** – Confident in technical evaluation and relevance assessment; moderate uncertainty on potential unseen empirical results.\n\n---\n\n**Summary Verdict:**  \nThe submission presents a potentially useful, modular enhancement to Mean Teacher–based semi-supervised segmentation but requires stronger justification of novelty and experimental robustness to meet the standards of *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a methodological contribution titled *Meta Mean Teacher*, targeting semi-supervised medical image segmentation. It introduces a Dynamic Decay Modulation (DDM) module that meta-learns to adjust the exponential moving average (EMA) decay rate between teacher and student models in a Mean Teacher framework. Experiments on MRI (LA) and CT (NIH Pancreas) datasets support the approach, showing modest but consistent performance improvements. The paper is generally clear, well-organized, and relevant to *IEEE TMI*, focusing on methodological innovation in imaging analysis.\n\n---\n\n**Major Comments**  \n1. **Novelty and Positioning:** The contribution builds upon established Mean Teacher and meta-learning paradigms. Clarify in greater depth how the dynamic decay mechanism differs conceptually and practically from existing adaptive EMA or learnable α heuristics, to better establish its originality.  \n2. **Experimental Validation:** Report statistical significance or confidence intervals for the quantitative improvements (~0.3–1% Dice gain). The limited performance difference needs statistical support to demonstrate robustness. Extend the evaluation to additional datasets (e.g., brain or liver segmentation) to confirm generalizability.  \n3. **Ablation and Sensitivity Analysis:** Include studies on the DDM architecture (network size, learning rate η_D, number of meta iterations) and analyze the impact on model stability and accuracy.  \n4. **Reproducibility and Transparency:** Provide code or at least detailed pseudo-code for the DDM module, including input features, dimensionality, and computational overhead. Explicitly mention random seed control and results averaged over multiple runs.  \n5. **Practical and Theoretical Discussion:** Expand on how dynamically modulated EMA influences training stability, convergence, and inference cost. The theoretical section remains heuristic; a more rigorous justification of the meta-learning update mechanism would strengthen the work’s technical depth.\n\n---\n\n**Minor Comments**  \n- Correct typographical and notation inconsistencies (e.g., α spacing, variable duplication, capitalization).  \n- Clarify Figure 2 with a schematic of the meta-learning update process.  \n- Cite additional recent studies on adaptive consistency and hyperparameter meta-learning (post-2023).  \n- Conduct language editing for clarity (“Teacher provides unbiased estimates only if…”).  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The topic is relevant and addresses an important limitation of Mean Teacher frameworks but yields only modest gains.  \n*Innovation:* The dynamic decay strategy is technically sound yet represents an incremental extension of existing methods.  \n*Evaluation:* Experiments are well described but limited in scope and lack statistical analysis.  \n*Reproducibility:* Dataset and implementation details are generally provided; however, public code, seed control, and computational cost reporting must improve transparency.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe paper is methodologically valid and appropriate for TMI’s scope but requires stronger evidence of novelty and broader, statistically supported evaluation to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "## Opening Paragraph\nThe manuscript proposes a novel methodological contribution to semi-supervised medical image segmentation through the introduction of a *Dynamic Decay Modulation (DDM)* module within the Mean Teacher framework. This approach aims to dynamically adjust the Exponential Moving Average (EMA) decay rate using a meta-learning mechanism, validated on MRI and CT datasets. The focus lies on advancing learning methodologies, aligning well with the interests of *IEEE TMI* in fostering methodological innovation for imaging analysis.\n\n## Major Comments\n1. **Novelty Clarification:** The manuscript should clearly differentiate the proposed dynamic decay from existing adaptive EMA heuristics (e.g., scheduled or learnable α) to avoid the perception of merely fine-tuning hyperparameters.\n2. **Experimental Validation:** The study lacks statistical significance testing and confidence intervals to substantiate performance improvements (0.3–1% Dice gain). Including at least one additional dataset (e.g., brain tumor, liver segmentation) would bolster claims of broader applicability.\n3. **Reproducibility:** Code or pseudo-code for the DDM architecture should be provided, detailing input features and dimensionality, along with documentation on computational overhead.\n4. **Ablation and Sensitivity:** There should be an exploration of how configurations such as (a) DDM network size, (b) meta-learning rate (η_D), and (c) the number of meta iterations affect the performance.\n5. **Practical Implications:** The impact of dynamic EMA on training stability, convergence, and inference efficiency should be discussed, given their significance for reproducibility and clinical integration.\n\n## Minor Comments\n- Address typographical inconsistencies (e.g., “αm” spacing issues, duplicated variable symbols, inconsistent capitalization).\n- Enhance the clarity of Figure 2’s legend and integrate a schematic representation of the meta-learning update process.\n- Relevant recent work on adaptive consistency and hyperparameter meta-learning in medical segmentation beyond 2023 should be cited.\n- Mention random seed settings explicitly and provide average results across multiple runs.\n- Minor language editing for grammatical accuracy and readability is recommended.\n\n## Summary Paragraph\nThe manuscript shows significant potential in applying a novel DDM module to the Mean Teacher paradigm, addressing a pertinent issue in semi-supervised learning. While the contribution is technically distinct, its novelty is somewhat limited as it extends existing frameworks rather than introducing a revolutionary architecture. The experimental validation lacks statistical analysis, which diminishes its robustness and raises concerns about reproducibility. Overall, it aligns moderately with standards of significance and innovation expected by *IEEE TMI*, yet requires enhanced evidence and exploration of its practical implications.\n\n## Decision Recommendation\n**Recommendation:** **Major Revision**  \nThe study is methodologically sound and remains within the scope of the journal. However, improvements in originality and evidential support are necessary to meet the acceptance criteria for *IEEE Transactions on Medical Imaging*. Further validation and methodological insight will enhance its publication prospects.\n\n## Confidence Level\n**4 / 5** – Confidence in the technical evaluation and relevance is high, with moderate uncertainty regarding potential undiscovered empirical results.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chen Chen",
      "Le Wang",
      "Ning Gao",
      "Sanping Zhou"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a2ba00a164f58076d0fbf59a182260f8e32b0178.pdf",
    "remote_url": "https://openreview.net/pdf/a2ba00a164f58076d0fbf59a182260f8e32b0178.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Conformal confidence sets for biomedical image segmentation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Deep learning",
      "neural networks",
      "uncertainty quantification",
      "confidence sets"
    ],
    "abstract": "We develop confidence sets which provide spatial uncertainty guarantees for the output of a black-box machine learning model designed for image segmentation. To do so we adapt conformal inference to the imaging setting, obtaining thresholds on a calibration dataset based on the distribution of the maximum of the transformed logit scores within and outside of the ground truth masks. We prove that these confidence sets, when applied to new predictions of the model, are guaranteed to contain the true unknown segmented mask with desired probability. We show that learning appropriate score transformations on an independent learning dataset before performing calibration is crucial for optimizing performance. We illustrate and validate our approach on polyps colonscopy, brain imaging and teeth datasets. To do so we obtain the logit scores from deep neural networks trained for polyps, brain mask and tooth segmentation segmentation. We show that using distance and other transformations of the logit scores allows us to provide tight inner and outer confidence sets for the true masks whilst controlling the false coverage rate.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes a conformal prediction based method to quantify the uncertainty for medical image segmentation. The proposed method is particularly designed for pre-trained segmentation models which notoriously make overconfident and wrong predictions. The proposed method learns thresholds using the maximum logit scores from a calibration set for the inside and outside of the ground truth masks and apply them on the logit scores of the test image to return conformalized segmentation prediction which guarantees to include the ground truth segmentation. The paper shows that naively learning the outside thresholds on max logits is not optimal and propose to transform the scores using a distance to make sure that far away pixels have lower scores. The method is validated on a single dataset for polyp segmentation and the results show that the proposed method produces conformal sets with narrower boundaries compared to using scores which are not transformed.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 1\n\n### Strengths\n\n- The idea of using transformed max logit scores is simple but quite effective strategy to produces conformal segmentation sets.\n- The presented experiments show the effectiveness of the method compared to using non-transformed logits.\n\n### Weaknesses\n\n1- Although I found the proposed idea of transforming max logit scores interesting, I don't think that the paper presents enough contribution to be presented in ICLR. The idea of applying conformal prediction to max logits for inside and outside of the boundaries is a direct extension of initial conformal prediction methods developed for segmentation, and applying transformations based on distance is an intuitive choice to refine predicted boundaries.\n\n2- The paper does not present any comparisons with the existing conformal prediction works for image segmentation.\n\n[1] Mossina et al. Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty, CVPR Workshops, 2024,\n\n3- The method is evaluated on only a single dataset. Multiple datasets should be included to make sure that the performance generalizes across datasets.\n\n4- In many segmentation tasks, we are interested in segmenting multiple structures. The paper only focuses on binary segmentation. I think the method should be validated on multi-class setting to make sure that it is also applicable in that setting.\n\n5- The explanation of how the method is applied at test time could also be clearer. As I understand it, during testing, the method applies the inner threshold on max logits to find inner boundaries, then applies a distance transformation based on each pixel’s distance from these inner boundaries, and finally applies an outer boundary threshold. However, the exact steps of the algorithm during test time need more clarification.\n\n6- In conventional uncertainty quantification algorithms for segmentation such as [2, 3] the uncertainty is quantified by the variance of the segmentation samples generated from the posterior distribution. How can the quantification be done in this case? Is it the margin between the inner and outer boundaries? Is the uncertainty quantified by the algorithm correlates with the uncertainty in the input image? For example, does the method output larger margins when there is greater disagreement between the segmentations of different experts?  \n\n[2] Kohl et al. A Probabilistic U-Net for Segmentation of Ambiguous Images\n[3] Erdil et al. MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\n\n7- The margin between the inner and outer boundaries appears quite large and there can be many unplausible segmentations within this area. For practical applications, an uncertainty quantification method should ideally produce a set of plausible segmentation samples within this margin, rather than simply indicating a large margin that may or may not include the ground truth segmentation. How could one obtain a plausible segmentation sample from this margin?\n\n### Questions\n\n- How does the results generalize to other datasets and segmentation of multiple structures?\n- How does the uncertainty quantified by the proposed method relates with the real uncertainty (assuming it can be measured by the disagreement between multiple experts)?\n- How one can use the proposed method in a practical application? Can we get samples of plausible segmentations within the margin outputted by the algorithm?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a method utilizing conformal prediction to quantify uncertainty in medical image segmentation, specifically aimed at addressing the overconfident predictions of pre-trained segmentation models. By learning thresholds from maximum logit scores within a calibration set and applying these to test images, the method generates conformalized segmentation predictions that are guaranteed to encompass the ground truth. While the approach shows promise through its validation on a polyp segmentation dataset, its contributions appear limited.\n\n### Major Comments\n1. The core idea of transforming max logit scores for improved segmentation is effective, yet the contribution seems minimal for ICLR consideration. The methodology seems to extend existing conformal prediction techniques without significant novelty.\n2. There is a lack of comparisons with existing conformal prediction works in image segmentation, such as those by Mossina et al. (2024).\n3. Evaluation is conducted on a single dataset, which raises concerns about the generalizability of the method's performance across diverse datasets.\n4. The focus on binary segmentation limits the applicability; validation in a multi-class setting is recommended.\n5. Clarity regarding test-time application of the method is insufficient. A step-by-step explanation of the process involving thresholds and distance transformations is needed.\n6. The comparison of uncertainty quantification with conventional methods remains unclear. It is unclear if the method’s margins effectively correlate with input image uncertainty or reflect expert disagreement in segmentation.\n7. The large margins between inner and outer boundaries suggest potential implausibility in segmentations within that area, indicating the need for a method that produces plausible segmentation samples.\n\n### Minor Comments\n- Figure captions could be clearer.\n- Some terminology and acronyms require definitions or additional context for clarity.\n- There are minor typographical errors that should be addressed.\n\n### Summary Paragraph\nThe paper offers an innovative approach to uncertainty quantification in medical image segmentation, showing potential effectiveness through transformed max logit scores. However, the significance and novelty of the contribution raise concerns, as the method largely relies on existing techniques without thorough comparisons or validation across multiple datasets. Issues related to clarity in methodology and questions regarding the real-world applicability of the uncertainty measures also detract from the overall impact of the work.\n\n### Decision Recommendation\nMajor revision is recommended, focusing on enhancing the novelty of the contributions, providing comparative analyses with prior work, expanding the evaluation across multiple datasets, and improving the clarity of the methodological exposition.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a conformal prediction–based method for quantifying uncertainty in medical image segmentation, targeting pre-trained models that may produce overconfident predictions. The approach calibrates thresholds for inside and outside regions of ground-truth masks using maximum logit scores and applies these thresholds to test images. A distance transformation is incorporated to adjust scores such that pixels farther from boundaries receive lower confidence. The method is validated on a single polyp segmentation dataset, demonstrating narrower conformal boundaries than versions without distance transformation. The paper is clearly written but limited in scope and contribution.\n\n---\n\n**Major Comments**  \n1. **Contribution and Novelty:** While the idea of transforming maximum logit scores is conceptually sound, the novelty is limited. The method extends existing conformal prediction frameworks for segmentation in a straightforward way. Using distance-based transformations appears as an intuitive refinement rather than a substantial methodological advance.  \n2. **Comparative Evaluation:** The paper lacks direct comparisons with prior conformal prediction approaches for image segmentation, such as Mossina et al., CVPR Workshops, 2024. Including such baselines would strengthen claims of improvement.  \n3. **Dataset Limitation:** Evaluation is restricted to a single dataset. Experiments on multiple datasets are needed to demonstrate generalizability.  \n4. **Scope of Segmentation Tasks:** The study only addresses binary segmentation. Validation on multi-class segmentation would be necessary to confirm applicability to broader clinical settings.  \n5. **Clarity of Method Description:** The procedure at test time is insufficiently explained. The sequence of applying inner thresholds, distance transformations, and outer thresholds should be clearly described.  \n6. **Quantification of Uncertainty:** The manuscript does not clarify how uncertainty is explicitly measured or interpreted. It remains unclear whether the margin between inner and outer boundaries meaningfully correlates with true uncertainty or expert disagreement.  \n7. **Practical Interpretability:** The large margins between boundaries may encompass many implausible segmentations. The paper should discuss how plausible segmentation samples could be derived within these conformal regions.\n\n---\n\n**Minor Comments**  \n- Improve clarity in the methodological description, especially regarding test-time implementation steps.  \n- Define how the proposed uncertainty measure relates to conventional metrics (e.g., variance-based approaches).  \n- Clarify notation for thresholds and logit transformations.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses a relevant problem—uncertainty quantification in medical image segmentation—through conformal prediction. Its innovation lies in applying distance-transformed logits, though the conceptual novelty and breadth of validation remain modest. Experimental evidence is limited, and the reproducibility could be improved through clearer methodological details. The study modestly advances the field but falls short in terms of generalization, comparison, and interpretability of uncertainty estimates.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors formally present an approach that aims at inferring uncertainty margins to segmentations. They propose either take the logit score of a CNN and to threshold it to obtain this margin, or to threshold at a certain distance to the predicted segmentation. Threshold and type of margin (logit score / distance) is to be identified experimentally for a given dataset. Experiments on one public dataset  are shown (containing still images from minmally invasive surgery).\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* The authors present the problem in a formal manner, relating it to existing work.  \n* The overall problem addressed is relevant.\n\n### Weaknesses\n\n* The motivation for the scores functions (logit, distance, ...) is weak. The necessity to choose the type and to even mix them gives the overall approach a bit of a heuristic touch. (While I do understand that you would consider your contribution here to be in the formal derivation of underlying theory, i.e., very much the opposite of a heuristic.)\n* The experiments only provide insights into one very narrow application. they are merely fulfilling the purpose of an illustation of the problem, but not a validation.\n\n### Questions\n\n* You are testing on public data. Has your pretrained polyp segmentation algorithm been trained on the same public data? \n* Are there any susequent video frames in the dataset, or images of the same polyp / patient? If there are, did you stratify your training / testing set accordingly?  \n* Please remove the reference to tumors throughout the paper. Polyps may be precursors to tumors, but they aren't any. \n* You are using a dataset from different centers, there may be systematic differences in how the polyp areas are annotated - some annotators being more inclusive with respect to surrounding tissue, others being less. How does this variability impact on your measure? \n*  I might have missed it but what is the accuracy of your underlying segmentation algorithm? I would be under the impression that it is a well performing algorithm on a rather easy segmentation task? How does your approach relate to extrema in algorithmic performance, i.e., perfect segmentations or complete misses? \n* You are stating \"In order to make efficient use of the data available, the learning dataset can in fact contain some or all of the data used to train the image segmentor.\" Your training data may be fairly overfittet impacting on your logit score and, hence, your choice of margin (logit/distance, thresholds). Wouldn't it be a safer approach to generate cross-validated logit functions and use them in the comparison?\n* I understand that the primary contribution of this study is the theory offered. Still, you are stressing that your algorithm is a very lightweight addition to any pretrained segmentation algorithm. And there are a lot of standard computer vision / biomedical image data sets for segmentation available, as well as pretrained algorithms. Would you be able to generate segmentations maps for predefined certainty levels, and compare these levels with the testing performances across a larger set of applications? It would be quite convincing, if e.g., your 90% certainty map of the outer margin would indeed include 90% pixels of a test set or lead to a sufficiently large overlap (that has previously been defined) in 90% of all test cases.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3\n\n### Details Of Ethics Concerns\n\nnone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents an approach for inferring uncertainty margins in segmentations using logit scores from Convolutional Neural Networks (CNNs) or distance-based thresholds relative to predicted segmentations. The method is explored through experiments conducted on a public dataset comprising still images from minimally invasive surgeries. While the problem addressed is relevant, the clarity and soundness of the contribution require improvement.\n\n**Major Comments:**  \n1. The motivation behind the choice of scoring functions (logit vs. distance) is inadequately justified, resulting in a heuristic character to the methodology.\n2. The experiments focus solely on a very narrow application, serving more as an illustration rather than comprehensive validation of the approach.\n3. The training of the polyp segmentation algorithm on the same public dataset should be clarified to ensure validity.\n4. It is essential to address whether subsequent video frames or images of the same polyp/patient exist and if the training/testing sets are appropriately stratified.\n5. The manuscript makes repeated references to tumors, which should be corrected as polyps are precursors rather than tumors themselves.\n6. The variability in annotation across different centers could impact the measures; this factor requires discussion.\n7. The accuracy of the underlying segmentation algorithm needs to be stated and discussed relative to its performance extremes.\n8. Concerns arise regarding overfitting if the learning dataset contains images used for training the segmentor; cross-validated logit functions should be considered for comparison.\n9. The authors should explore generating segmentation maps at predefined certainty levels and compare these to testing performances across a larger set of applications.\n\n**Minor Comments:**  \n- Remove references to tumors throughout the manuscript.  \n- Clarify the accuracy of the segmentation algorithm.  \n- Revise figures and notation for improved clarity.  \n- Address typos found within the text.\n\n**Summary Paragraph:**  \nIn conclusion, while the study presents a theoretically sound approach to addressing segmentation uncertainty, it struggles with empirical validation and clarity regarding its methodological foundations. The significance of the contribution is somewhat tempered by the limited scope of experiments, which impacts reproducibility. There remains a need for innovation within a broader context and a clearer linkage between methodology and algorithmic performance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a formal framework for estimating uncertainty margins in image segmentations. The authors propose two methods—using the CNN logit score or distance from the predicted segmentation—to define the margin. The threshold and margin type are determined experimentally for a specific dataset. Experiments are demonstrated on a single public dataset composed of still images from minimally invasive surgery. The paper addresses a relevant problem, and the formalization is clearly related to existing literature, though the presentation and validations remain limited.  \n\n**Major Comments**  \n1. **Motivation and Theoretical Coherence:** The justification for using logit or distance scores as uncertainty functions is weak. The need to manually select or even combine these adds a heuristic quality that contradicts the paper’s claim of formal rigor.  \n2. **Experimental Breadth and Validation:** The evaluation is restricted to a narrow dataset and mostly serves an illustrative purpose rather than a validation demonstrating generality or robustness.  \n3. **Training Protocol Clarifications:** It is unclear whether the pretrained polyp segmentation algorithm was trained on the same public data used for testing. Additional information about sample stratification is needed, particularly regarding possible overlap of frames or patients.  \n4. **Dataset and Annotation Variability:** The dataset originates from multiple centers. Differences in annotation style could systematically affect uncertainty estimates; the manuscript should address this potential bias.  \n5. **Base Model Performance:** The accuracy of the underlying segmentation model is unspecified. Clarifying this is necessary to interpret the proposed uncertainty margins, especially for extreme cases of very good or poor segmentation.  \n6. **Overfitting and Validation Strategy:** The suggestion to use cross-validated logits instead of potentially overfitted ones should be considered to ensure unbiased margin derivation.  \n7. **Generalizability of Approach:** Extending the experiments to additional standard datasets could strengthen the claims, e.g., validating whether a nominal 90% certainty margin corresponds to desired coverage across different tasks.  \n\n**Minor Comments**  \n- Remove references to “tumors,” as polyps are not tumors.  \n- Clarify whether sequential frames or repeated samples occur in the dataset.  \n- Ensure consistent terminology and fix minor typographical errors.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is conceptually relevant and formalizes a practical uncertainty estimation framework. However, innovation is tempered by heuristic design choices and limited empirical validation. The experimental evaluation is too narrow to confirm general significance, and reproducibility would depend on explicit disclosure of training/testing splits, base model performance, and threshold selection.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors propose a conformal prediction method that computes confidence sets with spatial uncertainty guarantees in image segmentation from any machine learning model. They illustrate the usefulness of the proposed method on medical images.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper is well-written and clear, although it took a second read-through to fully understand. The proposed method seems to work very well, and the presented experiments are convincing.\n\n### Weaknesses\n\nI am missing more quantitative results. For instance, aggregated coverage scores (e.g., mean; or other metrics, e.g., evaluate Equations 1 and 2) for the different versions on more than one dataset. This comparison should then also include some existing methods, to illustrate the relative strengths of different methods.\n\nAs just mentioned, for the results to be more convincing, I would also like to see examples on more than just one dataset.\n\nAlso, there must be other score transformation functions that could also be evaluated. Testing a couple more could strengthen the results and make it more convincing.\n\n### Questions\n\n- Couldn't a related/similar smooth distance be defined using kernels?\n- What is called \"original scores\", is this when you use the identity score transformation?\n- What are the dashed lines in Figures 4 and 5?\n\nMajor comments:\n- Add labels and/or legends to the rows and columns of the figures.\n\nMinor comments:\n- The word \"polyp\" is misspelled in different ways in almost every instance. Do check this.\n- It says \"... the set a side [num] images ...\", or something similar, a few times. Check the grammar there.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a conformal prediction method designed to compute confidence sets with spatial uncertainty guarantees specifically for image segmentation using any machine learning model. The authors illustrate its applicability through experiments on medical images, aiming to contribute to the field of image analysis under uncertainty.\n\n**Major Comments:**  \n1. The manuscript lacks sufficient quantitative results. It would benefit from aggregated coverage scores (e.g., mean or other metrics) for the different versions tested across multiple datasets. \n2. The comparison should also include existing methods to highlight the strengths and limitations of the proposed approach.\n3. Convincing results could be enhanced by providing examples from more than one dataset.\n4. The evaluation of additional score transformation functions is recommended, as this could further strengthen the results.\n\n**Minor Comments:**  \n- Figures should include labels and/or legends for both rows and columns to enhance clarity.\n- The word \"polyp\" is misspelled in various ways throughout the manuscript; please verify consistency.\n- There are grammatical issues present in the phrase “... the set a side [num] images ...,” found in multiple instances; a revision is necessary.\n\n**Summary Paragraph:**  \nThis study presents a new conformal prediction method that demonstrates potential significance in the field of image segmentation. While the innovation is noteworthy, the evaluation lacks depth due to insufficient quantitative data and comparative analysis against existing methodologies. The reproducibility of results is questionable, given the reliance on a single dataset for illustrative examples. Strengths include clear writing and a well-structured presentation, yet further data and comparative results are needed to enhance the overall robustness of the findings.\n\n**Decision Recommendation:**  \nMajor revision is recommended to address the outlined weaknesses and provide a more comprehensive assessment of the proposed method.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a conformal prediction framework for image segmentation that generates spatially grounded confidence sets applicable to outputs from any machine learning model. The authors demonstrate the method’s utility on medical imaging data. Overall, the paper is clearly written and the methodology appears sound, although some aspects required rereading for full comprehension. The reported experiments indicate promising performance.\n\n**Major Comments**  \n1. **Quantitative evaluation:** The results section lacks comprehensive quantitative analysis. It would strengthen the work to report aggregated coverage scores (e.g., mean values or other relevant metrics based on Equations 1 and 2) across multiple versions and datasets.  \n2. **Comparative analysis:** The study would be more convincing if it included results on more than one dataset and compared performance with existing methods to contextualize relative strengths.  \n3. **Score transformation evaluation:** Additional experiments exploring alternative score transformation functions are recommended, as other viable options may yield different or improved outcomes.  \n4. **Figure labeling:** Figures should include clear labels or legends for rows and columns to enhance readability and interpretation.  \n\n**Minor Comments**  \n- The word *polyp* is inconsistently spelled in several places and should be standardized.  \n- The phrase “set a side [num] images” appears multiple times with incorrect grammar and should be corrected.  \n- Clarify in the manuscript: (a) whether “original scores” refers to using the identity score transformation, (b) the meaning of dashed lines in Figures 4 and 5, and (c) whether a related smooth distance could be defined using kernel functions.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important problem by introducing spatially aware uncertainty quantification for segmentation models, a topic of clear significance. The approach offers innovative use of conformal prediction in this context. However, the evaluation remains limited in breadth and comparison, which reduces the strength of the evidence supporting its claims. The method appears reproducible, but additional datasets and clarified experimental details would improve confidence in the results.\n\n**Decision Recommendation**  \n**Major revision.** The concept is promising and the paper is generally well written, but stronger quantitative validation and broader experimental comparison are needed to substantiate the contribution.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper develops conformal confidence sets for biomedical image segmentation that provide spatial uncertainty guarantees with probabilistic coverage. The method adapts conformal inference to imaging by computing thresholds on calibration data based on maximum transformed logit scores within and outside ground truth masks (Section 2.2, equations 3-5). The approach constructs inner sets guaranteed to be contained within true masks and outer sets guaranteed to contain true masks with desired probability (Theorems 2.1-2.2, pages 3-4). The authors prove that learning score transformations, particularly distance transformations, is crucial for tight bounds (Theorem 2.8, page 5). Applications demonstrate the method on polyp colonoscopy segmentation using PraNet (Section 3, pages 5-8), brain mask segmentation using HD-BET (Section 4, page 9), and teeth segmentation using U-Net GAN (Section 5, pages 9-10), showing controlled coverage rates and informative spatial bounds.\n\n## Weaknesses\n\n• **Mathematical notation inconsistencies and clarity issues**\n  - Score function s maps X → X but elements Xi are also in X, creating notational confusion between input images and score images (Section 2.1, page 2)\n  - Distance transformation definition uses sign(A,v) without clear definition until later in the same paragraph (page 5, line 252)\n  - Assumption 2 independence condition lacks precise specification of what constitutes \"independence\" in this high-dimensional imaging context (page 3, line 121)\n\n• **Limited theoretical analysis of score transformation selection**\n  - No theoretical framework provided for systematically choosing optimal transformations beyond empirical evaluation on learning datasets (Section 2.4, pages 4-5)\n  - Theorem 2.8 only applies to distance transformations but applications use various transformations (smoothing, identity) without theoretical justification (pages 5, 9-10)\n  - Missing analysis of when different transformations are expected to perform well based on data characteristics or model properties\n\n• **Insufficient experimental validation scope**\n  - Brain imaging validation uses only 524 subjects with very small test set of ~174 subjects, limiting statistical power (Section 4, page 9)\n  - Teeth segmentation uses only 28 test subjects, providing inadequate validation (Section 5, page 9)\n  - No comparison with other uncertainty quantification methods beyond basic conformal approaches, missing important baselines\n\n• **Questionable independence assumptions in practice**\n  - Assumption 1 (exchangeability) may be violated when images come from same patients or medical centers, but this is not addressed (page 3, lines 113-118)\n  - Use of training data as learning dataset potentially violates Assumption 2 independence despite authors' claims (Section 2.4, page 4, lines 225-235)\n  - No empirical validation that exchangeability holds across different medical imaging datasets\n\n• **Computational complexity and scalability concerns**\n  - No analysis of computational costs for distance transformations on high-resolution medical images (Algorithm 1, page 16)\n  - Method requires separate calibration datasets which may be impractical in medical settings with limited labeled data\n  - Missing discussion of memory requirements and processing time for 3D medical volumes\n\n## Suggestions for Improvement\n\n• **Clarify mathematical formulations and notation**\n  - Define score function mapping more precisely, distinguishing between input images and score images with different notation (e.g., Xi for inputs, Si for scores)\n  - Provide complete definition of distance transformation components before usage, including explicit sign function definition\n  - Formalize independence assumption with specific conditions for high-dimensional imaging data, possibly using conditional independence\n\n• **Develop theoretical framework for transformation selection**\n  - Provide theoretical analysis of when specific transformations (distance, smoothing, identity) are optimal based on model characteristics or data properties\n  - Extend theoretical guarantees beyond distance transformations to cover the full range of transformations used in applications\n  - Develop systematic methodology for transformation selection that goes beyond empirical evaluation on learning datasets\n\n• **Expand experimental validation comprehensively**\n  - Increase sample sizes particularly for brain imaging and teeth segmentation to achieve adequate statistical power for coverage validation\n  - Include comparison with other uncertainty quantification methods (e.g., MC Dropout, deep ensembles, Bayesian approaches) to establish relative performance\n  - Validate across more diverse medical imaging domains and imaging modalities to demonstrate broader applicability\n\n• **Address practical independence assumption violations**\n  - Provide empirical tests for exchangeability assumption across different medical imaging scenarios and datasets\n  - Develop robust approaches when training data must be used for transformation learning while maintaining theoretical validity\n  - Investigate sensitivity of coverage guarantees to mild violations of exchangeability that may occur in practice\n\n• **Analyze computational requirements and scalability**\n  - Provide detailed computational complexity analysis including memory and time requirements for different image sizes and transformations\n  - Develop efficient implementations for 3D medical volumes and high-resolution images\n  - Address practical deployment considerations including calibration data requirements and processing time constraints in clinical settings",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a method for developing conformal confidence sets aimed at enhancing spatial uncertainty guarantees in biomedical image segmentation. The proposed approach adapts conformal inference techniques to imaging contexts by using thresholds calculated from maximum transformed logit scores within calibration data. Through detailed theoretical proofs and applications, the authors demonstrate the method's effectiveness across various segmentation tasks, including polyp colonoscopy, brain mask, and teeth segmentation, indicating controlled coverage rates and informative spatial bounds.\n\n### Major Comments\n1. **Mathematical Notation Inconsistencies**: The score function notation creates confusion between input images and score images, particularly in Section 2.1. Moreover, the definition of distance transformations lacks clarity until later in the document, and Assumption 2's independence condition requires a more precise specification in the context of high-dimensional imaging.\n\n2. **Limited Theoretical Analysis of Score Transformation Selection**: The manuscript does not provide a systematic theoretical framework for optimal transformation selection, relying primarily on empirical evaluations. Theorem 2.8 applies only to distance transformations while other employed transformations lack theoretical justification.\n\n3. **Insufficient Experimental Validation Scope**: The validation for brain imaging is limited to 524 subjects and a small test set, raising concerns about statistical power. Similarly, the teeth segmentation study uses only 28 test subjects, and there is a lack of comparisons against established uncertainty quantification methods.\n\n4. **Questionable Independence Assumptions in Practice**: Assumption 1 may be violated for images sourced from the same patients, yet this concern is not addressed. Incorporating training data into the learning dataset also poses potential violations to Assumption 2.\n\n5. **Computational Complexity and Scalability Concerns**: The manuscript does not analyze computational costs related to distance transformations on high-resolution images and fails to address the implications of requiring separate calibration datasets.\n\n### Minor Comments\n- Mathematical formulations should be clarified to differentiate input images from score images more distinctly.\n- Provide complete definitions for all mathematical components, including the explicit definition of the sign function.\n- Amend documentation regarding independence assumptions to be more rigorous.\n- Consider presenting more comprehensive statistics regarding computational requirements.\n\n### Summary Paragraph\nThe reviewed manuscript showcases significant contributions towards uncertainty quantification in biomedical image segmentation, yet it has notable weaknesses in theoretical analysis, experimental scope, and practical applicability. While the innovation aspect is strong, evidenced by the novel method for constructing confidence sets, the evaluation phase could be improved through broader statistical validation and comparisons with alternative methods. Reproducibility concerns stem from untested independence assumptions and the requirement for extensive calibration datasets, which could limit practical deployment.\n\n### Decision Recommendation\nMajor Revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a method for constructing conformal confidence sets for biomedical image segmentation, providing spatial uncertainty quantification with formal probabilistic guarantees. The authors adapt conformal inference to imaging by deriving inner and outer segmentation sets through calibrated thresholds on transformed logit scores. Theoretical guarantees are established for coverage, and the importance of learning suitable score transformations—particularly distance-based transformations—is demonstrated. Experiments on multiple segmentation tasks (polyp, brain, and teeth) illustrate the practical feasibility of the proposed framework. Overall, the paper addresses an important problem and is mathematically well-motivated, though several aspects require clarification, theoretical strengthening, and more comprehensive validation.\n\n---\n\n**Major Comments**  \n1. **Notation and Clarity:** The score function definition creates ambiguity between input and score domains, and several key notations (e.g., `sign(A,v)`) are used before being defined. Independence assumptions are insufficiently specified for high-dimensional imaging contexts.  \n2. **Transformation Theory:** Theoretical analysis is limited to distance transformations, with no framework provided for systematically selecting or analyzing other transformation types used in experiments. The paper should characterize when different transformations are expected to perform best.  \n3. **Experimental Scope:** The empirical validation is narrow, with small test sets (e.g., 174 subjects for brain imaging and 28 for teeth segmentation) and limited comparison against alternative uncertainty quantification methods. Broader and more statistically robust experiments are needed.  \n4. **Independence Assumptions:** Practical violations of exchangeability and independence—particularly when data share patients or acquisition sites—are not addressed or empirically validated. The reliance on the same dataset for training and transformation learning may compromise theoretical guarantees.  \n5. **Computational Efficiency:** No analysis is provided regarding the computational complexity, memory usage, or scalability of the proposed algorithm, which may pose challenges for high-resolution or 3D imaging datasets.\n\n---\n\n**Minor Comments**  \n- Clarify score mapping notation and define all variables before use.  \n- Provide precise definitions of distance transformation elements.  \n- Specify assumptions using measurable conditions or conditional independence language.  \n- Include details on the calibration data’s composition and size.  \n- Correct minor notational inconsistencies and page cross-references.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The work tackles clinically important uncertainty quantification in segmentation.  \n**Innovation:** The adaptation of conformal inference to spatial confidence regions is novel, though theoretical coverage is restricted to specific transformations.  \n**Evaluation:** Experiments demonstrate feasibility but lack comparative breadth and statistical robustness.  \n**Reproducibility:** The method is generally described, but incomplete definitions, ambiguous notations, and untested assumptions limit clarity and reproducibility. Strengthening theoretical generality, empirical scope, and computational transparency would notably improve the manuscript.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to IEEE TMI:**  \n   ✅ The manuscript presents a methodological contribution grounded in statistical learning theory applied to medical image analysis. It introduces *conformal confidence sets* for segmentation tasks with formal distribution-free probabilistic coverage guarantees. The focus on uncertainty quantification rather than base segmentation accuracy aligns well with TMI’s scope in quantitative imaging methodology.\n\n2. **Novelty & Contribution Level:**  \n   The work extends conformal prediction to pixel/voxel-level image segmentation, providing spatial probabilistic coverage through inner/outer confidence sets. While conformal inference in imaging has been explored previously, the paper introduces several methodological refinements:\n   - Use of maximum-transformed logit distributions to define image-level coverage sets.\n   - Theoretical coverage proofs under exchangeability for complex image-valued outputs.\n   - Adaptive score transformations (e.g., distance transform) optimized on an independent dataset.\n   - Empirical validation across medical domains (polyps, brain MRI, dental segmentation).  \n   This represents a non-trivial methodological advance rather than a simple application of existing conformal methods.\n\n3. **Technical and Experimental Rigor:**  \n   The theoretical formulation is mathematically detailed with formal proofs of coverage and supporting lemmas. Experimental validation spans multiple biomedical imaging modalities and datasets, using established segmentation backbones (PraNet, HD-BET, U-Net GAN). Extensive quantitative and visual analyses of coverage rates confirm the theoretical guarantees.  \n   Possible limitations include limited comparison with alternative uncertainty quantification frameworks (e.g., Bayesian segmentation or Monte Carlo dropout); such benchmarks would strengthen the claim of superiority. Reproducibility is aided by stated MATLAB code availability and computational cost reporting.\n\n4. **Clarity and Presentation:**  \n   The manuscript is clear, well-structured, and technically mature. Theory sections are dense but logically ordered. Figures effectively demonstrate spatial coverage. Minor stylistic issues (occasionally inconsistent spacing and typographical artifacts like \"I LLUSTRATING\" or “1 I NTRODUCTION”) likely stem from preprint formatting, not clarity problems.\n\n5. **Ethical and Reproducibility Compliance:**  \n   Publicly available datasets are properly cited (e.g., HyperKvasir, LPBA40, NFBS, CC-359, etc.), and data partitioning for calibration/test/learning sets is clearly described. There are no evident ethical infractions or patient-identifiable data. Code availability promises reproducibility; however, public release should be verified on acceptance.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n1. **Summary:**  \n   The authors propose a principled framework for uncertainty quantification in biomedical image segmentation through *conformal confidence sets*. Their method adapts conformal prediction to spatially structured outputs, producing inner and outer sets with guaranteed coverage of the true segmentation mask at user-specified confidence levels. The approach leverages transformations (particularly distance transforms of predicted masks) to yield compact and interpretable confidence sets. Validation on polyp colonoscopy, brain MRI, and dental imaging demonstrates consistent nominal coverage and practical interpretability, bridging a key gap between prediction accuracy and reliability in medical AI.\n\n2. **Strengths:**\n   - Solid statistical foundations with formal coverage guarantees.\n   - Innovative extension of conformal inference to spatial imaging domains.\n   - Demonstrated adaptivity via learned score transformations.\n   - Strong empirical validation across multiple modalities.\n   - Transparent discussion of trade-offs between precision and confidence.\n\n3. **Weaknesses:**\n   - Limited comparison with probabilistic baselines or other uncertainty methods (e.g., evidential deep learning, Bayesian neural nets).\n   - Clinical interpretability and potential integration into workflows are mentioned only briefly—further illustration of how clinicians might use these confidence maps would enhance impact.\n   - The method assumes exchangeability and independence assumptions that may not fully capture correlated volumetric data.\n\n4. **Major Comments:**\n   - **Positioning vs. prior work:** Strengthen comparison to existing conformal risk control and Bayesian segmentation papers in medical imaging (beyond Angelopoulos & Bates, Mossina et al.).  \n   - **Generality:** Discuss possible extensions to 3D volumetric datasets and correlated slices—currently the theoretical framework assumes discrete voxel sets.  \n   - **Quantitative comparison:** Include a table comparing uncertainty calibration metrics (Expected Calibration Error, coverage–efficiency trade-offs) against standard uncertainty quantifiers.  \n   - **Clinical interpretability:** Add a brief analysis of how practitioners could interpret confidence set overlays (e.g., in radiological review).  \n\n5. **Minor Comments:**\n   - Standardize notation between “distance transformed” vs. “DT scores.”  \n   - Clean formatting artifacts deriving from LaTeX typesetting.  \n   - Clarify in Section 2.4 the computational complexity and memory load of the distance transform step for large 3D data.  \n   - Verify consistent referencing of equations (some missing parentheses).  \n\n6. **Overall Recommendation:** **Major Revision**  \n   The paper is within IEEE TMI’s scope and presents novel, rigorous methodology. However, additional benchmarking, clearer positioning with existing uncertainty quantification methods, and enhanced discussion of clinical implications are needed before publication.\n\n7. **Confidence Level:** 5 (high confidence in technical and contextual assessment)\n\n---\n\n**Summary Statement to the Editor:**  \nThis manuscript introduces a statistically grounded and experimentally validated framework for conformal uncertainty quantification in medical image segmentation. It contributes to methodological advances that fit squarely within TMI’s mission of rigorous imaging science. With expanded comparative analysis and minor editorial improvements, it would constitute a strong technical paper for the journal.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a statistically principled method for uncertainty quantification in medical image segmentation using *conformal confidence sets*. The work adapts conformal prediction to structured image outputs, producing inner and outer confidence sets that provide distribution-free coverage guarantees of true segmentation masks. The paper is clearly written, technically rigorous, and closely aligned with *IEEE TMI*’s emphasis on quantitative imaging methodology. It focuses on uncertainty characterization rather than raw segmentation accuracy, providing a well-motivated methodological advance grounded in statistical learning theory.\n\n---\n\n**Major Comments**  \n1. **Positioning and Prior Work:** The manuscript would benefit from an expanded comparison to related conformal inference and Bayesian segmentation studies beyond current citations (e.g., existing probabilistic or conformal risk control frameworks). This would help clarify novelty and contextual placement.  \n2. **Comparative Evaluation:** While theoretical guarantees are sound, empirical comparisons are limited. Incorporating quantitative benchmarks (e.g., Expected Calibration Error, coverage–efficiency trade-offs) against Bayesian or evidential deep learning baselines would strengthen claims of superiority.  \n3. **Generalization to 3D Data:** The present framework assumes exchangeability across discrete voxels or slices. Discussion of how the method could extend to fully 3D volumetric settings, where spatial correlations are stronger, would enhance robustness.  \n4. **Clinical Interpretability:** Confidence sets are conceptually intuitive, but the paper could expand on how such representations might aid clinical review or decision support, perhaps through examples or user studies.  \n5. **Technical Details:** In Section 2.4, the computational complexity and memory usage of the distance transform step for large datasets should be clarified.\n\n---\n\n**Minor Comments**  \n- Standardize terminology between “distance-transformed scores” and “DT scores.”  \n- Correct minor LaTeX artifacts (e.g., inconsistent capitalization such as “I LLUSTRATING”).  \n- Ensure consistent equation numbering and formatting of references.  \n- Verify availability of MATLAB code and datasets upon acceptance to support reproducibility.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThis work is significant in bringing conformal inference to spatially structured medical imaging. Its innovation lies in extending formal coverage guarantees to pixel- and voxel-level segmentation through adaptive score transformations. Evaluation is thorough across several imaging modalities, although limited in baseline comparisons. Reproducibility is commendable, supported by detailed methodology and stated code availability. The primary areas for improvement are expanded benchmarking, stronger contextualization within related literature, and clarification of the clinical relevance of confidence maps.\n\n---\n\n**Decision Recommendation: Major Revision**  \nThe manuscript is methodologically strong and well aligned with *TMI*’s scope. However, it requires additional comparative evaluation, deeper positioning relative to existing uncertainty quantification techniques, and improved discussion of practical implications before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a novel methodological contribution to biomedical image analysis by utilizing statistical learning theory to develop *conformal confidence sets* for image segmentation tasks. It focuses on uncertainty quantification rather than just base segmentation accuracy, aligning well with the aims of IEEE Transactions on Medical Imaging (TMI) in promoting rigorous quantitative imaging methodologies. The authors introduce significant refinements to conformal prediction applicable at the pixel/voxel level, marking a noteworthy advance in the field.\n\n**Major Comments:**\n1. **Positioning vs. Prior Work:** The authors should strengthen the comparison with existing conformal risk control and Bayesian segmentation methods beyond those cited, including a broader literature review.\n2. **Generality:** Discussion on possible extensions to 3D volumetric datasets and correlated slices is needed, as the current theoretical framework focuses on discrete voxel sets.\n3. **Quantitative Comparison:** A table comparing uncertainty calibration metrics (e.g., Expected Calibration Error, coverage–efficiency trade-offs) against established uncertainty quantifiers should be included.\n4. **Clinical Interpretability:** A concise analysis of how clinicians can interpret confidence set overlays would enhance the manuscript's impact.\n\n**Minor Comments:**\n- Standardize terminology between “distance transformed” and “DT scores.”\n- Address formatting issues stemming from LaTeX typesetting.\n- Clarify the computational complexity and memory requirements of the distance transform step in Section 2.4, particularly for large 3D datasets.\n- Ensure consistent referencing of equations, correcting any missing parentheses.\n\n**Summary Paragraph:**  \nThe authors propose a robust framework for uncertainty quantification in medical image segmentation through *conformal confidence sets*. The methodology shows significant innovation by adapting conformal prediction for spatially structured outputs, achieving probabilistic coverage with practical applications demonstrated across diverse medical imaging domains. While the work's strengths lie in its solid statistical foundation, strong empirical validation, and transparent discussion of precision versus confidence, the weaknesses include limited comparison with other uncertainty methods and a brief exploration of clinical integrations. Overall, the paper meets TMI’s criteria for significance, innovation, evaluation, and reproducibility, albeit with areas requiring enhancement.\n\n**Decision Recommendation:** **Major Revision**  \nThe manuscript is relevant to the scope of IEEE TMI and offers a novel and rigorous methodology. However, further benchmarking, improved positioning regarding existing methods, and additional discussion on clinical utility are necessary prior to acceptance.\n\n**Confidence Level:** 5 (high confidence in technical and contextual assessment)",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Samuel Davenport"
    ],
    "url": "pdfs/iclr.cc-2025-conference_1bdddb81fd08be607c052dee4379dfe48532b252.pdf",
    "remote_url": "https://openreview.net/pdf/1bdddb81fd08be607c052dee4379dfe48532b252.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Is classification all you need for radiology report generation?",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "other topics in machine learning (i.e., none of the above)"
    ],
    "keywords": [
      "radiology report generation",
      "large language model",
      "multi-modalities"
    ],
    "abstract": "Automatic radiology report generation is an advanced medical assistive technology capable of producing coherent reports based on medical images, akin to a radiologist. However, current generative methods exhibit a notable gap in clinical metrics when compared to medical image classification. Recently, leveraging diagnostic results to improve report quality has emerged as a promising approach. We are curious whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. To investigate this question, this study designs an evaluation framework that integrates long-tail scenarios and summarizes potential combinations of LLM-based report generation models. We assess the impact of classification on report quality across four benchmarks. Initially, we introduce LLM-based language and clinical metrics and develop a pipeline to evaluate the model's performance on both in-domain and out-of-distribution (OOD) long-tail scenarios. Subsequently, we conduct a systematic evaluation of all potential model combinations. Our findings reveal that: 1) the impact of classification on report quality is positively correlated with the performance of classifiers, but the gap still exists, and 2) while classification can enhance report quality in in-domain long-tail scenarios, its benefits for OOD scenarios are limited.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nCurrent RRG systems exhibit a notable gap in clinical metrics when compared to classification models. The authors investigate whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. Key findings show that classification helps improve report quality in in-distribution settings but exhibits limited benefits in OOD scenarios.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper evaluates whether classification models can help improve the quality of radiology reports. This is a novel and interesting insight, with potential for aiding in real-world clinical workflows.\n2. The authors perform extensive evaluations across different strategies for incorporating classification information as well as in-domain vs. OOD settings. \n3. The authors also introduce novel LLM-based metrics for assessing the quality of radiology reports, which evaluate how well reports capture long-tail observations. This metric is an interesting contribution and has potential for aiding future works in the domain of radiology report generation.\n\n### Weaknesses\n\nThe key weakness of this paper is inadequate evaluations, as discussed below.\n\n1. **Inadequate evaluations of classifier:** The incorporation of the classifier is inadequately evaluated, making it difficult to understand the settings in which incorporating a classifier is useful, as detailed below. The paper could have benefitted from more nuanced insights on when including the classifier helps vs. detracts from the quality of generated reports.\n\n    a. **Classifier Performance:** Although the incorporation of the classifier is the key contribution of this paper, the authors do not provide any evaluations on the quality of the actual classifier. What conditions does the classifier perform well on and which conditions does the classifier perform worse on? Does this correlate with report generation performance on the subset of images with particular conditions when the classifier is included? Does improving the performance of the classifier improve report generation quality? In OOD settings, the evaluated classifier displays poor performance  (12.0-14.0 F1 points), and as a result, it is not surprising that incorporating the classifier will not result in benefits to the RRG model in OOD settings; does this result change if the classifier is instead trained on the PadChest or IU X-ray datasets? All of these questions are critical for understanding when/how classification helps in RRG, but none of these are evaluated.\n\n    b. **Upper Bound:** It would be useful to establish an upper bound on performance of the report generation model by utilizing an “oracle” classifier (i.e. a hypothetical classifier that predicts every condition correctly). This would establish whether classification, in the optimal setting, makes useful contributions to the report generation task. As of now, the evaluations are limited by the poor performance of the evaluated classifier, and the key findings/takeaways from the paper are based on this specific classifier.\n\n2. **Inadequate evaluations of proposed metrics:** Although several new metrics are presented as key contributions of this paper, the quality of these metrics is not evaluated. How accurate is the OpenAI GPT-4o model at extracting disease categories? Did the authors check for false negatives / positives in this extraction procedure? Does this metric align with ground-truth labels for the conditions where labels are provided? Evaluating the quality of the metrics via a dataset like ReXVal would have been useful [1].\n\n[1] https://physionet.org/content/rexval-dataset/1.0.0/\n\n### Questions\n\nMy questions are detailed above in the weaknesses section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript investigates the gap in clinical metrics between current Radiology Report Generation (RRG) systems and classification models. The authors explore whether including a classifier for long-tailed and rare diseases can enhance report robustness. Key findings indicate that while classification improves report quality in in-distribution settings, its benefits are limited in out-of-distribution (OOD) scenarios. The manuscript presents a compelling inquiry into the potential integration of classification in clinical workflows.\n\n### Major Comments\n1. **Inadequate Evaluations of Classifier**: The paper lacks thorough evaluation of the classifier's performance, limiting the understanding of its utility in improving report quality. Detailed insights on when classifier inclusion aids or hinders report generation are missing.\n   - **Classifier Performance**: No evaluations on the classifier's effectiveness are provided. Specific conditions for optimal classifier performance, correlation with report generation quality, and potential performance variations based on dataset choice (PadChest or IU X-ray) remain unexamined.\n   - **Upper Bound**: Establishing an upper performance limit using an \"oracle\" classifier could clarify the potential contributions of classification in report generation. The findings are hindered by low performance results from the assessed classifier.\n\n2. **Inadequate Evaluations of Proposed Metrics**: The manuscript introduces several new metrics without assessing their quality. Key concerns include the accuracy of the OpenAI GPT-4o model in disease category extraction, as well as the presence of false positives/negatives. A quality evaluation against a dataset like ReXVal would have been valuable.\n\n### Minor Comments\n- Consider improving clarity in the distinction between in-domain and OOD results.\n- There are minor grammatical errors and inconsistencies in terminology that should be addressed for readability.\n- Figures should be better labeled to enhance understanding.\n\n### Summary Paragraph\nIn summary, the manuscript demonstrates significance through its evaluation of classification models in the context of RRG, showing innovative ideas in the introduction of LLM-based metrics. However, the evaluations of both the classifier and the proposed metrics lack depth, which affects the overall rigor and reproducibility of the findings. While the potential for impact on clinical workflows is evident, clearer demonstrations of performance and robustness are necessary.\n\n### Decision Recommendation\nMajor revision is recommended to address the outlined weaknesses and enhance the overall quality of the manuscript.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether incorporating a classification model that accounts for all possible long‑tailed and rare disease categories can improve the robustness and clinical utility of radiology report generation (RRG) systems. The study aims to address the existing performance gap between RRG systems and classification models in standardized clinical metrics. Overall, the work is clearly written, explores an interesting direction for improving clinical report generation, and introduces novel evaluation metrics based on large language models (LLMs). However, the empirical support for the central claims is limited, primarily due to insufficient evaluation of the classifier and the proposed metrics.\n\n---\n\n**Major Comments**  \n\n1. **Evaluation of Classifier Integration**  \n   The analysis of how and when incorporating the classifier benefits report generation is insufficient. The paper does not clarify the contexts in which classifier inclusion improves or degrades performance. A more nuanced evaluation would strengthen the conclusions.  \n\n   a. **Classifier Performance Assessment:** The manuscript lacks direct evaluation of the classifier’s quality. Key questions remain unanswered: under which conditions does the classifier perform well or poorly? How does this relate to report generation outcomes? In the out‑of‑distribution (OOD) setting, the classifier performs poorly (12–14 F1 points), which likely explains the lack of improvement when incorporated into the RRG model. It would be valuable to test alternative training datasets (e.g., PadChest or IU X‑Ray) to see if this behavior changes.  \n\n   b. **Upper‑Bound Comparison:** The study should estimate an upper bound using an oracle classifier (perfect predictions) to show whether, in principle, an accurate classifier could meaningfully enhance RRG. As it stands, the conclusions depend solely on a specific, low‑performing classifier.\n\n2. **Evaluation of Proposed Metrics**  \n   Although the LLM‑based metrics are an interesting contribution, their validity and reliability are untested. The manuscript does not assess how accurately GPT‑4o extracts disease categories or whether false positives/negatives occur. Comparison against datasets with verified labels (e.g., ReXVal) would help establish metric fidelity and interpretability.\n\n---\n\n**Minor Comments**  \n- Clarify acronyms (e.g., RRG, OOD) upon first use.  \n- Provide brief methodological details for the metric extraction process to support reproducibility.  \n- Include quantitative validation or inter‑metric correlation for the newly proposed metrics.  \n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** Addresses a meaningful problem in clinical AI—bridging classification and report generation.  \n**Innovation:** Moderately innovative in exploring classifier‑informed RRG and proposing LLM‑based metrics.  \n**Evaluation:** The empirical evidence is not yet robust; key analyses of classifier quality and metric validity are missing.  \n**Reproducibility:** Limited by absent details on classifier performance analysis and missing code/data validation checks.  \nOverall, the concept is promising, but the evaluation requires substantial strengthening.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper explores the use of classification models in enhancing radiology report generation, specifically examining if a classifier encompassing all possible radiological findings can improve the robustness of generated reports. The study finds that while classification models improve report quality in in-domain scenarios, their impact is limited in out-of-distribution (OOD) settings, especially for long-tail disease categories. The authors propose a comprehensive evaluation framework with novel LLM-based metrics and conduct experiments across multiple benchmarks, concluding that classification aids in in-domain accuracy but may introduce errors in OOD cases due to model biases.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe study tackles a critical challenge in medical AI—how to generate reliable and clinically relevant radiology reports that generalize well across diverse clinical scenarios. By showing that classification models can enhance in-domain performance while introducing potential errors in OOD settings, this paper offers valuable insights into the limitations and trade-offs of using classification-aided generation models. The paper is well-structured, with each section building logically upon the previous one. The problem formulation, methodology, and experimental setup are clearly articulated, enhancing accessibility for readers. The experimental design is thorough, incorporating multiple benchmarks and a diverse set of evaluation metrics, including novel LLM-based metrics, to assess model performance across both in-domain and OOD settings. The rigorous use of benchmarks and detailed comparisons with existing methods underscore the study's robustness and the reliability of its findings.\n\n### Weaknesses\n\nThe primary weakness of this work lies in its motivation, or key argument, which has already been discussed in a prior publication that is not referenced in this submission. The previous study, Medical Report Generation Is A Multi-label Classification Problem (Fan et al., IEEE MedAI 2024), introduces a similar concept: that classification accuracy significantly impacts the quality of medical report generation. Both works share the same foundational idea, though they differ in specific model designs and classification categories. From a performance perspective, the previous work, which uses a cross-attention approach, demonstrates better outcomes and introduces the concept of ceiling performance. This concept suggests that the theoretical upper bound for this approach would be achieved if all ground-truth labels were provided to the model, given that real-world classifiers cannot reach 100% accuracy. \n\nWhile this submission’s strength lies in its extensive experiments, broader metrics, and a variety of models, its core insights are similar to the prior work, without addressing ceiling performance or providing a comparison to it. Additionally, the reason why the 28 classification categories used in the previous study outperform disease-based categories is not examined here. These points, along with further experimental and discussion-based additions, would significantly enhance the paper.\n\n### Questions\n\nplease read the weakness part.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript investigates the role of classification models in enhancing the generation of radiology reports, focusing on whether a classifier that includes all potential radiological findings can improve report robustness. The authors find that while classification models generally improve report quality in in-domain scenarios, their effectiveness diminishes in out-of-distribution (OOD) settings, particularly for long-tail disease categories. They propose a comprehensive evaluation framework utilizing novel LLM-based metrics, conducting experiments across various benchmarks. Ultimately, the study concludes that classification aids in in-domain accuracy but may introduce errors in OOD contexts.\n\n### Major Comments\n1. **Novelty**: The work's primary argument closely mirrors a previous study (Fan et al., IEEE MedAI 2024) without sufficient acknowledgment.\n2. **Evaluation Design**: Although the paper provides extensive experiments and multiple evaluation metrics, it fails to discuss ceiling performance, which could offer deeper insights into classification accuracy's impact.\n3. **Comparisons**: The submission does not compare its findings with the previous study's results, which could strengthen its argument regarding the chosen classification categories.\n4. **Reproducibility**: While thorough in its methodology, more details about replicating experimental conditions would enhance reproducibility.\n\n### Minor Comments\n- The motivation section would benefit from additional context regarding previous literature.\n- Consider incorporating a direct comparison of the classification categories with disease-based categories.\n- Minor formatting issues were noted in Section 3; please ensure consistency across all sections.\n\n### Summary Paragraph\nIn terms of **significance**, this paper addresses a pressing concern in medical AI by exploring how to achieve reliable radiology report generation across varied clinical scenarios. Regarding **innovation**, it attempts to enhance classification model performance, though its contributions are largely derivative. For **evaluation**, the thoroughness of experimental design is commendable, yet the lack of comparisons to previous findings diminishes the results' impact. Finally, in terms of **reproducibility**, further clarity on experimental replication would be beneficial. Overall, while the study presents valuable findings regarding in-domain classifications, it falls short of providing novel insights or addressing critical previous work.\n\n### Decision Recommendation\nMajor revision is recommended to address the identified weaknesses, enhance comparisons with prior studies, and clarify the contributions to the field.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the role of classification models in improving automatic radiology report generation. Specifically, it evaluates whether incorporating classifiers that predict a comprehensive set of radiological findings enhances the robustness and accuracy of generated reports. The study reports that classification-aided models improve report quality within in-domain data but perform less effectively in out‑of‑distribution (OOD) scenarios, particularly for rare disease categories. The paper introduces a novel evaluation framework with large language model (LLM)-based metrics and conducts experiments across several benchmarks. Overall, the manuscript is clearly organized, methodologically sound, and offers a detailed empirical analysis.\n\n**Major Comments**  \n1. **Overlap with Prior Work:** The main conceptual contribution—framing medical report generation as a classification-based problem—has been presented previously (Fan et al., IEEE MedAI 2024). The current study does not cite or contrast itself with that work, despite addressing a similar premise.  \n2. **Novelty and Positioning:** Because the earlier publication already demonstrated the relationship between classification accuracy and report quality, the novelty here lies primarily in the empirical scope rather than conceptual innovation. The authors should explicitly acknowledge and differentiate their approach, clearly outlining advances beyond prior findings.  \n3. **Missing Comparison to Ceiling Performance:** The related work introduced the notion of ceiling performance (upper bound given perfect classification). This submission does not adopt or analyze that concept, leaving unclear how the proposed method positions itself relative to theoretical limits.  \n4. **Category Design and Analysis:** The paper uses disease-based classification categories but does not investigate why prior label‑set designs with 28 classification categories achieved better performance. An empirical or discussion‑level analysis of this discrepancy would strengthen interpretability.\n\n**Minor Comments**  \n- The manuscript presentation is clear, but referencing prior related work is incomplete.  \n- Terminology around “in‑domain” and “OOD” could be more precisely defined.  \n- Ensure all figures and metrics are described consistently across sections.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study is significant in highlighting the trade‑offs between classification‑aided accuracy and model generalization in radiology report generation. Its innovation lies in the comprehensive evaluation and introduction of LLM‑based metrics, though the conceptual basis closely parallels prior research. The evaluation is rigorous and reproducible, drawing on multiple benchmarks and transparent comparisons. However, the limited novelty and absence of direct comparisons to prior established frameworks reduce its overall impact.\n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.  \nSubstantial clarification of novelty, inclusion of comparisons to related work, and deeper analysis of category selection and theoretical performance bounds are required before the paper can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper studies the question: will a classifier enhance LLM-based report generation models' performance in long-tail OOD scenarios? The authors explore different architectures on the combination of vision encoders, classifiers, and LLM, and perform lots of experiments. They get the findings that classification can enhance report quality in in-domain long-tail scenarios but is bounded by the performance of the classifier.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n1. Many research focuses on  LLM-based report generation models, which is an important task for the medical AI domain, the question this paper investigated may provide some useful information for future research.  \n2. The research comprehensively evaluates different architectural combinations (vision encoders, classifiers, and LLMs) across multiple diverse datasets (MIMIC-CXR, CXR-LT, PadChest, and IU X-Ray).\n\n### Weaknesses\n\n1. The comparison between the four baseline models lacks analysis. The metrics give inconsistent results, how could the author get the results that C+V+LLM is better than V+LLM, and the comparison between Refining and C+V+LLM, giving statistical significance analysis of the results would be helpful.\n\n2. The poor performance of the 'Expanding' approach may be due to the prompt rather than inherent limitations.  I do not find details about the prompts used (apologize if I overlooked them). The 'Expanding' prompt should include standard medical report templates to prove a fair comparison.\n\n3. The classifier only provides binary disease presence information without crucial details like location and severity. This limitation makes hallucinations in LLM-generated reports inevitable when using classification-only inputs. Instead of comparing with these inherently limited training-free approaches, the paper would be more valuable if it explored different variants of end-to-end Vision + LLM full training. \n\n4. The paper's title 'Is classification all you need for radiology report generation?' raises a clear question, but fails to provide the answer. While the findings show that classification helps with in-domain cases but struggles with OOD scenarios, the paper leaves crucial questions unanswered: Is better classification the solution? Or should we focus on pursuing end-to-end training approaches? If the answer is ' we need better classification models', then everyone knows it.\n\n### Questions\n\n1.  Have you optimized the Expanding and Refining prompt? we can't tell whether the performance difference is due to this method not being good enough or the prompt being the problem.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript investigates whether classifiers can improve the performance of LLM-based report generation models in long-tail out-of-distribution (OOD) scenarios. The authors examine various architectures combining vision encoders, classifiers, and LLMs through extensive experimentation. They conclude that classification can enhance report quality in in-domain long-tail situations, although this improvement is limited by the classifier's performance.\n\n**Major Comments:**\n1. The comparison between the four baseline models lacks sufficient analysis, as the metrics yield inconsistent results. Clarifications on how the authors determined that C+V+LLM outperforms V+LLM, as well as a statistical significance analysis between Refining and C+V+LLM, would strengthen the findings.\n2. The poor performance of the 'Expanding' approach may stem from the prompt rather than its inherent limitations. The manuscript does not provide details on the prompts used; including standard medical report templates in the 'Expanding' prompt would enable a fair standard for comparison.\n3. The classifier's limitation to binary disease presence information fails to incorporate essential aspects like location and severity. This restriction may lead to hallucinations in LLM-generated reports. Exploring end-to-end Vision + LLM models with full training would add significant value compared to the classification-only inputs assessed in the paper.\n4. While the manuscript's title, \"Is classification all you need for radiology report generation?\" poses a relevant question, it does not provide a definitive answer. Although the findings indicate the utility of classification in in-domain cases, they do not adequately address whether improved classification is the solution or if end-to-end training should be prioritized.\n\n**Minor Comments:**\n- Clarify the optimization status of the Expanding and Refining prompts, as it is unclear whether observed performance differences result from the method's efficacy or prompt design.\n\n**Summary Paragraph:**\nIn summarizing the review based on the TMI editorial criteria, the manuscript is significant due to its exploration of LLM-based report generation in medical AI, and it presents innovative combinations of architectures. However, evaluation weaknesses arise from inadequate comparisons and lack of comprehensive analysis, particularly around prompts and classifier limitations. Reproducibility may also be impacted due to unclear details about methodology. The overall contribution is noteworthy, though it leaves several critical questions unresolved.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether integrating a classifier can enhance large language model (LLM)-based radiology report generation, particularly in long-tail and out-of-distribution (OOD) scenarios. The study systematically examines combinations of vision encoders, classifiers, and LLMs across multiple datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray). The paper is generally sound in concept and has value for ongoing research in medical AI, though clarity and analytical depth are somewhat limited.  \n\n**Major Comments**  \n1. **Comparative Analysis and Statistical Support:** The comparisons among the four baseline models are insufficiently analyzed. Reported metrics produce inconsistent outcomes, yet the authors conclude that C+V+LLM outperforms V+LLM. Statistical significance testing or clearer justification is needed for these claims, including comparisons between the Refining and C+V+LLM approaches.  \n2. **Prompt Design and Fairness of Comparison:** The weak performance of the “Expanding” approach may be influenced by prompt construction rather than model limitations. Details on the prompts used are missing. Incorporating standard medical report templates could improve fairness and reproducibility.  \n3. **Classifier Information Limitation:** The classifier provides only binary disease presence, omitting key elements such as lesion location or severity. This design risks hallucinations in generated reports. A more informative classifier or a shift toward end-to-end Vision + LLM training variants would strengthen the study.  \n4. **Unresolved Research Question:** The paper’s title implies a definitive exploration (“Is classification all you need for radiology report generation?”), yet conclusions remain inconclusive. The results show classification helps in-domain but not OOD; however, the manuscript does not sufficiently address whether better classifiers or alternative end-to-end strategies are the path forward.  \n\n**Minor Comments**  \n- Clarify the prompts used for “Expanding” and “Refining” to help interpret results.  \n- Improve presentation quality (organization, figure clarity, and conciseness).  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is significant in addressing an important medical AI challenge and provides extensive experiments across multiple datasets. Its innovation lies in systematically examining the role of classification in LLM-based report generation. However, limitations in analysis, prompting detail, and interpretability reduce its overall impact. While the evaluation is broad, reproducibility suffers from missing methodological specifics.  \n\n**Decision Recommendation**  \n**Major Revision** — Strengthen comparative analysis, clarify prompts and classifier design, and refine conclusions to align with the posed research question.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis study investigates whether incorporating classification information into radiology report generation models can improve their robustness, particularly for long-tail and out-of-distribution scenarios. The authors design an evaluation framework with four benchmarks (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray) and propose a model design space combining vision encoders, classifiers, and LLMs in various configurations. They introduce LLM-based evaluation metrics alongside traditional language and clinical metrics. The main findings reveal that while classification information improves report quality in in-domain scenarios, it provides limited benefits for out-of-distribution long-tail cases when using large foundation models like Llama 3.1 70B and GPT-4. The study demonstrates a persistent gap between classification and generation performance, with classification accuracy being approximately 20% higher than report generation accuracy (Figure 4, Page 14).\n\n## Weaknesses\n\n• **Insufficient Mathematical Rigor and Inconsistent Notation**\n  - The paper lacks formal mathematical formulations for the proposed model architectures, particularly the hybrid input combinations shown in Figure 1 (Page 3)\n  - No explicit loss functions or optimization objectives are provided for the different training paradigms (end-to-end vs. training-free approaches)\n  - The classification token integration mechanism is described informally without mathematical precision (Section 2.2, Page 3-4)\n\n• **Limited Experimental Design and Baseline Comparisons**\n  - Only four state-of-the-art baselines are compared (Table 1, Page 6), which is insufficient for a comprehensive evaluation\n  - The long-tail classifier evaluation is restricted to only 100 and 200 categories (Table 2, Page 6), lacking systematic analysis of category scaling effects\n  - Cross-dataset evaluation methodology is inconsistent, with different sample sizes (500 for PadChest, 756 for IU X-Ray) without justification (Section 2.3.1, Page 4)\n\n• **Questionable Evaluation Framework Validity**\n  - The proposed LLM-based metrics rely on GPT-4o for ground truth extraction (Section 2.3.2, Page 5) without validation against expert annotations\n  - RadJudge scores show inconsistent patterns across experiments (Tables 1-2, Pages 6-7) with no analysis of metric reliability\n  - The long-tail F1 metrics use different observation counts across datasets (26/99/82) making cross-dataset comparisons problematic\n\n• **Inadequate Analysis of Core Research Question**\n  - The central hypothesis \"is classification all you need\" is not systematically tested with appropriate ablation studies\n  - Case studies are limited to single examples (Figures 3 and 5, Pages 9 and 17) without statistical significance analysis\n  - The finding that classification helps in-domain but not OOD scenarios lacks theoretical explanation or mechanistic analysis\n\n• **Technical Implementation Concerns**\n  - Vision encoder selection rationale is not provided despite using different architectures (Rad-DINO, CLIP, DINOv2) with varying input resolutions\n  - The connector architecture is oversimplified (single MLP with GELU, Section 2.1, Page 3) without justification for this choice\n  - Training details are relegated to appendix with insufficient discussion of hyperparameter sensitivity\n\n## Suggestions for Improvement\n\n• **Enhance Mathematical Formalization**\n  - Provide explicit mathematical formulations for each model variant in the design space, including input concatenation and attention mechanisms\n  - Define formal loss functions for both classification and generation objectives in the hybrid training paradigm\n  - Introduce consistent notation throughout the paper and include a notation table for clarity\n\n• **Strengthen Experimental Rigor**\n  - Include at least 8-10 recent state-of-the-art baselines to provide comprehensive comparison context\n  - Conduct systematic ablation studies with multiple category scales (50, 100, 200, 500) to understand scaling effects\n  - Standardize evaluation protocols across all datasets with consistent sample sizes and statistical significance testing\n\n• **Validate Evaluation Methodology**\n  - Conduct inter-annotator agreement studies between LLM-based metrics and expert radiologist evaluations\n  - Analyze metric correlation patterns and provide reliability statistics for all proposed evaluation measures\n  - Normalize observation categories across datasets or provide mapping schemes to enable meaningful cross-dataset comparisons\n\n• **Deepen Core Analysis**\n  - Design controlled experiments that systematically vary classification information quality to test the central hypothesis\n  - Provide statistical analysis across multiple case studies (minimum 50 examples per dataset) with significance testing\n  - Include theoretical analysis or mechanistic studies explaining why classification benefits diminish in OOD scenarios\n\n• **Improve Technical Documentation**\n  - Justify architectural choices through comparative studies or ablation experiments on connector designs and vision encoder selection\n  - Move critical implementation details to the main paper and provide sensitivity analysis for key hyperparameters\n  - Include computational complexity analysis and runtime comparisons across different model configurations",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThis study examines the incorporation of classification information into radiology report generation models, aiming to enhance robustness in long-tail and out-of-distribution scenarios. The authors propose an evaluation framework encompassing four benchmarks (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray) and present a model design space that integrates vision encoders, classifiers, and large language models (LLMs) in various combinations. While the findings indicate that classification information enhances report quality within in-domain contexts, it offers marginal benefits for out-of-distribution long-tail instances when assessed with large foundation models like Llama 3.1 70B and GPT-4. A notable discrepancy between classification and generation performance, with classification accuracy being approximately 20% higher than report generation accuracy, is revealed.\n\n## Major Comments\n1. **Insufficient Mathematical Rigor and Inconsistent Notation**\n   - The manuscript lacks formal mathematical representations for the model architectures, particularly those shown in Figure 1.\n   - No clear loss functions or optimization objectives are described for the various training approaches.\n   - The integration mechanism for classification tokens lacks mathematical precision.\n\n2. **Limited Experimental Design and Baseline Comparisons**\n   - The evaluation compares only four state-of-the-art baselines, which is inadequate.\n   - Long-tail classifier evaluations are restricted to only 100 and 200 categories without analyzing category scaling effects.\n   - The methodology for cross-dataset evaluation shows inconsistencies in sample sizes.\n\n3. **Questionable Evaluation Framework Validity**\n   - The LLM-based metrics depend on unvalidated ground truth from GPT-4.\n   - RadJudge scores exhibit inconsistencies without reliability analysis.\n   - Long-tail F1 metrics utilize varying observation counts, complicating dataset comparisons.\n\n4. **Inadequate Analysis of Core Research Question**\n   - The central hypothesis is not tested with sufficient ablation studies.\n   - Case studies are limited and lack statistical analysis for significance.\n   - The results indicating classification helps in-domain but not OOD scenarios lack theoretical exploration.\n\n5. **Technical Implementation Concerns**\n   - Justification for the selection of vision encoders is missing.\n   - The connector architecture is oversimplified without rationale.\n   - Key training details are located in the appendix, lacking thorough hyperparameter discussion.\n\n## Minor Comments\n- Enhance mathematical formalizations for each model variant.\n- Increase the number of comparative baselines and conduct systematic ablation studies.\n- Standardize evaluation protocols across datasets.\n- Validate inter-annotator agreement for evaluation metrics.\n- Provide comprehensive sensitivity and computational analyses.\n\n## Summary Paragraph\nThe study presents significant work in integrating classification information into radiology report generation, demonstrating some innovation within in-domain contexts. However, it falls short regarding evaluation depth and reproducibility, with vague methodologies, inconsistent comparative analysis, and insufficient mathematical rigor undermining its contributions. The manuscript does establish some significance, but the lack of thorough testing and theoretical justification for findings in out-of-distribution scenarios warrants major revision.\n\n## Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript explores whether integrating classification signals into radiology report generation models improves robustness, particularly under long-tail and out-of-distribution circumstances. The study establishes an evaluation framework using four datasets (MIMIC‑CXR, CXR‑LT, PadChest, IU X‑Ray) and examines combinations of vision encoders, classifiers, and large language models (LLMs). It also introduces LLM‑based evaluation metrics alongside standard language and clinical measures. The findings suggest classification guidance enhances in‑domain performance but yields limited benefit for out‑of‑distribution scenarios, highlighting a persistent gap between classification and report‑generation accuracy. The paper is timely and relevant, but methodological and analytical shortcomings reduce its technical rigor and evidential strength.  \n\n**Major Comments**  \n1. **Mathematical Formalization and Notation** – The paper lacks formal mathematical descriptions of model architectures, loss functions, and optimization objectives. Key mechanisms such as classification token integration are described informally, and notation is inconsistent.  \n2. **Experimental Design and Baselines** – Only four baselines are included, limiting comparative depth. The long‑tail classification tasks use few categories, and cross‑dataset sample sizes vary without justification, questioning the evaluation’s comprehensiveness.  \n3. **Evaluation Framework Validity** – The LLM‑based metrics depend on GPT‑4o for ground truth extraction without validation against expert annotations. Reported metric inconsistencies and varying observation counts hinder cross‑dataset comparability and metric reliability.  \n4. **Analysis of Core Research Question** – The central hypothesis (“is classification all you need”) is insufficiently tested; ablations are minimal, and case studies are anecdotal with no statistical support or theoretical interpretation for OOD performance differences.  \n5. **Technical Implementation** – Architectural choices (vision encoders, connectors) lack justification, and training details are placed in the appendix without hyperparameter or sensitivity analysis.  \n\n**Minor Comments**  \n- Introduce a unified notation table for symbols and formulae.  \n- Clarify figure references (e.g., Figures 1, 3, 5) and maintain consistent dataset naming.  \n- Provide explicit definitions of evaluation metrics and ensure table titles are self‑contained.  \n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe topic is significant for making radiology report generation more robust, and the idea of integrating classifiers is conceptually interesting. However, innovation is limited by insufficient methodological formalization and incomplete exploration of the design space. Evaluation lacks statistical rigor, fair baseline breadth, and validated metrics, weakening evidential strength. Reproducibility is compromised by incomplete training details and inconsistent protocols. Addressing formalization, experimental consistency, and methodological transparency would markedly enhance the manuscript.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper offers a promising direction but requires substantial methodological clarification, expanded experiments, and validation of evaluation measures before being ready for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope &amp; Relevance to TMI**  \nThe paper studies automatic radiology report generation (ARRG), focusing on how classification information from medical image classifiers affects the quality and robustness of large‑language‑model (LLM)‑based report generators. It contributes an evaluation framework that integrates in‑domain and out‑of‑distribution (OOD) long‑tail scenarios and introduces LLM‑based metrics for both language and clinical quality. The subject—methodological evaluation and modeling strategy in medical imaging AI—fits the scope of *IEEE Transactions on Medical Imaging*.\n\n✅ **In scope**; continue with full review.\n\n**2. Novelty &amp; Contribution Level**  \nRather than introducing a new model architecture, the work is primarily an analytic and benchmarking study. The novelty lies in (i) designing an LLM‑based evaluation pipeline combining conventional and semantic metrics, and (ii) systematically analyzing combinations of vision encoders, classifiers, and LLMs under long‑tail conditions. While useful, this is an incremental synthesis rather than a breakthrough methodological innovation.\n\n⚠️ Moderate originality; contribution mainly comparative and evaluative.\n\n**3. Technical &amp; Experimental Rigor**  \nExperiments are described in detail across multiple datasets (MIMIC‑CXR, CXR‑LT, PadChest, IU X‑Ray) with appropriate split between in‑domain and OOD testing. Metrics include standard language and clinical measures plus LLM‑based scoring. The study appears reproducible—dataset sources are public, model backbones are named, and hyperparameters are stated. However:\n- Quantitative results are extensive but sometimes descriptive more than statistically analyzed.  \n- Some claims (e.g., “20 % accuracy gap”) are based on macro‑F1 without clear variance or confidence.  \n- Ethical approvals or data‑use statements are not explicitly noted, though datasets are public and de‑identified.\n\n**4. Clarity &amp; Presentation**  \nThe manuscript is generally readable, following ICLR formatting. Figures and tables are plentiful; however, language flow is somewhat awkward and could benefit from professional editing. Some redundancy exists between abstract, methods, and discussion. Minor grammar and figure‑caption polishing are needed.\n\n**5. Ethical and Reproducibility Compliance**  \nUse of open datasets (MIMIC‑CXR, PadChest, IU X‑Ray) likely meets de‑identification standards. Code‑availability and IRB statements are not provided and should be added. No patient‑level data exposure is evident.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe manuscript investigates whether classification accuracy and representation can fully explain radiology report generation quality. It constructs an evaluation framework with long‑tail and OOD conditions, integrates two new LLM‑based metrics (semantic language‑level and automatic clinical extraction), and explores multiple design paradigms combining vision encoders, classifiers, and LLMs. Experiments on four datasets show that classifier information improves report quality in‑domain but adds little or negative value for OOD cases, highlighting limits of classifier‑driven generation.\n\n**2. Strengths**\n- Addresses an important methodological question bridging computer vision and language modeling in radiology.  \n- Proposes a unified benchmarking and metric framework that could benefit the community.  \n- Conducts extensive experiments covering diverse datasets and model scales.  \n- Offers clear insights into the failure modes of classifier‑assisted generation.\n\n**3. Weaknesses**\n- Methodological novelty is modest; the work functions more as an empirical study than a new algorithmic contribution.  \n- Absence of formal statistical tests and uncertainty estimates limits rigor.  \n- Ethical/compliance details and code‑release policy are missing.  \n- Writing requires tightening; figures and tables need improved captioning and integration.\n\n**4. Major Comments**\n1. **Position within TMI scope** – Clarify whether the key contribution is the LLM‑based evaluation metric (methodological) or the empirical observation of classifier influence (analytical). If mainly benchmarking, articulate how this advances imaging methodology beyond existing metrics such as RadEval or RadCliQ.  \n2. **Quantitative confidence** – Provide standard deviations or significance testing across runs; otherwise, trends may reflect random noise.  \n3. **Metric validation** – Describe how the proposed LLM‑based metrics correlate with human expert ratings; otherwise reproducibility of the metric itself remains uncertain.  \n4. **Ethics &amp; data governance** – Explicitly state dataset licenses, anonymization assurances, and whether any institutional approval was required.  \n5. **Clarity and structure** – Condense overlapping text; ensure all acronyms and figure references are defined. Improve English grammar for future submission rounds.\n\n**5. Minor Comments**\n- Correct minor grammatical errors (“aim to conducts → aims to conduct,” etc.).  \n- Standardize abbreviation formatting (e.g., “LLM‑based” vs “LLM based”).  \n- Ensure figure legends explain color coding and symbols.  \n- Include references to code repository if available.  \n- Move long tables (A–E) to supplement; provide concise summary tables in main text.\n\n**6. Overall Recommendation**  \n**Reject (out of scope / low novelty)** for *IEEE TMI* — While technically competent, the paper’s main value lies in benchmarking and empirical analysis rather than in a methodological advance in imaging science. It may be more suitable for an application‑oriented or NLP‑focused venue (e.g., ICLR, MIDL). To reach TMI standards, the authors should (i) formalize their LLM‑based metric as a validated methodological contribution, (ii) strengthen statistical evaluation, and (iii) demonstrate generalizable improvements to medical‑imaging model design.\n\n**7. Confidence Level:** 4 / 5 (High)",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates automatic radiology report generation (ARRG) and specifically examines how classification information from medical image classifiers influences the quality and robustness of large language model (LLM)–based report generators. It contributes an evaluation framework that integrates in‑domain and out‑of‑distribution (OOD) long‑tail scenarios and introduces LLM‑based metrics for language and clinical quality. The work is technically competent, clearly written overall, and suitable in scope for *IEEE Transactions on Medical Imaging*, although its contribution is primarily analytical and benchmarking rather than methodological.  \n\n**Major Comments**  \n1. **Clarification of contribution:** The paper should explicitly state whether its main novelty lies in the LLM‑based evaluation metrics or in the empirical analysis of classifier representations. If primarily a benchmarking effort, the authors should outline how it advances existing metrics such as RadEval or RadCliQ.  \n2. **Statistical rigor:** Quantitative comparisons lack uncertainty measures. Include standard deviations or significance testing to demonstrate that observed accuracy gaps are statistically meaningful.  \n3. **Metric validation:** The proposed LLM‑based metrics should be validated against human expert ratings to confirm reproducibility and interpretability.  \n4. **Ethical and compliance details:** Provide explicit dataset license, anonymization, and institutional review statements, even if datasets are public and de‑identified.  \n5. **Writing and structure:** Improve English expression and reduce redundancy across abstract, methods, and discussion. Ensure all acronyms and figure references are clearly defined.  \n\n**Minor Comments**  \n- Correct minor grammatical issues (e.g., “aim to conducts → aims to conduct”).  \n- Standardize abbreviation formatting (“LLM‑based,” “OOD”).  \n- Refine figure legends for color coding and symbols; move lengthy tables to supplementary materials and provide concise summaries in the main text.  \n- Reference any available code repository to support reproducibility.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe study is significant in addressing a timely methodological issue bridging vision and language modeling in radiology, offering an organized experimental benchmark and new evaluation metrics. However, its innovation is moderate, emphasizing empirical synthesis rather than new algorithmic methods. Evaluation is thorough across multiple datasets, but statistical analysis and validation of the proposed metrics require reinforcement to meet TMI rigor standards. Reproducibility is generally feasible given public datasets and detailed configurations, though the absence of code and formal ethical statements limits transparency.  \n\n**Decision Recommendation**  \n**Reject (low novelty / benchmarking focus).** The manuscript provides valuable analysis but lacks a clear methodological advance suitable for *IEEE TMI*. Strengthening statistical validation, formalizing the metric’s methodological contribution, and clarifying ethical and reproducibility statements are required for reconsideration.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript investigates automatic radiology report generation (ARRG), emphasizing the influence of classification information from medical image classifiers on the quality and robustness of large-language-model (LLM)-based report generators. It contributes an evaluation framework that merges in-domain and out-of-distribution (OOD) scenarios while introducing LLM-based metrics for both language and clinical quality. The focus on methodological evaluation in medical imaging AI aligns well with the scope of *IEEE Transactions on Medical Imaging*.\n\n**Major Comments:**\n1. **Position within TMI scope:** Clarify if the core contribution is the LLM-based evaluation metric (methodological) or the empirical observation of classifier influence (analytical). If primarily benchmarking, highlight how this progresses imaging methodology compared to existing metrics like RadEval or RadCliQ.\n2. **Quantitative confidence:** Provide standard deviations or significance testing across runs; otherwise, trends may represent random noise rather than significant differences.\n3. **Metric validation:** Articulate how the proposed LLM-based metrics correlate with human expert ratings, as the reproducibility of the metric itself remains uncertain.\n4. **Ethics & data governance:** Specify dataset licenses, anonymization assurances, and whether any institutional approvals were necessary.\n5. **Clarity and structure:** Streamline overlapping text; ensure all acronyms and figure references are clearly defined and improve English grammar for clarity.\n\n**Minor Comments:**\n- Correct minor grammatical errors (e.g., \"aim to conducts\" should be \"aims to conduct\").\n- Standardize abbreviation formatting (e.g., ensuring uniformity in LLM-based).\n- Ensure figure legends adequately explain color coding and symbols.\n- Reference a code repository if available.\n- Relocate long tables (A–E) to supplementary materials and summarize key data in the main text.\n\n**Summary Paragraph:**  \nThe manuscript evaluates whether classification accuracy and representation can fully account for the quality of radiology report generation. It builds an evaluative framework incorporating long-tail and OOD scenarios, introducing new LLM-based metrics while examining various design paradigms involving vision encoders, classifiers, and LLMs. Its findings reveal that classifier information enhances report quality in in-domain scenarios but marginally detracts from performance in OOD cases, illustrating the limitations of classifier-driven methodologies. While the study rigorously conducts experiments across multiple datasets, the lack of formal statistical testing and ethical transparency hinders its overall rigor.\n\n**Decision Recommendation:**  \n**Reject (out of scope / low novelty)** for *IEEE TMI*. Although technically competent, the manuscript's primary significance lies in benchmarking and empirical analysis rather than methodological advancement in imaging science. It might be better suited for an application-oriented or NLP-focused venue (e.g., ICLR, MIDL). To achieve TMI standards, the authors should (i) formalize their LLM-based metric as a validated methodological contribution, (ii) enhance statistical evaluations, and (iii) demonstrate generalized improvements in medical-imaging model design.\n\n**Confidence Level:** 4 / 5 (High)",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Bo Wang",
      "Dongsheng Li",
      "Guanzhou Ke",
      "XINYANG JIANG",
      "Xiaoli Wang",
      "Xufang Luo",
      "Yang Yu",
      "Yifan Yang",
      "Zilong Wang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_d7f9b710e10bdd36261f817262f60d57d47921bf.pdf",
    "remote_url": "https://openreview.net/pdf/d7f9b710e10bdd36261f817262f60d57d47921bf.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "SeLoRA: Self-Expanding Low-Rank Adaptation of Latent Diffusion Model for Medical Image Synthesis",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Text-to-Image Synthesis",
      "Low-Rank Adaptation",
      "Medical Imaging",
      "Parameter Efficient Finetuning"
    ],
    "abstract": "The persistent challenge of medical image synthesis posed by the scarcity of annotated data and the need to synthesize \"missing modalities\" for multi-modal analysis, underscored the imperative development of effective synthesis methods. Recently, the combination of Low-Rank Adaptation (*LoRA*) with latent diffusion models (LDMs) has emerged as a viable approach for efficiently adapting pre-trained large language models, in the medical field. However, the direct application of *LoRA* assumes uniform ranking across all linear layers, overlooking the significance of different weight matrices, and leading to sub-optimal outcomes. Prior works on *LoRA* prioritize the reduction of trainable parameters, and there exists an opportunity to further tailor this adaptation process to the intricate demands of medical image synthesis. In response, we present *SeLoRA*, a Self-Expanding Low-Rank Adaptation module, that dynamically expands its ranking across layers during training, strategically placing additional ranks on crucial layers, to allow the model to elevate synthesis quality where it matters most. Our analysis shows that *SeLoRA* strikes the best balance between synthesis quality and training efficiency. The proposed method not only enables LDMs to fine-tune on medical data efficiently but also empowers the model to achieve improved image quality with minimal ranking. The code of our *SeLoRA* method is publicly available at https://anonymous.4open.science/r/SeLoRA-980D.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a new parameter-efficient fine-tuning method SeLoRA (Self-Expanding Low-Rank Adaptation) for adapting Stable Diffusion to generate chest x-ray images. The main contribution of the paper lies in dynamically expanding the rank of LoRA during the training process, allowing it to adapt the rank according to the importance of different layers and thereby improving the quality of the synthesized images. The novelty of this approach is in using Fisher Information to guide the rank expansion, avoiding the limitations of the traditional LoRA method, which uses a \"uniform rank,\" especially when dealing with models (like Stable Diffusion) that have diverse weight matrix shapes. The paper demonstrates the effectiveness of SeLoRA on two chest x-ray datasets and provides detailed comparative experiments with other LoRA variants.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. \nThe papers proposes SeLoRA, a dynamic rank-expanding method using Fisher Information to guide rank expansion during fine-tuning large models with LoRA. This is novel and more applicable to models with diverse weight matrix shapes.\n2.\nExperimental results demonstrate the effectiveness of the proposed method.\n3. \nThe paper is well-organized.\n\n### Weaknesses\n\n1.\nThe contribution is vague. As a paper focusing on adaptive parameter efficient fine-tuning methods, the paper utilizes LoRA to adapt Stable Diffusion for Chest X-ray synthesis, limiting its technical contribution. For a paper dedicated to adapting foundation model like Stable Diffusion for Chest X-ray (medical image) synthesis, the exploration is also limited and does not compare with previous work (e.g. Chambon et al., 2022a;b). Through visual comparison with image displays in Chambon et al., 2022a;b, the proposed method seems at a disadvantage.\n2.\nExperiments are conducted on relatively small datasets. Large image-report paired Chest X-ray datasets exist (e.g. MIMIC-CXR). Is this because of the heavy burden of large model like Stable Diffusion, or may be also related to the proposed method? Can the authors provide training time comparisons between SeLoRA and the compared methods? Also, as the test set of IU X-RAY and Montgomery County CXR dataset has only contains 100~200 images, the validation of the effectiveness of the method is weak.\n3.\nEvaluation and explanation are insufficient. Using a CLIP model trained purely on natural images and a maximum text token length of 76 to compute CLIP-score may not faithfully reflect how good the text-image alignment is for Chest x-ray images.\nThe training/validation/testing split is strange, could the author explain why the test set only contain 4% of the data? Are Table 1 values computed with valid data or test data? This is unclear. The paper also lacks in-depth discussion of the distribution of final rank (Figure 4,5,6) and why other LoRA methods fail on Montgomery county CXR data (Figure7). How accurate can the model generate an image given the disease or abnormal findings in the text prompt? This may be revealed using pretrained Chest X-ray classification models or manually inspect a small subset of generated results.\n\nOther non-important issues:\n4.\nthe paper title is about \"medical image syntheis”; but it only focuses on chest x-ray image.\n5.\nThe formula derivation in section3.3 is unclear.\n6.\nThe Stable Diffusion model is trained to generate a resolution of 512x512, using a resolution of 224x224 may limit the performance.\n\n### Questions\n\nPlease answer the questions mentioned in the above 'weakness' section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript presents a novel parameter-efficient fine-tuning technique named SeLoRA (Self-Expanding Low-Rank Adaptation) designed for adapting Stable Diffusion to generate chest X-ray images. The primary innovation lies in the dynamic expansion of the rank of LoRA during training, guided by Fisher Information, which enhances the model's ability to generate high-quality images. The paper claims to outperform traditional LoRA methods that utilize a uniform rank, particularly in the context of models with diverse weight matrix shapes. The authors demonstrate the effectiveness of SeLoRA through experiments on two datasets, alongside a detailed comparison with existing LoRA variants.\n\n### Major Comments\n1. The contribution of the paper is unclear. While it aims to adapt Stable Diffusion for chest X-ray synthesis using LoRA, it lacks depth in exploration and comparison with previous works (e.g., Chambon et al., 2022a; 2022b), where the proposed method appears less effective based on visual evaluations.\n2. The experiments utilize relatively small datasets, raising questions about the implications on model performance. Larger datasets like MIMIC-CXR could be investigated. A comparison of training times between SeLoRA and alternative methods is warranted. The test sets from IU X-RAY and Montgomery County datasets, containing only 100-200 images, weaken the validation of the findings.\n3. Evaluation criteria and rationale need strengthening. Utilizing a CLIP model trained exclusively on natural images and with a fixed text token length of 76 for score computation may not accurately reflect text-image alignment effectiveness for chest X-ray images. Clarification on the unusual training/validation/testing splits is needed, particularly why the test set represents only 4% of the data. Furthermore, it's unclear whether Table 1 values were drawn from validation or test data. More detailed discussions on rank distribution (Figures 4, 5, 6) and the performance of other LoRA methods on the Montgomery County dataset (Figure 7) would enhance transparency. Lastly, insights on how accurately the model can generate images based on disease descriptions in prompts should be explored.\n\n### Minor Comments\n1. The title suggests a focus on \"medical image synthesis,\" yet concentrates solely on chest X-ray images.\n2. The formula derivation in Section 3.3 could be more clearly articulated.\n3. The original Stable Diffusion model targets a resolution of 512x512; using 224x224 may hinder performance.\n\n### Summary Paragraph\nThis study addresses significant issues in medical image generation, presenting a potentially innovative but somewhat narrowly focused approach. While the novelty of the rank expansion tied to Fisher Information is commendable, the contributions appear limited by inadequate comparisons and the use of small datasets. Evaluation methodologies require enhancement to establish robustness and reproducibility, especially in assessing the generated image alignments with respect to the medical application. Overall, while the manuscript demonstrates value in its proposed method, the gaps in evaluation and context necessitate further work.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *SeLoRA* (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning method designed to adapt Stable Diffusion for generating chest X-ray images. The central idea is to dynamically expand the LoRA rank during training, guided by Fisher Information, thus adapting model capacity to layer importance and improving output quality. Experiments on two chest X-ray datasets are provided to demonstrate the method’s effectiveness. Overall, the paper is well-organized and presents an innovative concept, although its technical and experimental contributions are somewhat limited.\n\n---\n\n**Major Comments**  \n1. **Scope and Contribution** – The paper’s contribution appears vague. While it proposes an adaptive LoRA method, the technical advance beyond existing LoRA-based fine-tuning approaches is modest. Furthermore, its application is limited to Stable Diffusion-based chest X-ray synthesis, with little exploration beyond this narrow setting. Comparisons with prior related works (e.g., Chambon et al., 2022a,b) are missing, and qualitative inspection suggests the proposed method may be at a disadvantage relative to these baselines.  \n2. **Experimental Limitations** – The datasets used are small (IU X-Ray, Montgomery County CXR), weakening the empirical validation. Larger chest X-ray datasets such as MIMIC-CXR are not utilized. Clarification is needed on whether dataset choice was due to computational constraints or the proposed method itself. Training-time comparisons with other LoRA variants would be valuable. Given the small test sets (100–200 images), the evidence for effectiveness is weak.  \n3. **Evaluation Clarity and Methodology** – Evaluation choices and explanations are insufficient. The CLIP model used for text–image alignment was trained on natural images and may not appropriately assess medical imagery; the 76-token limit further restricts evaluation fidelity. The data split is also unclear (only 4% test data). It should be specified whether results in Table 1 use validation or test data. Figures 4–7 require better discussion—especially regarding rank distribution and why some LoRA methods fail on specific datasets. Additionally, alignment between generated images and textual disease descriptions could be assessed using pretrained X-ray classifiers or manual inspection.  \n4. **Additional Issues** –  \n   - The title refers to *medical image synthesis*, but the experiments focus exclusively on chest X-rays.  \n   - Derivation in Section 3.3 lacks clarity.  \n   - Reducing image resolution from 512×512 (native SD resolution) to 224×224 may constrain performance.  \n\n---\n\n**Minor Comments**  \n- Clarify formula presentation in Section 3.3.  \n- Ensure consistency in naming (e.g., \"Chest X-ray\" vs. \"chest x-ray\").  \n- Correct minor typographical and grammatical issues.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work introduces a novel idea of dynamically expanding LoRA rank using Fisher Information, contributing to innovation in parameter-efficient fine-tuning. However, its significance is limited by narrow application scope and small-scale validation. The evaluation lacks rigor and appropriate metrics for the medical imaging domain, impairing significance. Methodological details, dataset splits, and reproducibility aspects require clarification to strengthen confidence in the findings.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper present SeLoRA, a Self-Expanding Low-Rank Adaptation module, that dynamically expands its ranking across layers during training. The proposed method increases the rank from 1 gradually. FI-Ratio and parameter \\lambda were used to determine when to expanding the rank.  The experiment is performed with stable diffusion and X-ray dataset and synthesis the X-ray image with text prompt.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper introduces a parameter-efficient method to e fine-tune stable diffusion models for generating X-ray images based on text (radiology) prompts. And the proposed method can progressive expansion in the rank of LoRA. FI-Ratio is used to guiding SeLoRA to expand its rank. The rank of different layers was given. The experiment shows the result is promisingly.\n\n### Weaknesses\n\n1. Computational overhead. While SeLoRA reduces the number of trainable parameters, its dynamic rank expansion mechanism introduces additional computational complexity. Computing Fisher information increases the overhead, which may become significant for larger datasets or more complex models.\n2. Limited dataset evaluation. Experiments were limited to two 2D X-ray datasets, with no evaluation of SeLoRA’s performance on other modalities, such as MRI or CT scans. Further validation on additional modalities would help confirm the generalizability of the method.\n3. Visual result is limited. The visual result is not satisfactory, such as in Figure 8, the contrast and details are not good.\n\n### Questions\n\n1. Computational complexity analysis A comparison of training time, memory usage and FLOPs between SeLoRA, LoRA, and other variants is needed to quantify the computational trade-offs introduced by dynamic rank expansion.\n2. Evaluation on a wider range of datasets Evaluating SeLoRA on larger datasets, such as MIMIC-CXR (containing approximately 377,000 images), would provide more insights into its scalability. Future work could also validate SeLoRA on MRI, CT, or ultrasound datasets, as testing on diverse datasets would better demonstrate its robustness and versatility.\n3. Incorporating related work. The idea of dynamically adjusting the rank of the LoRA matrix in SeLoRA is conceptually similar to the recently proposed ALoRA (NAACL 2024). However, the two methods differ in implementation: ALoRA utilizes pruning and redistribution strategies, while SeLoRA relies on Fisher information as the adjustment criterion. Insights from ALoRA could provide valuable inspiration for future improvements of SeLoRA.\n4. Evaluation with medical doctor may help verify the experiment results.\nOther questions\n1. Unconventional split of the IU X-Ray dataset. The 80:16:4 split results in a relatively small test set, which could compromise the robustness of the evaluation. A more conventional split (e.g., 80:10:10) might provide more reliable insights.\n2. Small sample size in the Montgomery County CXR dataset. With only 138 samples, the Montgomery County dataset is too small for deep learning applications, which may impact the stability and generalizability of the model’s results\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents SeLoRA, a Self-Expanding Low-Rank Adaptation module that expands its rank dynamically across layers during training. This parameter-efficient method fine-tunes stable diffusion models to generate X-ray images based on text prompts. Overall, the clarity of the manuscript is adequate, though certain aspects, particularly computational implications and evaluation breadth, require attention.\n\n**Major Comments:**\n1. **Computational Overhead:** While SeLoRA reduces the number of trainable parameters, its dynamic rank expansion introduces additional computational complexity. The computation of Fisher information could significantly increase overhead for larger datasets or more complex models.\n2. **Limited Dataset Evaluation:** The experiments were restricted to two 2D X-ray datasets. Testing SeLoRA on other modalities, such as MRI or CT scans, will be essential to confirm the method's generalizability.\n3. **Limited Visual Results:** The visual output, particularly as shown in Figure 8, shows inadequate contrast and detail, raising concerns about the quality of generated images.\n\n**Minor Comments:** \n- A computational complexity analysis comparing training time, memory usage, and FLOPs among SeLoRA, LoRA, and other variants is necessary.\n- A broader evaluation on larger datasets like MIMIC-CXR, which contains about 377,000 images, would enhance understanding of scalability.\n- Insights from related work, particularly ALoRA, which differs in methodology, may inspire future improvements to SeLoRA.\n- Evaluating SeLoRA's performance with medical professionals could help verify results from the experiments.\n\n**Summary Paragraph:**  \nIn terms of significance, the manuscript addresses a notable method for generating X-ray images using text prompts, highlighting an innovative approach to low-rank adaptation. However, the evaluation is limited, and the method's reproducibility may be affected due to insufficient dataset diversity. Enhancing the visual quality of results and incorporating broader evaluation metrics will strengthen the study's contributions.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *SeLoRA (Self‑Expanding Low‑Rank Adaptation)*, a parameter‑efficient fine‑tuning module that dynamically increases the rank of adaptation layers during training. The rank expansion is guided by a Fisher Information (FI) ratio and a threshold parameter λ. The method is applied to stable diffusion models for generating X‑ray images from text prompts. Overall, the paper is clearly written and introduces a technically sound and conceptually interesting approach, though the experimental validation and computational analysis are limited.\n\n---\n\n**Major Comments**  \n1. **Computational Overhead:** While SeLoRA reduces the number of trainable parameters, the dynamic rank‑expansion process introduces added computational burden, particularly from Fisher Information computation. The associated overhead should be quantitatively compared (e.g., training time, memory, FLOPs) with existing LoRA variants.  \n2. **Limited Dataset Evaluation:** Experiments are restricted to two small 2D X‑ray datasets (IU X‑Ray and Montgomery County). The small test set in the 80:16:4 split and the very limited size of the Montgomery dataset undermine the robustness and generalizability of results. Validation on larger or multimodal datasets (e.g., MIMIC‑CXR, MRI, CT, ultrasound) would strengthen the claims.  \n3. **Quality of Visual Results:** The generated images exhibit limited visual quality—e.g., Figure 8 shows poor contrast and insufficient detail. More convincing visual comparisons or metrics are needed.  \n4. **Relation to Prior Work:** The concept of dynamically adjusting LoRA rank is related to the recently proposed ALoRA (NAACL 2024). A comparison or discussion highlighting the conceptual and implementation differences would clarify SeLoRA’s novelty.  \n5. **Clinical Evaluation:** Involving radiologists or medical experts in qualitative assessment would provide practical validation of image realism and diagnostic plausibility.\n\n---\n\n**Minor Comments**  \n- Clarify dataset split rationale and provide consistency with standard practice (e.g., 80/10/10).  \n- Correct minor grammatical issues (“e fine‑tune” → “fine‑tune,” “promisingly” → “promising”).  \n- Ensure all figures are high‑contrast and properly labeled with clear captions.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nThe paper is significant in proposing an efficient fine‑tuning strategy for diffusion models in medical imaging. It shows moderate innovation through adaptive rank expansion, though similar ideas exist in related work. Experimental evaluation is preliminary and limited in scope and scale, reducing the strength of evidence. Reproducibility appears feasible given the described method, but additional computational benchmarks and broader validation are necessary to confirm robustness.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The concept is promising, but the current evaluation, computational analysis, and discussion of related methods require substantive expansion before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces SeLoRA (Self-Expanding Low-Rank Adaptation), an extension of the LoRA (Low-Rank Adaptation) technique, designed for the fine-tuning of large diffusion models specifically in medical image synthesis. The core idea is to dynamically expand the rank of low-rank matrices during training, based on a criterion derived from Fisher information. This adaptation is applied selectively across different layers, allowing for a more effective distribution of ranks that aligns with each layer's significance in the model, particularly within the denoising U-Net of the Stable Diffusion framework.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper is well-organized and easy to follow.\n\n2. The idea of adaptive computation rank selection is interesting and highly relevant for using pre-trained models for downstream tasks in a memory efficient fashion\n\n### Weaknesses\n\n- While the paper mainly focusses on making a more efficient LoRA design as claimed by authors, there is no Analysis of Training Efficiency in the paper.\n\n- Lack of detailed explanation of the procedure of selecting hyper-parameters needed.\n\n### Questions\n\n- How is the proposed method compared to other baselines in terms of max training GPU memory, training speed, and\ntraining time costs? An analysis of these criteria would strengthen the paper.\n\n- The paper mentions thresholds (λ and t) for triggering rank expansion. How sensitive is the method to these hyper-parameters?\n\n- In figure 7, why are the other methods not generating similar xray images? As per my understanding, they should at-least make a similar image like figure 8. \n\n- The proposed method is just compared to methods with rank pruning methods (dylora and adalora), could you mention why there is no comparing with adaptive rank selection papers? as they seem to be a similar approach to your paper.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNo ethics review needed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents SeLoRA (Self-Expanding Low-Rank Adaptation), an extension of the Low-Rank Adaptation (LoRA) technique aimed at enhancing the fine-tuning of large diffusion models for medical image synthesis. The primary innovation involves the dynamic rank expansion of low-rank matrices during training, driven by a criterion based on Fisher information. This selective adaptation across different model layers enhances rank distribution according to the significance of each layer, particularly within the denoising U-Net of the Stable Diffusion framework. \n\n**Major Comments:**\n1. The manuscript lacks a detailed analysis of training efficiency, despite focusing on improving LoRA design.\n2. There is insufficient explanation regarding the selection of hyper-parameters needed for the proposed method.\n3. The comparison of the proposed method against other baselines concerning maximum training GPU memory, training speed, and time costs is missing. This comparison would substantiate claims of efficiency.\n4. The manuscript needs to address the sensitivity of the proposed method to the thresholds (λ and t) mentioned for triggering rank expansion.\n5. In Figure 7, the disparity in x-ray image generation among methods is unclear; one would expect some similarity to the images in Figure 8.\n6. The paper focuses on comparisons with rank pruning methods (e.g., DyLoRA and AdaLoRA), yet does not include comparisons with adaptive rank selection papers, which appear relevant and similar to the proposed approach.\n\n**Minor Comments:**\n- The manuscript is well-organized and clear in presentation.\n- The concept of adaptive computation rank selection is intriguing and pertinent for efficiently deploying pre-trained models for downstream tasks.\n\n**Summary Paragraph:**  \nIn terms of significance, the introduction of SeLoRA demonstrates innovation through its approach to rank adaptation in large models, although a thorough evaluation of training efficiency and hyper-parameter sensitivity is lacking. The reproducibility of results would benefit from clearer comparisons with other methodologies and an analysis of resource utilization. While the study is organized and the main idea is relevant, the weaknesses in evaluation and parameter selection need to be addressed for greater clarity and rigor.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *SeLoRA* (Self-Expanding Low-Rank Adaptation), an extension of LoRA tailored for fine-tuning large diffusion models in medical image synthesis. The key innovation lies in dynamically expanding the rank of low-rank matrices during training, guided by Fisher information, and applying this expansion selectively across model layers. This approach aims to improve memory efficiency and model adaptation within the U-Net architecture of Stable Diffusion. Overall, the paper is clearly organized and easy to follow, with a conceptually interesting contribution relevant to efficient adaptation of pre-trained generative models.\n\n**Major Comments**  \n1. Although the paper claims improvements in LoRA efficiency, there is no analysis or quantitative evidence regarding *training efficiency* (e.g., computational cost, speed, GPU memory usage). Including such evaluation would substantiate the claimed benefits.  \n2. The procedure for selecting hyperparameters, particularly the thresholds (λ and *t*) controlling rank expansion, lacks sufficient explanation. A sensitivity analysis would improve reproducibility and understanding.  \n3. Comparisons are restricted to rank-pruning methods (DyLoRA and AdaLoRA). It would strengthen the contribution to include or at least justify the omission of comparisons with adaptive rank selection methods, which are conceptually closer.  \n4. Figure 7 raises questions about qualitative consistency: other methods do not generate x-ray images similar to those of SeLoRA, though one might expect partial similarity (as seen in Figure 8). Clarification of this difference would help interpret the results.\n\n**Minor Comments**  \n- Provide more details on hyperparameter choices for reproducibility.  \n- Clarify the criteria or computation of Fisher information used for adaptive expansion.  \n- Ensure figures and captions clearly indicate methodological differences.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study introduces a potentially useful adaptive rank selection mechanism with relevance for efficient medical diffusion model training (innovation, significance). However, its evaluation omits critical efficiency metrics and broader comparisons, limiting assessment of practical gains (evaluation). Reproducibility is also hindered by incomplete description of hyperparameter selection and sensitivity. The paper’s clarity is good, but methodological completeness should be improved to meet typical TMI standards.\n\n**Decision Recommendation**  \n**Major Revision.** The idea is promising and well-presented but requires added empirical analysis, clearer methodological detail, and broader comparisons to substantiate claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper proposes SeLoRA (Self-Expanding Low-Rank Adaptation), a method for adapting latent diffusion models to medical image synthesis. The key insight is that standard LoRA applies uniform ranks across all layers, which may be suboptimal for diffusion models with diverse weight matrix shapes in their U-Net architecture. SeLoRA starts with rank r=1 and dynamically expands ranks during training based on Fisher information ratios (Equation 5). The method uses a specific initialization strategy where expanded matrix B components are zero-initialized while matrix A components use random initialization (Equation 2). Experiments on IU X-RAY and Montgomery County CXR datasets show SeLoRA achieves better FID scores and CLIP scores compared to LoRA, AdaLoRA, and DyLoRA baselines (Tables 1-2). The method demonstrates adaptive rank allocation, placing higher ranks on attention query/key components and cross-attention layers (Figures 3-4). The approach aims to balance synthesis quality with parameter efficiency in medical image generation tasks.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and clarity issues**\n  - Equation 2 notation is inconsistent with standard matrix operations, as x[A K] implies concatenation but the dimensional analysis with the subsequent matrix multiplication is unclear\n  - The Fisher Information Score definition in Equation 4 lacks proper mathematical justification for why summing individual parameter Fisher information values represents module-level information content\n  - Algorithm 1 contains undefined notation (A', B') that appears without proper introduction, and the update step \"Update A ← A', B ← B'\" contradicts the conditional expansion logic\n\n• **Experimental design limitations and evaluation concerns**\n  - Table 1 shows SeLoRA performance (FID: 113.04±16.79) is only marginally better than LoRA (113.37±7.88) within statistical variance, questioning the significance of claimed improvements\n  - Dataset sizes are extremely small (138 samples for Montgomery County, 3,955 for IU X-RAY split 80/16/4), making generalization claims questionable for medical AI applications\n  - No statistical significance testing is provided despite reporting standard deviations across three seeds, limiting confidence in performance differences\n  - Baseline comparisons are limited to LoRA variants only, missing comparisons with other PEFT methods or full fine-tuning approaches mentioned in the introduction\n\n• **Theoretical foundation and convergence analysis gaps**\n  - Section 3.3's convergence analysis relies on informal approximations without rigorous mathematical proof, particularly the claim about theoretical upper bounds\n  - The Fisher Information criterion for expansion decisions lacks theoretical justification for why FI-Ratio > λ indicates beneficial rank increases\n  - No analysis of computational overhead during training, as Fisher Information computation requires additional forward/backward passes every t steps\n\n• **Methodological concerns and reproducibility issues**\n  - Hyperparameter selection (λ=1.1, t=40) appears arbitrary with limited ablation studies (only Table 3 explores λ values), potentially affecting reproducibility\n  - The initialization strategy in Equation 2 with zero-initialized B components may create optimization difficulties or gradient flow issues not adequately addressed\n  - Missing details about how the method handles convergence detection or maximum rank limits in practice\n\n## Suggestions for Improvement\n\n• **Clarify and correct mathematical formulations**\n  - Rewrite Equation 2 with proper dimensional analysis and clear notation for matrix concatenation operations\n  - Provide theoretical justification for the Fisher Information Score aggregation in Equation 4, potentially through information-theoretic analysis\n  - Revise Algorithm 1 to use consistent notation and clearly define all variables before use, ensuring the conditional update logic is mathematically sound\n\n• **Strengthen experimental validation and statistical analysis**\n  - Conduct statistical significance testing (t-tests or Mann-Whitney U tests) to validate performance differences reported in Tables 1-2\n  - Expand evaluation to larger, more representative medical datasets or provide multiple smaller datasets to demonstrate generalization\n  - Include confidence intervals and effect size measurements alongside standard deviations to better characterize performance improvements\n  - Add comparisons with additional PEFT methods (BitFit, Adapters) and full fine-tuning to establish more comprehensive baselines\n\n• **Develop rigorous theoretical analysis**\n  - Provide formal convergence proofs for the rank expansion algorithm, including conditions under which the method terminates\n  - Establish theoretical connections between Fisher Information ratios and optimal rank selection through information-theoretic or optimization theory frameworks\n  - Analyze computational complexity and provide empirical measurements of training overhead including memory usage and wall-clock time comparisons\n\n• **Improve methodological rigor and experimental design**\n  - Conduct comprehensive hyperparameter sensitivity analysis across multiple datasets to establish robust default values for λ and t\n  - Investigate alternative initialization strategies and compare with the proposed zero-initialization approach through ablation studies\n  - Implement and report early stopping criteria or maximum rank limits to address practical deployment concerns and prevent overfitting",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThe manuscript presents SeLoRA (Self-Expanding Low-Rank Adaptation), a novel approach for adapting latent diffusion models in medical image synthesis. The authors argue that conventional LoRA is suboptimal for the varying weight matrix structures in U-Net architectures. SeLoRA innovatively starts with a rank of r=1 and expands ranks based on Fisher information ratios during training. Experimental results on datasets such as IU X-RAY and Montgomery County CXR demonstrate that SeLoRA outperforms existing methods like LoRA and its variants in terms of FID and CLIP scores, while aiming to achieve a balance between synthesis quality and computational efficiency.\n\n## Major Comments\n1. **Mathematical formulation inconsistencies and clarity issues:**\n   - Equation 2's notation is unclear; it implies concatenation but lacks dimensional clarity for subsequent multiplication. \n   - The Fisher Information Score in Equation 4 lacks justification for summing parameter Fisher information values to represent module-level content.\n   - Algorithm 1 contains undefined notation (A', B') and the update logic is inconsistent with the intended conditional expansion.\n\n2. **Experimental design limitations and evaluation concerns:**\n   - SeLoRA's performance improvement (FID: 113.04±16.79 vs. LoRA: 113.37±7.88) is marginal within statistical variance, raising doubts about significance.\n   - The dataset size is small (138 for Montgomery County, 3,955 for IU X-RAY), challenging the robustness of generalization claims in medical AI.\n   - No statistical significance testing was performed, limiting confidence in the reported performance differences.\n   - Comparison with additional PEFT methods or full fine-tuning is absent, which constrains evaluation depth.\n\n3. **Theoretical foundation and convergence analysis gaps:**\n   - Section 3.3 presents informal convergence analysis without rigorous proofs, particularly regarding theoretical upper bounds.\n   - There is a lack of justification for the Fisher Information criterion regarding rank increase decisions.\n   - Computational overhead during training related to Fisher Information calculations is not analyzed.\n\n4. **Methodological concerns and reproducibility issues:**\n   - Hyperparameter selections appear arbitrary, and limited ablation studies potentially hinder reproducibility.\n   - The zero-initialization of matrix B components may complicate optimization, with insufficient discussion on gradient flow.\n   - The manuscript lacks details on convergence detection and maximum rank limits.\n\n## Minor Comments\n- Clarify Equation 2 for dimensional analysis.\n- Justify the aggregation method used for Fisher Information in Equation 4.\n- Ensure all notation in Algorithm 1 is defined beforehand; revise for consistency.\n- Conduct statistical significance testing to validate performance claims and include additional evaluation on larger datasets.\n- Expand analysis of hyperparameter robustness and potential alternative initialization strategies.\n\n## Summary Paragraph\nThe manuscript demonstrates some innovation through the introduction of SeLoRA, fulfilling societal significance in medical imaging. However, it faces concerns regarding mathematical inconsistencies, experimental validation, and theoretical underpinnings, limiting its potential impact. The methodology requires refinement to enhance reproducibility and robustness in practical applications. Therefore, while the contributions are noteworthy, significant revisions are needed to address the concerns raised.\n\n## Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **SeLoRA (Self‑Expanding Low‑Rank Adaptation)**, a method for adapting latent diffusion models to medical image synthesis. The key idea is to begin training with low-rank adaptations (r = 1) and dynamically increase rank based on Fisher Information ratios, thereby tailoring capacity allocation across U‑Net layers with different weight structures. The authors present mathematical formulations, an algorithmic description, and experimental results on the IU X‑RAY and Montgomery County datasets, reporting performance improvements over LoRA, AdaLoRA, and DyLoRA baselines. Overall, the paper is clearly written and addresses a relevant problem in efficient model adaptation, but exhibits several theoretical, methodological, and experimental limitations.  \n\n---\n\n**Major Comments**  \n1. **Mathematical formulation and clarity**  \n   - Equation (2) uses non‑standard notation and ambiguous dimensional relationships (e.g., the expression *x[A K]*).  \n   - The Fisher Information Score in Equation (4) lacks justification for using the sum of parameter‑wise Fisher values to represent module‑level information content.  \n   - Algorithm 1 employs undefined symbols (*A′, B′*) and inconsistent update rules that contradict the stated conditional expansion logic.  \n\n2. **Experimental design and evaluation**  \n   - Performance improvements are marginal (e.g., FID differences within reported standard deviations).  \n   - Very small dataset sizes limit the generalizability of results, and no statistical significance testing is provided.  \n   - Baseline comparisons are confined to LoRA variants, omitting other parameter‑efficient fine‑tuning or full‑tuning baselines noted in the introduction.  \n\n3. **Theoretical foundation and convergence analysis**  \n   - Convergence arguments rely on heuristic approximations without formal proof or upper‑bound derivations.  \n   - The use of Fisher Information ratio thresholding (FI‑Ratio > λ) lacks theoretical support.  \n   - Computational overhead from repeated Fisher computations is not analyzed.  \n\n4. **Methodological and reproducibility issues**  \n   - Hyperparameter choices (λ = 1.1, t = 40) seem ad‑hoc, with limited ablation coverage.  \n   - The proposed initialization scheme (zero‑initialized *B*) could hinder optimization, yet its effect is not explored.  \n   - Details about convergence detection, stopping criteria, or maximum rank limits are missing.  \n\n---\n\n**Minor Comments**  \n- Clarify notation consistency across Sections 3–4.  \n- Ensure all variables and symbols are introduced before use.  \n- Provide dataset splitting details and units in tables.  \n- Check for typographical consistency in formulas and captions.  \n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nSeLoRA addresses an important efficiency issue in adapting diffusion models for medical imaging, demonstrating potential significance through adaptive rank allocation. The approach is conceptually novel but its theoretical underpinnings are insufficiently formalized. Empirical evaluation is limited by small datasets, marginal gains, and missing statistical tests, weakening claims of clear superiority. Reproducibility is further impeded by incomplete algorithmic definitions, limited ablations, and absent computational analyses. Overall, while the topic is significant, the work requires stronger theoretical justification and more rigorous experimental validation.  \n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n✅ The manuscript introduces **SeLoRA**, a *self-expanding low-rank adaptation module* tailored for fine-tuning latent diffusion models (LDMs) in medical image synthesis. It sits squarely in the intersection of *machine learning methodology* and *medical image formation*, rather than being an application-only study. Novel algorithmic contributions and validation on medical image datasets suggest relevance to *methodological innovation in imaging AI*, which is consistent with *IEEE TMI*’s scope.\n\n**2. Novelty & Contribution Level**  \nThe work extends LoRA techniques with a dynamic, Fisher‑information–guided rank expansion mechanism, offering a new and interpretable method for parameter‑efficient fine‑tuning within diffusion architectures. While LoRA variants exist (DyLoRA, AdaLoRA, ALoRA), the “self‑expanding” concept and analytical justification via Fisher information appear novel and non‑incremental — provided results hold across multiple imaging domains.  \n\n**3. Technical and Experimental Rigor**  \nThe methodological description is mathematically complete, including equations for Fisher‑information–based rank adaptation, convergence discussion, and implementation details. Evaluation spans multiple datasets (IU X‑Ray, Montgomery CXR, PatchGastricADC22), metrics (FID, CLIP), and ablation/efficiency analyses. However, the evaluation protocols are limited to small datasets and mostly 2D synthesis tasks, with limited uncertainty quantification or comparison with conventional (non‑LoRA) fine‑tuning baselines. Statistical significance and clinical interpretability are not deeply analyzed.  \n\n**4. Clarity and Presentation**  \nThe manuscript is generally clear, structured in ICLR format, and readable. Figures and tables are informative but would benefit from improved visual contrast and inclusion of error bars for all metrics. Minor grammatical inconsistencies and formatting issues (e.g., spacing, capitalization in section headers) are present.  \n\n**5. Ethical and Reproducibility Compliance**  \nDatasets used (IU X‑Ray, Montgomery County CXR, PatchGastricADC22) are public and de‑identified. The text mentions code availability. No apparent ethical issues or breaches of patient confidentiality are observed. Reproducibility is reasonably supported through detailed configuration tables.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n### 1. Summary  \nThe paper proposes **SeLoRA**, a dynamic low‑rank adaptation mechanism for latent diffusion models in medical image synthesis. Standard LoRA methods use fixed rank across network layers, which can be suboptimal for architectures such as Stable Diffusion’s denoising U‑Net. SeLoRA introduces a Fisher‑information–based rule for *self‑expanding* rank, allowing each layer to adaptively adjust capacity during fine‑tuning. Experiments on chest X‑ray and histopathology datasets show improvements in Fréchet Inception Distance and CLIP scores, with lower trainable‑parameter counts and modest computational overhead.\n\n### 2. Strengths  \n- **Methodological novelty:** introduces data‑driven, layer‑specific rank expansion based on Fisher information.  \n- **Solid theoretical insight:** includes convergence argument and sensitivity study on the expansion threshold λ.  \n- **Parameter efficiency:** achieves comparable or better quality with a fraction of LoRA’s parameters.  \n- **Broad potential:** applicable to multiple imaging modalities and scales effectively within diffusion architectures.  \n\n### 3. Weaknesses  \n- **Limited clinical significance:** experiments emphasize numerical image quality rather than downstream diagnostic utility or radiologist assessment.  \n- **Evaluation breadth:** only small datasets are used; scalability to large 3‑D imaging modalities remains speculative.  \n- **Overlapping prior art:** comparisons to existing adaptive‑rank or pruning‑based LoRA variants lack unified statistical analysis to confirm superiority.  \n- **Fisher‑information estimation:** empirical formulation may be unstable; sensitivity to noisy gradient statistics is not deeply explored.  \n- **Writing issues:** minor typographical errors and inconsistent formatting detract from readability.\n\n### 4. Major Comments  \n1. **Validation depth:** Extend experiments to larger, clinically meaningful datasets (e.g., MIMIC‑CXR or BRATS) or include transfer tests across modalities to demonstrate generalizability expected of TMI papers.  \n2. **Quantitative uncertainty:** Provide variance or statistical significance for FID/CLIP metrics (e.g., across seeds or subsets) to substantiate claims.  \n3. **Methodological clarity:** Clarify the computational cost of Fisher‑information estimation—does it require double backpropagation or stored gradients? Include complexity comparison.  \n4. **Ablation completeness:** Examine the relative contribution of Fisher‑based expansion versus simpler heuristics (e.g., gradient norm‑based).  \n5. **Clinical evaluation:** Provide at least a radiologist‑reviewed visual assessment or task‑level measure (e.g., diagnostic consistency) to connect image synthesis quality to medical relevance.\n\n### 5. Minor Comments  \n- Proofread for inconsistent capitalization (e.g., “R ELATED WORK”).  \n- Improve figure titles for interpretability; ensure all values include uncertainty (mean ± std).  \n- State explicitly which GPU and memory environment were used.  \n- Provide a stable DOI or GitHub link for reproducibility.  \n- Consider summarizing main quantitative improvements in text rather than only in tables.\n\n### 6. Overall Recommendation  \n**Major Revision** – The work is methodologically strong and within TMI’s scope but requires deeper validation on diverse, clinically meaningful imaging tasks and stronger evidence of reproducibility and statistical robustness to meet TMI’s publication standards.\n\n### 7. Confidence Level  \n**4 / 5** – High confidence in assessment of methodological novelty and alignment with TMI expectations; moderate uncertainty regarding experimental sufficiency.\n\n---\n\n**Summary Judgment:**  \nSeLoRA presents a credible and innovative methodological advance in parameter‑efficient diffusion‑model adaptation. With expanded quantitative evaluation and better articulation of clinical relevance, it could become a strong candidate for *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **SeLoRA**, a self‑expanding low‑rank adaptation module designed for parameter‑efficient fine‑tuning of latent diffusion models (LDMs) in medical image synthesis. The work situates itself within methodological innovation rather than application, proposing a Fisher‑information–guided mechanism that allows adaptive rank expansion across network layers. Overall, the paper is clearly written, appropriately scoped for *IEEE TMI*, and offers an algorithmic contribution of potential relevance to medical imaging AI. While theoretical and implementation details are comprehensive, the experimental validation remains limited in scale and clinical depth.\n\n---\n\n**Major Comments**  \n1. **Validation Scope and Generalization:** Experiments rely on small 2‑D datasets (IU X‑Ray, Montgomery CXR, PatchGastricADC22). Broader validation on larger or 3‑D datasets (e.g., MIMIC‑CXR, BRATS) or cross‑modal transferability tests would better demonstrate generalization.  \n2. **Statistical Analysis:** Report quantitative uncertainty or significance for FID/CLIP metrics across random seeds or folds to substantiate performance gains.  \n3. **Computational Cost Clarification:** Clarify whether Fisher‑information computation requires double backpropagation or additional storage, and provide explicit complexity comparison to baseline LoRA variants.  \n4. **Ablation Completeness:** Differentiate the contribution of Fisher‑based dynamic rank selection from alternative heuristics such as gradient‑magnitude‑based adaptation.  \n5. **Clinical Evaluation:** Demonstrate downstream relevance—e.g., qualitative assessment by radiologists or diagnostic consistency—to connect synthesis quality to medical impact.\n\n---\n\n**Minor Comments**  \n- Correct typographical and capitalization inconsistencies (e.g., section headers like “R ELATED WORK”).  \n- Improve figure contrast and include error bars or variance estimates for all metrics.  \n- Summarize key quantitative results directly in the text for readability.  \n- Specify GPU setup and memory usage.  \n- Provide a stable repository or DOI for the released code to ensure reproducibility.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nIn terms of **significance**, SeLoRA addresses efficiency and interpretability in diffusion‑based medical image synthesis, aligning well with TMI’s focus on methodological advances. Its **innovation** lies in Fisher‑information–driven, self‑expanding rank adaptation, a concept not covered by prior LoRA variants. The **evaluation**, while methodologically sound and covering multiple datasets and metrics, remains restricted to small‑scale studies lacking statistical robustness and clinical validation. **Reproducibility** appears reasonable given the use of public datasets and announced code release, though explicit run‑time and configuration transparency should be strengthened. Overall, the work is promising but requires broader quantitative and clinical substantiation.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** – The study presents an innovative and relevant method with solid theoretical grounding, but it needs expanded experimental validation, quantitative uncertainty analysis, and clearer demonstration of medical significance before meeting *IEEE TMI* publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph  \nThe manuscript proposes **SeLoRA**, a self-expanding low-rank adaptation module designed for fine-tuning latent diffusion models (LDMs) in medical image synthesis. The study operates at the intersection of machine learning methodology and medical image formation, contributing novel algorithmic insights and validation using medical image datasets. This aligns well with the methodological innovation focus of *IEEE Transactions on Medical Imaging (TMI)*.\n\n### Major Comments  \n1. **Validation Depth:** Experiments should be extended to include larger, clinically significant datasets such as MIMIC‑CXR or BRATS to enhance generalizability, which is critical for TMI publications.  \n2. **Quantitative Uncertainty:** Variances or statistical significance for FID and CLIP metrics should be provided to substantiate claims, possibly across multiple trials.  \n3. **Methodological Clarity:** The computational cost of Fisher-information estimation requires clarification; is double backpropagation necessary, and how does it compare in complexity to simpler methods?  \n4. **Ablation Completeness:** Evaluate the contribution of Fisher-based rank expansion against simpler heuristics such as gradient norm measures.  \n5. **Clinical Evaluation:** Incorporate a radiologist-reviewed visual assessment or a diagnostic consistency measure to establish a clear link between image synthesis quality and medical relevance.\n\n### Minor Comments  \n- The text should be proofread for inconsistent capitalization (e.g., “RELATED WORK”).  \n- Figure titles could be improved for clarity, ensuring all metrics reflect uncertainty (mean ± std).  \n- Explicitly state which GPU and memory configuration were utilized in experiments.  \n- A stable DOI or GitHub link for reproducibility should be included.  \n- Consider summarizing key quantitative improvements in the textual narrative, rather than relying solely on tables.\n\n### Summary Paragraph  \nThis study presents significant methodological innovations, particularly through its dynamic, data-driven rank expansion method that enhances parameter efficiency in diffusion models. While the theoretical foundation and the experiments conducted are robust, the clinical significance and evaluation breadth are somewhat limited. Further validation with larger datasets and stronger emphasis on statistical analysis are necessary to ensure the work meets TMI's standards for significance, innovation, evaluation, and reproducibility.\n\n### Decision Recommendation  \n**Major Revision** – The manuscript demonstrates methodological strength and adheres to the scope of TMI but requires extensive validation and enhanced reproducibility measures to align fully with publication expectations.\n\n### Confidence Level  \n**4 / 5** – There is high confidence in the assessment of the manuscript's methodological novelty and its relevance to TMI, although there is moderate uncertainty regarding the sufficiency of the experimental evaluations.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chengjia Wang",
      "Giorgos Papanastasiou",
      "Guang Yang",
      "Hongwei Bran Li",
      "Wei Pang",
      "Yuchen Mao"
    ],
    "url": "pdfs/iclr.cc-2025-conference_f6f8edf9c65dcb297d770a2a371e14a022229bb1.pdf",
    "remote_url": "https://openreview.net/pdf/f6f8edf9c65dcb297d770a2a371e14a022229bb1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Transformer-Based CT Anomaly Detection and Auto-Segmentation of Sparse Lung Nodules",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Transformer",
      "CT scans",
      "lung nodules",
      "anomaly detection",
      "auto-segmentation",
      "Deformable-DETR",
      "sparse data",
      "medical imaging",
      "self-attention",
      "multi-scale learning",
      "object detection",
      "Focal Loss",
      "segmentation"
    ],
    "abstract": "Accurate segmentation of lung nodules in computed tomography (CT) scans is challenging due to extreme class imbalance, where nodules appear sparsely among healthy tissue. Lung tumor boards often review these scans manually, a time-consuming process. This paper introduces a novel two-stage approach for lung tumor segmentation by framing the problem as anomaly detection. The method is divided into two stages, allowing each model to leverage its strengths. Stage 1 focuses on region proposal, employing a custom Deformable Detection Transformer with Focal Loss to overcome class imbalance and localize sparse tumors. In Stage 2, the predicted bounding boxes are refined into pixel-wise segmentation masks using a fine-tuned variant of Meta's Segment Anything Model (SAM) for semantic segmentation. To address the challenge of nodule sparsity and improve spatial context, a 7.5 mm Maximum Intensity Projection (MIP) is applied, aiding in the differentiation between nodules, bronchioles, and vascular structures. The model achieves a Dice coefficient of 92.4%, with 95.2% sensitivity and 93.2% precision on the LUNA16 dataset, demonstrating robust performance in real-world clinical conditions where nodule sparsity is 5%.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a two-stage pipeline for lung nodule segmentation in CT scans, designed to support lung tumor boards by enhancing segmentation accuracy and efficiency. The first stage employs a custom Deformable Detection Transformer (DETR) architecture to detect sparse lung tumors, leveraging deformable attention to improve sensitivity to small nodules. The second stage utilizes a fine-tuned Segment Anything Model (SAM), enhanced with medical imaging capabilities (MedSAM), to refine the bounding boxes into pixel-level segmentation masks, ensuring precision in differentiating nodules from surrounding anatomy.\n\nTo address the class imbalance in CT data - where lung nodules are rare compared to healthy tissue - the framework incorporates focal loss, reducing model bias towards non-tumor areas and enhancing detection accuracy for hard-to-detect nodules. Achieving a 94.2% F1 score for bounding box prediction and a 92.4% Dice coefficient in segmentation accuracy, this pipeline demonstrates strong potential to improve clinical workflows, enhance tumor board decision-making, and contribute to better patient outcomes by streamlining nodule detection in a clinical setting.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper introduces a novel two-stage approach for lung tumor segmentation by framing the task as anomaly detection, addressing the challenges of sparse nodule identification in CT scans. This innovative structure uses a Deformable Detection Transformer (DETR) for region proposals and a fine-tuned Segment Anything Model (SAM) for precise segmentation, effectively handling class imbalance and complex image features.\n\nAdditionally, the paper’s clear, logical structure and well-explained methodology make complex concepts accessible. Detailed quantitative results further highlight the framework's effectiveness, making it a valuable and readable contribution to medical imaging and clinical decision support.\n\n### Weaknesses\n\n1. The paper’s experimental section lacks depth, with insufficient analysis to thoroughly validate the proposed method. \n\n2. There is no ablation study provided, which limits insight into how each component - such as the use of Deformable Detection Transformer (DETR), the fine-tuned Segment Anything Model (SAM), and the customized focal loss - contributes to overall performance. Without this breakdown, it’s difficult to assess which aspects of the framework are most effective. \n\n3. The paper relies solely on quantitative evaluation, omitting any qualitative assessment, such as visual comparisons among different methods, which could provide a clearer understanding of the model's segmentation accuracy and real-world applicability.\n\n4. The presentation of results is also weak, with layout issues that detract from readability and professionalism. For instance, Table 2 extends beyond the page margin, rendering the data difficult to interpret. Additionally, there are inconsistencies and errors in in-text citations, which may confuse readers and hinder the paper’s credibility. These issues in presentation and citation detract from the paper's overall clarity and polish, suggesting the need for more careful formatting and editing. There are also several grammatical errors, which make the paper somewhat challenging to read.\n\n### Questions\n\n- Why is there only quantitative evaluation/comparison provided in the manuscript? Could you provide some qualitative results, such as visual examples of segmentation outputs, to illustrate the model’s performance?\n\n- Can you elaborate on how the class imbalance was handled during training? Were any additional strategies (besides focal loss) considered or tested to further address this issue?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a two-stage pipeline aimed at lung nodule segmentation in CT scans, intended to enhance accuracy and efficiency for lung tumor boards. The first stage features a custom Deformable Detection Transformer (DETR) for detecting sparse tumors, utilizing deformable attention to bolster sensitivity towards small nodules. The second stage employs a refined Segment Anything Model (SAM), supplemented with medical imaging capabilities (MedSAM), to convert bounding boxes into precise pixel-level segmentation masks, effectively distinguishing nodules from adjacent anatomical structures.\n\n**Major Comments:**  \n1. The experimental section lacks comprehensive depth; there is insufficient analysis to validate the proposed method adequately.  \n2. An ablation study is absent, limiting insights into the contributions of components such as Deformable Detection Transformer (DETR), the fine-tuned Segment Anything Model (SAM), and the customized focal loss to overall performance.  \n3. The evaluation is solely quantitative, with no qualitative assessments provided, such as visual comparisons among methods, which would foster a clearer understanding of segmentation accuracy and real-world applicability.  \n4. Result presentation has layout issues detracting from readability, exemplified by Table 2 exceeding page margins, making data interpretation difficult. Additionally, inconsistencies and errors in in-text citations may cause confusion and undermine the manuscript's credibility, alongside several grammatical errors impacting readability.\n\n**Minor Comments:**  \n- The clarity of figures and tables could be improved.  \n- There is a need for careful proofreading to address grammatical mistakes.  \n- In-text citation formatting requires consistency.\n\n**Summary Paragraph:**  \nThis paper demonstrates significant potential in enhancing clinical workflows and decision-making via an innovative approach to lung nodule segmentation, scoring a 94.2% F1 for bounding box prediction and a 92.4% Dice coefficient in segmentation accuracy. However, while the contribution is notable in terms of novelty and structure, weaknesses in experimental validation, absence of qualitative data, and presentation issues affect its evaluation and reproducibility.\n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage deep learning framework for lung nodule segmentation in CT scans, aiming to assist lung tumor boards by improving segmentation accuracy and efficiency. The first stage employs a Deformable Detection Transformer (DETR) for sparse nodule detection, while the second utilizes a fine-tuned Segment Anything Model (SAM/MedSAM) to generate precise pixel-level masks. By incorporating focal loss to mitigate class imbalance between nodules and healthy tissue, the approach achieves high quantitative performance (94.2% F1 for detection and 92.4% Dice for segmentation). Overall, the paper is clearly written, methodologically sound in concept, and potentially impactful for clinical decision support, though its empirical validation remains limited.  \n\n**Major Comments**  \n1. **Experimental Depth:** The evaluation lacks sufficient analytical depth. The results are reported clearly but without detailed interpretation, limiting confidence in the robustness of the findings.  \n2. **Ablation Studies:** No ablation or component-wise analysis is provided. The contribution of individual elements—such as DETR, MedSAM, and focal loss—cannot be disentangled, making it difficult to assess which innovations drive the observed performance.  \n3. **Qualitative Evaluation:** Only quantitative results are presented. Including qualitative examples (e.g., visual segmentation outputs) would strengthen the evidence and demonstrate clinical applicability.  \n4. **Presentation and Formatting:** Certain layout problems, notably Table 2 exceeding page margins, compromise readability. In-text citation inconsistencies and minor grammatical errors further detract from clarity and professional polish.  \n\n**Minor Comments**  \n- Improve formatting of tables and figures for readability.  \n- Correct inconsistent citations and typographical issues.  \n- Clarify details of the class imbalance strategy beyond focal loss; if alternatives were considered, briefly mention them.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper addresses a significant clinical imaging challenge with an innovative combination of DETR and SAM frameworks, thoughtfully adapted to medical data. Its conceptual novelty and strong quantitative metrics reflect potential impact. However, the evaluation is incomplete—lacking ablation and qualitative evidence—thus weakening claims of generalizability and reproducibility. Presentation issues, while secondary, limit readability. Overall, the contribution is promising but requires additional validation and revision for publication readiness.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists. However, the primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists.\n\n### Weaknesses\n\nThe primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Questions\n\nPreprocessing is performed with 1 mm isocubic resolution, yet the method for generating 7.5 mm MIP using five slices in Figure 1-c needs clarification.\n\nThe authors should analyze the histogram of nodules under 10 mm from the LIDC dataset and include these results and discussions in the paper.\n\nThe paper lacks external validation, which raises concerns about the generalizability of the findings. A discussion on this limitation is recommended.\n\nIn Figure 1-b, lung segmentation is reportedly performed using Otsu segmentation; however, accuracy metrics such as DSC should be presented.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPublic dataset",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript proposes a two-stage approach for lung tumor segmentation through anomaly detection. The first stage involves region proposal using a deformable detection transformer with focal loss, followed by fine-tuning with a Segment Anything Model (SAM). Notably, the study employs the Maximum Intensity Projection (MIP) method to tackle issues related to nodule sparsity and spatial context—a technique often utilized by radiologists. Overall, the clarity of the manuscript can be improved, especially concerning its methodology.\n\n### Major Comments\n1. The entire preprocessing and modeling process is conducted in 2D. This may hinder the differentiation of diseased lungs or lung cancers that are located near the thoracic wall. \n2. Nodules with subsolid or ground-glass opacity (GGO) characteristics might not be effectively visualized in thicker MIP slices; thus, these types should be assessed separately.\n3. While the model achieves superior performance compared to prior models, it lacks subclass analysis and an ablation study, which limits its technical novelty.\n4. The paper lacks external validation, raising concerns regarding the generalizability of the findings.\n\n### Minor Comments\n- Clarification is needed on how preprocessing at a 1 mm isocubic resolution produces a 7.5 mm MIP using five slices, as shown in Figure 1-c.\n- The authors should analyze and include the histogram of nodules under 10 mm from the LIDC dataset in their discussion.\n- The segmentation performed with Otsu method as indicated in Figure 1-b should have accompanying accuracy metrics, such as Dice Similarity Coefficient (DSC).\n\n### Summary Paragraph\nIn terms of significance, the manuscript addresses an important issue in lung tumor segmentation but lacks depth in evaluation and analysis of results. The innovation is present through the novel use of MIP; however, the predominance of 2D processing and lack of performance validation diminishes its potential impact. The evaluation does provide some improvements over previous efforts, but the absence of thorough testing and validation limits reproducibility. Overall, while the manuscript shows promise, it requires substantial revisions to fulfill editorial criteria.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage method for lung tumor segmentation based on anomaly detection. Stage one involves region proposal using a deformable detection transformer with focal loss, and stage two applies a fine-tuned SAM model. The authors additionally employ Maximum Intensity Projection (MIP) to mitigate issues of nodule sparsity and contextual information—an approach commonly used by radiologists. While the paper is clear in motivation and design, concerns remain regarding the methodological choices, particularly the reliance on 2D processing and insufficient evaluation depth.\n\n**Major Comments**  \n1. **Dimensionality Limitation:** All preprocessing and modeling steps are performed in 2D, which may limit the model’s ability to accurately distinguish lung cancers adjacent to the thoracic wall. A 3D approach could better preserve spatial information.  \n2. **Nodule Visibility:** For subsolid or ground-glass opacity (GGO) nodules, visibility is potentially reduced in thicker MIP slices. Separate analysis for these subtypes would improve model robustness and interpretability.  \n3. **Lack of Analytical Studies:** The work omits crucial analyses such as an ablation study or subgroup performance evaluation, weakening claims of technical contribution and novelty.  \n4. **External Validation:** Absence of validation on external datasets reduces the generalizability of the reported results; discussion of this limitation is necessary.  \n5. **Preprocessing Clarification:** The process of generating the 7.5 mm MIP from five slices (Figure 1‑c) requires clearer explanation.  \n6. **Performance Metrics:** Lung segmentation is reportedly performed using Otsu thresholding (Figure 1‑b), yet quantitative accuracy metrics (e.g., Dice similarity coefficient) are missing. Their inclusion would strengthen the evaluation.\n\n**Minor Comments**  \n- The manuscript should include an analysis of nodules under 10 mm from the LIDC dataset and corresponding discussion.  \n- Ensure figures and captions clearly describe preprocessing parameters and segmentation outputs.  \n- Minor editorial refinements could improve readability.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study addresses an important clinical problem and achieves encouraging quantitative results. However, its significance is reduced by methodological constraints tied to 2D processing, limiting clinical applicability. Innovation is moderate, as the integration of MIP and SAM is incremental rather than conceptually novel. Evaluation lacks comprehensive validation and ablation analyses, and reproducibility would benefit from clearer methodological detail and external dataset testing.\n\n**Decision Recommendation**  \n**Major Revision.** The work presents promising preliminary results but requires expanded analysis, methodological clarification, and additional validation to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis manuscript presents a novel two-stage approach for automating lung nodule segmentation using transformer models. In the data preprocessing phase, Maximum Intensity Projection (MIP) enhances spatial features, helping to distinguish nodules from bronchioles and vessels in CT images. Next, region proposal bounding boxes are generated using the Deformable-DETR model. In Stage 2, these bounding boxes are processed by the SAM model to achieve pixel-level segmentation. To address class imbalance within the dataset, focal loss is incorporated into the original DETR loss function. The results demonstrate superior performance compared to state-of-the-art (SOTA) methods.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nIn preprocessing, maximum intensity projection is applied across slices to enhance visibility. The two-stage framework, combining DETR and SAM models, offers a straightforward approach. Additionally, the common issue of class imbalance in medical datasets is addressed. As a result, segmentation performance is significantly improved. The paper is well organized.\n\n### Weaknesses\n\nThis manuscript lacks novel insights, as the deep learning models used in each stage are well-established, and focal loss is widely applied across various domains. Additionally, the ROI-based segmentation approach is considered somewhat conventional. There is no ablation studies on original SAM performance and some critical models to compare or discuss are missing.\n\n### Questions\n\n1. What is the technical insight of this work? The concept of ROI-based segmentation is not new, and both DETR and SAM are well-established models. As a result, the framework appears to lack novelty, which is a critical concern.\n\n2. There are several existing studies on lung nodule segmentation in CT images. For instance, the IEEE TMI paper, \"Closing the Gap between Deep Neural Network Modeling and Biomedical Decision-Making Metrics in Segmentation via Adaptive Loss Functions,\" addresses not only lung segmentation but also class imbalance. It would be beneficial for the authors to compare or discuss their work in relation to such prior studies.\n\n3. Does the model function in an end-to-end learning manner? If it does or does not, the authors should provide a discussion on the merits and limitations of the learning method used in this framework.\n\n4. Which stage of the framework provides the most significant performance improvement? \n\n5. Additionally, what is the baseline performance of the original SAM model pretrained with MedSAM? Are the datasets used in MedSAM aligned with the LUNA dataset, and how does this impact model performance?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThis manuscript proposes a two-stage approach for automating lung nodule segmentation using transformer models. The first stage employs Maximum Intensity Projection (MIP) for data preprocessing, which improves spatial feature visibility to differentiate nodules from bronchioles and vessels in CT images. In the second stage, bounding boxes generated using the Deformable-DETR model are processed by the SAM model for pixel-level segmentation. The integration of focal loss into the DETR loss function addresses class imbalance within the dataset, and the reported results indicate superior performance compared to state-of-the-art (SOTA) methods.\n\n### Major Comments\n1. **Novelty**: The manuscript lacks novel insights, as the deep learning models employed (DETR and SAM) are well-established in the field, and focal loss is commonly utilized across various applications. This raises significant concerns regarding innovation.\n2. **Evaluation Design**: The ROI-based segmentation approach is somewhat conventional and fails to present innovative methodologies or direct comparisons with critical studies. Moreover, no ablation studies are conducted to evaluate the original SAM performance.\n3. **Comparisons**: Missing discussions regarding existing literature on lung nodule segmentation, like the IEEE TMI paper that tackles segmentation and class imbalance, should be included for context.\n4. **Learning Method**: There is no discussion on whether the model functions in an end-to-end learning manner, and further exploration of the advantages and limitations of the chosen learning method is warranted.\n5. **Performance Metrics**: Clarification is needed on which stage of the framework contributes most significantly to performance enhancement, as well as baseline performance metrics for the original SAM model pretrained with MedSAM.\n\n### Minor Comments\n- The paper is well organized, but clarity in some technical explanations could be enhanced.\n- Figures could benefit from clearer labels and descriptions.\n- Avoid typographical errors and ensure consistent notation and acronyms throughout the manuscript.\n\n### Summary Paragraph\nOverall, the work demonstrates potential significance in lung nodule segmentation but suffers from limitations in innovation and novelty, as established methods dominate the proposed framework. The evaluation design lacks comparative analysis with current literature, impacting the reproducibility and broader applicability of results. While the manuscript is organized and addresses class imbalance effectively, it would benefit from deeper exploration of the model's performance and methodologies applied.\n\n### Decision Recommendation\nMinor revision is recommended to address the highlighted concerns and improve overall clarity and depth of the analysis.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a two-stage framework for automated lung nodule segmentation from CT scans, combining a Deformable-DETR model for region proposal with the SAM model for pixel-level segmentation. Preprocessing employs Maximum Intensity Projection (MIP) to enhance nodule visibility and distinguish lesions from vascular and bronchial structures. The method incorporates focal loss to mitigate dataset imbalance and reports superior segmentation performance relative to state-of-the-art methods. Overall, the paper is clearly written and well organized.  \n\n**Major Comments**  \n1. **Novelty and Technical Insight:** The approach offers limited novelty since both DETR and SAM are established models, and ROI-based segmentation strategies are widely used. The manuscript would benefit from a clearer articulation of technical innovation beyond model combination.  \n2. **Lack of Ablation and Comparative Analyses:** No ablation studies are provided to assess the contributions of individual components, such as the original SAM versus the proposed integration. Comparative analysis with key prior works—particularly those addressing class imbalance with adaptive loss functions—is missing.  \n3. **Framework Discussion:** Clarification is needed on whether the model operates in an end-to-end manner, along with a discussion of advantages or drawbacks of the chosen training strategy.  \n4. **Performance Attribution:** The paper does not specify which stage of the framework contributes most significantly to performance gains; such insight would strengthen the technical interpretation.  \n5. **Dataset and Pretraining Alignment:** The relationship between the datasets used for MedSAM pretraining and the LUNA dataset should be discussed to contextualize performance improvements and ensure fair comparison baselines.  \n\n**Minor Comments**  \n- Minor language and formatting adjustments could improve presentation clarity.  \n- Ensure that all models and acronyms are defined at first mention.  \n- Figures and tables could more explicitly indicate the comparative baselines.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses a meaningful medical imaging problem with a well-structured pipeline and solid implementation. However, its significance is limited by modest methodological innovation. The evaluation lacks sufficient ablation and comparative experimentation to convincingly establish novelty or performance attribution. Presentation and reproducibility are good, but clearer methodological detail and dataset transparency are needed for validation.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes a two-stage transformer-based approach for lung nodule segmentation in CT scans to support tumor board applications. Stage 1 employs a custom Deformable Detection Transformer (Deformable-DETR) with focal loss to generate region proposals, addressing class imbalance where nodules appear in only 5% of slices. Stage 2 uses a fine-tuned Segment Anything Model (SAM) initialized with MedSAM weights for pixel-wise segmentation within the proposed regions. The preprocessing pipeline includes 7.5mm Maximum Intensity Projection (MIP) to enhance nodule visibility and distinguish nodules from vascular structures. Evaluated on LUNA16 dataset with 9,676 processed slices, the method achieves 94.2% F1 score for region proposal and 92.4% Dice coefficient for segmentation, with 95.2% sensitivity and 93.2% precision overall.\n\n## Weaknesses\n\n• **Insufficient methodological novelty and technical contribution**\n  - The paper combines existing architectures (Deformable-DETR and SAM/MedSAM) without substantial architectural modifications (Section 3, Figures 2-3), limiting technical innovation\n  - The two-stage detection-then-segmentation pipeline follows established computer vision paradigms rather than introducing novel approaches for medical imaging\n  - The customization appears limited to hyperparameter tuning (γ = 2, αt = 0.25 in Section 4) rather than fundamental algorithmic advances\n\n• **Mathematical formulations lack rigor and completeness**\n  - Equation (1) for resampling factor R appears to define R as both a scalar ratio and a vector without clear notation consistency (Page 3, line 118-121)\n  - The computational complexity claim \"O(H²W²C)\" in Section 3.3 (Page 4, line 207-208) lacks derivation and context for the deformable attention mechanism\n  - Missing mathematical description of the focal loss integration into DETR's overall loss function, despite claiming customization (Section 3.6)\n\n• **Experimental evaluation demonstrates significant limitations**\n  - Limited to single dataset (LUNA16) without cross-dataset validation, reducing generalizability claims (Section 3.2, Table 1)\n  - Inconsistent class distribution between training (12.7% nodule rate) and testing (5% nodule rate) confounds performance interpretation (Section 3.2, Page 4, lines 180-186)\n  - Comparison in Table 2 lacks statistical significance testing and doesn't control for different preprocessing approaches used by baseline methods\n\n• **Technical implementation details insufficient for reproducibility**\n  - Missing critical hyperparameters for SAM fine-tuning including learning rate, batch size, and number of epochs (Section 3.5, Page 5, lines 255-260)\n  - Unclear how bounding boxes from Stage 1 are converted to prompts for Stage 2, despite this being crucial for the pipeline (Section 3.4, Figure 3)\n  - No ablation studies to validate design choices like 7.5mm MIP thickness or the necessity of the two-stage approach\n\n• **Limited clinical validation and applicability assessment**\n  - No evaluation with real clinical workflows or tumor board settings despite targeting this application (Abstract, Section 1)\n  - Missing analysis of computational requirements and inference time critical for clinical deployment\n  - Insufficient discussion of failure modes or edge cases relevant to clinical decision-making scenarios\n\n## Suggestions for Improvement\n\n• **Strengthen technical contributions through architectural innovations**\n  - Develop novel attention mechanisms specifically designed for sparse medical object detection rather than adapting general computer vision models\n  - Propose end-to-end trainable architecture that jointly optimizes detection and segmentation rather than sequential stages\n  - Introduce domain-specific modifications to transformer architectures that leverage medical imaging characteristics\n\n• **Enhance mathematical rigor and consistency**\n  - Provide complete mathematical derivation showing how focal loss integrates into the full DETR loss function with proper notation\n  - Clarify vector/scalar notation in Equation (1) and provide dimensional analysis for all mathematical formulations\n  - Include detailed complexity analysis for the deformable attention mechanism in the medical imaging context with proper justification\n\n• **Expand experimental validation comprehensively**\n  - Conduct cross-dataset validation using additional lung nodule datasets (e.g., LIDC-IDRI, NLST) to demonstrate generalizability\n  - Maintain consistent class distributions across training/validation/test sets or provide thorough analysis of distribution shift effects\n  - Include statistical significance testing and confidence intervals for all reported metrics, with proper baseline comparisons using identical preprocessing\n\n• **Provide complete implementation details for reproducibility**\n  - Document all hyperparameters, training procedures, and implementation specifics for both stages including SAM fine-tuning parameters\n  - Clearly specify the bounding box to prompt conversion mechanism with mathematical formulation and implementation details\n  - Conduct comprehensive ablation studies examining MIP thickness, loss function components, and single-stage vs. two-stage approaches\n\n• **Validate clinical applicability through real-world evaluation**\n  - Collaborate with tumor boards to evaluate the system in actual clinical workflows with radiologist feedback and usability studies\n  - Provide detailed computational analysis including inference times, memory requirements, and hardware specifications for clinical deployment\n  - Include failure case analysis with clinical expert input to identify and address limitations relevant to medical decision-making",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph\nThis manuscript presents a two-stage transformer-based methodology for lung nodule segmentation in CT scans aimed at improving tumor board applications. The first stage employs a custom Deformable Detection Transformer (Deformable-DETR) with focal loss to generate region proposals, addressing the challenge of class imbalance where nodules are present in only 5% of slices. The second stage utilizes a fine-tuned Segment Anything Model (SAM) initialized with MedSAM weights for precise pixel-wise segmentation. Notably, the preprocessing involves a 7.5mm Maximum Intensity Projection (MIP) to enhance nodule visibility. Evaluated on the LUNA16 dataset with 9,676 processed slices, the approach achieves competitive metrics, including a 94.2% F1 score for region proposal and 92.4% Dice coefficient for segmentation.\n\n## Major Comments\n1. **Methodological Novelty**: The research relies on existing architectures (Deformable-DETR, SAM) and shows limited technical innovation, primarily adjusting hyperparameters instead of introducing novel algorithmic contributions (Section 3, Figures 2-3).\n2. **Mathematical Rigor**: The mathematical formulations, such as Equation (1), exhibit inconsistencies and lack thorough explanations, particularly regarding computational complexity claims without derivation (Page 3, lines 118-121; Page 4, lines 207-208).\n3. **Experimental Limitations**: Testing is confined to a single dataset, reducing the claim of generalizability; there are discrepancies in class distributions between training and testing that affect performance assessments (Section 3.2, Page 4, lines 180-186).\n4. **Reproducibility Concerns**: Important hyperparameters for SAM tuning are absent, and the process of transitioning bounding boxes to prompts for segmentation is unclear (Section 3.5, Page 5; Section 3.4, Figure 3).\n5. **Clinical Validation**: The study lacks real-world clinical workflow assessments and discussions of computational requirements necessary for deployment (Abstract, Section 1).\n\n## Minor Comments\n- Revise the mathematical notation in Equation (1) for clarity.\n- Ensure consistent class distribution analysis is included in results.\n- Provide complete hyperparameter documentation for reproducibility efforts.\n\n## Summary Paragraph\nIn terms of significance, the work addresses a pertinent clinical application in lung cancer detection; however, it falls short on innovation as it primarily amalgamates existing methods instead of proposing novel designs. The evaluation is limited due to single dataset reliance, raising concerns about reproducibility and application in diverse clinical settings, while the absence of concrete computational assessments and lack of clinical validation further detracts from its potential impact.\n\n## Decision Recommendation\nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage transformer-based framework for lung nodule segmentation in CT scans, intended to support tumor board applications. The first stage employs a modified Deformable DETR with focal loss for nodule region proposals, while the second stage fine-tunes a MedSAM-initialized Segment Anything Model for pixel-wise segmentation. Using 7.5 mm MIP preprocessing, the method achieves strong quantitative results (F1 = 94.2%, Dice = 92.4%) on the LUNA16 dataset. Overall, the paper is clearly written and demonstrates careful experimental design; however, its methodological novelty and reproducibility are limited, and claims of clinical relevance are insufficiently supported.  \n\n**Major Comments**  \n1. **Limited methodological novelty and contribution** – The approach mainly combines existing architectures (Deformable DETR and SAM/MedSAM) with minor hyperparameter tuning and lacks novel architectural or algorithmic elements. The two-stage design aligns with established computer vision strategies and offers limited innovation for medical imaging applications.  \n2. **Incomplete and inconsistent mathematical formulation** – Equation (1) inconsistently defines variables; the complexity estimate lacks justification; and the focal-loss integration into DETR is not mathematically specified. These issues reduce rigor and clarity.  \n3. **Constrained experimental evaluation** – Experiments rely solely on LUNA16, without cross-dataset validation. Differences in class distribution between training and testing sets confound performance interpretation. Reported comparisons omit statistical testing and fail to standardize preprocessing across baselines.  \n4. **Insufficient implementation and reproducibility details** – Key training parameters for SAM fine-tuning are missing. The conversion between Stage 1 bounding boxes and Stage 2 prompts is unclear, and no ablation studies assess the necessity of design choices such as the MIP thickness or the two-stage setup.  \n5. **Lack of clinical validation and applicability assessment** – The claimed tumor-board relevance is untested. The study omits computational performance metrics, and no failure-case or clinical workflow analysis is provided.  \n\n**Minor Comments**  \n- Improve consistency in notation and vector-scalar definitions.  \n- Provide explicit derivations for complexity and loss equations.  \n- Add confidence intervals, significance testing, and standardized baselines in tables.  \n\n**Summary Paragraph (Significance–Innovation–Evaluation–Reproducibility)**  \nThe study addresses a clinically relevant task and achieves competitive quantitative performance, supporting its **significance**. However, **innovation** is modest, as the pipeline primarily integrates established models with minor adjustments. The **evaluation** is narrow in scope and lacks statistical validation and cross-dataset testing. Finally, **reproducibility** is hindered by incomplete reporting of hyperparameters, training protocols, and implementation details. Strengthening methodological originality, mathematical rigor, and experimental depth would be required for journal-level impact.  \n\n**Decision Recommendation**  \n**Major Revision** – Substantial methodological clarification, expanded evaluation, and stronger clinical validation are needed before the work can be reconsidered.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI – ✅**  \nThe manuscript describes a two-stage transformer-based framework for automated CT segmentation of lung metastases, combining Deformable-DETR for region proposal and a fine-tuned MedSAM for pixel-level delineation. The technical focus is squarely on imaging methodology (architecture design, training, and validation for CT segmentation), which falls well within the scope of *IEEE Transactions on Medical Imaging*.  \n\n**2. Novelty & Contribution Level – ⚠️ Moderate Novelty**  \nWhile the work effectively integrates several established components—Deformable-DETR, SAM, Focal Loss, and Maximum Intensity Projection—it does not appear to introduce new fundamental algorithms or theoretical insights. The novelty lies in the pipeline integration and data-configured optimization for highly sparse CT nodule detection, rather than a new methodological formulation. This is likely considered an *engineering innovation* rather than a *theoretical advance*. It might not meet TMI’s expectation for methodological originality unless the joint training or architectural coupling between detection and segmentation is made deeper and more generalizable beyond LUNA16.\n\n**3. Technical and Experimental Rigor**  \nThe paper is technically detailed: preprocessing, augmentation, architecture configuration, loss formulation, and hyperparameters are clearly described. However, evaluation is limited to LUNA16, and no external validation or uncertainty quantification is provided. The reproducibility and statistical robustness of the reported Dice (92.4%) are unclear—no variance or confidence intervals are shown. Generalization and noise robustness remain open.\n\n**4. Clarity and Presentation**  \nThe manuscript is generally readable, with figures and equations enhancing clarity. Some typographical, grammatical, and stylistic inconsistencies appear (e.g., inconsistent capitalization, spacing errors, and redundant section headers). Figures are schematic but should have scale bars and axis annotations for publication-level clarity.\n\n**5. Ethical and Reproducibility Compliance**  \nLUNA16 is public and anonymized; ethical compliance is acceptable. However, there is no mention of code or model availability—TMI typically expects reproducibility statements.\n\n---\n\n## Phase 2 – IEEE TMI Review Report\n\n### 1. Summary\nThe paper proposes a two-stage transformer-based auto-segmentation framework for lung nodule analysis in CT scans. Stage 1 employs a customized Deformable-DETR with focal loss to generate candidate regions under severe class imbalance. Stage 2 refines these detections using a fine-tuned version of Segment Anything Model (MedSAM) to produce pixel-accurate tumor masks. Applied to the LUNA16 dataset with artificially controlled sparsity (5% positive slices), the system yields a Dice coefficient of 92.4%, outperforming several CNN-based baselines.\n\n### 2. Strengths\n- Addresses an important challenge—small object segmentation under class imbalance—in a clinically relevant task.  \n- Clear engineering implementation with attention to data preprocessing, resampling, and augmentations specific to CT.  \n- Quantitative comparison against multiple baselines and well-defined metrics.  \n- Effective adaptation of transformer models to medical imaging data.\n\n### 3. Weaknesses\n- Limited methodological novelty: integration rather than invention of core algorithms.  \n- Evaluation confined to a single dataset (LUNA16), without cross-dataset or clinical validation.  \n- Absence of uncertainty analysis, statistical significance testing, or comparison under degraded imaging conditions.  \n- No discussion of inference time, computational cost, or potential deployment feasibility for tumor boards.  \n- Clarity and English usage occasionally hinder readability.  \n\n### 4. Major Comments\n1. **Novelty Clarification** – The method mainly combines existing architectures (Deformable-DETR + SAM). To meet TMI’s innovation criteria, authors should emphasize new algorithmic contributions—e.g., a unified loss, cross-attention coupling, or training joint optimization—rather than a pipeline integration.  \n2. **Validation Depth** – Extend evaluation beyond LUNA16 (e.g., LIDC-IDRI subsets, local clinical data) to demonstrate generalization and robustness to scanner variability.  \n3. **Statistical Reporting** – Provide confidence intervals or standard deviations for Dice, Sensitivity, and Precision. Report per-case distributions rather than a single average.  \n4. **Reproducibility Statement** – Include code availability or structured pseudocode to facilitate reproducibility, consistent with TMI’s guidelines.  \n5. **Ablation Analyses** – Demonstrate the incremental effect of each component (focal loss, MIP, SAM fine-tuning, Deformable attention). This would substantiate that the proposed integrations matter beyond an assembled pipeline.  \n6. **Clinical Integration Discussion** – Analyses of computation time, user interface considerations, and tumor board workflow integration would support translational value.  \n\n### 5. Minor Comments\n- Improve overall grammatical quality and consistency in figure/table captions.  \n- Clarify parameters in equations (e.g., specify α, γ for focal loss).  \n- Add resolution or voxel spacing information to tables for clarity.  \n- Some references lack correct citation formatting (e.g., double listings for Lin et al. 2017a,b).  \n- Figure readability (font size, labels) should meet journal standards.  \n\n### 6. Overall Recommendation: **Major Revision**\nThe paper presents a technically competent and carefully executed study, but with limited methodological novelty relative to TMI’s threshold. Expanding validation, introducing deeper algorithmic coupling, or providing theoretical justification for the two-stage transformer synergy could elevate it to TMI standards. Without such additions, the work may be more suitable for a specialized applied imaging journal.\n\n### 7. Confidence Level: **4 / 5**\nHigh confidence in assessment of scope, technical rigor, and relevance to TMI’s standards.\n\n---\n\n**Summary Judgment:**  \nSolid and well-engineered integration of transformer paradigms for CT auto-segmentation with strong performance on LUNA16, but innovation lies in system integration rather than new methodological theory. Recommend **Major Revision** pending evidence of methodological advancement, cross-cohort validation, and enhanced reproducibility.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage transformer-based framework for automated segmentation of lung metastases on CT images. The approach combines Deformable-DETR for region proposal with a fine-tuned Segment Anything Model (MedSAM) for pixel-level delineation. The study targets challenges of small-object detection and severe class imbalance in lung nodule segmentation, reporting a Dice coefficient of 92.4% on the LUNA16 dataset. The work is clearly within the scope of *IEEE Transactions on Medical Imaging*, and the paper is generally well organized and detailed, though the methodological novelty appears limited to integration and engineering innovations rather than new algorithmic developments.\n\n---\n\n**Major Comments**  \n1. **Methodological Novelty** – The contribution primarily integrates existing models (Deformable-DETR and SAM) rather than introducing new algorithms or theoretical mechanisms. To satisfy TMI’s innovation expectations, the authors should clarify any new algorithmic component, such as unified losses, cross-attention coupling, or joint optimization.  \n2. **Validation Extent** – The evaluation is limited to LUNA16. Broader validation using additional datasets (e.g., LIDC-IDRI or local clinical data) would better demonstrate generalization and robustness to imaging variability.  \n3. **Statistical Reporting** – Performance metrics lack uncertainty quantification. Confidence intervals, standard deviations, or per-case distributions should be provided to establish statistical reliability.  \n4. **Reproducibility** – No information is given on code or model availability. A clear reproducibility statement or release plan is needed to comply with TMI standards.  \n5. **Ablation Studies** – Analyses showing the effect of each component (focal loss, maximum intensity projection, SAM fine-tuning, deformable attention) should be added to support the design choices.  \n6. **Clinical Integration** – Discussion of runtime, computational cost, and potential integration into clinical workflows would strengthen the manuscript’s practical value.\n\n---\n\n**Minor Comments**  \n- Improve grammar and consistency across figure and table captions.  \n- Specify parameters for equations, particularly α and γ in focal loss.  \n- Report voxel spacing or resolution in quantitative tables.  \n- Correct reference formatting inconsistencies (e.g., repeated citations).  \n- Ensure figures have appropriate scale bars, axis labels, and font clarity.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The topic is clinically relevant, addressing a persistent challenge in CT-based tumor segmentation.  \n**Innovation:** The work represents engineering integration of existing architectures rather than a novel methodological advance.  \n**Evaluation:** Results on LUNA16 are promising but insufficiently validated for generalizability or statistical robustness.  \n**Reproducibility:** Descriptions are detailed, yet reproducibility remains limited without code or additional datasets. Overall, the study offers useful application-level insights but requires broader validation and enhanced methodological depth to meet TMI standards.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe paper demonstrates technical competence and clear experimental execution but needs stronger algorithmic novelty, broader validation, statistical rigor, and reproducibility documentation before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "## Opening Paragraph\nThe manuscript proposes a two-stage transformer-based framework for automated segmentation of lung metastases in CT scans. It combines Deformable-DETR for region proposal generation with a fine-tuned Segment Anything Model (MedSAM) for pixel-level delineation. The study's technical focus on imaging methodology—specifically architecture design, training, and validation—aligns well with the scope of *IEEE Transactions on Medical Imaging*. \n\n## Major Comments\n1. **Novelty Clarification**: The manuscript primarily integrates existing models (Deformable-DETR, SAM) without presenting new fundamental algorithms or theoretical insights. To fulfill TMI's standards for innovation, the authors should emphasize any algorithmic contributions, such as a unified loss or enhanced training optimization.\n2. **Validation Depth**: The evaluation is restricted to the LUNA16 dataset. It is recommended to expand this evaluation to include other datasets (e.g., LIDC-IDRI) to demonstrate the framework's generalization capabilities and robustness to variations.\n3. **Statistical Reporting**: Confidence intervals or standard deviations for the reported Dice coefficient (92.4%) should be provided, alongside per-case distributions to ensure statistical robustness.\n4. **Reproducibility Statement**: The manuscript lacks information on code or model availability, which is essential for reproducibility consistent with TMI's guidelines.\n5. **Ablation Analyses**: Conducting ablation studies to demonstrate the impact of each component (e.g., focal loss, MIP, SAM fine-tuning) would strengthen the claims about the proposed integrations.\n6. **Clinical Integration Discussion**: Discussing computation time, interface considerations, and integration into tumor board workflows would enhance the translational potential of the work.\n\n## Minor Comments\n- The manuscript would benefit from improved grammatical quality and consistency in figure and table captions. \n- Clarify parameters in equations, including defining α and γ for focal loss. \n- Tables should include resolution or voxel spacing information for added clarity. \n- Citations in the references section require correction, particularly for Lin et al. 2017a,b.\n- Ensure figure readability meets journal standards, including appropriate font size and label clarity.\n\n## Summary Paragraph\nIn summary, while the manuscript addresses a significant challenge in small object segmentation relevant to clinical practice, its methodological novelty is limited to the integration of established components without introducing new theoretical insights. Key areas needing improvement include validation beyond a single dataset, reporting of statistical metrics, and enhancements in reproducibility. Addressing these points could bolster the work’s significance and impact, aligning it more closely with TMI editorial criteria.\n\n## Decision Recommendation\n**Decision Recommendation**: Major Revision  \nThe study is technically sound but lacks the necessary methodological novelty and depth of evaluation required by TMI. Expanding validation efforts and clarifying algorithmic contributions would significantly elevate the manuscript. \n\n## Confidence Level\n**Confidence Level**: 4 / 5  \nThe assessment of the scope, technical rigor, and relevance meets TMI standards with high confidence.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Hooman Ramezani"
    ],
    "url": "pdfs/iclr.cc-2025-conference_3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "remote_url": "https://openreview.net/pdf/3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "A New 3D Image Block Ranking Method Using Axial, Coronal and Sagittal Image Patch Rankings for Explainable Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "interpretability and explainable AI"
    ],
    "keywords": [
      "convolutional neural networks",
      "feature selection",
      "gradcam",
      "medical imaging",
      "disease diagnosis",
      "image classification"
    ],
    "abstract": "Although a 3D Convolutional Neural Network (CNN) has been applied to explainable\nmedical imaging in recent years, understanding the relationships among input\n2D image patches, input 3D image blocks, extracted feature maps, top-ranked\nfeatures, heatmaps, and final diagnosis remains a significant challenge. To help\naddress this important challenge, firstly, we create a new 2D Grad-CAM-based\nmethod using feature selection to produce explainable 2D heatmaps with a small\nnumber of highlighted image patches corresponding to top-ranked features. Secondly,\nwe design a new 2D image patch ranking algorithm that leverages the newly\ndefined feature matrices and relevant statistical data from numerous heatmaps to\nreliably rank axial patches, coronal patches, and sagittal patches. Thirdly, we create\na novel 3D image block ranking algorithm to generate a “Block Ranking Map\n(BRM)” by using the axial patch ranking scores, coronal patch ranking scores, and\nsagittal patch ranking scores. Lastly, we develop a hybrid 3D image block ranking\nalgorithm to generate a reliable hybrid BRM by using different block ranking\nscores generated by the 3D image block ranking algorithm using different top feature\nsets. The associations between brain areas and a brain disease are reliably\ngenerated by using hybrid information from ChatGPT and relevant publications.\nThe simulation results using two different 3D data sets indicate that the novel hybrid\n3D image block ranking algorithm can identify top-ranked blocks associated\nwith important brain areas related to AD diagnosis and autism diagnosis. A doctor\nmay conveniently use the hybrid BRM with axial, coronal, and sagittal views\nto better understand the relationship between the top-ranked blocks and medical\ndiagnosis, and then can efficiently and effectively make a rational and explainable\nmedical diagnosis.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nAuthors present a method for modifying Grad-CAM feature attribution maps that is able to identify the most important 'blocks' in a 3D MRI brain image. The method is applied to output features of a trained CNN by sectioning each 3D image axis into an equal number of patches. The features from Grad-CAM are passed through a feature selection (FS) method consisting of a combination of standard library functions including recursive feature elimination (RFE). The resulting `k` ranked features are accumulated axis- (or patch-) wise, and the patches from different combinations of FS methods are passed into final step that aggregates the 2D features into 3D block features. The blocks that contain the most patches with highest ranking features are selected as the most important blocks. Authors evaluate their method by verifying that the selected blocks correspond with those that are know to be important in the literature, as well as asking Chat-GPT. Author's provide clinical reasoning to explain the attributions.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* The target of the Authors' work is important and moves towards a more explainable and trustworthy result for use by clinicians.\n* Authors are thorough in their definitions and attempt to give the reader the detail to reproduce their work.\n\n### Weaknesses\n\n* In general, this paper is badly formatted and overly verbose. The notation is difficult to keep consistent and often badly defined. Further comments to this are made in the `Questions` section below.\n* Authors spend very little time reviewing prior work and setting their method in context - Authors should add explanations of related prior work including methods for using statistical analysis of Grad-CAM attribution and saliency maps to identify significant regions in input data.\n* Authors inexplicably use Chat-GPT to test the reliability of their method by asking it to verify the important brain regions associated with AD - there is no explanation for this, and no reason to be doing that instead of actually asking clinicians.\n* Authors present their method only on a single 3D MRI dataset and do not discuss its applicability to other modalities, or indeed any other domain. Authors cannot claim that this is a method for \"Explainable Medical Imaging\" when results on a single dataset are reported.\n\n### Questions\n\n### Major Comments\n* The definitions and notation presented in the paper are cumbersome, and difficult to read. This starts with the statement `P (H-bar/H) x (W-bar/W) patches for P=HW` (line 92) that is repeated several times (187, 214 etc.) which could be replaced by a less obtuse definition of patch size. Authors also do not define _n_ in this Section 2. Authors should better define their variables. Additionally, the cumbersome notation in definitions 1-6 is largely unnecessary - Authors can simplify this section by removing superfluous 'definitions' and describing the process through which they yield the ranking matrices: this will avoid repetition of the `i` and `j` indices and shorten this bloated paper.\n* On a similar note, Authors introduce additional notation for the 'top feature map' `T^Q`. This terminology is confusing. It is not a top \"feature map\", but rather an aggregated \"top k features\" map combining the Grad-CAM features selected by 'some feature selection method'. Authors should consider re-wording this.\n* Lines 221-234 - Authors present their 8 steps for Image Block Ranking algorithm. This is presented badly - the reader is capable of understanding that the same steps are applied to the 3 different axis without making each step of this method so verbose. Figure 1 shows this much better in fact. Authors should describe steps 1-7 on a single axis to improve readability.\n* Authors show a visualization of important blocks identified by their method in Figure 2b. The full Grad-CAM output without feature selection is shown in Figure 3b. It is evident that applying some thresholding to the full feature map, and even just applying the brain-boundary regional constraints, would yield a similar map to their own. Can author's comment on the significant differences between using the Grad-CAM values directly in this way, rather than the additional steps in their method? It would have been helpful to show some quantitative comparison in their results given this is supposed to be an extension of Grad-CAM -based methods.\n* Why did Authors use Chat-GPT in their evaluations? What is the benefit of this over asking the clinicians which Authors repeated claim that this system is aimed at helping?\n* Have Authors used this method on other domains or modalities to demonstrate its effectiveness?\n\n### Minor Comments\n* The paragraph from lines 57 contains a lot of repetition and should be pared down: this sentiment is reflected in many parts of the paper. The reader is clear that having better explainability is important and that a clinician can use this information to inform their diagnosis.\n* Line 147 \"... makes a more impact on the decision\" - to what decision are the Authors referring?\n* Line 148 \"... we use a trained CNN to generate L heatmaps...\" - the language here is confusing: Authors are referring to the model on which they are performing Grad-CAM, not a random trained model that generates heatmaps.\n* Table 2 is unclear - why is much of the table blank?\n* Line 489 - this is confusing: what is meant by the 'the 9th ... patches are shown'?. There are 16x16 patches in each of the 3 axis images shown, and a single patch at (6, 10, 9) is highlighted.\n* Section 4.1 - it is absolutely unnecessary to bloat this section with the indicies that correspond to the brain boundary - put this in the appendix, or show it as an image if the Authors feel it adds to their explanation.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nI question the validity of using Chat-GPT in this work - but no significant Ethics concerns.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a method for enhancing Grad-CAM feature attribution maps to identify significant 'blocks' in 3D MRI brain images. The proposed approach segments each 3D image axis into patches and applies a feature selection technique, including recursive feature elimination (RFE), to rank features. The selected patches are consolidated into 3D block features based on their importance. The authors assess their method's validity by comparing the identified blocks to established critical brain regions in the literature and by consulting Chat-GPT for additional verification. While the contribution towards improving explainability in medical imaging is notable, the manuscript's clarity and organization require significant enhancement.\n\n**Major Comments:**\n1. The definitions and notation used throughout the paper are cumbersome and inconsistent, creating unnecessary complexity. The notation in line 92, repeated throughout, could be simplified for clarity. Additionally, variables such as _n_ in Section 2 remain undefined.\n2. The term 'top feature map' `T^Q` leads to confusion; it should accurately represent 'aggregated top k features' derived from Grad-CAM.\n3. The explanation of the Image Block Ranking algorithm is overly verbose; a concise description focusing on a single axis would enhance readability.\n4. There is a lack of quantitative comparison between the proposed method and traditional Grad-CAM outputs, which is essential given that this work claims to extend Grad-CAM methodologies.\n5. The rationale for utilizing Chat-GPT for evaluations is unclear. Direct consultations with clinicians would be more fitting for the method's intended purpose.\n6. There is no discussion of the method's applicability to other modalities or domains beyond the presented dataset.\n\n**Minor Comments:**\n- The paragraph on lines 57 is repetitive and could be condensed to enhance clarity.\n- The statement in line 147 regarding 'impact on the decision' is vague and requires specification.\n- The phrase in line 148 is misleading; the model referred to should be clearly defined as the one under Grad-CAM analysis.\n- Table 2 is unclear due to many blank entries; clarification is needed.\n- Line 489's reference to 'the 9th ... patches are shown’ is confusing and requires adjustment.\n- Section 4.1 includes excessive details regarding indices related to the brain boundary, which may be better suited for an appendix or visual representation.\n\n**Summary Paragraph:**  \nThis manuscript addresses significant challenges in explainable medical imaging and aims to provide methods that enhance the interpretability of CNN outputs. However, there are critical weaknesses, including issues with notation, a lack of thorough analysis of related work, and an unclear evaluation design. While the overall significance of the research is commendable, its innovation is limited by the narrow focus on a single dataset, which undermines reproducibility. The authors need to address these issues to strengthen their contribution.\n\n**Decision Recommendation:**  \nMajor revision is recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a method to modify Grad-CAM feature attribution maps for identifying the most important 3D “blocks” within MRI brain images. The approach partitions each 3D image into patches, applies recursive feature elimination and other selection methods to Grad-CAM outputs, and aggregates results across axes to identify key regions. The study compares these identified blocks to known disease-relevant regions and includes additional qualitative verification using ChatGPT and clinical interpretations. While the research target—enhancing interpretability of CNN outputs for clinical use—is valuable, the paper suffers from formatting and clarity issues that hinder comprehension and reproducibility.\n\n---\n\n**Major Comments**  \n1. **Notation and definitions:** The mathematical notation is cumbersome and inconsistently defined. Key variables (e.g., _n_) are missing, and Definitions 1–6 add unnecessary complexity. Simplifying notation and focusing on the process leading to ranking matrices would improve readability.  \n2. **Terminology clarity:** The “top feature map” \\(T^Q\\) terminology is misleading—it refers to aggregated top _k_ Grad-CAM features rather than a feature map. Rewording is recommended.  \n3. **Algorithm presentation:** The 8-step “Image Block Ranking” procedure is verbose and repetitive. Describing the process for one axis and referencing repetition across others would streamline presentation; Figure 1 already conveys this clearly.  \n4. **Evaluation design:** The comparison between the proposed method and direct Grad-CAM thresholding is lacking. Authors should explain whether the additional processing meaningfully improves interpretability, ideally through quantitative assessment.  \n5. **Validation approach:** Using ChatGPT for validation is inappropriate and unexplained. Clinician input would provide more credible evaluation for a medically oriented study.  \n6. **Generality and scope:** The method is tested on a single MRI dataset without discussion of applicability to other modalities or domains, limiting claims of generalizability.\n\n---\n\n**Minor Comments**  \n- Many paragraphs (e.g., line 57) are repetitive and could be condensed.  \n- Clarify “makes a more impact on the decision” (line 147) and specify what decision is referenced.  \n- Rephrase “we use a trained CNN to generate L heatmaps” (line 148) to specify that Grad-CAM is applied to a pre-trained network.  \n- Table 2 lacks clarity—explain blank cells.  \n- Line 489: clarify description of patch indices; only (6, 10, 9) is highlighted out of 16×16×16.  \n- Section 4.1 excessively details brain boundary indices; move to appendix or depict graphically.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n**Significance:** The goal of improving model explainability is important for clinical adoption.  \n**Innovation:** The concept of aggregating Grad-CAM outputs via feature selection is somewhat novel but not convincingly differentiated from simpler thresholding.  \n**Evaluation:** Current validation is weak and relies on ChatGPT rather than expert review or quantitative metrics.  \n**Reproducibility:** Although detailed, the overcomplicated notation and unclear definitions hinder reproducibility. Overall, the manuscript requires substantial revision before it can meet publication standards.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper attempt to provide a more explainable 3D GradCam map by combining ideas of GradCam in each of the respective projections (coronal, axial, sagittal) of MRI data with feature selection concepts. The feature maps produced in the maxpooling convolutional layers are used to derive various representations such as heatmap matrics, feature matrices, and the values within them are ranked individually per view and then combined into a 3D block ranking. The authors claim the resulting visualization gives better indication of the disease and demonstrate this on ADNI data for Alzheimer's disease. \n\nThe paper is poorly written and many of the details seem to be automatically written through a translation software or perhaps LLM judging by the language used. For example, reading the abstract had a lot of details  that typically seen in results sections later rather than focusing on a high-level summary of the approach.  Another example is a sentence in line 164-165 which reads \"Different from traditional CAM-based methods without FS a new FS-Grad-CAM methods uses a FS method to select the top k features from m flattened features.\" - Is this referring to their proposed approach. Normally we would phrase it as \"Unlike traditional CAM-based methods, we propose a new method called FS-Grad_CAM where we employ a feature selection method to select the top K feature from m flattened features first before applying GradCAM.\" \n\n Many details are unclear including the novelty with respect to other 3DGradCam methods (see eblow). Overall, it needs a major rewrite and using clinically relevant terminology with better motivation of the healthcare problem addressed.\n\n### Soundness: 1\n\n### Presentation: 1\n\n### Contribution: 1\n\n### Strengths\n\nPaper is about explainable AI showing clinicians relevant features useful for diagnosis in the 3D MRI images by taking slices in multiple views and offering top-ranked patches that are correlated with disease and understand the relationship between the top blocks and the decisions made by 3D CNN. As such, the paper attempts an important problem, namely, making the disease classification more explainable to clinicians. The familiar mechanisms of GradCam are used and attempt is made to process complex MRI datasets in multiple views. If the method could be clearly explained, one could even see value in the technique for 3DGradCam in general although other 3D GradCam tools are available. The main argument appears to be that GradCam should be applied after feature selection in the feature maps.\n\n### Weaknesses\n\nAs mentioned above, the paper is poorly written to determine if the idea being proposed is a variation of 3D gradCam. No comparisons are made to any methods to even see its merit. Several open questions arise (see below). \nThere are several tools available for 3DGradCam such as these. If they are not relevant for comparison, it would be helpful to at least explain how your method differs from these. \nhttps://github.com/fitushar/3D-Grad-CAM\nhttps://www.researchgate.net/publication/357899396_Automated_grading_of_enlarged_perivascular_spaces_in_clinical_imaging_data_of_an_acute_stroke_cohort_using_an_interpretable_3D_deep_learning_framework\n\nThe paper in current problem needs a full rewrite starting with explanation of the MRI disease visualization problem, the role of existing 3D GradCam and the need for feature selection prior to GradCam. The whole idea of class activation maps was to allow us to see the rationale for the classification with the visualization itself in a way doing regional feature selection. By applying a separate feature selection operator a priori, what would be the impact on the gradient operators and the resulting activation maps?\n\nChatGPT is briefly mentioned and it is not clear what it is being used for.\nWhat does it mean to say ChatGPT is used to verify if a brain area is associated with a disease? What is the prompt used? What are the input, only text or text and image, is a bounding box and a prompt given as input. How accurate is ChatGPT in identifying  the brain areas associated with the disease. All these should be added to explain the use of ChatGPT.\n\n### Questions\n\n1. What is the rationale for a top-ranked 3Dimage block being correlated with diseases? Real-life experiences with 2d heat maps alone indicate they are not always a reliable indicator of a disease. Since a disease may be seen in some view better than other views, the method of fusion is important between the 3 views. Provide more discussion or justification for the correlation between top-ranked blocks and diseases, and the fusion method would help clarify better. \n\n2. How is the ranking of the blocks done? In general, what does ranking mean in your context, is it just a matter of selecting high-valued entries in the feature and heat map matrices?\n\n3. What is the purpose of ChatGPT in the work? It is said it is to verify if a brain area is associated with a disease. How does it work? Are both prompt text and image supplied as input? How accurate is ChatGPT in identifying  the brain areas associated with the disease. This discussion is brief and unconvincing so elaborating on how exactly ChatGPT is used and what its inputs are would explain this section better.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript aims to enhance the interpretability of 3D GradCam maps by integrating GradCam with feature selection techniques across different MRI projections (coronal, axial, sagittal). The authors claim that their visualization approach improves the understanding of Alzheimer's disease by analyzing data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, the paper suffers from significant clarity issues and appears poorly written, which undermines its overall quality.\n\n### Major Comments\n1. **Clarity and Writing Quality**: The paper contains many sentences that seem to be generated through translation software, as seen from the abstract's structure and grammar, which lacks a high-level summary.\n2. **Novelty**: The authors do not clearly articulate how their approach differs from existing 3D GradCam methods. Comparisons with established tools are missing.\n3. **Explanation of Methodology**: There is insufficient discussion regarding how feature selection affects the GradCam outputs and the rationale behind their approach.\n4. **Use of ChatGPT**: The role of ChatGPT in verifying disease association with brain areas is unclear, including its input requirements and accuracy.\n\n### Minor Comments\n- The terminology used throughout the paper often deviates from clinically relevant language, confusing the reader.\n- A specific sentence (lines 164-165) lacks clarity and should be rewritten for better understanding. \n- References to established 3D GradCam tools should be more explicit and detailed.\n- The description of the correlation between top-ranked 3D image blocks and disease lacks supporting discussion.\n\n### Summary Paragraph\nIn terms of the TMI editorial criteria, the manuscript addresses an important healthcare problem concerning explainable AI in clinical settings. However, the significance of the proposed technique is diminished by writing quality issues and lack of clarity about its novelty and methodology. In terms of reproducibility, there are significant gaps in explaining the ranking mechanisms and the role of ChatGPT. Overall, the paper requires substantial revisions to improve its clarity, comprehensiveness, and connection to existing research.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes an explainable AI framework intended to generate more interpretable 3D Grad-CAM visualizations for MRI data by combining projection-wise Grad-CAM maps (axial, coronal, sagittal) with feature selection. The approach ranks activations within convolutional layers per view and fuses them into a 3D block ranking to indicate disease-relevant regions, demonstrated on ADNI data for Alzheimer’s disease. While the topic—improving interpretability of 3D CNNs in medical imaging—is important, the paper is poorly written, with unclear methodological descriptions and evident linguistic and organizational issues that hinder comprehension.\n\n**Major Comments**  \n1. **Clarity and Writing Quality:** The manuscript appears to suffer from translation or automated text generation issues, leading to confusing or grammatically incorrect statements. The structure and flow do not follow conventions; for instance, the abstract contains result details rather than an overview, and technical sections are ambiguously phrased.  \n2. **Novelty and Relation to Prior Work:** The distinction from existing 3D Grad-CAM methods is unclear. The paper should explicitly compare or conceptualize differences with prior tools such as publicly available 3D Grad-CAM implementations.  \n3. **Methodological Explanation:** The process of feature selection before Grad-CAM is inadequately described. The rationale for combining feature selection with Grad-CAM and how this affects gradient-based activation mapping remains unexplained.  \n4. **Use of ChatGPT:** The inclusion of ChatGPT for verifying disease-region associations is insufficiently justified. The paper must clarify the inputs (text, image, prompts) and discuss the validity and purpose of this step.  \n5. **Experimental Evaluation:** No baseline comparisons or quantitative evaluation are provided. The claimed interpretability improvement is not demonstrated against existing methods.  \n6. **Scientific Rationale:** The claim that top-ranked 3D blocks correlate with disease lacks theoretical or empirical justification. Clarification is needed on the block ranking procedure and fusion across multiple MRI views.\n\n**Minor Comments**  \n- Numerous language errors and mistranslations throughout; a full rewrite is recommended.  \n- Use consistent and clinically appropriate terminology when describing MRI slices and disease visualization.  \n- Clarify notation (e.g., k, m, feature maps) and reformat equations and sentences for readability.\n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The goal of improving interpretability of deep learning for Alzheimer’s MRI analysis is valuable.  \n*Innovation:* Combining Grad-CAM with feature selection could be novel, but the implementation and differentiation from prior work are unclear.  \n*Evaluation:* No solid experimental validation or comparison is presented, limiting confidence in the method’s utility.  \n*Reproducibility:* The current text lacks sufficient methodological detail for replication. Major reorganization and clearer exposition are required.\n\n**Decision Recommendation**  \n**Major Revision** – The manuscript requires substantial rewriting for clarity, complete methodological explanation, meaningful comparison to existing approaches, and validation before it can be properly evaluated.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a feature-selected (FS) Grad-CAM method to generate more focused explainable heatmaps with smaller highlighted areas. Additionally, a novel 2D image patch ranking algorithm was developed to reliably rank patches along the axial, sagittal, and coronal axes using features extracted by FS-Grad-CAM. These ranked scores are then used to create a Block Ranking Map (BRM) via a newly developed 3D block ranking algorithm. The resulting block-ranked scores are further refined through a novel hybrid 3D block ranking algorithm to produce a reliable hybrid BRM. The method was validated on Alzheimer’s Disease (AD) data and identified the top 10 ranked blocks associated with AD.\n\n### Soundness: 1\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper used the 2D images in axial, sagittal and coronal axes to rank the 3D images, which makes the research novel.  This paper has detailed explanation on the algorithm, which makes the method replication easier.\n\n### Weaknesses\n\n1. Lack of competition: The cited papers (He et al., 2019 and Selvaraju et al., 2017) in the introduction part have compared their new algorithm (Grad-CAM) with other common machine learning algorithm (e.g. logistic regression) to demonstrate their better performance in the binary classification. But this paper doesn’t have compared with any baseline to demonstrate its superiority. There exists 3D medical imaging visual explanation algorithm (e.g. Respond-CAM) which might be a good benchmark to compare with.\n2. Lack of generality: The paper only evaluated on one dataset, which cannot guarantee the generality of the proposed method. More tests are needed to justify the statement.\n\n### Questions\n\n1. It would be great if any qualitative/quantitative comparison with similar algorithm could be added to help the readers to better evaluate the performance of the proposed method.\n2. It would be ideal if more dataset could be used to evaluate the performance of the proposed method (e.g. LUNA 16 lung nodule dataset)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript presents a novel approach by introducing a feature-selected (FS) Grad-CAM method aimed at generating more precise explainable heatmaps with smaller highlighted areas. In addition, it develops a new 2D image patch ranking algorithm to rank patches across axial, sagittal, and coronal axes using features extracted by FS-Grad-CAM. The authors further enhance this approach with a Block Ranking Map (BRM) generated through a 3D block ranking algorithm, which is refined by a hybrid 3D block ranking algorithm. The method is validated on Alzheimer’s Disease (AD) data, showing promise in identifying significant blocks related to the disease.\n\n**Major Comments:**\n1. **Lack of Competition:** The manuscript does not compare the proposed algorithm with established baselines such as Respond-CAM, which hinders the assessment of its competitive edge. Relevant citations (He et al., 2019; Selvaraju et al., 2017) illustrate comparisons in their works that highlight the importance of benchmarking against common machine learning algorithms.\n2. **Lack of Generality:** Testing is confined to a single dataset, raising concerns about the broader applicability of the proposed method. Expansion of the evaluation to include multiple datasets is necessary to substantiate the claims made.\n\n**Minor Comments:**\n- Consider adding qualitative or quantitative comparisons to enhance the evaluation of the proposed method.\n- Additional datasets like the LUNA 16 lung nodule dataset would be beneficial for performance assessment.\n- The explanation of the algorithm is detailed, which aids in replication.\n\n**Summary Paragraph:**  \nIn accordance with the TMI editorial criteria, the manuscript shows innovation through its unique ranking methods and focused explainable heatmaps. However, it falls short on evaluation and generality by not benchmarking against existing methods and limiting testing to a single dataset, which affects its reproducibility. Addressing these weaknesses could enhance the manuscript's overall significance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a feature-selected Grad-CAM (FS-Grad-CAM) approach aimed at generating more localized and interpretable heatmaps. Building on this, the authors develop a 2D image patch ranking algorithm that orders patches across axial, sagittal, and coronal planes and integrates these rankings into a 3D Block Ranking Map (BRM) using a hybrid 3D block ranking algorithm. The work is applied to Alzheimer’s disease (AD) imaging data to identify the top-ranked brain regions associated with AD. The manuscript is generally clear in describing the algorithmic steps and provides sufficient methodological details to facilitate reproducibility.  \n\n**Major Comments**  \n1. **Lack of Comparative Evaluation:** The study does not include comparisons against baseline methods or existing 3D medical imaging explainability algorithms (e.g., Respond-CAM). Prior works, such as He et al. (2019) and Selvaraju et al. (2017), benchmarked their methods against standard classifiers, demonstrating clear advantages. Similar comparative analysis would strengthen this paper’s claims.  \n2. **Limited Dataset and Generality:** The method is evaluated only on a single Alzheimer’s disease dataset, which limits assessment of its generalizability. Validation on additional datasets would provide stronger evidence for the robustness of the approach.  \n\n**Minor Comments**  \n- The explanation of the method is detailed and supports reproducibility.  \n- Including qualitative and quantitative comparisons with existing algorithms would improve the clarity of the method’s relative performance.  \n- Testing on additional datasets, such as the LUNA16 lung nodule dataset, could enhance the paper’s scope.  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the paper tackles an important problem in medical image explainability, specifically for 3D data. The **innovation** lies in integrating 2D feature selection with 3D block ranking for improved interpretability. Regarding **evaluation**, the absence of baseline comparisons and use of a single dataset weaken evidence of superiority and generalizability. In terms of **reproducibility**, the methodological description is comprehensive, supporting replication. Overall, while the concept is original, the evaluation requires significant strengthening.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes a hybrid 3D image block ranking method for explainable medical imaging by combining axial, coronal, and sagittal 2D image patch rankings. The authors develop a new FS-Grad-CAM method that applies feature selection before generating heatmaps, define informative feature matrices (feature accumulation, distribution, ranking matrices) and heatmap matrices (count, activation, strength matrices), and create a 2D patch ranking algorithm using these matrices. The method extracts 2D slices from 3D images in three orientations, ranks patches in each orientation, then combines these rankings to generate Block Ranking Maps (BRMs). Experiments on ADNI dataset (982 3D brain images) for Alzheimer's disease diagnosis and autism dataset (286 images) demonstrate that top-ranked blocks correspond to brain regions associated with respective diseases according to literature and ChatGPT verification.\n\n## Weaknesses\n\n• **Mathematical formulation inconsistencies and notation clarity issues**\n  - Definition 6 is duplicated (pages 3-4, lines 155 and 163), creating confusion about which definition applies to which matrix\n  - Equation (2) uses undefined notation tᵢⱼᵍ while the text refers to tᵢⱼᵍ, creating ambiguity about which feature selection maps are being used\n  - The function f(θⱼₖ, θᵢₖ, θᵢⱼ) in Algorithm 2 is never explicitly defined, making the core block ranking computation unclear\n  - Matrix dimension specifications are inconsistent, with some matrices described as H×W while others lack clear dimensional constraints\n\n• **Insufficient experimental validation and comparison baselines**\n  - No comparison with existing 3D explainability methods beyond brief mentions of 3DGradCAM (page 4, line 184), limiting assessment of relative performance\n  - Validation relies primarily on ChatGPT and literature verification rather than quantitative metrics or clinical expert evaluation (Tables 1-2, pages 8-10)\n  - Only two datasets tested with relatively small sample sizes (982 and 286 images), insufficient for robust validation of the complex multi-step pipeline\n  - No ablation studies to assess contribution of individual components (FS selection, different orientations, hybrid averaging)\n\n• **Methodological complexity without clear justification for design choices**\n  - The 8-step framework (Figure 1, page 5) involves numerous sequential processing stages without analysis of computational complexity or failure modes\n  - Multiple arbitrary hyperparameter choices (k=250, k=100 features, specific patch elimination rules in Appendix A) lack systematic justification\n  - The hybrid averaging approach in Algorithm 3 (page 6) provides no theoretical foundation for why averaging multiple ranking matrices improves reliability\n  - Feature selection rules appear ad-hoc and dataset-specific (Appendix A), raising questions about generalizability\n\n• **Limited technical novelty and unclear contributions over existing methods**\n  - Core concepts (feature selection, Grad-CAM, patch ranking) are well-established; primary contribution appears to be combining them across orientations\n  - The \"informative feature matrices\" (Definitions 2-6, pages 2-3) are straightforward extensions of standard feature analysis techniques\n  - Validation methodology using ChatGPT responses (pages 7-9) lacks scientific rigor compared to established medical imaging evaluation protocols\n  - No clear demonstration that the multi-orientation approach provides advantages over simpler alternatives\n\n## Suggestions for Improvement\n\n• **Resolve mathematical formulation issues and improve notation consistency**\n  - Remove the duplicate Definition 6 and ensure all definitions are uniquely numbered and clearly distinguish between different matrix types\n  - Provide explicit mathematical definition of the ranking function f(θⱼₖ, θᵢₖ, θᵢⱼ) with concrete examples of how block scores are computed\n  - Standardize notation throughout the paper, ensuring consistency between equation symbols and text references\n  - Add dimensional analysis for all matrices and clearly specify input/output dimensions for each algorithm step\n\n• **Strengthen experimental validation with comprehensive comparisons and quantitative metrics**\n  - Include quantitative comparison with existing 3D explainability methods (3D-CAM, 3D-LIME, direct 3D CNN interpretation) on the same datasets\n  - Replace ChatGPT validation with clinical expert evaluation or established medical imaging benchmarks for explainability assessment\n  - Expand evaluation to larger, more diverse datasets and include statistical significance testing of results\n  - Conduct thorough ablation studies examining individual component contributions and sensitivity to hyperparameter choices\n\n• **Simplify methodology and provide theoretical justification for design decisions**\n  - Reduce pipeline complexity by eliminating redundant steps and provide computational complexity analysis for the entire framework\n  - Develop principled methods for hyperparameter selection (feature counts, thresholds) with cross-validation or theoretical guidance\n  - Provide mathematical justification for why hybrid averaging improves ranking reliability, possibly through ensemble theory or bias-variance analysis\n  - Design generalizable feature selection criteria that work across different anatomical structures and imaging modalities\n\n• **Demonstrate clear technical advances and practical impact over existing approaches**\n  - Clearly articulate novel technical contributions beyond combining existing methods and provide theoretical analysis of when multi-orientation approaches are beneficial\n  - Develop more sophisticated feature matrix definitions that capture clinically relevant patterns beyond basic statistical measures\n  - Establish rigorous medical validation protocols involving clinical experts and compare against established diagnostic methods\n  - Provide concrete evidence of practical utility in clinical decision-making scenarios with user studies or clinical workflow integration",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThis manuscript presents a novel hybrid 3D image block ranking method aimed at enhancing explainability in medical imaging. By integrating axial, coronal, and sagittal 2D image patch rankings, the authors introduce the FS-Grad-CAM method, which involves a systematic approach to feature selection and the creation of heatmaps. The proposed method, validated through experiments on Alzheimer’s disease and autism datasets, indicates promising results in identifying brain regions associated with respective conditions.\n\n**Major Comments:**\n1. **Mathematical formulation inconsistencies and notation clarity:**\n   - Definition 6 appears twice (pages 3-4, lines 155 and 163), leading to confusion.\n   - Equation (2) includes undefined notation tᵢⱼᵍ, creating ambiguity about feature selection maps.\n   - The function f(θⱼₖ, θᵢₖ, θᵢⱼ) in Algorithm 2 lacks explicit definition.\n   - Inconsistent specifications for matrix dimensions hinder clarity.\n\n2. **Insufficient experimental validation and comparison baselines:**\n   - Limited comparisons to 3D explainability methods restrict performance assessment.\n   - Validation relies on non-quantitative approaches (e.g., literature and ChatGPT) rather than metrics or expert evaluations.\n   - The small sample size of datasets tested fails to ensure robust validation.\n   - Absence of ablation studies to evaluate individual component contributions.\n\n3. **Methodological complexity without justification:**\n   - The 8-step framework lacks analysis of computational complexity and potential failure modes.\n   - Hyperparameters (e.g., k=250) have arbitrary justifications, lacking a systematic approach.\n   - No theoretical foundation is provided for the hybrid averaging approach in Algorithm 3.\n   - Feature selection rules are dataset-specific, raising generalizability concerns.\n\n4. **Limited technical novelty and unclear contributions:**\n   - Suggestions of combining well-established concepts offer moderate novelty.\n   - \"Informative feature matrices\" appear as extensions of existing techniques.\n   - Validation methodology with ChatGPT lacks rigor compared to established protocols.\n   - No evidence that the multi-orientation approach outperforms simpler methods.\n\n**Minor Comments:**\n- Clarification on the duplicate Definition 6 is needed.\n- Provide a concrete definition for the ranking function mentioned for improved clarity.\n- Ensure uniform notation across text and equations.\n- Specify input/output dimensions for algorithms.\n\n**Summary Paragraph:**\nThis study's significance lies in addressing explainability in medical imaging, though notable weaknesses exist in its novelty and validation. The evaluation framework lacks rigorous quantitative metrics and diverse datasets, which impacts reproducibility. While the methodological complexity reflects innovative approaches, the absence of theoretical justifications raises questions about reliability. Overall, despite its intent to enhance medical imaging interpretability, the manuscript requires substantial improvements regarding experimental validation and methodological clarity.\n\n**Decision Recommendation:**\nMajor Revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a hybrid 3D image block-ranking framework for explainable medical imaging, integrating axial, coronal, and sagittal 2D patch analyses into Block Ranking Maps (BRMs). A new FS-Grad-CAM technique introduces feature selection prior to heatmap generation, defining multiple “informative feature” and “heatmap” matrices. The study validates the method on Alzheimer’s disease (ADNI) and autism (ABIDE) datasets, reporting that the top-ranked regions correspond to disease-relevant brain areas according to prior literature and ChatGPT-assisted verification. While the paper is ambitious in scope, its execution suffers from mathematical ambiguities, limited experimental rigor, and unclear methodological novelty.  \n\n**Major Comments**  \n1. **Mathematical and Notational Inconsistencies:** Duplicate definitions (e.g., Definition 6) and undefined notation (e.g., \\(t_{ij}^g\\)) undermine clarity. Several functions and dimensions are unspecified, leaving the core ranking formulation ambiguous.  \n2. **Weak Experimental Validation:** Evaluation relies primarily on ChatGPT checks rather than quantitative or clinician-based validation. Comparisons with existing 3D explainability methods are superficial, and sample sizes (982, 286 images) are modest for such a complex pipeline. No ablation studies assess the contribution of individual components.  \n3. **Methodological Complexity Without Justification:** The multi-step pipeline includes numerous unmotivated hyperparameters and lacks computational complexity analysis. The hybrid averaging approach has no theoretical basis, and feature selection criteria appear ad hoc and dataset‑specific.  \n4. **Limited Novelty and Contribution Clarity:** The proposed approach primarily fuses established techniques (Grad-CAM, feature selection, patch ranking) without clear innovation or evidence that multi‑orientation fusion yields consistent performance gains. Validation using textual evidence or language models lacks scientific rigor.  \n\n**Minor Comments**  \n- Ensure consistent notation and dimension specifications across equations and matrices.  \n- Provide explicit definitions for all functions and parameters in algorithms.  \n- Eliminate redundant definitions and improve figure and algorithm labels for ease of reference.  \n- Include more precise dataset descriptions and standardize table labeling.  \n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe topic—explainable 3D medical image interpretation—is significant, yet the presented method’s innovation and rigor are limited. Methodological complexity and inconsistent notation hinder reproducibility. Evaluation design lacks quantitative comparisons and expert validation, reducing confidence in the claimed contributions. While the idea of integrating multi‑orientation patch rankings is interesting, the work requires clearer theoretical grounding and stronger empirical evidence to meet publication standards.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n   ✅ The manuscript proposes a methodological contribution directly relevant to medical imaging — a novel *3D image block ranking framework* integrating axial, coronal, and sagittal patch-based ranking derived from CNN feature maps. The focus on explainable 3D imaging and visualization aligns with TMI’s methodological scope.\n\n2. **Novelty & Contribution Level**  \n   The concept of using 2D Grad-CAM-derived patch rankings across three anatomical planes and integrating them into a 3D hybrid Block Ranking Map (BRM) appears to be a new formulation. However, the degree of *conceptual novelty* beyond extensions of existing Grad-CAM and feature selection (FS) techniques is modest. The “FS-Grad-CAM” method merges known tools (Grad-CAM + feature selection), and the hybrid block ranking algorithm is algorithmically clear but largely heuristic (using monotonic functions without mathematical justification or optimization).  \n   The manuscript introduces multiple matrix definitions but lacks comparative or theoretical analysis demonstrating why this framework would generalize beyond the evaluated datasets. Hence, novelty is present but incremental.\n\n3. **Technical and Experimental Rigor**  \n   - The mathematical formulation defines various intermediate matrices comprehensively but lacks formal validation of the aggregation and ranking functions \\(f(\\cdot)\\).  \n   - Experimental setup: uses ADNI and autism datasets, but classification performance is secondary; the main evaluation concerns interpretability. Verification of the method relies heavily on qualitative identification of “known brain areas,” partly using ChatGPT outputs, which is not scientifically rigorous.  \n   - No quantitative benchmark or established explainability metric (e.g., faithfulness, localization error) is reported.  \n   - Dataset usage and feature selection pipelines appear deterministic but might not be reproducible based on the description alone (e.g., precise parameter settings for RFE, Chi2 thresholds are missing).\n\n4. **Clarity and Presentation**  \n   The manuscript is generally well structured and grammatically adequate, with detailed steps and algorithm summaries. However, it is lengthy, repetitive, and reads more like technical documentation than a polished scientific article. Some notation is inconsistent (index symbols duplicated, undefined terms). Figures and tables help visualization but do not quantify improvement over baselines. The frequent reliance on ChatGPT as a “verification” source diminishes academic tone.\n\n5. **Ethical and Reproducibility Compliance**  \n   Public datasets (ADNI, open autism MRI dataset) are appropriately cited, but there is no explicit mention of IRB adherence or data anonymization (typically assumed for public data but worth stating). Reproducibility would benefit from code release; currently absent.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n### 1. Summary\nThe paper proposes a multi-plane explainable analysis framework for 3D medical imaging. It introduces:\n- A **Feature-Selection-enhanced Grad-CAM** (FS-Grad-CAM) to create sparser, more interpretable 2D activation maps.  \n- A **2D patch ranking algorithm** aggregating five factors (feature distribution, ranking, averages, activation, and strength matrices).  \n- A **3D image block ranking algorithm** that fuses axial, coronal, and sagittal patch rankings into a Block Ranking Map (BRM).  \n- A **Hybrid BRM** combining multiple feature-selection results for robustness.\n\nThe method is evaluated on Alzheimer’s disease (ADNI dataset) and autism MRI datasets, showing that top-ranked 3D blocks correspond to anatomically plausible disease-related regions.\n\n### 2. Strengths\n- Addresses a recognized gap in explainability for 3D CNNs in medical imaging.  \n- Provides a systematic formulation linking 2D and 3D representations.  \n- Includes implementation detail (matrix formulations, pseudo-code).  \n- Evaluates on two distinct neurological conditions, demonstrating cross-dataset feasibility.  \n- Potentially useful visualization concept for clinicians (multi-view BRM).\n\n### 3. Weaknesses\n- Methodological innovation is limited — primarily a structured integration of existing explainability tools rather than a fundamentally new theory.  \n- Evaluation mainly qualitative; lacks quantitative explainability comparison (e.g., to 3D Grad-CAM, Layer-wise Relevance Propagation).  \n- Dependence on ChatGPT to “verify” brain–disease associations is inappropriate for a scientific claim. Validation should rely solely on peer-reviewed sources.  \n- The ranking functions \\(f(\\cdot)\\) are undefined, making reproducibility unclear.  \n- No measures of uncertainty, reproducibility, or statistical validation across folds or subjects.  \n- Overuse of tabular anatomical listings distracts from methodological focus.  \n- Text presentation and writing style are verbose and repetitive.\n\n### 4. Major Comments\n1. **Clarify the mathematical framework:**  \n   Explicitly define the functional form of the ranking functions \\(f(\\cdot)\\) in Algorithms 1–3 and justify why they yield better interpretability or stability.\n2. **Provide quantitative evaluation:**  \n   Compare the proposed hybrid BRM to baseline explainability techniques (e.g., Grad-CAM, 3DGrad-CAM, SmoothGrad) using localization or faithfulness metrics.\n3. **Replace ChatGPT validation with scientific citation:**  \n   Validation of brain–disease relationships must rely on established literature, not conversational AI outputs.\n4. **Reproducibility and code release:**  \n   Include sufficient implementation detail (or a repository) for replication of FS, ranking, and aggregation procedures.\n5. **Scope shedding:**  \n   If the intended contribution lies in interpretability, emphasize methodological grounding rather than listing anatomical findings.\n\n### 5. Minor Comments\n- Revise notation for clarity (avoid overlapping indices \\(i,j,k,H,W,D\\)); ensure each matrix has defined dimensions.  \n- Simplify or compress repetitive definitions (Definitions 2–8).  \n- Improve figure legibility; add color bars and consistent scales.  \n- Proofread for punctuation and spacing (e.g., “A N EW 3D…” in title likely artifact of OCR).  \n- Clarify dataset splits (subject-level vs slice-level) to avoid data leakage.  \n- Explicitly cite software packages and version numbers (e.g., scikit-learn, Python).\n\n### 6. Overall Recommendation  \n**Major Revision**  \nThe manuscript introduces an interesting concept for explainable 3D medical imaging, but substantial methodological clarification and scientifically rigorous validation are required before meeting IEEE TMI standards. Without quantitative evidence and theoretical justification of the ranking framework, acceptance is premature.\n\n### 7. Confidence Level  \n**4/5** – confident in evaluation of methodological rigor and journal-scope alignment.\n\n---\n\n**Summary Statement for the Editorial Board:**  \nThe paper presents a structured though largely heuristic extension of Grad‑CAM combined with feature selection for 3D imaging explainability. Conceptually relevant to TMI but currently below publication standard due to lack of quantitative validation and reliance on non‑scientific verification sources. With stronger methodological justification and comparative metrics, it could evolve into a meaningful methodological contribution.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a methodological framework for explainable 3D medical imaging, introducing a multi‑plane *Feature‑Selection–enhanced Grad‑CAM* (FS‑Grad‑CAM) approach and a 3D image block ranking method. The authors aim to integrate 2D Grad‑CAM patch rankings across axial, coronal, and sagittal planes to form a 3D Block Ranking Map (BRM) and its hybrid variant. The work targets improved interpretability in disease‑related brain imaging, evaluated on Alzheimer’s (ADNI) and autism MRI datasets. The topic aligns with *IEEE TMI*’s methodological scope, and the manuscript is clearly organized but lengthy and occasionally repetitive.\n\n---\n\n**Major Comments**  \n1. **Mathematical formulation:** The ranking functions \\( f(\\cdot) \\) in the algorithms are undefined and lack theoretical or empirical justification. The aggregation process remains heuristic; additional derivation or rationale is needed for interpretability claims.  \n2. **Quantitative evaluation:** The study relies mainly on qualitative anatomical plausibility. Quantitative comparisons with baseline explainability methods (e.g., 3D Grad‑CAM, LRP, SmoothGrad) using standard faithfulness or localization metrics are required to support performance claims.  \n3. **Validation sources:** Reliance on ChatGPT to verify brain‑region associations is inappropriate for academic validation. Verification should be supported by peer‑reviewed literature.  \n4. **Reproducibility:** Key implementation details such as parameter settings for feature selection (RFE, Chi2 thresholds), training splits, and random seeds are missing. Providing code or a reproducibility package would be essential.  \n5. **Scope and emphasis:** The manuscript devotes excessive space to listing anatomical findings instead of focusing on methodological innovation and analysis of robustness.\n\n---\n\n**Minor Comments**  \n- Harmonize and clarify notation (avoid overlapping indices and undefined variables).  \n- Simplify repetitive matrix definitions (Definitions 2–8).  \n- Improve figure readability with labeled scales and color bars.  \n- Edit for brevity and grammar; remove typographical artifacts (e.g., “A N EW 3D…”).  \n- State dataset split strategy clearly to prevent potential leakage.  \n- Document software libraries and version numbers used.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The work addresses a recognized challenge in explainable 3D CNNs for medical imaging.  \n*Innovation:* Conceptually interesting but incremental, extending existing Grad‑CAM and feature‑selection concepts rather than establishing a new theoretical foundation.  \n*Evaluation:* Current validation is qualitative and lacks quantitative rigor or statistical analysis across datasets.  \n*Reproducibility:* Although public data are used, insufficient methodological detail and absent code limit reproducibility. With added rigor and clearer justification of algorithms, the contribution could become more impactful.\n\n---\n\n**Decision Recommendation:** **Major Revision** — The study presents a potentially useful visualization approach but requires substantial methodological clarification, quantitative evaluation, and academically robust validation before it is suitable for publication in *IEEE TMI*.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript proposes a novel methodological contribution to medical imaging through a *3D image block ranking framework* that integrates axial, coronal, and sagittal patch-based ranking derived from CNN feature maps. This focus on explainability in 3D imaging aligns well with the methodological interests of the IEEE Transactions on Medical Imaging (TMI). While the framework introduces an interesting concept, its overall contribution exhibits modest novelty when viewed against existing Grad-CAM and feature selection techniques.\n\n### Major Comments\n1. **Scope & Relevance to TMI**: The proposed framework develops a new formulation but primarily builds on existing concepts, resulting in incremental novelty.\n2. **Novelty & Contribution Level**: The integration of Grad-CAM and feature selection appears mainly heuristic, lacking extensive theoretical justification or optimization.\n3. **Technical and Experimental Rigor**: \n   - The manuscript does not validate the aggregation and ranking functions comprehensively.\n   - The evaluation of the method relies heavily on qualitative assessments of known brain areas, which decreases scientific rigor.\n   - There are no quantitative benchmarks or established explainability metrics reported.\n   - Dataset usage and feature selection processes lack enough detail for reproducibility.\n4. **Clarity and Presentation**: While the manuscript is generally well-structured, its lengthy presentation and repetitive nature hinder its readability. Notation inconsistencies and reliance on ChatGPT for verification diminish its academic quality.\n5. **Ethical and Reproducibility Compliance**: Although public datasets are cited, the manuscript does not explicitly address IRB compliance or data anonymization, and it lacks a code release for reproducibility.\n\n### Minor Comments\n- Clarify overlapping notation and ensure all matrix dimensions are defined.\n- Reduce repetition in definitions and improve the legibility of figures by adding color bars.\n- Proofread to correct punctuation and spacing errors.\n- Specify dataset splits to prevent data leakage.\n- Include citations for software packages used.\n\n### Summary Paragraph\nThe manuscript addresses a significant gap in the explainability of 3D CNNs in medical imaging, presenting a systematic formulation that connects 2D and 3D representations. However, its methodological innovation is constrained largely to the integration of existing tools rather than introducing new theories. The evaluation predominantly relies on qualitative assessments, and there is a critical absence of quantitative validation. Moreover, clarity and reproducibility could be improved through better presentation and detailed methodology.\n\n### Decision Recommendation\n**Major Revision**  \nThe manuscript presents an intriguing concept for explainable 3D medical imaging, but substantial revisions are required to enhance methodological clarity and scientific validation before it meets IEEE TMI standards. \n\n### Confidence Level\n**4/5** – confident in the evaluation of the manuscript's methodological rigor and its relevance to the journal's scope.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Luna Zhang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_727c34225dfc91a5f7681403d6688e41dc3bcb20.pdf",
    "remote_url": "https://openreview.net/pdf/727c34225dfc91a5f7681403d6688e41dc3bcb20.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Build your own cell: Diffusion Models for Multichannel 3D Microscopy Image Generation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Tolga"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "3D Diffusion Models"
    ],
    "abstract": "Three-dimensional (3D) cellular morphology is a critical indicator of cellular function, disease states, and drug responses. However, capturing and interpreting the complex relationships between cell shape, treatment conditions, and their biological implications remains a challenge. To address this, we present \"Build Your Own Cell'' (BYOC), a multichannel 3D generative framework that combines vector quantisation and diffusion models to synthesise biologically realistic 3D cell structures. BYOC captures intricate morphological changes induced by different drug treatments, enabling high-throughput in silico simulations and screening of cell shapes in response to varied conditions. This novel framework represents a significant step towards accelerating pre-clinical drug development by synthesising high-resolution, biologically realistic 3D cells, potentially reducing reliance on labour-intensive experimental studies. By ensuring phenotypic consistency between cell and nucleus volumes through joint modelling, BYOC provides high-fidelity reconstructions that could facilitate downstream analyses, including drug efficacy evaluation and mechanistic studies.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces a multi-channel 3D diffusion model designed for generating two-channel cell images from volumetric fluorescence microscopy data. By focusing on the coupling of the two channels within the diffusion process, the model aims to improve the quality of generated dual-channel 3D cell images. The results presented show an improvement over the current state-of-the-art in this area.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- Addresses a challenging and pertinent problem in the field of biomedical microscopy, specifically in cellular imaging.\n- The overall motivation behind the proposed methodological enhancements is generally clear.\n- The experimental outcomes demonstrate promising improvements over existing methods.\n\n### Weaknesses\n\n- The biological rationale behind the model is not thoroughly convincing or well-articulated.\n- Some concrete methodological choices lack clear motivation or detailed explanation, leading to potential confusion (e.g. a clear motivation why and how to use VQGANs would be nice).\n- Some details are missing or inadequately explained in the formal equations and overall framework.\n- The manuscript tends to be imprecise in its language, which affects clarity and understanding.\n- The conclusion lacks specificity regarding the contributions, limitations and future directions of the methods-aspects of the work.\n\n### Questions\n\n1. How could biological or mechanistic understanding arise from generative models in your context? Can you expand and provide a stronger motivation for this idea?\n2. You mention that \"GANs often struggle with generating coherent latent representations.\" Since GANs do not inherently produce latent representations in the same way as e.g. Variational Autoencoders, could you clarify what \"coherent latent representations\" means in the context of GANs, and how this specifically relates to your proposed method's advantages?\n3. The claim that multiple color channels can be treated as distinct modalities is not clearly explained in my opinion but is crucial to the suggested method. Do you have examples from related work where color channels have been treated as distinct modalities? Could you explain the biological basis for considering cell and nucleus channels as separate modalities?\n4. In Equation 1, are the variables h,w,d the same dimensions as H,W,D? If not, what is their relationship? Similarly, in Equation 2, the depth dimension d seems to be omitted—was this intentional or a typo? Please add a brief explanation of these variables and their relationships directly after the equations.\n5. How does the simultaneous recovery of both channels relate specifically to latent diffusion? Can you provide a specific example or illustration of how the simultaneous recovery process works in your model, and how it differs from standard latent diffusion approaches?\n6. What is the reason for using unquantized embeddings in your framework? If they drift from the codebook vectors, how does this affect the model, and what is the underlying motivation?\n7. In Equation 12, the variable t should be defined. Additionally, in Equation 13, what exactly is μ_θ_cn(⋅) computing—only the mean or is there an associated variance? If not, what is the variance of your Gaussian?\n8. There is an existing WNet in medical imaging literature [1]. To avoid confusion, would you consider renaming your model?\n9. Can you provide more precise details about your dual-channel 3D architecture, perhaps with references or a schematic in the supplementary material?\n10. On page 6, you state that attention mechanisms are \"strategically placed\" to focus on regions of interest. Could you elaborate on the strategy behind their placement and how regions of interest are determined?\n11. The numerical differences in Table 1 are hard to interpret without context. Could you explain or hint to what these differences mean in terms of image quality and their significance in your application? Can you provide a brief interpretation guide for the FID and MMD scores, perhaps indicating what range of differences would be considered significant in this context? You could also include a qualitative comparison of images corresponding to different score ranges to help readers understand the practical implications of these differences.\n12. Could you provide more context about the ResNet50 model used—for instance, what type of medical images it was trained on?\n\n**Additional Feedback for Improvement:**\n\n- In Figure 1, please explain what the rows and columns represent to enhance understanding.\n- In the introduction, you mention that \"single-cell data is often high-dimensional and inconsistent in size.\" Could you clarify whether this inconsistency refers to the images, cells, biological structures, or image resolutions?\n- In the Related Work section, the statement about discriminative frameworks needing a \"deep understanding of the underlying input distribution\" is unclear. Providing an example or reference could help clarify this point.\n- It might be beneficial to first introduce and describe the dataset before delving into implementation details like volume padding.\n- Please specify the size and resolution of the microscopy images. Are the single-cell images crops from larger stacks, or are they the direct output from the microscope?\n- Consider citing relevant works such as the 3D U-Net architecture [2] to situate your work within the existing literature.\n- The conclusion would be stronger if it discussed potential methodological developments and acknowledged limitations of the current approach.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript presents a multi-channel 3D diffusion model intended to generate two-channel cell images from volumetric fluorescence microscopy data. The model emphasizes the coupling of the two channels within the diffusion process, with the aim of enhancing the quality of generated dual-channel 3D cell images. The results indicate a notable improvement over existing methodologies in the field.\n\n**Major Comments:**\n1. The biological rationale for the proposed model is not articulated convincingly, which may undermine its relevance.\n2. Several methodological choices lack sufficient justification or detail, particularly the use of VQGANs.\n3. Important details are missing from the formal equations and overall framework, potentially leading to confusion.\n4. The language used throughout the manuscript is often imprecise, impacting clarity and comprehension.\n5. The conclusion fails to clearly outline the contributions, limitations, and future directions associated with the proposed methods.\n\n**Minor Comments:**\n- In Figure 1, it is essential to clarify what the rows and columns represent for better understanding.\n- In the introduction, clarification is needed regarding whether the inconsistency of \"high-dimensional data\" pertains to images, cells, biological structures, or image resolutions.\n- The statement regarding discriminative frameworks in the Related Work section needs elaboration or an example for clarity.\n- It would be beneficial to introduce and describe the dataset before discussing implementation details, such as volume padding.\n- Specify the size and resolution parameters of the microscopy images and clarify if single-cell images are crops or direct outputs from the microscope.\n- References to related works, such as the 3D U-Net architecture, would help position this study within existing literature.\n- Enhance the conclusion by discussing possible methodological advancements and acknowledging the limitations of the current approach.\n\n**Summary Paragraph:**\nThis study addresses a significant challenge in biomedical microscopy by developing an innovative approach for generating dual-channel 3D cell images. While the proposed model demonstrates potential for improved evaluation and quality, concerns regarding its biological rationale, clarity in methodology, and overall presentation may hinder reproducibility. The manuscript’s strengths lie in its novel contributions and promising results; however, weaknesses in detail and clarity need to be addressed to meet the editorial expectations of significance, innovation, evaluation, and reproducibility.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a multi-channel 3D diffusion model aimed at generating dual-channel cell images from volumetric fluorescence microscopy data. By coupling two image channels within the diffusion process, the method seeks to enhance the realism and consistency of generated 3D cell images. The paper is motivated by challenges in biomedical microscopy imaging and reports quantitative improvements compared to current state-of-the-art approaches. The presentation is generally clear, though certain biological and methodological aspects could be better justified and explained.\n\n**Major Comments**  \n1. The biological rationale for modeling two image channels as coupled modalities is insufficiently argued. Clarifying the biological or mechanistic insight that could emerge from generative modeling would strengthen the paper.  \n2. Several methodological choices lack adequate motivation or description—for example, the use of VQGANs and how unquantized embeddings influence the model.  \n3. Key equations (e.g., Equations 1, 2, 12, 13) contain undefined variables and inconsistencies in notation (e.g., h,w,d vs. H,W,D, missing depth term, undefined t). These should be clearly defined and related.  \n4. The claim that multiple color channels represent distinct modalities is not sufficiently justified. Related literature or biological reasoning should support this assumption.  \n5. The relationship between simultaneous dual-channel recovery and latent diffusion processes requires a more explicit conceptual and empirical explanation.  \n6. The placement strategy of attention mechanisms and the design of the dual-channel 3D architecture need clearer description, possibly with a schematic.  \n7. Quantitative results (Table 1) lack interpretability—additional context for FID and MMD score differences, or qualitative visual examples, would be helpful.\n\n**Minor Comments**  \n- Clarify Figure 1’s layout (rows/columns).  \n- Specify image sizes, resolutions, and whether volumes are cropped or raw outputs.  \n- Introduce the dataset before implementation details.  \n- Clarify ambiguous statements, e.g., “single-cell data inconsistencies” and “deep understanding of the input distribution.”  \n- Include references such as 3D U‑Net to situate the method.  \n- Consider renaming the model to avoid confusion with existing “WNet” architectures.  \n- Minor language imprecision throughout should be revised for clarity.\n\n**Summary Paragraph**  \nIn terms of **significance**, the work tackles an important problem in microscopy image synthesis. **Innovation** lies in coupling multi-channel diffusion, but the biological and conceptual motivation require reinforcement. The **evaluation** shows numerical gains, yet lacks interpretive depth and qualitative context. **Reproducibility** would benefit from clearer presentation of equations, architectural details, dataset description, and implementation parameters. Strengths include topic relevance and promising results; weaknesses concern motivation, clarity, and methodological completeness.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.**  \nSubstantial clarification of the biological context, methodological details, and evaluation interpretation is necessary before reconsideration.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper presents the novice Build Your Own Model (BYOL), a multichannel generative framework leveraging the diffusion model, to generate a population of simulated 3D multichannel data that shows the morphological changes in cells perturbed by drug treatments. The model captures the relation between the nuclear and cytoplasmic channels used for model training when generating the simulated images and presents a high spatial resolution of the images. The authors benchmarked the model against already available models like GAN-based models and MedicalDiffusion, useful for 3D image generation for the same test case, and found the best overall model performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\nThe model outperforms existing models, generating nuanced morphological changes due to perturbations like drug treatments. Compared to existing models, it can also accurately capture the 3D resolved morphology of the cellular tags. The model is best at generating cellular data and matches real data.\n\n### Weaknesses\n\nThe model captures the morphological changes associated with the perturbations it has been trained on but has not yet been shown to be generalizable to different cell types and drug treatments. This has been marked as a future prospect of the study. This is important in biological studies as tagging and imaging the markers are expensive for generating training data and a very important domain where biology communities would benefit.\n\n### Questions\n\nThe metrics used to evaluate the model are good for evaluating the overall model performance. But do the metrics evaluate the inter- and intra-channel prediction accuracy? The authors stated that it is biologically relevant and an improvement brought by the work. But how can you evaluate this specific aspect using relevant metrics from a biological point of view?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces the Build Your Own Model (BYOL), a novel multichannel generative framework utilizing the diffusion model to simulate 3D multichannel data that illustrates morphological changes in cells subjected to drug treatments. This model effectively captures the relationship between nuclear and cytoplasmic channels during image generation, achieving high spatial resolution. The authors benchmark BYOL against existing models, including GAN-based models and MedicalDiffusion, demonstrating superior overall performance.\n\n**Major Comments:**  \n1. **Novelty**: The model shows significant improvements over existing frameworks by accurately capturing nuanced morphological changes.\n2. **Evaluation Design**: While the model performs well, it has not been demonstrated to generalize across different cell types and drug exposures, which limits its applicability. This remains a future direction for the research.\n3. **Comparisons**: The authors benchmark their model against notable existing frameworks, but further clarification on inter- and intra-channel prediction accuracy metrics is needed to enhance the evaluation.\n4. **Reproducibility**: The manuscript does not address how biological data generation can be generalized, a crucial factor in biological research due to costs associated with tagging and imaging.\n\n**Minor Comments:**  \n- Ensure clear explanations are provided for the evaluation metrics employed, particularly in relation to biological relevance.\n- The manuscript would benefit from concise delineation of acronyms and terms used within the context.\n\n**Summary Paragraph:**  \nThe BYOL framework significantly contributes to the field by innovatively generating simulated cellular data, addressing the significance and novelty criteria. However, concerns regarding its evaluation and reproducibility remain, as the model’s applicability to varied biological contexts is not yet established. The authors should strengthen the evaluation framework by providing clearer metrics that reflect biological relevance.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel diffusion-based generative framework, Build Your Own Model (BYOL), designed to produce simulated 3D multichannel cellular images that reflect morphological alterations under drug perturbations. The approach models the relationship between nuclear and cytoplasmic channels to generate high-spatial-resolution images and is benchmarked against existing 3D generative models such as GAN-based methods and MedicalDiffusion. The paper is clearly presented, and the experimental comparisons demonstrate strong performance relative to prior approaches.  \n\n**Major Comments**  \n1. **Generalizability:** While the framework effectively captures morphological changes for the perturbations it was trained on, it remains unclear how well it generalizes to other cell types or drug treatments. The authors mention this as future work, but a discussion of potential strategies or limitations would help clarify scope and applicability.  \n2. **Evaluation Metrics:** The metrics employed are suitable for assessing overall model performance; however, it is uncertain whether they adequately measure inter- and intra-channel prediction accuracy. As this is a key claim of biological relevance, additional clarification or targeted evaluation metrics would reinforce the study’s biological interpretability.  \n\n**Minor Comments**  \n- Clarify whether training data diversity might influence generalization claims.  \n- Ensure all acronyms are clearly defined at first mention.  \n- The presentation and figures are clear, though a brief visual comparison with baseline outputs could enhance comprehension.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is significant in its application of diffusion models for biologically realistic 3D multichannel image generation, potentially reducing the need for costly experimental imaging. The innovation lies in modeling inter-channel morphology within simulated perturbations. The evaluation is adequate but would benefit from more biologically specific performance metrics. Reproducibility appears achievable, though broader validation across additional biological conditions remains to be demonstrated. Overall, the contribution is promising but somewhat limited in scope and validation.  \n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The method is technically sound and innovative but requires further clarification of generalizability and evaluation metrics to strengthen its biological relevance and reproducibility claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors combine vector quantized GANs to learn representations of microscopy images of cells and develop a denoising diffusion model for latent representations. By combining vector quantized representations and the process of diffusion, they seek to generate 3D images of cells that belong to the distribution of realistic microscopy images.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* Strategy: The strategy of using diffusion modeling to improve the accuracy of prediction of GANs is promising.\n\n### Weaknesses\n\n* Incorrect assumptions about microscopy image data: Microscopy images often consist of more than two channels and many of them cannot be just binned into cells and nuclei. The authors seem to be familiar with medical imaging datasets but unaware of datasets such as cell painting (JUMP, CHAMMI), human protein atlas, and virtual staining. These datasets illustrate that microscopy data often consists of channels that encode multiple organelles and cellular compartments.\n* Lack of 3D predictions: Although the paper claims to be the first to build a 3D generative model of microscopy images, all the presented data is 2D. The authors should show orthogonal slices of generated volumes.\n* Relevance of metrics: Fre ́chet Inception Distance and Maximum Mean Discrepancy seem reasonable. However, the authors do not clarify how these metrics may be affected by the typical failure modes of GANs, such as hallucinations of spurious cellular processes.\n\n### Questions\n\n* What is the effect of the diffusion on the quantized codebook? The way diffusion is used during inference was not apparent from the text or figures.\n* Does the approach work only with a specified number of input channels?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel approach combining vector quantized Generative Adversarial Networks (GANs) with denoising diffusion models to generate 3D representations of microscopy images of cells. The authors aim to improve representation learning to align better with real microscopy data distributions. The clarity of the overall presentation and the impact of the methodology on existing models warrant careful evaluation.\n\n### Major Comments\n1. **Assumptions about Microscopy Data:** The authors make incorrect assumptions regarding microscopy image data, which typically comprises more than two channels. The work seems limited due to a lack of familiarity with comprehensive datasets such as cell painting (JUMP, CHAMMI), the Human Protein Atlas, and virtual staining, which show that data often encompass multiple organelles and cellular compartments.\n2. **3D Predictions Absence:** Despite claiming to develop a 3D generative model, the presented examples are solely 2D. The authors should include orthogonal slices of generated volumes in their results for validation.\n3. **Metric Relevance:** While the use of Frechet Inception Distance and Maximum Mean Discrepancy as evaluation metrics is reasonable, the authors do not discuss how these may be influenced by GAN failure modes, such as generation of spurious cellular features.\n\n### Minor Comments\n- The text lacks clarity regarding the effects of diffusion on the quantized codebook.\n- It is unclear if the proposed method is contingent on a specific number of input channels.\n\n### Summary Paragraph\nIn terms of TMI editorial criteria, this study presents significant novelty through its use of diffusion modeling in GANs, although the assumptions about data and the lack of 3D validation raise concerns regarding innovation and reproducibility. The evaluation approach, while using established metrics, requires a more thorough discussion to ensure methodical rigor.\n\n### Decision Recommendation\nMajor revision required to address the identified weaknesses and improve clarity regarding methodology and evaluation.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript combines vector-quantized generative adversarial networks (VQ-GANs) and denoising diffusion models to learn latent representations of microscopy images and generate realistic 3D cellular structures. By integrating vector quantization and diffusion processes, the authors aim to improve generative quality and realism in cellular imagery. The overall presentation is understandable but limited in scope and clarity, and several methodological and conceptual aspects require more substantiation.\n\n**Major Comments**  \n1. **Assumptions about microscopy data** – The manuscript makes oversimplified assumptions regarding the structure of microscopy data. Many datasets (e.g., Cell Painting, Human Protein Atlas, virtual staining studies) include multiple imaging channels representing diverse organelles and compartments, not merely nuclei and cells. The proposed approach seems to disregard this complexity, limiting its generalizability.  \n2. **3D data claims versus 2D results** – Although the authors claim the method generates 3D microscopy images, all presented demonstrations are based on 2D data. The work would be more convincing with visualization of orthogonal slices or volumetric reconstructions.  \n3. **Evaluation metrics** – While Fréchet Inception Distance (FID) and Maximum Mean Discrepancy (MMD) are reported, their relevance to microscopy data and robustness to common GAN artifacts (e.g., spurious or hallucinatory structures) are not adequately discussed. A critical analysis of how these metrics behave for this domain is needed.  \n4. **Unclear diffusion process** – The manuscript does not make explicit how the diffusion step interacts with the quantized codebook during inference, which hinders reproducibility. Clarification is required concerning model components and channel dependencies.  \n\n**Minor Comments**  \n- Specify whether the method can handle varying numbers of input channels.  \n- Improve clarity of figures to illustrate the inference process explicitly.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study presents an innovative attempt to merge diffusion modeling with latent quantization for cellular image synthesis, suggesting potential for improved realism. However, its significance is reduced by conceptual oversights about microscopy data characteristics, limited empirical validation in 3D, and insufficient methodological transparency. The evaluation metrics are not fully justified for this context, and reproducibility is affected by incomplete procedural details.\n\n**Decision Recommendation**  \n**Recommendation: Major Revision.** The approach is conceptually promising but requires substantial methodological clarification, stronger validation, and alignment with realistic microscopy data standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper introduces \"Build Your Own Cell\" (BYOC), a generative framework for synthesizing 3D multichannel fluorescence microscopy images of cellular structures. The method addresses the challenge of generating biologically realistic cell and nucleus volumes by combining vector quantization with diffusion models. The approach employs a \"library of codebooks\" where separate VQGAN-based codebooks independently process cell and nucleus channels (Section 3.1), followed by a multichannel diffusion model using a DualChannelUNet architecture to capture inter-channel dependencies (Section 3.2). The authors evaluate BYOC on a dataset of 7,083 metastatic melanoma cells treated with three different drugs (Nocodazole, Binimetinib, Blebbistatin) and demonstrate superior performance compared to baselines including MedicalDiffusion, HA-GAN, W-GAN, and α-GAN using FID and MMD metrics (Table 1). The framework aims to enable high-throughput in silico drug screening and accelerate pre-clinical drug development.\n\n## Weaknesses\n\n• **Mathematical notation inconsistencies and errors**\n  - Equation 1 contains a clear error where both encoded representations are defined as E(xnuc) instead of E(xcell) and E(xnuc) respectively, creating confusion about the encoding process\n  - The quantization operations in Equations 3-4 use inconsistent indexing notation (ij vs mn) without clear definition of these spatial indices\n  - The library of codebooks definition in Equation 2 lacks clarity about how K and P (codebook sizes) relate to the spatial dimensions and encoding process\n\n• **Limited experimental validation and dataset scope**\n  - The evaluation is restricted to a single cell type (metastatic melanoma) and only three drug treatments, severely limiting generalizability claims about biological applications\n  - The dataset size of 7,083 cells, while substantial, represents samples from a single imaging modality and experimental setup, raising questions about robustness across different microscopy conditions\n  - No validation on independent datasets or cross-institutional data is provided, which is critical for medical AI applications as stated in the abstract\n\n• **Insufficient baseline comparisons and evaluation methodology**\n  - The comparison baselines (HA-GAN, W-GAN, α-GAN) are not specifically designed for multichannel 3D biological data, making the performance comparison potentially unfair as acknowledged in Section 4.3\n  - The evaluation relies solely on FID and MMD metrics without biological validity assessments or expert evaluation of morphological accuracy\n  - Missing comparisons with recent 3D medical image generation methods beyond MedicalDiffusion, despite citing relevant work like Tudosiu et al. (2024) and Sun et al. (2022)\n\n• **Unclear technical implementation and reproducibility concerns**\n  - The DualChannelUNet architecture description in Section 3.2 lacks sufficient detail about the attention mechanisms and how spatial-depth attention specifically captures inter-channel dependencies\n  - The normalization process described in the final assembly (Section 3.2) uses \"minimum and maximum values from respective codebooks\" but this procedure is not formally defined or validated\n  - Implementation details in Section 4.1 mention padding to 64³ but do not address how variable-sized inputs affect training stability or generation quality\n\n## Suggestions for Improvement\n\n• **Correct mathematical formulations and improve clarity**\n  - Fix the encoding error in Equation 1 by properly defining ẑcell = E(xcell) and ẑnuc = E(xnuc) with consistent notation throughout\n  - Provide clear definitions for all spatial indexing notation (ij, mn) and explain their relationship to the downsampling factors (h < H, w < W)\n  - Add formal definitions for the codebook construction process and the relationship between codebook sizes K, P and the latent dimensions nz, nv\n\n• **Expand experimental validation scope and rigor**\n  - Include evaluation on additional cell types and imaging modalities to demonstrate broader applicability beyond melanoma cells\n  - Incorporate biological validation metrics such as morphological feature analysis or expert pathologist evaluation to complement computational metrics\n  - Provide cross-validation results on independent datasets or different experimental conditions to establish robustness and generalizability\n\n• **Enhance baseline comparisons and evaluation methodology**\n  - Include comparisons with more recent and relevant 3D medical image generation methods, particularly those designed for multichannel biological data\n  - Add ablation studies beyond the codebook analysis to evaluate individual components like the attention mechanisms in DualChannelUNet\n  - Implement biological plausibility metrics such as nucleus-to-cell volume ratios or spatial relationship consistency to validate the biological realism claims\n\n• **Provide comprehensive technical details and reproducibility information**\n  - Include detailed architectural specifications for the DualChannelUNet, particularly the spatial-depth attention mechanisms and their specific role in capturing inter-channel dependencies\n  - Formally define and validate the normalization procedure used in the diffusion process, including its impact on generation quality\n  - Address the handling of variable input sizes and provide analysis of how padding to 64³ affects the preservation of original cellular morphologies and biological characteristics",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents \"Build Your Own Cell\" (BYOC), a generative framework designed for synthesizing 3D multichannel fluorescence microscopy images of cellular structures. The approach integrates vector quantization and diffusion models to produce biologically realistic volumes of cells and nuclei. By employing a library of VQGAN-based codebooks processed independently for cell and nucleus channels, followed by a DualChannelUNet model, the authors illustrate significant improvements in synthetic image quality compared to several baselines. The framework aims to facilitate high-throughput in silico drug screening and enhance pre-clinical drug development protocols.\n\n### Major Comments\n1. **Mathematical Notation Inconsistencies**:\n   - Equation 1 incorrectly defines both encoded representations as E(xnuc), which may confuse readers. It should distinctly define E(xcell) and E(xnuc).\n   - Inconsistent indexing notations (ij vs mn) in Equations 3-4 are used without proper definitions.\n   - Equation 2's definitions of K and P lack clarity regarding their relation to spatial dimensions and the encoding process.\n\n2. **Limited Experimental Validation**:\n   - Evaluation is confined to a single cell type (metastatic melanoma) and three drug treatments, limiting generalizability.\n   - The dataset, although substantial with 7,083 cells, represents only one imaging modality, raising concerns regarding robustness.\n   - Absence of validation on independent datasets is a significant drawback, particularly for medical AI.\n\n3. **Inadequate Baseline Comparisons**:\n   - Baseline methods (HA-GAN, W-GAN, α-GAN) are not explicitly designed for multichannel 3D data, potentially compromising the fairness of performance assessments.\n   - Evaluation relies only on FID and MMD metrics, ignoring biological validity and expert assessments.\n   - The manuscript does not compare with more recent 3D image generation methods, limiting its context.\n\n4. **Concerns Regarding Implementation and Reproducibility**:\n   - Insufficient detail on DualChannelUNet's attention mechanisms and their role in capturing inter-channel dependencies.\n   - The normalization procedure lacks formal definitions and validation in the final assembly.\n   - The manuscript does not adequately address how variable input sizes impact training stability and image quality.\n\n### Minor Comments\n- Mathematical errors in notations should be corrected for clarity.\n- Definitions related to spatial indices require formalization.\n- Specific improvements can include robust validations with various cell types and metrics for biological relevance.\n\n### Summary Paragraph\nThe BYOC framework demonstrates significant potential in the realm of drug development through enhanced image synthesis of cellular structures. However, its limitations in experimental validation and generalizability, alongside inadequate baseline comparisons and reproducibility concerns, hinder its impact. By addressing these issues, especially regarding the significance, innovation, evaluation methods, and reproducibility, the manuscript could further contribute to the field.\n\n### Decision Recommendation\nMajor revision is recommended to address the identified weaknesses and enhance the overall rigor of the study.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Build Your Own Cell* (BYOC), a generative framework for synthesizing 3D multichannel fluorescence microscopy images of cellular structures. BYOC integrates vector quantization with diffusion modeling to generate biologically realistic cell and nucleus volumes through a library of VQGAN-based codebooks and a DualChannelUNet for multichannel diffusion. The authors validate the method on 7,083 metastatic melanoma cells exposed to three drugs (Nocodazole, Binimetinib, Blebbistatin) and report improved performance over existing baselines (MedicalDiffusion, HA-GAN, W-GAN, α-GAN) using FID and MMD metrics. The work is clearly motivated and potentially impactful for high-throughput *in silico* drug screening, though several methodological and validation issues limit its current rigor and generalizability.  \n\n**Major Comments**  \n1. **Mathematical notation and formulation issues:**  \n   Equations contain inconsistencies (e.g., both encodings in Eq. 1 defined as E(xₙᵤc)), ambiguous spatial indices (ij vs mn), and insufficient explanation of codebook parameters (K, P) relative to spatial and latent dimensions.  \n2. **Limited experimental scope:**  \n   Evaluation focuses solely on melanoma cells and three drug treatments, constraining claims of biological generalizability. No tests on other cell types, imaging modalities, or independent datasets are included, undermining robustness for medical AI applications.  \n3. **Baseline choice and evaluation methodology:**  \n   Compared methods are not tailored for multichannel 3D biological data, potentially biasing results. The assessment relies narrowly on FID and MMD without biological plausibility or expert-based validation. Relevant recent 3D generation works (e.g., Tudosiu et al., Sun et al.) are not compared.  \n4. **Technical detail and reproducibility gaps:**  \n   Architectural description of DualChannelUNet lacks specification of attention mechanisms and spatial–depth interactions. The normalization procedure (“min–max from respective codebooks”) is not formally defined or validated. Handling of variable cell sizes after padding to 64³ lacks justification and analysis.  \n\n**Minor Comments**  \n- Ensure consistent mathematical notation and indexing throughout.  \n- Clarify definitions of h, w vs H, W in downsampling relations.  \n- Explicitly describe codebook construction and parameterization.  \n- Improve figure captions and references for clarity.  \n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe submission is innovative in combining VQ-based codebooks with diffusion modeling for multichannel 3D microscopy synthesis and could significantly advance data generation for cellular imaging. However, limited validation scope, incomplete technical details, and insufficient evaluation methodology weaken confidence in the reported performance and reproducibility. Strengthening mathematical clarity, expanding biological validation, and providing comprehensive implementation details would considerably improve the manuscript’s scientific rigor and generalizability.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase 1 – Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI:**  \n✅ The manuscript presents a methodological framework for multichannel 3D biological image synthesis based on diffusion models and vector quantisation. The proposed “Build Your Own Cell (BYOC)” system contributes new algorithmic components—specifically a *library of codebooks* and a *dual-channel diffusion process*—for volumetric image generation in microscopy. While currently oriented toward cellular microscopy (rather than clinical-scale imaging), the methodological contribution clearly falls within medical imaging methodology, particularly image synthesis and model-based image representation.\n\n**2. Novelty & Contribution Level:**  \nModerate–High. The integration of channel-specific vector-quantised autoencoders with a multichannel diffusion process for 3D volumetric generation is a novel combination. Previous 3D diffusion works (MedicalDiffusion, HA-GAN) have been single-channel or limited in modelling channel interdependence. The proposed “library of codebooks” introduces an original concept supporting cross-channel modelling.  \nHowever, the core diffusion and VQGAN elements are adaptations of existing frameworks; the novelty largely resides in their combination and application to fluorescence microscopy rather than a fundamental diffusion-theoretic advancement.\n\n**3. Technical and Experimental Rigor:**  \nThe experimental section is reasonably thorough:  \n- Dataset with >7k 3D single-cell volumes described in detail (drugs, imaging modality, voxel size).  \n- Quantitative metrics (FID, MMD) are computed using a medically relevant 3D feature extractor (Med3D).  \n- Appropriate baselines and ablation analyses are included.  \n\nYet, validation remains limited to a single dataset from melanoma cells, and biological fidelity is assessed only visually and by distributional statistics, without biological annotation or expert assessment. Reproducibility appears good (code repository link provided), but training and parameter choices are described in a machine-learning rather than medical-imaging standard (e.g., no uncertainty analysis, no runtime evaluation, no mention of statistical significance across folds).\n\n**4. Clarity and Presentation:**  \nOverall clear, well-structured, and polished in writing. Figures are informative conceptually (though not visible here, the descriptions suggest clear visualization). Mathematical sections are complete, though somewhat dense. Minor notation inconsistencies (e.g., equation numbering, variable reuse). The paper reads like an ML conference paper (ICLR style) rather than adhering to TMI formatting and focus on methodological rigor tied to imaging physics or quantitative validation.\n\n**5. Ethical and Reproducibility Compliance:**  \nThe dataset is said to be derived from microscopy of metastatic melanoma cell lines; no ethical approval statement or data accessibility/IRB statement is mentioned. If data are proprietary, this must be clarified. Code availability is promised anonymously. For TMI submission, explicit statements regarding data use, anonymization, and availability should be required.\n\n---\n\n## **Phase 2 – IEEE TMI Review Report**\n\n### **1. Summary**\nThe authors present *Build Your Own Cell (BYOC)*, a generative framework for multichannel 3D fluorescence microscopy image synthesis. The method combines (i) channel-specific vector-quantised autoencoders that encode cell and nucleus channels into separate latent codebooks, and (ii) a dual-channel denoising diffusion model that jointly refines these latents to maintain inter-channel biological consistency. BYOC aims to synthesize realistic 3D cell–nucleus volumes under different drug conditions to support in‑silico morphological analysis. Evaluation on >7,000 single-cell volumes shows improved Fréchet Inception Distance (FID) and Maximum Mean Discrepancy (MMD) scores compared to existing 3D generative baselines (e.g., MedicalDiffusion, HA-GAN).\n\n### **2. Strengths**\n- **Methodological novelty:** Creative integration of vector-quantised latent coding and multichannel diffusion into a dual‑stream 3D generative model.  \n- **Relevance to imaging methodology:** Provides a modelling framework for multichannel volumetric data—a recognized challenge in biomedical imaging.  \n- **Thorough technical documentation:** Includes algorithmic detail, equations, training setup, and ablation experiments.  \n- **Quantitative and qualitative comparisons:** Demonstrates measurable performance gains on both structural fidelity and inter‑channel consistency.  \n- **Potential downstream utility:** Synthetic cellular morphologies could augment limited microscopy data and enable hypothesis generation.\n\n### **3. Weaknesses**\n- **Limited generalization and biological validation:** Validation on one melanoma dataset with three drug types restricts evidence for broader biomedical applicability.  \n- **Unclear clinical or translational significance:** The work focuses on cell microscopy synthesis rather than clinically relevant medical imaging (CT, MRI, PET), which may place it below TMI’s desired translational impact threshold.  \n- **No expert biological fidelity assessment:** Visual realism is discussed qualitatively without blinded expert scoring or quantitative morphological feature analysis.  \n- **Missing ethical and data-governance statements:** No documentation of dataset source permissions or IRB approval.  \n- **Evaluation metrics limited to distributional similarity (FID/MMD); absence of uncertainty or statistical significance evaluation.**\n\n### **4. Major Comments**\n1. **Scope alignment with TMI:** Clarify how this framework advances medical imaging methodology beyond microscopy data synthesis. Consider discussing its extension to other volumetric imaging modalities (MRI, CT) or its utility for clinical analysis (e.g., segmentation pretraining, simulation).  \n2. **Biological validation:** Include domain‑level evaluation—morphometric feature correlation with real samples or expert grading—to demonstrate biological realism beyond visual similarity.  \n3. **Reproducibility:** Provide explicit data availability and ethical statements.  \n4. **Methodological justification:** Strengthen theoretical motivation for the “library of codebooks” and how it relates to information sharing or disentanglement between channels. Quantify its effect statistically (e.g., using ANOVA over multiple runs).  \n5. **Evaluation robustness:** Expand to additional datasets or include cross‑validation across cell types to substantiate generalizability.  \n6. **Clinical or computational imaging relevance:** Discuss how the algorithm might generalize to multisequence MRI, multimodal imaging (PET–CT), or other medical volumetric data, aligning it with the TMI scope.  \n\n### **5. Minor Comments**\n- Ensure consistent mathematical notation (e.g., encoder notations for cell vs. nucleus channels).  \n- Improve alignment of equations (3)–(5)—a few typos in variable naming (E(x_nuc) used twice).  \n- Clarify all hyperparameters in the main paper rather than appendix only.  \n- Grammar is good overall; minor edits to shorten long sentences and reduce redundancy between Introduction and Related Work.  \n- Figures (e.g., Fig. 4–6) should include clear axis scales or volume dimensions to enable reproducibility.  \n- Include DOI or accession details for the dataset if available.\n\n### **6. Overall Recommendation**\n**Recommendation:** ⚠️ *Reject (Potentially Below TMI Novelty Threshold / Out of Scope)*  \n\n**Rationale:**  \nThe work is technically sound and thoughtfully executed but primarily addresses a microscopy‑level generative problem with limited extension to general medical imaging. Although the model introduces a credible methodological innovation (multichannel diffusion with codebook libraries), its demonstrated scope and impact do not yet meet IEEE TMI’s emphasis on broadly generalizable advances in medical imaging science or quantitative analysis frameworks. It would be better suited for a biomedical imaging informatics or machine‑learning venue (e.g., IEEE JBHI, MedIA, or MICCAI).  \nIf revised for TMI, the authors should emphasize methodological generalizability to clinical imaging modalities and provide stronger quantitative validation and ethical compliance information.\n\n### **7. Confidence Level**\n**4/5 – High Confidence.**  \nAssessment supported by clear technical exposition and strong familiarity with generative modelling in medical imaging.\n\n---\n\n**Overall Summary:**  \n*BYOC* presents an elegant, well-crafted generative ML framework with genuine 3D methodological interest. While the algorithmic novelty is clear and the experiments are credible, the limited biomedical breadth and absence of rigorous clinical imaging validation suggest it may not yet satisfy TMI’s threshold for methodological generalizability. With further extension to multimodal or diagnostic imaging contexts and robust biological validation, this line of work could evolve into a significant contribution for TMI in future revisions.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Build Your Own Cell (BYOC)*, a generative framework for synthesizing multichannel 3D fluorescence microscopy images. The method integrates channel‑specific vector‑quantised autoencoders with a dual‑channel diffusion model to jointly model cell and nucleus volumes while maintaining inter‑channel biological consistency. The approach demonstrates improved Fréchet Inception Distance (FID) and Maximum Mean Discrepancy (MMD) scores on a dataset of over 7,000 single‑cell volumes compared with existing 3D generative models. The paper is clearly written, methodologically detailed, and technically executed to a high standard, though its scope remains primarily at the microscopy level rather than clinical medical imaging.\n\n---\n\n**Major Comments**  \n1. **Scope and Relevance:** The study is well aligned with imaging methodology but its focus on cellular microscopy may place it below the translational impact expected for TMI. The authors should clarify how BYOC advances general medical imaging methodology or discuss possible extensions to modalities such as MRI or CT.  \n2. **Validation and Generalization:** Evaluation is limited to a single melanoma cell dataset with three drug conditions. Broader validation across datasets or cell types, and inclusion of quantitative biological fidelity assessments (e.g., morphometric correlations or expert scoring), would improve confidence in generalizability.  \n3. **Reproducibility and Ethics:** Explicit statements regarding data accessibility, permissions, and IRB compliance are required. The code link is noted, but reproducibility should also include runtime, hyperparameter, and uncertainty analyses.  \n4. **Methodological Justification:** The conceptual role and benefit of the “library of codebooks” require further motivation and empirical analysis, ideally supported by statistical comparisons (e.g., across multiple runs).  \n5. **Evaluation Robustness:** Current metrics (FID, MMD) assess distributional similarity but omit measures of uncertainty or statistical significance. More comprehensive quantitative evaluation is encouraged.  \n6. **TMI Alignment:** Articulate potential clinical or computational imaging relevance—such as applications to multisequence or multimodal volumetric data—to better match TMI’s methodological scope.\n\n---\n\n**Minor Comments**  \n- Ensure consistent mathematical notation and equation numbering; some variable reuse occurs in Eqs. (3)–(5).  \n- Clarify all hyperparameters within the main text rather than the appendix.  \n- Edit long sentences for conciseness and reduce overlaps between Introduction and Related Work.  \n- Include units or scale bars for all figures showing volumetric data.  \n- Add dataset identifiers (DOI or accession number) where applicable.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe paper presents a technically sound and innovative generative framework that combines vector‑quantised encoding with multichannel diffusion for realistic 3D microscopy synthesis. Its methodological contribution is clear and well‑executed, representing a moderate‑to‑high level of innovation. However, its experimental validation is constrained to a narrow domain, with limited biological verification and no clinical imaging demonstration. While reproducibility appears good through code availability, ethical documentation and statistical analyses need strengthening. Overall, the work holds significance for biomedical image generation but currently falls short of TMI’s desired breadth and translational emphasis.\n\n---\n\n**Decision Recommendation:** **Reject (Potentially Below TMI Novelty/Scope Threshold)**  \nThe study is well performed and contributes a credible generative approach but is limited in biomedical scope and lacks rigorous validation across imaging contexts. A version emphasizing generalization to broader medical imaging modalities and stronger ethical and quantitative evaluation could be reconsidered for TMI in the future.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "## **Phase 1 – Analytical and Scope Audit**\n\n**Opening Paragraph:**  \nThe manuscript presents a methodological framework for multichannel 3D biological image synthesis using diffusion models and vector quantization. The proposed \"Build Your Own Cell (BYOC)\" system contributes new algorithmic components, specifically a library of codebooks and a dual-channel diffusion process, aimed at enhancing volumetric image generation in microscopy. While its focus is currently on cellular microscopy, the contribution is relevant to medical imaging, particularly in the realms of image synthesis and model-based representation.\n\n**Major Comments:**  \n1. **Scope alignment with TMI:** Clarify how this framework advances medical imaging methodology beyond microscopy data synthesis and consider discussing its application in other volumetric imaging modalities (MRI, CT) or its utility for clinical applications (e.g., segmentation pretraining).  \n2. **Biological validation:** Incorporate domain-level evaluation, such as correlation of morphometric features with actual samples or expert grading, to substantiate biological realism beyond subjective visual assessments.  \n3. **Reproducibility:** Provide detailed ethical statements and explicit data availability information.  \n4. **Methodological justification:** Enhance the theoretical motivation for the “library of codebooks” by demonstrating its relationship to information dispersal or disentanglement across channels, and quantify its impact statistically.  \n5. **Evaluation robustness:** Extend validation to additional datasets or include cross-validation among different cell types to substantiate claims of generalizability.  \n6. **Clinical or computational imaging relevance:** Discuss how the algorithm may generalize to various medical volumetric data settings, including multimodal imaging contexts.\n\n**Minor Comments:**  \n- Ensure consistent mathematical notation and clarify all hyperparameters within the main document rather than relegating them to the appendix.  \n- Improve alignment of equations, specifically equations (3)–(5), and rectify variable naming inconsistencies.  \n- Figures should depict clear axis scales or volume dimensions, enhancing reproducibility.  \n- Minor grammatical edits are recommended to improve clarity and reduce redundancies, particularly between the Introduction and Related Work sections.  \n- Include dataset DOI or accession details, if available.\n\n**Summary Paragraph:**  \nIn summary, the BYOC framework demonstrates significant methodological innovation by integrating vector-quantized autoencoders with a dual-channel diffusion model, thereby addressing challenges in multichannel volumetric data synthesis. However, the current scope is limited to a single dataset and does not adequately assess clinical relevance or broader applicability, which may hinder its alignment with IEEE TMI's focus on generalizability in medical imaging. \n\n**Decision Recommendation:**  \n**Recommendation:** ⚠️ *Reject (Potentially Below TMI Novelty Threshold / Out of Scope)*  \n\n**Rationale:**  \nThe work is technically solid and carefully executed, yet it primarily addresses a microscopy-focused generative issue. The novelty presented may not fully meet IEEE TMI's emphasis on broadly applicable advancements in medical imaging science. Future revisions should focus on generalizability to clinical contexts and include robust validation and greater adherence to ethical standards.\n\n**Confidence Level:**  \n**4/5 – High Confidence.**  \nThe assessment is informed by clear technical exposition and a strong familiarity with generative modeling in the medical imaging domain. \n\n**Overall Summary:**  \nThe BYOC framework offers an elegant and technically proficient approach to 3D image synthesis. While the algorithmic contributions are notable, the limited applicability to broader biomedical contexts and lack of rigorous validation highlight the need for further development to meet TMI's standards for contribution to the field.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chris Bakal",
      "Chris Dunsby",
      "Lucas G Dent",
      "Matt De Vries",
      "Nathan Curry",
      "Olga Fourkioti",
      "Reed Naidoo"
    ],
    "url": "pdfs/iclr.cc-2025-conference_dd6f5c6509a71b82b803fd05cb68835d3440c1b0.pdf",
    "remote_url": "https://openreview.net/pdf/dd6f5c6509a71b82b803fd05cb68835d3440c1b0.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Justin"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "LVLM",
      "Eye Gaze",
      "Video",
      "Medicine",
      "Medical Image",
      "Chest X-ray",
      "Chest X-ray Report Generation"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists’ eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. The video, featuring a red gaze point overlaid on CXR images, emphasizes regions of focused attention during interpretation. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with a video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 25.4% on Impression generation task and on average 7.9% for all tasks using scaled evaluation metrics. Our approach enhanced open-domain LVLM models, when combined with exemplar reports for in-context learning, outperform medical models as well as those specifically trained for CXR report generation on the benchmark dataset. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks, pointing out a new effective approach of utilising LVLMs in healthcare and beyond.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis article introduces a novel prompting method called RadEyeVideo, which presents radiologists’ eye-tracking data as a video sequence to capture the temporal and spatial order of their gaze (i.e., scan paths). The authors evaluated the effectiveness of this approach in chest X-ray report generation and disease diagnosis tasks using large vision-language models (LVLMs) with video input capabilities. Results show that incorporating eye-gaze video prompts improved model performance by 25.4% on the Impression generation task, with an average performance increase of 7.9% across all tasks.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The RadEyeVideo method converts radiologists’ eye-tracking data into video sequences for use in chest X-ray report generation and diagnostic tasks, significantly enhancing the performance of general large vision-language models (LVLMs). Experimental results indicate a 25.4% improvement in the Impression generation task and an average improvement of 7.9% across all tasks, outperforming models specifically designed for medical applications.\n2. Comprehensive Evaluation of Eye-Tracking Prompting Methods: This study presents the first thorough evaluation of various eye-tracking integration techniques, including static heatmaps, eye-tracking text prompts, and dynamic video prompts. The results indicate that RadEyeVideo outperforms the other methods in terms of diagnostic accuracy and clinical relevance, establishing a new standard for eye-tracking data prompts.\n\n### Weaknesses\n\n- Methodological Clarity: a) The paper lacks sufficient detail about the video prompt integration process, particularly how the video information is encoded and processed by the LVLMs. The technical implementation of combining video sequences with textual prompts could be more thoroughly explained. b) There's limited discussion of potential alternatives to full video sequence processing that might achieve similar results with lower computational costs.\n\n- Data Collection and Generalizability Limitations: a) The methodology requires synchronized eye-tracking data collection during radiologist readings, which is resource-intensive and difficult to scale. b) The approach may not generalize well to scenarios where real-time eye-tracking data is unavailable or impractical to collect. c) The current evaluation is limited to a relatively small dataset (2,298 CXR images), raising questions about broader applicability.\n\n- Computational Efficiency Concerns: The direct use of complete eye-tracking video sequences as prompts likely increases computational overhead significantly. While the authors mention sampling k frames (typically 16) from the total sequence, there's limited analysis of the computational trade-offs or optimal sampling strategies. The method may be computationally prohibitive for real-time clinical applications.\n\n### Questions\n\nPlease address the concern raised in weakness part.\nMoreover, there are some questions may help you to address the weakness.\n1) What is the computational overhead of processing video prompts compared to traditional image-only or text-only prompts?\n2) Have you explored more efficient alternatives to using complete video sequences, such as key frame selection or compressed representations? 3) How do you envision this approach being implemented in real-world clinical settings where real-time eye-tracking data may not be available? 4) Have you considered alternative methods for generating synthetic eye-tracking data that could make the approach more broadly applicable? 5)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents a novel prompting method named RadEyeVideo, which utilizes radiologists’ eye-tracking data in the form of video sequences to enhance chest X-ray report generation and disease diagnosis tasks using large vision-language models (LVLMs). The proposed method demonstrates a significant improvement, with results indicating a 25.4% enhancement in the Impression generation task and an average increase of 7.9% across all evaluation metrics. Overall, the work is clearly articulated and contributes to advancing the application of eye-tracking in medical imaging.\n\n### Major Comments\n1. **Methodological Clarity**: \n   - The paper lacks sufficient detail regarding the video prompt integration, specifically how the video data is encoded and processed by the LVLMs. A detailed explanation of the technical implementation combining video sequences with textual prompts is warranted.\n   - There is limited discussion of potential alternatives to utilizing full video sequences that could yield similar results at lower computational costs.\n  \n2. **Data Collection and Generalizability**: \n   - The requirement for synchronized eye-tracking data during readings is resource-intensive and could limit scalability.\n   - The approach may not generalize effectively in scenarios where real-time eye-tracking data is unattainable.\n   - The evaluation is confined to a relatively small dataset (2,298 CXR images), which raises concerns regarding broader applicability.\n\n3. **Computational Efficiency**: \n   - The reliance on complete eye-tracking video sequences as prompts likely incurs significant computational overhead. While the authors mention sampling k frames, there is insufficient analysis of the computational trade-offs or the optimal sampling strategies needed for practical applications.\n\n### Minor Comments\n- The paper could benefit from enhanced clarity in figures depicting the RadEyeVideo methodology.\n- There are minor typographical errors that require attention throughout the manuscript.\n\n### Summary Paragraph\nThis study introduces significant innovation through the RadEyeVideo method, showcasing notable improvements in model performance within the context of eye-tracking data integration. However, issues regarding methodological clarity, generalizability, and computational efficiency present challenges for implementation. Addressing these concerns is crucial to meet the editorial criteria of significance, innovation, evaluation, and reproducibility effectively.\n\n### Decision Recommendation\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *RadEyeVideo*, a novel prompting method that represents radiologists’ eye-tracking data as video sequences to capture temporal and spatial gaze order. The approach is evaluated on chest X-ray report generation and disease diagnosis tasks using large vision–language models (LVLMs) capable of processing video inputs. The study reports substantial performance improvements—25.4% in the Impression generation task and an average of 7.9% across all tasks—demonstrating the potential impact of incorporating dynamic eye-gaze information. Overall, the paper addresses an interesting idea with well-structured experiments but has methodological and generalizability concerns that require clarification.  \n\n**Major Comments**  \n1. **Methodological Clarity:** The paper does not sufficiently explain how video prompts are integrated and processed within the LVLMs. Details about video encoding, temporal representation, and combination with textual prompts are limited. Greater transparency on architectural choices, training setup, and data flow is needed to ensure reproducibility.  \n2. **Alternative and Efficiency Analysis:** There is minimal discussion of computational trade-offs associated with full video sequence processing. Explorations of lighter alternatives—such as key-frame selection, compressed representations, or partial sequences—could inform efficiency and practicality while maintaining performance.  \n3. **Data Collection and Generalizability:** The method depends on synchronized eye-tracking data collected during radiologist readings, which is expensive and challenging to scale. The study’s reliance on a relatively small dataset (2,298 CXRs) further limits generalizability. The approach’s applicability in clinical scenarios without real-time eye-tracking data should be discussed.  \n4. **Computational Burden:** Although the authors use frame sampling (typically 16 frames), no quantitative analysis of computational cost versus benefit is provided. This raises concerns about feasibility for real-time diagnostic use.\n\n**Minor Comments**  \n- Clarify the process of selecting “k” frames for input and justify sampling choices.  \n- Ensure consistent notation and acronym definitions (e.g., LVLM).  \n- Provide additional details in figures or appendices to illustrate video prompt construction.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe submission offers *significant* potential by introducing a data-driven mechanism that bridges human visual attention with multimodal model reasoning. Its *innovation* lies in transforming gaze data into a dynamic prompt format, showing clear quantitative gains. However, *evaluation* is constrained by limited dataset size and sparse computational analysis. *Reproducibility* suffers from incomplete methodological detail and unclear implementation steps. Addressing these aspects would substantially strengthen the work.  \n\n**Decision Recommendation**  \n**Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nTo enhance the reliability of large vision-language models (LVLMs) in real clinical environments, this study proposes a novel video prompting method called RadEyeVideo, which integrates radiologists' eye-tracking data as video sequences, capturing the spatial and temporal dynamics of gaze. Red fixation points are superimposed on CXR images to highlight the areas that doctors pay attention to, so as to dynamically present the doctor's eye movement path. Improve the ability of radiologists or AI models to diagnose chest diseases by providing rich contextual information.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed method, RadEyeVideo, combines video, text, and eye movement data to realize the fusion of multi-modal information and improve the accuracy and efficiency of diagnosis.\n2. The authors conducted a thorough assessment of various eye-tracking integration techniques, providing strong empirical support for their claims.\n\n### Weaknesses\n\n1. In Figure 2, the authors illustrate different prompting methods; however, the figure does not clearly distinguish between the use of text descriptions and video inputs. This ambiguity makes it challenging to understand whether the authors used text descriptions to guide the prompt along with the video input or if they only provided video data. \n2. The author's experiments conducted on only one dataset are clearly insufficient in terms of persuasiveness. This limitation may affect the generalizability and applicability of the research findings. To enhance the credibility of the study, it is recommended that the authors validate their approach on different datasets.\n3. When the authors use radiologist's eye-tracking sequences as video input, this approach does enhance the model's understanding of prior knowledge in the diagnostic reading process to a certain extent. However, converting eye-tracking data into video format substantially increases the number of tokens, leading to significant computational resource consumption and longer inference times for the model.\n\n### Questions\n\n1. The RadEyeVideo method integrates the eye movement data of the radiologist as a video sequence to capture the spatiotemporal dynamics of his gaze. However, the article does not explain in detail how this video sequence is specifically generated, for example, how the eye movement data is sampled and converted into video frames, and how these frames contain temporal and spatial information.\n2. In Section 2.3 INPUT REPRESENTATION, to fit the input requirements of LVLM, the authors uniformly sampled the video sequence and selected 16 frames as inputs. However, the article does not explain why 16 frames were chosen and whether this choice had a significant impact on model performance. In addition, whether the impact of using more or fewer frames on model performance has been explored is also a question worth exploring. This helps to further understand the effect of video data length and quality on model performance.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript introduces RadEyeVideo, a novel video prompting method aimed at enhancing the reliability of large vision-language models (LVLMs) in clinical environments. By integrating eye-tracking data from radiologists as video sequences, it captures the spatial and temporal dynamics of gaze to improve diagnosis of chest diseases. The approach aims to enrich diagnostic capabilities for radiologists and AI models alike.\n\n**Major Comments:**  \n1. **Novelty and Contribution:** RadEyeVideo effectively integrates video, text, and eye movement data, advancing multi-modal information fusion to enhance diagnostic accuracy and efficiency. \n2. **Empirical Assessment:** The authors provide substantial empirical support by thoroughly evaluating different eye-tracking integration techniques. \n3. **Clarity in Figures:** Figure 2 does not clearly differentiate between text descriptions and video inputs, resulting in ambiguity regarding their combined use in prompting.\n4. **Dataset Limitations:** Conducting experiments on only one dataset undermines the generalizability of the findings. Validation across multiple datasets is recommended for stronger credibility.\n5. **Computational Efficiency:** The conversion of eye-tracking data to video format increases token count, raising concerns about computational resource usage and inference time.\n\n**Minor Comments:**  \n- The manuscript lacks clarity on how eye-tracking data is sampled and converted into video frames, particularly regarding the generation process and how temporal and spatial information is retained.\n- In Section 2.3, the choice of using 16 uniformly sampled frames as inputs is not justified, and the impact of varying frame numbers on model performance is not explored.\n\n**Summary Paragraph:**  \nIn terms of significance, the study addresses an important need in diagnostic imaging. While the integration of eye-tracking data is innovative, the evaluation design is limited by the use of a single dataset, potentially affecting reproducibility. The lack of clarity in presentation and depth of explanation regarding technical choices invites further scrutiny. Overall, the manuscript displays both strengths and weaknesses, necessitating revisions to bolster its contribution to the field.\n\n**Decision Recommendation:**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RadEyeVideo*, a novel video prompting method designed to improve the reliability of large vision-language models (LVLMs) in clinical diagnostics. The approach integrates radiologists’ eye-tracking data as video sequences, capturing spatial and temporal gaze dynamics to enrich contextual information and support disease diagnosis from chest X-rays. Overall, the paper aims to enhance multimodal understanding in AI-assisted radiology workflows. The submission is generally clear, though certain methodological details and experimental validations require further explanation and expansion.\n\n**Major Comments**  \n1. **Methodological Clarity:** Figure 2 does not clearly differentiate between text-based and video-based prompting. The description of how text and video inputs interact in the prompting process should be clarified to avoid confusion about the experimental setup.  \n2. **Dataset Limitation:** The evaluation is conducted on only one dataset, which limits the generalizability and persuasiveness of the claimed improvements. Testing the approach on additional, independent datasets is necessary to validate robustness and applicability.  \n3. **Computational Efficiency:** While converting eye-tracking data into video sequences can better represent diagnostic behavior, this increases token counts and computational costs, significantly affecting inference speed. A quantitative discussion or mitigation strategy for this trade-off should be included.  \n4. **Generation of Eye-Tracking Videos:** The paper lacks detail on how gaze data are sampled and transformed into video frames, including how temporal continuity and spatial fixation cues are preserved.  \n5. **Frame Selection Rationale:** Section 2.3 mentions uniform sampling of 16 frames, but the choice of this number is not justified. An analysis of the impact of frame count on model performance would strengthen the methodological soundness.\n\n**Minor Comments**  \n- Clarify figure legends to distinguish modalities more explicitly.  \n- Ensure consistent terminology when referring to eye movement data, fixation points, and video sequences.\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work is innovative in integrating visual attention data with LVLMs, contributing to the interpretability and contextual understanding of diagnostic models. However, significance is undermined by limited empirical validation and incomplete methodological descriptions. The evaluation supports feasibility but does not establish generalizability. Reproducibility could be improved through clearer documentation of preprocessing and sampling strategies.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces an innovative approach that incorporates radiologists' eye-tracking data as video sequences. This method captures both spatial and temporal patterns in gaze, which provides a more accurate representation of radiologists' attention during chest X-ray interpretation. The approach was evaluated using three general-domain LVLMs, showing significant improvements.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- RadEyeVideo's use of video-based eye-gaze data is a unique contribution that effectively captures the temporal and spatial dynamics of radiologists' focus. I like this idea.\n- The study demonstrates substantial improvements, particularly in impression generation, highlighting RadEyeVideo's effectiveness in enhancing diagnostic tasks.\n- The language is clearly presented. The authors use precise and concise language so that the reader can easily understand the methodology, and results of the study.\n\n### Weaknesses\n\n- Although this idea is interesting, it still relies on temporal and spatial information in the inference phase, which is difficult to apply to real clinical scenarios. Do the authors consider involving multiple information inputs only in the training phase and simulating zero-shot scenarios as much as possible in the inference phase?\n- The study’s findings are limited by the small size of the MIMIC-Eye dataset, which may not fully capture the variability in real-world clinical settings, raising questions about the generalizability of the results.\n- The comparisons primarily focus on selected models with minimal tuning for this domain. Including a wider range of task-specific medical LVLMs could provide a more comprehensive evaluation.\n\n### Questions\n\n- Why the paper lacks an section of related work? e.g., some recent evaluation work of Med-LVLMs [1,2,3]\n- The formats of reference is weird. e.g., in Line 311-321. Please check it.\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[3] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\nThe manuscript presents an innovative methodology that integrates radiologists' eye-tracking data in the form of video sequences, capturing both spatial and temporal patterns of gaze. This approach offers a refined representation of radiologists' attention during chest X-ray interpretation and has been evaluated against three general-domain large vision language models (LVLMs), showing notable improvements in diagnostic efficacy.\n\n### Major Comments\n1. The novelty of RadEyeVideo relies heavily on using temporal and spatial information during the inference phase, which may not translate effectively to real clinical settings. The authors should consider involving multiple information inputs during the training phase while simulating zero-shot scenarios in the inference phase.\n2. The findings are constrained by the limited size of the MIMIC-Eye dataset, which may not adequately reflect the variability present in real-world clinical environments, thus questioning the generalizability of the results.\n3. Evaluations predominantly focus on a select few models with minimal tuning specific to this domain. A more diverse selection of task-specific medical LVLMs would enhance the comprehensiveness of the evaluation.\n\n### Minor Comments\n- The manuscript lacks a section on related work, particularly regarding recent evaluations of Med-LVLMs.\n- The formatting of references (lines 311-321) requires attention and correction to meet standard citation guidelines.\n\n### Summary Paragraph\nIn terms of the TMI editorial criteria, this study demonstrates significant innovation through its unique contribution of video-based eye-gaze data and effective diagnostic enhancement. However, its evaluation is limited by the size of the dataset and the narrow scope of model comparisons. Addressing these weaknesses could improve its impact, particularly in relation to reproducibility and transferability to real clinical practices.\n\n### Decision Recommendation\nMajor revision",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RadEyeVideo*, a novel method integrating radiologists’ eye-tracking data as video sequences to model both spatial and temporal gaze patterns during chest X-ray interpretation. This approach aims to enhance large vision–language models (LVLMs) by incorporating human visual attention dynamics. The authors evaluate the method using three general-domain LVLMs and report notable improvements, especially for impression generation tasks. Overall, the paper is clearly written, and the methodology and results are presented in a straightforward and comprehensible manner.\n\n**Major Comments**  \n1. **Clinical applicability:** While the concept of incorporating spatiotemporal gaze data is innovative, it remains dependent on such information during inference. This reliance limits practical application in real clinical contexts where eye-tracking data are unavailable. The authors should consider strategies that constrain this dependency to the training phase and explore zero-shot or gaze-free inference scenarios.  \n2. **Dataset limitations:** The experiments are based on the relatively small MIMIC-Eye dataset, which may not sufficiently represent the diversity of real-world imaging conditions or reading behaviors. This restricts the generalizability of the conclusions.  \n3. **Comparative evaluation:** The comparison is limited to a few general LVLMs with minimal adaptation to the medical domain. Incorporating additional task-specific or medical LVLMs would provide a more robust and representative evaluation.  \n4. **Related work:** The manuscript currently omits a related work section. It should reference recent benchmarking and evaluation studies of medical LVLMs (e.g., MedVH, MedThink, CARES) to properly situate the contribution.\n\n**Minor Comments**  \n- Reference formatting is inconsistent (e.g., Lines 311–321) and should be corrected.  \n- Clarify minor typographical or formatting issues throughout the reference list.\n\n**Summary Paragraph**  \nIn terms of *significance*, the work explores an engaging way to integrate human perceptual cues into LVLM training, addressing an underexplored direction. Its *innovation* lies chiefly in the use of video-based eye-gaze representations, which successfully capture temporal–spatial dynamics. However, *evaluation* is constrained by limited dataset size and restricted model comparisons, and *reproducibility* may be affected by the dependency on specialized gaze data and missing methodological details about model tuning.\n\n**Decision Recommendation**  \n**Major Revision.** The manuscript proposes a novel and promising concept but requires a broader evaluation, clearer positioning within prior work, and discussion on real-world applicability to strengthen its contribution.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes RadEyeVideo, a novel approach for integrating radiologists' eye-tracking data as video sequences to enhance large vision-language models (LVLMs) for chest X-ray analysis. The method converts eye fixation coordinates and durations into video frames showing red dots overlaid on CXR images, preserving both spatial and temporal dynamics of radiologist gaze patterns (Section 2.2, Equations 1-3). The authors evaluate three general-domain video-capable LVLMs (LongVA, VideoLLaMA2, LLaVA-OneVision) on report generation and diagnosis tasks using the MIMIC-Eye dataset (2,298 CXR images with eye-gaze annotations). Results show performance improvements of up to 25.4% on impression generation and 7.9% average improvement across all tasks when using scaled evaluation metrics (Table 3). The approach outperforms existing eye-gaze integration methods (heatmaps, textual prompts) and enables general-domain models to surpass specialized medical LVLMs on benchmark datasets.\n\n## Weaknesses\n\n• **Limited Dataset Scale and Generalizability**: The evaluation relies on a relatively small dataset that may not represent diverse clinical scenarios or imaging conditions.\n  - The MIMIC-Eye subset contains only 2,298 CXR images with severe imbalance (2,206 alpha vs 92 beta split images, Table 1), limiting statistical power for robust evaluation\n  - The dataset covers only posterior-anterior chest X-rays (Section 3.1), excluding other common radiological views and imaging modalities that would demonstrate broader clinical applicability\n  - No validation on external datasets or different institutional settings is provided, raising questions about generalizability across diverse patient populations and imaging protocols\n\n• **Mathematical Formulation Inconsistencies**: The video construction methodology contains unclear notation and potentially problematic assumptions about fixation significance.\n  - Equation 2 defines Fi = ti × fps but then states fps = 10 without justification for this specific frame rate choice, and the relationship between ti (presumably in seconds) and frame count lacks dimensional analysis\n  - The filtering criterion in Equation 1 assumes longer fixations are more diagnostically significant without empirical validation or literature support for this threshold-based approach\n  - The uniform sampling strategy in Equation 3 may lose critical temporal information by potentially skipping important fixation sequences, particularly for cases with highly variable gaze patterns\n\n• **Evaluation Methodology Limitations**: The experimental design and metrics present several methodological concerns that may affect result interpretation.\n  - The scaling approach using CheXagent as baseline (Equation 7) introduces potential bias since this model may not represent optimal performance across all metrics, and relative improvements may not reflect absolute clinical utility\n  - The temperature parameter set to 0 (Section 3.3.1) eliminates model stochasticity, which may not reflect realistic clinical deployment scenarios where some variability in report generation could be beneficial\n  - Missing statistical significance testing across the reported improvements makes it difficult to distinguish meaningful gains from random variation, particularly given the small beta split size\n\n• **Incomplete Technical Validation**: The approach lacks sufficient ablation studies and comparative analysis to validate core design decisions.\n  - The gaze radius fixed at 5 pixels (Section 2.2) lacks justification or sensitivity analysis to determine optimal values for different image resolutions or display conditions\n  - Limited exploration of alternative video construction methods (e.g., different frame interpolation strategies, dynamic radius sizing) prevents assessment of whether the proposed approach is optimal\n  - The in-context learning component shows inconsistent effects across models (Figure 4, right panel) but lacks deeper analysis of why certain models benefit while others (CheXagent, CXR-LLaVA) show performance degradation\n\n## Suggestions for Improvement\n\n• **Expand Dataset Scope and External Validation**: Address the limited scale and diversity of the current evaluation dataset.\n  - Conduct evaluation on larger, more balanced datasets with sufficient samples in both training and test splits to enable robust statistical analysis\n  - Include multiple chest X-ray views (lateral, oblique) and potentially other imaging modalities to demonstrate broader clinical applicability\n  - Validate the approach on external datasets from different institutions to establish generalizability across diverse patient populations and imaging protocols\n\n• **Strengthen Mathematical Foundations**: Provide rigorous justification and analysis for the mathematical formulations underlying video construction.\n  - Provide empirical validation for the frame rate choice (fps = 10) through systematic comparison of different frame rates and include proper dimensional analysis for the temporal-to-frame conversion\n  - Validate the assumption that longer fixations indicate diagnostic significance through correlation analysis with ground truth annotations or expert assessment studies\n  - Develop and evaluate alternative sampling strategies that preserve critical temporal sequences, such as importance-weighted sampling or adaptive frame selection based on fixation patterns\n\n• **Enhance Evaluation Rigor**: Implement more robust experimental design and statistical analysis to strengthen result interpretation.\n  - Replace or supplement the CheXagent-based scaling with absolute metrics and establish clinical significance thresholds rather than relying solely on relative improvements\n  - Include statistical significance testing with appropriate corrections for multiple comparisons, and consider incorporating confidence intervals for performance estimates\n  - Conduct sensitivity analysis across different temperature settings and report variability measures to better reflect realistic deployment scenarios\n\n• **Conduct Comprehensive Technical Analysis**: Perform systematic ablation studies to validate and optimize core design components.\n  - Systematically evaluate different gaze radius values and provide guidelines for selecting optimal parameters based on image characteristics and display conditions\n  - Compare alternative video construction approaches including different interpolation methods, dynamic sizing strategies, and temporal encoding schemes\n  - Analyze the inconsistent in-context learning effects through detailed investigation of model architectures, training procedures, and potential overfitting patterns to provide clearer guidance for practical implementation",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "## Opening Paragraph \nThis manuscript presents RadEyeVideo, an innovative method that integrates radiologists' eye-tracking data in the form of video sequences to enhance large vision-language models (LVLMs) specifically for chest X-ray analysis. By converting eye fixation coordinates into video frames, the authors claim to preserve the spatial and temporal dynamics of radiologist gaze patterns. The paper evaluates three general-domain LVLMs on report generation and diagnosis tasks using the MIMIC-Eye dataset, reporting significant performance improvements.\n\n## Major Comments\n1. **Limited Dataset Scale and Generalizability**:  \n   - The MIMIC-Eye dataset, with only 2,298 CXR images and a severe imbalance (2,206 alpha vs. 92 beta), limits statistical power and generalizability.  \n   - The focus on only posterior-anterior chest X-rays restricts broader clinical applicability, neglecting other common views and imaging modalities.  \n   - Absence of validation on external datasets raises concerns about the model's applicability across diverse patient populations.  \n\n2. **Mathematical Formulation Inconsistencies**:  \n   - Equation 2 defines Fi = ti × fps, with an unexplained frame rate of 10, raising issues with dimensional analysis.  \n   - The filtering criterion in Equation 1 lacks empirical validation for its assumption that longer fixations correlate with diagnostic significance.  \n   - The sampling strategy in Equation 3 risks losing critical temporal information due to potential skipping of important gaze sequences.  \n\n3. **Evaluation Methodology Limitations**:  \n   - The use of CheXagent as a baseline for scaling introduces bias, impairing the interpretation of absolute clinical utility.  \n   - Setting the temperature parameter to 0 might not reflect realistic clinical scenarios, which typically involve some variability in report generation.  \n   - The lack of statistical significance testing complicates the differentiation of meaningful improvements from random variation, especially with a small beta split.\n\n4. **Incomplete Technical Validation**:  \n   - Justification for the fixed gaze radius of 5 pixels is not provided, along with a lack of sensitivity analysis for different resolutions.  \n   - The exploration of alternative video construction methods is limited, restricting assessment of the proposed approach's optimality.  \n   - The in-context learning effects show variability across models without deeper analysis to clarify performance differences.\n\n## Minor Comments\n- Clarify notation and mathematical assumptions throughout.  \n- Include dimensional analysis for temporal conversions.  \n- Provide additional context or rationale for the choice of gaze radius and frame interpolation methods.\n\n## Summary Paragraph \nThe manuscript demonstrates significant innovation in integrating eye-tracking data into LVLMs for chest X-ray analysis, but some weaknesses hinder its overall contribution. While it shows potential in enhancing diagnostic capabilities, concerns regarding dataset limitations, mathematical inconsistencies, evaluation methodology, and technical validation are significant. Addressing these issues is crucial to ensure the study’s findings are both reproducible and clinically relevant.\n\n## Decision Recommendation \nMajor revision is recommended to address the issues identified, particularly related to dataset scale, statistical validation, and methodological rigor before considering acceptance.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RadEyeVideo*, a method for integrating radiologists’ eye-tracking data into large vision–language models (LVLMs) for chest X-ray interpretation. Eye fixation coordinates and durations are converted into video sequences that capture spatial–temporal gaze patterns, enabling general-domain video-capable LVLMs (LongVA, VideoLLaMA2, LLaVA-OneVision) to process visual attention cues. Evaluations on the MIMIC-Eye dataset (2,298 X-rays with gaze annotations) show notable performance gains—up to 25.4% in impression generation and 7.9% on average across tasks—when scaled against baseline metrics. The method reportedly exceeds previous gaze-integration techniques and allows general-domain LVLMs to outperform specialized medical models. Overall, the paper presents an interesting concept with promising results, though several methodological, mathematical, and validation limitations temper its conclusions.  \n\n**Major Comments**  \n1. **Dataset Limitations and Generalizability** – The study relies on a small, imbalanced dataset (2,206 vs 92 images) limited to posterior–anterior CXRs and lacks external validation, reducing confidence in generalizability across institutions and imaging contexts.  \n2. **Mathematical Formulation Issues** – Key equations (1–3) are under-justified: the fps value of 10 lacks rationale, the assumed relation between fixation duration and frame count is dimensionally unclear, and the fixation–significance assumption remains empirically untested. The uniform sampling step may discard important temporal details.  \n3. **Evaluation Methodology Concerns** – The use of CheXagent-based scaling may bias performance interpretation. Setting temperature = 0 removes stochasticity from text generation, and the absence of statistical significance testing makes it difficult to assess whether observed gains are meaningful.  \n4. **Incomplete Technical Validation** – Critical components, such as the 5-pixel gaze radius and in-context learning variations across models, are insufficiently analyzed. Lack of ablation and sensitivity studies limits understanding of design choices.\n\n**Minor Comments**  \n- Clarify notation and variable definitions in Section 2.2.  \n- Ensure dimensional consistency in equations.  \n- Improve figure readability (especially Figure 4).  \n- Define all acronyms upon first occurrence.  \n\n**Summary Paragraph (TMI Criteria)**  \nThe paper’s **significance** lies in proposing a creative method for incorporating human gaze data in LVLMs for radiology. **Innovation** is moderate to high, as the approach applies video reasoning to encode diagnostic attention patterns. However, the **evaluation** is weakened by the limited dataset, imbalanced design, and absence of statistical analyses. **Reproducibility** is constrained by incomplete methodological detail and missing ablation studies. Strengthening methodological transparency and external validation would substantially improve the work’s robustness.  \n\n**Decision Recommendation**  \n**Major Revision.** The concept is novel and potentially impactful, but methodological, mathematical, and validation deficiencies must be addressed before the paper can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** –  \n   ✅ The paper contributes a new methodological framework for integrating radiologists’ eye-tracking information into large vision–language models (LVLMs). The proposed *RadEyeVideo* reformulates eye-gaze data as video sequences that encode the spatiotemporal order of visual fixations, extending multimodal input representation and prompting strategies for chest X-ray report generation and disease diagnosis. This falls within IEEE TMI’s interests in medical imaging methodology (multimodal fusion, machine learning, and human–AI interaction).\n\n2. **Novelty & Contribution Level** –  \n   The conceptual novelty lies in representing gaze as a dynamic video prompt rather than as static heatmaps or textual coordinates, showing that temporal gaze information modestly improves diagnostic and report‑generation performance for general‑domain LVLMs. While the algorithmic implementation is not mathematically deep, the framing of gaze modeling as spatiotemporal video prompting for clinical image interpretation is a novel methodological contribution. The work is more than incremental but not at the level of a rigorous theoretical advance, suggesting a “methodological innovation with empirical demonstration” rather than a new imaging theory.\n\n3. **Technical and Experimental Rigor** –  \n   The pipeline is clearly described, and major baselines (heatmap, fixation‑text, and no‑gaze) are compared. Multiple LVLMs are evaluated with radiology‑specific metrics (CheXbert, RadGraph, RaTEScore). However, the test size is small (MIMIC‑Eye subset ≈ 2.3 k images, only ≈ 90 in validation/test). The use of both training‑ and test‑set images to overcome imbalance raises data‑contamination concerns. The experiments are zero‑shot evaluations, so claims of outperforming specialized medical models may be somewhat overstated. Quantitative differences are relatively modest (average +7.9 %). A stronger validation on independent eye‑tracking data or additional imaging modalities would improve generality.\n\n4. **Clarity and Presentation** –  \n   The manuscript is readable and well structured. Figures are informative, though occasionally dense with fine print. Grammar and English quality are good. The mathematical notation is clear but simplistic; references are properly cited.  \n\n5. **Ethical and Reproducibility Compliance** –  \n   The paper states adherence to MIMIC‑Eye license and anonymization rules, and code is available for dataset recreation, satisfying reproducibility expectations. No ethical or privacy concerns noted.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n1. **Summary**   \n   The authors propose *RadEyeVideo*, a dynamic eye‑gaze video prompting approach for enhancing LVLM‑based chest‑X‑ray interpretation. By converting sequences of gaze fixations into short videos encoding spatial and temporal attention, the method supplies radiologist‑like perceptual cues to general‑domain LVLMs for report generation, summarization (“Impression”), and diagnosis tasks. Evaluations on the MIMIC‑Eye dataset using several LVLMs show improvements—particularly a ~25 % increase in scaled metrics for the Impression section versus no‑gaze prompting—and performances comparable to or exceeding domain‑specific models such as CheXagent and CXR‑LLaVA.\n\n2. **Strengths**\n   - Introduces a new formulation for incorporating eye‑tracking into LVLMs through temporal video representation.  \n   - Demonstrates measurable gains across diverse models and metrics.  \n   - Addresses human‑in‑the‑loop and explainable AI aspects relevant to medical imaging practice.  \n   - Provides implementation details and ablation studies (frame count, fixation filtering, in‑context exemplars).  \n   - Promotes reproducibility and ethical compliance.\n\n3. **Weaknesses**\n   - Evaluation dataset is small and partially overlapping with training data, limiting statistical confidence.  \n   - Gains, while consistent, are moderate; some models show regressions on diagnosis tasks.  \n   - The method is largely empirical—no explicit modeling or theoretical analysis of spatiotemporal gaze dynamics.  \n   - Comparison to state‑of‑the‑art medical LVLMs may not be strictly fair due to scale and data leakage.  \n   - Limited discussion on clinical applicability, user workflow integration, and model interpretability.\n\n4. **Major Comments**\n   1. **Dataset validity:** Clarify exactly how training/test overlap was mitigated and provide quantitative assessment of any Leakage risk. Consider additional evaluation on a held‑out or external dataset.  \n   2. **Quantitative significance:** Include confidence intervals or statistical testing for the reported percentage gains to show robustness.  \n   3. **Ablation scope:** Extend ablation to examine the temporal order contribution explicitly (e.g., shuffle sequence vs. preserve order).  \n   4. **Methodological depth:** The filtering and sampling equations are straightforward; strengthen by formalizing how temporal gaze cues influence attention alignment or embedding fusion in LVLMs.  \n   5. **Clinical interpretation:** Provide qualitative examples or radiologist review demonstrating the clinical meaningfulness of gaze‑prompted outputs.  \n   6. **Generalization:** Discuss applicability to other imaging modalities (CT, MRI) and potential limitations of using general‑domain LVLMs for regulated medical use.\n\n5. **Minor Comments**\n   - Improve figure readability; ensure labels and legends are legible in grayscale.  \n   - Typographical issues: e.g., spacing in “E NHANCING” of title, minor capitalization inconsistencies.  \n   - Include units for memory measurements in Appendix A.1 more clearly.  \n   - Provide abbreviated metric definitions inline for clarity rather than relegating all to appendix.  \n   - Streamline abstract to emphasize methodological contribution rather than quantitative metrics first.\n\n6. **Overall Recommendation** – **Major Revision**  \n   The manuscript presents a promising and timely methodological idea—capturing expert visual behavior as spatiotemporal prompts—but current validation is limited by dataset scale and depth of analysis. With expanded evaluation, stronger statistical treatment, and clearer methodological rationale, the paper could merit publication in IEEE TMI.\n\n7. **Confidence Level** – **4/5** (high confidence in technical and methodological assessment)\n\n---\n\n**Constructive Outlook for Authors**\n\nTo elevate this work to IEEE TMI standards, the authors should (1) validate on independent and larger datasets to demonstrate generalizability, (2) quantify statistical significance of improvements, (3) articulate theoretical or interpretive insight into why temporal gaze order benefits multimodal alignment, and (4) better discuss clinical impact and reproducibility within imaging workflows.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RadEyeVideo*, a methodological framework for integrating radiologists’ eye‑tracking information into large vision–language models (LVLMs) for chest X‑ray interpretation. By converting sequences of gaze fixations into short video prompts encoding spatial and temporal viewing patterns, the authors aim to enhance LVLM performance on report generation and diagnosis tasks. The work is well written and organized, providing clear motivation and implementation details. It aligns well with IEEE TMI’s interests in multimodal medical imaging, human–AI interaction, and machine learning methodology.\n\n---\n\n**Major Comments**  \n1. **Dataset validity and overlap:** The evaluation uses a small MIMIC‑Eye subset (~2.3 k images, with ≈ 90 for validation/test). Clarify how potential training/test overlap was prevented and quantify any data‑contamination risk. Independent or external testing would strengthen claims of generalizability.  \n2. **Quantitative robustness:** The reported average improvement (~7.9 %) appears modest; some tasks show regressions. Include statistical tests or confidence intervals to assess significance.  \n3. **Ablation and temporal order:** Extend ablation experiments to evaluate the effect of preserving versus shuffling temporal gaze order, explicitly showing how sequence dynamics contribute.  \n4. **Methodological depth:** The core filtering and sampling steps are straightforward. Consider formalizing how temporal gaze cues inform LVLM attention alignment or embedding fusion to add theoretical rigor.  \n5. **Clinical interpretability:** Provide qualitative cases or expert annotations illustrating how gaze‑prompted outputs improve diagnostic reasoning.  \n6. **Generalization and scope:** Discuss applicability to other modalities (CT, MRI) and acknowledge potential regulatory barriers when deploying general‑domain LVLMs in clinical contexts.\n\n---\n\n**Minor Comments**  \n- Improve figure readability, ensuring legible labels and grayscale clarity.  \n- Correct minor typographic issues (e.g., title spacing, inconsistent capitalization).  \n- Specify units for memory metrics in Appendix A.1.  \n- Define specialized metrics (CheXbert, RadGraph, RaTEScore) inline where first used.  \n- Refine abstract to emphasize methodological contribution before quantitative results.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study offers a methodologically interesting contribution by treating radiologists’ gaze as dynamic video input to LVLMs, a step beyond static heatmap representation. Its significance lies in linking human perceptual behavior with automated image interpretation, addressing explainability and human–AI synergy. Innovation is moderate—conceptually novel but empirically driven. Evaluation, while promising, is constrained by dataset scale, overlapping samples, and limited statistical analysis. Reproducibility is strong, with code availability and adherence to MIMIC‑Eye licensing. Overall, the work presents a creative approach that requires broader and more rigorous validation before publication readiness.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe paper demonstrates a promising and relevant methodological idea but would benefit from expanded evaluation, formal statistical analysis, and deeper exploration of temporal gaze dynamics to meet IEEE TMI’s standards for significance and rigor.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph:**  \nThe manuscript introduces *RadEyeVideo*, a novel methodological framework designed to integrate radiologists’ eye-tracking information into large vision–language models (LVLMs). By reformulating eye-gaze data as dynamic video sequences, this work aims to enhance multimodal input representation and improve chest X-ray report generation and disease diagnosis. Overall, the manuscript is clear and situated within the scope of IEEE TMI, aligning with interests in medical imaging methodologies, multimodal fusion, and human–AI interaction.\n\n**Major Comments:**\n1. **Dataset Validity:** Clarify how the overlap between training and testing datasets was managed and provide a quantitative assessment of any potential data leakage. Additionally, consider evaluating an external dataset to enhance robustness.\n2. **Quantitative Significance:** Include confidence intervals or statistical tests for reported gains to bolster claims of robustness.\n3. **Ablation Scope:** Extend the ablation studies to explicitly assess the contribution of temporal order by comparing shuffled sequences versus preserved order.\n4. **Methodological Depth:** The filtering and sampling equations presented are straightforward; further formalization is needed to clarify how temporal gaze cues influence attention alignment and embedding fusion within LVLMs.\n5. **Clinical Interpretation:** Provide qualitative examples or feedback from radiologists to demonstrate the clinical significance of the gaze-prompted outputs.\n6. **Generalization:** Discuss the applicability of this method to other imaging modalities such as CT and MRI, addressing potential limitations in utilizing general-domain LVLMs for regulated medical applications.\n\n**Minor Comments:**\n- Improve figure readability by ensuring that labels and legends are discernible in grayscale formats.\n- Note typographical issues, including spacing inconsistencies in the title (\"E NHANCING\") and minor capitalization errors.\n- Clarify units for memory measurements in Appendix A.1.\n- Provide abbreviated definitions for metrics within the main content rather than relegating them entirely to the appendix.\n- Streamline the abstract to center more on the methodological contributions as opposed to emphasizing quantitative metrics.\n\n**Summary Paragraph:**  \nThis manuscript presents *RadEyeVideo*, a method aimed at enhancing LVLM-based chest X-ray interpretation through eye-tracking video prompts. While demonstrating clear improvements in specific metrics, particularly a ~25% increase in performance for the 'Impression' section, the evaluation is constrained by a small dataset size, partial training overlap, and the empirical nature of the claims. Although the study addresses reproducibility and ethical compliance, additional validation and deeper analytical insight are necessary to achieve the significance, innovation, evaluation, and reproducibility criteria expected for IEEE TMI publications.\n\n**Decision Recommendation:**  \nMajor Revision. The manuscript provides a promising methodological framework, but its validation is limited due to dataset scale and the depth of analysis. With enhanced evaluations, stronger statistical treatment, and clearer methodological justification, the paper could warrant publication in IEEE TMI.\n\n**Confidence Level:** 4/5 (high confidence in technical and methodological assessment)  \n\n**Constructive Outlook for Authors:**  \nTo meet IEEE TMI standards, authors should (1) validate findings on larger, independent datasets, (2) quantify the statistical significance of improvements, (3) articulate the theoretical basis for the benefits of temporal gaze order, and (4) better address potential clinical impacts and reproducibility within real-world imaging workflows.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Honghan Wu",
      "Jinge Wu",
      "Yunsoo Kim"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "remote_url": "https://openreview.net/pdf/a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning Robust Representations for Medical Images via Unifying (Self-)Supervisions",
    "status": "not_started",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "medical image pre-training",
      "medical image representation learning"
    ],
    "abstract": "Pre-training medical image encoder to provide robust, task-agnostic representations is highly valuable, as it enhances the understanding of medical images and is important for performing many data-scarce analysis tasks. Current pre-training works are unable to integrate various types of supervisions, including self-supervision and external supervision such as segmentation annotations, while they are highly valuable for medical image understanding. Therefore, in this paper, we take the first step toward exploring unifying all common types of supervisions into a pre-training framework through a same scalable way. This require the pre-training framework being both unified, for accommodating diverse data and extensible, and effective, for making heterogeneous data synergistically assist unknown downstream tasks. To this end, we propose UmiF, whose principle is that once converted into token embeddings in a unified space, all diverse supervisions can be effectively utilized via contrastive learning and mask modeling with a same way. With UmiF, we pre-train on 1.66M samples from 14 public datasets, significantly surpassing previous efforts in terms of the dataset scale. We obtain and release the UmiF model, which achieved state-of-the-art performance across various downstream tasks, including classification, segmentation, and detection, retrieval and VQA.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe Authors propose UmiF, a framework that aims to unify pre-training of any arbitrary Image-Label pair to train a robust encoder for any modality. To achieve this the authors propose three tokenizers, that each allows embedding to a shared unified token space (One Image, One Text, One Image Labels/Segmentation). These image / label tokens are either left split or merged to a certain degree, before being used for SSL training through contrastive training in a CLIP'esque fashion or through a reconstruction task. \nThey train their model on a wide variety of paired pre-training datasets and evaluate it on a broad set of downstream tasks, highlighting the final performance of UmiF's method.\n\nWhile innovative, the experiments are insufficient to highlight the proposed methodology. The authors stack a) a larger pre-training dataset b) Token mixing c) Masked and Contrastive losses together and don't provide experiments that disentangle which part brings performance and which part does not. Moreover, the presentation and language used in this paper are of insufficient quality and need a lot of work.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 3\n\n### Strengths\n\nThe proposal to not only include Image-Report pairs but also Images with other Supervision signals is an interesting and to my knowledge novel premise for the medical domain. Their idea of mixing image and supervision tokens are also innovative.\nMoreover, the amount of experiments conducted is broad, highlighting the generality of the ViT feature extractor.\n\n### Weaknesses\n\nWhile the premises are highly interesting the paper in it's current form has some issues:\n1. **Stacking of contributions** Currently the authors stack a variety of things on-top and don't ablate it properly, namely a) a larger pre-training dataset b) the token mixing block and c) the multiple SSL losses. Currently, it is impossible for a reader to know if their methodology is better downstream than the competing methods, as they create a much larger training data corpus. Maybe it's the masked reconstruction component, maybe not.\n2. **Presentation** The presentation of the paper as of right now is poor. It was very hard to read, as the language quality leaves a lot of room for improvement and should be checked by an English speaker to rework the manuscript. Moreover, Fig. 1 does a bad job of explaining the method contributing to difficulties in understanding the proposed method. Figures in the appendix are badly presented: Fig. 2 text is way too large.  Table 3 does not have a caption, The description of the Image-Segment Dataset (Section A.4) is basically non-existent and should be filled accordingly. \n3. **Reproducibility** Currently the author's don't sufficiently explain their configuration of their methods. How was _r_ chosen ? In the text it is mentioned sometimes 1 and sometimes random between [0,1]. How did the authors split their data? Was there a train-test split during pre-training and fine-tuning? Was there different weightings between the losses?\n\n### Minor Points\n- The ablation of _r_ values does not contain r of 0.9 and 1.0 - It's mentioned in the text that these performed substantially worse, but I would like to have these values included in the table. Moreover this table should provide not only RSNA 1% AUC values. The downstream adaptation are just learning of a linear-layer so please provide ablations on more datasets and all values to show if the mixing of tokens actually provides a benefit.\n- The distinction between what this paper does relative to other paper's feels not well worked out. It would help a lot to see what makes this work distinct.\n- Similarities to MedUniC Paper. This paper's Table 2 is very similar to their Table 2 -- I would prefer to highlight this in the caption. \n- There are so many typos in this manuscripts. E.g. spellings of baseline methods:  MedKLIIP/MedKILP/MedKLIP. It feels like no one proof-read this paper ever.\n- The Algorithm 1 is way too text heavy. If the authors want to go into detail about the sampling of their datasets they should move this into a separate algorithm to keep readability high.\n- The authors mention the importance of sampling smaller dataset more regularly but provide no results. Would be great to see an ablation table on this claim in the appendix.\n\n### Questions\n\n- Q1: Did you ablate the importance of using the masking and the contrastive loss by themselves?\n- Q2: How was _r_ chosen?\n- Q3: Shouldn't r be symmetric around 0.5? r=0 would just flip image embeddings to supervision and the other way around. Is this correct, or if not why not? \n- Q4: Have you tried replacing the mixing with a standard masking/token drop-out layer? Would be interesting to see if one actually has to mix tokens or if the dropping of tokens provides a similar regularizing effect.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Overview of the Manuscript**  \nThe authors present UmiF, a framework designed to unify the pre-training of arbitrary image-label pairs, aiming to enhance encoder training across various modalities. The framework incorporates three distinct tokenizers that create a shared token space for images, texts, and image labels/segmentations. By evaluating their model on diverse paired pre-training datasets and a range of downstream tasks, the authors highlight the performance outcomes of UmiF. While the concept is innovative, the experimental design lacks clarity, and the overall presentation requires significant improvement.\n\n**Major Comments**  \n1. **Ablation Studies**: The authors combine several components (a larger pre-training dataset, token mixing, and multiple SSL losses) without clear ablation studies to determine the contribution of each to performance. This limits the ability to evaluate which aspects of the methodology are effective compared to competing methods.\n2. **Presentation Quality**: The manuscript suffers from poor language quality and lacks clarity, necessitating a thorough review by a proficient English speaker. Additionally, Figure 1 fails to adequately convey the methodology, contributing to reader confusion. Figures in the appendix also have issues, such as overly large text in Fig. 2 and a lack of caption for Table 3. The description of the image-segment dataset (Section A.4) is insufficient and requires further elaboration.\n3. **Reproducibility Issues**: The configuration of methods is inadequately explained (e.g., the selection of parameter _r_ is inconsistent). Clarification on data splitting for pre-training and fine-tuning, as well as weighting between losses, is necessary.\n\n**Minor Comments**  \n- The ablation study of _r_ values omits values of 0.9 and 1.0, which should be included in the table. The table should also present AUC values on additional datasets to assess token mixing efficacy.\n- The differentiation between this manuscript and previous works is unclear and should be better articulated.\n- There are notable similarities with the MedUniC paper that should be acknowledged in the caption of Table 2.\n- Numerous typos are present throughout the manuscript, notably in baseline method names (e.g., MedKLIIP/MedKILP/MedKLIP).\n- Algorithm 1 is text-heavy and would benefit from a separate algorithm for dataset sampling details to enhance readability.\n- Claims regarding the sampling of smaller datasets are not supported by results; including an ablation table in the appendix would be beneficial.\n\n**Summary Paragraph**  \nThe manuscript introduces an innovative framework with potential significance for the medical field; however, it lacks robust evaluations to validate its claimed advantages (evaluation), and the presentation is not suitable for publication (clarity). The reproducibility of the results is hampered by unclear methodological details. Despite its strengths in concept development and broad experimentation, substantial improvements are necessary to meet publication standards.\n\n**Decision Recommendation**  \nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified multimodal pretraining framework designed to handle arbitrary image–label pairs by learning a robust encoder adaptable to different modalities. The approach employs three tokenizers (image, text, and image-label/segmentation) to embed inputs into a shared token space, enabling contrastive and reconstruction-based self-supervised learning. The method is trained on multiple paired datasets and evaluated across diverse downstream tasks. While the idea of unifying image and supervision tokens is novel and supported by a broad experimental setup, the current presentation and experimental analysis are insufficient to clearly validate the proposed contributions.\n\n---\n\n**Major Comments**  \n1. **Disentangling Contributions:** The method combines several components—large-scale pretraining data, token mixing, and dual SSL losses (masked and contrastive)—without clear ablation studies. This stacking makes it impossible to determine which factor drives performance improvements. The paper should provide controlled experiments isolating each contribution.  \n2. **Presentation and Readability:** The manuscript is very difficult to follow due to weak language quality, dense text, and poor figure design. Figure 1 does not effectively convey the method, Figure 2’s text is oversized, Table 3 lacks a caption, and the Image‑Segment dataset description (Sec. A.4) is missing. Substantive English editing and clearer visual explanations are needed.  \n3. **Reproducibility:** Key implementation details are missing. The selection and variability of *r* are unclear (sometimes fixed, sometimes random). Data splits for pretraining and fine‑tuning, and relative loss weightings, are not specified. Without this, reproducing the work is infeasible.\n\n---\n\n**Minor Comments**  \n- Include *r* = 0.9 and 1.0 results in ablations and expand evaluation beyond RSNA 1% AUC to additional datasets.  \n- Clarify how this work differs from related approaches (e.g., MedUniC); explicitly note similarity of Table 2 content.  \n- Numerous typos and inconsistent baseline spellings (e.g., MedKLIIP/MedKLIP) require correction.  \n- Simplify Algorithm 1 or split excessive detail; it hinders readability.  \n- Provide evidence for the stated benefit of oversampling smaller datasets.  \n\n---\n\n**Summary (Significance, Innovation, Evaluation, Reproducibility)**  \nThe work addresses an important problem of multimodal representation unification and proposes an original approach that mixes supervision and image tokens. However, insufficient experimental isolation and incomplete methodological reporting undermine the validity of the results. The innovation is promising, yet the weak presentation, unclear evaluation design, and reproducibility gaps significantly limit its current impact.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents UmiF, a pre-training framework for medical image encoders that integrates multiple types of supervision, including self-supervision and annotations like segmentation labels, into a unified approach. UmiF creates a common embedding space with a token grouping strategy to leverage diverse data types for various downstream tasks. Pre-trained on 1.66 million samples from 14 public datasets, UmiF was evaluated in classification, segmentation, detection, retrieval, and VQA tasks.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- The paper introduces unifying representations from multiple supervisions into a single embedding space for self-supervised learning and proposes a grouping strategy for mixed learning of representation vectors.\n\n- The model’s effectiveness is validated through evaluations across four different downstream tasks.\n\n### Weaknesses\n\n- **Contrastive Learning Design Concerns**: The design of the contrastive learning setup after grouping raises questions. According to the paper, a positive pair is represented by \\(f1_i, f0_j\\) where i and j are indices from different data points, meaning \\(f1_i\\) and \\(f0_j\\) are from different samples. Typically, a positive pair should be \\(f1_i, f0_i\\), where both elements come from the same sample, making the current approach unclear.\n\n- **Unfair Comparisons in Downstream Tasks**: There are substantial fairness issues in the downstream task comparisons. Competing models, such as Med-Unic and MGCA, are pre-trained on datasets with 380K and 217K samples respectively, whereas this study uses 1.66 million data pairs, including 1 million images. The model’s performance advantage in downstream tasks may stem from this large data disparity, making it difficult to attribute improvements solely to the proposed pre-training strategy.\n\n- **Performance in Table 2**: In Table 2, despite using more training data and supervision than Med-Unic, the proposed model does not achieve the best performance, which raises questions about the efficiency of the approach.\n\n- **Limited Ablation Study on Parameter r**: In the ablation study on the parameter r, only 1% of the RSNA dataset is used, rather than the full dataset, and no similar experiments are conducted on other datasets. It is unclear if the chosen r value on RSNA is robust and generalizable to other tasks, as this limited evaluation does not provide strong evidence of robustness.\n\n- **Inconsistencies Between Text and Figures**: There are inconsistencies between the text and figures. For instance, the text describes vector groups as Group 1 and Group 0, but the figure labels them as Group 1 and Group 2. \n\n- **CLS Token Generation Unclear**: The generation of the CLS token information is not clearly explained.  According to the figure, the CLS token appears to be an output of the Flexible Token Grouping, but the paper does not specify how the CLS token is produced. Further clarification on this process would improve understanding.\n\n### Questions\n\nplease refer to Weaknesses\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "### Opening Paragraph\n\nThis manuscript introduces UmiF, a pre-training framework designed for medical image encoders. UmiF combines various types of supervision, including self-supervision and segmentation labels, into a cohesive approach intended to create a common embedding space. It leverages a token grouping strategy to harness diverse data types for several downstream tasks, having been pre-trained on 1.66 million samples from 14 public datasets and evaluated across classification, segmentation, detection, retrieval, and VQA tasks.\n\n### Major Comments\n\n1. **Contrastive Learning Design Concerns**: The positive pair representation indicated by \\(f1_i, f0_j\\) raises uncertainty, as typically, positive pairs are derived from the same sample.\n  \n2. **Unfair Comparisons in Downstream Tasks**: The use of significantly larger training datasets (1.66 million samples) compared to competitors, which used 380K and 217K samples, may unfairly skew performance comparisons, making it hard to attribute effectiveness solely to the proposed pre-training strategy.\n  \n3. **Performance in Table 2**: Despite the increased training data and supervision over Med-Unic, the proposed model does not achieve superior performance, questioning the efficiency of the approach.\n  \n4. **Limited Ablation Study on Parameter r**: The ablation study employs only 1% of the RSNA dataset, and other datasets are not similarly evaluated, leaving the robustness and generalizability of the chosen r value unproven.\n  \n5. **Inconsistencies Between Text and Figures**: Discrepancies exist between the textual descriptions and figure labels regarding vector groups, which could lead to confusion.\n  \n6. **CLS Token Generation Unclear**: The method through which the CLS token is generated remains ambiguous. Although suggested as an output of the Flexible Token Grouping, details on its production are insufficient.\n\n### Minor Comments\n\n- Additional clarity on the generation of the CLS token is required to enhance reader understanding.\n- The manuscript should ensure consistency between text and figures to avoid confusion.\n\n### Summary Paragraph\n\nIn summary, while this study presents a notable contribution to the field by proposing a unified framework for medical image encoders, it faces several challenges related to its design, evaluation, and clarity. The significance and innovation of UmiF are evident, yet the evaluation has limitations that undermine reproducibility and robustness. The strengths lie in the comprehensive approach and extensive dataset used, but weaknesses in methodological transparency and fairness in comparisons must be addressed.\n\n### Decision Recommendation\n\nMajor revision is recommended to address the identified issues before the manuscript can be considered for acceptance.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified pre-training framework for medical image encoders that combines multiple forms of supervision—including self-supervised learning and annotated signals such as segmentation labels—into a single embedding space. The approach employs a token grouping strategy to align heterogeneous data sources, aiming to improve performance across diverse downstream medical imaging tasks. The authors pre-train on 1.66 million samples from 14 public datasets and evaluate results on classification, segmentation, detection, retrieval, and visual question answering tasks. Overall, the paper is clearly written, but some methodological choices and comparisons raise concerns about fairness and interpretability.\n\n---\n\n**Major Comments**  \n1. **Contrastive Learning Design** – The description of positive and negative pair formation is unclear. The paper defines positive pairs across different samples \\((f1_i, f0_j)\\), whereas contrastive frameworks typically use features from the same sample \\((f1_i, f0_i)\\). This mismatch requires clarification and potentially undermines the soundness of the learning formulation.  \n2. **Fairness of Downstream Comparisons** – Competing models (e.g., Med-Unic, MGCA) are trained on substantially smaller datasets (380K and 217K samples) compared to UmiF’s 1.66 million samples. This disparity raises questions about whether observed improvements stem from methodological advantages or simply from larger data exposure.  \n3. **Performance Results (Table 2)** – Despite greater data volume and supervision, UmiF does not surpass Med-Unic in some metrics, suggesting limited efficiency or benefit from the proposed strategy.  \n4. **Ablation Study on Parameter r** – The ablation is restricted to 1% of the RSNA dataset and not replicated on other datasets. As such, the claimed generalization of the selected r value is weak.  \n5. **Inconsistencies Between Text and Figures** – Figure labels (Group 1, Group 2) do not match the text designations (Group 1, Group 0), causing confusion in interpreting the grouping mechanism.  \n6. **CLS Token Generation** – The role and derivation of the CLS token are insufficiently explained. Although shown as an output of the Flexible Token Grouping module, the paper lacks details on how this token is generated.\n\n---\n\n**Minor Comments**  \n- Improve alignment of figure terminology with text to ensure consistency.  \n- Clarify legends and notations in figures related to the grouping process.  \n- Check for small phrasing ambiguities in method descriptions.\n\n---\n\n**Summary Paragraph (Significance, Innovation, Evaluation, Reproducibility)**  \nThe study tackles an important problem—building unified medical image representations across supervision types—and contributes an innovative grouping strategy. However, methodological ambiguity in contrastive pair design and limited ablation analysis weaken the evaluation of robustness. Comparative advantages are difficult to attribute solely to the method because of notable data imbalance. While reproducibility appears feasible based on public datasets, greater clarity in implementation details (e.g., CLS token, parameter settings) would improve confidence. The work is conceptually interesting but requires stronger experimental justification.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a multi-supervision unification strategy for medical image pretraining. The method allows report, segmentation, and classification (+ some others) types of supervision to jointly train one representation. The used modality is Chest X-ray (CXR). The authors collected a large-scale dataset sourced from the public domain, reaching 1M images and 1.66M supervision labels. It is reported the model, namely, UmiF reaches SOTA for a number of downstream tasks.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\n1. The authors have gone through tremendous effort in collecting and inventorying the datasets. I can imagine the implementation wouldn't be easy to iron out the differences in the datasets and put them together to train one model. For that, I believe the significance of the paper should be pointed out.\n\n2. the benchmarking is comprehensive, ranging over the common medical image analysis tasks.\n\n### Weaknesses\n\n1. The clarity of the paper is a concern. Many places in the text lack proper explanation and are somewhat confusing.  For example, L230 \"This interesting design allows more diverse views and enriches the learning tasks with many possibilities, surpassing previous VL learning approaches.\", The authors should clearly state why it is interesting. What are the many possibilities? What are the other diverse views (isn't the modality just CXR)? What evidence indicates your method surpasses the previous VL learning approach?\n2. Furthermore, Sec 3.2, perhaps the most important section in the paper is not well written, I've read it a few times and I still don't believe I have grasped the exact approach. \n3. I find Figure 1 hard to follow, the quantities in Sec 3.2 should be mapped to the figure. I also don't get the colour coding in Figure 1 for those tokens. The yellow/blue/no boundary cubes are also a very confusing way of presentation.\n4. The improvement over the previous state-of-the-art is marginal around 1 point in various measurements. As the authors claim a large-scale dataset of 1.66M image-supervision pairs vs \"previous effort of mostly limited to 380K image-report pairs or 838K images\", it is worth rethinking whether the effort spent on training such a large model on the twice amount of data makes sense.\n5. The title claims \"learning robust representation for medical images ...\", medical images are not just CXR, I would recommend claiming a lesser scope unless common modalities such as MRI/CT are also used.\n6. In Sec 3.1, the authors use \"modality abstraction\", which sounds cool but I would say it is actually confusing, the procedure is a label format conversion.\n\n### Questions\n\nPlease address weaknesses #2&4. The paper could use some professional editing services.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**  \nThis manuscript presents a multi-supervision unification strategy for the pretraining of medical images, specifically targeting Chest X-ray (CXR) data. The proposed method enables the contemporaneous training of various supervisory signals, including report, segmentation, and classification. The authors have compiled an extensive public dataset consisting of 1 million images and 1.66 million supervision labels. The results indicate that the UmiF model achieves state-of-the-art performance on several downstream tasks.\n\n**Major Comments:**\n1. **Clarity Issues:** The paper's clarity is insufficient, with several sections lacking adequate explanations. For instance, on line 230, the claim that the design \"allows more diverse views and enriches the learning tasks\" is vague. The authors should clarify what these views entail and present evidence for how their method surpasses previous vision-language (VL) learning approaches.\n2. **Key Section Quality:** Section 3.2, a critical part of the manuscript, is poorly articulated. Multiple readings have not clarified the specific approach, necessitating a clearer exposition.\n3. **Figure Presentation:** Figure 1 is difficult to interpret; quantities discussed in Section 3.2 should correspond more directly with the figure elements. The color coding for tokens is unclear, and the use of yellow/blue/no boundary cubes complicates the communication of ideas.\n4. **Marginal Improvements:** The reported advancement over prior state-of-the-art results is marginal, showing only about a 1-point improvement across various metrics. Given the dataset's vast scale, the authors should re-evaluate whether this investment in training a large model is justified.\n5. **Scope of Claim:** The title suggests a broad claim of \"learning robust representation for medical images.\" However, as the work focuses solely on CXR, a more accurate title should reflect this narrower scope unless other modalities like MRI or CT are incorporated.\n6. **Terminology Confusion:** The term \"modality abstraction\" used in Section 3.1 is misleading; it would be clearer to refer to it as label format conversion.\n\n**Minor Comments:**\n- The paper would benefit from professional editing services to enhance overall clarity, particularly in addressing weaknesses noted in comments 2 and 4.\n\n**Summary Paragraph:**  \nIn relation to the editorial criteria, this study demonstrates significance through its extensive dataset and comprehensive benchmarking across common medical imaging tasks. However, clarity and presentation weaknesses are significant drawbacks. The innovation is tempered by marginal performance improvements, raising questions about the justification for the extensive dataset. Evaluation of the methods requires better articulation and clearer linkage between figures and text. Finally, reproducibility may be hindered by localized definitions and terminologies that could confuse readers.\n\n**Decision Recommendation:**  \nMajor revision recommended.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a multi-supervision unification strategy for medical image pretraining, termed **UmiF**. The key contribution lies in enabling multiple supervision types—report, segmentation, and classification, among others—to jointly train a single, unified representation using chest X-ray (CXR) data. The authors assemble a large-scale dataset of approximately one million images with 1.66 million supervision labels from public sources. Experimental results suggest that the proposed model achieves state-of-the-art performance on several downstream CXR tasks. While the scope and data collection effort are commendable, the manuscript’s clarity and justification of design choices require substantial improvement.\n\n---\n\n**Major Comments**  \n1. **Clarity and Explanation** – The manuscript lacks adequate detail in several places. Statements such as “enriches the learning tasks with many possibilities” (around L230) should be explicitly supported. The authors should define what constitutes “diverse views,” provide evidence for performance advantages, and explain how the approach surpasses prior vision-language methods.  \n2. **Method Description (Sec. 3.2)** – This section, central to the contribution, is difficult to follow. The exact formulation and procedural steps of the proposed method remain unclear even after multiple readings.  \n3. **Figure 1 Interpretability** – The figure is hard to interpret. The notation and quantities described in Sec. 3.2 should be directly linked to figure elements. The color coding (yellow/blue and partitioned cubes) is confusing and needs clearer explanation.  \n4. **Performance Significance** – Improvements over previous state-of-the-art methods are marginal (~1 point). Given the dataset’s significantly larger scale, the paper should discuss whether such incremental gains justify the extensive effort and computational cost.  \n5. **Scope of Claims** – The title and text refer broadly to “medical images,” but the experiments are limited to CXR. The claim should be narrowed to this modality unless evidence for other imaging types (e.g., CT, MRI) is provided.  \n6. **Terminology Precision** – The phrase “modality abstraction” (Sec. 3.1) is misleading; it appears to describe a label format conversion rather than true modality abstraction.\n\n---\n\n**Minor Comments**  \n- The manuscript would benefit from professional language editing to improve readability.  \n- Ensure consistency between textual descriptions and figure notation.  \n- Clarify visual elements and legends in all figures.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the large-scale dataset and unified training approach are valuable contributions. Regarding **innovation**, the concept of multi-supervision unification is interesting but insufficiently articulated. **Evaluation** appears comprehensive across common CXR tasks but shows only modest gains. **Reproducibility** is limited due to unclear methodological details and missing descriptions of implementation procedures. Overall, the work has potential but requires major clarification and more critical discussion of the incremental performance improvement relative to the significant data and model scale.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes UmiF (Unified medical image pre-training Framework), which aims to unify all common types of supervisions for medical image representation learning through a single scalable approach. The method converts diverse input pairs (image-report, image-caption, image-class, image-segment) into a unified token space using modality-specific tokenizers for radiology, language, and segmentation mask modalities (Section 3.1). A novel flexible token grouping strategy randomly splits tokens into two groups for contrastive learning and mask modeling (Section 3.2). The authors pre-train on 1.66M samples from 14 public datasets (Table 1) and evaluate on classification, segmentation, detection, retrieval, and VQA tasks. Results show state-of-the-art performance across most downstream tasks compared to existing medical vision-language pre-training methods (Tables 2-5).\n\n## Weaknesses\n\n• **Mathematical formulation lacks clarity and precision**\n  - The flexible token grouping strategy notation is confusing, with inconsistent use of subscripts and superscripts in binary bit sampling (Section 3.2, lines 225-233)\n  - The contrastive loss formulation in Equation 1 uses unclear indexing where i,j represent \"index of data point\" but the relationship between positive pairs is not mathematically precise\n  - The consistency loss in Equation 2 lacks explanation of how teacher network output t relates to the masked student inputs, creating ambiguity in the learning objective\n\n• **Experimental design contains significant methodological flaws**\n  - Dataset splits and evaluation protocols are inconsistently described across tasks, with some referencing prior work without clear specification (Section 4.2, lines 309-333)\n  - The comparison with baseline methods is unfair as acknowledged by authors - many baselines use \"designs tailored to specific data and annotations\" while UmiF claims generality but doesn't demonstrate this advantage systematically (Section 4.3, lines 359-367)\n  - Ablation studies are insufficient, particularly lacking analysis of the core unified tokenization approach and missing comparison of computational costs across different supervision types (Table 7 only shows input pair type ablation)\n\n• **Technical approach lacks sufficient novelty and theoretical justification**\n  - The unified token space concept is borrowed from prior general domain work (Wang et al., 2022b; Zhang et al., 2023) without substantial medical-specific innovation (Section 3.1, lines 171-173)\n  - The flexible token grouping strategy is not well-motivated theoretically - the choice of probability distributions and ratio r sampling lacks principled justification (Section 3.2, Table 6 shows only empirical results)\n  - The claim of \"first step toward exploring unifying all common types of supervisions\" is overstated given existing multi-modal pre-training work in medical domain (lines 58-61)\n\n• **Presentation quality suffers from unclear explanations and missing details**\n  - The relationship between different modality tokenizers and how they ensure compatibility in the unified space is not clearly explained (Section 3.1)\n  - Implementation details are sparse, particularly regarding batch construction strategy which is claimed to be \"critical for final performance\" but described only algorithmically in appendix (Section 3.3, lines 254-259)\n  - Figure 1 is cluttered and doesn't clearly illustrate the flexible token grouping mechanism or how different supervision types are processed\n\n## Suggestions for Improvement\n\n• **Enhance mathematical rigor and notation consistency**\n  - Provide precise mathematical definitions for the token grouping strategy with clear notation distinguishing between image tokens Xi and supervision tokens Xs throughout all equations\n  - Reformulate the contrastive loss to explicitly define positive pair construction and provide theoretical justification for the temperature parameter selection\n  - Add detailed mathematical derivation showing how the consistency loss enforces alignment between complete and masked views\n\n• **Strengthen experimental validation through comprehensive comparisons**\n  - Establish standardized evaluation protocols across all tasks with clearly specified train/validation/test splits independent of prior work citations\n  - Design fair comparison baselines by either implementing data-specific designs for UmiF or creating unified versions of competing methods\n  - Conduct extensive ablation studies including computational cost analysis, tokenization strategy comparisons, and systematic analysis of different architectural choices\n\n• **Provide stronger technical motivation and theoretical foundations**\n  - Develop medical domain-specific innovations beyond adapting general vision-language techniques, potentially incorporating medical knowledge graphs or anatomical priors\n  - Establish theoretical framework justifying the flexible token grouping approach with analysis of optimal probability distributions and sampling strategies\n  - Position contributions more accurately by clearly distinguishing novel aspects from adaptations of existing unified multimodal frameworks\n\n• **Improve clarity and completeness of presentation**\n  - Restructure Section 3.1 to clearly explain modality tokenizer design choices and compatibility mechanisms with concrete examples\n  - Move critical implementation details from appendix to main text, particularly batch construction and training procedures that affect reproducibility\n  - Redesign Figure 1 with separate panels for different components and add detailed captions explaining the flexible grouping mechanism with specific examples",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph:**\nThe manuscript presents UmiF (Unified medical image pre-training Framework), which aims to unify various supervision types for medical image representation learning within a single, scalable approach. By converting diverse input pairs—such as image-report, image-caption, image-class, and image-segment—into a unified token space through modality-specific tokenizers, the authors pre-train on a substantial dataset and demonstrate state-of-the-art performance across multiple tasks. However, the clarity of mathematical formulations and overall presentation quality could be significantly improved.\n\n**Major Comments:**\n1. **Mathematical formulation lacks clarity and precision:**\n   - Notation for the flexible token grouping strategy is inconsistent and confusing.\n   - Contrastive loss formulation lacks precise indexing, leading to ambiguity in representing positive pairs.\n   - Explanation of consistency loss in relation to teacher and student networks is insufficient.\n\n2. **Experimental design contains significant methodological flaws:**\n   - Dataset splits and evaluation protocols are not consistently described, with references to prior works lacking clarity.\n   - Comparison with baseline methods lacks fairness; the authors do not demonstrate the claimed generality of UmiF.\n   - Ablation studies are insufficient, particularly concerning the unified tokenization approach.\n\n3. **Technical approach lacks sufficient novelty and theoretical justification:**\n   - The unified token space is not significantly innovated for the medical field and relies heavily on prior general domain work.\n   - The theoretical motivation for the flexible token grouping strategy is weak and lacks principled justification.\n   - The assertion of pioneering unification across various supervisions is overstated given existing research.\n\n4. **Presentation quality suffers from unclear explanations:**\n   - The interplay between different modality tokenizers and their compatibility in a unified space is inadequately addressed.\n   - Critical implementation details are sparse and located in the appendix.\n   - Visual presentation in figures, particularly Figure 1, is cluttered and lacks clarity.\n\n**Minor Comments:**\n- Improve the mathematical rigor and notation consistency throughout the paper.\n- Clarify Section 3.1 regarding modality tokenizer design choices.\n- Include a more detailed explanation of batch construction and training procedures in the main text.\n- Redesign Figure 1 for better illustration and comprehension.\n\n**Summary Paragraph:**\nThe manuscript holds significance by attempting to unify medical image representation learning; however, its innovation is undermined by a lack of theoretical justification for its components. The evaluation suffers from methodological flaws, with comparisons that do not substantiate the claimed advantages of UmiF. Reproducibility is hindered due to unclear presentation and sparse implementation details. While there are strengths in the breadth of tasks and datasets utilized, critical revisions are essential for clarity, rigor, and context.\n\n**Decision Recommendation:**\nMajor revision.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          },
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *UmiF* (Unified medical image pre-training Framework), a large-scale approach designed to unify multiple types of supervision for medical image representation learning, including image-report, image-caption, image-class, and image-segmentation signals. Using modality-specific tokenizers and a flexible token grouping strategy, the framework integrates contrastive and masked modeling objectives. The authors pre-train the model on 1.66 million samples across 14 public datasets and evaluate it on diverse tasks—classification, segmentation, detection, retrieval, and VQA—reporting state-of-the-art results on most benchmarks. While the paper is ambitious and addresses a relevant challenge, several methodological, theoretical, and presentation issues limit its clarity and impact.  \n\n**Major Comments**  \n1. **Mathematical clarity:** The flexible token grouping strategy (Section 3.2) suffers from inconsistent notation, particularly in binary bit sampling. Equations 1–2 lack clear variable definitions and relationships among terms, leaving the contrastive and consistency loss formulations ambiguous. The connection between teacher outputs and masked student inputs should be better specified.  \n2. **Experimental design and evaluation:** The description of dataset splits and protocols is inconsistent across tasks, and several evaluations rely on unclear references to prior work. Baseline comparisons are potentially unfair, as noted by the authors themselves, and ablation studies are inadequate—focusing only on input pair types without analyzing core components or computational cost.  \n3. **Novelty and justification:** The unified token space extends existing general-domain frameworks (e.g., Wang et al., 2022b; Zhang et al., 2023) with limited domain-specific contributions. The theoretical motivation for the token grouping strategy and its sampling ratios is weak, and claims of being the “first unified framework” are overstated given existing multimodal pre-training methods.  \n4. **Presentation and reproducibility:** The explanation of how modality tokenizers align in the unified space is insufficiently detailed. Key implementation aspects—such as batch construction—appear only in the appendix. Figure 1 is visually dense and fails to clearly demonstrate the grouping mechanism or multi-supervision processing.  \n\n**Minor Comments**  \n- Improve mathematical notation, ensuring consistent subscripts and superscripts for image and supervision tokens.  \n- Clarify Section 3.1 with explicit examples of tokenizer compatibility.  \n- Move essential training and construction details into the main text.  \n- Redesign Figure 1 for readability and add descriptive captions.  \n\n**Summary Paragraph (TMI Criteria)**  \nIn terms of **significance**, the paper tackles an important problem—the unification of multimodal supervision in medical image learning. Regarding **innovation**, while the integration is ambitious, intellectual novelty is limited by dependence on prior general-domain frameworks. The **evaluation** is extensive but undermined by inconsistent data handling, limited ablations, and potentially unfair baselines. For **reproducibility**, missing details on batch formation and weak mathematical specification reduce transparency and replicability. Overall, the work is promising but requires substantial refinement before meeting publication standards.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 – Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \nThe manuscript proposes *UmiF*, a unified medical‑image pre‑training framework designed to integrate diverse supervision sources—self‑supervised, language‑supervised, and segmentation‑based—into a single tokenized contrastive/masked‑modeling paradigm. The paper deals with representation learning, transformer architectures, and large‑scale model training for medical imaging. This falls squarely within *IEEE TMI*’s methodological scope (imaging informatics, machine learning for image analysis, foundation‑model methodology).\n\n✅ Meets scope for a full technical review.\n\n**2. Novelty & Contribution Level**  \nThe unification of heterogeneous supervision (reports, captions, labels, segmentation) via a token‑level grouping strategy is conceptually novel for medical imaging. Previous works combined limited subsets (e.g., vision‑language or self‑supervision), but the authors propose a single token space and flexible grouping probability that extend contrastive/masked pre‑training beyond standard CLIP‑like formulations. The contribution is methodological rather than incremental, though evolutionarily related to OFA/Painter‑type ideas. Novelty level: moderate‑to‑high in the medical imaging context.\n\n**3. Technical and Experimental Rigor**  \nExperiments cover multiple downstream tasks (classification, segmentation, detection, retrieval, VQA) with strong baselines and large‑scale pre‑training (1.66 M pairs). Mathematical formulation of losses and training algorithm is clear. However:\n- Details on data harmonization, token dimension settings, and computational cost are limited.  \n- GPT‑4–generated or translated reports raise reproducibility/consistency questions.  \n- No quantitative uncertainty or statistical significance analysis.\n\nOverall rigor is good but could benefit from ablations on label noise and data quality.\n\n**4. Clarity and Presentation**  \nWell‑structured, but occasional grammatical errors and typographic slips (\"to to\", inconsistent capitalization). Figures are informative; minor editing for polish would help readability. The paper is lengthy but coherent.\n\n**5. Ethical & Reproducibility Compliance**  \nAuthors note the use of public, de‑identified datasets and intent to release code/model. Use of LLMs for data translation/generation should be accompanied by provenance and verification references. Otherwise meets typical reproducibility standards.\n\n---\n\n**Phase 2 – IEEE TMI Review Report**\n\n### 1. Summary\nThe paper introduces **UmiF**, a unified pre‑training framework for medical X‑ray representation learning. All supervision types (self‑supervised, vision‑language, label‑, and segmentation‑based) are converted into a common token space processed by one Vision Transformer. A *flexible token‑grouping* strategy randomly partitions tokens to form contrastive and masked‑modeling objectives, allowing diverse cross‑modal interactions. Trained on >1 M images from 14 datasets, UmiF achieves or exceeds state‑of‑the‑art performance across classification, segmentation, detection, zero‑shot, and VQA benchmarks, demonstrating generalizable medical‑image features.\n\n### 2. Strengths\n- Methodologically ambitious attempt to unify heterogeneous supervision within a token‑based transformer.  \n- Comprehensive experimental evaluation with strong results on diverse downstream tasks.  \n- Clearly documented data aggregation and ablation analyses.  \n- Public‑data training ensures reproducibility potential.  \n\n### 3. Weaknesses\n- Empirical victory may partly stem from vastly larger data scale rather than framework design.  \n- Limited theoretical justification of why token‑grouping yields synergy across modalities.  \n- Translation and GPT‑generated reports may introduce uncontrolled biases.  \n- No quantitative cost analysis (training time, compute) or uncertainty statistics.  \n- Focused solely on 2D chest X‑rays—generalizability to 3D or other modalities untested.\n\n### 4. Major Comments\n1. **Justify methodological benefit vs. scale.** Provide controls where UmiF and comparator models are pre‑trained on identical subsets to separate architectural from data‑scale effects.  \n2. **Clarify data provenance.** Explicitly describe quality checks for translated/generated reports and their ethical implications.  \n3. **Ablate grouping ratio mechanism.** Deeper analysis of how the sampling probability influences different downstream tasks would strengthen methodological insight.  \n4. **Discuss generalization to other modalities.** Explain adaptation to CT/MRI volumes and potential challenges in unified tokenization.  \n5. **Provide error and significance analyses.** Include variance or confidence intervals over multiple runs to gauge robustness.\n\n### 5. Minor Comments\n- Correct minor typos and spacing (“to to acquire”, inconsistent capitalization of “Supervision”).  \n- Improve figure readability (enlarge Figure 1 texts).  \n- Confirm all dataset citations use consistent years and full references.  \n- Describe critical hyper‑parameters (token dimension D, r‑sampling distribution, loss weighting).  \n- Consider shortening redundant dataset descriptions in Appendix for conciseness.\n\n### 6. Overall Recommendation\n**Major Revision**\n\n*Rationale:* The work is in‑scope, technically sound, and potentially impactful, but it requires stronger empirical isolation of algorithmic novelty from data scale, fuller methodological clarification, and improved reproducibility/ethical transparency before meeting IEEE TMI’s publication standard.\n\n### 7. Confidence Level\n**4 / 5** – High confidence in assessment; detailed understanding of medical imaging foundation‑model literature.\n\n---\n\n**Summary Verdict:**  \nPromising unified pre‑training framework for medical imaging with meaningful methodological advancement. Publishable after major revision emphasizing methodological clarity, scalability justification, and reproducibility verification.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *UmiF*, a unified medical image pre‑training framework designed to integrate heterogeneous supervision sources—self‑supervised, language‑supervised, and segmentation‑based—within a single tokenized contrastive/masked‑modeling paradigm using a Vision Transformer backbone. The paper targets large‑scale representation learning for medical imaging and reports consistent performance gains across classification, segmentation, detection, retrieval, and visual question‑answering tasks. Overall, the manuscript is well organized and technically solid, though some methodological clarifications and reproducibility details require improvement.\n\n---\n\n**Major Comments**  \n1. **Isolation of architectural benefit from data scale:** The impressive performance may largely reflect the very large pre‑training dataset rather than the unified design. The authors should include controlled experiments where UmiF and comparator models train on equivalent subsets to distinguish scale effects from framework advantages.  \n2. **Data provenance and ethics:** The paper references translated and GPT‑generated reports but does not detail provenance, quality control, or verification. Clear documentation of these aspects is required to ensure ethical and reproducible practice.  \n3. **Ablation on grouping ratio mechanism:** Further examination of the flexible token‑grouping probability and how it influences different downstream tasks would clarify the mechanism’s contribution.  \n4. **Generality across modalities:** The approach is validated only on 2D chest X‑rays. Discussion or preliminary evidence on how UmiF could be extended to 3D modalities (CT, MRI) would strengthen claims of generalizability.  \n5. **Quantitative robustness analysis:** The manuscript lacks uncertainty or significance statistics. Including variance or confidence intervals over multiple runs would enhance rigor.  \n6. **Computational transparency:** Provide details on model dimension settings, training time, and resource requirements to support reproducibility.\n\n---\n\n**Minor Comments**  \n- Correct minor grammatical and typographical issues (“to to,” inconsistent capitalization of “Supervision”).  \n- Improve Figure 1 readability by enlarging text.  \n- Standardize dataset citations and include complete references.  \n- Specify core hyperparameters (token dimension, sampling distribution, and loss weights).  \n- Condense repetitive dataset descriptions in the appendix.\n\n---\n\n**Summary Paragraph (TMI Criteria)**  \n*Significance:* The topic is timely and relevant as foundation models emerge in medical imaging.  \n*Innovation:* The unified tokenized approach integrating multiple supervision types provides moderate‑to‑high novelty for this field.  \n*Evaluation:* The experimental coverage is broad, but the absence of data‑scale controls and uncertainty reporting limits interpretability.  \n*Reproducibility:* While public data are used and code release is planned, more transparency on data generation, translation checks, and computational details is needed. Collectively, the paper demonstrates strong potential but requires deeper methodological validation and clarity to satisfy TMI standards.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe work is technically competent and potentially impactful but needs stronger evidence of architectural contribution, improved methodological transparency, and enhanced reproducibility documentation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          },
          {
            "text": "### Opening Paragraph\nThe manuscript presents **UmiF**, a unified pre-training framework designed for medical image representation learning. By integrating various supervision sources—self-supervised, language-based, and segmentation—the study proposes an innovative tokenized approach utilizing contrastive and masked modeling paradigms. The research encompasses significant areas in representation learning and transformer architectures, aligning well with the methodological scope of *IEEE TMI*.\n\n### Major Comments\n1. **Justification of Methodological Advantage vs. Data Scale:** A clearer distinction is needed between the benefits of the UmiF framework compared to smaller-scale pre-training approaches.\n2. **Data Provenance Clarification:** The manuscript should explicitly detail quality checks related to translated and GPT-generated reports and address any associated ethical concerns.\n3. **Ablation of Grouping Mechanism:** A more in-depth analysis of how the token sampling probabilities affect various downstream tasks will enhance the methodological understanding.\n4. **Generalization to Other Modalities:** The paper should discuss the challenges and methods for adapting UmiF for use with 3D imaging modalities such as CT and MRI.\n5. **Error and Statistical Significance Analysis:** Include a thorough examination of variance, confidence intervals, or uncertainty statistics to establish results' robustness.\n\n### Minor Comments\n- Address minor grammatical errors and spacing issues (e.g., “to to acquire”, inconsistent capitalization).\n- Enhance figure readability by enlarging text in Figure 1.\n- Ensure dataset citations are consistent, complete, and accurately reference publication years.\n- Provide details about key hyper-parameters, including token dimension \\(D\\), sampling distribution \\(r\\), and loss weighting.\n- Streamline redundant dataset descriptions in the Appendix for clarity and conciseness.\n\n### Summary Paragraph\nThis review highlights the strengths and weaknesses of the submitted manuscript in terms of significance, innovation, evaluation, and reproducibility. The framework shows substantial methodological advancement and potential for significant impact within the field. However, further attention is required regarding the empirical separation of algorithmic novelty from data size, as well as improved discourse on reproducibility and ethical considerations.\n\n### Decision Recommendation\n**Major Revision**  \n*Rationale:* The manuscript is relevant and technically sound but necessitates further modifications to adequately clarify its methodological contributions, justification of the data scale impact, and enhancements in reproducibility before it can meet the standards for publication in *IEEE TMI*.\n\n### Confidence Level\n**4 / 5** – High confidence in the assessment, demonstrating a comprehensive understanding of the relevant literature in medical imaging foundation models.",
            "model": "gpt-4o-mini",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Dongsheng Li",
      "Hoifung Poon",
      "Lili Qiu",
      "Naoto Usuyama",
      "Sheng Zhang",
      "XINYANG JIANG",
      "Xiaoxuan He",
      "Xufang Luo",
      "Yifan Yang",
      "Yuqing Yang",
      "Zilong Wang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_901796e0b8b3eeca3d767c044473e3b38b2d8cc1.pdf",
    "remote_url": "https://openreview.net/pdf/901796e0b8b3eeca3d767c044473e3b38b2d8cc1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  }
]