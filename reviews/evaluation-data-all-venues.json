[
  {
    "title": "Hierarchical Uncertainty Estimation for Learning-based Registration in Neuroimaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)"
    ],
    "keywords": [
      "Image registration",
      "uncertainty estimation",
      "medical image analysis"
    ],
    "abstract": "Over recent years, deep learning based image registration has achieved impressive accuracy in many domains, including medical imaging and, specifically, human neuroimaging with magnetic resonance imaging (MRI). However, the uncertainty estimation associated with these methods has been largely limited to the application of generic techniques (e.g., Monte Carlo dropout) that do not exploit the peculiarities of the problem domain, particularly spatial modeling. Here, we propose a principled way to propagate uncertainties (epistemic or aleatoric) estimated at the level of spatial location by these methods, to the level of global transformation models, and further to downstream tasks.  Specifically, we justify the choice of a Gaussian distribution for the local uncertainty modeling, and then propose a framework where uncertainties spread across hierarchical levels, depending on the choice of transformation model. Experiments on publicly available data sets show that Monte Carlo dropout correlates very poorly with the reference registration error, whereas our uncertainty estimates correlate much better. Crucially, the results also show that uncertainty-aware fitting of transformations improves the registration accuracy of brain MRI scans. Finally, we illustrate how sampling from the posterior distribution of the transformations can be used to propagate uncertainties to downstream neuroimaging tasks. Code is available at: https://github.com/HuXiaoling/Regre4Regis.",
    "reviews": [
      {
        "text": "### Summary\n\nAuthors present a method to estimate the uncertainty in medical image registration at 3 different stages: (1) on the estimate of the distribution of the deformation field at each voxel (assuming this is Gaussian and gathering a mean and standard deviation per point), (2) on the distribution of the fitted transformation, and (3) on the distribution of possible outcomes on the downstream task. The uncertainty from the first level is used in the transformation fitting step weighting down contributions from less certain pixels. By drawing samples of the fitted transformation, a distribution of results for the same downstream task is generated which is used to estimate the 3rd level of uncertainty. Authors demonstrate their approach in the context of registration-based Brain image segmentation where the given input image is deformed to match the standard MNI atlas and the labels from the atlas are propagated to the deformed image.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n* Authors clearly lay out their proposed framework describing the 3 level of uncertainty they they aim to model.\n* The results are presented such that each level of uncertainty is evaluated - this makes it easy for the reader to better understand the Authors important contribution.\n* Authors provide analysis of the estimates of uncertainty, and demonstrate that the aleotoric uncertainty corresponds better with coordinate prediction error at the first level.\n* The figures in the paper are very useful for visualizing the variations that arise from sampling the fitted transform - they demonstrate the fundamental problem with biomedical image registration in that the results from a downstream task are highly dependend on the transformation paramaters, but the uncertainty can be successfully quantified.\n\n### Weaknesses\n\n* The addition of uncertainty in the fit transform did not impact the result from the Demons transformation model. Authors explain that this is likely because the loss from the Demons method is used in the segmentation loss and that leads to the average coordinates landing close to the optimum. This is unclear - please can Authors expand on this and also elaborate on whether they would expect to see improvements if they used an alternative to Demons?\n\n### Questions\n\n* The panel labels in Figure 2 are badly formatted - please make these clearer.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces a framework to integrate uncertainty estimation into deep learning-based image registration for neuroimaging. By propagating epistemic and aleatoric uncertainties from voxel-level predictions to transformation models and downstream tasks, the framework enhances registration accuracy and reliability. Experiments show that this uncertainty-aware approach can boost brain MRI registration performance.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper addresses an important problem in neuroimaging, aiming to use uncertainty estimation to enhance the accuracy and reliability of deep learning-based registration.\n2. The figures and explanations are clear and well-organized, which makes complex ideas easier to understand.\n3. The empirical findings are insightful. The authors compare their proposed method with other different methods.\n\n### Weaknesses\n\n1. The presentation could be improved. The background, motivation, and knowledge gap are not clearly explained, making it challenging to follow the paper’s purpose and direction. Although the study is about why uncertainty estimation matters in brain registration, it doesn’t make a strong case for why this is important. There isn’t enough support or reasoning behind this focus, which leaves readers unsure about the value or impact of the work.\n2. The experiments are insufficient. This paper only compares its method to a single approach, RbR. Although RbR was released in 2024, it is still unpublished and available only on arXiv, which makes it less robust as a baseline for comparison. Including more established and published baselines would be essential to provide a solid foundation for evaluating the effectiveness of the proposed approach.\n3. From the experimental results (e.g., Table 1), the introduction of uncertainty estimation does not appear to provide a statistically significant improvement to the model. This makes it difficult to assess its effectiveness and raises questions about the practical value of incorporating uncertainty estimation into the approach.\n4. Most of the latest work in brain registration focuses on unsupervised learning, largely due to the high cost of collecting accurate transformation ground-truth for high-dimensional images (e.g., 3D MRI). However, this paper still stay on a supervised learning approach, making it seem less aligned with current trends and potentially less practical for real-world applications where labeled data is limited.\n5. The evaluation criteria lack validity. This paper collects ground-truth transformations using NiftyReg, a tool introduced around 15 years ago. Since then, more state-of-the-art methods have been shown to outperform NiftyReg, indicating that the ground-truth it provides may be inaccurate. This undermines the reliability of the results presented in the paper and raises concerns about the credibility of its findings.\n6. The novelty of this work is limited. The main contribution appears to be the addition of an uncertainty loss term, while the other loss terms and network architecture are based on existing work. This incremental improvement does not significantly advance the field, as much of the framework relies on previously established methods.\n\n### Questions\n\n1. Why is uncertainty estimation important in neuroimaging registration? What specific benefits does it bring, and why is it necessary?\n2. For modeling epistemic uncertainty, why did the author choose to use Monte Carlo dropout, and how exactly was it implemented?\n3. Given that the deformation field (non-linear transformation) is a high-dimensional tensor, how do authors verify the accuracy of the ground-truth labels when collecting them? As I understand, NiftyReg is a registration method—if this method is used to generate ground-truth labels, does that imply the proposed method can only perform as well as NiftyReg at best?\n4. The total training loss includes multiple terms marked as optional in the paper. Are these terms truly necessary? Was an ablation study conducted to assess their impact on performance?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this paper, the authors propose a registration framework that includes uncertainty estimation at three different levels (network output, transform models' parameters and downstream tasks). They show that uncertainty estimates correlate with registration errors and that estimating uncertainty can improve registration performance. The approach is applied to brain MRI registration.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The framework proposed is flexible and can be applied to a broad range of registration techniques.\n- The authors performed diverse experiments to validate their approach.\n- The results are mostly convincing.\n- The paper is well written and related works well introduced.\n\n### Weaknesses\n\n- The ground truths considered originate from automatic registration and segmentation approaches but it is not mentioned whether quality control was performed and so to which extent they can be relied on.\n- The experiments regarding the downstream task are limited to qualitative results and the fact that the segmentations are not overlaid on the T1w image only allows assessing differences between the segmentation maps but not whether one matches better the anatomy than another one.\n\n### Questions\n\n- In Figure 2, what does the error correspond to exactly?\n- Could the authors comment on the run time cost of their approach or other considerations regarding its practical application?\n\n===\n- The authors already mention many related works but they could also consider:\n    - Chen J et al. \"A survey on deep learning in medical image registration: New technologies, uncertainty, evaluation metrics, and beyond.\" arXiv preprint arXiv:2307.15615 (2023).\n    - Zhang X et al. \"Heteroscedastic Uncertainty Estimation Framework for Unsupervised Registration.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\nAnd comment especially on the second one.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Juan Eugenio Iglesias",
      "Karthik Gopinath",
      "Koen Van Leemput",
      "Malte Hoffmann",
      "Oula Puonti",
      "Peirong Liu",
      "Xiaoling Hu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a77671b88465fcc7a9611f96ed45df99721c1be7.pdf",
    "remote_url": "https://openreview.net/pdf/a77671b88465fcc7a9611f96ed45df99721c1be7.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "EndoAssistant: A Large-scale Vision-Language Dataset for Endoscopic Surgery Understanding from Open-Source Videos",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical image",
      "endoscopy",
      "vision-language model"
    ],
    "abstract": "Endoscopic interventions offer a minimally invasive approach, minimizing patient discomfort and facilitating expedited recovery. Proficient training of junior surgeons necessitates the ability to analyze and interpret endoscopic scenes through questioning and answering. Consequently, the development of a robust foundation model for endoscopic visual language understanding holds immense value for medical training and surgical education. However, existing endoscopy vision-language datasets are limited in scale and diversity, consisting of only 50 videos sourced from a few clinical sites, thus posing a significant hurdle to the advancement of generalized and robust artificial intelligence models for endoscopic surgical applications. To address this challenge, we present a large-scale, meticulously curated image-text dataset of surgical endoscopic scenes from expert surgeons, designed to propel a vision-language assistant in medical scene understanding. Encompassing 590 open-source videos spanning more than 91 hours, our curated dataset includes 65,844 unique images, 30,002 unique captions, and 157,589 image-caption/question-answering pairs. This dataset aims to assist the development of automated systems to support medical professionals by mitigating repetitive tasks. We present a comprehensive endoscopic surgery assisting pipeline, (1) a first-ever image-caption dataset specifically for endoscopic scenes; (2) an image-question-answer dataset that offers greater size and diversity compared to existing collections; (3) rigorous evaluation demonstrating its efficacy in downstream surgical endoscopic scene comprehension tasks like classification, retrieval and visual question answering.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents the first large-scale, meticulously curated image-text dataset of surgical endoscopic scenes and demonstrates its effectiveness in downstream surgical endoscopic scene comprehension tasks.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe curated dataset is a valuable resource for developing automated systems (e.g., LLMs, VLMs) to assist medical professionals in surgical endoscopic scenes.\n2.\tThe paper employs a well-designed data processing pipeline, including rigorous data cleaning and optimization procedures, to generate high-quality image-text pairs from a large collection of endoscopic surgical videos.\n3.\tThe CLIP model pretrained in EndoAssistant demonstrates superiority over mainstream vision-language pretraining frameworks through a broad range of empirical experiments.\n4.\tThe paper is very clear and easy to follow.\n\n### Weaknesses\n\n1.\tThe proposed Visual Question Answering (VQA) models should be evaluated on internal datasets, such as parts of EndoAssistant, to better assess the endoscopic knowledge learned by the models. Evaluating solely on external datasets can only provide a limited view of the model's capabilities.\n2.\tThe data, model, and training details should be openly released.\n\n### Questions\n\n1.\tHow are the Question-Answer Pairs constructed? Looking at the Question-Answer Pairs in Fig. 2, some pairs appear to be divergent questions unrelated to the images. Will such data (Question-Answer Pairs) improve performance on specific downstream tasks?\n2.\tThe proposed Visual Question Answering (VQA) models seem more akin to a VLM model than a traditional VQA model. Could you clarify this distinction?\n3.\tAll symbols used in the tables should be clearly defined, such as shading and underlining.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces EndoAssistant, a large-scale, expert-annotated vision-language dataset for endoscopic surgery, designed to enhance AI-driven medical training and surgical decision support systems.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The creation of a large-scale dataset with 65,844 unique images and over 157,589 image-caption pairs holds great potential to facilitate robust model training.\n\n2. Incorporating both image-caption and image-question-answer pairs in the dataset supports diverse applications, from simple classification to complex question-answering.\n\n3. The detailed data curation pipeline involving keyframe extraction, captioning, and alignment is methodologically sound for ensuring quality data preparation.\n\n4. The involvement of domain experts throughout the data curation process helps ensure high factual correctness and relevancy.\n\n### Weaknesses\n\n1. The sampling method ignores temporal dynamics in endoscopic videos, potentially limiting the model's ability to perform cross-frame reasoning and handle dynamic scenes effectively.\n\n2. The image-text sampling process does not fully capitalize on multimodal associations, possibly lowering the model's performance in complex surgical scene understanding that requires integrated visual-text analysis.\n\n3. The paper adopts a lower-performing method from Surgical-VQA (MICCAI 2023 paper) without demonstrating previous SOTA performance, casting doubts on the model's comparative effectiveness in the surgical domain.\n\n4. The paper lacks a discussion on the downstream task (surgery-specific tasks such as phase or tool recognition) performance of models trained on the proposed dataset, which are essential for assessing practical applicability in the surgical domain.\n\n5. The criteria for assessing the quality of text and images in the dataset are unclear, which may raise questions about the reliability of the dataset.\n\n### Questions\n\n1. Endoscopic surgery videos contain a large number of dynamic scenes, but the sampling process mainly processes static images or single-frame data, without fully considering the temporal information of the video. The sampling method that lacks temporal association will cause the model to be insufficient in dealing with cross-frame reasoning, thereby limiting the performance of the model in dynamic scenes. Discussion/experiments may be added for this problem.\n\n2. The sampling process segments and processes images and texts. Although contrastive learning is used to project images and texts into a shared embedding space, the sampling process does not fully utilize the multimodal associations between images and texts. In particular, the semantic associations between images and corresponding surgical step descriptions and tool instructions are not strengthened during sampling. This weakened multimodal association may limit the performance of the model in understanding complex surgical scenes, especially in tasks that require the integration of visual and textual information (such as complex scene question answering or context understanding). A detailed analysis could be made.\n\n3. Based on Surgical-VQA (MICCAI 2023, first release EndoVis-18-VQA & Cholec-VQA dataset), the accuracy of EndoVis-18-VQA & Cholec-VQA has reached 0.632 & 0.898, respectively. However, this submission selects the method with the lowest performance in Table 1 of Surgical-VQA. They have not proved their SOTA performance on benchmark VQA datasets in the surgical domain. BTW, after the first release, the specialized models have reached even higher performance.\n\n4. Surgical data science researchers may focus on some surgery-specific tasks, e.g., phase/tool recognition. The zero/few-shot performance of a VLM trained on the proposed dataset may be expected. Besides, fine-tuning LLaVA on different datasets but evaluating on the same benchmark surgical datasets can further demonstrate the effectiveness of the proposed dataset.\n\n5. What are the criteria for confirming the quality of text and images? Are there definite criteria for filtering low-quality images? What criteria do doctors use for text annotation?\n\n### Flag For Ethics Review\n\n- Yes, Legal compliance (e.g., GDPR, copyright, terms of use)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nThe dataset obtained from the website may have copyright problems.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors used a set of 590 endoscopic surgery videos to collect 157,589 image / caption pairs using a custom data curation pipeline. The image / caption pairs were further turned into open-ended and multiple choice question-answer pairs. \n\nThe utility of the two datasets (image / caption pairs and QA pairs) was validated by training a CLIP model and a LLaVa model respectively, and evaluating them on downstream tasks (zero-shot classification / retrieval and linear probe for CLIP and VQA for LLaVa), demonstrating comparable or superior performance than several other biomedical CLIP models and VQA models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe authors' proposed dataset appears 5 - 10 fold larger than previous endoscopic surgery video datasets in terms of metrics like hours of video content sourced from, question length and number of frames. \n\nThe value of the dataset is validated across a range of different scenarios including zero-shot eval, representation learning (linear probe / few-shot learning) and VQA.\n\n### Weaknesses\n\nThe quality of the dataset remains unclear to me - and would benefit from more clarification as well as investigation. While the dataset may be a valuable resource for computational researchers in the endoscopic surgical field, the paper otherwise does not appear to present novel ideas or evaluation. In fact, previous work published in CVPR 2024 have developed a much more sophisticated pipeline for curating instruction tuning data from Youtube videos in the field of pathology, involving more extensive quality control + mouse cursor location tracking: https://openaccess.thecvf.com/content/CVPR2024/papers/Seyfioglu_Quilt-LLaVA_Visual_Instruction_Tuning_by_Extracting_Localized_Narratives_from_Open-Source_CVPR_2024_paper.pdf. \n\nFor example, ASR is expected to be noisy and the transcript associated with a given key frame might have very limited context or be mismatched with the visual content displayed. Using GPT4 for retain only medically relevant information can help correct some incorrectly transcribed medical terms, but would not resolve the issues of limited context / mismatch with visual content. It is also not clear why out of 150k image / caption pairs, there are only 30,002 unique captions.\n\nThe created QA pairs similarly, might suffer from the same issues. I notice in the examples presented in Figure 2, are answers all very concise, and lack detailed explanation - which could arise due to both suboptimal prompting (e.g. only using zero-shot prompting instead of combining it with carefully, expert curated seed examples) and the concise nature of the source captions, and as a result limit their usefulness in training interactive AI assistants that can produce high quality responses in open-ended question answering (where a more detailed explanation or a specific response format is required). \n\nLastly, while the experiments are helpful for validating the usefulness of the dataset in the scenarios the authors investigated, crucial experimental details appear to be missing (i.e. hyperparemeters of training), which are needed to reproduce the results presented. Similarly, I cannot currently find a link to a github or hf repo that links to code and data used in the experiments in the study or the proposed dataset itself, and therefore can only draw conclusions about the quality of the data based on select examples / statistics presented in the paper.\n\n### Questions\n\n1. What is the average / std for the length of captions? \n2. How are the 120 image / caption pairs used in \"Cross-modal retrieval\" selected? Will this data be made available for future works to allow comparisons?\n3. Can the authors offer any additional insights about the quality of the dataset? (perhaps having experts review a random subset and rate accuracy / descriptiveness, etc.)\n4. What is the motivation for using both a CLIP model and a custom pretrained CNN classifier to classify endoscopic vs. irrelevant content?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces EndoAssistant, a large-scale vision-language dataset designed to enhance understanding of endoscopic surgery scenes. It addresses the limitations of existing datasets, which are small in scale and diversity, by providing a significantly larger collection of 590 videos, 65,844 unique images, 30,002 captions, and 157,589 image-caption/question-answer pairs. The dataset focuses on improving tasks like cross-modal retrieval, visual question answering (VQA), and image classification within the surgical context. The data curation process involves keyframe extraction, ASR transcription, hierarchical image classification, and rigorous text cleaning with clinical validation. EndoAssistant's vision-language data pipeline includes EndoCaption (image-caption pairs) and EndoQA (image-question-answer pairs), both of which are shown to improve baseline model performance across multiple benchmarks.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n(1)EndoAssistant is the first large-scale, open-source vision-language dataset explicitly tailored for endoscopic surgery, surpassing previous datasets like Cholec80 in scale and semantic diversity. By integrating multiple existing models (CLIP, Whisper, GPT-4) into a surgeon-in-the-loop framework, this dataset provides a novel approach to generating diverse, medically relevant Q&A data from endoscopic videos.\n\n(2)The paper clearly outlines each stage of the data pipeline, from video collection to model evaluation. The inclusion of figures detailing the dataset creation process and examples of the Q&A pairs and image captions adds clarity. Each stage is accompanied by performance metrics that demonstrate the impact of EndoAssistant on downstream tasks.\n\n### Weaknesses\n\n(1)Although EndoAssistant is curated for endoscopic tasks, some baseline models used (e.g., CLIP) are pre-trained on general vision-language datasets, which might limit their performance in highly specialized domains like medical imagery. Fine-tuning on similar medical datasets could make the evaluation more aligned with the dataset's intended use.\n\n[1] Hecvl: Hierarchical video-language pretraining for zero-shot surgical phase recognition\n[2] Procedure-Aware Surgical Video-language Pretraining with Hierarchical Knowledge Augmentation\n\n(2) How does EndoAssistant perform on surgical tasks beyond classification and VQA, such as surgical phase recognition or anomaly detection?\n\n(3) While the dataset draws from multiple open sources, there is a limited analysis of potential biases within the data. Different hospitals, surgical types, anatomical regions, or patient demographics could introduce significant variability, impacting the generalizability of the model.\n\n(4) The dataset relies on relatively straightforward image-text pairing and may not fully capture deeper semantic alignment between the visual and language modalities (e.g., multi-level semantic alignment or co-occurrence patterns). Surgical procedures often involve subtle contextual changes, and certain tools or anatomical structures may carry different meanings across procedural stages.\n\n### Questions\n\nIt is recommended to include in the limitations a discussion on the challenging conditions often faced in endoscopic surgery, such as inconsistent lighting, obstructed views, interference from bodily fluids, as well as data biases arising from differences in hospitals, types of surgery, anatomical regions, or patient demographics.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Balu Harshavardan Koduru",
      "David Doermann",
      "Junsong Yuan",
      "Nan Xi",
      "Shun Liu",
      "Tenzin Lhakpa",
      "Tianyu Luan",
      "Xi Tang",
      "Xuan Gong",
      "Yuan Zhang",
      "Yuanhao Zhai",
      "Yunjie Tian",
      "Yuxuan Sun",
      "Ziqing Xue"
    ],
    "url": "pdfs/iclr.cc-2025-conference_b187719ea9a245e64eadc2a235fa89f67d271bd3.pdf",
    "remote_url": "https://openreview.net/pdf/b187719ea9a245e64eadc2a235fa89f67d271bd3.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Revolutionizing EMCCD Denoising through a Novel Physics-Based Learning Framework for Noise Modeling",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "EMCCD",
      "physics-based noise modeling",
      "deep high-sensitivity imaging",
      "fluorescence microscopy image denoising"
    ],
    "abstract": "Electron-multiplying charge-coupled device (EMCCD) has been instrumental in sensitive observations under low-light situations including astronomy, material science, and biology. \nDespite its ingenious designs to enhance target signals overcoming read-out circuit noises, produced images are not completely noise free, which could still cast a cloud on desired experiment outcomes, especially in fluorescence microscopy.\nExisting studies on EMCCD's noise model have been focusing on statistical characteristics in theory, yet unable to incorporate latest advancements in the field of computational photography, where physics-based noise models are utilized to guide deep learning processes, creating adaptive denoising algorithms for ordinary image sensors.\nStill, those models are not directly applicable to EMCCD.\nIn this paper, we intend to pioneer EMCCD denoising by introducing a systematic study on physics-based noise model calibration procedures for an EMCCD camera, accurately estimating statistical features of observable noise components in experiments, which are then utilized to generate substantial amount of authentic training samples for one of the most recent neural networks.\nA first real-world test image dataset for EMCCD is captured, containing both images of ordinary daily scenes and those of microscopic contents.\nBenchmarking upon the testset and authentic microscopic images, we demonstrate distinct advantages of our model against previous methods for EMCCD and physics-based noise modeling, forging a promising new path for EMCCD denoising.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a physics-based noise model for EMCCD cameras. The statistical model includes some typical noise components for EMCCD sensors, and a calibration method is proposed for adaptation this noise model on each sensor. Through careful noise modeling and calibration, the authors synthesize realistic EMCCD noise data for training, and effectively improve the learning of deep denoisiers in both macroscopic testset and microscopic testset.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The paper introduces the first EMCCD denoising method utilizing physics-based noise modeling method.\n- The overall writing of this paper is good and easy to follow.\n\n### Weaknesses\n\n- This paper proposes the first noise modeling method for EMCCD sensors, and there are indeed some new adaptations on this sensor type. However, the main idea borrows many contributions from the similar task of CMOS noise modeling, and seems to be a EMCCD-version of ELD [1]. Specifically, the entire pipeline, i.e., physics-based noise modeling ->  calibration -> synthesis -> denoise pipeline is the same with ELD. The noise components and calibration process are also similar with ELD. In addition, the modeling of FPN and pre-processing operation comes from PMN [2] .\n- For Fig. 7, why ELD presents banding patterns, even after calibration using the target device? ELD calibrates row noise using bias frames, and the variance for row noise would be close to zero on sensors without obvious banding patterns if correctly calibrated. I wonder why ELD still causes such row patterns on EMCCD sensors.\n- There should be more comparisons with sota methods, for both noise modeling and self-supervised denoising methods. For example, [3] proposes a general noise modeling method which uses poisson sampling for signal-dependent noise and GAN for signal-independent noise. I think [3] can also handle EMCCD sensors. Stronger baselines for self-supervised methods are also recommended to compare [4].\n- I concern that it is not rigorous to use SID clean images to synthesize noisy pairs for training. Different from EMCCD sensors, SID dataset uses Sony cameras with CMOS sensors. Each sensor type has its own unique recipe for generating RAW data; even using clean images from one type of CMOS sensor to generate synthetic noisy pairs and then testing on real data from a different CMOS sensor can lead to negative effects, not to mention EMCCD data. Therefore, I believe that SID clean data is not a suitable choice for this application.\n- Section 2.3 is not necessary since no deep denoiser architecture is proposed.\n\n\n[1] Physics-based Noise Modeling for Extreme Low-light Photography. TPAMI 2021\n[2] Learnability Enhancement for Low-Light Raw Image Denoising- A Data Perspective. TPAMI 2023\n[3] Towards General Low-Light Raw Noise Synthesis and Modeling. ICCV 2023\n[4] Exploring Efficient Asymmetric Blind-Spots for Self-Supervised Denoising in Real-World Scenarios. CVPR 2024\n\n### Questions\n\nWhy ELD presents banding patterns in Fig. 7?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper focuses on EMCCD noisy data and denoising. The authors propose a physics-based noise model specifically for EMCCD cameras, which generates synthetic noisy images based on both the camera's properties and EMCCD-specific noise characteristics. This method makes the data clorser to real-life scenarios. They then train a deep learning model, Uformer, on these noisy image pairs for denoising. The Uformer model achieves better denoising results comparing to other methods.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. Proposed the first dataset specific to EMCCD.\n2. Provides a detailed and clear explanation of the noise model, including settings and parameter estimation.\n3. The experiments compare the proposed method to other state-of-the-art methods.\n\n### Weaknesses\n\n1. For important equations, such as Eq. (1) and (5), the dimensions of each parameter are not provided, especially for N_p, f, and I.\n2. The denoising model, Uformer, should be discussed more thoroughly, with additional details explaining its design, such as the key differences compared to the Uformer model from Wang et al., 2024.\n3. The total number of image pairs is 224, which is relatively small, and the use of only 24 images for fine-tuning could lead to overfitting.\n\n### Questions\n\n1. Could you provide the dimensional details of the variables in the key equations listed in the paper? It would help in understanding if you state that 'X' represents the inner product in Eq. (1).\n2. It seems that adding N_r and N_q makes the image blurry. Could you visualize both N_r and N_q?\n3. I feel that the proposed noise addition might be similar to the negative binomial low-photon noise. Could you explain the key differences between them?\n4. In line 076, could you elaborate on the differences between the EMCCD and other models, if possible?\n5. Could you provide a big-map plot or additional explanation of your Uformer model? What is the novel design aspect of this denoising model, and how does it differ from Wang's model?\n6. Did you use any method to measure whether the results indicate overfitting? Will using data augmentation techniques to generate more data improve the model's accuracy? Perhaps training the model on simulated data and testing it on the original true data could be a way to assess the quality of the simulated data.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper presents a novel approach to denoising images captured by electron-multiplying charge-coupled devices (EMCCDs) by introducing a physics-based noise model and a calibration procedure tailored for EMCCD-specific noise characteristics. The proposed method synthesizes authentic training data for a deep learning framework, enhancing denoising performance in fluorescence microscopy and achieving state-of-the-art results compared to existing methods. Additionally, they establish a comprehensive pipeline that connects noise parameter calibration with advanced neural network training strategies. This work paves the way for improved image quality in sensitive imaging applications across various scientific fields.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n1. The introduction and the method of this paper are clear and easy to understand. Even readers who may not be familiar with EMCCD can grasp the motivation behind the noise model.\n\n2. The novelty of this work is commendable. While many key designs are inspired by existing research, they incorporate unique adjustments specific to the characteristics of EMCCD sensors. The analysis of FPN, blooming effects, and readout noise heatmaps is particularly impressive.\n\n3. The experiments presented in this paper are excellent, and I believe they will significantly contribute to sensitive imaging applications across various scientific fields.\n\n### Weaknesses\n\n1. In Eq. (5), D' includes $N_r$ and $N_q$; however, this seems unreasonable from a formulaic perspective. I suggest explaining why $B^{-1}$ doesn't affect $N_r$ and $N_q$. For instance, it might be beneficial to analyze the expected interactions between these two components.\n\n2. Figure 3(b) appears to exhibit some abrupt transition points (e.g., log(time) = -7, -4), and the explanation provided in L252-255 seems insufficient to cover this phenomenon. Please confirm the reproducibility of these data and clarify why an S-shaped curve is used instead of multiple piecewise functions. If these transition points are related to circuit switching, a piecewise function fitting, similar to what has been reported in PMN, should be employed.\n\n### Questions\n\n### Original Question\nThe relationship between the ablation study and the proposed method in this paper is unclear. \n\nAs it stands, I find it difficult to directly correlate the FPNt, blooming effect, and readout noise heatmap with the ablation learning presented. Additionally, the current preprocessing appears to resemble contributions from PMN rather than from this work. \n\nI suggest clarifying the incremental contributions of the proposed method in the experiments to emphasize the original contributions of this paper.\n\n### After Rebuttal\nThe authors have addressed my concerns.\n\nI found the authors’ response to reviewer e1uJ very well-written. This work stands out because it takes a practical, problem-specific approach, using appropriate innovations to solve a real-world task. Looking at recent noise modeling research, I consider LLD [1] and PNNP [2] to be practical, while LRD [3] seems less so. LRD faces challenges with the data dependency problem, whether **paired data or noise models come first**, and the instability of the GAN-based training strategy. As a result, GAN-based noise modeling methods like LRD, CA-GAN, and Starlight often overfit the training data.  \nFor example, LRD, which includes a **Fournier Transformer Discriminator**, leaves visible row patterns in *Scene-07_IMG-0010* of the ELD dataset that are more noticeable than those in the ELD baseline, even though LRD achieves higher PSNR and SSIM scores. For this reason, I believe reviewer e1uJ’s initial rejection of this work was not well-founded.\n\nIn conclusion, I acknowledge the contributions of this paper and am inclined to keep my current rating.\n\n**Reference**  \n[1] Y. Cao, M. Liu, S. Liu, X. Wang, L. Lei, and W. Zuo, ‘Physics-Guided ISO-Dependent Sensor Noise Modeling for Extreme Low-Light Photography’, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 5744–5753.  \n[2] H. Feng, L. Wang, Y. Huang, Y. Wang, L. Zhu, and H. Huang, ‘Physics-guided Noise Neural Proxy for Practical Low-light Raw Image Denoising’, arXiv preprint arXiv:2310. 09126, 2023.  \n[3] F. Zhang et al., ‘Towards General Low-Light Raw Noise Synthesis and Modeling’, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 10820–10830.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Haiyang Jiang",
      "Imari Sato",
      "Takeharu Nagai",
      "Tetsuichi Wazawa",
      "Yinqiang Zheng"
    ],
    "url": "pdfs/iclr.cc-2025-conference_b0ed39c76ba54d2f078d1263ff3edaac3b15cfdb.pdf",
    "remote_url": "https://openreview.net/pdf/b0ed39c76ba54d2f078d1263ff3edaac3b15cfdb.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning 3D Medical Image Models From Brain Functional Connectivity Network Supervision For Mental Disorder Diagnosis",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to neuroscience & cognitive science"
    ],
    "keywords": [
      "3D medical image",
      "functional connectivity network",
      "contrastive learning",
      "mental disease diagnosis"
    ],
    "abstract": "For mental disorder diagnosis, most previous works are task-specific and focus primarily on functional connectivity network (FCN) derived from functional MRI (fMRI) data. However, the high cost of fMRI acquisition limits its practicality in real-world clinical settings. Meanwhile, the more easily obtainable 3D T1-weighted (T1w) MRI, which captures brain anatomy, is ofen overlooked in standard diagnostic processes of mental disorders.\nTo address these two issues, we propose CINP (Contrastive Image-Network Pre-training), a framework that employs contrastive learning between 3D T1w MRI and FCNs. CINP aims to learn a joint latent semantic space that integrates complementary information from both functional and structural perspective. During pre-training, we incorporate masked image modeling loss and network-image matching loss to enhance visual representation learning and modality alignment.\nFurthermore, thanks to contrastive pre-training which facilitates knowledge transfer from FCN to T1w MRI, we introduce network prompting. This protocol leverages 3D T1w MRI from suspected patients and FCNs from confirmed patients for differential diagnosis of mental disorders.  \nExtensive experiments across three mental disorder diagnosis tasks demonstrate the competitive performance of CINP, using both linear probing and network prompting, compared with FCN-based methods and self-supervised pre-training methods.\nThese results highlight the potential of CINP to enhance diagnostic processes with the aid of 3D T1w MRI in real-world clinical scenario.",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors have proposed an interesting contrastive pretraining framework that utilizes 3D T1 MRI and functional connectivity networks (FCNs) derived from fMRI data to learn robust representations for mental disorder diagnosis. The model has been evaluated in both linear probing and retrieval settings, showcasing intriguing ideas. However, the organization of the paper and the presentation of results could be enhanced.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The authors effectively describe the motivation behind this study and highlight the potential contributions of integrating contrastive pretraining with MRI and FCN data. The approach of cross-modal contrastive learning is well-founded and offers a solid framework for using information from both modalities to enhance diagnostic accuracy while also enabling retrieval/diagnosis when one of the modalities is missing. \n2. The three objective functions are well-described, and the authors provide detailed ablation studies, which contribute to a better understanding of their impact on model performance. \n3. The clear delineation between pretraining and evaluation sets facilitates relatively fair comparisons across out-of-domain datasets.\n\n### Weaknesses\n\n1. The writing could be improved to enhance the presentation of results. For instance, the experimental setup for network prompting is somewhat challenging to follow, as detailed in my questions below. \n2. The authors hypothesize that the “CINP” model has potential for improvement through fine-tuning; it would be better to directly include corresponding results in the tables for a more comprehensive understanding. \n3. The comparisons are primarily between CINP and single-modality models (sMRI or FCN). There is a lack of discussion and direct comparisons with existing multi-modal methods for mental health diagnosis, both in linear probing and fine-tuning contexts. At least, some consensus on FCN and SSP-based model predictions would allow for fairer comparisons. \n4. Although several metrics are presented, the authors did not discuss in detail the differences, especially when two metrics offer contrasting results. \n5. The authors did show the advantages of pretraining over simply fine-tuning a model directly on the evaluation dataset that utilizes both modalities as input. This is an important aspect to demonstrate the value of pretraining. Further improvements could include testing in a low-data regime to see if pretraining can reduce data requirements for subsequent fine-tuning. \n6. While improvements are shown, the absolute values of metrics appear low for potential clinical applications. Providing context on results from the literature for the same task or similar datasets would help readers unfamiliar with this specific field better interpret the model's performance.\n\n### Questions\n\n1. Was the linear probing of CINP performed on concatenated MRI and FCN embeddings, or was it based on one of the modalities? \n2. Regarding network prompting, what is meant by partitioning all samples into 5/10 subsets? What is the purpose of this partitioning? Additionally, could the authors clarify why they chose to use 10% or 50% of the data? Is this to assess retrieval performance in a low-data regime? It would also be helpful to explain why the results are better when using 10% of the training data compared to using 100%.Also, the number 29.33% seems inconsistent with the figures in Table 3.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors propose a contrastive learning approach to learn from both fMRI and anatomical T1w MRI with the aim to differentiate various psychiatric disorders.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- The approach appears original in the way it allows the diagnosis of psychiatric disorders based on the patient's T1w MRI only. At the inference stage, two components are required: i) the T1w MR image of the patient to be diagnose and ii) fMRI data of patients with known diagnosis arranged in different diagnostic classes. The predicted diagnosis corresponds to one for which the similarity between the T1 and fMRI embedding was the highest.\n- This particular approach seems novel.\n\n### Weaknesses\n\n- The paper is not always easy to follow, in particular the Quantitative results section. Careful proofreading would help.\n- It is not entirely clear how the ABIDE, ADHD and SRPBS data sets were split and used during the evaluation: i) were the splits stratified, and if so on what criteria, ii) were the results of the ablation study displayed in Table 4 obtained on the test set (and so was hyper-parameter selection based on the test set)?\n- The diagnostic classes are not balanced and no metric adapted to this scenario is used to assess the performance.\n- The references of the first paragraph of the introduction mostly do not seem appropriate:\n    - 'Over recent years, there has been growing evidence that mental disorders arise from dysfunction of interconnected patterns of regions-of-interest (ROIs) in the whole brain (Krishna et al., 2023) […].' This paper is about glioblastoma, it has nothing to do with fMRI nor mental disorders.\n    - '[…] fMRI-derived functional connectivity network (FCN) […] has received considerable attention in diagnosis of mental disorders (Yang et al., 2021; Bastos & Schoffelen, 2016) […].' The first paper is about diffusion MRI and the second one describes functional connectivity analysis in general and is not focused at all on mental disorders.\n- The organisation of the paper does not seem optimal. I do not see the point of having a Related Work section at the end of the paper knowing that the methods are only briefly describes and no conclusion is being drawn.\n- Several works with similar aims should be cited and discussed, e.g.\n    - He, Zhibin, et al. \"F2TNet: FMRI to T1w MRI Knowledge Transfer Network for Brain Multi-phenotype Prediction.\" International Conference on Medical Image Computing and Computer-Assisted Intervention. Cham: Springer Nature Switzerland, 2024.\n    - Fedorov, Alex, et al. \"Self-supervised multimodal learning for group inferences from MRI data: Discovering disorder-relevant brain regions and multimodal links.\" NeuroImage 285 (2024): 120485.\n    - Li, Tongtong, et al. \"Automated diagnosis of major depressive disorder with multi-modal MRIs based on contrastive learning: a few-shot study.\" IEEE Transactions on Neural Systems and Rehabilitation Engineering (2024).\n\n### Questions\n\n- Please see weaknesses above.\n- How were the confidence intervals computed?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper studies the problem of mental disorder diagnosis using multimodal MRI data. Specifically, it proposes a framework called CINP (Contrastive Image-Network Pre-training), which applies contrastive learning between 3D T1-weighted (T1w) MRI and functional connectivity networks (FCNs) derived from fMRI. CINP aims to create a joint latent space integrating functional and structural information, enhancing diagnostic capabilities. During pre-training, the framework incorporates masked image modeling and network-image matching losses to improve modality alignment and representation quality. Moreover, the CINP contains a network prompting, enables the use of 3D T1w MRI from suspected patients and FCNs from confirmed cases to differentiate mental disorders. Extensive results on public datasets shows CINP has good performance.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper addresses a significant problem in mental disorder diagnosis by leveraging both structural and functional MRI data, a topic relevant and valuable to the ICLR communities.\n2. The figures and descriptions in the paper are well-organized and clear, which aids in understanding the proposed approach and the CINP framework.\n3. The writing in the paper is clear, making the methodology and results easy to follow.\n\n### Weaknesses\n\n1. The paper's technical contributions appear limited, as it mainly combines existing methods like contrastive learning and masked autoencoders (MAE) without significant innovation in methodology. The CINP framework may be seen as a straightforward integration of known techniques rather than a novel approach.\n2. There is still room for performance improvement. The performance of CINP on the ABIDE dataset is noticeably lower than that of the baselines, as indicated in Table 2. This suggests that the framework may not be fully optimized or may have limitations, and further improvements are needed to make it competitive across all datasets.\n3. The assumption that sMRI and fMRI features can be effectively aligned using contrastive learning lacks theoretical or empirical support from a neuroimaging or neuroscience perspective. This forced alignment may overlook important modality-specific differences, making the approach less effective for capturing unique structural-functional relationships in brain data. (See my questions below.)\n\n### Questions\n\nMy main concern is with the rationale behind using contrastive loss in CINP to model sMRI and fMRI representations. Based on my knowledge, there is a clear distinction between neuroimaging and image-text domains. In the image-text domain, it makes sense to maximize the differences between different categories while minimizing the differences within the same category. For instance, aligning a cat’s textual description with its image, while creating a contrast with dog-related text and images, is logical. However, in neuroimaging studies, why should we align sMRI and fMRI feature representations in this way? And why should we increase inter-subject differences?\n\nI also note that a healthy cohort was used for CINP’s pre-training. The differences within a healthy population do not fall into distinct categorical boundaries, so it is unclear what meaningful patterns are learned by maximizing inter-subject representation differences. Brain imaging features of healthy individuals are generally quite similar in terms of overall structure and function. By maximizing these differences within healthy individuals, the model may pick up biologically insignificant or irrelevant details, diverting its focus from core features. Such differences are more likely to be noise than meaningful information. In contrastive learning, distinct category boundaries are typically required to generate positive and negative samples, but healthy individuals do not naturally fall into clear categories, making the construction of inter-subject contrasts somewhat artificial. \n\nAdditionally, enforcing a “strict alignment” between sMRI and fMRI features may lack generalizability. For mental disorders, patients do not always exhibit abnormalities in both sMRI and fMRI simultaneously. For example, a patient might have a normal gray matter thickness in the prefrontal cortex as shown by sMRI, yet fMRI may reveal weakened functional connectivity between the prefrontal cortex and other regions (such as the parietal lobe or hippocampus). In such cases, a “strict alignment“ seems less appropriate. Similarly, even among healthy individuals, while there may be some correlation between gray matter thickness and functional connectivity, it is not necessarily consistent. Forcing alignment within a healthy cohort may lead the model to mistakenly assume a strong correlation between structural and functional features, thereby overlooking the natural variability and dynamic nature of functional connectivity. \n\nI look forward to seeing the authors' response and would consider raising my score if they can adequately address my concerns.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposed a contrastive image network pre-training method for mental disorder diagnosis. By using contrastive learning between the structure MRI and functional MRI, the proposed can use the useful information from both sMRI and fMRI for mental disorder diagnosis. The proposed method has been compared with several competing methods. The results show that the proposed method achieves better performance than the  competing methods.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1) A contrastive image-network pre-training method is proposed to use the multi-modal data for mental disorder diagnosis.\n2) The performance shows that the proposed method achieves better performance.\n\n### Weaknesses\n\n1) The significant advantage of brain networks lies in their interpretability, yet the paper lacks an interpretability analysis related to diseases.\n2）Many multimodal methods based on functional and structural MRI have been proposed, but this paper does not compare with these methods.\n3) The details of the methods are not clear enough.\n\n### Questions\n\n1) Present findings related to diseases and provide analysis.\n2）Compared with the multi-model fusion methods.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Li Xiao",
      "Wei Wang",
      "Xingcan Hu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_983ae776d2e1fe0addea1af14f86675d2246fc77.pdf",
    "remote_url": "https://openreview.net/pdf/983ae776d2e1fe0addea1af14f86675d2246fc77.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Score-based Self-supervised MRI Denoising",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Generalized Denoising Score Matching; Self-supervised Learning; Self-supervised Denoising; Score-based denoising; Medical Image Denoising;"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is a powerful noninvasive diagnostic imaging tool that provides unparalleled soft tissue contrast and anatomical detail. Noise contamination, especially in accelerated and/or low-field acquisitions, can significantly degrade image quality and diagnostic accuracy. Supervised learning based denoising approaches have achieved impressive performance but require high signal-to-noise ratio (SNR) labels, which are often unavailable. Self-supervised learning holds promise to address the label scarcity issue, but existing self-supervised denoising methods tend to oversmooth fine spatial features and often yield inferior performance than supervised methods. We introduce Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising. At the core of C2S is a generalized denoising score matching (GDSM) loss, which extends denoising score matching to work directly with noisy observations by modeling the conditional expectation of higher-SNR images given further corrupted observations. This allows the model to effectively learn denoising across multiple noise levels directly from noisy data. Additionally, we incorporate a reparameterization of noise levels to stabilize training and enhance convergence, and introduce a detail refinement extension to balance noise reduction with the preservation of fine spatial features. Moreover, C2S can be extended to multi-contrast denoising by leveraging complementary information across different MRI contrasts. We demonstrate that our method achieves state-of-the-art performance among self-supervised methods and competitive results compared to supervised counterparts across varying noise conditions and MRI contrasts on the M4Raw and fastMRI dataset. The project website is available at: https://jiachentu.github.io/Corruption2Self-Self-Supervised-Denoising/.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents Corruption2Self (C2S), a score-based self-supervised denoising framework specifically designed for MRI data. C2S uses a reparameterized noise schedule and applies Generalized Ambient Denoising Score Matching (GADSM) to extend traditional score matching approaches to scenarios where only noisy data are available. Key contributions include the introduction of a reparameterized noise level function to stabilize training, as well as a multi-contrast extension to leverage complementary information across MRI contrasts. Experimental evaluations on M4Raw and fastMRI datasets show competitive results for C2S, often surpassing both classical and self-supervised methods in denoising performance metrics.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n* Reparameterization of Noise Levels: The proposed reparameterization of noise levels is a noteworthy contribution, offering enhanced training stability and convergence. This allows the model to sample uniformly across the noise range, leading to smoother training curves and better generalization.\n* Comparison with Self-Supervised and Supervised Methods: The paper includes extensive quantitative comparisons with self-supervised and supervised denoising models, establishing C2S as a strong self-supervised alternative in terms of PSNR and SSIM.\n* Multi-Contrast Extension: Incorporating multi-contrast data is a beneficial approach that leverages complementary MRI contrasts, enhancing structural preservation and improving the quality of denoised images.\n\n### Weaknesses\n\n* Limited Novelty Beyond Classical Denoising Diffusion Probabilistic Models (DDPM): The use of a score-based approach is similar to DDPM without substantial differentiation. Although the reparameterization is innovative, the rest of the framework closely resembles classical score-based diffusion models, raising concerns about the originality of the overall approach.\n* Noise Level Estimation Error Not Clearly Specified: While Figure 5 attempts to show robustness to noise level estimation error, the specific impacts and handling of these errors in practical settings remain unclear. Further clarity on this aspect would strengthen the model’s practical applicability.\n(Fluctuations in Training Stability: Although the reparameterization claims to stabilize training, Figure 2 indicates some fluctuations, suggesting that stabilization might not be consistent across all noise levels. This could impact reproducibility and model robustness, especially under varying noise conditions.\n* Parameter Specification in Equations: The notation for key parameters, such as $\\lambda_{out}$ ​and $\\lambda_{skp}$​ , lacks clear definitions in the methods section, which could hinder understanding and replication of the proposed framework.\n* Effects of Corruption Level (T) in Training: The paper does not provide guidance on the relationship between higher corruption levels (T) and the required number of training iterations to achieve optimal performance. This omission could limit the applicability of the method in datasets with different noise characteristics or levels of corruption.\n* Counterintuitive Results in Multi-Contrast Experiments: In the multi-contrast experiments, incorporating T1 contrast data seems to worsen the results. This is surprising since T1 typically offers structure-rich information that could enhance denoising. An analysis of this phenomenon would provide deeper insights into the limitations of C2S in multi-contrast applications.\n\n### Questions\n\n1. Given that this method is closely aligned with score-based DDPM, what aspects of C2S differentiate it from classical diffusion-based approaches, aside from the reparameterized noise level function?\n1. How does the noise level estimation error affect denoising quality in practical scenarios? Is there a threshold or method for mitigating significant deviations in noise estimation?\n1. How would the training duration change if a higher maximum corruption level $T$ were used? Does this require additional training epochs to reach convergence, as hinted at in the ablation study?\n1. Could the authors clarify why adding T1 contrast data in the multi-contrast experiments led to reduced performance? This is counterintuitive, as T1 contrast generally provides valuable anatomical information.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposed Corruption2Self, a self-supervised MRI denoising method based on ambient diffusion (training diffusion models using corrupted data). The authors proposed an algorithm named Reparametrized Generalized Ambient Denoising Score Matching and showed its superior performance compared to a number of baselines, supervised and self-supervised methods, on M4Raw (containing pairs of real noise and ground truth) and FastMRI datasets (synthetic noisy images).\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The proposed algorithm GADSM has sound math groundings. \n- Authors did extensive experiments to compare the proposed algorithm to a number of baselines and showed its superior performance.\n- Authors discussed the application of the algorithm to multi-contrast MRI, which is an overlooked field in MRI denoising.\n\n### Weaknesses\n\n- This paper's originality seems to be limited. The proposed method Reparametrized GADSM is a straightforward extension to ADSM [1]. In addition, authors failed to point out the challenges when applying self-supervised denoising methods in natural images to MRI images. It seems to be that except for the multi-contrast part, the others are natural extensions of techniques that have already been tested on natural images.\n- The paper's problem setup is very similar to Noiser2noise [2], both handling Gaussian noise with known sigma. I think it is an important baseline to be tested. \n- The self-supervised method used as baselines in this paper are too outdated. There are a number of newer methods under the category of blind-spot network (J-invariance) such as LG-BPN [3] and PUCA [4], which has more powerful architectures to increase the accuracy in predicting the value in blind spots, therefore better PSNR/SSIM numbers. Even though these methods were proposed to handle noise with spatial correlation (i.e. not pixelwise independent), the architectures can be easily optimized for independent noise (by making the blind spot be just 1 pixel). \n- In Fig. 4, C2S seems to have more blurry results than R2R. Some details seem to be harder to see.\n- The dataset shown in Fig. 4 seems to be unfair for supervised method, since the label is very noisy. Authors may want to re-consider the statement that \"the potential of self-supervised learning to match or even surpass supervised methods in MRI denoising\" (Introduction).\n- The multi-contrast experiments are not fair to other baselines such as Noise2noise and R2R, since the other contrasts can be easily included as an extra channel in the model input to boost performance. \n- Is hallucination problem of generative diffusion models a concern here? How can it be addressed?\n\n[1] Daras G, Dimakis AG, Daskalakis C. Consistent Diffusion Meets Tweedie: Training Exact Ambient Diffusion Models with Noisy Data. arXiv preprint arXiv:2404.10177. 2024 Mar 20.\n[2] Moran N, Schmidt D, Zhong Y, Coady P. Noisier2noise: Learning to denoise from unpaired noisy data. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2020 (pp. 12064-12072).\n[3] Wang Z, Fu Y, Liu J, Zhang Y. Lg-bpn: Local and global blind-patch network for self-supervised real-world denoising. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 18156-18165).\n[4] Jang H, Park J, Jung D, Lew J, Bae H, Yoon S. PUCA: patch-unshuffle and channel attention for enhanced self-supervised image denoising. Advances in Neural Information Processing Systems. 2024 Feb 13;36.\n\n### Questions\n\n- Authors may want to point out clearly how MRI denoising is different from natural image denoising. What are the challenges? Why does it worths special attention? \n- In the multi-contrast experiment, how extra contrasts were used as inputs? They were directly modeled by diffusion models (i.e. a channel of X_0 ... X_T) or as a conditional channel (i.e. diffusion models learn p(target contrast|other contrasts))?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes Corruption2Self, a score-based self-supervised framework for MRI denoising. \n\nMotivation:\n\nThe goal is to learn denoising directly from noisy data (without relying on high-quality labels). Their framework aims to fix the label scarcity and over-smoothing of finer details issues in existing supervised and self-supervised methods respectively. \n\nContributions:\n\nThe framework comprises a generalized ambient denoising score matching (GADSM) loss followed by reparametrization to improve convergence and detail refinement extension to preserve finer spatial features. Further, the authors extend the framework to incorporate additional MRI contrasts to improve performance. The authors finally claim that their framework achieves state-of-the-art performance among self-supervised methods and comparable performance among supervised methods.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The paper comprehensively analyzes and applies existing self-supervised and supervised denoising approaches in the context of MRI\n\n2. The paper is well written, original and provides great detail into the workflow of the Corruption2Self (C2F) framework.\n\n3. Incorporation to reparametrization (Table 4) and extensions to multi-contrast settings to improve denoising.\n\n4. Showcases robustness of methodology on varied noise level estimations compared to true noise (Table 9)\n\n### Weaknesses\n\n### A. Detail refinement extension claim\nAccording to the metrics in Table 1, it is unclear if the detail refinement extension is being effective. The improvements in PSNR / SSIM does not seem notable. It would be helpful to include an error bar (for the table), statistical significance test (to show notability) and visuals to show effectiveness.\n\n### B. Applicability and impact\nThe paper can cover how their workflow can be used in practice while denoising. The following aspects can add more value-add in terms of real-world impact to the paper.\n1. Estimation of $σ_{tdata}$ in real-time during inference.\n2. Given that traditional methods such as BM3D work decently well (maybe 1-3 points less performant than C2S), how difficult is it to go about training / deploying C2S in the MRI denoising workflow than using non-learning based methods?\n3. Would this methodology potentially result in MRI image acquisition?\n\n### Questions\n\n1. Robustness of C2S (M4Raw):\nIn the context of matching test-train SNR on M4Raw dataset, the authors claim that \n    1. (Line 327-328) supervised methods such as SwinIR and Restormer perform better when the noise characteristics of train and test data are similar\n    2. (Line 328-330) C2S achieves better generalization.\nAccording to \n    1. Table 3 (Results where test data SNR > train data SNR): C2S perform similar to SwinIR for eg.\n    2. Table 7 (Results where test data SNR ~ train data SNR): C2S perform similar to SwinIR (although both have higher metrics here).\n\n    It seems that both methods perform similar to each other in both the conditions?\n    Are the metrics higher in latter because the labels in Table 7 are noisier than in Table 3? I'm not sure if we can conclude that C2S is more generalized and supervised is not from the above data?\n\n2. I'd be interested to know the intuition behind why reparametrization works. Mathematically, it seems very similar except that we sample ${\\tau}$ ~ $U(0, T]$ rather than ${\\tau}$ ~ $U(0, T']$ due to $T >> T'$ . Does this approximation help in convergence?\n\n3. Does reparametrization and detail refinement extension help in the case of FastMRI dataset also? Curious as effectiveness for only M4Raw dataset have been reported.\n\n4. How would one estimate `T` (max corruption level) before training the model?\n\n5. Given that complementary pair of contrasts improve denoising, curious to know if all the 3 contrasts can be used (eg. T1, T2 and FLAIR) for instance to further improve denoising?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Fan Lam",
      "Jiachen Tu",
      "Yaokun Shi"
    ],
    "url": "pdfs/iclr.cc-2025-conference_0e3177d1e260e7b92cc5e0071bea1ffaacfd145a.pdf",
    "remote_url": "https://openreview.net/pdf/0e3177d1e260e7b92cc5e0071bea1ffaacfd145a.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Image-text pairs generation",
      "Vision-language models",
      "Multi-agent collaboration"
    ],
    "abstract": "Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model (LMM) to generate captions for extracted images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access at https://github.com/PathFoundation/PathGen-1.6M.",
    "reviews": [
      {
        "text": "### Summary\n\nPathGen-1.6M represents a significant advancement in pathology AI, introducing the largest high-quality pathology image-text dataset created through multi-agent collaboration. The approach leverages whole slide images from TCGA to extract representative patches and generate accurate, detailed captions, achieving 88-90% accuracy validated by pathologists. The resulting models, PathGen-CLIP and PathGen-LLaVA, demonstrate superior performance across various tasks including zero-shot classification, few-shot learning, and whole slide image analysis, outperforming existing models including GPT-4V. This work provides a scalable pathway for generating high-quality pathology data and developing more capable AI models for clinical applications.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- Well written introduction\n- Good literature survey on LMMs and Pathology--CLIPs\n- Brilliant idea on how to develop the revise LLM agent\n- Representative patch selection using GPT-4 is an excellent idea\n- Interesting approach to select patches through clustering but modulating the number of clusters as the square root of the size of a slide\n- Strong evaluation pipeline across a range of tasks\n\n### Weaknesses\n\n- It's an overkill to say that this is an agent based system. Agent based systems are autonomously operating on a set of predefined rules and behaviors, while this approach appears to be more of a sequential pipeline with different models performing specific tasks rather than truly autonomous agents interacting with each other.\n- Lit survey on multi-agent architectures could be expanded\n- Usage of GPT-4s internal knowledge about the morphology of an organ is a good idea but deters diversity of patches collected. Also unclear if those prompts are well represented in the CLIP training dataset. Additional details on how many prompts obtained for each WSI will help the reader. It's unclear if each patch is matched against 2 prompts (report and attribute based) or more?\n- Added details on motivations for design choices such as including both prompt and image retrieval will help.\n- The methods section needs more details and fleshing out the writing will help; I think this is also generally true for most of the paper\n- While not the goal of the paper, it will help to include fully supervised baselines as well to educate the readers of the gap with CLIP like models\n- Details on how the instruction tuning data was curated are not provided\n\n### Questions\n\n- How many real pathology report findings did you'll extract in section 3 and how did you verify the quality of it\n- How do you evaluate the performance of the images retrieved for the prompts in section 3.1?\n- How many prompts do you use for each WSI?\n- How did you arrive at the design choice of using both prompt-based and clustering-based retrieval?\n- Why chose a threshold of 0.88 for similarity?\n- Is similarity computed globally in all the extracted patches across slides?\n- Does the revision agent have a no-operation capability as well? What happens when a correct description is passed to the revision agent?\n- What does first and second stage training in PathGen-CLIP mean?\n- Why did you use different datasets for zero and few shot experiments? As an example, Camelyon is not included in zero-shot examples\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper proposes a new 1.6 million dataset of paired image-caption pathology data.\nThe authors propose a scalable way to construct the dataset using existing LMMs and refinement strategies.\nThey show this dataset is useful in developing a pathology specific CLIP model which performs better than existing domain specific and general purpose CLIP style models in zero-shot an few-shot problems\nFinally they show that scaling dataset size and model size can lead to improvements in some tasks.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nOriginality:\nThe paper proposes a new way to generate quality image-caption pairs for pathology data. \nIt uses existing publicly available pathology image-caption data and the ability of newer general purpose LMMs like GPT-4 to generate detailed descriptions to create an initial dataset for training a LLaVA style captioning model.\nIt then uses this captioning model in conjunction with a revision and summary agent to generate synthetic image-caption data.\nThis approach of using existing data and LMMs to build a model and refine its outputs using other agents is interesting and not explored in the context of pathology.\nGiven the large nature of WSI images and significant redundancy and similarity of image content, the authors propose ways to construct a dataset of diverse image patches\n\nEvaluation:\nThe authors evaluate the model on various zero-shot and few-shot patch-level tasks and on WSI-level prediction tasks and compare its performance against various existing models. The results with GigaPath a vision encoder are promising and show we can use existing vision encoders and improve their language capabilities using the captioning data.\nThey also qualitatively evaluate a small subset of the generated captions using expert pathologists. \n\nExperiments:\nThe authors show various ablations around dataset construction which are valuable in understanding some of the limitations of using captioning models. The section scaling dataset and model size is interesting and shows some evidence around usefulness of scaling datasets and models using this approach.\n\nAll the data and models are publicly available which is great for the pathology community and this will also be the largest publicly available image caption dataset for pathology.\n\n### Weaknesses\n\nClarity:\nWhile I appreciate the authors covering a lot of ablations and experiments and describing the prompts, many of the design choices aren't clearly explained well. I've added some in the questions below. \n\nEvaluation:\nWhile the authors do compare with many older pathology VLM models, its unclear why they couldn't get access to the more recent CONCH which is publicly available on HuggingFace. \nFor WSI tasks while its helpful they added Gigapath, they don't compare against better publicly available pathology vision encoders like \nH-Optimus.\n\nImprovements:\nGiven the setup, its unclear how much of the improvements are coming from the new PathGen data and the refinement through revision agent, given they are one of the main contributions of the paper. i.e There isnt a comparison of the performance of the original PathGen-CLIP-L_init encoder with the improved PathGen-CLIP-L. Another comparison which would be useful is understanding with/out data generated using the revision agent.\nIt would be useful to highlight these as it helps understand how well such a setup can scale in generating synthetic data for iterative refinement. \nQuality and Scale of Initial Dataset:\nIts also unclear how important the scale of the detailed caption dataset 30K used for training PathGenLlava and the quality of it. Does scaling the dataset size and having some refinement here help? Have the authors checked the quality of captions generated by GPT-4 here and can provide some insight.\n\n### Questions\n\nDataset Construction:\nIts unclear why and how the subset of 700K samples was choosen from existing datasets to create PathGen_init. \nFor training the description agent, the authors mention using 10K initial image-caption pairs to generate 30K dataset, so are 3 captions generated per image?\n\nRevision Agent:\nWhat model is used for training the revision agent? Its also unclear what the inputs for revision agent are at inference? Does it take the generated caption and produce possible edits?\n\n\nIn figure 10, its unclear the w/ PathGen_init two stage performance doesnt vary when scale of PathGen data is varied? When scale is 0 it means all data is PathGen_init is that correct?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces PathGen-1.6M, a large-scale dataset containing 1.6 million high-quality image-caption pairs extracted from Whole Slide Images (WSI).  It also developed a scalable approach for high-quality pathology image-text data generation by multiple agent models collaborating, paving the way for next-generation general pathology models.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.\tThe paper presents PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Based on this dataset, the authors develop PathGen-CLIP, a pathology-specific CLIP model, which achieves substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks.\n2.\tThe authors propose a data construction pipeline that employs multiple LLM agents for description, revision, and summarization. This multi-agent collaboration approach generates more accurate image-caption pairs, validated through human evaluation.\n3.\tThe experiments conducted in the paper are solid and well-executed.\n4.\tThe release of the dataset, code, and model contributes significantly to the advancement of the pathology image research community.\n\n### Weaknesses\n\n1.\tThe Revise LMM appears to have limited utility, primarily providing editing capabilities such as additions, deletions, or modifications. It seems ineffective in addressing common pathological inaccuracies in the generated descriptions.\n2.\tOnly evaluated in CLIP model. Can be work in more pretrained Vision-Language model.\n\n### Questions\n\n1.\tThe writing of the paper requires improvement, as there are several shortcomings. For example, there is a typo in “PathGen-LLaVAdesp” in Section 4.1, and the conclusion summarizes, “we train two advanced models: PathGen-CLIP and PathGen-CLIP-L.” It appears there is an additional model, “PathGen-LLaVA,” which needs clarification.\n2.\tA significant concern arises regarding potential data leakage in the downstream tasks and benchmarks evaluated. To my knowledge, many tasks and benchmarks are derived from TCIA pathology data, which raises suspicions about the homogeneity of the constructed dataset.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nBuilt upon domain-specific WSI-Reports data from TCGA, this paper proposed the largest high-quality patch-text pairs dataset for Computational Pathology (CPath) by prompting LMM to generate text descriptions for pathology patches. To this end, a scalable data curation method is proposed by leveraging several LMM agents to describe, revise and summarize the generated descriptions.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. To address data scarcity in CPath for pretraining large models, a scalable data curation method is proposed to expand the limited image-text dataset.\n2. The largest patch-text pairs dataset is curated and used for pretraining to enhance the foundational power in CPath tasks.\n3. The scalability to more WSI-only data is interesting, which has the potential to greatly expand the data scale for large pathology models.\n\n### Weaknesses\n\n1. To support the superiority of the proposed data construction method by introducing extra patch-level supervision, the performance of directly utilizing original report data along with patch images to pretrain a pathology foundation model should be presented.\n2. During data construction, some representative patches were filtered out. These patches are supposed to align well with pathology reports, as they are retrieved based on the report data. To validate the extra contribution of generated patch descriptions, the proposed model should be compared with the one trained on these selected representative patches as well as its corresponding report data.\n3. The model’s generalizability to out-of-domain data has not been validated. The authors tried to scale the model to non-WSI report paired data. However, these data are still from TCGA.\n4. Some SOTA pathology foundation models are missing, especially in experimental comparison, such as UNI [1], CONCH[2], mSTAR[3] (which is also a CLIP-style model trained on TCGA data as well), etc.\n5. Few-shot’s capability significantly relies on how well-aligned the vision and text spaces are. The proposed method is supposed to compare with VLM like CONCH, instead of vision-only GigaPath.\n6. Details in EXPERT EVALUATION are missing. For example, how do authors define what correct or incorrect findings are?\n\nMinor concerns:\n\n- It is hard to recognize different models according to their colors. Please choose ones with higher contrast.\n- The citation of GigaPath seems to be lost.\n\n[1] Towards a general-purpose foundation model for computational pathology, Nature Medicine, 2024\n\n[2] A visual-language foundation model for computational pathology, Nature Medicine, 2024\n\n[3] A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model, arxiv, 2024\n\n### Questions\n\n1. In Step 5 of data construction, how do authors ensure that no essential details are lost?\n2. How can be validated the effectiveness of each step in data curation? This should be discussed in the ablation study.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chenglu Zhu",
      "Jingxiong Li",
      "Kai Zhang",
      "Lin Yang",
      "Tao Lin",
      "XINHENG LYU",
      "Xuan Gong",
      "Yixuan Si",
      "Yunlong Zhang",
      "Yuxuan Sun",
      "Zhongyi Shui"
    ],
    "url": "pdfs/iclr.cc-2025-conference_3f75159968c8c758cc291df4acca4de108949140.pdf",
    "remote_url": "https://openreview.net/pdf/3f75159968c8c758cc291df4acca4de108949140.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Generative Editing via Convolutional Obscuring (GECO): A Generative Adversarial Network for MRI de-artifacting",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to neuroscience & cognitive science"
    ],
    "keywords": [
      "Deep convolutional neural networks",
      "computer vision",
      "medical machine learning",
      "image analysis",
      "generative adversarial networks",
      "artifact removal",
      "machine learning model generalization"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is the dominant diagnostic technique to non-invasively image the brain, and deep learning has proven a powerful tool for analyzing these images. However, machine learning models trained on such MRI data have empirically shown an ability to detect complex and invisible artifacts, such as which type of machine a scan was taken from to a high degree of accuracy. Such artifacts are potentially invisible to the human eye, but can be identified by machine learning systems, leading them to focus on irrelevant features rather than scientifically and/or medically useful ones. For example, machine learning systems can often “shortcut” past the actual features researchers would like to detect and utilize separate spurious correlations to make predictions. Several such undesired features have been reported to interfere with cross-institutional medical imaging deep learning research, and more are likely to be identified as time goes on. Here, we develop a method capable of removing these spurious correlations in an unsupervised manner, leveraging generative techniques to produce images which maintain image quality while learning how to remove technical artifacts. Generative Adversarial Networks are a class of deep learning architectures which have shown impressive efficacy in image generation and editing tasks, and our work builds upon this success. Here, we propose Generative Editing via Convolutional Obscuring (GECO), a Generative Adverserial Network for MRI deartifacting. GECO is based on a CycleGAN, a GAN architecture designed for image-to-image translation that is transforming an input image into a new image with one or more desirable properties. By formulating the CycleGAN loss as a two-player game with a regularization term and incentivizing the generator to erase spurious correlations the original image quality can be better preserved. Beginning with classifiers trained on original images to identify images based on artifacts of interest, GECO reduced the classifiers’ ability to detect these spurious correlations from 97% down to a difference which is nearly equal to a classifier making purely random guesses. We also observe over 98% structural similarity between the original and deartifacted images, indicating the preservation of the vast majority of non-spurious information contained in the original images. In addition to solving the known problem of avoiding artifacts from scanner type, this method opens the door to potentially removing many other types of spurious correlations from medical images and other data modalities across many fields.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes GECO, which is based on Generative Adversarial Network for MRI deartifacting. Brain MRI images from 4 different manufactures are involved as the testbed. Multiple network architectures are explored and the harmonization seems to be successful as the classifier accuracy drops to around 25%.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The motivation of the paper is reasonable and removing artifacts does play an important role in medical imaging application.\n\n2. The deartifacting effect of the GECO is expected, especially according to the dropping accuracy to around 25%, which is close to random guessing.\n\n### Weaknesses\n\n1. Structure of the proposed GECO lacks novelty compared to GAN, which involves one generator and one discriminator. Recently, diffusion models have been proposed and also applied in medical image translation and harmonization tasks [1].\n\n2. It is said in the paper no previous baseline to compare. However, in my opinion, deartifacting task falls in medical imaging harmonization, where there are previous works [2][3]. It would be better for authors to conduct downstream tasks, for example, classification or segmentation, of the images harmonized using GECO, and the previous methods.\n\n3. In related work (line 85-86), there is no evidence/papers supporting the sentence “artifacts are well-established to significantly impair the efficacy and generalizability of models trained on medical images”. It would be better to be more specific of how artifacts can affect the downstream tasks and include one or two examples.\n\n[1] Özbey, Muzaffer, et al. \"Unsupervised medical image translation with adversarial diffusion models.\" IEEE Transactions on Medical Imaging (2023).\n\n[2] Bashyam, Vishnu M., et al. \"Deep generative medical image harmonization for improving cross‐site generalization in deep learning predictors.\" Journal of Magnetic Resonance Imaging 55.3 (2022): 908-916.\n\n[3] Abbasi, Soolmaz, et al. \"Deep learning for the harmonization of structural MRI scans: a survey.\" BioMedical Engineering OnLine 23.1 (2024): 90.\n\n### Questions\n\nArtifacts in medical imaging can have multiple types. Some of them can significantly harm the generalizability of the models, for example, the text labels in the medical imaging. Based on the figure 1, the difference between the generated and the original images is minor. There is a concern that such artifacts would affect the performance on downstream tasks and to what extent. For example, one way to verify the harmonization process is to train the models using original and harmonized images, and test on set from another manufactures. Adding evaluation on potential downstream tasks can add more strengths to the paper.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors proposes a GAN approach to remove image artifacts arising from particular machine to take the scan.  The authors then present an ablation study from the Brain Tumor Progression and TCGA-LGG datasets and show the difference between generated images and original images are negligible.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe authors address an important that is important to the MRI community and in the real world.  The qualitative improvements are excellent and radiologists could not distinguish the results.\n\n### Weaknesses\n\nThe authors have not addressed prior work that is related to this topic.\n\nThe task of removing cross-protocol and cross-scanner image attributes in scanners is known as 'Data Harmonization' and is well-studied in the MRI community.  Please see the following recent reviews:\n\nhttps://www.nature.com/articles/s41597-024-02956-3\n\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC10135601/\n\nhttps://pubmed.ncbi.nlm.nih.gov/37084926/\n\nThere are many baselines the authors could have considered.\n\nThe authors could have also considered the field of domain adaptation/transfer for inspiration and baselines.\n\n### Questions\n\nSuggestions:\nUse ` to start quotation marks in latex.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIt is an urgent issue that machine learning systems tend to focus on spurious information in images while overlooking relevant features, as these spurious correlations hinder cross-institutional medical imaging research and model generalizability. The authors propose Generative Editing via Convolution Obscuring (GECO), a generative adversarial network designed to remove artifacts in MRI. By applying GECO, the classifier's ability to detect these spurious correlations dropped from 97% to near random-chance accuracy. Additionally, a high structural similarity was observed between the original and artifact-free images, indicating that the vast majority of non-spurious information in the original images was retained. This is an interesting study, however both the experiments and the writing lack sufficient rigor, with several issues present.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nRemoving spurious correlations from images is crucial for enhancing the generalizability of downstream tasks (such as classification, segmentation, and detection) and represents a unique and underexplored research direction. This study analyzes the impact across different classifier architectures and generative network structures, preventing classifiers from accurately capturing spurious information related to manufacturer and field strength categories.\n\n### Weaknesses\n\n1. The writing lacks rigor and needs improvement, including missing symbol explanations for X and Y in Equations (1)-(4). Additionally, there are misstatement errors, such as in line 423 where \"achieved\" should be replaced with \"dropped.\"\n2. The study lacks experimental comparisons with state-of-the-art methods and verification of spurious information removal for downstream tasks.\n3. In Figure 1, the visualized “Differences” appear primarily as high-frequency edge information variations and background artifact removal. The edge high-frequency differences are largely due to the network's smoothing effect on outputs, which is common in generative tasks and does not necessarily represent spurious information. Removing background artifacts as spurious information has limited practical value, as downstream tasks typically apply background removal preprocessing to prevent background artifacts from impacting performance.\n\n### Questions\n\n1. These results are currently insufficient to demonstrate that this method effectively removes spurious correlations. Is there a better explanation for this? I believe background artifacts may be a significant influencing factor and should be excluded.\n2. Also, why are there no comparative experiments with state-of-the-art methods in the classification stage or validation on downstream tasks?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Bin Jiang",
      "Bryce Allen Bagley",
      "Eric Tranvinh",
      "Ge Cheng",
      "Matei Armănașu",
      "Michael Iv",
      "Michael Zeineh",
      "Nancy J. Fischbein MD",
      "Olivier Gevaert",
      "Sergei Petrov"
    ],
    "url": "pdfs/iclr.cc-2025-conference_94af9b211141ca98f65376d7c0e542c7e6fcfded.pdf",
    "remote_url": "https://openreview.net/pdf/94af9b211141ca98f65376d7c0e542c7e6fcfded.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Joint Denoising of Cryo-EM Projection Images using Polar Transformers",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "denoising",
      "microscopy",
      "tomography",
      "attention",
      "symmetries"
    ],
    "abstract": "Deep neural networks (DNNs) have proven powerful for denoising individual images, but there is a limit to the noise level they can handle.\nIn applications like cryogenic electron microscopy (cryo-EM), the noise level is extremely high but datasets contain hundreds of thousands of projections of the same molecule, each taken a different viewing direction.\nThis redundancy of information is useful in traditional denoising techniques known as class averaging methods, where images are clustered, aligned, and then averaged to reduce the noise level.\nWe present a neural network architecture based on polar representation of images and transformers that simultaneously clusters, aligns, and denoises cryo-EM projection images.\nResults on synthetic data show accurate denoising performance using this architecture, with a relative mean squared error of $0.06$ at signal-to-noise (SNR) level of $0.05$, outperforming traditional filter-based methods by a factor of $2\\times$.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper develops a strategy for jointly denoising single particle cyroEM particle projections. The proposed approach is based upon applying a CNN (to one image) or a transformer architecture (to many images) that is/are transformed into polar coordinates. The network architectures are specialized to have desirable invariances in the polar domains. The proposed method is tested on simulated data with varying amount of Poisson and Gaussian noise. The proposed method outperforms simple Weiner filtering.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n+++Proposed method (using an invariant network to jointly denoise multiple images in the polar domain) is novel and well-motivated\n\n### Weaknesses\n\n---Contextualization/Practical relevance: The manuscript states the single particle reconstruction pipeline \"involves first extracting images of individual particles from the micrographs, denoising these images, estimating their corresponding viewing directions, and then reconstructing a 3D density map\". However, it is my understanding that modern reconstruction techniques (e.g., expectation maximization [A]) marginalize over the various view directions and avoid denoising the 2D projections all together. \nThe manuscript mentions contamination detection as another possible use case for denoising, which may be valid. The manuscript needs to clarify which parts of the reconstruction pipeline it may and may not improve.\n\n[A] Scheres, Sjors HW. \"RELION: implementation of a Bayesian approach to cryo-EM structure determination.\" Journal of structural biology 180.3 (2012): 519-530.\n\n\n---Weak baselines: The proposed method outperforms Weiner filtering by a large margin. How does it compare to advanced non-learning-based methods like BM3D? How does it compare to a conventional CNN without a polar transformation?\n\n---Missing ablations: There is no evidence any of the proposed network choices are useful. How does the proposed method compare to a single U-net applied to cartesian coordinates? How important are the various architectural choices?\n\n---Section 1 says the proposed method clusters, aligns, and denoises but the next section states it is assumed the images have been centered. If so, why is alignment necessary?\n\n----How does proposed method compare to denoising entire micrographs?\n\n---Typo: L464 a reference is missing\n\n---Ignores CTF effects. To the paper's credit, this limitation is clearly acknowledged.\n\n### Questions\n\nHow relevant is the proposed contributions in modern single particle cryoEM pipelines? Do denoisers have a role to play?\n\nHow does the proposed method compare to high-performance image denoisers?\n\nWhich of the design choices (e.g, polar coordinates) matter and which don't?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper presents a deep learning approach to denoise cryo-EM images, which is challenging due to high noise levels and varied viewing directions. The authors apply polar representation within both proposed CNN and transformer models, achieving improved results in denoising.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. Leveraging polar representation can simplify the denoising task by translating rotational variance into angular shifts. This approach is promising for denoising given the rotational nature of cryo-EM.\n2. Using a dataset of 5,000,000 images strengthens the model's robustness and enables it to learn effectively from diverse viewing angles.\n3. Testing both Gaussian and Poisson noise further validates the proposed model.\n4. Testing both CNN and transformer architectures increases the reliability of the proposed method.\n\n### Weaknesses\n\n1. Some key details in the approximations lack mathematical measurements (see Q1).\n2. Results are only compared to the Wiener filter method. Including comparisons with more state-of-the-art methods would be better.\n3. Adding a mathematical proof explaining why and how polar representation improves denoising accuracy would strengthen the theoretical foundation of the approach (Related to Q3). \n4. At leat two typos. (See Q5).\n\n### Questions\n\n1. Could you provide more details on the approximations in Eq. (8) and Eq. (10), such as the error order?\n\n2. Will your model work with negative binomial noise?\n\n3. You stated that \"f(R_alpha(x)) = R_alpha(f(x))\" when f could be a statistical noise distribution. If we consider the denoising model H_theta as an inverse of a statistical noise distribution, would \"H_theta(R_alpha(x)) = R_alpha(H_theta(x))\"? If so, how does the polar representation improve accuracy? If not, could you explain the key difference?\n\n4. In Section 5.2, I think \"f_key\" and \"f_query\" are symmetric. That is, using \"m - l\" in either should work. So, why is f_key chosen over f_query? I also wonder if adding both query and key with the same value, but with output as \"S_l = (sqrt(2) / 2 * S_l_key) * (sqrt(2) / 2 * S_l_query)\" would yield more accurate results, since it allows the model more freedom to learn more features.\n\n5. On line 178 and 179, should “the” be changed to “they”? On line 229 and 230, should it be \"L/2 - 1\"? And on line 464 and 465, there is an a question mark.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper addresses denoising in cryo-EM images by designing a neural network that is equivariant to rotations. The authors achieve this by constructing a polar representation of the image and applying convolutions in this transformed space, thereby effectively converting the translational equivariance of standard convolutions into rotational equivariance. These _polar CNNs_ then serve as the key and query networks in a transformer to obtain a rotation-equivariant attention mechanism between images that should allow for solution of the full set denoising task, i.e. the implicit clustering of images by viewing angle for an improved denoising performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\nThe paper is clearly written and provides a great introduction to the problem of denoising in cryo-EM reconstruction. The authors introduce their method step-by-step, making it easy to follow the rationale of each of their design choices.\n\nThe use of a polar representation to achieve rotational equivariance is innovative and well-founded, facilitating deep learning-based denoising that can leverage the signal from images taken at the same viewing angle without requiring an explicit preprocessing step to cluster by angle.\n\n### Weaknesses\n\nWhile the theoretical foundation of the method is convincing, the experimental validation of its effectiveness is lacking. \n\nThe only baseline comparison is to Wiener filtering, with no comparisons to other deep learning-based methods, despite several being mentioned in the related work section. I suggest to include a comparison to at least one of the deep-learning based methods, such as the DnCNN (Zhang et al., 2017) or a U-Net denoiser adapted for cryoEM data.\n\nFurther, all experiments are conducted on simulated data. It therefore remains unclear whether the method would generalize to real data acquisitions as well. Do the authors envision the method to be trained on simulated data and then be applied to real data? In particular, the training and evaluation both use sets of images that only contain two viewing angles. How can this be achieved for real data? This apparent limitation is not discussed. I suggest the authors either 1) include experiments where they apply their method to real data or 2) discuss challenges they foresee in the application to real data, including possible domain shift and a random sampling of viewing angles.\n\nIn conclusion, the paper could be strengthened significantly by an extended set of experiments that show this theoretically-compelling method to work well in practice and a discussion of its real-world limitations. Specifically, I suggest the authors include a deep-learning based method as an additional baseline to make their evaluation more comprehensive and demonstrate or discuss the applicability of their method to real data. If application to real data is currently not feasible, I suggest to include a discussion of those limitations and an outline for future work for how those limitations could potentially be overcome.\n\n### Questions\n\nQuestions:\n- Are the weights shared between key and query networks?\n- The conclusion mentions that important future work includes scaling the method to larger datasets. Which part of the method does not scale right now?\n\nTypos/Format:\n- L142: providing a stronger priors -> providing stronger priors\n- L229/230: -L/2, ..., -L/2-1 -> -L/2, ... , L/2 -1 \n- L269: fourier transform of $\\phi$?\n- L464: reference not resolved\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper deals with the denoising of images produced in the context of cryogenic electron microscopy (cryo-EM) single-particle analysis. In cryo-EM single-particle analysis, an electron microscope records 2D projections of the unknown 3D structure of a single type of particle (e.g., a specific protein) at random orientations. These projections can then be used to estimate a 3D model of the particle. Due to data acquisition limitations, the 2D particle projections have very low signal-to-noise ratios, requiring denoising.\n\nThe authors propose two novel deep learning architectures to improve the denoising of 2D particle projections: The Polar CNN and the polar transformer.\n- The polar CNN is designed to be equivariant to 2D rotations. The authors argue that this property is desirable because the 2D particle projections capture the particle at random orientations.\n\n- The polar transformer is based on the polar CNN and additionally exploits redundancy in the data by combining information from projection images that show the particle from a similar direction. \n\nThe authors test their new architectures on synthetic data and compare them to denoising with a Wiener filter.\n\n**Recommendation:** I do not recommend the paper for publication at ICLR. In my opinion, the weaknesses in the experiments outweigh the reasons to accept the paper (see \"Weaknesses\" for more details). However, I would like to emphasize that the paper has potential and I am willing to change my evaluation if all my concerns regarding the experiments are sufficiently addressed.\n\n**Post rebuttal:** I have increased my score from **3 to 5**.  As part of their rebuttal, the authors have clarified the details of the angular attention mechanism. Moreover, they have included a comparison to a deep learning-based denoiser in which they show that the polar transformer architecture outperforms \"vanilla\" networks. Overall, I think that the proposed architectures are interesting contributions.\nI have not increased my score further because one of the main concerns I described in my review remains: The authors have not tested their methods on real data which comes with additional challenges such as the need for self-supervised training due to the lack of ground truth.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- The use of architectures that are rotationally equivariant and explicitly exploit redundancy for denoising individual particle projections is a novel and interesting idea and has the potential to improve upon existing deep learning-based denoising methods.\n- Experiments on synthetic data suggest that exploiting the redundancy in the data (as done by the polar transformer in the \"directional\" setup) is beneficial to the denoising performance.\n- The paper is clearly written in most parts (only section 5.2 is a bit unclear, see \"Weaknesses\").\n\n### Weaknesses\n\nMajor Weaknesses:\n\n\n- Weaknesses in the experiments (Section 6):\n\n\t- The authors only compare their polar CNN and their polar transformer to a denoising Wiener filter, and there is no comparison to other deep learning-based denoisers, which typically outperform classical denoising methods. Good candidates for deep learning based baselines could be, e.g., CryoCARE [1] and Topaz [2].\n\n\t- There is no experiment that compares the denoising performance of a standard 2D convolutional network with that of a polar CNN and a polar transformer.\n\t\tTherefore, it is unclear how much denoising actually benefits from the equivariance properties of the polar CNN and the polar transformer. \n\n\t- The authors train their polar CNN and their polar transformer in a supervised manner. In my opinion, this setup is not realistic, since pairs of noisy images and clean underlying ground truth are typically not readily available in cryo-EM single-particle analysis (see also \"Questions\").\n\t\tPerhaps it would be possible for the authors to consider self-supervised training without clean ground truth for their experiments. (This has already been done for methods such as CryoCARE and Topaz).\n\n\t- All experiments are on synthetic data. There are no experiments on real images produced with cryo-EM. This recent dataset proposed by Dhakal et al. [3] contains real cryo-EM images with picked particles. Maybe this data can be used for further evaluation. \n\n\n- Section 5.2 on angular attention, which is one of the central contributions of the paper, is unclear to me. There is no explicit definition of the angular attention layer $f_\\theta^{\\text{ang-attention}}$. I have formulated some questions about angular attention in the \"Questions\" section.\n\nMinor weaknesses:\n- It is unclear to me which parts of the polar CNN and polar transformer architectures are novel and which are from existing work. Perhaps the authors could make this clearer in the paper.\n- This is more of a suggestion: Figures illustrating angular convolution and angular attention mechanisms might make these concepts a bit easier for readers to grasp.\n\n\n\n\n--------------------------------------------\n[1] Buchholz, Tim-Oliver, et al. \"Cryo-CARE: Content-aware image restoration for cryo-transmission electron microscopy data.\" 2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE, 2019.\n\n[2] Bepler, Tristan, et al. \"Topaz-Denoise: general deep denoising models for cryoEM and cryoET.\" Nature communications 11.1 (2020): 5208.\n\n[3] Dhakal, Ashwin, et al. \"A large expert-curated cryo-EM image dataset for machine learning protein particle picking.\" Scientific Data 10.1 (2023): 392.\n\n### Questions\n\n- Given the challenges associated with supervised training of a denoiser (see \"Weaknesses\"), how would the authors train a model that can be used to denoise real, experimental images? Is it possible for the model to generalize well from synthetic to real data?\n\n- In lines 63 - 65, the authors state that denoising a micrograph is easier than denoising selected projections of individual particles. If this is true, wouldn't a method that denoises the entire micrograph *before* picking perform better than the authors' proposed approach of denoising individual particle projections? \n\n- Questions about angular attention:\n\t- Why are only the keys rotated by the angle $\\alpha_\\ell$? (line 373-374)\n\t- The authors state that \"[they] apply a corresponding rotation to the value vectors\". Does this require knowing the angle $\\alpha_\\ell$? If so, why can the angle be assumed to be known?\n\t- Could the authors please provide an explicit formula or algorithm for the angular attention mechanism?\n\t\t\n- Why is the value net in the polar transformer \"set to identity\" (line 417)?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Joakim Andén",
      "Justus Sagemüller"
    ],
    "url": "pdfs/iclr.cc-2025-conference_47b5add70675692f7233dca28a272cf5c357e497.pdf",
    "remote_url": "https://openreview.net/pdf/47b5add70675692f7233dca28a272cf5c357e497.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "LVLM",
      "Eye Gaze",
      "Video",
      "Medicine",
      "Medical Image",
      "Chest X-ray",
      "Chest X-ray Report Generation"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists’ eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. The video, featuring a red gaze point overlaid on CXR images, emphasizes regions of focused attention during interpretation. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with a video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 25.4% on Impression generation task and on average 7.9% for all tasks using scaled evaluation metrics. Our approach enhanced open-domain LVLM models, when combined with exemplar reports for in-context learning, outperform medical models as well as those specifically trained for CXR report generation on the benchmark dataset. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks, pointing out a new effective approach of utilising LVLMs in healthcare and beyond.",
    "reviews": [
      {
        "text": "### Summary\n\nTo enhance the reliability of large vision-language models (LVLMs) in real clinical environments, this study proposes a novel video prompting method called RadEyeVideo, which integrates radiologists' eye-tracking data as video sequences, capturing the spatial and temporal dynamics of gaze. Red fixation points are superimposed on CXR images to highlight the areas that doctors pay attention to, so as to dynamically present the doctor's eye movement path. Improve the ability of radiologists or AI models to diagnose chest diseases by providing rich contextual information.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed method, RadEyeVideo, combines video, text, and eye movement data to realize the fusion of multi-modal information and improve the accuracy and efficiency of diagnosis.\n2. The authors conducted a thorough assessment of various eye-tracking integration techniques, providing strong empirical support for their claims.\n\n### Weaknesses\n\n1. In Figure 2, the authors illustrate different prompting methods; however, the figure does not clearly distinguish between the use of text descriptions and video inputs. This ambiguity makes it challenging to understand whether the authors used text descriptions to guide the prompt along with the video input or if they only provided video data. \n2. The author's experiments conducted on only one dataset are clearly insufficient in terms of persuasiveness. This limitation may affect the generalizability and applicability of the research findings. To enhance the credibility of the study, it is recommended that the authors validate their approach on different datasets.\n3. When the authors use radiologist's eye-tracking sequences as video input, this approach does enhance the model's understanding of prior knowledge in the diagnostic reading process to a certain extent. However, converting eye-tracking data into video format substantially increases the number of tokens, leading to significant computational resource consumption and longer inference times for the model.\n\n### Questions\n\n1. The RadEyeVideo method integrates the eye movement data of the radiologist as a video sequence to capture the spatiotemporal dynamics of his gaze. However, the article does not explain in detail how this video sequence is specifically generated, for example, how the eye movement data is sampled and converted into video frames, and how these frames contain temporal and spatial information.\n2. In Section 2.3 INPUT REPRESENTATION, to fit the input requirements of LVLM, the authors uniformly sampled the video sequence and selected 16 frames as inputs. However, the article does not explain why 16 frames were chosen and whether this choice had a significant impact on model performance. In addition, whether the impact of using more or fewer frames on model performance has been explored is also a question worth exploring. This helps to further understand the effect of video data length and quality on model performance.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces an innovative approach that incorporates radiologists' eye-tracking data as video sequences. This method captures both spatial and temporal patterns in gaze, which provides a more accurate representation of radiologists' attention during chest X-ray interpretation. The approach was evaluated using three general-domain LVLMs, showing significant improvements.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- RadEyeVideo's use of video-based eye-gaze data is a unique contribution that effectively captures the temporal and spatial dynamics of radiologists' focus. I like this idea.\n- The study demonstrates substantial improvements, particularly in impression generation, highlighting RadEyeVideo's effectiveness in enhancing diagnostic tasks.\n- The language is clearly presented. The authors use precise and concise language so that the reader can easily understand the methodology, and results of the study.\n\n### Weaknesses\n\n- Although this idea is interesting, it still relies on temporal and spatial information in the inference phase, which is difficult to apply to real clinical scenarios. Do the authors consider involving multiple information inputs only in the training phase and simulating zero-shot scenarios as much as possible in the inference phase?\n- The study’s findings are limited by the small size of the MIMIC-Eye dataset, which may not fully capture the variability in real-world clinical settings, raising questions about the generalizability of the results.\n- The comparisons primarily focus on selected models with minimal tuning for this domain. Including a wider range of task-specific medical LVLMs could provide a more comprehensive evaluation.\n\n### Questions\n\n- Why the paper lacks an section of related work? e.g., some recent evaluation work of Med-LVLMs [1,2,3]\n- The formats of reference is weird. e.g., in Line 311-321. Please check it.\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[3] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces eye-tracking data into medical LVLMs and uses video formats to retain the temporal and spatial characteristics of radiologist’s eye movements, providing more comprehensive prior information for the diagnostic model. Compared to heatmaps and textual prompts, this method fully considers temporal features. Evaluation on multiple downstream tasks demonstrate the effectiveness of the approach.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. This work introduces radiologists' eye-tracking data into LVLMs in video format, highlighting the temporal features of eye movement sequences.\n2. The experiments in the paper are comprehensive, validating multiple diagnostic tasks across various datasets.\n\n### Weaknesses\n\n1. The paper mentions using simple stacking of gaze points for heatmap generation, but it does not specify the radius size of the gaze points. Different gaze point sizes can affect the model's interpretation.\n2. In-context learning often heavily relies on the provided examples, which can significantly influence the generated results. The paper does not discuss what constitutes suitable examples.\n\n### Questions\n\n1. Are the eye-tracking data collected from a single radiologist? If there are multiple radiologists, how do you handle individual behavioral differences?\n2. How is the duration mentioned in line 176 used to control the frame count? This aspect is not explained in the paper. Similarly, the sampling method mentioned in line 194 is not clearly described.\n3. Does the sampling of eye-tracking data risk losing short-term diagnostic behaviors, potentially affecting the diagnosis of various types of diseases?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis article introduces a novel prompting method called RadEyeVideo, which presents radiologists’ eye-tracking data as a video sequence to capture the temporal and spatial order of their gaze (i.e., scan paths). The authors evaluated the effectiveness of this approach in chest X-ray report generation and disease diagnosis tasks using large vision-language models (LVLMs) with video input capabilities. Results show that incorporating eye-gaze video prompts improved model performance by 25.4% on the Impression generation task, with an average performance increase of 7.9% across all tasks.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The RadEyeVideo method converts radiologists’ eye-tracking data into video sequences for use in chest X-ray report generation and diagnostic tasks, significantly enhancing the performance of general large vision-language models (LVLMs). Experimental results indicate a 25.4% improvement in the Impression generation task and an average improvement of 7.9% across all tasks, outperforming models specifically designed for medical applications.\n2. Comprehensive Evaluation of Eye-Tracking Prompting Methods: This study presents the first thorough evaluation of various eye-tracking integration techniques, including static heatmaps, eye-tracking text prompts, and dynamic video prompts. The results indicate that RadEyeVideo outperforms the other methods in terms of diagnostic accuracy and clinical relevance, establishing a new standard for eye-tracking data prompts.\n\n### Weaknesses\n\n- Methodological Clarity: a) The paper lacks sufficient detail about the video prompt integration process, particularly how the video information is encoded and processed by the LVLMs. The technical implementation of combining video sequences with textual prompts could be more thoroughly explained. b) There's limited discussion of potential alternatives to full video sequence processing that might achieve similar results with lower computational costs.\n\n- Data Collection and Generalizability Limitations: a) The methodology requires synchronized eye-tracking data collection during radiologist readings, which is resource-intensive and difficult to scale. b) The approach may not generalize well to scenarios where real-time eye-tracking data is unavailable or impractical to collect. c) The current evaluation is limited to a relatively small dataset (2,298 CXR images), raising questions about broader applicability.\n\n- Computational Efficiency Concerns: The direct use of complete eye-tracking video sequences as prompts likely increases computational overhead significantly. While the authors mention sampling k frames (typically 16) from the total sequence, there's limited analysis of the computational trade-offs or optimal sampling strategies. The method may be computationally prohibitive for real-time clinical applications.\n\n### Questions\n\nPlease address the concern raised in weakness part.\nMoreover, there are some questions may help you to address the weakness.\n1) What is the computational overhead of processing video prompts compared to traditional image-only or text-only prompts?\n2) Have you explored more efficient alternatives to using complete video sequences, such as key frame selection or compressed representations? 3) How do you envision this approach being implemented in real-world clinical settings where real-time eye-tracking data may not be available? 4) Have you considered alternative methods for generating synthetic eye-tracking data that could make the approach more broadly applicable? 5)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Honghan Wu",
      "Jinge Wu",
      "Yunsoo Kim"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "remote_url": "https://openreview.net/pdf/a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Transformer-Based CT Anomaly Detection and Auto-Segmentation of Sparse Lung Nodules",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Transformer",
      "CT scans",
      "lung nodules",
      "anomaly detection",
      "auto-segmentation",
      "Deformable-DETR",
      "sparse data",
      "medical imaging",
      "self-attention",
      "multi-scale learning",
      "object detection",
      "Focal Loss",
      "segmentation"
    ],
    "abstract": "Accurate segmentation of lung nodules in computed tomography (CT) scans is challenging due to extreme class imbalance, where nodules appear sparsely among healthy tissue. Lung tumor boards often review these scans manually, a time-consuming process. This paper introduces a novel two-stage approach for lung tumor segmentation by framing the problem as anomaly detection. The method is divided into two stages, allowing each model to leverage its strengths. Stage 1 focuses on region proposal, employing a custom Deformable Detection Transformer with Focal Loss to overcome class imbalance and localize sparse tumors. In Stage 2, the predicted bounding boxes are refined into pixel-wise segmentation masks using a fine-tuned variant of Meta's Segment Anything Model (SAM) for semantic segmentation. To address the challenge of nodule sparsity and improve spatial context, a 7.5 mm Maximum Intensity Projection (MIP) is applied, aiding in the differentiation between nodules, bronchioles, and vascular structures. The model achieves a Dice coefficient of 92.4%, with 95.2% sensitivity and 93.2% precision on the LUNA16 dataset, demonstrating robust performance in real-world clinical conditions where nodule sparsity is 5%.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a two-stage approach for lung tumor segmentation in LUNA16 CT scans tackling the challenge of nodule sparsity and class imbalance. Stage 1 uses a custom Deformable Detection Transformer with Focal Loss for region proposals while Stage 2 refines these into segmentation masks with a fine-tuned variant of segment anything model. Maximum intensity projection enhances spatial context, improving differentiation between nodules, bronchioles, and vessels. The model achieves a dice coefficient of 92.4%, with high sensitivity and precision.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 1\n\n### Strengths\n\nI like the idea of using maximum intensity projection to help neural networks clearly distinguish between nodules, bronchioles, and vessels.\n\n### Weaknesses\n\n- proposed framework brings together known architectural components like Deformable-DETR and SAM. I think the paper fall short in showing how these individual components are fundamentally innovated upon rather than simply integrated. Without clear evidence of novel adaptations or improvements to each component, the approach might seem like an assemblage of established methods rather than a groundbreaking technique\n- although model achieves strong results on a modified version of the LUNA16 dataset, its robustness in diverse real-world settings is uncertain. Validating the approach on additional lung datasets could reinforce its practical impact and help mitigate concerns of overfitting to a single dataset\n- paper introduces encoding and decoding processes using Deformable-DETR and SAM yet it lacks a detailed mathematical explanation of how refined feature maps are encoded and subsequently reconstructed. Without mathematical formulations and a proof of concept, it remains unclear how effectively the feature maps capture and retain essential characteristics through the two stages\n\n### Questions\n\nn/a\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists. However, the primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists.\n\n### Weaknesses\n\nThe primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Questions\n\nPreprocessing is performed with 1 mm isocubic resolution, yet the method for generating 7.5 mm MIP using five slices in Figure 1-c needs clarification.\n\nThe authors should analyze the histogram of nodules under 10 mm from the LIDC dataset and include these results and discussions in the paper.\n\nThe paper lacks external validation, which raises concerns about the generalizability of the findings. A discussion on this limitation is recommended.\n\nIn Figure 1-b, lung segmentation is reportedly performed using Otsu segmentation; however, accuracy metrics such as DSC should be presented.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPublic dataset",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a two-stage pipeline for lung nodule segmentation in CT scans, designed to support lung tumor boards by enhancing segmentation accuracy and efficiency. The first stage employs a custom Deformable Detection Transformer (DETR) architecture to detect sparse lung tumors, leveraging deformable attention to improve sensitivity to small nodules. The second stage utilizes a fine-tuned Segment Anything Model (SAM), enhanced with medical imaging capabilities (MedSAM), to refine the bounding boxes into pixel-level segmentation masks, ensuring precision in differentiating nodules from surrounding anatomy.\n\nTo address the class imbalance in CT data - where lung nodules are rare compared to healthy tissue - the framework incorporates focal loss, reducing model bias towards non-tumor areas and enhancing detection accuracy for hard-to-detect nodules. Achieving a 94.2% F1 score for bounding box prediction and a 92.4% Dice coefficient in segmentation accuracy, this pipeline demonstrates strong potential to improve clinical workflows, enhance tumor board decision-making, and contribute to better patient outcomes by streamlining nodule detection in a clinical setting.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper introduces a novel two-stage approach for lung tumor segmentation by framing the task as anomaly detection, addressing the challenges of sparse nodule identification in CT scans. This innovative structure uses a Deformable Detection Transformer (DETR) for region proposals and a fine-tuned Segment Anything Model (SAM) for precise segmentation, effectively handling class imbalance and complex image features.\n\nAdditionally, the paper’s clear, logical structure and well-explained methodology make complex concepts accessible. Detailed quantitative results further highlight the framework's effectiveness, making it a valuable and readable contribution to medical imaging and clinical decision support.\n\n### Weaknesses\n\n1. The paper’s experimental section lacks depth, with insufficient analysis to thoroughly validate the proposed method. \n\n2. There is no ablation study provided, which limits insight into how each component - such as the use of Deformable Detection Transformer (DETR), the fine-tuned Segment Anything Model (SAM), and the customized focal loss - contributes to overall performance. Without this breakdown, it’s difficult to assess which aspects of the framework are most effective. \n\n3. The paper relies solely on quantitative evaluation, omitting any qualitative assessment, such as visual comparisons among different methods, which could provide a clearer understanding of the model's segmentation accuracy and real-world applicability.\n\n4. The presentation of results is also weak, with layout issues that detract from readability and professionalism. For instance, Table 2 extends beyond the page margin, rendering the data difficult to interpret. Additionally, there are inconsistencies and errors in in-text citations, which may confuse readers and hinder the paper’s credibility. These issues in presentation and citation detract from the paper's overall clarity and polish, suggesting the need for more careful formatting and editing. There are also several grammatical errors, which make the paper somewhat challenging to read.\n\n### Questions\n\n- Why is there only quantitative evaluation/comparison provided in the manuscript? Could you provide some qualitative results, such as visual examples of segmentation outputs, to illustrate the model’s performance?\n\n- Can you elaborate on how the class imbalance was handled during training? Were any additional strategies (besides focal loss) considered or tested to further address this issue?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis manuscript presents a novel two-stage approach for automating lung nodule segmentation using transformer models. In the data preprocessing phase, Maximum Intensity Projection (MIP) enhances spatial features, helping to distinguish nodules from bronchioles and vessels in CT images. Next, region proposal bounding boxes are generated using the Deformable-DETR model. In Stage 2, these bounding boxes are processed by the SAM model to achieve pixel-level segmentation. To address class imbalance within the dataset, focal loss is incorporated into the original DETR loss function. The results demonstrate superior performance compared to state-of-the-art (SOTA) methods.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nIn preprocessing, maximum intensity projection is applied across slices to enhance visibility. The two-stage framework, combining DETR and SAM models, offers a straightforward approach. Additionally, the common issue of class imbalance in medical datasets is addressed. As a result, segmentation performance is significantly improved. The paper is well organized.\n\n### Weaknesses\n\nThis manuscript lacks novel insights, as the deep learning models used in each stage are well-established, and focal loss is widely applied across various domains. Additionally, the ROI-based segmentation approach is considered somewhat conventional. There is no ablation studies on original SAM performance and some critical models to compare or discuss are missing.\n\n### Questions\n\n1. What is the technical insight of this work? The concept of ROI-based segmentation is not new, and both DETR and SAM are well-established models. As a result, the framework appears to lack novelty, which is a critical concern.\n\n2. There are several existing studies on lung nodule segmentation in CT images. For instance, the IEEE TMI paper, \"Closing the Gap between Deep Neural Network Modeling and Biomedical Decision-Making Metrics in Segmentation via Adaptive Loss Functions,\" addresses not only lung segmentation but also class imbalance. It would be beneficial for the authors to compare or discuss their work in relation to such prior studies.\n\n3. Does the model function in an end-to-end learning manner? If it does or does not, the authors should provide a discussion on the merits and limitations of the learning method used in this framework.\n\n4. Which stage of the framework provides the most significant performance improvement? \n\n5. Additionally, what is the baseline performance of the original SAM model pretrained with MedSAM? Are the datasets used in MedSAM aligned with the LUNA dataset, and how does this impact model performance?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Hooman Ramezani"
    ],
    "url": "pdfs/iclr.cc-2025-conference_3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "remote_url": "https://openreview.net/pdf/3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications->health_medicine"
    ],
    "keywords": [
      "Multimodality",
      "Distillation",
      "Cross Modal Distillation",
      "Transcriptomics",
      "Data Augmentations",
      "Microscopy Imaging",
      "Gene Relationships"
    ],
    "abstract": "Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) *Semi-Clipped*, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) *PEA* (**P**erturbation **E**mbedding **A**ugmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper aims to extract representations of transcriptomics by distilling knowledge from microscopy images. The authors introduce (1) semi-clipped for cross-modal distillation from pretrained foundation models, and (2) perturbation embedding augmentation to generalize transcriptomics data.\n\n### Claims And Evidence\n\nThe concept of 'semi-clipped' is not essentially different to the vanilla CLIP, but simply with fixed pre-trained microscopy image features using weakly pair samples.\n\n### Methods And Evaluation Criteria\n\nOne significant concern: Microscopic images and transcriptomics data, although they may have shared information, but the feature spaces may not be highly overlapping. In other words, these two modals have complementary information, which cannot be mutually replaced. Forcing the transcriptomics data to extract only the features that are similar to the image data may cause the loss of distinct information of transcriptomics. This paper doesn't show empirically that the learnt representation of transcriptomics is relatively comprehensive.\n\n### Theoretical Claims\n\nNo Theoretical contribution is made in this work.\n\n### Experimental Designs Or Analyses\n\n* No t-SNE to show the distribution of learnt features. \n* Lack ablation study when teacher representations are not frozen.\n\n### Supplementary Material\n\nThe supplementary material provides more detailed implementation and more results.\n\n### Relation To Broader Scientific Literature\n\nN/A\n\n### Essential References Not Discussed\n\n[1] Spatially Resolved Gene Expression Prediction from Histology Images via Bi-modal Contrastive Learning. This paper may be relevant which uses CLIP for spatial transcriptomics learning.\n\n### Other Strengths And Weaknesses\n\nWell written and easy to follow, yet the methodology contribution is limited.\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\nN/A\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces Semi-Clipped, a method that transfers morphological information from microscopy images to transcriptomic data through cross-modal knowledge distillation. The authors adapt the CLIP loss by freezing a pretrained teacher encoder (for images) and learning a trainable adapter for transcriptomics. The authors also introduce PEA (Perturbation Embedding Augmentation), a data augmentation technique based on batch correction methods to introduce biologically plausible variation into the transcriptomic profile. Experiments on multiple out-of-distribution datasets (e.g., HUVEC-KO, LINCS, and SC-RPE1) are presented. The paper claims improved performance and enhanced biological signal relative to both unimodal baselines, existing cross-modal distillation approaches and augmentation techniques.\n\n### Claims And Evidence\n\n- The claim that Semi-Clipped achieves improved cross-modal knowledge distillation is convincingly demonstrated through comprehensive experiments and comparisons with multiple competitive baseline methods (including KD, SHAKE, VICReg, and others).\n- PEA augmentation is interesting and generally improves the performance of multiple methods tested, the authors validated through statistically significant improvements.\n\n### Methods And Evaluation Criteria\n\n- Although Figure 1 has motivated Semi-Clipped over vanilla clip, the paper could benefit from comparing the methods with Clip loss in distillation benchmark in Figure 2(a). The reviewer is concerned that Semi-Cipped outperforms Clip in a context dependent way, as the scGPT + Tx Adaptor + Image Adaptor is omitted in the Figure 1.\n- There's a potential distribution shift issue since scVI was pretrained on single-cell data, while this study uses arrayed bulk sequencing data.\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nThe choice of benchmark metrics is interesting. The experimental designs and analyses appear sound.\n\n### Supplementary Material\n\nThe appendix has been reviewed; no additional supplementary materials were attached to the paper.\n\n### Relation To Broader Scientific Literature\n\nThe paper adequately discusses relevant works in the field.\n\n### Essential References Not Discussed\n\nN/A\n\n### Other Strengths And Weaknesses\n\nN/A\n\n### Other Comments Or Suggestions\n\nThe paper adequately covers relevant literature but contains numerous citation errors,  the reviewer did not do an exhaustive check but the following are obviously wrong:\n\n- Geneformer is incorrectly cited as \"Chen, T. K., Wang, Z., Li, X., Li, Y., and Huang, K. Gene-former: A foundation model for generalizable gene ex-pression learning. bioRxiv, pp. 2023.01.14.524028, 2023.\n  - instead of \"Theodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo et al. \"Transfer learning enables predictions in network biology.\" *Nature* 618, no. 7965 (2023): 616-624.\"\n- scGPT is wrongly cited as: \"Wang, Z., Song, B., Zhu, T., Li, B., Hu, Q., Tao, X., Chen, F., Wang, L., and Xie, P. scgpt: Transformer-based single-cell rna-seq data analysis. bioRxiv, pp.2023.02.24.529891, 2023.\"\n  - which should be \"Cui, Haotian, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. \"scGPT: toward building a foundation model for single-cell multi-omics using generative AI.\" Nature Methods 21, no. 8 (2024): 1470-1480.\"\n- Similar errors appear for scBERT, Drug-seq, and others\n\n### Questions For Authors\n\nCan you please share your thoughts on previous sections?\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nUnderstanding how cells respond to stimuli such as genetic perturbations or chemical compounds forms a crucial part of drug discovery. This work proposes a method to enrich representations of transcriptomic data with paired morphological data. Measuring paired transcriptomic and morphological features of cells is complex, and hence datasets of this type are rare, even when considering datasets in which these measurements are only weakly paired by sharing the same metadata attribute, such as the applied perturbagen. This necessitates a method that enriches transcriptomic representations with morphological features using only a limited number of paired measurements. This work proposes using a CLIP style loss, applied to the morphological representations from a frozen pertained model, and transcriptomic representations from a pretrained transcriptomic model with an additional trainable adapter. In doing so, this work demonstrably shows that this method, SemiClipped, allows morphological features to enrich the transcriptomic representations, and that this particular method outperforms other cross-modal distillation methods. Crucially, this allows this model to retrieve a greater number of known biological relationships from transcriptomic data, whilst also maintaining a high-level of interpretability at the level of genes. \n\nIn addition to proposing SemiClipped this work also proposes a novel augmentation method for transcriptomic data. Transformations which apply batch correction, in which samples are aligned to unperturbed control measurements, are applied to transcriptomic data to yield augmented samples, and simulate greater variance in the changes in experimental conditions under which transcriptomic data may be measured. This augmentation is shown to improve performance in biological retrieval tasks and maintains strong interpretability, a weakness of more common augmentation methods.\n\n## Update after rebuttal \nAfter reading all reviews and rebuttals, I have decided to leave my score unchanged. I welcome the additional figures and tables the authors have shared during rebuttal. To raise my score from a 4 to a 5, my assessment of the overall significance of the paper would have to be changed such that I felt this paper will have major impact on the community. This is more a question of the scope of the paper, and hence has not changed during rebuttal.\n\n### Claims And Evidence\n\nThere are three key claims made in the article, namely:\n1. that SemiClipped, the proposed method of cross-modal distillation, provides SOTA performance in the data limited regime\n2.  that by freezing the encoder of the teacher modality (in this case morphological features) they prevent drift from student to teacher. \n3. that they devise a novel biologically inspired data augmentation, PEA, that is capable of improving cross-modal distillation, and outperforms existing augmentations on benchmarks related to uncovering known biological relationships, and transcriptomic interpretability. \n\nEach of these claims are well supported by evidence. Three datasets were used for OOD evaluations, reflecting generalisation to cell types, experimental conditions and gene expression quantification technologies. Each of these datasets represents a distribution-shift that is expected between training and inference in production, hence providing a realistic evaluation of performance. The claim that SemiClipped provides SOTA performance is demonstrated throughout the paper, as it is shown to perform better than a number of strong baseline models from the literature. By comparing, in Figure 6, the biological relationships that are recalled by a unimodal pretrained transcriptomic model with those recalled by a transcriptomic model trained via cross-modal distillation, the authors demonstrate clearly that i) cross-modal distillation can allow transcriptomic models to recall biological relationships typically found in microscopy representations ii) that current methods recall more relationships than unimodal models, but can lose some relationships captured before distillation and iii) that SemiClipped, which freezes the microscopy imaging encoder, recalls the most relationships overall and from those recalled by the unimodal transcriptomic model. \n\nIn combination with Figure 1, which shows that including a trainable adapter for both the imaging and transcriptomic models leads to poorer biological recall than using a frozen imaging module and trainable adapter for transcriptomics only, this supports the claim that SemiClipped provides SOTA performance and prevents drift from student to teacher. \n\nFurthermore, Figure 2 demonstrates clearly that PEA i) provides consistent superior performance in the recall of biological relationships, and ii) in the transcriptomic interpretability of the representations for a number of benchmark augmentations in isolation and combination. The evaluation metrics for the biological recall task and transcriptomic interpretability are well explained in the Appendices. Including both of these evaluations provides a clear insight into how cross-modal distillation effects performance and interpretability.\n\n### Methods And Evaluation Criteria\n\nAs mentioned above, the work focuses on two tasks, recall of known biological relationships and transcriptomic interpretability. The biological recall task is motivated by the problem at hand - in drug discovery a model that can predict changes in biological relationships from transcriptomic data is crucial for the automation of assessing the impact of the many compounds that exist in the space of all possible small molecules. This work focusses on cross-modal distillation with microscopy imaging, which provides rich insight into cell state, but lacks gene level interpretability. This motivates the transcriptomic interpretability task. \n\nAdditionally, by including three benchmark datasets that simulate real changes that one would expect between model training and deployment (changes in cell types, experimental conditions and gene expression quantification technologies) the author’s provide a realistic measure of model performance.\n\n### Theoretical Claims\n\nThis is not applicable for this work.\n\n### Experimental Designs Or Analyses\n\nI specifically checked the experiment for which results are shown in Figure 2, since these results support the key claims of this work. By using the three OOD evaluation datasets there were no concerns of data leakage between training and inference, and made comparisons between the proposed model and baseline models fair. I therefore see no issues with the design of the experiments used to form this figure and have confidence in these results.\n\n### Supplementary Material\n\nI reviewed the appendices describing the evaluation metrics and batch correction techniques used to form PEA.\n\n### Relation To Broader Scientific Literature\n\nIt is well known that microscopy imaging can be combined with deep learning to extract useful features that describe cell phenotype, for example see [1]. However, while these models can be used to infer relationships between genes via comparing embeddings of cells perturbed by different gene KO, these models to not provide a direct transcriptomic interpretability, in the same way that a model utilising transcriptomic data would. By leveraging these modalities together, the growing power of microscopy models can be leveraged to create strong transcriptomic prediction models. \n\n\n[1] Kraus, O., et al Masked autoencoders for microscopy are scalable learners of cellular biology. In CVPR, 2024.\n\n### Essential References Not Discussed\n\nI think it is worth mentioning [1] for demonstrating that encoders trained in SSL fashion with multi-modal data can outperform unimodal encoders in the limited data regime, with imaging and tabular data (which is somewhat related to transcriptomic data).\n\n\n[1] Hager, P., Menten, M.J. and Rueckert, D., 2023. Best of both worlds: Multimodal contrastive learning with tabular and imaging data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 23924-23935).\n\n### Other Strengths And Weaknesses\n\nThis is a strong paper, with high quality authorship, clear and concise results, and well thought out experiments. The novel augmentation method for transcriptomics data could have impact alone, and it would be interesting to see how this could be adapted outside of a preclinical setting. \n\nThe main weakness of this work is the presentation of some of the results, there a few cases where figures are not well labelled. This is overcome by the clarity of the main text, but is a point that could be improved on.\n\n### Other Comments Or Suggestions\n\n- Add y-labels to Figure 1, and Figure 2. \n- Figure 3 took some time to parse, could this data be more clearly represented in a tabular format? I suppose the quantity of interest to highlight is the number of additional relationships gained via cross-modal distillation, and how many have been lost, and focussing on just these two may be enough to demonstrate the success of the method in a more immediate manner.\n\n### Questions For Authors\n\n- Can you foresee the impact of PEA beyond transcriptomics data used in a preclinical setting, where datasets do not typically have a well defined notion of unperturbed controls?\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a cross-modal knowledge distillation framework (Semi-Clipped) and a biologically inspired data augmentation method (PEA). The aim is to enhance the biological significance and predictive power of transcriptomic representations using weakly paired multimodal data (microscopy images + transcriptomics). By freezing the pre-trained morphological feature encoder and training a lightweight adapter, unidirectional knowledge transfer from morphological features to transcriptomics is achieved. Meanwhile, PEA enhances data diversity through randomized batch correction techniques, preserving biological information.\n\n### Claims And Evidence\n\n\"PEA preserves biological information\": There is a lack of direct evidence for the biological validity of the augmented data (e.g., comparison with real perturbations).\n\n### Methods And Evaluation Criteria\n\nSemi-Clipped avoids modality drift by freezing the teacher model, which is a reasonable design; PEA transforms batch correction into an augmentation strategy, demonstrating strong innovation.\n\n### Theoretical Claims\n\nThere is no theoretical proof that the optimization objective of modality alignment maximizes cross-modal information transfer.\n\n### Experimental Designs Or Analyses\n\nReasonable design:\n1. 15 random seed tests to improve statistical significance\n2. Controlled variable studies.\n\nProblems:\\\nLack of hyperparameter experiments (learning rate, batch size, temperature, etc.)\n\n### Supplementary Material\n\nThe supplementary materials include detailed experimental settings, evaluation metric calculations, batch processing descriptions, and other experimental results.\n\n### Relation To Broader Scientific Literature\n\n1. Semi-Clipped Framework and Cross-Modal Distillation Research\n\n① Adaptation of CLIP: Semi-Clipped is based on the CLIP framework but solves the problem of requiring a large amount of paired data in traditional CLIP by freezing the pre-trained encoder of the teacher modality (microscopy images). This contrasts with methods like XKD and C2KD, which require online adjustment of dual-modal encoders and are prone to modality drift with weakly paired data.\n② Advantage of Unsupervised Alignment: Compared to distillation methods like KD and SHAKE that rely on label supervision, Semi-Clipped achieves unsupervised alignment through CLIP loss, improving biological relationship recall by 23% on the HUVEC-KO dataset, validating the limitations of biological labels.\n③ Breakthrough in Single-Modality Inference: Unlike multimodal fusion methods such as VICReg and DCCA, this framework allows single-modality (transcriptomics) inference, inheriting the predictive power of the microscopy modality while maintaining the interpretability of transcriptomics.\n\n2. PEA Data Augmentation and Bioinformatics Methods\n\n① Creative Transformation of Batch Correction: Traditional batch correction techniques like TVN are redefined as random augmentation operations. Compared to conventional image augmentations (rotation, scaling), PEA introduces controlled variations while preserving biological signals, improving performance on the LINCS dataset by 69%.\n② Innovation in Biological Data Augmentation: Unlike general augmentations such as scVI denoising and MDWGAN-GP, PEA achieves a Spearman correlation of 37.56 on the single-cell dataset SC-RPE1 by randomly controlling sample sampling and reweighting PCA variance, demonstrating its adaptability to complex biological noise.\n\n3. Expansion of Multimodal Learning Paradigms\n\n① Basic Model Adaptation: Adopting the pre-trained adapter approach of Fradkin et al., but avoiding the error accumulation of bidirectional distillation through unidirectional knowledge binding, improving the biological relationship recall of the scGPT adapter by 40%.\n② Balance of Interpretability: Under the transcriptomics interpretability evaluation framework proposed by Bendidi et al., this is the first to achieve dual optimization of structural integrity (93.15) and biological discovery (39.84 recall) in cross-modal distillation, overcoming the information loss dilemma of traditional distillation methods.\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\nAdvantages:\\\nFor the first time, batch correction is restructured as data augmentation, addressing the scarcity of biological multimodal data while ensuring performance improvement with interpretability.\n\nDisadvantages:\n1. The specific definition of weakly paired data is not clearly provided. \n2. Computational costs (time complexity, space complexity) are not discussed.\n3. Hyperparameter experiments for adapter temperature, learning rate, batch size, etc., are not conducted.\n\n### Other Comments Or Suggestions\n\nThe visualization is not clear, and the y-axis of Figure 1 is not labeled.\n\n### Questions For Authors\n\n1,Biological fidelity of PEA: How can it be proven that randomized batch correction does not disrupt key biological signals? Providing an analysis of TF activity changes could strengthen the conclusion.\\\n2,Computational efficiency: What is the training time for Semi-Clipped on a dataset with 1.3 million samples? This information affects the evaluation of the method's practicality.\\\n3,Negative results disclosure: Have other modality combinations (e.g., proteomics + transcriptomics) been attempted? Discussing failed cases could enhance the rigor of the method.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Alisandra Kaye Denton",
      "Auguste Genovesio",
      "Emmanuel Noutahi",
      "Ihab Bendidi",
      "Karush Suri",
      "Kian Kenyon-Dean",
      "Yassir El Mesbahi"
    ],
    "url": "pdfs/icml.cc-2025-conference_311f107f36cddce7139ed9afc9a6495595500098.pdf",
    "remote_url": "https://openreview.net/pdf/311f107f36cddce7139ed9afc9a6495595500098.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "Bridging Protein Sequences and Microscopy Images with Unified Diffusion Models",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications->everything_else"
    ],
    "keywords": [
      "AI for biology",
      "Image generation",
      "Diffusion model",
      "MultiModal learning",
      "Cell biology"
    ],
    "abstract": "Fluorescence microscopy is ubiquitously used in cell biology research to characterize the cellular role of a protein. To help elucidate the relationship between the amino acid sequence of a protein and its cellular function, we introduce CELL-Diff, a unified diffusion model facilitating bidirectional transformations between protein sequences and their corresponding microscopy images. Utilizing reference cell morphology images and a protein sequence, CELL-Diff efficiently generates corresponding protein images. Conversely, given a protein image, the model outputs protein sequences. CELL-Diff integrates continuous and diffusion models within a unified framework and is implemented using a transformer-based network. We train CELL-Diff on the Human Protein Atlas (HPA) dataset and fine-tune it on the OpenCell dataset. Experimental results demonstrate that CELL-Diff outperforms existing methods in generating high-fidelity protein images, making it a practical tool for investigating subcellular protein localization and interactions.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper, Bridging Protein Sequences and Microscopy Images with Unified Diffusion Models, presents CELL-Diff, a novel generative model that enables bidirectional transformations between protein sequences and fluorescence microscopy images. By leveraging a transformer-based U-Net architecture and integrating both continuous and discrete diffusion processes, CELL-Diff outperforms prior methods in generating high-resolution protein images. The model is trained on the Human Protein Atlas (HPA) dataset and fine-tuned on OpenCell, demonstrating its ability to reconstruct subcellular protein localization with improved fidelity. The proposed approach has significant implications for biomedical research, particularly in protein function prediction and cellular imaging.\n\n### Claims And Evidence\n\nThe paper claims that CELL-Diff facilitates accurate bidirectional transformation between protein sequences and their corresponding microscopy images, improving upon previous methods like CELL-E and CELL-E2. Experimental results support this claim, demonstrating that CELL-Diff produces higher-resolution images with better spatial fidelity. The authors provide quantitative metrics such as Maximum Spatial Frequency (MSF) resolvability, Intersection over Union (IoU), and Frechet Inception Distance (FID), all of which indicate that CELL-Diff outperforms baselines.\n\n### Methods And Evaluation Criteria\n\nThe methodology is well-defined, employing diffusion models in both continuous (for images) and discrete (for sequences) state spaces. The evaluation framework includes comparisons with prior models (CELL-E2) using established quantitative metrics.\n\n### Theoretical Claims\n\nThe paper does not introduce new theoretical developments but builds upon existing diffusion models. The combination of continuous and discrete diffusion processes is well-motivated.\n\n### Experimental Designs Or Analyses\n\nThe experiments are well-structured, with evaluations on multiple datasets and comparisons against prior work. The use of multiple quantitative metrics strengthens the findings. However, the potential for dataset biases or domain shifts between HPA and OpenCell is not explicitly explored.\n\n### Supplementary Material\n\nNo\n\n### Relation To Broader Scientific Literature\n\nThis work aligns with research on multimodal generative modeling, fluorescence microscopy, and protein function prediction. It extends prior work on text-to-image generation by applying diffusion models to biological data. The references to related work in protein structure prediction (e.g., AlphaFold) and generative models are appropriate.\n\n### Essential References Not Discussed\n\nThe paper covers relevant prior work but does not discuss alternative generative approaches, such as GAN-based models, which have also been applied to biological image synthesis. Including a comparison with these methods could provide a broader context for CELL-Diff’s contributions.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n\n- Introduces an innovative bidirectional generative model for protein sequences and microscopy images.\n- Demonstrates significant improvements over previous methods in image quality and sequence prediction.\n- Uses well-established benchmarks and evaluation metrics.\n\nWeaknesses:\n\n- Limited discussion on potential biases in dataset selection and domain adaptation.\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\n1. How does CELL-Diff handle proteins with highly disordered or ambiguous subcellular localization?\n2. Were any domain adaptation techniques used to mitigate differences between HPA and OpenCell datasets?\n3. Could alternative generative architectures, such as GANs, be competitive with the proposed approach?\n\n### Overall Recommendation: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces CELL-Diff, a unified diffusion model that enables bidirectional generation between protein sequences and fluorescence microscopy images. It combines continuous diffusion for image synthesis and discrete diffusion for sequence prediction, integrating transformer-based cross-attention to fuse multimodal representations. The model is trained on Human Protein Atlas and OpenCell datasets, demonstrating improvements in protein localization prediction and potential applications in virtual staining and protein sequence design.\n\n## Update after rebuttal:\nI do not see sufficient justification to change my original rating. Please see my reply to rebuttal below.\n\n### Claims And Evidence\n\n- The paper claims that CELL-Diff, a unified diffusion model, can bidirectionally transform between protein sequences and their corresponding fluorescence microscopy images by integrating continuous diffusion (for images) and discrete diffusion (for sequences) within a single framework.\n\n- Quantitative metrics show better image generation than prior models. The concept of bidirectional transformation is novel in this context. Case studies suggest potential biological applications.\n\n- Unseen proteins were not rigorously tested. CELL-Diff is most likely memorizing dataset-specific patterns. The paper oversells its claims. While diffusion models are powerful, they cannot overcome the fundamental limitation of tiny training datasets in an enormous search space. This is likely more of an interpolation method rather than a true sequence-to-image generative model.\n\n### Methods And Evaluation Criteria\n\n- CELL-Diff conditions image generation on reference cell morphology images (e.g., nucleus, ER, microtubules). Uses latent diffusion models to process microscopy images efficiently. The training objective combines: Noise prediction loss for continuous diffusion (image generation). Masked value prediction loss for discrete diffusion (sequence prediction). \n\n- Method uses several metrics that measures image clarity and fine structural details (MSF), assesses how well the generated protein images align with ground truth (IoU), evaluates similarity between generated and real images (Frechet Inception Distance)\n\n### Theoretical Claims\n\n- The paper does not have any strong theoretical claims. Section 3.1 presents the standard formulation of continuous diffusion models. Section 3.2 discusses OA-ARDM for discrete data (Hoogeboom et al. 2022). Uses random ordering to make the model agnostic to token positions. \n- Section 4 of the paper introduces the CELL-Diff model and describes how it integrates continuous and discrete diffusion models for bidirectional transformation between protein sequences and microscopy images. The authors claim to integrate continuous diffusion and discrete diffusion within a single framework. Continuous diffusion follows standard diffusion formulations. Discrete diffusion follows OA-ARDM (Hoogeboom et al., 2022). Not a novel theoretical contribution. The loss formulations are also standard for diffusion models and masked language modeling.\n\n### Experimental Designs Or Analyses\n\n- The authors compare CELL-Diff to CELL-E2, a previous protein-to-image generation model. Cell-Diff improves image clarity over CELL-E2. Although visual inspection shows more detailed subcellular structures, none of the metrics used for comparison would necessarily correlate with biological accuracy. \n- The image generation results seem promising within the dataset, but there is no proof of generalization to novel proteins. There is no held-out test set of completely unseen protein families, making it impossible to evaluate generalization. The sequence prediction claim is not validated by experts or in wet-lab studies. The methodological contribution (unified diffusion) is not rigorously tested. Molecular interaction predictions could be misleading. If two proteins often appear together in the dataset, the model might just learn their cooccurrence rather than discovering true interactions.\n- While ambitious in scope, this paper attempts to tackle an inherently intractable problem without the necessary scale of data, established benchmarks, or biological validation, making its claims more speculative than substantive.\n\n### Supplementary Material\n\nSupplementary material shows additional images and generated sequences.\n\n### Relation To Broader Scientific Literature\n\n- The paper positions itself at the intersection of protein sequence modeling, fluorescence microscopy, and generative AI. Although the scope might be slightly different, there are many methods that use diffusion to predict structure from sequence (AlphaFold3, RoseTTAFold, Baek et al. 2021, RFDiffusion Waston et al., 2023, FrameDiff Wu et al. 2023). RFdiffusion and FrameDiff use diffusion models, but they generate structured, atomic-level representations, whereas CELL-Diff tries to map sequences directly to microscopy images. There are also diffusion-based generative models for de novo protein sequence design.\n\n- While ambitious, structure prediction and protein design have strong theoretical foundations and are being tackled by leading labs with rigorous validation, making them credible scientific pursuits. In contrast, sequence-to-microscopy image generation lacks structural constraints, relies on limited data, and has no established evaluation metrics, making it far more speculative. Moreover, the state space of high-resolution microscopy images is vastly larger than that of 3D protein structures, further highlighting the impracticality of learning a direct mapping from sequence to image.\n\n### Essential References Not Discussed\n\nThese are not essential but literature review would be more complete with some of these diffusion-based structure prediction methods. \n\nAlphaFold3 (Abramson et al., 2024) - AlphaFold that uses diffusion\nRFdiffusion (Watson et al., 2023) – The first diffusion model for protein structure generation.\nFrameDiff (Wu et al., 2023) – Rigid-body diffusion for protein structure prediction.\nProteinSGM (Trippe et al., 2022) – Diffusion for protein sequence design.\nFoldFlow (Anand et al., 2022) – Normalizing flows for protein backbone generation.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n- The paper introduces an ambitious multimodal diffusion framework that, if validated, could open new directions in bridging protein sequences and cellular imaging\n\nWeaknesses:\n- The Training data is woefully small: The Human Protein Atlas (HPA) dataset contains around 10K proteins with fluorescence microscopy images. Even though prominent structure prediction methods also use hundreds of thousands of protein structures protein folding problem is not random meaning the number of viable structures is vastly smaller than the full combinatorial space. On the other hand microscopy images have no equivalent constraints. The mapping from sequence to fluorescence image is far less structured than sequence-to-structure. The same protein can exhibit multiple localizations depending on cell type, environment, modifications. No universal, physics-driven constraints like those in protein folding.\n- Microscopy Images Are Not Sufficient to Capture Protein Function: Protein function is not solely determined by sequence; it depends on post-translational modifications, cellular environment, and binding interactions. Even if a model memorized all available images, it would not generalize to unseen proteins effectively.\n- Bidirectional Mapping Between Sequence and Image is Ill-Defined: A single protein sequence can adopt multiple conformations and localizations depending on: Cell type, Post-translational modifications, Interaction partners. One-to-one mapping between protein sequences and fluorescence images does not exist.\n- Diffusion Models May Be Overfitting: The authors claim CELL-Diff outperforms prior methods like CELL-E2, but if the model is trained on such a small dataset, it could be: Memorizing protein localizations rather than learning meaningful structure-function relationships. Hallucinating plausible but incorrect images, which may still look visually appealing but lack biological relevance.\n- No Evidence of Generalization to Unseen Proteins: A true sequence-to-image model should be tested on novel sequences never seen in training, but the paper does not provide convincing results for this. Without proper benchmarking on completely held-out protein families, this model might just be fitting noise or dataset-specific patterns.\n\n### Other Comments Or Suggestions\n\nn/a\n\n### Questions For Authors\n\nn/a\n\n### Overall Recommendation: 1",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces CELL-Diff, a unified diffusion model designed for bidirectional transformations between protein sequences and their corresponding microscopy images.  Given cell morphology images and a protein sequence, CELL-Diff generates corresponding protein images.  Conversely, it can also output protein sequences from protein images.  CELL-Diff integrates continuous and discrete diffusion models within a unified framework and is implemented using a transformer-based network.  The model is trained on the Human Protein Atlas (HPA) dataset and fine-tuned on the Open-Cell dataset.  Experimental results demonstrate that CELL-Diff outperforms existing methods in generating high-fidelity protein images.\n\n### Claims And Evidence\n\nYes\n1.Claim: CELL-Diff facilitates bidirectional generation between protein sequences and images.    \n\n1.Evidence: The paper provides Figure 1 and discusses the model's ability to generate protein images from sequences and vice versa.  The methodology section details how the model is trained to handle both types of generation.  Experimental results in Section 5.3 and Appendix B also visually support this claim.    \n\nThe claim is supported by the presented evidence.\n\n2.Claim: CELL-Diff outperforms existing methods in generating high-fidelity protein images.    \n\n2.Evidence: The paper compares CELL-Diff with CELL-E2 and uses metrics like MSF-resolvability, IoU, and FID to demonstrate superior performance.  Visual comparisons in Figure 4 and Appendix B also highlight the improved image quality.    \n\nThe claim is well-supported by quantitative and qualitative evidence.\n\n3.Claim: CELL-Diff can be applied for virtual screening of protein localization signals, virtual staining, and protein localization signal generation.    \n\n3.Evidence: Section 6.2 details these potential applications and provides supporting figures (Figure 5, 6, and 9) and generated sequence tables.    \n\nThe applications are well-described, and the results seem promising, but further experimental validation would strengthen these claims.\n\n### Methods And Evaluation Criteria\n\nMethods: The proposed CELL-Diff method combines continuous and discrete diffusion models within a unified framework.  It employs a transformer-based U-Net architecture with cross-attention mechanisms.  The training objective function includes noise prediction loss for the continuous diffusion model and masked value prediction loss for the discrete diffusion model.  A latent diffusion model is used to reduce computational costs. \n\nThe methods are clearly described and seem appropriate for the problem. The combination of continuous and discrete diffusion, along with the transformer-based architecture, is a reasonable approach.\n\nEvaluation Criteria: The paper uses metrics:\n\n1.MSF-resolvability: This metric measures the capability to discern fine structural details in microscopy images.    \n\n2.IoU (Intersection over Union): This metric measures the similarity between two masks, used here to compare predicted and real protein image masks.    \n\n3.FID (Fréchet Inception Distance): This metric evaluates the similarity between the real and generated images regarding their feature distributions.\n\nThese evaluation criteria are appropriate for assessing the quality and accuracy of generated microscopy images. MSF-resolvability is a particularly relevant metric for this task.\n\n### Theoretical Claims\n\nThe paper includes theoretical claims related to the diffusion models and the derivation of the training objective. \n\n1.The forward process of the continuous diffusion model is defined in Equation 1, and the subsequent derivation of  q(I_t∣I_0) is standard.    \n\n2.The reverse process is defined in Equation 2, which is also a standard formulation.    \n\n3.The ELBO (Equation 3) and its simplified form (Equation 4) are correctly presented.    \n\n4.The derivation of the loss function for OA-ARDM (Equation 7) appears to be correct.    \n\n5.The conditional ELBO for continuous and discrete diffusion models (Equations 8 and 9) are derived logically from the previous equations.    \n\n6.The combined loss function (Equation 12) is a straightforward combination of the individual losses.\n\n### Experimental Designs Or Analyses\n\nThe experiments are designed to evaluate the protein image generation performance of CELL-Diff and to demonstrate its potential applications.    \n\nThe model is trained on the HPA dataset and fine-tuned on the OpenCell dataset.    \n\nThe performance is compared with CELL-E2 using MSF-resolvability, IoU, and FID metrics.    \n\nAblation studies are conducted to evaluate the effectiveness of the cross-attention mechanism.    \n\nPotential applications are demonstrated through virtual staining, virtual screening of protein localization signals, and localization signal generation.\n\n### Supplementary Material\n\nAppendix A: Implementation of Discrete Diffusion Model    \n\nAppendix B: Protein image generation (additional results)\n\n### Relation To Broader Scientific Literature\n\nThe paper builds upon previous work in the field of predicting protein properties using learning-based methods. It cites examples such as predicting protein structure, interaction partners, and subcellular localization.    \n\nIt is related to the development of generative models for designing functional proteins and drug-like molecules.    \n\nThe work focuses on the relationship between protein sequences and their cellular functions, as characterized by microscopy images, particularly fluorescence microscopy.    \n\nIt is specifically related to recent work that proposed CELL-E, a text-to-image transformer that predicts fluorescence protein images from sequence input and cell morphology condition, and its enhancement CELL-E2.\n\n### Essential References Not Discussed\n\nSome reference of Stable Diffusion\n\n### Other Strengths And Weaknesses\n\nStrengths:\n1.The paper addresses an important problem: understanding the relationship between protein sequences and their cellular functions.    \n2.The proposed model has the potential to be a valuable tool for investigating subcellular protein localization and interactions, with potential applications in drug discovery and disease research.    \n3.The paper demonstrates the model's potential through several applications, including virtual staining, virtual screening of protein localization signals, and localization signal generation.\n\nWeakness:\n1.While generally clear, some parts of the paper, especially the technical details of the model and the training process, could be challenging for readers without a strong background in machine learning and diffusion models.\n2.The datasets used in the experiments, while relevant, might be considered limited in size and diversity, which could affect the generalizability of the model.\n\n### Other Comments Or Suggestions\n\nIn the methodology section, providing more visual aids or diagrams to illustrate the diffusion processes and the network architecture could further improve clarity.\n\nIt would be beneficial to discuss the computational cost and scalability of CELL-Diff in more detail, as this is an important consideration for practical applications.\n\n### Questions For Authors\n\nThe authors mentioned that \"Some parts of the paper, especially the technical details of the model and the training process, could be challenging for readers without a strong background in machine learning and diffusion models.\" Could the authors provide more clarification or additional explanations on these technical details to make the paper more accessible to a broader audience?\n\nThe authors used the HPA and OpenCell datasets for their experiments. Given the limited size and diversity of these datasets, could the authors discuss the potential impact of this limitation on the generalizability of the model and how future work could address this?\n\n### Overall Recommendation: 4\n\n### Ethical Review Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces CELL-Diff, a diffusion-based model capable of bidirectionally generating microscopy images from protein sequences and protein sequences from microscopy images. Using conditional morphology reference images (nucleus, ER, microtubules), the model combines continuous diffusion for images and discrete diffusion for sequences. CELL-Diff significantly improves image quality compared to previous methods, evaluated on the HPA and OpenCell datasets.\n\n### Claims And Evidence\n\nThe authors claim CELL-Diff provides high-fidelity microscopy image generation and accurate sequence-to-image and image-to-sequence transformations. While the image generation performance is strongly supported through quantitative (FID, IoU, MSF metrics) and qualitative evaluations, the claim of accurate image-to-sequence transformation lacks rigorous quantitative evidence. Evidence provided for sequence generation is limited to qualitative motif analyses.\n\n### Methods And Evaluation Criteria\n\n- The methodological choice (combining continuous diffusion for images and discrete diffusion for sequences in a unified transformer-based U-Net) is interesting and sound. \n- However, the authors' evaluation of image-to-sequence generation is weak, relying primarily on qualitative assessments rather than robust quantitative analyses. The paper could benefit from a larger-scale motif analysis / cluster overlap with localization etc.\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nThe experimental design for image generation is solid with clear metrics and appropriate baselines.\n\n### Supplementary Material\n\nThe appendix has been reviewed; no additional supplementary materials were attached to the paper.\n\n### Relation To Broader Scientific Literature\n\nThe paper adequately discusses relevant works in the field.\n\n### Essential References Not Discussed\n\nN/A\n\n### Other Strengths And Weaknesses\n\nN/A\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\n- Can you please share your thoughts on previous sections?\n- How deterministic or diverse are the sequences generated from a given microscopy image?\n- Is there any cycle consistency between the two directions? Say for a protein sequence, generate an image, and then feed that generated image back into the model to generate a sequence, do you get back similar sequence?\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Bo Huang",
      "Dihan Zheng"
    ],
    "url": "pdfs/icml.cc-2025-conference_f447029f83b094b6b90f0c5043453e975c659aa2.pdf",
    "remote_url": "https://openreview.net/pdf/f447029f83b094b6b90f0c5043453e975c659aa2.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "\"Why Is There a Tumor?\": Tell Me the Reason, Show Me the Evidence",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications->health_medicine"
    ],
    "keywords": [
      "Trustworthy Medical Image Analysis"
    ],
    "abstract": "Medical AI models excel at tumor detection and segmentation. However, their latent representations often lack explicit ties to clinical semantics, producing outputs less trusted in clinical practice. Most of the existing models generate either segmentation masks/labels (localizing where without why) or textual justifications (explaining why without where), failing to ground clinical concepts in spatially localized evidence. To bridge this gap, we propose to develop models that can justify the segmentation or detection using clinically relevant terms and point to visual evidence. We address two core challenges: First, we curate a rationale dataset to tackle the lack of paired images, annotations, and textual rationales for training. The dataset includes 180K image-mask-rationale triples with quality evaluated by expert radiologists. Second, we design rationale-informed optimization that disentangles and localizes fine-grained clinical concepts in a self-supervised manner without requiring pixel-level concept annotations. Experiments across medical benchmarks show our model demonstrates superior performance in segmentation, detection, and beyond. The anonymous link to our code.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper addresses the challenge of enhancing interpretability in medical AI models for tumor detection and segmentation. The authors propose a novel framework that generates predictions supported by both clinical concepts and visual evidence. To achieve this, they curate a “first-of-its-kind” dataset (will make the dataset publicly available.) containing 180K image-mask-rationale triples, where rationales are high-quality textual justifications for clinical assessments. Additionally, they introduce a rationale-informed optimization method that disentangles and localizes fine-grained clinical concepts without requiring pixel-level annotations. Experiments on multiple medical benchmarks demonstrate superior performance in segmentation, detection, and rationale correctness compared to state-of-the-art models.\n\n### Claims And Evidence\n\nThe authors' experiments (both internal and external dataset) can support their claims. However, one weakness is that while the authors' method is claimed to be generalizable across different types of tumors, it has only been validated on prostate cancer, lacking verification on other types of tumors.\n\n### Methods And Evaluation Criteria\n\nIs the formulation of PDT (PI-RADS Decision Tree) based on clinical standards, rather than merely “developed with radiologist alignment”. That is, whether there are clinical standards as a basis rather than the radiologist 's subjective opinion.\n\n### Theoretical Claims\n\nThe proofs in the paper are validated through experiments rather than relying on theoretical demonstrations. However, the explanations of the optimization formulas should be made clearer.  Specifically, the meanings of \\( c^{'}_{k} \\) and ϵ1 and ϵ2 are not clearly defined in Equations (3) and (4). Additionally, a more detailed Loss and clear explanation of the formulas (3) and (4) should be provided.\"\nIn the part of Localization constraint, provide examples to explain what is meant “Our idea is that different concepts describing the same anatomical structure”\n\n### Experimental Designs Or Analyses\n\n1. The authors should analyze failed cases in the segmentation examples (including true positives and false positives) along with their corresponding explanatory rationales. It is important to investigate whether incorrect segmentations lead to errors in the rationales and to evaluate the consistency between segmentation results and rationales. Such an analysis of failure cases would be more clinically meaningful in assisting doctors to assess the model's predictions.\n2. The authors have only validated the experiments on the MedSAM backbone. As a general method, they should validate the universality of the proposed approach on different segmentation backbones (SwinUNETR, UNet, nnUNet).\n\n### Supplementary Material\n\nThe supplementary material is thorough, explaining the details of the dataset and the settings of the supplementary experiments.\n\n### Relation To Broader Scientific Literature\n\nI find this to be an interesting work that addresses the limitations of traditional methods (e.g. GradCAM) in terms of medical AI interpretability. Additionally, the approach used for constructing the dataset is also noteworthy and provides valuable reference for future research.\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\nEven though the author designed Human evaluation to assess the quality of rationales, it is insufficient with only two readers. Besides the reader study, I think the author should design an automated quantitative assessment of the rationales' quality to evaluate the accuracy of the tumor diganosis.\n\n### Other Comments Or Suggestions\n\nIf the author can conduct verification on multiple tumors and multiple backbones (such as the more mainstream nnUNet), it would be more convincing.\n\n### Questions For Authors\n\n1.\tThe authors' experiments (both internal and external dataset) can support their claims. However, one weakness is that while the authors' method is claimed to be generalizable across different types of tumors, it has only been validated on prostate cancer, lacking verification on other types of tumors.\n2.\tIs the formulation of PDT (PI-RADS Decision Tree) based on clinical standards, rather than merely “developed with radiologist alignment”. That is, whether there are clinical standards as a basis rather than the radiologist 's subjective opinion.\n3.\tThe meanings of $ c^{'}_{k} $ and ϵ1 and ϵ2 are not clearly defined in Equations (3) and (4). Additionally, a more detailed Loss and clear explanation of the formulas (3) and (4) should be provided.\"\n4.\tIn the part of Localization constraint, provide examples to explain what is meant “Our idea is that different concepts describing the same anatomical structure”\n5.\t. The authors should analyze failed cases in the segmentation examples (including true positives and false positives) along with their corresponding explanatory rationales. It is important to investigate whether incorrect segmentations lead to errors in the rationales and to evaluate the consistency between segmentation results and rationales. Such an analysis of failure cases would be more clinically meaningful in assisting doctors to assess the model's predictions.\n6.\tThe authors have only validated the experiments on the MedSAM backbone. As a general method, they should validate the universality of the proposed approach on different segmentation backbones (SwinUNETR, UNet, nnUNet).\n\n### Overall Recommendation: 3\n\n### Ethical Review Concerns\n\nNo",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis work addresses a highly interesting topic by formulating the task of tumor localization, which involves explaining and identifying tumor regions in medical data. The authors construct a dedicated dataset for this task and establish methods to quantify both the performance and explainability of their approach.\n\n### Claims And Evidence\n\nyes\n\n### Methods And Evaluation Criteria\n\nyes\n\n### Theoretical Claims\n\nthis is not a theory work.\n\n### Experimental Designs Or Analyses\n\nyes\n\n### Supplementary Material\n\nyes\n\n### Relation To Broader Scientific Literature\n\nThe limits of fair medical imaging AI in real-world generalization. Nature Med\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\n#### **Strengths**\n1. **Novel Framework for Tumor Localization**  \n   - The authors propose a novel framework to analyze the reasoning behind tumor localization tasks, which is an innovative and underexplored area in medical imaging research.\n\n2. **Dataset Contribution**  \n   - The introduction of a new dataset specifically designed for this task is a significant contribution. If the authors release this dataset publicly, it could greatly benefit the research community and foster further advancements in the field.\n\n3. **Quantifying Explainability**  \n   - This work is the first to quantify the rationality and explainability of tumor localization, providing a systematic way to evaluate both performance and interpretability, which are critical in medical applications.\n\n#### **Weaknesses**\n1. **Generalizability to Other Modalities or Body Parts**  \n   - Since all experiments are conducted on Prostate MRI scans, it raises concerns about the generalizability of the proposed method. Can this framework be extended to other modalities (e.g., CT, X-ray) or different body parts? The authors should clarify why they focused solely on Prostate MRI and discuss the potential limitations or adaptations required for broader applicability.\n\n### Other Comments Or Suggestions\n\nplease see the weakness part.\n\n### Questions For Authors\n\nplease see the weakness part.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nMedical AI models effectively detect and segment tumors but often fail to provide explicit clinical reasoning, making their outputs less trustworthy in practice. Existing methods either localize abnormalities without justification or generate textual explanations without spatial grounding. To bridge this gap, the authors curate a dataset of 180K image-mask-rationale triples, verified by expert radiologists, and develop a self-supervised model that disentangles and localizes fine-grained clinical concepts without requiring pixel-level annotations.\n\n### Claims And Evidence\n\nThe authors mostly backed up their claims with evaluations. However, some claims lack explicit evidence, as detailed in the questions below.\n\n### Methods And Evaluation Criteria\n\nThe methods and evaluation criteria are clearly presented and justified.\n\n### Theoretical Claims\n\nThe theoretical claims have been checked and appear to be correct.\n\n### Experimental Designs Or Analyses\n\nSoundness/validity of any experimental designs or analyses are checked.\n\n### Supplementary Material\n\nNo.\n\n### Relation To Broader Scientific Literature\n\nThe topic is important, and the paper makes a valuable contribution to explainable AI in medical imaging.\n\n### Essential References Not Discussed\n\nThe paper does not discuss relevant works such as: PadChest (2020): A large-scale dataset of over 160,000 Spanish chest X-rays with radiology reports and multi-label annotations. The recent PadChest-GR (2024) extension provides 4,555 images with grounded reports linking findings to bounding boxes, which directly relates to the authors' goal of grounding clinical concepts in images.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n- The curated rationale dataset is a significant contribution to improving model explainability.\n- The proposed self-supervised optimization method is innovative and addresses the need for fine-grained concept localization.\n- The model demonstrates superior performance in segmentation and detection tasks.\n\nWeaknesses:\n- The disentanglement constraint assumes strict spatial separation of clinical concepts, which may not always hold due to overlapping or co-existing conditions.\n- Lack of discussion on how to handle diffuse diseases that affect the entire image.\n- Some experimental results require further clarification (see questions below).\n\n### Other Comments Or Suggestions\n\nThere is a small typo in line 220: \"resample(.)\"\n\n### Questions For Authors\n\n- Loss Function Weighting: Do the Dice loss and InfoNCE loss have the same weight in Eq. 2? If so, can you elaborate on this decision?\n- Disentanglement Constraint: The claim \"clinically different concepts should highlight different regions in the image\" does not always hold. Some diseases affect multiple regions or share visual features (e.g., pulmonary edema and pneumonia in X-rays), while others are diffuse. The localization constraint may address the first case, but how do you handle the second? Can you clarify how the model adapts to conditions requiring multiple highlighted areas?\n- Comprehensiveness Score: Why is comprehensiveness scored lower than other criteria in Section 5.2?\n- Table 2 - Second Average Column: What does the second \"average\" column in Table 2 represent? If it averages precision and AUROC, what is the motivation for this, given that they are different metrics?\n- Backbone Choice: Why was MedSAM chosen as the backbone instead of U-Net? Have you compared performance between the two architectures?\n- MedSAM Comparison: Is there a direct comparison between MedSAM and your rationale dataset? If so, what benefits does the rationale dataset provide in conjunction with MedSAM?\n- Gaze Information: In similar works using different imaging modalities, gaze tracking has been incorporated for model explainability. Could gaze data be integrated into your approach to further enhance grounding?\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Binsheng Zhao",
      "Lu Lin",
      "Mengmeng Ma",
      "Oguz Akin",
      "Tang Li",
      "Volkan Beylergil",
      "Xi Peng",
      "Yunxiang Peng"
    ],
    "url": "pdfs/icml.cc-2025-conference_b7757095906dbc4f6d027bacad170dfab3434f95.pdf",
    "remote_url": "https://openreview.net/pdf/b7757095906dbc4f6d027bacad170dfab3434f95.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "Screener: Self-supervised Pathology Segmentation Model for 3D Medical Images",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications->health_medicine"
    ],
    "keywords": [
      "Unsupervised Visual Anomaly Segmentation",
      "Self-supervised learning",
      "Density estimation",
      "Computed Tomography"
    ],
    "abstract": "Accurate segmentation of all pathological findings in 3D medical images remains a significant challenge, as supervised models are limited to detecting only the few pathology classes annotated in existing datasets. To address this, we frame pathology segmentation as an unsupervised visual anomaly segmentation (UVAS) problem, leveraging the inherent rarity of pathological patterns compared to healthy ones. We enhance the existing density-based UVAS framework with two key innovations: (1) dense self-supervised learning (SSL) for feature extraction, eliminating the need for supervised pre-training, and (2) learned, masking-invariant dense features as conditioning variables, replacing hand-crafted positional encodings. Trained on over 30,000 unlabeled 3D CT volumes, our model, Screener, outperforms existing UVAS methods on four large-scale test datasets comprising 1,820 scans with diverse pathologies. Code and pre-trained models will be made publicly available.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper presents Screener, a framework based on unsupervised visual anomaly segmentation (UVAS) for 3D medical scans. The proposed model aims to reduce the dependency on ground truth (GT) annotations. It is trained on a large dataset of 30K CT scans and evaluated on 1.8K scans, covering a variety of pathological segmentations. The authors claim that the model effectively addresses the challenges of anomaly segmentation without requiring manual annotations.\n\n### Claims And Evidence\n\nThe authors claim that their method is designed for 3D medical image segmentation. However, this claim appears overstated based on the current evaluation. The framework has only been tested on CT scans, without validation on other key modalities such as MRI. Furthermore, while the authors repeatedly reference segmentation, the model has not been explicitly tested on segmentation tasks, nor are segmentation-specific metrics reported. This omission weakens the claim that the method is applicable to segmentation.\n\n### Methods And Evaluation Criteria\n\nThe proposed evaluation methodology is incomplete. To substantiate the generalizability of the approach, the method should be evaluated on additional imaging modalities, such as MRI, to ensure its robustness across different medical datasets. Furthermore, the segmentation task itself is not explicitly evaluated, which is necessary given the claims in the title and introduction.\nAdditionally, Table 2 lacks a sufficient number of comparative baselines, limiting the ability to assess the proposed method's relative performance.\n\n### Theoretical Claims\n\nI reviewed the theoretical claims presented in the paper, and they appear to be correct.\n\n### Experimental Designs Or Analyses\n\nWhile the experiments are generally well-executed, there are critical gaps in evaluation:\n\n1- Lack of Segmentation Task Evaluation: Given the frequent references to segmentation, the paper should explicitly evaluate segmentation performance and report relevant metrics (e.g., Dice score, IoU).\n \n2- Need for More Comprehensive Analysis: Additional metrics and statistical tests should be included to validate the significance of the results.\n\n3- Lack of Robustness Testing: The method is tested on high-quality CT scans, but its performance on noisy or degraded scans remains unclear. Evaluating robustness to noise, artifacts, and variations in acquisition settings would strengthen the study.\n\n### Supplementary Material\n\nI reviewed the supplementary material.\n\n### Relation To Broader Scientific Literature\n\nThe paper builds upon prior work in visual anomaly segmentation (UVAS) for 3D CT scans. The authors review density-based approaches and propose leveraging dense self-supervised learning (SSL) techniques to pre-train feature maps, which are then used in a density-based UVAS framework. This approach is well motivated and aligns with recent trends in self-supervised representation learning for medical imaging.\n\n### Essential References Not Discussed\n\nThe paper is missing references to key related works that are essential for contextualizing its contributions. For instance, the following papers could be discussed:\n\nVISA-FSS: A volume-informed self-supervised approach for few-shot 3D segmentation, MICCAI 2023.\nTransformer-based models for unsupervised anomaly segmentation in brain MR images, MICCAI Workshop 2022.\n\nThese studies provide valuable insights into self-supervised learning for medical image segmentation and anomaly detection, which are directly relevant to the proposed method.\n\n### Other Strengths And Weaknesses\n\nThe paper tackles an important problem and introduces an interesting approach. However, several issues need to be addressed:\n\n1- Limited Scope of Evaluation: The model is tested exclusively on CT scans, and there is no exploration of other medical imaging modalities (e.g., MRI).\n\n2- Potential Bias in Dataset: The qualitative results (Fig. 1) suggest that the scans used are high-quality, but robustness to noisy or low-quality scans is not evaluated.\n\n3- Lack of Comparison with Fully Supervised Methods: The method should be compared with fully supervised segmentation models (e.g., UNet) to assess its performance in a more practical clinical setting.\n\n### Other Comments Or Suggestions\n\n- Expand the Literature Review: The authors should discuss existing methods that incorporate registration-based approaches for segmentation, as well as the strengths and weaknesses of UVAS compared to other self-supervised pretraining techniques.\n\n- Clarify Key Claims: The paper should explicitly differentiate between anomaly detection and segmentation to avoid overstating its contributions.\n\n### Questions For Authors\n\n1- How does the proposed method generalize to other medical imaging modalities, such as MRI?\n\n2- Since segmentation is frequently mentioned in the paper, why is segmentation performance not explicitly evaluated with standard metrics?\n\n3- How does the proposed model compare to fully supervised segmentation methods, such as UNet or nnU-Net?\n\n4- What is the method’s robustness to low-quality scans, noise, and artifacts?\n\n### Overall Recommendation: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposed Screener, a self-supervised anomaly segmentation framework for volumetric CT images. \nThe Screener was built upon dense self-supervised learning and a density-based anomaly segmentation framework. Specifically, it utilizes dense pixel-wise self-supervised learning (i.e., VICReg) to pretrain two encoders serving as descriptor and condition models. A density model (Gaussian or normalizing flow) takes the joint embedding to estimate and assign pixel-wise anomaly scores. The model was pretrained on large-scale 30k CT scans and was evaluated on four different CT datasets and outperformed other baseline methods.\n\n### Claims And Evidence\n\nThe claims of contribution are mostly supported by convincing evidence. However, the value of conditioning variables is arguable as it does not affect the performance when using normalization flow as the density model.\n\n### Methods And Evaluation Criteria\n\nThe methods and evaluation criteria make sense.\n\n### Theoretical Claims\n\nThere is no proof of any theoretical claims.\n\n### Experimental Designs Or Analyses\n\nThe reviewer found the experimental designs not comprehensive enough. Although the authors listed a few representative methods from different perspectives (i.e., synthetic anomalies, recon-based, density-based for nature image, and domain-specific medical unsupervised anomaly localization), the current manuscript misses a few of the most recent studies [1-5]. For example, the f-AnoGAN, a 2019 baseline, is the only one specifically designed for medical images in experiments. This incomplete baseline comparison weakens the convincingness of the results. \n\n[1]: Pinaya, Walter HL, et al. \"Unsupervised brain imaging 3D anomaly detection and segmentation with transformers.\" Medical Image Analysis 79 (2022): 102475.\n\n[2]: Liu, Zhikang, et al. \"Simplenet: A simple network for image anomaly detection and localization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[3]: Iqbal, Hasan, et al. \"Unsupervised anomaly detection in medical images using masked diffusion model.\" International Workshop on Machine Learning in Medical Imaging. Cham: Springer Nature Switzerland, 2023.\n\n[4]: Zhao, Yuzhong, Qiaoqiao Ding, and Xiaoqun Zhang. \"AE-FLOW: Autoencoders with normalizing flows for medical images anomaly detection.\" The Eleventh International Conference on Learning Representations. 2023.\n\n[5]: Zou, Yang, et al. \"Spot-the-difference self-supervised pre-training for anomaly detection and segmentation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n### Supplementary Material\n\nYes, the reviewer has gone through the supplementary material.\n\n### Relation To Broader Scientific Literature\n\nThis paper explores using self-supervised learning to enhance the density-based anomaly segmentation framework, focusing on medical images. It is related to previous studies focusing on 3D medical anomaly segmentation, density-based anomaly segmentation, and self-supervised learning for anomaly segmentation.\n\n### Essential References Not Discussed\n\nRecent medical anomaly segmentation studies [1,3, 4] focus on the same problem (medical anomaly segmentation), and are more recent developments compared to f-AnoGAN discussed in the manuscript. [4] also utilizes normalizing flow, the same as the proposed method. \n\n[2, 5] are for the natural images but represent the most recent development. [5] also explore utilizing self-supervised pretraining for anomaly segmentation.\n\n[1]: Pinaya, Walter HL, et al. \"Unsupervised brain imaging 3D anomaly detection and segmentation with transformers.\" Medical Image Analysis 79 (2022): 102475.\n\n[2]: Liu, Zhikang, et al. \"Simplenet: A simple network for image anomaly detection and localization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[3]: Iqbal, Hasan, et al. \"Unsupervised anomaly detection in medical images using masked diffusion model.\" International Workshop on Machine Learning in Medical Imaging. Cham: Springer Nature Switzerland, 2023.\n\n[4]: Zhao, Yuzhong, Qiaoqiao Ding, and Xiaoqun Zhang. \"AE-FLOW: Autoencoders with normalizing flows for medical images anomaly detection.\" The Eleventh International Conference on Learning Representations. 2023.\n\n[5]: Zou, Yang, et al. \"Spot-the-difference self-supervised pre-training for anomaly detection and segmentation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n### Other Strengths And Weaknesses\n\nThe overall results look superior to the listed baseline methods, although a few more recent and relevant studies are not included in the experiments. \n\nThe presentation could be improved and the logical flow can be optimized (e.g., move related work to an earlier position).\n\n### Other Comments Or Suggestions\n\n1. The visualization of Figure 3 should be corrected. The color bar for other methods looks very strange; for example, MSFlow has a color bar from 0 to 8000.\n\n### Questions For Authors\n\nWhy not consider other self-supervised pretext tasks besides SimCLR and VICReg, such as masked autoencoding?\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors introduce Screener, a self-supervised 3D pathology segmentation model that formulates the task as an unsupervised visual anomaly segmentation (UVAS) problem. It utilizes self-supervised feature learning and a masking-invariant condition model within a density-based UVAS framework. Trained on 30,000+ unlabeled CT scans, Screener is evaluated on 1,820 test scans across four datasets, achieving AUROC up to 0.96. The study conducts a large-scale evaluation of UVAS for 3D CT images and explores self-supervised learning for medical image pathology segmentation.\n\n### Claims And Evidence\n\nThe paper provides quantitative and qualitative evidence to support its claims through experiments, ablation studies, and comparisons with multiple baseline methods.\n\n### Methods And Evaluation Criteria\n\nThe evaluation criteria are generally appropriate but warrant some scrutiny:\n\n- AUROC and AUPRO are standard for anomaly detection tasks, making them fitting choices given the UVAS framing. They effectively evaluate the model’s ability to detect rare pathological pixels, which aligns with the problem’s focus on identifying deviations from normal tissue. However, traditional segmentation metrics like the Dice coefficient or Jaccard index, which measure overlap between predicted and ground truth segments, are more common in clinical segmentation tasks. These metrics provide direct interpretability for clinicians (e.g., “How much of the tumor was correctly segmented?”), which AUROC and AUPRO do not. Including both anomaly detection and segmentation metrics would offer a more holistic assessment.\n\n- Outperforming existing UVAS methods demonstrates the effectiveness of Screener’s innovations. However, a comparison with supervised methods (where labeled data is available) would contextualize the performance gap, providing insight into how close the self-supervised approach comes to fully supervised benchmarks—a relevant consideration for clinical adoption.\n\n### Theoretical Claims\n\nNA\n\n### Experimental Designs Or Analyses\n\nIt is better to provide some supervised baseline for reference—comparison with a fully supervised segmentation model could help contextualize how much performance is lost by using UVAS.\n\n### Supplementary Material\n\nYes. Most of the parts.\n\n### Relation To Broader Scientific Literature\n\nThe key contributions of the paper build on and extend several existing approaches in self-supervised learning, anomaly detection, and medical image segmentation.\n\n- Traditional supervised segmentation models rely on large labeled datasets (e.g., UNet, Ronneberger et al., 2015), which are scarce for medical imaging.\n\n- Self-supervised learning (SSL) has been successfully applied to natural images (e.g., SimCLR, VICReg), but its application to 3D CT medical images is less explored. This work uses dense self-supervised learning for 3D CT volumes.\n\n### Essential References Not Discussed\n\nThe authors may refer more to self-supervised medical image segmentation, e.g.,[1] and [2].  \n\n\n[1] Tang, Yucheng, et al. \"Self-supervised pre-training of swin transformers for 3d medical image analysis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[2] Zhou, Hong-Yu, et al. \"A unified visual information preservation framework for self-supervised pre-training in medical image analysis.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 45.7 (2023): 8020-8035.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n- The paper extends density-based unsupervised visual anomaly segmentation (UVAS) to 3D CT pathology segmentation, an area with limited prior work.\n\n- Trains on 30,000+ unlabeled CT scans and evaluates on 1,820 labeled scans from four pathology datasets, providing strong empirical validation.\n\nWeaknesses:\n- While the paper compares against unsupervised baselines, it does not include a fully supervised segmentation model (e.g., UNet trained on labeled data). This makes it difficult to quantify how much performance loss occurs due to using UVAS instead of a supervised approach.\n\n- The model is trained on high-resolution 3D CT scans, but the paper does not discuss computational cost or inference speed, which are critical for clinical deployment.\n\n- The authors may try to improve the writing of this paper, especially for the method sections (e.g., notations and clarity of descriptions of concepts).\n\n### Other Comments Or Suggestions\n\nNA\n\n### Questions For Authors\n\nPlease try to address the weaknesses.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this paper, an unsupervised visual anomaly detection algorithm is proposed, which the authors describe as a segmentation algorithm, although I disagree with this characterization. The method exploits the inherent rarity of pathological patterns compared to healthy ones. Two different self-supervised learning strategies are employed to train a descriptor and a condition model. The outputs of these models are then used to train a density model that generates voxel-wise anomaly scores. The model, trained on over 30,000 unlabeled 3D CT volumes, appears to outperform existing methods on four test datasets, comprising 1,820 scans with diverse pathologies.\n\n### Claims And Evidence\n\nThe authors claim to introduce a pathology segmentation algorithm designed for accurate segmentation of all pathological findings in 3D medical images, with the ability to handle pathology classes beyond those in the training datasets. They also claim to reframe pathology segmentation as an unsupervised visual anomaly segmentation problem. However, I believe it is inaccurate to describe the algorithm as a segmentation model, and there is no actual reframing—the algorithm is designed for anomaly detection. Additionally, while I noticed that disjoint data are used for training and testing, this doesn't mean novel pathology classes exist in the testing data and the first claim is well justified. Do testing images include novel pathology classes not present in the training data?\n\n### Methods And Evaluation Criteria\n\nYes\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nMy concerns are: 1) Whether the claim that the proposed method can be used for novel anomaly detection is justified, and 2) that the algorithm is for anomaly detection rather than segmentation. For an anomaly detection algorithm evaluation, the experimental design and evaluation metric seem okay.\n\n### Supplementary Material\n\nYes. I reviewed all four sections in SM.\n\n### Relation To Broader Scientific Literature\n\nThis paper is closely related to anomaly detection algorithms like DRAEM in both computer vision and medical imaging domain.\n\n### Essential References Not Discussed\n\nSome recent related works, such as [1], are not mentioned or compared. It remains unclear whether the proposed method outperforms them and to what extent improvement is achieved. \n\n[1] Huang, Chaoqin, et al. \"Adapting visual-language models for generalizable anomaly detection in medical images.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n### Other Strengths And Weaknesses\n\nThe idea is interesting and the results seem promising. However, I have the following two concerns that need to be addressed:\n\n1. It is unclear what assumptions are made about the training data but not explicitly stated. Does the training data need to be dominated by images without pathology?\n\n2. It is unclear how different the features extracted by the descriptor and condition models are and if different, whether one can be inferred from the other, which is crucial for training the density model. My concern is that if the input feature pairs are significantly different, how can one be reliably inferred from the other? Conversely, if they are too similar, the output of density model would be less meaningful.\n\n### Other Comments Or Suggestions\n\n1. I believe the proposed method can only be considered a segmentation method if its final output is a segmentation mask. I encourage reconsidering the use of terms like ‘segment’ and ‘segmentation’ throughout the paper. \n\n2. I suggest analyzing the differences between the outputs of the descriptor and condition models and providing more insights and visualizations to help readers better understand the rationale.\n\n### Questions For Authors\n\n1. Are there any assumptions about the training data? Would the method still work with a dataset consisting entirely of images with pathology?\n\n2. It is mentioned that \"the descriptor model must generate descriptors that effectively differentiate between pathological and normal positions.\" However, how this is guaranteed with the adopted training strategy?\n\n3. How different are the features extracted by the descriptor and condition models? How reliably can the feature from the condition model be used to infer the feature from the descriptor model? Have you compared the inferred features with the extracted ones? My concern is that if the input feature pairs are significantly different, how can one be reliably inferred from the other? Conversely, if they are too similar, the density model’s output may be less meaningful, calling into question the rationale of the proposed method.\n\n4. How did you determine the patch size for training? Would a smaller or larger patch size work as well? Specifically, I am interested in how the patch size affects the differences between the features extracted by the descriptor and condition models.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Eugenia Soboleva",
      "Ivan Oseledets",
      "Mariia Donskova",
      "Marina Munkhoeva",
      "Maxim Panov",
      "Mikhail Goncharov"
    ],
    "url": "pdfs/icml.cc-2025-conference_dcbea3b1a9451162f82f1af4575308a68db2f808.pdf",
    "remote_url": "https://openreview.net/pdf/dcbea3b1a9451162f82f1af4575308a68db2f808.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "Introducing 3D Representation for Dense Volume-to-Volume Translation via Score Fusion",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "deep_learning->generative_models_and_autoencoders"
    ],
    "keywords": [
      "Diffusion models",
      "3D medical image generation",
      "video generation"
    ],
    "abstract": "In volume-to-volume translations in medical images, existing models often struggle to capture the inherent volumetric distribution using 3D voxel-space representations, due to high computational dataset demands. We present Score-Fusion, a novel volumetric translation model that effectively learns 3D representations by ensembling perpendicularly trained 2D diffusion models in score function space. By carefully initializing our model to start with an average of 2D models as in existing models, we reduce 3D training to a fine-tuning process, mitigating computational and data demands. Furthermore, we explicitly design the 3D model's hierarchical layers to learn ensembles of 2D features, further enhancing efficiency and performance. Moreover, Score-Fusion naturally extends to multi-modality settings by fusing diffusion models conditioned on different inputs for flexible, accurate integration. We demonstrate that 3D representation is essential for better performance in downstream recognition tasks, such as tumor segmentation, where most segmentation models are based on 3D representation. Extensive experiments demonstrate that Score-Fusion achieves superior accuracy and volumetric fidelity in 3D medical image super-resolution and modality translation. Additionally, we extend Score-Fusion to video super-resolution by integrating 2D diffusion models on time-space slices with a spatial-temporal video diffusion backbone, highlighting its potential for general-purpose volume translation and providing broader insight into learning-based approaches for score function fusion.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes to improve 3D representation learning of medical image volumes in diffusion models. Unlike earlier methods that ensemble 2D models by averaging their weights, the proposed method does “fusion in score function space”. This is shown to improve. They show improved performance in downstream tasks such as segmentation. Additionally, “ensembling in score function space” is also shown to result in feasible training of large 3D Diffusion Models by initializing using pretrained 2D models and then fine-tuning the 3D model. The synthetic generation quality is also shown to be better both via qualitative examples and quantitative image quality metrics.\n\n### Claims And Evidence\n\nYes, compared to multiple baselines, improved representation learning and subsequently improvement in downstream tumor segmentation task is shown.\n\n### Methods And Evaluation Criteria\n\nYes\n\n### Theoretical Claims\n\nn/a\n\n### Experimental Designs Or Analyses\n\nYes, the experiment design and analysis looks sound.\n\n### Supplementary Material\n\nn/a\n\n### Relation To Broader Scientific Literature\n\nThe paper seems to have gone a step further in merging diffusion models to obtain a better 3D representation learning from pretrained 2D models. Whereas earlier methods simplify the problem of learning 3D data distribution by simplifying into product of 2D distributions, the proposed method ensembles the estimations from the 2D models to obtain weight vector and residual terms as learnable parameters for fine tuning.\n\n### Essential References Not Discussed\n\nn/a\n\n### Other Strengths And Weaknesses\n\nThe paper is very well written and easy to follow with good synthesis of literature.  \n\nThe contribution is well motivated and shows good synthesis performance and improvement on downstream tasks. The possibility to train a full capacity 3D diffusion models without having to resort to compromises such as diffusion in latent space rather than full 3D volume space is very enticing. \n\n \n\nIn the downstream BraTS tumor segmentation task, it is advisable to report challenge-specific metrics such as Lesion-wise Dice Score (rather than Dice Score over the entire image) that penalizes not only overlap but also heavily penalizes missing lesions. Additionally, 95% Haussdorff Distance may also be reported. \n\nAdditional, downstream tasks such as sparse-view reconstruction and other volume-to-volume translation tasks could have been added to substantially add weight to the claim of better representation learning.\n\n### Other Comments Or Suggestions\n\nn/a\n\n### Questions For Authors\n\nAgain, to reiterate, Additional, downstream tasks such as sparse-view reconstruction and other volume-to-volume translation tasks could have been added to substantially add weight to the claim of better representation learning.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors study medical volume-to-volume translation, presenting Score Fusion, a 3D volumetric translation model. The model is based on a fine-tuning process, which starts from an average of 2D models. The method is tested on multiple tasks on two medical datasets, being compared with a number of approaches.\n\n### Claims And Evidence\n\nThe claims are supported by empirical evidence.\n\n### Methods And Evaluation Criteria\n\nThe proposed solution is reasonable and efficient.\n\n### Theoretical Claims\n\nN/A.\n\n### Experimental Designs Or Analyses\n\nIt is typical to perform super-resolution at multiple scales. This should be performed by the authors. Aside from this, I did not find any flaws in the experiments.\n\n### Supplementary Material\n\nYes, read it all.\n\n### Relation To Broader Scientific Literature\n\nThe topic is interesting and method is timely.\n\n### Essential References Not Discussed\n\nThere are some relevant references on medical image-to-image translation that are not acknowledged, e.g. [A, B]\n\n[A] Haimour, Fatima, Rizik Al-Sayyed, Waleed Mahafza, and Omar S. Al-Kadi. \"Bidirectional brain image translation using transfer learning from generic pre-trained models.\" Computer Vision and Image Understanding 248 (2024): 104100.\n\n[B] Ristea, Nicolae-Cătălin, Andreea-Iuliana Miron, Olivian Savencu, Mariana-Iuliana Georgescu, Nicolae Verga, Fahad Shahbaz Khan, and Radu Tudor Ionescu. \"CyTran: A cycle-consistent transformer with multi-level consistency for non-contrast to contrast CT translation.\" Neurocomputing 538 (2023): 126211.\n\n### Other Strengths And Weaknesses\n\nThe paper is mostly easy to follow.\n\n### Other Comments Or Suggestions\n\nSection titles are not consistently capitalised.\n\n### Questions For Authors\n\nN/A\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this work, the authors focus on using diffusion models for 3d volume-to-volume translation tasks such as super resolution and modality translation. Since 3d volumes (characteristic of medical data) are too big to computationally run a diffusion model, the authors first train two diffusion models in perpendicular axes. Then, they train a 3d diffusion model which is able to fuse both views to reconstruct the desired volume.\n\n### Claims And Evidence\n\nYes\n\n### Methods And Evaluation Criteria\n\n- Yes, the authors conduct experiments on tasks like super resolution and modality translation which are common tasks in medical volume-to-volume translation.\n- They evaluate on metrics like PSNR, SSIM, FID which are standard metrics.\n- They evaluate both tasks on two datasets: Brats and HCP\n- Authors also conduct downstream task (medical segmentation) to justify the superior quality of their generated volumes\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\n- Good experiments. See Method section above for more details.\n- Is there a reason why the authors did not considere LDM for the diffusion framework? Because DDPM-based models have image size constraints in 2D (authors had to use 192x192 sizes which is small).\n- The standard deviation and t-test of the results was not provided. Hence, unsure if the quantitative performance is statistically significant or not.\n\n### Supplementary Material\n\nYes, reviewed all.\n\n### Relation To Broader Scientific Literature\n\nThis work relates to practical medical volume generation. Existing works in literature focus on 2D slices which has limited applicability in the real setting. Since the authors' work is in 3D, it has potential for real-world usage.\n\n### Essential References Not Discussed\n\nN/A\n\n### Other Strengths And Weaknesses\n\nMy score is reduced mainly due to the comments in the Experiments section. If the authors can address this, then i can consider increasing the score after discussing with other reviewers as well.\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\nnone.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors present Score-Fusion, a volumetric translation model that learns 3D representations by assembling perpendicularly trained 2D diffusion models in score function space. It can reduce the 3D training computational cost and data demand, and the results is comparable in different downstream tasks. However, limitations exist.\n\n### Claims And Evidence\n\nI have concerns about the claim, since the authors made very broad claims about “Medical Image” in general. \n1. A concern is that the experimentations were done on brain only. Brain is well-known to be relatively the best structured and clean among all organs, compared to breast, liver, prostate, etc. \n2. The experimentations were done on single modality of medical image - MRI only, without any evidences from CT, ultrasound, Mammogram, etc.\n3. The authors ignoring the physical limitations of the medical imaging. It seems like the authors are treating all medical imaging as isotropic or nearly-isotropic. However, many of the MRI sequences do not generate such results with isotropic properties. For example, prostate MRI. T2Tse prostate MRI suffers from 320x320x20 dimensionality, which means higher resolution in x-y plane (320x320), and much lower resolution (20x320) in both x-z and y-z planes. In this case, I would doubt if the authors’ methodology still works or not.\nTherefore, although the authors have done experimentations on two different downstream tasks, the organ & image modality & image physics limitations let me feel concerned about the generalizability of the proposed method, and the validity of the claims made by the authors.\n\n### Methods And Evaluation Criteria\n\nThe concerns are stated under “Claims and Evidence” together.\n\n### Theoretical Claims\n\nYes. About Diffusion models and model fusions.\n\n### Experimental Designs Or Analyses\n\nYes.\n\n### Supplementary Material\n\nYes, technical details like ablation studies, etc.\n\n### Relation To Broader Scientific Literature\n\nMore efficient way of using 2D Diffusion models to mimic 3D Diffusion models’ results, in Brain and in MRI specifically.\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\nN/A\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\n1. About generalizability, I have mentioned some of my concerns under “Claims And Evidence”, please response to them\n2. I’m not clear of the network architecture, although you mentioned it is UNet-like. I don’t think you need to describe everything about the parameters you set in this network, but apparently the current description is not enough.\n3. “To further enhance the speed of multi-modality fusion, we employ a smaller variant of our model, adjusting the number of channels in each layer.” Would like to learn more about this.\n4. “For instance, DDMM-Synth (Li et al., 2023) suggested using both MRI and low-resolution CT scans to produce high resolution CT images. Training a separate model for each possible combination of input conditions would result in exponential time complexity, making it generally impractical …” I hold my opinion regarding the discussions here. T2 MRI -> T1 MRI image translation, and the T2 MRI -> CT image translation, are VERY different as MRI and CT have different physical properties. T2->T1 is within the same image modality, however, MRI->CT is using two different image modalities. From my understanding, the statement made by Li et al. 2023 was majorly becuase - at least you have to get some physics information of CT so that you can create an “accurate” high-resolution CT image that won’t impact the diagnosis. For example, it is possible that some tiny lesions could be identified by the CT but cannot be identified by the MRI. If not providing ANY CT information, the translation results is useless in terms of diagnosis.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Dou Hoon Kwark",
      "Kaiwen Hong",
      "Ruike Zhu",
      "Shirui Luo",
      "Volodymyr Kindratenko",
      "Xiyue Zhu",
      "Yiqi Tao",
      "Yudu Li",
      "Zhi-Pei Liang"
    ],
    "url": "pdfs/icml.cc-2025-conference_08478dcf3f2899ba2939eb0b48a6225626ed11f8.pdf",
    "remote_url": "https://openreview.net/pdf/08478dcf3f2899ba2939eb0b48a6225626ed11f8.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "GraphCL: Graph-based Clustering for Semi-Supervised Medical Image Segmentation",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "deep_learning->selfsupervised_learning"
    ],
    "keywords": [
      "Semi-supervised learning;medical image segmentation;GraphCL;clustered features"
    ],
    "abstract": "Semi-supervised learning (SSL) has made notable advancements in medical image segmentation (MIS), particularly in scenarios with limited labeled data and significantly enhancing data utilization efficiency. Previous methods primarily focus on complex training strategies to utilize unlabeled data but neglect the importance of graph structural information. Different from existing methods, we propose a graph-based clustering for semi-supervised medical image segmentation (GraphCL) by jointly modeling graph data structure in a unified deep model. The proposed GraphCL model enjoys several advantages. Firstly, to the best of our knowledge, this is the first work to model the data structure information for semi-supervised medical image segmentation (SSMIS). Secondly, to get the clustered features across different graphs, we integrate both pairwise affinities between local image features and raw features as inputs. Extensive experimental results on three standard benchmarks show that the proposed GraphCL algorithm outperforms state-of-the-art semi-supervised medical image segmentation methods.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes GraphCL, a novel graph-based clustering framework for semi-supervised medical image segmentation (SSMIS). The key contribution is integrating graph data structures into deep learning models which leverages both labeled and unlabeled data, leading to better segmentation performance. The authors propose a dense-connected instance graph constructed from CNN features, combined with a Graph Convolutional Network (GCN) to propagate structural information. Additionally, they introduce a k-less clustering strategy to automatically group similar nodes without specifying the number of clusters. The method is evaluated on three public medical image segmentation benchmarks (ACDC, LA, and Pancreas-NIH), demonstrating superior performance over state-of-the-art methods. Ablation studies confirm the effectiveness of structure-aware alignment and graph clustering.\n## update after rebuttal\n\n### Claims And Evidence\n\nThe paper claims that GraphCL is the first approach to model data structure information in graph form for SSMIS and that it achieves state-of-the-art performance. The empirical results support these claims with strong improvements in segmentation accuracy across multiple datasets. The authors provide extensive ablation studies to validate the effectiveness of each component (e.g., structure-aware alignment and graph clustering loss). The results show consistent improvements across most metrics, particularly in scenarios with limited labeled data.\n\n### Methods And Evaluation Criteria\n\nUsing graph-based clustering to capture structural relationships in medical images is innovative and addresses the challenge of limited labeled data. The evaluation is conducted on standard datasets with widely accepted metrics (DSC, Jaccard, 95HD, ASD). The choice of benchmarks (ACDC, LA, Pancreas-NIH) is appropriate, as they include diverse medical imaging tasks and modalities (CT, MRI).\n\n### Theoretical Claims\n\nThe authors provide a clear formulation of the graph construction and clustering mechanisms. The paper does not present formal theoretical proofs.\n\n### Experimental Designs Or Analyses\n\nThe experiments are well-designed, with thorough ablation studies and sensitivity analyses to validate the impact of key components (graph clustering and structure-aware alignment) and hyperparameters (e.g., κ and τ). The datasets used are appropriate for the task, and the results are consistently reported. The paper could benefit from a discussion of the computational complexity of the proposed method, especially in comparison to existing approaches.\n\n### Supplementary Material\n\nSource code is provided and well-structured. This helps the community to further develop advanced methods upon the current work.\n\n### Relation To Broader Scientific Literature\n\nThe paper builds on prior work in semi-supervised learning and graph-based methods for medical image segmentation. It extends the use of GCNs to SSMIS, which has not been extensively explored in the medical imaging domain. The authors effectively position their work within the broader literature, citing relevant studies in semi-supervised learning, graph neural networks, and medical image segmentation.\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\nStrengths:\n1.The integration of graph-based clustering with semi-supervised learning is novel and addresses a critical challenge in medical image segmentation.\n2.Strong empirical results with multiple datasets and baselines. The ablation studies and sensitivity analyses provide strong evidence for the effectiveness of each component of the proposed method.\n\nWeaknesses:\n1.There lacks insightful discussion of why data structure information helps fine-grained semi-supervised medical image segmentation. This limits the methodological contribution of the proposed method. The rebuttal partly solved this problem.\n2.The paper lacks a discussion of the computational complexity of the proposed method, which could be a concern for large-scale datasets that contain many unlabeled data. This problem has been solved in the rebuttal.\n\n### Other Comments Or Suggestions\n\n1.Consider discussing potential limitations such as computational overhead from GCN operations with existing methods, which would provide valuable insights for practical applications. This problem has been solved in the rebuttal.\n2.The paper would benefit from visualizations of the graph structures and clustering results to provide a more intuitive understanding of the method. This problem has been solved in the rebuttal.\n3.Clarify the impact of different dataset sizes on performance improvements. The current datasize (both labeled and unlabeled) is too small to genralize to large-scale datasets. This problem has been solved in the rebuttal.\n\n### Questions For Authors\n\nNo\n\n### Overall Recommendation: 4\n\n### Ethical Review Concerns\n\nNo significant ethical concerns were identified.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces a graph-based clustering for semi-supervised medical image segmentation by modeling data structure in a unified network. A graph clustering loss function was proposed to optimize the correlation clustering task in SSMIS.\n\n### Claims And Evidence\n\nThe authors claim that 1) previous methods neglect the importance of graph structural information, and 2) no research has explored semi-supervised medical image segmentation (SSMIS) from the perspective of data structure. However, they do not specify what graph structural information can be utilized or explain why it is crucial. This claim is not well substantiated, as the incorporation of GCN into the framework yields only a modest performance gain, suggesting that graph structural information may not be as critical as the authors suggest. Regarding the second claim, there are existing works that have explored the use of graphs in semi-supervised medical image segmentation.\n\n[1] Sun, Junxiao, et al. \"Semi-supervised medical image semantic segmentation with multi-scale graph cut loss.\" 2021 IEEE International Conference on Image Processing (ICIP). IEEE, 2021.\n[2] Li, Gang, et al. \"Dynamic graph consistency and self-contrast learning for semi-supervised medical image segmentation.\" Neural Networks 184 (2025): 107063.\n\n### Methods And Evaluation Criteria\n\nIncorporating a graph into the SSMIS framework is a viable approach. However, the lack of a high-level explanation and sufficient technical details raises questions about whether the proposed method will have a significant impact on the problem at hand.\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nThe experimental design seems acceptable overall. However, the significant variation in hyperparameters across datasets raises concerns about the method’s generalizability and its usability across different applications.\n\n### Supplementary Material\n\nYes, I read the related work section in the SM.\n\n### Relation To Broader Scientific Literature\n\nThe paper addresses an area of active research in semi-supervised medical image segmentation, which has been well-explored. Methods like MT, UA-MT, DTC and co-training-based methods, have already demonstrated effective results in this domain. However, the approach proposed in this paper is based on MT with GCN-based regularization, offering limited innovation in terms of methodology or application. The performance gains appear to be marginal, and as such, it is unclear whether the proposed method can substantially advance the field. A clearer demonstration of its advantages over existing approaches would help establish its novelty and relevance.\n\n### Essential References Not Discussed\n\nSince the proposed method targets SSMIS, I recommend reviewing related papers in more detail. For instance, co-training is a significant approach in SSMIS, but relevant papers are not discussed.\n\n### Other Strengths And Weaknesses\n\n1. In the abstract, it is stated that 'The proposed GraphCL model enjoys several advantages. Firstly, to the best of our knowledge, this is the first work to model the data structure information for semi-supervised medical image segmentation (SSMIS). Secondly, to get the clustered features across different graphs, we integrate both pairwise affinities between local image features and raw features as inputs.' However, these two points are not advantages. The statement after 'firstly' is more of a novelty claim, which may not be accurate, while the sentence after 'secondly' describes a feature of the proposed method rather than an advantage.\n\n2. Terms like Structural Graph Model and Data Structure Analyzer are not widely established or standardized, causing difficult to read.\n\n3. An overview of the proposed method is necessary to help readers gain a clearer understanding, but it is missing.\n\n### Other Comments Or Suggestions\n\nI have no other comments or suggestions.\n\n### Questions For Authors\n\n1. Could you provide more specifics on the type of graph information that can improve semi-supervised medical image segmentation? How is this information effectively utilized in the proposed method?\n\n2. It is stated that \"To address the challenge of effectively integrating both labeled and unlabeled medical images within the semi-supervised medical image segmentation (SSMIS) framework, we propose a Structural Graph Model (SGM).\" I don't know what SGM is? It is not shown in Figure 2. And how does it integrate labeled and unlabeled medical images?\n\n3. It is stated that \"...This component generates structure scores that quantify the similarity between different samples based on their internal spatial structure, as derived from the learned CNN features.\" However, I am unsure why CNN features would contain internal spatial structure. Can you explain?\n\n4. What the so-called Data Structure Analyzer is like? What is the relation between the $X$ in equation 12 and $X_{sa}$ in equation 13?\n\n5. I noticed that the three datasets used in the paper are all small. Why would you choose small datasets to demonstrate the utility of SSMIS? Unlike large labeled datasets, small datasets are relatively easy to curate. It would be more impactful to test the proposed method in real-world scenarios with larger, more challenging datasets.\n\n### Overall Recommendation: 1",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this work, the authors tackle semi-supervised medical image segmentation (SSMIS) by proposing GraphCL. This is the first work to model data in a graph network for SSMIS. The authors propose a graph clustering loss function for optimization.\n\n### Claims And Evidence\n\nYes\n\n### Methods And Evaluation Criteria\n\n- The authors introduce a k-less strategy for clustering (k = number of clusters), enabling similar nodes to automatically form clusters.\n- They follow a teacher-student framework, the outputs of which are used to construct the graph data structure. The graph loss-function is used to train the whole framework in an end-to-end fashion.\n- The different components of the method are well-motivated\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\n- The authors compare with adequate baselines, which are state-of-the-art methods in SSMIS task. They compare across three publicly available medical image seg datasets. They conduct experiments under different unlabeled % data settings as well.\n- The authors use 4 metrics (Dice, Jaccard, HD, ASSD) to evaluate the segmentation quality. The authors' proposed GraphCL outperforms all the baselines. \n- The standard deviation of the performance is missing, however. The authors would benefit by showing standard deviation and conducting t-test to determine if the performance improvement is statistically significant or not. \n- The authors provide good ablation studies of the different components in their method.\n- Appreciate the code release in the supplementary.\n\n### Supplementary Material\n\nYes, I reviewed the entire supplementary.\n\n### Relation To Broader Scientific Literature\n\nThe current work has real-life applications as medical datasets tend to have few labeled and largely unlabeled data. As the authors' work outperforms existing SSMIS methods, it has relevance to the community.\n\n### Essential References Not Discussed\n\nN/A\n\n### Other Strengths And Weaknesses\n\nN/A\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\nnone.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Chuan Li",
      "Jiao Li",
      "Jingcai Guo",
      "Li Shen",
      "Mengzhu Wang",
      "Nan Yin",
      "houcheng su"
    ],
    "url": "pdfs/icml.cc-2025-conference_9febd060ecd28fce2b77ec8dab4babca4ce5b557.pdf",
    "remote_url": "https://openreview.net/pdf/9febd060ecd28fce2b77ec8dab4babca4ce5b557.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "Scalable Generation of Spatial Transcriptomics from Histology Images via Whole-Slide Flow Matching",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications->health_medicine"
    ],
    "keywords": [
      "Spatial transcriptomics",
      "histology images"
    ],
    "abstract": "Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models.",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors propose STFlow, a model for spatially resolved gene-expression prediction from WSIs.\n\nSTFlow is based on flow matching, modelling the joint distribution of the full spatial gene-expression data across each WSI, through an iterative refinement process. This enables explicit modelling of spot-to-spot interactions. \n\nThe denoiser network is a frame-averaging transformer, integrating spatial context and gene interactions within the attention mechanism.\n\nThe proposed STFlow is evaluated on both the HEST-1k and STImage-1K4M datasets. It performs well compared to recent spot-based and slide-based baseline models.\n\n\n\n\n\n\n##########\n\n##########\n\n##########\n\n##########\n\nUpdate after the rebuttal:\n\nAll reviewers are positive overall, and the authors have provided a very solid rebuttal.\n\nThis is an interesting and very solid paper, I don't really see any reason for why it shouldn't be accepted. I have increased my score to \"_4: Accept_\".\n\n### Claims And Evidence\n\nYes.\n\n### Methods And Evaluation Criteria\n\nYes.\n\n### Theoretical Claims\n\nN/A.\n\n### Experimental Designs Or Analyses\n\nSolid experimental setup.\n\n### Supplementary Material\n\nQuickly read the appendix.\n\n### Relation To Broader Scientific Literature\n\nGood discussion of related work in Section 2.\n\n### Essential References Not Discussed\n\nN/A.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n- Although the paper contains quite a few typos, it's well written overall.\n- The studied problem is interesting and relevant.\n- The proposed STFlow model is quite interesting, I think it makes sense to jointly model the full spatial gene-expression data using flow matching.\n- The experimental evaluation is solid, utilizing both the HEST-1k and STImage-1K4M datasets.\n- The proposed STFlow seems to perform well compared to relevant and recent baselines (BLEEP: NeurIPS 2023, TRIPLEX: CVPR 2024). Moreover, Section 4.3 presents relevant ablation studies which indicate that the main model components positively affect the performance.\n\n\n\n\nWeaknesses:\n- The paper is well written overall but does contain quite a few typos etc, it would definitely benefit from some additional careful proofreading.\n- I found it quite difficult to follow and understand the method description in Section 3.3. I think Section 3.1 and 3.2 are fine, but I struggled with 3.3. I think an effort should be made to work through this section again, making sure everything is described as clearly as possible.\n\n### Other Comments Or Suggestions\n\nQuestions/Suggestions:\n- It would be interesting to see the computational cost for STFlow and UNI reported in Table 1. I don't really consider added computational cost a major issue, but just to see how much the transformer model and iterative refinement slows down STFlow compared to the UNI + regression layer baseline.\n- Section 4.1, _\"Additionally, some ST-based approaches fail to predict significantly correlated gene expression\"_: Could you clarify exactly what you refer to in Table 1 here? The results for STNet and HisToGene?\n- In Section 4.2, could perhaps clarify that these 3 datasets are from HEST-1k?\n- It's not entirely clear to me what \"STFlow w/o FM\" in Table 2, Figure 4 and Table 4 means, does this correspond to setting S = 1 in Algorithm 2?\n- Figure 4 is neat, it would be interesting to see more examples like these, could you perhaps add a couple to the appendix?\n- It would also be interesting to see Figure 4-like visualizations of the predicted gene-expression during the iterative refinement process? I.e., what does the initial random sample $Y_0$ look like? And how does this then evolve during the S=5 refinement steps?\n- It would also be interesting to see the regression accuracy as a function of the number of refinement steps S (e.g. for S = 1, 2, 4, 8, 16), does the performance increase with more and more steps, or does it quickly plateau?\n- Regarding the Limitations paragraph in Section 5: I think it would be relevant to refer to the results in Table 9 in the appendix here, or at least somewhere in the main paper? Because, I think these are encouraging results? If initializing $Y_0$ with all zeros instead of sampling from the ZINB distribution, the performance drops from 0.415 to 0.407 for UNI, which is not a lot? This would still beat all baselines in Table 1? I.e., this indicates that the model is quite robust to this choice of prior?\n\n\n\n\n\nMinor things:\n- Line 86: \"This enables explicit modeling cell-cell interactions\" --> \"This enables explicit modeling of cell-cell interactions\"?\n- Line 94: \"WSI collections comprising total 17 benchmark datasets\" --> \"WSI collections comprising a total of 17 benchmark datasets\"?\n- Line 83: \"have enabled the detecting of RNA\" --> \"have enabled the detection of RNA\"?\n- Line 96: \"et al., 2024).One concurrent work\" --> \"et al., 2024). One concurrent work\".\n- Line 97: \"leverages diffusion model for\" --> \"leverages a diffusion model for\" / \"leverages diffusion models for\"?\n- Line 111: \"modeling joint distribution\" --> \"modeling the joint distribution\"?\n- Line 120: \"In this work, we repurpose\" --> \"In this work, we reformulate\"?\n- Line 142: \"whole-slide images (WSIs) using an FA-based Transformer\", don't need to define WSIs again here.\n- Line 137: \"In this study, the goal of STFlow aims to predict\" --> \"In this study, the goal of STFlow is to predict\"?\n- Line 185: \"However, standard regression objective cannot model cell-cell interaction as it predicts\" ---> \"However, the standard regression objective cannot model cell-cell interaction as it predicts\"?\n- Line 194: \"denoised model\" --> \"denoiser model\"?\n- \"Algorithm 1 STFlow: Train\" --> \"Algorithm 1 STFlow: Training\"?\n- I think all \"<--\" in Algorithm 1 and 2 could be replaced with just \"=\"?\n- Line 244: \"an E(2)-invariant transformation for point cloud\" --> \"an E(2)-invariant transformation for point clouds\"?\n- Line 247: \"minimal modification to Transformer\" --> \"minimal modifications to the Transformer\"?\n- Line 266: \"guaranteed by frame averaging framework\" --> \"guaranteed by the frame averaging framework\"?\n- Section 3.4: I think it's more common to use \"Eq.\" instead of \"Equ.\".\n- _Spatially Resolved Gene Expression Prediction from H&E Histology Images via Bi-modal Contrastive Learning_ is a NeurIPS 2023 paper, not 2024?\n- Line 363: \"Table 2 achieves the highest correlation across all biomarkers\" --> \"In Table 2, STFlow achieves the highest correlation across all biomarkers\"?\n- Line 767: \"Table 10 and presents\", typo.\n\n### Questions For Authors\n\n1. Could you update Section 3.3?\n\n2. Could you clarify what setting \"STFlow w/o FM\" corresponds to?\n\n3. Could you add results for other refinement steps S (not just S = 5)?\n\n4. Could you add some more Figure 4-like visualizations?\n\n\n\n\n\nJustification of overall recommendation:\n\nThe studied problem is interesting and relevant, the proposed STFlow model conceptually makes sense overall, the experimental setup is solid, and STFlow seems to perform well compared to relevant baselines. While the current version requires some more proofreading and polishing, and could benefit from some additional results and visualizations (at least added to the appendix), I think that a solid rebuttal by the authors should make me want to accept this paper. I'm definitely leaning towards accept right now at least.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces a scalable and efficient framework for predicting spatial transcriptomics from histology images. By integrating flow matching for progressive gene refinement, E(2)-invariant spatial attention for robust spatial modeling, and whole-slide scalability, STFlow formulates gene expression prediction as a generative modeling task, effectively capturing spatial dependencies and cell-cell interactions while overcoming the limitations of existing approaches.  Experimental results demonstrate that STFlow achieves state-of-the-art performance on 17 benchmark datasets (HEST-1k and STImage-1K4M), excelling in gene expression prediction and biomarker identification tasks. It outperforms pathology foundation models in spatial gene expression prediction and achieves the highest correlation scores in biomarker gene prediction (GATA3, ERBB2, UBE2C, VWF), demonstrating its effectiveness in modeling spatial dependencies and gene interactions.\n\n### Claims And Evidence\n\nOverall, the claims in the paper are well-supported by quantitative evidence. However, to further validate the choice of ZINB, an ablation study on its hyperparameters (μ,ϕ,π) should be conducted to demonstrate their impact on prediction performance. Additionally, a hyperparameter study on the number of refinement steps in the flow matching process is needed to assess its contribution to model accuracy and convergence stability.\n\n### Methods And Evaluation Criteria\n\nThe proposed methods and evaluation criteria in the submission appear well-aligned with the problem of spatial transcriptomics prediction.\nSTFlow addresses the challenge of predicting spatial transcriptomics from histology images by introducing a flow matching-based generative model. STFlow explicitly models the joint distribution of gene expression across the entire slide, incorporates cell-cell interactions, and employs a local spatial attention-based slide-level encoder to reduce computational overhead. This approach overcomes the limitations of previous methods, which struggled with capturing spatial dependencies and suffered from high computational complexity. \nEvaluated on two large-scale benchmark datasets, HEST-1k and STImage-1K4M, STFlow outperforms eight state-of-the-art baselines in gene expression and biomarker prediction, achieving a relative improvement compared to pathology foundation models while demonstrating superior computational efficiency. \nThe evaluation criteria include Pearson correlation for gene expression prediction, accuracy in predicting four key biomarkers, and computational efficiency metrics such as runtime and memory usage, ensuring a robust and comprehensive assessment.\n\n### Theoretical Claims\n\nNo\n\n### Experimental Designs Or Analyses\n\nThe study introduces STFlow, a deep learning-based approach for predicting spatial transcriptomics (ST) data from histology images. Overall, the experimental design and analysis are rigorous, and the effectiveness of the model has been validated through multiple evaluations.\nRegarding the experimental design, the study compares spot-based and slide-based methods to ensure a comprehensive performance assessment. Additionally, it employs two large-scale benchmark datasets, HEST-1k and STImage-1K4M, which help reduce dataset-specific biases. To prevent data leakage, k-fold cross-validation is used, with patient-stratified splits ensuring that training and test sets do not overlap at the patient level. The study also conducts ablation experiments to assess the contributions of key components, such as the flow matching mechanism and spatial attention module, to model performance.\nIn terms of analysis, the study employs Pearson correlation coefficient as the primary evaluation metric to measure the relationship between predicted and actual gene expression levels. Additionally, it evaluates model performance on four critical biomarkers (GATA3, ERBB2, UBE2C, and VWF), demonstrating superior predictive accuracy over existing methods. The study also provides visualizations of gene expression predictions, showing a strong alignment between STFlow’s predictions and ground-truth expression levels, enhancing interpretability.\n\n### Supplementary Material\n\nI reviewed the supplementary material, including implementation details, dataset statistics and additional ablation studies.\n\n### Relation To Broader Scientific Literature\n\nFirst, Previous methods either predicted gene expression independently for each spot, neglecting cell-cell interactions, or relied on computationally expensive global attention mechanisms. STFlow overcomes these issues by introducing a flow matching-based generative modeling framework, which models the joint distribution of gene expression across an entire slide. This allows for iterative refinement, leading to more biologically meaningful predictions. \nSecond, STFlow leverages spatial attention with E(2)-invariant properties, ensuring robustness to spatial variations such as rotation and translation.\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\nThis paper is clearly written and easy to read. It presents a novel approach to spatial transcriptomics prediction by introducing flow matching-based generative modeling, which effectively models joint gene expression distributions across entire slides while incorporating cell-cell interactions. STFlow demonstrates state-of-the-art performance on HEST-1k and STImage-1K4M, achieving improvement over pathology foundation models. Additionally, it excels in biomarker prediction experiments, accurately predicting key genes such as GATA3, ERBB2, UBE2C, and VWF, highlighting its potential for clinical applications.\n\n### Other Comments Or Suggestions\n\nNo\n\n### Questions For Authors\n\n1.\tThe authors should perform a hyperparameter study on the number of refinement steps in the flow matching process.\n2.\tThe authors should conduct an ablation study on the hyperparameters of the ZINB prior (μ, ϕ, π) to demonstrate their impact on prediction performance.\n\n### Overall Recommendation: 4\n\n### Ethical Review Concerns\n\nNo",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a flow matching (FM) approach (called STFlow) for predicting the spatial transcriptomics (ST) from pathological Whole-Slide Images (WSIs). The core designs of STFlow contain i) learning the joint distribution $p(Y_0,\\cdots,Y_N|I_0,\\cdots,I_N)$ using the FM approach and ii) the E(2)-invariant spatial attention that adapts frame averaging (FA) to the attention operation for learning invariant spot-level representations. Two large-scale benchmarks are adopted to verify the effectiveness of STFlow. The experimental results show the superiority of STFlow over existing methods.\n\n## update after rebuttal\n\nThanks for the author's rebuttal and the additional results. My concerns are well addressed. I am happy to raise my score from 2 to 3. The authors are encouraged to include the addition results into the revised paper and also carefully revised the paper to further improve the presentation quality.\n\n### Claims And Evidence\n\nMost of the claims are justified by empirical experiments. However, \n- the effectiveness of the author's one core design, *i.e.*, modeling the joint distribution $p(Y_0,\\cdots,Y_N|I_0,\\cdots,I_N)$, seems to lack convincing evidence. \n- In addition, I have several concerns about the experimental design and the results, which are specified below for the authors to answer in the rebuttal.\n\n### Methods And Evaluation Criteria\n\n**Methods**: Yes, the proposed methods make sense for the problem studied in this paper.\n\n**Evaluation Criteria**: Some commonly-used metrics are not presented in the paper.\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nYes, I have carefully checked the experimental design and the results. I have several concerns about them:\n- Lack of the experiments for the setting of $S$ and the visualization of the inference process of FM. These experiments could help to validate the stability of the proposed method and help readers better understand the model. \n- Some results have large variance (Table 1). This calls into question the stability of the proposed method.\n\n### Supplementary Material\n\nN/A\n\n### Relation To Broader Scientific Literature\n\nNo. The key contribution of this paper seems independent with existing works.\n\n### Essential References Not Discussed\n\nNo\n\n### Other Strengths And Weaknesses\n\nStrengths:\n- This paper propose a new generative approach to predicting ST from WSIs. It learns the joint distribution $p(Y_0,\\cdots,Y_N|I_0,\\cdots,I_N)$ using the FM framework. This seems novel and could be a valid contribution to the field.\n- The proposed STFlow considers to learn the invariant representation for spots via E(2)-invariant spatial attention.\n- Impressive results are obtained in this paper. STFlow often outperforms existing methods by large margins.\n\nWeaknesses:\n- The presentation quality is subpar. The authors fail to clarify how they tackle the issues of existing methods in Introduction. Please see *Questions For Authors* for more details\n- Some important experiments are not presented in this paper. In addition, an important design in STFlow, *i.e.*, modeling the joint distribution $p(Y_0,\\cdots,Y_N|I_0,\\cdots,I_N)$, seems to lack justification. Please see *Questions For Authors* for more details\n\n### Other Comments Or Suggestions\n\nPlease see *Questions For Authors*\n\n### Questions For Authors\n\nThe proposed approach seems interesting and novel. Moreover, some promising results are obtained in this paper. Overall, I acknowledge this paper's technical contribution to the field and the novelty of the proposed scheme. However, I have several concerns about the presentation quality and experiments of this work. My concerns and questions are as follows:\n- The Introduction analyzes the issues of existing schemes yet does not clearly explain how this work tackles these issues. Although some explanations are made in the following sections, this way could lead to inefficient reading.\n- The layout of some Tabs and Figs could be improved, *e.g.*, Tab 2 on page 6.\n- Could the authors provide the visualization of the inference process of FM. This experiments could help readers better understand STFlow. \n- Could the authors represent the results of STFlow with different $S$. It may help to validate the stability of the proposed method.\n- Some results have large variance (Table 1). This may call into question the stability of the proposed method.\n- Some commonly-used metrics seem missing, *e.g.*, those metrics presented in the compared methods, TRIPLEX and HisToGene.\n- How is the `w/o FM` implemented? I fail to find the details.\n- The effectiveness of one core design, *i.e.*, modeling the joint distribution $p(Y_0,\\cdots,Y_N|I_0,\\cdots,I_N)$, seems to lack convincing evidence. How is this core design justified? Could the authors explain this?\n- It would be better to provide an intuitive expatiation for how FM realizes the modeling of joint distribution. This could be more friendly to the reader unfamiliar with FM.\n- In Algo.2, is $Y_0$ sampled once or multiple times when deriving the prediction of one WSI from the model?\n\nIf my concerns could be resolved, I would be happy to raise my rating.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a method to predict gene expression from histology whole-slide images using generative flow matching. Spatial transcriptomics (ST) datasets are used to train and evaluate the model. A foundation-model encoder is used to extract visual features. Spatial attention is used to model spatial dependencies. Flow matching models the joint distribution of gene expression over the whole slide in an iterative fashion.\n\n## update after rebuttal:\nAfter carefully reviewing the author's rebuttal and considering other reviewer's input, i decide to keep my accept rating.\n\n### Claims And Evidence\n\nThe method claim to better consider cell-cell interaction through both spatial attention at the encoder and the flow matching iterative generative process. Although spatial attention clearly provides a way to model spatial dependencies, it is not very clear intuitively how the flow matching (FM) helps. Nevertheless, the ablation study shows that without FM, there's a small performance degradation on all benchmarks.\n\n### Methods And Evaluation Criteria\n\nThe proposed method is appropriate for the problem. In particular, it has been proven in the litterature that ST model benefits from spatial attention as the tumor generally grows out spatially. THe evaluation benchmark dataset are also appropriate for the task and have been used previously in the literature.\n\n### Theoretical Claims\n\nNo theoretical claims are made in the paper.\n\n### Experimental Designs Or Analyses\n\nThe experimental section includes 8 baselines (5 spot-base and 3 slide-based) across two benchmark ST datasets (each covering several organs). Further, two ablation studies verify the effectiveness of each module of the proposed approach, showing the improvement for each one.\nOverall, this is a sound and valid experimental section.\n\n### Supplementary Material\n\nSupplementary material is about implementation details of the 8 baselines, stats of the datasets and details of the ablation studies.\n\n### Relation To Broader Scientific Literature\n\nThe key contributions are well-related to specific litterature, in my opinion. Both key contributions (spatial attention and flow matching) are properly related to current publications and the ST litterature is also well referenced.\n\n### Essential References Not Discussed\n\nNot that i know of.\n\n### Other Strengths And Weaknesses\n\nstrengths:\n- The paper applies flow-matching methods (used in generative modeling of molecule and proteins) to gene expressions regression from image encodings. This is novel, as far as i know.\n- a strong experimental analysis with extensive benchmark, ablation + complexity studies\n- outperforms SOTA methods\n- well written and organized. Clear math and algorithms.\n- code repository is proposed\n\ncons:\n- a complex system with several modules and hyperparameters makes it challenging to evaluate\n-\n\n### Other Comments Or Suggestions\n\nno typos to report!\n\n### Questions For Authors\n\nNo questions.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Mehrtash Babadi",
      "Tianyu Liu",
      "Tinglin Huang",
      "Wengong Jin",
      "Zhitao Ying"
    ],
    "url": "pdfs/icml.cc-2025-conference_9c9b7eccefbc51466d0970935c6738ed7b763e7d.pdf",
    "remote_url": "https://openreview.net/pdf/9c9b7eccefbc51466d0970935c6738ed7b763e7d.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications->health_medicine"
    ],
    "keywords": [
      "Medical Image Segmentation",
      "Domain Generalization",
      "Energy-based Models"
    ],
    "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DA). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches. We propose LangDAug, a novel **Lang**evin **D**ata **Aug**mentation for multi-source domain generalization in 2D medical image segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via contrastive divergence to traverse between source domains, generating intermediate samples through Langevin dynamics. Theoretical analysis shows that LangDAug induces a regularization effect, and for GLMs, it upper-bounds the Rademacher complexity by the intrinsic dimensionality of the data manifold. Through extensive experiments on Fundus segmentation and 2D MRI prostate segmentation benchmarks, we show that LangDAug outperforms state-of-the-art domain generalization methods and effectively complements existing domain-randomization approaches. The codebase for our method is available at https://github.com/backpropagator/LangDAug.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a data augmentation method via Langevin dynamics with theoretical analyses. They also conduct experiments to show its usefulness.\n\n### Claims And Evidence\n\nIn Line 019-023, \"DA methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representation learning approaches.\" Is there empirical evidence for this?\n\n### Methods And Evaluation Criteria\n\nN/A\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nThere seem to be too few baselines considering the abundant literature of domain generalization. To name a few: CORAL [1], RSC [2], SWAD [3], SagNet [4], etc.\n\n\n\n[1] Sun B, Saenko K. Deep coral: Correlation alignment for deep domain adaptation[C]//Computer vision–ECCV 2016 workshops: Amsterdam, the Netherlands, October 8-10 and 15-16, 2016, proceedings, part III 14. Springer International Publishing, 2016: 443-450.\n\n[2] Huang Z, Wang H, Xing E P, et al. Self-challenging improves cross-domain generalization[C]//Computer vision–ECCV 2020: 16th European conference, Glasgow, UK, August 23–28, 2020, proceedings, part II 16. Springer International Publishing, 2020: 124-140.\n\n[3] Cha J, Chun S, Lee K, et al. Swad: Domain generalization by seeking flat minima[J]. Advances in Neural Information Processing Systems, 2021, 34: 22405-22418.\n\n[4] Nam H, Lee H J, Park J, et al. Reducing domain gap via style-agnostic networks[J]. arXiv preprint arXiv:1910.11645, 2019, 2(7): 8.\n\n### Supplementary Material\n\nN/A\n\n### Relation To Broader Scientific Literature\n\nN/A\n\n### Essential References Not Discussed\n\nN/A\n\n### Other Strengths And Weaknesses\n\nIn this paper, \"DA\" is short for data augmentation. However, in OOD literature, DA usually stands for domain adaptation. It is better to change another abbreviation for data augmentation to avoid confusion.\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\nWhat is the benefit of the proposed method in domain generalization specifically in medical image segmentation? It seems that it can be applied to normal domain generalization tasks as well.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors propose LangDAug, a Langevin Data Augmentation technique to improve multi-source domain generalization in medical image segmentation tasks. The core idea is to train Energy-Based Models (EBMs) via contrastive divergence to model the transitions between pairs of source domains and use Langevin dynamics (LD) to generate intermediate samples that serve as augmented data bridging domain gaps. Theoretically, the authors demonstrate that this augmentation method induces a regularization effect, leading to smoother optimization landscapes and better generalization by bounding model complexity based on intrinsic data manifold dimensionality. Experimental evaluations conducted on retinal fundus and prostate MRI segmentation datasets show that LangDAug outperforms previous methods across unseen domains.\n\n### Claims And Evidence\n\nOverall, the paper’s main claims regarding the effectiveness and robustness of the proposed LangDAug approach are convincingly supported by empirical evidence (quantitative benchmarks and visualizations) and theoretical analysis. Some limitations exist concerning scalability and detailed validation of anatomical fidelity, but these do not substantially undermine the validity or clarity of the core contributions.\n\n### Methods And Evaluation Criteria\n\nThe proposed methodology and evaluation criteria used in the submission are clearly appropriate and well-justified for the stated objectives of the paper; however, I would also want to know if the proposed LangDAug could be used for 3D medical image segmentation tasks and the efficiency of doing so.\n\n### Theoretical Claims\n\nOverall, the theoretical claims made by the authors are clearly articulated, logically consistent, and mathematically rigorous. The authors provide a coherent theoretical framework that convincingly justifies why LangDAug would improve domain generalization. There are no substantial mathematical errors or inconsistencies evident from the provided derivations.\n\n### Experimental Designs Or Analyses\n\nThe experimental design and analyses used in this submission are sound, valid, and appropriately rigorous for assessing domain generalization performance.\n\nHowever, there are still some limitations:\n- Missing detailed computational cost analyses as the proposed LangDAug could increase training time.\n- It is better to analyze the contribution of individual components of LangDAug (e.g., varying Langevin steps, EBMs complexity, or the number of augmented samples).\n- How about applying LangDAug to 3D medical image segmentation tasks?\n\n### Supplementary Material\n\nYes. Implementation details and the proof.\n\n### Relation To Broader Scientific Literature\n\n- The key contributions (LangDAug) of this paper closely align with and extend several ideas from the broader scientific literature, specifically related to domain generalization, energy-based models, Langevin dynamics, and data augmentation strategies in machine learning, particularly in medical imaging. \n\n- LangDAug directly extends prior work on domain generalization, especially the notion that models trained under the Empirical Risk Minimization (ERM) paradigm often fail to generalize under domain shifts (Gulrajani & Lopez-Paz, 2021; Ganin et al., 2016). It is positioned within data manipulation approaches, specifically leveraging the concept of intermediate-domain traversal via Langevin dynamics, distinguishing itself from prior DG methods that primarily relied on style-mixing (MixStyle), random convolutions (RandConv), or frequency domain augmentation (FedDG, RAM, TriD).\n\n### Essential References Not Discussed\n\nThe manuscript thoroughly references relevant literature on domain generalization (DG), energy-based models (EBMs), Langevin dynamics, and medical image segmentation.\n\n### Other Strengths And Weaknesses\n\nNA\n\n### Other Comments Or Suggestions\n\nNA\n\n### Questions For Authors\n\n- Could the authors provide a detailed computational cost analysis (e.g., GPU hours, memory requirements, inference and training times) of the proposed LangDAug method compared to the baseline domain generalization approaches?\n\n- The current implementation and validation of LangDAug focus exclusively on 2D medical image segmentation. Could the authors explain any specific technical or methodological reasons why you have not extended LangDAug directly to 3D medical image segmentation tasks? Are there fundamental challenges, such as increased complexity, computational demands, or theoretical limitations, that prevented immediate extension?\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper presents LangDAug, a novel data augmentation method designed to address multi-source domain generalization challenges in medical image segmentation. Leveraging Langevin Dynamics (LD) and Energy-Based Models (EBMs), LangDAug generates intermediate samples bridging source domains to enhance model generalization across unseen target domains. The approach involves training EBMs to capture energy distributions between source domains, using LD to create bridging samples, and incorporating these samples into the segmentation model's training process. Theoretical analysis demonstrates that LangDAug induces regularization, reducing Rademacher complexity and improving generalization. Experiments on retinal fundus segmentation and 2D MRI prostate segmentation datasets show that LangDAug outperforms existing domain generalization methods and effectively complements domain randomization techniques like FedDG and RAM.\n\n## update after rebuttal\nWhile this work is application-oriented, after thoroughly reviewing all the responses, I remain concerned that the high computational cost and storage requirement could significantly limit the practical applicability of the proposed method. Therefore, I maintain my original score as 2.\n\n### Claims And Evidence\n\nN/A\n\n### Methods And Evaluation Criteria\n\nN/A\n\n### Theoretical Claims\n\nN/A\n\n### Experimental Designs Or Analyses\n\nN/A\n\n### Supplementary Material\n\nYes, all.\n\n### Relation To Broader Scientific Literature\n\nN/A\n\n### Essential References Not Discussed\n\nN/A\n\n### Other Strengths And Weaknesses\n\n**Strengths:**\n\n1. LangDAug introduces a new data augmentation method combining Langevin Dynamics and Energy-Based Models, offering a new perspective for solving multi-source domain generalization problems in medical image segmentation.\n2. The method is supported by theoretical analysis, demonstrating its regularization effect and its ability to reduce Rademacher complexity, enhancing the reliability and scientific foundation of the approach.\n3. This method can be used independently or integrated with existing domain randomization techniques (such as FedDG and RAM), showing its flexibility and practical applicability.\n\n**Weaknesses:**\n\n1. Training an EBM for every pair of source domains results in significant computational and storage requirements, especially when the number of source domains increases. Generating a large number of intermediate samples substantially increases training time, which may hinder the method's practicality for large-scale datasets.\n2. The effectiveness of LangDAug heavily relies on the quality of the trained EBMs, but the paper lacks an in-depth discussion of EBM training stability and its potential impact on results.\n3. Generating inter-domain samples to improve generalization is a reliable solution. However, this requires that the samples in the training set cover the sample space uniformly to obtain good interpolation results. What happens to the performance of the model if the distribution of samples in the training set is restricted?\n\n### Other Comments Or Suggestions\n\nN/A\n\n### Questions For Authors\n\nPls refer to Weaknesses\n\n### Overall Recommendation: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Kinjawl Bhattacharyya",
      "Piyush Tiwary",
      "Prathosh AP"
    ],
    "url": "pdfs/icml.cc-2025-conference_1c537e6ffbb2a5ddaf9d0ef3218f05ed09760150.pdf",
    "remote_url": "https://openreview.net/pdf/1c537e6ffbb2a5ddaf9d0ef3218f05ed09760150.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "Context Matters: Query-aware Dynamic Long Sequence Modeling of Gigapixel Images",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications->health_medicine"
    ],
    "keywords": [
      "Computational Pathology",
      "Whole Slide Image",
      "Cancer Diagnosis and Prognosis"
    ],
    "abstract": "Whole slide image (WSI) analysis presents significant computational challenges due to the massive number of patches in gigapixel images. While transformer architectures excel at modeling long-range correlations through self-attention, their quadratic computational complexity makes them impractical for computational pathology applications. Existing solutions like local-global or linear self-attention reduce computational costs but compromise the strong modeling capabilities of full self-attention. In this work, we propose **Querent**, *i.e.*, the **quer**y-awar**e** long co**nt**extual dynamic modeling framework, which achieves a theoretically bounded approximation of full self-attention while delivering practical efficiency. Our method adaptively predicts which surrounding regions are most relevant for each patch, enabling focused yet unrestricted attention computation only with potentially important contexts. By using efficient region-wise metadata computation and importance estimation, our approach dramatically reduces computational overhead while preserving global perception to model fine-grained patch correlations. Through comprehensive experiments on biomarker prediction, gene mutation prediction, cancer subtyping, and survival analysis across over 10 WSI datasets, our method demonstrates superior performance compared to the state-of-the-art approaches. Codes are available at https://github.com/dddavid4real/Querent.",
    "reviews": [
      {
        "text": "### Summary\n\nTo alleviate the self-attention o(n^2) complexity when modeling WSI, this paper introduces query-based lager-region pruning method to replace linear-attention and local-global attention mechanisms. By ignoring the irrelevant regions to current patches, all the computational cost between current patch and all patches in these regions can be pruned. The evaluations in experiments demonstrate the computational efficiency and performance effectiveness.\n\n### Claims And Evidence\n\nThe claim in line 023-025 (abstract) 'the query-aware long contextual dynamic modeling framework, which maintains the expressive power of full self-attention while achieving practical efficiency' is not evident.'\nIf the full self-attention is maintained, how to speed up. I think this expression should be refined\n\n### Methods And Evaluation Criteria\n\nThe proposed methods and evaluation criteria make sense.\n\n### Theoretical Claims\n\nI have reviewed all the proofs and found no issues.\n\n### Experimental Designs Or Analyses\n\nI have checked the experimental design. An issue of experiment in this paper is that the performance of full self-attention is not included. To the best of my knowledge, the full self-attention can be implemented via FlashAttention to avoid out-of-memory problem in WSI tasks.\n\n### Supplementary Material\n\nI have reviewed the computational complexity part of supp.\n\n### Relation To Broader Scientific Literature\n\nThe method of this paper may also be applied in other tasks with long-sequence modeling using Transformer, e.g. document-level language understanding and AI4Science tasks with long-sequence DNA, RNA.\n\n### Essential References Not Discussed\n\nI find that all the essential references are discussed.\n\n### Other Strengths And Weaknesses\n\nStrengths: The proposed method is novel and can highly speed up Transformer WSI modeling.\n\nWeakness: \n1) The claim on the relationship between 'Querent' and 'full self-attention' should be further discussed.\n2) The Flash-attention (with full self-attention) but linear-memory and quadratic time cost should be compared.\n3) A very important issue: There seems no explanation or motivation on why the method can improve the results. If the motivation is just like weakness 1), the weakness 2) should be validate in rebuttal. If full self-attention (implemented via Flash-attention) cannot reach the results just like Querent, how to explain it?\n\n### Other Comments Or Suggestions\n\nThe author should provide more discussion on Flash-attention. \nThe Flash-attention is of linear-memory cost and quadratic-time cost, but the time cost or speed was accelerated by their hardware operations optimization.\nDoes your method can be combined with it? How much of your method surpass it?\n\n### Questions For Authors\n\nNo further question.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces Querent, a query-aware dynamic modeling framework for analyzing whole-slide images in computational pathology. To address the computational inefficiency of standard transformer architectures, which struggle with the quadratic complexity of self-attention in large-scale WSI analysis, the authors propose a novel approach that dynamically adapts attention computation to the most relevant regions for each query patch. The framework includes: 1) Region-Level Metadata Summarization, 2) Query-Aware Attention Mechanism and 3）Efficient Importance Estimation. Experiments on biomarker prediction, gene mutation prediction, cancer subtyping, and survival analysis demonstrate that Querent achieves state-of-the-art performance while significantly reducing computational overhead, enabling efficient processing of gigapixel WSIs.\n\n### Claims And Evidence\n\nYes, the claims made in the manuscript are largely supported by clear and convincing evidence.\n\n### Methods And Evaluation Criteria\n\nYes, the proposed method(s) and/or evaluation criteria (e.g., benchmark datasets) are appropriately justified for the current problem or application.\n\n### Theoretical Claims\n\nYes, I reviewed the principles of the method proposed in the paper, primarily focusing on Query-Aware Attention Approximation. I examined the detailed derivation provided in the paper, and after referencing the cited literature (Kaban et al., 2015), I found the methodological derivation to be reasonable.\n\n### Experimental Designs Or Analyses\n\nYes, I have checked the validity of experimental designs. The experimental design and analysis conducted for the proposed method in the article are methodologically sound and empirically valid.\n\n### Supplementary Material\n\nI have reviewed the supplementary material in Appendix A.\n\n### Relation To Broader Scientific Literature\n\nThe paper introduces query-aware sparse attention, which dynamically selects relevant regions for each query patch, maintaining the expressive power of full self-attention while achieving near-linear computational complexity. This aligns with findings from prior studies (e.g., HIPT, Chen et al., 2022) that proposed various attention mechanisms and region selection strategies to improve computational efficiency and model performance in WSI analysis.\n\n### Essential References Not Discussed\n\nTo the best of my knowledge, I think the authors have already provided sufficient explanation and discussion.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n1.The paper introduces a novel query-aware attention mechanism that dynamically adapts to the context of each patch, addressing the computational bottleneck of standard transformers in large-scale WSI analysis.\n2.The paper provides theoretical guarantees for the query-aware attention mechanism, proving its error bounds in approximating full self-attention.\n\nWeaknesses:\nThe performance of the model depends on the quality of the region-level metadata. In the computation of region-level metadata, using min/max/mean/mean-std feature to summarize the patch features within a region may lead to the loss of important local information, especially in regions with high tissue heterogeneity or significant noise.\n\n### Other Comments Or Suggestions\n\nSome minor issues:\n1.The experiments did not leverage features from latest foundation models (e.g., UNI and CONCH). Incorporating these advanced features could potentially reduce the performance disparity between existing MIL methods with Querent.\n2.The region size in Querent significantly impacts performance, potentially limiting its generalizability across diverse WSI datasets. When applied to new datasets, it may require careful tuning, posing challenges for real-world applications.\n\n### Questions For Authors\n\n1.The method can visualize the original WSI corresponding to the K regions selected by the Querent and their min/max feature to prove the accuracy of the method. The author could provide some visualization results to demonstrate the effectiveness of the proposed method. For example, visualizing the metadata feature score for each region could help illustrate that the model indeed selects influential patches.\n2.It is not clear how the method can avoid the situation where the top K regions miss the regions containing key information.\n3.It is uncertain what the advantages of the proposed method are compared with the latest methods that can also achieve efficient and fast classification through Mamba (such as MambaMIL, MamMIL) in existing research. MamMIL can also perceive the topological structures among the instances and incorporate short-range feature interactions.\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces Querent, a framework for dynamic long-range contextual modeling of gigapixel WSIs through the adaptive determination of patch relationships. The key idea is to maintain the modeling power of full self-attention while achieving computational efficiency through dynamic sparsification. The method adaptively predicts which surrounding regions are most relevant for each patch, enabling focused yet unrestricted attention computation only with potentially important contexts. By using efficient region-wise metadata computation and importance estimation, their approach dramatically reduces computational overhead while preserving global perception to model fine-grained patch correlations. The effectiveness of the proposed method is validated on benchmark datasets, showing improvements over existing approaches.\n\n### Claims And Evidence\n\nThe authors claim that their method outperforms existing techniques in both efficiency and accuracy when analyzing WSIs. The experimental results presented partially support these claims, showing improvements in key metrics.\n\n### Methods And Evaluation Criteria\n\nThe proposed query-aware attention mechanism dynamically adapts to the unique context of each patch in gigapixel WSIs, preserving global attention while substantially reducing computational complexity. This results in enhanced computational efficiency, making it suitable for the intended application.\n\n### Theoretical Claims\n\nThe paper includes theoretical justifications for the proposed approach, particularly in the modeling techniques used. The proofs and derivations appear sound.\n\n### Experimental Designs Or Analyses\n\nThe experimental design and analyses are sound, offering lucid delineations of datasets, metrics, and methodologies. However, the comparative methods delineated in the paper appear to diverge from the tasks reported in the original studies. For instance, the performance of the RRT-MIL method, as reported on the TCGA-BRCA dataset, pertains to a sub-typing task, whereas this study utilizes the BRCA dataset for survival prediction. A comparative evaluation of the sub-typing task’s performance could enhance the persuasiveness of the experimental findings.\n\n### Supplementary Material\n\nThe supplementary material provides additional experimental results and technical details that complement the main text. This material enhances the paper's comprehensiveness and provides valuable insights for replication and further study.\n\n### Relation To Broader Scientific Literature\n\nThe paper builds upon existing work in sequence modeling, introducing novel adaptations for WSIs. It contributes to the literature by addressing specific challenges associated with WSI and proposing a method that integrates context-aware mechanisms.\n\n### Essential References Not Discussed\n\nThe paper covers relevant literature.\n\n### Other Strengths And Weaknesses\n\nStrengths:\n- The integration of query-aware mechanisms with dynamic sequence modeling in WSI analysis.\n- The paper is clearly written and well-structured.\n- The proposed methodology has yielded commendable performance across a diverse array of tasks and datasets.\n\nWeaknesses:\n- The primary contribution of the paper lies in its ability to reduce computational complexity while preserving global attention. As evidenced in Tables 1 and 2, the proposed methods outperform existing approaches; however, the analysis appears somewhat deficient. For instance, the absence of results derived from global attention computations raises questions about whether the superior performance of the proposed method stems predominantly from the MIL paradigm, the extraction of patch features via PLIP, or the novel Dynamic Attention mechanism introduced in the text.\n\n### Other Comments Or Suggestions\n\n- Some sections, particularly the theoretical derivations, could be elaborated for better clarity. For instance, the meaning of B in Theorem 3.1 should be promptly elucidated.\n- Exploring the integration of the proposed method with other advanced models, such as vision transformers, could be a valuable direction.\n\n### Questions For Authors\n\n- Should a comprehensive global attention mechanism be employed instead of this approximate variant, what impact might it have on performance? \n- Would the adoption of alternative patch feature extractors—such as CHIEF, UNI/UNI2, Virchow/Virchow2, PRISM, or GigaPath—yield analogous conclusions?\n\n### Overall Recommendation: 3\n\n### Ethical Review Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces Querent, a query-aware long contextual modeling framework for whole slide image (WSI) analysis, addressing the challenge of computational efficiency in gigapixel images. Unlike standard transformer architectures with quadratic complexity, Querent dynamically selects relevant regions for each patch using region-wise metadata summarization and importance estimation. This enables efficient self-attention while preserving long-range dependencies. The method outperforms existing approaches in biomarker prediction, gene mutation prediction, cancer subtyping, and survival analysis across multiple WSI datasets. Empirical results show that Querent achieves state-of-the-art accuracy while significantly reducing computational costs.\n\n## update after rebuttal\nI think the authors well address my concerns, so I will raise my score.\n\n### Claims And Evidence\n\nThe authors have demonstrated, through theoretical analysis and experimental validation, that their proposed query-aware attention mechanism possesses expressiveness comparable to that of full self-attention, while achieving greater computational efficiency. Moreover, the effectiveness of the region-level metadata summarization and importance estimation modules introduced by the authors has also been empirically substantiated.\n\n### Methods And Evaluation Criteria\n\nI believe that the methods and evaluation criteria proposed by the authors are well-aligned with the problem at hand.\n\n### Theoretical Claims\n\nUpon reviewing the authors' theoretical proofs, I have some reservations and cannot guarantee their complete accuracy.\n\n### Experimental Designs Or Analyses\n\nThe experimental design and analysis conducted by the authors are reasonably sound.\n\n### Supplementary Material\n\nI have specifically reviewed Appendices B, C, and G.\n\n### Relation To Broader Scientific Literature\n\nThe paper builds on MIL and transformer-based WSI analysis, addressing efficiency challenges seen in TransMIL and HIPT. While prior work uses linear approximations (Shao et al., 2021) or local-global attention (Chen et al., 2022), Querent introduces query-aware attention, dynamically selecting relevant regions, inspired by context-dependent tissue relationships (Heindl et al., 2015). This improves efficiency while maintaining long-range modeling, advancing adaptive sparse attention in pathology AI.\n\n### Essential References Not Discussed\n\nNo, there aren’t.\n\n### Other Strengths And Weaknesses\n\n### Strengths:\n\n1. The paper introduces query-aware attention, a novel approach to dynamically selecting relevant regions in WSIs, improving upon rigid local-global attention and linear approximations.\n2. By significantly reducing computational costs while maintaining long-range dependencies, Querent advances scalable WSI analysis, impacting biomarker prediction, cancer subtyping, and survival analysis.\n3. Strong performance across 11 datasets and multiple CPath tasks demonstrates robustness, outperforming state-of-the-art MIL and transformer-based models.\n\n### Weaknesses：\n\n1. The assumptions in the theoretical proofs may be challenging to satisfy in practical code implementation, which undermines their persuasiveness. For instance, the authors did not specify how to ensure that the neural networks $f_{min}$ and $f_{max}$ satisfy the L-Lipschitz continuity. Additionally, the fulfillment of the four conditions in Theorem B.6 during actual network training was not addressed.\n2. Could you provide a comparison of training and inference times between this method and other networks? How does the training convergence speed of this method fare? In practical inference for WSI, the most time-consuming part is likely the extraction of patch features, with aggregation taking up a relatively small portion of the time. How much does this method improve the overall inference time compared to other methods during inference?\n\n### Other Comments Or Suggestions\n\nIt appears that there is an error in Equation 12 (NLL survival loss) in the appendix. The second and third terms on the right side of the equation should be preceded by a minus sign rather than a plus sign. Additionally, the second term should be $y^{(i)}_{j-1}$ instead of $y^{(i)}_j-1$.\n\n### Questions For Authors\n\n1. In the second set of ablation experiments, why does the Estimation Side Network perform worse than the Random Region Selection, which serves as the lower bound?\n2. In the final step of Lemma B.4, since $q$ and $\\hat{q}$ are not in the same feature space, how can they be combined to yield the final result? Could you provide a detailed derivation?\n3. In the first set of ablation experiments and in Appendix G.1, it is mentioned that distance matrices are calculated. After summarization, each region has a feature vector, but before summarization, since a region contains multiple patches, it has multiple feature vectors. How, then, are the distance matrices calculated in this case? Could you elaborate?\n4. Is the $\\alpha$ in Theorem 3.1 the same as the $\\alpha$ in Equation 8？\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Hao Chen",
      "Jiabo MA",
      "Jinzhuo Wang",
      "Lishuang Feng",
      "Qichen Sun",
      "Zhengrui Guo"
    ],
    "url": "pdfs/icml.cc-2025-conference_ac652385f7d8b09a9e754dc7295d3e4c5be790c1.pdf",
    "remote_url": "https://openreview.net/pdf/ac652385f7d8b09a9e754dc7295d3e4c5be790c1.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "L-Diffusion: Laplace Diffusion for Efficient Pathology Image Segmentation",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications->computer_vision"
    ],
    "keywords": [
      "Diffusion Model",
      "Contrastive Learning",
      "Multi-label Segmentation",
      "Pathological slides"
    ],
    "abstract": "Pathology image segmentation plays a pivotal role in artificial digital pathology diagnosis and treatment. Existing approaches to pathology image segmentation are hindered by labor-intensive annotation processes and limited accuracy in tail-class identification, primarily due to the long-tail distribution inherent in gigapixel pathology images. In this work, we introduce the Laplace Diffusion Model, referred to as L-Diffusion, an innovative framework tailored for efficient pathology image segmentation. L-Diffusion utilizes multiple Laplace distributions, as opposed to Gaussian distributions, to model distinct components—a methodology supported by theoretical analysis that significantly enhances the decomposition of features within the feature space. A sequence of feature maps is initially generated through a series of diffusion steps. Following this, contrastive learning is employed to refine the pixel-wise vectors derived from the feature map sequence.  By utilizing these highly discriminative pixel-wise vectors, the segmentation module achieves a harmonious balance of precision and robustness with remarkable efficiency. Extensive experimental evaluations demonstrate that L-Diffusion attains improvements of up to  7.16\\%,  26.74\\%,  16.52\\%, and  3.55\\% on tissue segmentation datasets, and  20.09\\%,  10.67\\%,  14.42\\%, and  10.41\\% on cell segmentation datasets, as quantified by DICE, MPA, mIoU, and FwIoU metrics. The source are available at https://github.com/Lweihan/LDiffusion.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces L-Diffusion, a novel Laplace Diffusion Model designed for efficient pathology image segmentation. Unlike traditional approaches relying on Gaussian distributions, L-Diffusion employs multiple Laplace distributions to better differentiate component features in pathology images. The model follows a diffusion process, generating a sequence of feature maps, and enhances pixel-wise vector representations using contrastive learning. This approach significantly improves segmentation performance, particularly for tail-class components, which are often difficult to identify due to the long-tail distribution in pathology images. Extensive experiments on six tissue and cell segmentation datasets show that L-Diffusion achieves substantial improvements over state-of-the-art models, with up to 7.16% higher DICE score for tissue segmentation and 20.09% improvement for cell segmentation. The paper also provides theoretical analysis supporting the advantages of using Laplace distributions, demonstrating their ability to enhance component differentiation and model efficiency.\n\n### Claims And Evidence\n\nThe claims made in the paper are well-supported by both theoretical analysis and experimental results. The authors claim that using Laplace distributions instead of Gaussian distributions enhances feature decomposition and improves segmentation accuracy, particularly for tail-class components. This claim is backed by mathematical derivations and empirical comparisons of Gaussian vs. Laplace distribution differentiation, which show that the latter leads to greater separability of pixel-wise feature vectors.\n\n### Methods And Evaluation Criteria\n\nThe proposed L-Diffusion method and evaluation criteria are well-suited for pathology image segmentation. The use of Laplace distributions enhances feature differentiation, and contrastive learning improves pixel-wise representation, addressing challenges in long-tail class segmentation. The model is evaluated on six benchmark datasets covering both tissue and cell segmentation, ensuring a comprehensive assessment. Metrics such as DICE, MPA, mIoU, and FwIoU are appropriate for measuring segmentation accuracy and robustness.\n\n### Theoretical Claims\n\nThe paper provides theoretical justification for using Laplace distributions over Gaussian distributions, arguing that the steeper gradient of the Laplace distribution enhances feature differentiation. The derivations, including probability density functions, gradient calculations, and diffusion step equations, appear mathematically sound. The Laplace noise formulation and reverse diffusion process are derived systematically, and the transition from Gaussian to Laplace-based modeling is well-supported. While I did not rigorously verify every step, the overall framework aligns with established diffusion model theory. No obvious errors were found, but external validation would further confirm the proofs' correctness.\n\n### Experimental Designs Or Analyses\n\nThe experimental design is generally sound and appropriate for pathology image segmentation. The model is tested on six diverse datasets, covering both tissue and cell segmentation, ensuring broad applicability. Metrics such as DICE, MPA, mIoU, and FwIoU are correctly chosen for evaluating segmentation performance. The comparison with state-of-the-art models is comprehensive, showing clear performance improvements. Ablation studies confirm the contributions of Laplace distributions and contrastive learning, and a generalization test on a remote sensing dataset suggests potential broader applicability. A minor limitation is the lack of evaluation on other medical imaging domains, which could further validate the model’s versatility.\n\n### Supplementary Material\n\nThe supplementary material includes mathematical derivations, additional dataset details, and extended visualizations of segmentation results. The theoretical derivations justify the use of Laplace distributions in the diffusion process, appearing logically sound. The dataset details provide transparency about sample sizes and categories, supporting the validity of experiments. Additional visualizations illustrate segmentation performance and latent feature distributions, reinforcing the model’s improvements. While I reviewed these key sections, I did not verify every mathematical step in detail. No major issues were found, but external validation of derivations would strengthen confidence in the theoretical claims.\n\n### Relation To Broader Scientific Literature\n\nThe paper builds on diffusion models and contrastive learning, extending them to pathology image segmentation. Traditional segmentation models, such as U-Net, DeepLab, and Transformers, have been widely used, but they struggle with long-tail class segmentation and multi-scale feature extraction. The introduction of Laplace distributions instead of Gaussian distributions aligns with prior work on improving feature separability in latent spaces. Contrastive learning, which has been effective in self-supervised learning, is adapted here to enhance pixel-wise feature differentiation. The paper also connects to recent efforts in medical image segmentation using diffusion models, such as MedSegDiff [1], but uniquely applies component-wise latent distribution modeling. \n\n[1] Wu, Junde, et al. MedSegDiff: Medical image segmentation with diffusion probabilistic model.\n\n### Essential References Not Discussed\n\nThe paper provides a comprehensive discussion of related literature, covering key works in diffusion models, contrastive learning, and pathology image segmentation. It cites foundational methods such as U-Net, DeepLab, and Transformer-based models, as well as recent diffusion-based segmentation approaches like MedSegDiff. The discussion on Laplace distributions and their role in enhancing feature separability is well-supported by prior probabilistic modeling research. Additionally, the application of contrastive learning to pathology image segmentation is contextualized within the broader field of self-supervised learning. Overall, the references are sufficient, and no essential prior works appear to be missing.\n\n### Other Strengths And Weaknesses\n\n### Strengths\n\n1. The paper introduces a novel approach by replacing Gaussian distributions with Laplace distributions, improving feature separability for pathology image segmentation.\n2. The use of pixel latent vector contrastive learning enhances segmentation accuracy, especially for tail-class components, addressing long-tail distribution challenges.\n3. L-Diffusion outperforms state-of-the-art segmentation models on six benchmark datasets, demonstrating significant improvements in both tissue and cell segmentation.\n4. The mathematical derivations provide a solid foundation for Laplace-based diffusion modeling, supporting the claimed improvements in feature differentiation.\n\n### Weaknesses\n\n1. While the model performs well on pathology images, how would it generalize to other medical imaging tasks such as radiology or ophthalmology?\n2. The proposed diffusion process involves multiple steps; can the authors provide runtime comparisons with other state-of-the-art segmentation models to assess efficiency?\n3. The model introduces multiple hyperparameters (e.g., diffusion steps, contrastive learning settings). How sensitive is the performance to these hyperparameters, and could the authors provide guidelines for optimal tuning?\n\n### Other Comments Or Suggestions\n\nThe second sentence in the third paragraph of the Introduction seems a bit unclear.\n\n### Questions For Authors\n\nPlease see the weaknesses part above.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a new diffusion-based method to tackle pathology image segmentation. The pathology image segmentation is a challenging task because of the large, gigapixel resolution, diverse scales, and imbalanced tissue distributions in these images.  Traditional segmentation models like U-Net and DeepLab have shown promise but struggle with labor-intensive annotation and feature extraction for tail categories. To address these issues, the paper introduces the Laplace Diffusion Model (L-Diffusion), which leverages Laplace distributions instead of Gaussian to model distinct components within the images. L-Diffusion uses contrastive learning to enhance the differentiation between components while maintaining intra-component similarity, improving segmentation precision and robustness, particularly for tail components. The model reduces the reliance on annotated data by capturing the distributional characteristics of different components. Extensive experiments show that L-Diffusion outperforms existing methods in accuracy and robustness across various benchmarks, providing an innovative approach to pathology image segmentation.\n\n### Claims And Evidence\n\nBased on the challenges summarized in the submission: \"current pathology image segmentation tasks grapple with labor-intensive annotation processes or limited accuracy in identifying tail samples\", there are two main claims made by the submission:\n(1). Laplace distribution is advantageous for broadening distribution disparities based on the analysis in the submission.\n(2). Laplace diffuse model learns better component distribution  for pathology image segmentation, due to it's long-tail nature. \nThese two claims are well supported by the method theory and experiments.\n\n### Methods And Evaluation Criteria\n\nThe proposed method has been evaluated in the context of pathology image segmentation on a series of benchmarking datasets: CRCD , PUMA, BCSS, against a series of state of the art baseline methods\n\n### Theoretical Claims\n\n1. The proofs in Section 3.1 for computing the gradient of Laplace distribution is correct. \n2. The methmetical dereivation in supplementary material A. is verified.\n\n### Experimental Designs Or Analyses\n\nThere are 3 main experiments conducted for evaluating the effectiveness of the proposed method: (1) Quantitative evaluations agains baseline methods (2) Ablation study (3) Generalization on large-scale data. \n\nThe quantitative evaluations are made comprehensively with many state of the art methods and various metrics. The proposed method seems to get a decent gain consistently on each of the benchmarking dataset.\n\n### Supplementary Material\n\nThe entire supplementary material, especially part A for mathematical derivation of the equation and part C D, E for more results and visualizations.\n\n### Relation To Broader Scientific Literature\n\nThe proposed Laplace model can be applied to a broader domains such as image segmentation e-commerce product data, which also shows a long-tail distribution.\n\n### Essential References Not Discussed\n\nN.A.\n\n### Other Strengths And Weaknesses\n\nStrengths\n1. The reasoning in Section 3.1 is sound in term of showing the comparison between Laplace distribution gradients and Guassian distribution gradients. \n2. The experiments are comprehensive and the code is available anonymously. The experiment is compare a large number of baseline methods and the proposed method is outperforming all the previous methods consistently.\n\n### Other Comments Or Suggestions\n\n1. The equations in the paper section 3 is not numbered, making it difficult for reference.\n\n### Questions For Authors\n\nThe authors are suggested to address the concerns listed in the previous sections during rebuttal period.\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces L-Diffusion, an innovative framework designed to advance pathology image segmentation by utilizing Laplace distributions and contrastive learning.  The primary contribution of the paper lies in its use of Laplace distributions to model distinct components within pathology images, which enhances distributional divergence and facilitates more precise and robust segmentation.  \nThe novel pixel latent vector contrastive learning mechanism further reduces reliance on annotated data, addressing the challenges associated with long-tail components.  The approach significantly improves segmentation performance on tissue and cell datasets, showing substantial gains over existing methods.\n\n### Claims And Evidence\n\nYes, the claims made in the submission are generally supported by clear and convincing evidence.   The authors provide both theoretical analysis and extensive experimental evaluations to substantiate their claims about the effectiveness of the L-Diffusion framework for pathology image segmentation.  \n\n- Theoretical Analysis:\nThe paper clearly explains the rationale behind using Laplace distributions for component modeling, as opposed to Gaussian distributions.   It presents mathematical derivations comparing the gradients of the two distributions, showing that Laplace distributions are more sensitive to noise, which is beneficial for pathology image segmentation tasks.\n\n- Experimental Evidence:\nThe paper includes quantitative results demonstrating the improvements in segmentation performance across various tissue and cell datasets (CRCD, PUMA, BCSS, PanNuke) compared to several state-of-the-art models.   The reported results are statistically significant, showing substantial performance gains (e.g., improvements in DICE, MPA, mIoU, and FwIoU metrics).\nThe qualitative visualizations further support these findings, showing that L-Diffusion achieves better boundary segmentation and handles tail-class components more effectively.\n\n- Comparison to Existing Methods:\nThe authors compare L-Diffusion with several mainstream segmentation models (e.g., U-Net++, DeepLab, Swin-UNet) and show that their model consistently outperforms these methods, especially in terms of segmenting components with lower proportions (long-tail distribution).\n\n- Ablation Studies:\nThe paper conducts detailed ablation studies to show the importance of each component of L-Diffusion, particularly the integration of the Laplace distribution and contrastive learning, which provides additional evidence.\n\n### Methods And Evaluation Criteria\n\nYes, the proposed methods and evaluation criteria are well-suited for pathology image segmentation.  Using Laplace distributions to model distinct components effectively addresses multi-scale features and long-tail distributions in pathology images, enhancing segmentation precision, especially for rare components.  The integration of contrastive learning further refines component differentiation while reducing reliance on annotated data.  The use of diffusion steps to refine feature maps and latent vectors contributes to more accurate segmentation.  \n\nThe evaluation metrics (DICE, MPA, mIoU, and FWIoU) are standard and effective, assessing various aspects of segmentation performance, including accuracy, precision, and class imbalance.  Qualitative visualizations complement these metrics by showing better boundary segmentation, particularly for tail-class components.  \n\nThe benchmark datasets, such as CRCD, PUMA, BCSS, and PanNuke, cover diverse pathology segmentation tasks, while the inclusion of a remote sensing dataset for generalization tests demonstrates the method's robustness across different domains.  Overall, the proposed methods and evaluation criteria are appropriate and robust for addressing the challenges in pathology image segmentation.\n\n### Theoretical Claims\n\nYes, I focused on the rationale behind applying Laplace distributions to pathology image segmentation, particularly in terms of the formulaic principles. This is thoroughly addressed in Section A, \"Mathematical Derivations\", in the appendix, and the visualization of the differences between the Laplace and Gaussian distributions is commendable. It provides convenience for readers when interpreting the paper. The formal proof and the visualizations in the paper demonstrate that the probability change range $\\Delta y$ of the Laplace distribution is larger than that of the Gaussian distribution $\\Delta y'$, which aligns well with the segmentation strategy for pathology images. Additionally, the introduction of contrastive learning, which brings similar class samples closer and pushes different class samples apart, is a sound approach both conceptually and methodologically.\n\n### Experimental Designs Or Analyses\n\nYes, the experimental designs and analyses appear sound and provide strong evidence for the claims made in the paper. \n\n1. Benchmark Datasets:\nThe paper uses a variety of benchmark datasets (e.g., CRCD, PUMA, BCSS, and PanNuke) for testing the model. These datasets cover different aspects of pathology image segmentation, including tissue and cell segmentation, and represent diverse challenges such as multi-scale features and long-tail distributions. The choice of datasets is appropriate for testing the model's generalizability across multiple types of pathology image segmentation tasks.\n\n2. Comparison with SOTAs:\nThe paper compare L-Diffusion with a variety of SOTA methods (e.g., U-Net, DeepLabv3, Swin-UNet, FastFCN), which is a solid approach to demonstrate the advantages of their proposed method. The reported quantitative results show significant improvements across tissue and cell segmentation datasets, providing strong evidence of the method’s effectiveness.\n\n3. Qualitative and Quantitative Results:\nThe qualitative results (visualizations) demonstrate that L-Diffusion performs well in segmenting boundaries and handling tail-class components, which aligns with the claims made by the authors. The quantitative results (based on the metrics mentioned) show clear improvements over existing methods, further supporting the paper’s claims of better performance.\n\n4. Ablation Studies:\nThe paper conduct ablation studies to isolate the impact of key components in their model, such as the integration of Laplace distributions and contrastive learning. This is an essential and well-designed experiment, as it provides insights into which parts of the model contribute most to its success. The ablation results confirm that the combination of these two techniques is a major factor in the model's improved performance.\n\n### Supplementary Material\n\nYes, I have read the entire supplementary materials section. In summary, the supplementary materials provide the theoretical proofs related to L-Diffusion (which I believe is the most important part). Upon review, I found that the proof section starts with well-established diffusion frameworks such as DDPM and gradually derives a Laplace-based pathology image segmentation method. The reasoning behind the proof seems correct to me. Additionally, I would like to praise the authors for providing segmentation results at various levels of L-Diffusion in the appendix, and extending the approach to remote sensing images, which further demonstrates the robustness of the method.\n\n### Relation To Broader Scientific Literature\n\nThe contributions of this paper are built upon a broad foundation of existing research, spanning multiple areas such as pathology image segmentation, diffusion models, and contrastive learning. The practical application of L-Diffusion is positioned within the realm of pathology image segmentation, following the Diffusion + Pathology paradigm. Interestingly, the authors have innovatively approached the problem by utilizing component distributions across diffusion steps, which not only fine-tunes the decomposition of pathological semantic information but also enhances the richness of the data through the diffusion process. This represents a significant innovation in addressing pathology image segmentation challenges. Furthermore, the introduction of contrastive learning aligns with ideas from multimodal learning and other related fields.\n\nOverall, the L-Diffusion model presented in this paper is convincing in its relation to the broader scientific literature, adhering to sound research principles. The novel application of the Laplace diffusion process adds a substantial innovation within the field, making the approach a noteworthy contribution to the domain.\n\n### Essential References Not Discussed\n\nIn my personal opinion, this paper provides a thorough introduction to the algorithms and ideas involved in the relevant work section and experimental implementation. The approach is innovative, and there are no instances of withholding key literature that would be crucial to the paper's significance.\n\n### Other Strengths And Weaknesses\n\nAs mentioned earlier, this paper overall meets the standards for publication at ICML. I believe the authors' L-Diffusion approach also offers valuable insights for feature engineering implementation.\n\n### Other Comments Or Suggestions\n\nI personally suggest that the Related Work section be placed after the Introduction to help readers quickly engage with the core implementation ideas.At the same time, the specific meaning of the abbreviation is given in the dataset section of the appendix, so that the reader can better understand the processing effect of the segmentation model on different categories.\n\n### Questions For Authors\n\nN/A\n\n### Overall Recommendation: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces L-Diffusion, a novel approach to pathology image segmentation by leveraging Laplace distributions and contrastive learning, achieving good performance, and demonstrating generalization capabilities.\n\n### Claims And Evidence\n\nSupported Claims:\n\n1) $\\textbf{Laplace Distributions Improve Segmentation.}$ Laplace distributions are more effective than Gaussian distributions for modeling pathology image components, especially for tail categories. The authors provide a detailed theoretical analysis comparing the gradients of Laplace and Gaussian distributions, showing that Laplace distributions have sharper gradients and better separation between categories. Empirical results, including visual comparisons of pixel value distributions (Figure 2) and quantitative improvements in segmentation metrics (Tables 1 and 2), further support this claim.\n\n2) $\\textbf{Contrastive Learning Enhances Component Separation.}$ The proposed pixel latent vector contrastive learning mechanism improves the separation of different tissue and cellular components. The ablation studies (Table 4) demonstrate that combining contrastive learning with Laplace distributions significantly improves segmentation performance. The visualization of pixel latent vector separation (Figure 7) also provides qualitative evidence of the effectiveness of contrastive learning.\n\n3) $\\textbf{Superior Performance on Pathology Image Segmentation.}$ L-Diffusion achieves state-of-the-art performance on multiple pathology image segmentation benchmarks. The paper presents extensive quantitative results (Tables 1 and 2) showing that L-Diffusion outperforms existing methods on datasets such as CRCD, PUMA, BCSS, and PanNuke. The improvements in metrics like DICE, MPA, mIoU, and FwIoU are substantial and well-documented.\n\n4) $\\textbf{Generalization to Remote Sensing Images.}$ L-Diffusion generalizes well to other large-scale image segmentation tasks, such as remote sensing. The authors provide quantitative results (Table 5) and qualitative visualizations (Figure 11) showing that L-Diffusion performs competitively on the Massachusetts-Building dataset, a remote sensing image segmentation task.\n\nClaims That Could Benefit from Additional Evidence:\n\n1) $\\textbf{Efficiency of L-Diffusion.}$ L-Diffusion is efficient and reduces the dependency on annotated data. While the paper mentions that L-Diffusion can achieve competitive performance with limited annotated data (Table 3), it does not provide a detailed analysis of the computational cost or training time compared to other methods. Diffusion models are generally computationally expensive, and it would be valuable to understand how L-Diffusion compares in terms of resource requirements. Additionally, a sensitivity analysis on the number of diffusion steps (T) and its impact on performance and computational cost would strengthen this claim.\n\n2) $\\textbf{Applicability to Other Medical Imaging Modalities.}$ L-Diffusion is broadly applicable to medical image segmentation tasks. The paper focuses on pathology and remote sensing images but does not explore the model's performance on other medical imaging modalities, such as MRI or CT. Extending the experiments to these domains would provide stronger evidence for the model's generalizability.\n\n3) $\\textbf{Robustness to Noisy or Imperfect Annotations.}$ L-Diffusion is robust and can handle long-tail distributions effectively. While the paper demonstrates strong performance on tail components, it does not explicitly test the model's robustness to noisy or imperfect annotations, which are common in medical imaging. Including experiments with noisy labels or partial annotations would further validate the model's robustness.\n\n4) $\\textbf{Ethical and Clinical Impact.}$ L-Diffusion provides a powerful tool for advancing tumor diagnosis and microenvironment analysis. The paper briefly mentions ethical approval for the datasets but does not discuss the broader ethical implications or clinical impact of using L-Diffusion in real-world medical settings. A more detailed discussion of potential risks, limitations, and guidelines for deployment would strengthen this claim.\n\n### Methods And Evaluation Criteria\n\nYes, the proposed methods and evaluation criteria in the paper are well-suited for the problem of pathology image segmentation and the broader application of medical image analysis.\n\n### Theoretical Claims\n\nThe theoretical claims and proofs in the paper are generally correct and well-supported by mathematical derivations. The authors effectively demonstrate why Laplace distributions are more suitable than Gaussian distributions for pathology image segmentation, and they provide a solid theoretical foundation for the proposed L-Diffusion model.\n\n### Experimental Designs Or Analyses\n\nThe authors evaluate L-Diffusion on several well-established pathology image datasets, including CRCD (colorectal cancer), PUMA (melanoma), BCSS (breast cancer), and PanNuke (multi-class cellular segmentation). These datasets are representative of the challenges in pathology image segmentation, such as gigapixel resolution, multi-scale features, and long-tail distributions.\n\nThe paper uses standard evaluation metrics for segmentation tasks, including DICE, MPA (Mean Pixel Accuracy), mIoU (Mean Intersection over Union), and FwIoU (Frequency Weighted IoU). These metrics are widely accepted in the medical imaging community and provide a balanced assessment of segmentation performance.\n\nThe authors compare L-Diffusion with a variety of state-of-the-art methods, including U-Net++, Swin-UNet, DeepLabv3, and SAMPath. \n\nThe paper does not provide a detailed analysis of the computational cost or training time compared to other methods. Diffusion models are generally computationally expensive, and it would be valuable to understand how L-Diffusion compares in terms of resource requirements.\n\nThe paper does not discuss the sensitivity of L-Diffusion to key hyperparameters, such as the scale parameter (b) in the Laplace distribution.\n\n### Supplementary Material\n\nI reviewed all the supplementary materials\n\n### Relation To Broader Scientific Literature\n\nMedical image segmentation, particularly in pathology, has been extensively studied using deep learning models such as U-Net (Ronneberger et al., 2015), DeepLab (Chen et al., 2017), and more recently, Transformers (Atabansi et al., 2023). These models have shown success in segmenting tissues and cells in pathology images, but they often struggle with long-tail distributions and multi-scale features. L-Diffusion addresses these challenges by introducing Laplace distributions and contrastive learning to enhance the separation of different components, particularly tail categories. This builds on prior work by providing a novel approach to handling the inherent complexities of pathology images, such as gigapixel resolution and imbalanced tissue distributions.\n\nDiffusion models, such as Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020), have gained popularity for their ability to model complex data distributions. These models have been applied to various tasks, including image generation, denoising, and segmentation. However, most prior work in diffusion models uses Gaussian distributions for noise modeling. L-Diffusion introduces Laplace distributions as an alternative to Gaussian distributions in diffusion models. The authors argue that Laplace distributions provide sharper gradients and better separation between different categories, making them more suitable for pathology image segmentation. This is a novel contribution that extends the applicability of diffusion models to medical imaging tasks with long-tail distributions.\n\nContrastive learning has emerged as a powerful technique in self-supervised learning, particularly in computer vision (Chen et al., 2020). It has been applied to various tasks, including image classification, object detection, and segmentation. In medical imaging, contrastive learning has been used to improve feature representations and reduce the dependency on annotated data. L-Diffusion incorporates pixel latent vector contrastive learning to enhance the separation of different tissue and cellular components. This builds on prior work by applying contrastive learning to the latent space of diffusion models, which is a novel approach. The authors demonstrate that contrastive learning significantly improves segmentation performance, particularly for tail components, by amplifying the distributional differences between different categories.\n\n### Essential References Not Discussed\n\nI think the Laplace noise was previously found to be better schedule for image generation. (See https://arxiv.org/abs/2407.03297  and https://arxiv.org/abs/2304.05907)\n\n### Other Strengths And Weaknesses\n\nsee above\n\n### Other Comments Or Suggestions\n\nnot found\n\n### Questions For Authors\n\nWhile the paper focuses on Laplace distributions, have the authors explored other distributions, such as Cauchy or Student's t, for modeling pathology image components? If so, how do these distributions compare to Laplace distributions in terms of segmentation performance?\n\nHow does L-Diffusion compare to unsupervised or semi-supervised methods for pathology image segmentation, particularly in scenarios with limited annotated data? Have the authors explored unsupervised or semi-supervised variants of L-Diffusion?\n\n### Overall Recommendation: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "ChaoqingXu",
      "Jing Zhang",
      "Linyun Zhou",
      "Mingli Song",
      "Shengxuming Zhang",
      "Weihan Li",
      "Xiangtong Du",
      "Xiuming Zhang",
      "YangJian",
      "Zunlei Feng"
    ],
    "url": "pdfs/icml.cc-2025-conference_b21c48b00076579c2208d420c271adb587f7076a.pdf",
    "remote_url": "https://openreview.net/pdf/b21c48b00076579c2208d420c271adb587f7076a.pdf",
    "venue_id": "icml.cc-2025-conference"
  },
  {
    "title": "DiffusionBlend: Learning 3D Image Prior through Position-aware Diffusion Score Blending for 3D Computed Tomography Reconstruction",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "diffusion_based_models"
    ],
    "keywords": [
      "diffusion models",
      "CT reconstruction",
      "inverse problems"
    ],
    "abstract": "Diffusion models face significant challenges when employed for large-scale medical image reconstruction in real practice such as 3D Computed Tomography (CT).\nDue to the demanding memory, time, and data requirements, it is difficult to train a diffusion model directly on the entire volume of high-dimensional data to obtain an efficient 3D diffusion prior. \nExisting works utilizing diffusion priors on single 2D image slice with hand-crafted cross-slice regularization would sacrifice the z-axis consistency, which results in severe artifacts along the z-axis. \nIn this work, we propose a novel framework that enables learning the 3D image prior through position-aware 3D-patch diffusion score blending for reconstructing large-scale 3D medical images. To the best of our knowledge, we are the first to utilize a 3D-patch diffusion prior for 3D medical image reconstruction. \nExtensive experiments on sparse view and limited angle CT reconstruction\nshow that our DiffusionBlend method significantly outperforms previous methods\nand achieves state-of-the-art performance on real-world CT reconstruction problems with high-dimensional 3D image (i.e., $256 \\times 256 \\times 500$). Our algorithm also comes with better or comparable computational efficiency than previous state-of-the-art methods. Code is available at https://github.com/efzero/DiffusionBlend.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a novel method for learning 3D diffusion priors for CT reconstruction, which does not require large-scale data or computational resources. It presents two approaches: DiffusionBlend and DiffusionBlend++. The former learns a specific frame given adjacent slices, while the latter learns a 3D patch. Experimental results demonstrate that both methods are efficient and outperform previous works.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. This method enables 3D reconstruction without the need for large-scale data and resources.\n2. It achieves excellent inter-slice consistency without relying on external regularizations.\n3. Additionally, this method outperforms existing baselines.\n\n### Weaknesses\n\nThe only concern is that this method may be too simplistic to be extended to a broader field. For example, I believe that inter-slice smoothness cannot be guaranteed if this method is applied to videos. However, given that this method is proposed for CT reconstruction, I don't think this issue warrants the rejection of the paper.\n\n### Questions\n\nI don't have questions about this paper.\n\n### Limitations\n\nThe authors have discussed the limitations.\n\n### Flag For Ethics Review\n\n- Ethics review needed: Data privacy, copyright, and consent\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\n1. The method proposed a method that learns the 3D-patch image prior incorporating the cross-slice dependency.\n2. The method achieves state-of-the-art reconstruction results for 3D volumetric imaging for the task of ultra-sparse-view and limited-angle\n3D CT reconstruction on  \"AAPM 2016 CT challenge\" dataset and \"LIDC-IDRI\" dataset.\n\n### Soundness: 1\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The probabilistic modeling of the 3D volume taking into account neighboring slices is novel.\n2. The paper describes the author's motivation and implementation plan very well.\n\n### Weaknesses\n\n1. Sparse angle CT imaging is typically defined as using fewer than 100 angles. The paper should evaluate the method's performance at various angles (e.g., 20, 40, 60, 80, and 100 angles). Demonstrating that this method outperforms the comparison method in PSNR/SSIM at fewer than 10 angles does not necessarily indicate better performance at other sparse angles (e.g., 20, 40, 60, 80, and 100 angles).\n\n2. The comparison should include classic CT reconstruction methods, not just FBP, as FBP lacks prior information and performs poorly among traditional methods. Suggested comparison methods include:\n2.1 SIRT (Simultaneous Iterative Reconstruction Technique) algorithm\n2.2 Conjugate Gradient Least Squares (CGLS) algorithm\n2.3 Split-Bregman (SB) Total Variation: Goldstein, T. and Osher, S., 2009. The split Bregman method for L1-regularized problems. SIAM journal on imaging sciences, 2(2)\nBecause the traditional iterative method also involves the adjustment of hyperparameters, for a fair comparison, please first use grid search to search the hyperparameters of the traditional method to obtain the best performance of the traditional method, and then compare the performance of the traditional method with the performance of this paper. Also, please report the extent of the grid search. Only in this way can it can be fully evaluated in the numerous algorithms for CT image reconstruction.\n\n3. Figure 3 shows a large number of structures that do not exist in CT images, which will interfere with the doctor's diagnosis. Please explain the reasons for this phenomenon and how to avoid it. (For example, increase the number of angles, which is why I am very concerned about increasing the angles). If increasing the angles can avoid the artifacts that come with the generative model on a large scale, the potential for medical applications will increase.\n\n4. As the abstract said, the method's purpose is to decrease the cost of memory, time. In order to prove that this method has achieved the original intention of it, please show the time consuming and memory consuming during training phase and testing phase. And compare the memory and time consuming between different methods.\n\n### Questions\n\n1. If the \"conditioned slices\" in equation (2) increase(\"conditioned slices\" is: x[:,:,i-j:i-1] and x[:,:,i+1:i+j]), will the numerical performance better? If it is better, how does the performance gain change as the number of adjacent slices increases?\n\n2. The paper mentions using a different partition so that the previous border slices can be included in another partition. However, if the method creates a new partition and then uses this new partition to create 3D patches, the adjacent slices of different 3D patches still cannot be updated simultaneously when the algorithm attempts to update the slices based on the scores calculated in each 3D patch. So, my question is, if the method uses the joint distribution modelling method (Eq. (5)), how can it avoid the situation I mentioned?\n\n3.  For the experimental part,  The reconstruction mentioned here is the reconstruction from the simulated projection to the 3D CT volume or the reconstruction  from  the real CT projection(real measurement) to the 3D CT volume? If it is a simulated projection, is simulated noise added to the simulated projection? (Note: The noise I am referring to here is not the noise added in the diffusion model to train the neural network, but the noise added to simulate the real projection, which is generally a combination of Poisson noise and Gaussian noise.)\n\n### Limitations\n\nThe artifacts generated by the diffusion model may lead to misjudgment in the doctor's diagnosis. Please provide methods to avoid artifact, such as increasing the angle or the amount of training data?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper used the diffusion model to solve the widely known inverse problem in computed tomography. The paper is well-written and well-organized. \nI reckon this paper has two main contributions:\n1. The first diffusion model in CT considered z-axis consistency and used 3d data as neural network input. \n2. A SOTA results with astonishing visualization and metrics results.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The first diffusion model in CT considered z-axis consistency and used 3d data as neural network input. \n2. A SOTA results with astonishing visualization and metrics results.\n\n### Weaknesses\n\n1. I admit this is the first paper I read using the diffusion model while considering z-axis consistency. However, the y-axis has been widely discussed and solved using various types of regularization between 2D slices or just using 3D data before. I consider the author just used a 3D way here to solve this point. Yeah, it is new for the diffusion model, but I do think it is less attractive to me considering the works already made before. \n2. The results achieved by the authors, particularly in sparse-view reconstruction using only four angles, are indeed remarkable. The quality of the reconstruction with such limited data is surprising and commendable. Given the significance of these results, it would be beneficial for the community if the authors consider sharing their code upon acceptance to enable further research and validation.\n3. The paper's novelty is somewhat overshadowed by the work \"Solving Inverse Problems in Medical Imaging with Score-Based Generative Models,\" which the authors have cited. It would be beneficial for the authors to clearly state the innovations and contributions of their work, especially in light of the foundational problem settings of inverse problem solving for medical imaging already addressed by the cited paper.\n\n### Questions\n\n1. What is your window center and width for CT images in the main body of the paper? I think the current window width value you selected for the main part of the paper is quite wide, considering using a narrower one. I think some details are not so clear in this setting, though I can tell your result is better than others.\n\n### Limitations\n\n1. Although the author used some way to speed up the process both for training and sampling. However, diffusion models are still slow compared to other methods both traditional and deep learning and non-diffusion way.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 7\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Bowen Song",
      "Jason Hu",
      "Liyue Shen",
      "Zhaoxu Luo",
      "Jeffrey A Fessler"
    ],
    "url": "pdfs/neurips.cc-2024-conference_9b112d91fdd7998d7f8ecad87a812525eedd2554.pdf",
    "remote_url": "https://openreview.net/pdf/9b112d91fdd7998d7f8ecad87a812525eedd2554.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "CryoGEM: Physics-Informed Generative Cryo-Electron Microscopy",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Bernhard"
    ],
    "primary_area": [
      "generative_models"
    ],
    "keywords": [
      "Image Synthesis",
      "Contrastive Learning",
      "Cryo-EM"
    ],
    "abstract": "In the past decade, deep conditional generative models have revolutionized the generation of realistic images, extending their application from entertainment to scientific domains. Single-particle cryo-electron microscopy (cryo-EM) is crucial in resolving near-atomic resolution 3D structures of proteins, such as the SARS-COV-2 spike protein. To achieve high-resolution reconstruction, a comprehensive data processing pipeline has been adopted. However, its performance is still limited as it lacks high-quality annotated datasets for training. To address this, we introduce physics-informed generative cryo-electron microscopy (CryoGEM), which for the first time integrates physics-based cryo-EM simulation with a generative unpaired noise translation to generate physically correct synthetic cryo-EM datasets with realistic noises. Initially, CryoGEM simulates the cryo-EM imaging process based on a virtual specimen. To generate realistic noises, we leverage an unpaired noise translation via contrastive learning with a novel mask-guided sampling scheme. Extensive experiments show that CryoGEM is capable of generating authentic cryo-EM images. The generated dataset can be used as training data for particle picking and pose estimation models, eventually improving the reconstruction resolution.",
    "reviews": [
      {
        "text": "### Summary\n\nIn this paper, the authors introduce Physics-Informed Generative Cryo-Electron Microscopy (CryoGEM), a novel generative model for cryo-electron microscopy (Cryo-EM) micrographs. CryoGEM is trained to produce micrographs that accurately replicate the ice gradient, point spread function (PSF), and noise characteristics of experimental micrographs. The model offers two main applications in Cryo-EM analysis through the annotations provided by the generated micrographs: a) The generated micrographs include precise positional annotations for particles (2D projections of proteins in the micrograph). b) In addition to positional information, CryoGEM provides data on particle orientations and conformations. This information can be utilized with methodologies such as CryoFIRE to train models that distinguish particle orientations and conformations in experimental micrographs.\nThe authors propose this innovative generative model to assist Cryo-EM data analysts in a) Fine-tuning particle picking models, and b) Training deep learning models for heterogeneous 3D reconstruction. This approach has the potential to significantly enhance the particle-picking process in experimental micrographs, ultimately leading to improved 3D reconstructed volumes of proteins.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nOriginality: CryoGEM presents an innovative approach by integrating physics-informed modeling with generative techniques, addressing a significant gap in cryo-EM analysis.\nQuality: The study features well-designed experiments that demonstrate the model's capabilities. The application of CryoFIRE provides additional validation of CryoGEM's practical utility.\nClarity: The paper is articulated precisely, offering detailed explanations and a well-structured layout that enhances reader comprehension.\nSignificance: CryoGEM shows considerable potential to impact cryo-EM analysis significantly. It provides valuable tools that can improve particle picking accuracy and the quality of 3D reconstruction processes, potentially advancing structural biology research.\n\n### Weaknesses\n\nCode Availability: Currently there is no code availability, which may limit the accessibility of CryoGEM to the broader research community.\n\nResolution and Time Requirements: There is a lack of detailed discussion on the resolution of the initial coarse cryo-EM density map obtained from the ab-initio reconstruction of CryoSPARC, as well as the time required for this process.\n\nPipeline Efficiency: The time consumption for fine-tuning Topaz through CryoGEM raises concerns about the practicality of the proposed pipeline compared to manually picking a small number of micrographs.\n\nComparison with Template Matching: The particle picking approach of CryoGEM is not compared with template matching from the coarse cryo-EM input map, which could provide a more comprehensive evaluation of its advantages.\n\nIncomplete Quantitative Comparisons: The quantitative comparisons for pose estimation are incomplete without the fine-tuning of CryoFIRE using pose estimations of particles from the coarse cryo-EM map input from CryoSPARC.\n\n### Questions\n\nOriginality: CryoGEM presents an innovative approach by integrating physics-informed modeling with generative techniques, addressing a significant gap in cryo-EM analysis.\nQuality: The study features well-designed experiments that effectively demonstrate the model's capabilities. The application of CryoFIRE provides additional validation of CryoGEM's practical utility.\nClarity: The paper is articulated with precision, offering detailed explanations and a well-structured layout that enhances reader comprehension.\nSignificance: CryoGEM shows considerable potential to impact cryo-EM analysis significantly. It provides valuable tools that can improve both particle picking accuracy and the quality of 3D reconstruction processes, potentially advancing structural biology research.\n\n### Limitations\n\nThe authors have adequately addressed the limitations of their work, discussing potential areas for future improvements, such as generalizing the model to different experimental conditions and the need for a coarse cryo-EM map as an input, which may be hard to produce. However, a more explicit discussion on the accessibility of CryoGEM to the wider research community and the efficiency of the proposed pipeline would strengthen the paper, as well as the code availability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces CryoGEM, an innovative method combining physics-based cryo-EM simulation with unpaired noise translation via contrastive learning to generate high-quality synthetic cryo-EM datasets. The approach significantly improves the visual quality of generated images and enhances downstream tasks like particle picking and pose estimation, leading to better 3D reconstructions.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n1 Extensive experiments demonstrate that CryoGEM produces high-quality synthetic cryo-EM images that significantly outperform existing methods like CycleGAN and CUT. The visual quality of the generated images is notably superior, preserving structural details and realistic noise patterns.\n\n2. The synthetic datasets generated by CryoGEM improve the performance of downstream tasks, such as particle picking and pose estimation. The paper reports substantial improvements in these tasks, leading to better resolution in the final 3D reconstructions.\n\n### Weaknesses\n\nThe physics-based simulation in CryoGEM relies on a coarse result as an input. This requirement can be a significant limitation in scenarios where obtaining a reliable coarse result is challenging, such as with very small or highly dynamic molecules.\n\n### Questions\n\nIs there any reference indicating whether the Gaussian noise distribution accurately represents the actual physical noise?\n\nIn practice, it is relatively easy to obtain a large number of observed samples of the target image in transmission images. Even if the proposed approach enhances the results, can we still easily access more samples of the target particle with minimal effort?\n\n### Limitations\n\nSee Weakness\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this paper, the authors introduce a method to generate large annotated cryo-EM datasets from a small number (100) of real micrographs. The method combines a physics-based model of the image formation model with a contrastive learning strategy. The authors show that their method can be used to improve the quality obtained with downstream tasks such as particle picking, homogeneous reconstruction and pose estimation.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nAlthough acquiring more data in cryo-EM is usually easy, getting access to annotated data is hard. CryoGEM combines a physics-based model with a contrastive learning strategy to generate realistic annotated data. This is particularly interesting because, due to the low SNR in cryo-EM images, most downstream tasks often significantly benefit from pretraining (pose estimation) or finetuning (particle picking). The method introduced in this paper holds the potential to improve the accuracy of cryo-EM reconstruction pipelines. Notably, the authors explicity showed that cryoGEN can improve the performances obtained on downstream tasks.\n\nThe particularly appreciated the following points:\n- the method is described in a clear way ;\n- the experiments are fully described and seem reproducible ;\n- all the contributions claimed are illustrated with an experiment -- I appreciated the effort made by the authors to evaluate the quality of particle picking, pose estimation and homogeneous reconstruction after cryoGEM ;\n\n### Weaknesses\n\nI find that some parts of Section 3 (description of the method) lack clarity and that information on the 3D models used by the simulator of cryoGEM are missing (see \"Questions\").\n\n### Questions\n\n**Mutual information extraction.** This paragraph of the method was unclear to me. Why are $\\mathbf{v}$ and $\\mathbf{v}^+$ not indexed by $q$ while $\\mathbf{v}^-$ is indexed by $k$ in (5)? Why does (5) correspond to the probability of \"selecting\" a positive sample?\n\n**Origin of coarse models.** CryoGEM needs to a coarse 3D model to generate synthetic images. For the experiments conducted in this paper, where do these models come from? I did not find this information in the paragraph \"Datasets\".\n\n**Resolution of coarse models.** What is the resolution of the coarse models used in this paper? What is the influence of the resolution of the initial model on the accuracy obtained on downstream tasks?\n\n**Pose accuracy** What does $v$ correspond to in Eq (15)?\n\n### Limitations\n\nYes, limitations and potential negative impacts are discussed in the paper.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 7\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Jiakai Zhang",
      "Jingyi Yu",
      "Qihe Chen",
      "Wenyuan Gao",
      "Xuming He",
      "Yan Zeng",
      "Zhijie Liu"
    ],
    "url": "pdfs/neurips.cc-2024-conference_28c9b2115b82c3166b55e72a5bbf65dc9b1611fc.pdf",
    "remote_url": "https://openreview.net/pdf/28c9b2115b82c3166b55e72a5bbf65dc9b1611fc.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "Image-aware Evaluation of Generated Medical Reports",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "machine_learning_for_healthcare"
    ],
    "keywords": [
      "medical report generation",
      "evaluation metric",
      "vision language"
    ],
    "abstract": "The paper proposes a novel evaluation metric for automatic medical report generation from X-ray images, VLScore. It aims to overcome the limitations of existing evaluation methods, which either focus solely on textual similarities, ignoring clinical aspects, or concentrate only on a single clinical aspect, the pathology, neglecting all other factors. The key idea of our metric is to measure the similarity between radiology reports while considering the corresponding image. We demonstrate the benefit of our metric through evaluation on a dataset where radiologists marked errors in pairs of reports, showing notable alignment with radiologists' judgments. In addition, we provide a new dataset for evaluating metrics. This dataset includes well-designed perturbations that distinguish between significant modifications (e.g., removal of a diagnosis) and insignificant ones. It highlights the weaknesses in current evaluation metrics and provides a clear framework for analysis.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces a new image and language-based metric for generated report evaluation of X-ray images.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe concept of utilizing both semantic and vision-based representations in the form of a similarity score is very interesting.\n2.\tThe supplementary material effectively reflects different clinical cases and showcases the performance of the proposed metric alongside other SOTA models, both for report generation and evaluation metrics.\n3.\tApplying perturbations to the data provides an in-depth investigation of the proposed metric under different conditions.\n4.\tThe ablation studies complement the main findings of the paper and address potential questions proactively.\n\n### Weaknesses\n\n1.\tThe paper claims to introduce VLScore for automatic medical report generation from X-ray images. However, the primary contribution is the metric itself, which is validated in several scenarios. My main concern about the paper is the technical novelty. \n2.\tAll perturbations have been applied at the level of text, but it is intuitively obvious that the removal of such information has a lesser impact on image-report-based metrics. It would be beneficial for the authors to conduct another supplemental study, in which several regions of the image are blurred or removed and their corresponding report is accordingly changed. This would more accurately assess the proposed metric in the context of \"Image-aware evaluation of generated reports.\"\n\n### Questions\n\nI have no question/confusion about the technical aspects of the paper.\n\n### Limitations\n\n1.\tThe constant (C) introduced in Eq. 2 should be numerized for each model in Table 5.\n2.\tThe ablation studies should further investigate the impacts of image-related perturbations.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this work, the authors introduce a novel metric (VLScore) for evaluating the quality of generated medical reports. The key idea is to measure the similarity between radiology reports while also taking the image itself into account; this is a distinction from prior works, which generally only compare radiology reports without considering the source image. Evaluations demonstrate that the proposed metric aligns closely with the judgments of expert radiologists. The authors also provide a new dataset where they perturb the quality of reports (e.g. by removing a critical diagnosis) to evaluate how metrics react; evaluations on this dataset also demonstrate the utility of the proposed metric.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The approach introduced by the authors addresses an important, high-impact problem. There is a shortage of effective metrics in the medical domain for evaluating the quality of generated text. \n2. A particular strength of this paper is the evaluations on perturbed data. Whereas most existing works evaluate solely on the human-annotated ReXVal dataset, the authors of this work introduce a new dataset with perturbed samples. This dataset can be used to evaluate the extent to which metrics respond to variations in report quality. The dataset is well-designed and has the potential to be useful for future works.\n\n### Weaknesses\n\n1. **Limited in scope**: One weakness of this paper is that the proposed approach is limited in scope. In particular, the method is designed to operate on a single type of medical image (chest X-ray) and is evaluated using a single dataset (MIMIC-CXR).\n    - The proposed method assumes access to a multimodal model sensitive to local features. This is a strong assumption, and such models may not be available for more diverse use cases.\n    - Can the authors comment on whether they expect their approach to generalize to other chest X-ray datasets? Is the approach expected to generalize to other imaging modalities and anatomical features beyond chest X-rays? Would the model need to be retrained to operate on other types of radiology reports?\n2. **Need for finer-grained evaluations**: The paper could benefit from some additional analysis of the proposed metric, particularly with respect to important subgroups such as disease labels. For instance, when evaluating the removal of a pathology sentence vs. insignificant sentence, does the quality of the metric vary for different pathologies?\n\n### Questions\n\nIn addition to the concerns listed above in the “weaknesses” section, I have listed some additional questions below: \n\n- The authors are also encouraged to also report comparisons with the RadCliq metric, which is currently state-of-the-art on ReXVal.\n- Ablation Table 4 could benefit from a comparison with a RefCLIPScore-like approach, where the image embeddings are generated from LIMITR rather than CLIP.\n- Can the authors comment on inference time and how expensive it would be to run this approach in this practice?\n- Section 4.2 can benefit from additional implementation details related to the creation of the perturbed dataset. For instance, how were “pathology description words” selected? How were modifications made (e.g. was an external LLM utilized)? Some qualitative examples from the perturbed dataset may be useful here.\n\n### Limitations\n\nYes, the authors have adequately described the limitations of their work.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 4\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposed a novel evaluation metric, VLScore, which can more accurately reflect the alignment of generated reports and radiologists’ judgments. It is achieved by measuring the similarity in visual-textual space. To demonstrate the effectiveness of the proposed metrics, a new dataset with specific perturbations (e.g. location, severity) is created.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. Motivation: Existing report generation methods are benchmarked with natural language generation (NLG) metrics and clinical efficacy (CE) metrics. However, those metrics cannot accurately reflect the medical correctness of the generated reports. Thus, the advancement of this aspect is important.\n2. Identified Problems and Solutions: 1) NLG metrics cannot capture nuanced but significant differences in long reports. The description beyond images and differences of expression will cause a drop in evaluation. 2) CE metrics rely on a trained classification model with 14 disease classes, which means the detailed measurement of the report will be neglected. The proposed similarity measurement of representation space can solve the problems to some extent.\n3. Evaluation: The dataset with tailored perturbations can provide detailed comparisons in different aspects and amplify the differences between the proposed VLScore and the conventional NLG and CE metrics.\n\n### Weaknesses\n\n1. The effectiveness of the proposed metric highly depends on the performance of multi-modal embedding models.\n2. The proposed metrics cannot evaluate the similarities of writing style between generated reports and ground truths as the metrics mainly focus on the similarities in latent space.\n\n### Questions\n\n1. What is the running time of VLScore compared to NLG and CE metrics?\n2. Is there any justification for the selection of multi-modal embedding models?\n3. As the paper proposes new evaluation metrics for benchmarking, it will be more convincing to provide the evaluation of existing methods using the proposed VLScore.\n\n### Limitations\n\nThe authors adequately addressed the limitations and, if applicable, potential negative societal impact of their work.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper propose a novel evaluation metric, VLScore, for automatic medical report generation, considering both textual and clinical aspects. It key idea is to map the reports and their corresponding image to the joint visual-textual space and measure the similarity. Experiments demonstrate that they gain better performance than other metrics in both ReXVal dataset and their new proposed dataset.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper’s performance looks good. The results show the proposed method improves the correlation with radiologists’ judgments by 20%.\n2. A series of perturbed data are proposed to demonstrate the robustness of different metrics.\n\n### Weaknesses\n\n1. In terms of motivation, evaluating the quality of radiology report generation through image modality is a far-fetched and unreasonable process. \n2. Images and text need to be mapped to the same feature space, and the evaluation process proposed in the paper relies too heavily on the shared image-text space. However, according to the results reported in LIMITR, the retrieval results are not very high, and I do not have enough confidence that all anomalies mentioned in the report can be corresponded to the images. Additionally, I doubt whether the distance between the reports and the image embedding could reliably measure the similarity.\n\n### Questions\n\n1. I don’t understand the generation procession of the perturbed dataset. Please explain over the length of report and how much content was deleted. I think this greatly affects the final relevance results.\n2. In Figure 1b, the inconsistency in position while everything else is described correctly results in a score of 0.675. If, for the same case, some synonyms are used in the report, or other perturbations are applied, is it possible to obtain a similarly reasonable score?\n\n### Limitations\n\nYes\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Ayellet Tal",
      "Elad Hirsch",
      "Gefen Dawidowicz"
    ],
    "url": "pdfs/neurips.cc-2024-conference_0b0fac5775eaa03414fa5955dad7c7e264bc2a1f.pdf",
    "remote_url": "https://openreview.net/pdf/0b0fac5775eaa03414fa5955dad7c7e264bc2a1f.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "Free Lunch in Pathology Foundation Model: Task-specific Model Adaptation with Concept-Guided Feature Enhancement",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "machine_learning_for_healthcare"
    ],
    "keywords": [
      "Computational Pathology",
      "Whole Slide Image",
      "Vision Language Model",
      "Mutiple Instance Learning"
    ],
    "abstract": "Whole slide image (WSI) analysis is gaining prominence within the medical imaging field. Recent advances in pathology foundation models have shown the potential to extract powerful feature representations from WSIs for downstream tasks. However, these foundation models are usually designed for general-purpose pathology image analysis and may not be optimal for specific downstream tasks or cancer types. In this work, we present Concept Anchor-guided Task-specific Feature Enhancement (CATE), an adaptable paradigm that can boost the expressivity and discriminativeness of pathology foundation models for specific downstream tasks. Based on a set of task-specific concepts derived from the pathology vision-language model with expert-designed prompts, we introduce two interconnected modules to dynamically calibrate the generic image features extracted by foundation models for certain tasks or cancer types. Specifically, we design a Concept-guided Information Bottleneck module to enhance task-relevant characteristics by maximizing the mutual information between image features and concept anchors while suppressing superfluous information. Moreover, a Concept-Feature Interference module is proposed to utilize the similarity between calibrated features and concept anchors to further generate discriminative task-specific features. The extensive experiments on public WSI datasets demonstrate that CATE significantly enhances the performance and generalizability of MIL models. Additionally, heatmap and umap visualization results also reveal the effectiveness and interpretability of CATE.",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors propose an approach to enhance the performance of Multiple Instance Learning (MIL) models by incorporating concept prompts within the context of pathological Vision-Language Models (VLMs). Two modules, i.e., the Concept-guided Information Bottleneck (CIB) module and the Concept-Feature Interference (CFI) module, are designed to calibrate and inject similarity characteristics between features and concepts. Experiments are conducted on three Whole Slide Image (WSI) datasets, yielding interesting results. The paper includes qualitative and ablation analyses.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nTechnically Sound Method:\nThe proposed approach demonstrates a well-founded methodology, leveraging concept prompts within pathological Vision-Language Models for enhancing Multiple Instance Learning models.\n\nWell-Written Paper:\nThe paper is clearly and coherently written, providing a logical flow and structure that makes the complex ideas accessible and easy to follow.\n\nWell-Illustrated Figures:\nThe figures are well-designed and effectively illustrate key modules, significantly aiding in the understanding of the methodology.\n\n### Weaknesses\n\nDependence on Text Prompts: The performance heavily relies on the quality of text prompts/concepts. A more principled approach to generating expert-designed prompts would be beneficial. Only the hard-crafted ones make the  \"free lunch\" not that free. \n\nExperimental Settings: The choice of the number of IND and OOD sites for different datasets is unclear. How to choose the number of IND and OOD sites? It would be helpful to see performance under more traditional settings on NSCLC and RCC. What's more, it is noticed that the gain of CATE decreases with an increasing number of sites, raising questions about the practicality of the main experimental settings in real-world WSI analysis.\n\n\nAttention Maps: The attention maps are not that convincing to me. Since CATE-MIL provides almost identical attention maps to the baseline ABMIL except for intensity. If ABMIL predicts incorrect attention, CATE-MIL could potentially worsen the situation.\n\n\nUnderperformance on NSCLC and RCC: The authors claim that the underperformance of CATE-MIL on NSCLC and RCC due to the elimination of site-specific patterns is concerning. Clarification on why site-specific information for the IID test is not considered task-relevant is needed, especially as this issue does not appear with BRCA.\n\n### Questions\n\nThe paper is technically sound, but I am not convinced by the provided experimental results.\n\n1) Could the authors suggest more principled ways to obtain expert-designed prompts? Relying on hand-crafted prompts may not be scalable or generalizable.\n\n2) Please refer to the weakness and clarify the experimental settings. More detailed qualitative and quantitative analyses are needed.\n\n3) Lack of comparison with ref [1], which also uses the Information Bottleneck theory to improve the feature quality of MIL.\n\n\n[1] Task-specific Fine-tuning via Variational Information Bottleneck for Weakly-supervised Pathology Whole Slide Image Classification. CVPR 2023.\n\n### Limitations\n\nN/A\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors introduce a new tool called Concept Anchor-guided Task-specific Feature Enhancement (CATE) for analysis of whole slide images.  The authors are taking advantage of the open source weight now available for the CONCH pathology vision language model trained on paired captions of images from journals and other publicly available image-caption pairs.  The high level conceptulization is that language prompts related to a task at the whole slide image level can be enrich for the features most relevant to the downstream task.  The title cleverly asserts this is a free lunch when using a vision language encoder for feature extraction the expertly curated concepts can \"bring foward\" the most relevant features when aggregating at the whole slide level.  \n\nInterestingly, they split up the CATE task into Predictive Information Maximization to enhance features associated with related concepts and Superfluous Information Minimization is used to supress the concepts likely to be associated with unimportant features. \n\nThe outputs of PIM and SIM are used during aggregation task to determine weighting that MIL should use for a given feature vector derived from the tile. \n\nFinally a module called concept-feature interference is used calibrate the CIB scores. \n\nThese module provide enhanced features to the aggregation function that provides relative weighting based on the relationship between the semantic concepts relationship to the downstream task. \n\nCATE and weights are not backpropogated through the feature embedding tasks and they are static in the MIL function (to the best of my understanding). \n\nThe authors chose RCC, NSCLC, and BRCA tasks from the TCGA to demonstrate performance\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe model is build such that one can introduce to any MIL-like framework that uses a VLM encoder. \n\nIt is a clever idea to guide the aggregation function with the concepts one can get from VLM encoder. \n\nThe paper is clearly written and claims are not over stated.  \n\nOn the chosen tasks, it is reasonably clear that a performance boost is acheived consistently and in some cases by a dramatic margin.  Given that is is a bolt-on method these are very good apples to apples comparisons. \n\nThe ablations study suggests that the 3 novel modules (PIM, SIM, and CFI and  parts are additive to peformance, at least on these tasks.\n\n### Weaknesses\n\nI agree with the weaknesses the authors have put forth.  \nThe model scope is ~mostly~ limited to downstream task for the the concepts are part of the VLM training.  The tasks that are chosen for benchmarking are actually tasks that are likely well represented in VLM training sets with robust descriptions of SCC vs. LUAD, etc.  For tasks that source material does not contain common descriptions, say more challenging tasks like predicting mutations, it is unclear to what extent, this work would provide advantage over other approaches.  \nMy presumption is that VLM encoders are less robust for feature extraction than the now available large foundation models trained with 10^6 plus whole slide images. If one is not able to use these models and must use an inferior model, then one might argue it is not a \"free lunch\".\n\n### Questions\n\nOut of distribution (OOD) is a term that means many different things to different people.  Will you please spare a paragraph to better explain what you mean by OOD. If you find that OOD is not the best term for what you mean, I think it would be better to use a less loaded term. \n\nHave you considered a hybrid approach where the vision feature vectors can be extracted with a different foundation model but the VLM is use for the concept guidance?\n\nHow the normal distribution of SIM in equation (8) align with Bernoulli distribution of ABMIL? Are there alternative distributions that could be used here? (may be Poisson distribution to define random occurance of a non-informative patch?)\n\nMinor comments for improvement in readability: \n1) In figure 2a, the image encoder cartoon is larger on the side of image which implies the feature vectors are larger than the original patches. \n2) Equation (6) LHS should be I(x; alpha | c)\n\n### Limitations\n\nLimitations section is reasonable.  I have provided further feedback in the weaknesses section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors introduce CATE, a novel approach designed to enhance the generalizability of histopathology models by leveraging task-specific concepts derived from the text endoder of pathology vision language models(VLM). CATE includes two modules: CIB and CIF, which work synergistically to improve model robustness. The CIB module identifies and retains features that contribute positively to predicting the task outcome, while it eliminates features that contain irrelevant or superfluous information.  Additionally, CIF specializes in creating task-specific features by leveraging similarities between the enhanced image features the and predefined concept anchors. The authors demonstrate the effectiveness of CATE across an extensive series of histopathology datasets, showcasing superior results in model performance and generalization.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 4\n\n### Strengths\n\n- The concept introduced by CATE is highly relevant as it addresses a major challenge in histopathology: ensuring models generalize effectively across diverse datasets and clinical scenarios. In computational pathology, the lack of external validation cohorts often leads to uncertainty about whether models have learned genuine signals or have simply overfitted to confounding factors or batch effects. To validate their approach, the authors split existing datasets into indoor and outdoor test sets, demonstrating CATE's robustness across different conditions.\n- The paper is well-presented and clearly written.\n- The strength of the paper lies in its innovative approach of repurposing general-purpose vision language models to extract rich, meaningful representations for downsream tasks.\n\n### Weaknesses\n\n- CATE's performance hinges largely on the quality of its concept anchors, which in turn depends on domain expertise and the quality of the pre-trained pathology VLM, a weakness that is also highlighted by the authors.\n\n### Questions\n\n- Why did you use accuracy as an evaluation metric? Accuracy is straightforward to interpret, but it can be misleading in imbalanced datasets. Metrics like F1-score, weighted accuracy, precision, and recall offer more nuanced insights into model performance, particularly in contexts where class distribution is skewed such as the BRCA dataset.\n\n- What strategy was used to split the dataset into in-domain and out-of-domain test sets? Understanding this process ensures transparency in evaluating the model's generalizability across different clinical settings or data sources. \n\n- How do you define the class-agnostic concepts?\n\n- What is the intuition behind maziming the distance between the enhanced featrures and the class-agnostic concepts? When features are overly tailored to the training samples by maximizing their separation from class-agnostic concepts, the model may become too specialized and prone to overfitting.\n\n### Limitations\n\n- Exploring the potential of concept anchors for applications beyond subtyping, such as mutation predictio could be an intersting avenue to explore.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper proposes a way to enhance image features from VLM pathology foundation models for downstream tasks by aligning them with task-specific text prompts. The authors use two modules -- one which extracts an image representation aligned with task concepts and other which measures similarity of representation with tasks, to enrich image representations for downstream tasks.\nThe authors show results for the cancer subtyping task across 3 datasets indicating the usefulness of the proposed approach.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\n**Clarity and Quality**:\n\nThe paper is well written and the key ideas are explained clearly. \nThe experiments are well setup, the results and ablations provide good evidence of the proposed method for the specific problems and the limitations are discussed.\nThe code is shared and the supplementary section provides more details around ablations and the theoretical assumptions.\n\n**Originality**\nThere has been prior work in using task anchors to improve MIL using learnt or clustered task prototypes (ProtoMIL, TPMIL, etc) \nThe idea of using textual prompts to create task anchors and use these to filter patches and enrich patch representations is interesting. \nThe predictive information maximization (PIM) objective is interesting and is similar to applying CLIP on-the-fly with fixed text embeddings, to extract more task-specific image features.\nThe SIM objective seems useful to suppress task-irrelevant information.\n\n**Significance**\nThe proposed approach is a simple addition to MIL and can be applied in conjunction with existing MIL methods.\n\n### Weaknesses\n\nThe main weaknesses are around some of the assumptions made in the paper listed below which limit the applicability of the approach.\n\n\n**Aligning patches with tasks**\n\nIn WSIs the labels are only available at slide-level and not patch-level and for many problems only a fraction of the patches in the slide have task-relevant info and for many others the information may need to be aggregated from multiple patches.\n\nEven though there is a filtering step to filter patches, the step of aligning patches to tasks is noisy when many patches dont have task info and it may not even be appropriate for problems which need aggregation of information across patches (as the approach homogenizes representations of all patches belonging to the same class).\n\nThe authors show results on only one type of task -- subtyping where most patches on the slide have the sub-type information and is also typically a simpler MIL task indicated by the strong results on the benchmarks for most methods. \n\nInstead of aligning patch representations, it would be better to align slide-level representations with task-concepts, which addresses both the drawbacks listed above. There is also evidence from prior works around usefulness of this like SC-MIL (https://arxiv.org/abs/2303.13405 which align representations of slides belonging to same-class) and they also show aligning patch-level representations (patch SC-MIL) does poorly compared to aligning at the slide-level\n\n\n**Unclear Improvements**\n\nIt is unclear how useful the task-alignment objective with the information bottleneck layer is. \nThe key part seems like the filtering step which extracts relevant patches for the task and I suspect if thats contributing to majority of the improvements.\nOnce filtering is done, is the alignment step really needed as it seems like a dimensionality reduction step to extract task specific features. \nA sufficiently strong model should be able to able to extract task relevant features from the embedding dimensions with cross-entropy loss.\nThe alignment step might also be unsuitable as it homogenizes patch representations which may not be good for many MIL tasks and directly optimizing with cross entropy on task-labels might be more flexible and general.\n\nIts also unclear if the improvements are coming from a bigger model due to the CIB and CFI modules. Scaling ABMIL to similar number of params would provide more context on this.\n\n\n**Task Concept Alignment**\nThe other key assumption is using text prompt embeddings as task anchors which is very dependent on factors like the quality of the pre-trained VLM, the textual prompts and descriptions used and the idea that it is possible to write morphological descriptors for tasks which is typically not possible for many MIL tasks like survival or molecular signature prediction.\n\nThe sufficiency and consistency constraints imposed could be limiting specially when any of the factors above break. The SIM objective specifically could be problematic when text doesnt fully capture the required features for the task and other relevant image features could be suppressed.\n\n### Questions\n\nIts unclear how much improvements are coming from the patch filtering step and the increased number of parameters and if those alone are sufficient enough.\n\nI think its worth trying the alignment on the slide-level representations instead of patch-level representations (similar to SC-MIL) which makes the approach more generic and limits the drawbacks with using slide-level labels for patches.\n\n### Limitations\n\nHighlighted the limitations in weaknesses.\n\nProvided some suggestions and improvements in the questions. \n\nGiven the weaknesses and the lack of evidence and limited applicability of the approach to more complex MIL problems like scoring (gleason grading, HER2 grading) which need aggregation of information across patches or other analytical tasks like survival prediction, molecular signature prediction where there might not be possible to write text descriptions, I am basing my decision\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Lequan Yu",
      "Yanyan Huang",
      "Yihang Chen",
      "Yu Fu",
      "Weiqin Zhao"
    ],
    "url": "pdfs/neurips.cc-2024-conference_db908e6ff75332631fe62db1beffd84b71e3c5d6.pdf",
    "remote_url": "https://openreview.net/pdf/db908e6ff75332631fe62db1beffd84b71e3c5d6.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "MotionTTT: 2D Test-Time-Training Motion Estimation for 3D Motion Corrected MRI",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "machine_learning_for_healthcare"
    ],
    "keywords": [
      "deep learning based motion estimation",
      "3D imaging",
      "MRI",
      "motion artifacts",
      "medical imaging",
      "test-time-training",
      "motion correction"
    ],
    "abstract": "A major challenge of the long measurement times in magnetic resonance imaging (MRI), an important medical imaging technology, is that patients may move during data acquisition. This leads to severe motion artifacts in the reconstructed images and volumes. In this paper, we propose MotionTTT a deep learning-based test-time-training (TTT) method for accurate motion estimation. The key idea is that a neural network trained for motion-free reconstruction has a small loss if there is no motion, thus optimizing over motion parameters passed through the reconstruction network enables accurate estimation of motion. The estimated motion parameters enable to correct for the motion and to reconstruct accurate motion-corrected images. Our method uses 2D reconstruction networks to estimate rigid motion in 3D, and constitutes the first deep learning based method for 3D rigid motion estimation towards 3D-motion-corrected MRI. We show that our method can provably reconstruct motion parameters for a simple signal and neural network model. We demonstrate the effectiveness of our method for both retrospectively simulated motion and prospectively collected real motion-corrupted data. Code is available at \\url{https://github.com/MLI-lab/MRI_MotionTTT}.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper presents MotionTTT, a deep learning-based method for estimating and correcting rigid motion in 3D MRI images. The approach leverages a neural network pre-trained for 2D motion-free image reconstruction and employs test-time-training (TTT) to estimate motion parameters from motion-corrupted 3D measurements. The effectiveness of the method is demonstrated through evaluations on both simulated and real datasets.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper proposed a novel approach for 3D rigid motion estimation using a neural network trained on 2D motion-free images and TTT for motion parameter estimation. The application of test-time-training for motion estimation is a novel contribution that effectively addresses motion artifacts in 3D MRI images, and outperformed classical alternating optimization methods in terms of speed and accuracy, especially under severe motion conditions.\n\n### Weaknesses\n\nThe theoretical foundations of MotionTTT are generally robust, but there are notable gaps. The method builds on existing neural networks for 2D image reconstruction, extending them to handle 3D rigid motion estimation. \n\nDuring the pre-training step, $f_\\theta$ is trained to map under-sampled k-space data back to fully sampled k-space data. The authors show that under-sampled k-space data with motion (where certain regions of the k-space undergo rotation and phase shifting) results in higher reconstruction loss for $f_\\theta$. However, this indirect optimization lacks theoretical proof and needs more robust experimental validation. \n\nFor instance, the performance of this method may heavily depend on the under-sampling pattern of the k-space (such as the undersampling factor and the linear sampling trajectory) and the specific regions in k-space that are affected by motion. However, in the experiments, the setup of the undersampling of the k-space was fixed. \n\nA more thorough theoretical and experimental investigation into why this optimization works and under what conditions it is most effective would strengthen the soundness of the method.\n\n### Questions\n\n1. The idea behind minimizing loss (4) is not intuitive and clear. How do you prove the effectiveness of using loss (4) to recover and remove the motion corruption? Will the effectiveness be affected by the undersampling factor and trajectory? What if you have a higher or lower factor? or maybe a spiral trajectory? Can you find the optimal factor for your method? \n2. Based on the assumption that rigid motion leads to rotation and phase shift in k-space, is the linear trajectory the optimal undersampling trajectory? \n3. Do you really need the fully sampled k-space data to learn $f_\\theta$?\n\n### Limitations\n\nThe indirect TTT optimization method lacks theoretical justification. The authors should investigate both theoretically and experimentally why this approach works and under what conditions it is most effective.\n\nThe performance of MotionTTT may be heavily dependent on the under-sampling pattern of the k-space and the specific regions affected by motion. The method’s effectiveness might vary significantly with different sampling patterns and motion artifacts.\n\nThe method relies on a pre-trained network using fully-sampled data, which may not always be available in practice.\n\n### Flag For Ethics Review\n\n- Ethics review needed: Research involving human subjects\n\n### Rating: 4\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a method for estimating the motion of patients, such that accurate motion-corrected images could be reconstructed. The key idea is that a neural network trained for motion-free reconstruction has a small loss if there is no motion, thus optimizing over motion parameters passed through the reconstruction network enables estimation of the motion.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- The estimation of motion of patients is important in MRI measurement. This paper is a good trial to address this problem.\n\n### Weaknesses\n\n- There is no technical novelty in this work. It introduces some existing modules to address the proposed problem.\n- The experiments are weak to demonstrate the efficacy of the proposed method. \n  1）There lacks strong baselines and there are no compared methods. Although according to the authors, there are no other methods designed for 3D rigid motion estimation for 3D motion-corrected MRI, there should be some methods that could reconstruct the images.\n  2) The data are little for evaluating the methods, and more averaged quantitative results are suggested to report.\n\n### Questions\n\nSee the weaknesses.\n\n### Limitations\n\nYes, limitations are discussed.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 4\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a motion correction MRI reconstruction algorithm for 3D brain MRI. The proposed technique consists in a deep learning-based estimation of rigid motion parameters, which allows to correct the k-space before a final reconstruction.\nEstimation of motion parameters are based on a single optimization step freezing the reconstruction UNet parameters, and assuming loss will only (or mainly) be influenced by motion.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper addresses an important problem, namely motion correction, using a fully data-driven approach. Moreover, the originality of the approach lies in tackling this problem directly on the 3D frequency (or k-)space , when most deep-learning techniques are focusing on 2D acquisitions.\n\n### Weaknesses\n\nThe overall structure of the paper is quite confusing, which makes it hard to follow. Experiments and alternative techniques seem to be presented throughout the result section.\nA proper ablation study seems required, in order to assess the added-value of each step. Proper baseline also seems necessary especially regarding the reconstruction step or the removal of motion artifacted lines, as more recent techniques could have been used as SOTA.\nThe overall motion estimation relies on a first 2D reconstruction pipeline however the performance of the technique seems to be lower than a standard L1 reconstruction technique. If so, how could the authors be sure that motion parameters are not biased or affected by the low performance of the 2D reconstruction pipeline.\nWhy did the authors not try to directly reconstruct from the 3D k-space domain, in order to estimate motion parameters in 3D? Moreover, I believe the authors should also provide further details on they are switching back and forth from 3D k-space domains to 2D, as a simple slicing in a given direction of the 3D k-space is not appropriate.\n\n### Questions\n\nCould the authors find another database of 3D data in order to train a 3D reconstruction UNet directly?\nGiven the proposed technique aims also at suppressing corrupted k-spaces lines, would it be sensible to train a reconstruction technique with higher a acceleration factor?\nWould the proposed technique be applicable for other sampling strategies, especially would it be possible to apply on stack of stars acquisitions or other 3D sampling strategy.\nWhy did the authors not apply other reconstruction (standard L1) reconstruction for the real dataset for comparison? Especially since it was outperforming the UNet approach on simulated motion data.\n\n### Limitations\n\nThe authors should discuss the risks of hallucinations and bias of the proposed technique.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis work proposes a novel deep learning-based framework for recovering high-quality 3D MR images from undersampled and motion-corrupted k-data. The proposed approach is well motivated and technically sound. The authors perform extensive experiments on simulated and real MR datasets, which confirm the effectiveness of the proposed framework.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n**Motivation and significance**\n\nMotion correction is an important issue in the field of MR imaging. The proposed method removes the reliance on motion simulation by training a neural network that reconstructs high-quality MR image from under-sampled k-data in advance. This new paradigm significantly improves the robustness of the reconstructed images.\n\n**Technical solid**\n\nIn the steps 2 of test-time training for motion estimation, the MRI acquisition knowledge, such as forward model, sampling trajectory, are effectively integrated into the framework, which improve the reliability of the reconstruction.\n\n**Clarity and organization**\n\nThis submission is well-written and easy to follow.\n\n**Experimental evaluation**\n\nThe authors perform experimental evaluations on simulation and real-world datasets. I am pleased to see the experiments based on the real-world dataset. I think it greatly improves this paper.\n\n### Weaknesses\n\nFor this submission, I have a few minor suggestions as follows.\n\n**Various types of rigid motion**\n\nIn line 205, the authors show that random rigid motions ($M_\\text{max}=[2,5,10]$) are simulated. However, the rigid motion in the real world could follow some patterns, such as involuntary motion and abrupt motion. I think it is better to test these different movements.\n\n**Compared methods**\n\nFor supervised methods, only U-net is used as a baseline. Advanced supervised models [1][2] are not discussed. Furthermore, motion correction methods [3] based on diffusion models are not compared. \n\n> [1] Han Y, Yoo J, Kim H H, et al. Deep learning with domain adaptation for accelerated projection‐reconstruction MR[J]. Magnetic resonance in medicine, 2018, 80(3): 1189-1205.\n\n> [2] Liu J, Kocak M, Supanich M, et al. Motion artifacts reduction in brain MRI by means of a deep residual network with densely connected multi-resolution blocks (DRN-DCMB)[J]. Magnetic resonance imaging, 2020, 71: 69-79.\n\n> [3] B. Levac, A. Jalal, and J. I. Tamir. “Accelerated Motion Correction for MRI Using Score-Based 388 Generative Models”. ISBI 2023.\n\n**Some typos**\n\nFor example, the symbol $\\mathcal{E}$ in Eq. 3 is not defined.\n\n### Questions\n\nSee the section of Strengths and Weaknesses, please.\n\n### Limitations\n\nThe authors mention enough limitations of their work.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Kun Wang",
      "Reinhard Heckel",
      "Stefan Ruschke",
      "Tobit Klug"
    ],
    "url": "pdfs/neurips.cc-2024-conference_f5a7386557b925b38510cffd237c30490992faa8.pdf",
    "remote_url": "https://openreview.net/pdf/f5a7386557b925b38510cffd237c30490992faa8.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "pcaGAN: Improving Posterior-Sampling cGANs via Principal Component Regularization",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "machine_vision"
    ],
    "keywords": [
      "Image recovery",
      "inverse problems",
      "MRI",
      "posterior sampling",
      "GAN"
    ],
    "abstract": "In ill-posed imaging inverse problems, there can exist many hypotheses that fit both the observed measurements and prior knowledge of the true image. Rather than returning just one hypothesis of that image, posterior samplers aim to explore the full solution space by generating many probable hypotheses, which can later be used to quantify uncertainty or construct recoveries that appropriately navigate the perception/distortion trade-off. In this work, we propose a fast and accurate posterior-sampling conditional generative adversarial network (cGAN) that, through a novel form of regularization, aims for correctness in the posterior mean as well as the trace and K principal components of the posterior covariance matrix. Numerical experiments demonstrate that our method outperforms competitors in a wide range of ill-posed imaging inverse problems.",
    "reviews": [
      {
        "text": "### Summary\n\nThis work considers using posterior sampling to solve inverse problems via conditional GANs. Previous work has shown that cGANs are competitive in terms of reconstruction quality as compared to other generative models to sample from the posterior (e.g., diffusion models), while also having the advantage that, once trained, sampling is fast as it simply requires a forward pass. In this paper, the authors build on such approaches by adding regularization terms to encourage the cGAN to learn K principal components of the posterior covariance matrix, where K is user-defined quantity. Due to the computational burden of such an approach, a training scheme is devised where learning of the eigenvectors and eigenvalues is done after optimizing a regularized cGAN objective. The method is validated on a variety of linear inverse problems (denoising, accelerated MRI, inpainting) and datasets (mnist, fastmri, ffhq).\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The paper is well-written and easy to read. In particular, the idea of adding regularization to fit to principal components of the posterior distribution is a simple and natural idea. It is also one of importance, as being able to visualize uncertainty in reconstructions is crucial in understanding which regions of the reconstruction can be “trusted”\n- The method is validated on a nice range of problem types and datasets.\n- Using a conditional cGAN allows for fast sample generation, especially as compared to diffusion models. \n- The additional use of the pca regularization term appears to improve performance of rcGAN, while also allowing for visualization of directions of uncertainty.\n\n### Weaknesses\n\n- The proposed approach is a slight, potentially somewhat incremental, improvement to the prior rcGAN approach of [21]. In particular, rcGAN also aims to better model the posterior distribution by matching certain statistics of the posterior mean and covariance. This work takes this reasoning a step further by aiming to fit the principle components of the covariance. This has also been done (in a different way) in prior work [29], that the authors compare to. The bonus for this work, however, is that the authors can sample quickly, whereas in [29] sampling is not possible.\n- Coupled with the concern regarding novelty, the additional regularization terms are computationally expensive to optimize, especially for high-dimensional problems. This is evidenced by, for example, only being able to set K = 1 in the MRI experiments and K = 2 in the large-scale inpainting experiments, and is discussed by the authors in their limitations. Moreover, the training strategy becomes a bit more complicated in order to aid in efficiency, as one needs to first optimize the rcGAN objective and then integrate the PCA terms into the learning process.\n\n### Questions\n\n- As the parameter $K$ is fixed prior to training with knowledge of the inverse problem, it would be good to discuss how the choice of $K$ relates to the particular inverse problem itself. For example, how does the choice of $K$ relate to how challenging the inverse problem is? I presume that ill-posedness plays a role here (e.g., comparing a linear inverse problem with a non-linear inverse problem, such as phase retrieval).\n\n### Limitations\n\nThe authors discuss some of the above weaknesses I mentioned near the end of their paper, which I appreciate. They also discuss other issues regarding how to properly incorporate this approach for rigorous uncertainty quantification.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper focuses on learning posterior samplers for inverse problems, specifically aiming to accurately estimate the principal components of the posterior covariance matrix. To achieve this, the authors consider training a conditional generative adversarial network (cGAN), building upon previous work [21]. They introduce two new regularization terms: one for the eigenvalues and another for the eigenvectors. Recognizing the challenge of estimating these statistics from a single ground truth solution per input measurement, the authors propose gradually adding these terms during training, while utilizing intermediate results to estimate the statistics.\nThe authors present a series of experiments to demonstrate their method, ranging from toy examples to real-world MRI data. Comparisons are primarily made with rcGAN and NPPC.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\n* The paper is well-written, with clear motivation, method presentation, and background (mostly).\n* The detailed algorithm description, beyond pseudocode, significantly enhances understanding and reproducibility.\n* The authors nicely addressed the problem of estimating the desired statistics from a single ground truth vector.\n* The authors provide sufficient evidence to demonstrate their method (but additional evidence could further strengthen their claims).\n\n### Weaknesses\n\nMajor Issues:\n* The paper lacks an ablation study which is crucial. While the authors followed [21] and set $P_{rc}=2$, I still expect an ablation study on this parameter, especially since using 2 samples seems insufficient to estimate statistics properly. \nFurthermore, there is no ablation study regarding the different regularization terms and their relative weights. Given the newly regularization terms, it is possible that other terms, introduced in [21], are redundant, especially when $K=d$. Since accurately recovering the eigenvalues implies perfect recovery of the trace, for example.  \n\n* I find the comparison to competing methods limited. I suggest adding comparisons to additional recent posterior sampling techniques. For example, the authors of NPPC presented comparison to DDRM, DDNM, RePaint, and MAT.\n\n*  Unlike work [21], the paper present no proposition or theoretical guarantees regarding the estimated statistics, limiting the novelty of the paper. \n\n* The description of work [21] is imprecise. Specifically, the usage of the notations $P$ and $P_{rc}$ is incorrect, leading to confusion.\n\n* The authors state in lines 103-104 that they apply the regularization term (11) only after allowing the generated posterior mean to converge near the true mean. This raises the question of whether (11) is necessary or if its impact is marginal, as it only leads to small variations in the generated posterior mean. The same question applies to the generated eigenvalues. I suggest adding plots to illustrate the variation in generated statistics over training epochs, comparing them to their final values or their values before applying the new regularization term during training.\n\n* While the relevance of NPPC to this work is clear, the experimental comparison seems unfair as NPPC is not designed for generation tasks. Furthermore, the authors demonstrate a speedup against DPS, but neglect to discuss the computational load of their method compared to NPPC.\n\nMinor:\n* The authors in lines 24-26 correctly point out that bias can be an issue in single-sample recovery. However, this is also a potential concern for recovery methods that yield multiple solutions.\n\n### Questions\n\n* Please address the weaknesses above.\n* I suggest conducting an ablation study to assess the impact of key priors in the method, such as the different terms and $P_{rc}$.\n* I suggest expanding the experiments to include comparisons with more recent posterior samplers in standard image inverse problems, like super-resolution and inpainting.\n* Please provide plots illustrating the variation in the generated statistics over the training epochs.\n* Do the additional regularization terms theoretically change the optimal solution, or does it remain the same?\n\n### Limitations\n\nSee weaknesses.\n\n### Flag For Ethics Review\n\n- Ethics review needed: Research involving human subjects\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe authors address inverse problems and aim to improve GAN-based posterior samplers by adopting regularization to correct the principal components of the covariance matrix of the approximated posterior distribution.\n\n### Soundness: 2\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper is well written and easy to follow.\n2. The idea presented in the paper is novel and clear, especially given the tendency of GAN-based posterior samplers to experience mode collapse. This serves as a strong motivation for the proposed method in the paper.\n3. The authors conducted empirical study with a variety of datasets.\n\n### Weaknesses\n\n1. For each dataset in the empirical study, a different set of baseline methods was chosen, possibly due to the requirement of pretraining. However, for a fair comparison, this is necessary for a complete research.\n2. The authors claim that the proposed method outperforms existing diffusion models (line 52). However, the experiments conducted with diffusion models are not comparable due to the much smaller test size (line 210 in MRI experiments and line 235 in inpainting experiments).\n3. Figure 4 presents inpainting posterior samples, but for the diffusion model referred to as DPS, the masking was not applied, which results in incorrect and misleading visualization.\n4. Some minor errors:\n    - In line 28, “There the goal\" should be revised.\n    - In lines 33-34, there is a grammar mistake.\n\n### Questions\n\n1. While diffusion models are slower compared to GANs, they have demonstrated success over GAN-based models in enhancing posterior richness for conditional tasks. A fair comparison with diffusion models would be valuable, as it remains uncertain whether the proposed method outperforms diffusion models. For faster sampling approaches, the authors might consider DDIM[1], which accelerates the diffusion process.\n\nReferences:\n\n[1] Song, Jiaming, Chenlin Meng, and Stefano Ermon. \"Denoising diffusion implicit models.\" (2020).\n\n### Limitations\n\nThe authors explained the limitations of their work.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nTo solve image inverse problems (denoising, super resolution, etc.), many works attempt to sample from the posterior distribution of ground truth images given a degraded measurement. One line of works propose to train a stochastic neural network as a CGAN (conditioned on the degraded image and an additional random seed as inputs), while regularizing the conditional mean and the conditional covariance matrix of the outputs. This paper proposes yet another, improved regularization term for such a CGAN approach. Specifically, the authors propose to correct (regularize) the K principal components of the conditional covariance matrix in additional to the conditional mean and the trace of the covariance.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 3\n\n### Strengths\n\nThe paper is mostly written well. The proposed method is demonstrated on toy examples (such as Gaussian random signals and MNIST images), which I find appealing to gain intuition into the advantages of the proposed approach. The paper shows results on MRI reconstruction, which is not very common to report but an incredibly important inverse problem to solve.\n\n### Weaknesses\n\nEven though I personally like this type of work and the proposed approach, unfortunately I see several major weaknesses in this paper, starting even with wrong/inaccurate claims in the introduction & motivation. I'll list some of the weaknesses below.\n\n1) I disagree with most of the claims made in L17 onwards regarding the shortcomings of point estimators (introduction). The perception-distortion tradeoff theorem in [6] does not prove anything specific about point estimators (a claim made in L20-L21). In fact, this theorem proves that distortion can only be at odds with perceptual quality (but now always), regardless of whether the estimator is \"deterministic\" or not. Moreover, [6] defines perceptual quality as the statistical distance (e.g., KL) between $p_{\\hat{X}}$ (the distribution of the reconstructions) and $p_{X}$ (the distribution of the source images), and not as the distance between $\\hat{x}$ and the manifold of natural images (as claimed in L22). What is the definition of such a distance? Lastly, the perception-distortion tradeoff is not always a strict one, as hinted in L21. This tradeoff  depends on the distortion measure, as discussed by the authors of [6]. Some distortion measures (e.g., \"perceptual\" losses in the deep image features space) virtually do not contradict with perceptual quality (as demonstrated experimentally in [6]). I would be pleased to see a complete revision of this paragraph in which the errors are corrected, or otherwise a clear explanation for why am I wrong. This is the entry point of the paper, it should at least be accurate.\n\n2) I also do not agree with L27: \"to address these shortcomings, posterior-sampling-based image recovery has been proposed\". Posterior sampling as a solution for inverse problems has been proposed and suggested, e.g., with Langevin dynamics (though, it didn't work properly for high-dimensional data) much before we knew anything about the perception-distortion tradeoff or fairness issues in image reconstruction. As I understand it, the true motivation for the rise of posterior-sampling is the following: We are often dealing with ill-posed inverse problems, and performing posterior sampling allows us to conduct uncertainty quantification of the inverse task at hand, explore the space of solutions visually, and make more informed decisions in down-stream tasks. It has also been shown that stochastic estimators are more robust to adversarial attacks [1]. I don't see why the rise of posterior sampling is linked to the perception-distortion tradeoff and/or to fairness. Please provide evidence (citations and explanation) if that's the case.\n\n3) L30: \"posterior samples will display attributes that reflect minority and majority populations in the training data\". As far as I know, this is often incorrect. In fact, we may often require to produce an intractable amount of posterior samples to observe an outcome for a minority. Please see [2] as an example.\n\n4) L43: You are missing citations (and comparison) with highly related work that proposed to use posterior sampling using a CGAN with conditional mean and conditional covariance regularizations, prior to the work of Bendel et al. For example, [3] and [4]. In fact, the work of Bendel et al. is based on the idea proposed in [2] (to regularize the neural network by enforcing agreement of the average posterior sample with the posterior mean), and in [3] (to use a second-moment penalty of the covariance matrix as well), as you suggest in L75. As far as I understand, your algorithm is a different (and maybe improved) version of [4], but I don't know which one is better since a comparison with [3] and [4] is not reported in the paper. Please address the differences in your work so I could better understand why it's not appropriate to cite such papers and/or include them in your analysis.\n\n5) I would suggest explaining L76-L79 better, by adding appropriate definitions for all the terminology used. For instance, the definition of a $P_{rc}$-sample supervised-$l_{1}$ loss is missing and unclear. I would be happy to see a more self-contained background section. Make sure every notation you are using is clearly defined before using it.\n\n6) Some results seem wrong to me. For example, the results of DPS in Figure 4. I have been using DPS in many different image inverse problems, and never observed such inconsistencies between the outputs and the inputs (see, e.g., Figure 14 in the DPS paper appendices, which shows almost perfect consistency with the measurements for severe inpainting tasks). The FID results of DPS also seem odd to me. I find it very hard to believe that a CGAN algorithm would outperform DPS, given that the hyper-parameters of DPS are properly tuned. Can you please provide the code you used to execute these experiments so I could verify them? Maybe the hyperparemeters you used to run DPS are suboptimal?\n\n7) The contribution of this work is quite limited in my opinion. The proposed algorithm only slightly improves its predecessor (Badler et al.), and the experimental evaluation is limited. A comparison with more works is missing, such as [3] and [4], and other, more recent state-of-the-art stochastic image restoration methods that produce better results than DPS. What about DDNM, DDRM, etc.? Why are these not included in the analysis? (besides the fact that they don't \"promise\" posterior sampling). I would be happy if you could clarify the contribution of your work and its impact on the field, as it seems quite incremental to me.\n\n\n[1] Reasons for the Superiority of Stochastic Estimators over Deterministic Ones: Robustness, Consistency and Perceptual Quality, Guy Ohayon et al., ICML 2023\n\n[2] From Posterior Sampling to Meaningful Diversity in Image Restoration, Noa Cohen et al, ICLR 2024.\n\n[3] High Perceptual Quality Image Denoising with a Posterior Sampling CGAN, Guy Ohayon et al., ICCV workshops 2021.\n\n[4] High-Perceptual Quality JPEG Decoding via Posterior Sampling, Sean Man et al, CVPR workshops 2023.\n\n### Questions\n\nPlease address each of the weaknesses.\n\n### Limitations\n\nThe authors do provide a dedicated limitations section, but I observe additional limitations in this paper which are not addressed, such as incremental novelty of the proposed algorithm over previous work, limited experimental section (comparing only to one predecessor), and more, as explained in the weaknesses section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 4\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Matthew C Bendel",
      "Philip Schniter",
      "Rizwan Ahmad"
    ],
    "url": "pdfs/neurips.cc-2024-conference_99777b05b995bd1e6c1a5bbdf9c5ca669a32ff8a.pdf",
    "remote_url": "https://openreview.net/pdf/99777b05b995bd1e6c1a5bbdf9c5ca669a32ff8a.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "E2ENet: Dynamic Sparse Feature Fusion for Accurate and Efficient 3D Medical Image Segmentation",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "machine_learning_for_healthcare"
    ],
    "keywords": [
      "Medical Image Segmentation",
      "Sparse Training",
      "Feature Fusion"
    ],
    "abstract": "Deep neural networks have evolved as the leading approach in 3D medical image segmentation due to their outstanding performance. However, the ever-increasing model size and computational cost of deep neural networks have become the primary barriers to deploying them on real-world, resource-limited hardware. To achieve both segmentation accuracy and efficiency, we propose a 3D medical image segmentation model called Efficient to Efficient Network (E2ENet), which incorporates two parametrically and computationally efficient designs. i. Dynamic sparse feature fusion (DSFF) mechanism: it adaptively learns to fuse informative multi-scale features while reducing redundancy. ii. Restricted depth-shift in 3D convolution: it leverages the 3D spatial information while keeping the model and computational complexity as 2D-based methods. We conduct extensive experiments on AMOS, Brain Tumor Segmentation and BTCV Challenge, demonstrating that E2ENet consistently achieves a superior trade-off between accuracy and efficiency than prior arts across various resource constraints. %In particular, with a single model and single scale, E2ENet achieves comparable accuracy on the large-scale challenge AMOS-CT, while saving over 69% parameter count and 27% FLOPs in the inference phase, compared with the previous\nbest-performing method. Our code has been made available at: https://github.com/boqian333/E2ENet-Medical.",
    "reviews": [
      {
        "text": "### Summary\n\nIn this paper, the authors propose a novel architecture that addresses the challenges observed in increasing the model size and computational complexity of neural network architectures. This leads to concerns in the deployment stage, mainly because of resource-limited hardware. The authors propose a 3D medical image segmentation model named Efficient to Efficient Network (E2ENet). They incorporated two designs to make the model efficient while preserving accuracy: Dynamic sparse feature fusion (DSFF) mechanism and Restricted depth-shift in 3D convolution.  Extensive experiments on three benchmarks show that E2ENet consistently achieves a superior trade-off between accuracy and efficiency compared to prior state-of-the-art baselines.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper is well-organized and well-written. \nThe paper reads well.\nThe motivation behind the study is clear.\nThe proposed restricted depth shift method is interesting and somewhat novel.\n\n### Weaknesses\n\nTables 3 and 4 don’t provide consistent results. It seems that the combination that works for CT does not optimally work for MRIs.\nMissing discussion on limitations.\n\n### Questions\n\nCan the proposed method accelerate training speed in terms of time to achieve SOTA accuracies? \nCould the authors provide information on the inference times?\n\n### Limitations\n\nThe authors didn't discuss the limitations of the proposed methodology.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThe paper introduces E2ENet, a novel neural network designed for 3D medical image segmentation, which emphasizes efficiency in computational resource usage without compromising accuracy. This paper introduces a Dynamic Sparse Feature Fusion (DSFF) mechanism that adaptively learns to integrate multi-scale features effectively and a novel application of restricted depth-shift in 3D convolution that aligns with the computational simplicity of 2D methods. The model demonstrates superior performance on various benchmarks like AMOS-CT challenge and BraTS Challenge in MSD, showcasing significant reductions in parameters and computational costs while maintaining competitive accuracy.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n(1) DFSS mechanism proposed provides a more efficient feature fusion process while reducing the computational and memory overhead. (2) E2ENet integrates depth-shift strategy in 3D convolution networks, enabling the ability for network to capture 3D spatial relationships. (3) E2ENet significantly reduces parameter size to 7.63 M minimally.\n\n### Weaknesses\n\nSection 3.2 Ablation Studies lacks of more insights about Table 3, detail question will be shown in Question part below.\n\n### Questions\n\nFor Section 3.2 Table 3, it seems that for the different shift size, the mDice score does not vary much from each other. What are authors’ insights about this? Based on this, how do authors justify in Section 2.3 that the shift magnitude would have a negative impact on the effectiveness of the shift operation?\n\n### Limitations\n\nYes, authors have addressed the limitations.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper introduces E2ENet, a 3D medical image segmentation model designed for efficiency and performance. E2ENet incorporates Dynamic Sparse Feature Fusion (DSFF) to adaptively fuse informative multi-scale features and a Restricted Depth-Shift mechanism in 3D convolution to maintain low model complexity. Extensive experiments demonstrate that E2ENet achieves a superior trade-off between accuracy and efficiency, significantly reducing parameter count and FLOPs compared to previous methods, particularly in large-scale datasets like AMOS-CT.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nImproved Efficiency: E2ENet significantly reduces the parameter count and FLOPs, making it more computationally efficient and suitable for deployment on resource-limited hardware without compromising on performance.\nInnovative Mechanisms: The introduction of Dynamic Sparse Feature Fusion (DSFF) and Restricted Depth-Shift in 3D convolution effectively balances the need for high accuracy with lower computational complexity, offering a novel approach to 3D medical image segmentation.\nRobust Validation: Extensive experiments on multiple challenging datasets demonstrate E2ENet's consistent performance, ensuring its reliability and applicability in various medical imaging scenarios, especially for exceeding nnUNet.\n\n### Weaknesses\n\n1. Unclear Backbone Network: The backbone network of E2ENet is not clearly defined. Figure 2 labels the left part as an \"efficient backbone,\" but it is ambiguous whether this refers to an EfficientNet-based backbone or simply a group of CNN layers. This lack of clarity can confuse readers and detract from the paper's overall comprehensibility.\n2. Segmentation Performance and Kernel Size: The segmentation performance is primarily evaluated on the BraTS and AMOS datasets. However, the ablation study on AMOS does not adequately address the suitability of the DSFF kernel size for BraTS. Table 4 lacks results for a kernel size of 3x3x3, which raises questions about the generalizability of the chosen kernel sizes across different datasets.\n3. Lack of Comparison with Recent Lightweight Networks: The paper does not include comparisons with recent lightweight network structures specifically designed for medical image analysis. This omission limits the assessment of how E2ENet stands relative to other contemporary, efficient models and reduces the comprehensiveness of the evaluation.\n\n### Questions\n\n1. Clarification on the Backbone Network:\nQuestion: Can you provide a more detailed description of the backbone network used in E2ENet? Specifically, is it an EfficientNet-based backbone, or is it composed of a different set of CNN layers?\nSuggestion: Consider revising Figure 2 and the corresponding text to clearly define the architecture of the backbone network. Providing explicit details will enhance the readers' understanding and reduce ambiguity.\n2. Kernel Size Generalization:\nQuestion: Why was the 3x3x3 kernel size not included in the ablation study results for the BraTS dataset in Table 4?\nSuggestion: It would be beneficial to include the results for the 3x3x3 kernel size in the ablation study for the BraTS dataset. This would help in understanding the generalizability of the DSFF mechanism across different datasets and ensure that the chosen kernel sizes are suitable for various segmentation tasks.\n3. Comparison with Recent Lightweight Networks:\nQuestion: Have you considered comparing E2ENet with other recent lightweight network structures specifically designed for medical image analysis?\nSuggestion: Including comparisons with recent lightweight networks would strengthen the paper by providing a more comprehensive evaluation of E2ENet's performance. This could involve benchmarking against models that are known for their efficiency and effectiveness in medical image segmentation.\n\n### Limitations\n\nNo\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Boqian Wu",
      "Decebal Constantin Mocanu",
      "Elena Mocanu",
      "Lu Yin",
      "Maurice van Keulen",
      "Mykola Pechenizkiy",
      "Qiao Xiao",
      "Shiwei Liu"
    ],
    "url": "pdfs/neurips.cc-2024-conference_8a0a4586b53364bfb4c24a094f9633385ac9ae31.pdf",
    "remote_url": "https://openreview.net/pdf/8a0a4586b53364bfb4c24a094f9633385ac9ae31.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "Active preference learning for ordering items in- and out-of-sample",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "active_learning"
    ],
    "keywords": [
      "ordering",
      "active learning",
      "preference learning",
      "medical imaging",
      "human feedback",
      "pairwise comparison"
    ],
    "abstract": "Learning an ordering of items based on pairwise comparisons is useful when items are difficult to rate consistently on an absolute scale, for example, when annotators have to make subjective assessments. When exhaustive comparison is infeasible, actively sampling item pairs can reduce the number of annotations necessary for learning an accurate ordering. However, many algorithms ignore shared structure between items, limiting their sample efficiency and precluding generalization to new items. It is also common to disregard how noise in comparisons varies between item pairs, despite it being informative of item similarity. In this work, we study active preference learning for ordering items with contextual attributes, both in- and out-of-sample. We give an upper bound on the expected ordering error of a logistic preference model as a function of which items have been compared. Next, we propose an active learning strategy that samples items to minimize this bound by accounting for aleatoric and epistemic uncertainty in comparisons. We evaluate the resulting algorithm, and a variant aimed at reducing model misspecification, in multiple realistic ordering tasks with comparisons made by human annotators. Our results demonstrate superior sample efficiency and generalization compared to non-contextual ranking approaches and active preference learning baselines.",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper considers the setting of learning an ordering between items according to some scoring rule. The assumption is that this ordering is determined by a contextual scoring rule, determined from the features of each item. Such contextual structure can aid in more rapidly learning an ordering, and generalizing to out of sample items. This ordering is learned from asking pairwise preferences to an oracle, reducing uncertainty about the total order. Since there are a large number of possible comparisons to be asked, active learning is deployed to only query labels for a subset of comparisons. A theoretical argument is made about the optimal balance between aleatoric and epistemic uncertainty to target in adaptive selecting queries, motivating the GURO adaptive sampling strategy and variants. The performance of GURO is demonstrated in several empirical experiments on simulated data and data collected (offline) from real humans.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 4\n\n### Strengths\n\nI believe this paper is excellent - it is *clearly* written, has a great flow, and theoretical and empirical arguments are tied together nicely to motivate the problem setting, establish the problem fundamentals, convey mathematical intuition about uncertainty reduction, and justify active selection. There is also a robust set of experiments demonstrating GURO (and variants) in practice against baselines, along with explanatory discussion and implementation details. To my knowledge the analysis and algorithmic ideas here are *original*, and this is a *high quality* submission. Although not the centerpiece of this work, in an age where RLHF and efficiently learning from human preferences is paramount in training large models and ranking queries, work in active preference learning and the contributions made here are *significant*. There is also a robust and thorough appendix providing details and theoretical proofs (disclaimer: I have read the main paper in careful detail, but only skimmed the appendix). Overall, this is elegant, interesting, and impactful work (both theoretically and empirically).\n\n### Weaknesses\n\nI do not have any explicit weaknesses to list. Instead, I have a list of comments and questions below that I would like the authors to address. However I feel confident that these can be addressed during the rebuttal phase.\n\nOne comment is that there is no discussion (unless I missed it) about computational complexity stating and comparing the big-O computational complexity of GURO, its variants, and other selection methods. This would be an interesting and strengthening addition to this work.\n\n### Questions\n\n- I think the statement \"Moreover, the set we want to order is often larger than the set of items observed during training—we may want to rank new X-rays in relation to previous ones. This cannot be solved using per-item scores alone.\" should be clarified. If one knew absolute scores for all items (regardless if they are observed in training), isn't it trivial to compute an ordering? Or did the authors mean that pairwise responses collected during training could not generalize outside of training, without example features to predict from? [Edit: this does seem to be clarified in Section 2, but should be made more clear in the introduction]\n- I find the sentence \"However, as we show in Section 4, learning this map to recover a complete ordering is distinct from the general preference learning setting, and existing algorithms lack theoretical justification for this application\" to be vague and should be clarified. What exactly is the \"general preference learning setting\", versus learning a map to recover complete orderings? What does \"this application\" refer to? Which of these two settings are you concerned with here?\n- I'm confused by line 159. Why can one lower bound $\\Delta_{ij}$ by a factor depending on the index difference $\\lvert i -j \\rvert$? Aren't the indices arbitrary, and agnostic to the underlying geometry of the feature space? Does this mean that a simple index permutation would drastically change this $\\Delta$ quantity?\n- in line 166, should the dependence not be on $\\theta_T$ rather than $\\theta_*$?. See line 157 which uses $\\theta_T$. line 169 also jumps back to $\\theta_T$\n- line 189 is missing an important point: by definition the entirety of $\\mathcal{I}$ is unavailable, only $\\mathcal{I}_D$ is available to select from. This should be commented on.\n- I think line 198 is too vague: \"As θt converges to θ∗, this pair becomes representative of the maximizer of (4) provided there is no major systematic discrepancy between ID and I.\" Can you comment more on what constitutes acceptable vs unacceptable discrepancies between I and I_D? This does bring up a problematic point: what if $I_D$ is not sufficiently representative of $I$? Line 215 starts to hint at this discussion but I think it needs to be elaborated on, ideally more formally\n- in GURO Hybrid, how are these $\\zeta_i$ parameters actually learned? Is it just a joint MLE on $\\theta$ and $\\zeta_i$? In this case, what prevents the model from learning an arbitrary $\\theta$ (i.e., $\\theta = 0$) and just using the full expressivity of $\\zeta_i$? Is there some sort of regularization in practice?\n- for completeness, can you include a figure in the Appendix showing the experiment in Fig 1b, but just plotting $R_{I_D}$ instead of the difference? It would be good to know how each algorithm does in an absolute sense on $I_D$\n- figure 2 would benefit from a log y scale - it is very difficult to discern between methods\n\nMinor:\n- line 147 uses the notation $\\widehat{H}$ instead of $\\widetilde{H}$. I assume this is a typo\n- missing left parentheses in line 5 of GURO algorithm\n- the word BayesGURO in Algorithm 1 should also probably be colored green to show the association to (7) and (11)\n- be careful with red and green as distinguishing colors for readers with color vision deficiency\n- fix quotes on line 258\n\n### Limitations\n\nYes\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nActive preference learning is different from deriving a complete ordering from preferences. It focuses on “If we collect comparisons D_T, how good is the resulting model’s predicted ordering in the item set”.  The paper proposes a sampling method in the active learning scenario. Theoretical analysis is also provided.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n--Preference learning is critical to many downstream tasks.\n\n--The proposed method is somewhat novel.\n\n--The theoretical analysis is provided.\n\n--Experimental results are shown to verify its effectiveness.\n\n### Weaknesses\n\n--The assumption 1 and 2 are not so intuitive. It is better to illustrate an example.\n\n--Baseline methods are weak. Though many related studies are mentioned in the related work section, performances of baselines are not shown in the experiments.\n\n--The number of comparisons is  not reduced tremendously on the ImageClarity in Table1.\n\n--Performances should be emphasized in terms of the prediction ordering quality.\n\n### Questions\n\nState-of-the-art baselines should be added for performance comparison.\n\n### Limitations\n\nLimitation should be added.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 4\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper considers the ranking problem based on pairwise comparisons. The goal is to get the best sampling strategy for the best ordering from a limited number of comparisons. Under a logistic model on the difference between scores, the authors provide the analysis for the upper bound on the ordering error, which provides insights on sample selections. Following the idea of minimizing the bound, this paper proposes the GURO algorithm for pair selections. The proposed method is evaluated in four image ordering tasks with either synthetic labels or real world labels.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe proposed algorithm is well motivated by the theoretical result on the ordering error. It has very strong theoretical guarantees on the performance. The theory presented in the paper looks good to me. And it helps the reader to understand the algorithm better with some justification from Bayesian analysis. \nI find the paper very well written. The way the authors presented the results is very clear and easy to follow.\n\n### Weaknesses\n\nAlthough the theory presented in the paper looks good to me, I find it very similar to the result presented in the original Logistic bandit paper [1]. By treating the input space as the difference between features, the problem is simplified to a standard logistic bandit problem. And The result in Lemma 1 and some analysis before Theorem 1 are very similar to Lemma 2 and 3 in [1]. While I understand Theorem 1 is specifically for the ranking error, I think it is still straightforward to get Theorem 1 from existing lemmas.\nAnd for the empirical study, I see the proposed approach actually does not always perform better than baselines, especially BALD. The good result is on synthetic data, there is no significant advance on real data. Therefore, I am not convinced with the claimed statement in the paper.\n\n### Questions\n\nPlease provide more discussion on how difference the proposed method is comparing to logistic bandits.\n\n### Limitations\n\nYes, the authors touch upon the limitation in the conclusion section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes an active learning algorithm for selecting pairs of items for comparison in order to learn a ranking of the items.\nThe ranking error is measured by the (normalized) number of swapped pairs compared to the true ordering (Kendall’s Tau distance). The algorithm chooses pairs of items based on an upper bound on this ranking error. Experiments are conducted on one synthetic and 3 real-world benchmarks.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n* Active learning for pairwise comparisons is an interesting problem to study\n* The approach is justified by theoretical analysis\n* The empirical evaluation looks promising\n\n### Weaknesses\n\n* The paper uses strong assumptions on the pairwise comparisons. Specifically, the response is assumed to be: $P(C_{ij}=1) = \\sigma(\\theta_*\\cdot(x_i-x_j))$, where $\\sigma$ is the sigmoid function (eq 2). These assumptions are used for fitting MLE parameters. These assumptions are common in the literature.\n\n* The optimization problem for choosing a pair (eq 5 and 7) seems hard. The cost is quadratic in the number of items since all pairs are considered. An approximation that depends linearly on the number of items would make the approach more practical.\n\n### Questions\n\n* “We restrict algorithms to only query pairs for which an annotation exists and remove the pair from the pool once queried.” – What is the number of available annotations for each dataset in figure 2 in the experiments? Only the number of items n and the dimension d are specified. Also, there is no notion of annotation noise here, since the labels are set, right?\n\n* What is $\\Delta_*$? It seems from line 159 that: $\\Delta_*=\\min_{i,j} \\Delta_{ij}/|i-j|$, but it would be better to define it explicitly. Does it really depend on the index difference $|i-j|$? What is the reasoning behind this?\n\nMinor:\n* Line 74: “Ordering algorithms based only on preference feedback cannot solve this problem since observed comparisons are uninformative of new items.” This statement is not clear, as long as items are represented as attributes/features $x_i$ then comparisons may be informative for new items.\n\n### Limitations\n\nI suggest adding the assumption on response (eq 2) and the $O(n^2)$ complexity to the list of limitations.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Devdatt Dubhashi",
      "Emil Carlsson",
      "Herman Bergström",
      "Fredrik D. Johansson"
    ],
    "url": "pdfs/neurips.cc-2024-conference_da593e89226482a54e1e10352890bd419526fb72.pdf",
    "remote_url": "https://openreview.net/pdf/da593e89226482a54e1e10352890bd419526fb72.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "Towards Multi-dimensional Explanation Alignment for Medical Classification",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "machine_learning_for_healthcare"
    ],
    "keywords": [
      "Explainable Medical Image",
      "Interpretable ML",
      "Explainable AI"
    ],
    "abstract": "The lack of interpretability in the field of medical image analysis has significant ethical and legal implications. Existing interpretable methods in this domain encounter several challenges, including dependency on specific models, difficulties in understanding and visualization, and issues related to efficiency. To address these limitations, we propose a novel framework called Med-MICN (Medical Multi-dimensional Interpretable Concept Network). Med-MICN provides interpretability alignment for various angles, including neural symbolic reasoning, concept semantics, and saliency maps, which are superior to current interpretable methods. Its advantages include high prediction accuracy, interpretability across multiple dimensions, and automation through an end-to-end concept labeling process that reduces the need for extensive human training effort when working with new datasets. To demonstrate the effectiveness and interpretability of Med-MICN, we apply it to four benchmark datasets and compare it with baselines. The results clearly demonstrate the superior performance and interpretability of our Med-MICN.",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors proposed a novel interpretable model called Med-MICN in this work. This method manages medical image classification tasks with multi-dimensional aspects with neural symbolic and concept semantics. With the help of LLMs, the work performed superior on four medical benchmark datasets. In the ablation study, the authors presented that the performance of the proposed method performed better with the complementary multi-dimensional loss functions in evaluating the classification result.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- The paper proposed a novel method that combines extra information (text and logic rules) to enhance the classification performance and the interpretability of the model outputs. This enables the interpretability of the deep learning classification model with multi-dimensional information.\n- Instead of post-hoc interpretable methods, the authors integrate fuzzy logic rules into the proposed method. This gives a clear decision rule for image classification and interpretability.\n- The method description is relatively clear and easy to follow. The ablation study gave a good overview of the usage of multi-dimensional information.\n\n### Weaknesses\n\n- The authors show that the proposed method performed superior to the baseline methods in image classification (Tab. 1). However, there is a missing part on the evaluation of the interpretability. How correct are those heatmaps generated from the proposed model, do they align with the labels or the clinicians?\n\n- Also the comparison of the interpretability between the proposed method and the baseline methods is missing. In Tab. 1, the authors mentioned those methods are interpretable, but they did not evaluate the interpretability. This could be crucial for medical applications.\n\n- For section 4.3, the description of Neural-Symbolic Layer is not easy to follow. What is the difference between the functions of \"Concept Polarity\" and \"Concept Relevance\"? It seems like their outputs are the same.\n\n### Questions\n\n- The authors used LLM (GPT-4V) in their proposed method. Would it be better or make a difference, if the LLM is a more medical-specific model?\n\n- How do authors deal with different numbers of concept sets from the LLM? If I understood correctly, the LLM could output several different concepts and does not necessarily fit into the input size of the text encoder. (Figure 2.a)\n\n- Did authors try with different $\\lambda_1$ and $\\lambda_2$?\n\n### Limitations\n\n- The proposed method provides an opportunity to interpret the medical image classification results and in the meanwhile also increase the classification performance. However, heatmaps look somehow blurry and may not be trustable (Fig 1).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 7\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis work introduces Med-MICN, an explainable framework for medical image classification. This framework leverages a concept bottleneck framework combined with a neural symbolic reasoning framework to generate simple explanations for its predictions. In general, strong performance is demonstrated.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 4\n\n### Strengths\n\nThe overall framework is interesting, and the idea of “multi-dimensional explainability” seems widely applicable. The ability to see some explanation for each concept’s logic seems useful, and the decision rules generated are quite simple. The performance of Med-MICN is quite compelling, with superior accuracy across multiple medical datasets.\n\n### Weaknesses\n\nIn general, the notation used in the paper is inconsistent and somewhat ill defined. For example, in section 3, N is used to denote the number of sample in a dataset, and k the number of concepts. In section 4, N becomes the number of concepts, and M the number of samples. Lower case k is then used to index along the height dimension of a feature map. Several objects ($I_{o, j}$ and $I_{r,j}$, for example) are not clearly described upon their introduction.\n\nThere are a few seemingly key hyper parameters ($\\lambda_1$, $\\lambda_2$, the threshold used for binarizing concept vectors), but the experimental details do not indicate that there was a validation/development set used to select these values.\n\nIf these two points can be addressed, I believe this paper will warrant acceptance.\n\n### Questions\n\nThere appear to be some notational issues in section 4.2 — initially H and W are used as the height and width of the feature map, but in the definition of $H_{p,k,i}$, it seems that P and K are used to fill the same roll.\n\n“to ensure the truthfulness of 196 concepts, we discard all concepts for which the similarity across all images is below 0.45. “ — does this refer to all training images?\n\nThe equation following line 214 is quite confusing — what is the dimensionality of each of the three values returned by $\\theta_c$? How does it return $f(x_m)$?\n\nDuring training, which components are trained jointly? It seems like $L_{neural}$ only applies to the neural symbolic component, while $L_c$ and $L_{task}$ apply only to the concept extraction component and some additional classification fully connected layer.\n\nFigure 4 is somewhat unclear — which cells in the figure should be compared to which?\n\n### Limitations\n\nDiscussion of limitations is appropriate.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper focuses on the explainable classification of medical images with multi-dimensional explanation. The proposed Med-MICN framework contains four modules including feature extraction, auto-annotation, concept embedding, and neural symbolic layer. The incorporation of fuzzy logic rules is novel in the interpretation study of deep neural networks. The experimental results show that Med-MICN surpasses the baseline networks such as vgg and resent, and also outperforms concept bottleneck models (CBM). The generated attention maps and predicted concepts match better than traditional CBM.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nSee summary\n\n### Weaknesses\n\n1. The whole presentation of the paper should be further improved including mathematical symbols, figures 2 and 3, and equations. The symbols should be better matched with Figures 2 and 3 to improve readability. It is difficult to understand these figures without reading the contents. Some symbols like Cosine should be given in the figures. Detailed notations are needed in the figures or in the captions.\n2. It is better to give pseudo codes of training and inferences to help understand the whole workflow.\n3. At the bottom of Page 3, \"In instances ... judgment\", why do the authors claim alternative explanations can aid judgment? Any references? More explanations may confuse the physicians.\n4. In line 131, does $y_{i}$ the one-hot labels?\n5. In the Concept labeling alignment module, it is not clear why the authors use \"average pooling to heatmaps\". I think this will lose some spatial localizations of lesions, for example, one heatmap with multiple lesions and another with one lesion, both of them may have similar average pooling results.\n6. Under the line 183, the $V$ is three-dimensional, and why here it has two subscripts.\n7. In line 194, does the representation be $c \\in \\{C_{1}, ..., C_{M}\\}$ ?\n8. In line 142, the logic symbols should be defined may be in supplementary.\n9. Lack of training details. In line 277, how the learning rate decay? Any data augmentations were used?\n\n### Questions\n\nSee weakness.\n\n### Limitations\n\nThe whole framework of Med-MICN is complex, and the so-called multi-dimensional explanations are attention maps and predicted concepts which have been proposed in traditional CBM models.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis work introduces a novel end-to-end concept-based framework called Med-MICN, which is quite inspiring and important for the next XAI era due to its multi-dimensional, powerful interpretability, and efficient performance. Furthermore, they propose an automated process for efficiently and accurately obtaining concept labels, which are costly to acquire in reality.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n1. This paper introduces a novel attempt that creatively integrates medical image classification, neural symbolic solving, and concept semantics. This integration enables multidimensional interpretability, making it more comprehensive and powerful compared to previous interpretable models.\n\n2. Med-MICN demonstrates superior accuracy and interpretability across datasets from different modalities.\n\n3. The ample experiments are convincing, as the authors conducted experiments on multiple datasets and baselines to validate the model's interpretability and accuracy, while also providing rich and impressive visualization results.\n\n### Weaknesses\n\n1. The caption of the picture is not detailed enough, as shown in figure 3.\n\n2. The author considers interpretability on the basis of concept embedding, neural-sybolic reasoning and saliency map. The author lacks the reason why it is necessary to analyze the interpretation from these aspects and whether there are other more dimensions.\n\n### Questions\n\nSee weeknesses.\n\n### Limitations\n\nNA\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Di Wang",
      "Hongbin Lin",
      "Hongru Xiao",
      "Jingfeng Zhang",
      "Lijie Hu",
      "Lu Yu",
      "Songning Lai",
      "Wenshuo Chen"
    ],
    "url": "pdfs/neurips.cc-2024-conference_e2bc722a923c9dbaf0468dbd8b8855e3ebe40fa3.pdf",
    "remote_url": "https://openreview.net/pdf/e2bc722a923c9dbaf0468dbd8b8855e3ebe40fa3.pdf",
    "venue_id": "neurips.cc-2024-conference"
  },
  {
    "title": "SegVol: Universal and Interactive Volumetric Medical Image Segmentation",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "machine_learning_for_healthcare"
    ],
    "keywords": [
      "Volumetric Medical Image Segmentation",
      "3D Segmentation Foundation Model",
      "Universal and Interactive 3D Segmentation"
    ],
    "abstract": "Precise image segmentation provides clinical study with instructive information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of a 3D foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. To facilitate efficient and precise inference on volumetric images, we design a zoom-out-zoom-in mechanism. Extensive experiments on 22 anatomical segmentation tasks verify that SegVol outperforms the competitors in 19 tasks, with improvements up to 37.24\\% compared to the runner-up methods. We demonstrate the effectiveness and importance of specific designs by ablation study. We expect this foundation model can promote the development of volumetric medical image analysis. The model and code are publicly available at https://github.com/BAAI-DCAI/SegVol.",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper describes the utilization of SegVol, a deep learning model, to segment any organ/tumor/lesion on 3D CT data.\n\n**objectives**\nCreate a universal segmentation model that can segment with :\n-  any type of labeling\n-  good performances on complex tasks\n-  low computational cost (i.e., no sliding window)\n  \n**contributions**\nThe authors contribute to the state of the art in 3 ways :\n- first model for segmenting over 200 anatomical categories\n- support spatial (points/bbox) and semantic prompt (text describing each category)\n- introduction of a zoom-out-zoom-in mechanism to improve the model's efficiency and accuracy\n\n**datasets**\n- 25 open-source segmentation CT datasets + background filtering \n- generation of pseudo-labels with Felzenswalb-Huttenlocher to remove spurious correlation between the datasets\n\n**architecture**\nThe whole architectures is divided into four blocks:\n- image encoder: 3D ViT pretrained using SimMIM on 96K unlabeled CTs + fine-tuning on 6K labeled CTs\n- text encoder: the CLIP model is used to generate a text embedding from the text prompt. The text prompt only consists in the name of the anatomical region to be segmented.\n- prompt encoder: text embedding and spatial prompt (bbox+points) are given to generate the prompt embedding.\n- mask decoder: from the prompt embedding, the segmentation is generated.\n\n**training algorithm**\n- the encoder is pretrained using SimMIM\n- the model is fine-tuned using an algorithm described in appendix B. For each epoch, an iteration of training is made using the ground truth, and a second one using the pseudo masks.\n\n**results**\nExperimental setup is given, as well as a description of the three external datasets used for the evaluation.\n- comparison with SAM-like methods : Table 2 shows the superiority of the method with Dice score > 0.7 on each task. Figure 2 helps to visualize the dice score distribution for each model, on each task. An additional comparison to standard fully supervised segmentation models such as nnUNet, SwinUNetR is provided. Superiority of SegVol is also shown compared to this model on Figure 8 and Table 6.\n- ablation studies\n  - Zoom-out-zoom-in mechanism : Table 3 shows the interest of the method, improving performances and reducing duration compared to sliding window\n  - Scaling up training data : Fig 3.(a) shows that the model performances improve as the number of training data increases.\n  - Prompt impact : Fig 3 (a and b) shows that point prompt is inferior to text prompt, and best prompt combination is bbox+text\n- Case studies\n  - Disambiguation via semantic-prompt: Figure 4 shows that when the spatial prompt is not sufficient to describe the segmenting task, the text prompt adds the complementary information to locate the good anatomical region.\n  - spatial-prompt impact: Figure 5 shows how the points prompt are related to the results. When choosing two points, the category becomes clear, whereas with one point can identify two categories.\n\n**discussion**\n- Scalability : the authors believe that the model have a strong model scalability.\n- Generalizability : Generalization of SegVol to MRI is discussed, with an example given in appendix C showing a strong dice score of 80%.\n- Limitations : only limitation is that text prompt only consider a category, and not a full sentence with logical reasoning.\n- Broader impact : the authors believe that their method is universal and doesn't have any negative impact.\n\n### Soundness: 4\n\n### Presentation: 2\n\n### Contribution: 4\n\n### Strengths\n\n- **originality** : the concept of SegVol is not original itself. However, the addition of the zoom-in-zoom-out principle and the combination of bbox + text + points prompts is new in this kind of models.\n- **quality** : the work is done rigorously, respecting a scientific reasoning all along the article. \n- **clarity** : the paper lacks clarity as there are two parts difficult to understand.\n  - 1) The description of the training algorithm is not made in the text, only the steps are written in appendix B.\n  - 2) The hyperparameters regarding the model's dimension (number of layer, ViT dimension) is not explicited.\n- **significance** : The results are impactful for the community of medical image segmentation, and could have plenty of applications. The property of segmenting multiple organs, from multiple entirely different datasets is remarkable.\n\n### Weaknesses\n\nThe only weakness regards the clarity of the training algorithm, which is hard to understand even with appendix B and training steps.\nThis point could be improved by describing the steps in the appendix with a paragraph, or maybe with another diagram that follows the exact same steps.\n\n### Questions\n\n**datasets**\nl. 83 : Why is Felzenswalb-Huttenlocher (FH) algorithm to generate pseudo masks ?\nl. 81 : How is it supposed to remove spurious correlation between datasets ?\n\n**architecture**\nl. 96 : The architecture principle is described, but some details are lacking such as the number of layers of each network, the number of heads in the ViT, etc.\nl. 133 : why is the computational cost reduced using the zoom-in-zoom-out principle compared to sliding window? clarify this point.\nl. 140 : how are prompts generated from the coarse segmentation masks? clarify if the prompts are generated thanks to the prompt encoder as a bbox, or points.\n\n**results**\nl. 169 : Could you indicate the training time (SimMIM and finetuning) of the model?\nl. 169 : Why did you choose these 3 specific datasets as external test sets?\n\n**discussion**\nl. 257 : Why is data salability a good thing ? Why is this part in the discussion? The data scalability of the model is not discussed or compared with other models. If not discussed, move to the conclusion.\n\n### Limitations\n\nOnly one limitation is addressed, considering the text prompt which cannot be a logical sentence at the moment.\nHowever, the amount of data required to train the model is not discussed. Would a few-shot finetuning method work on small datasets (10 samples)?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nThis paper proposes a 3D foundation segmentation model, named SegVol, supporting universal and interactive volumetric medical image segmentation. By scaling up training data to 90K unlabeled Computed Tomography (CT) volumes and 6K labeled CT volumes,  this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. Besides, a zoom-out-zoom-in mechanism is designed to facilitate efficient and precise inference on volumetric images.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n(1) Collect and process 25 public volumetric medical segmentation datasets, encompassing over 200 anatomical categories. The pseudo label is introduced to relieve the spurious correlation in the training data.\n(2) Implement massive 3D pre-training on 96K CT volumes and supervised fine-tuning on the  6k labeled datasets.\n(3) Support spatial-prompt, semantic-prompt, and combined-prompt segmentation, achieving  high-precision segmentation and semantic disambiguation.\n(4) Design a zoom-out-zoom-in mechanism that significantly reduces the computational cost, meanwhile preserving precise segmentation.\n\n### Weaknesses\n\n(1) As shown in Appendix Table 5, the numbers of trainset volumes is very dunbalanced， which will affect the performance of the proposed method. How the author address the above issue? They should give further analysis and validation.\n(2) For unseen anatomical categories, what is the segmentation effect of SegVol? Provide relevant experiments for further explanation.\n(3) What are the complexity、number of parameters and running speed of the SegVol, and provide a comparison with existing SOTA methods.\n(4) Minor: The reference format should be unified，for example,[8]、[43]\n\n### Questions\n\nAnswer the questions in Weakness Section\n\n### Limitations\n\nNot Applicable\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 7\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      },
      {
        "text": "### Summary\n\nIn this paper, the authors proposed SegVol, a 3D foundation segmentation model for medical images. This model supports universal and interactive volumetric medical image segmentation of more than 200 anatomical categories. This model is also well-designed with spatial and semantic prompts.\n\nIn the inference stage, the authors designed a zoom-out-zoom-in mechanism to enable efficient and precise segmentation results\n\nTheir extensive experiments show their method surpasses other SAM-like interactive segmentation methods.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 4\n\n### Strengths\n\nInteresting concepts. The idea of integrating spatial and semantic prompts is interesting and achieves prospective results.\n\nPaper clarity. The paper is overall well-written and structured. I also enjoyed the quality of the figures which help understanding the method.\n\nGood results. The method achieves SoTA results compared with the SAM-like interactive segmentation methods.\n\n### Weaknesses\n\nI had a hard time finding weaknesses in the paper. Those I find are either nitpick or more directions for future work. \n\nI put the weaknesses in the questions section.\n\n### Questions\n\nI would appreciate it if the authors could answer my questions.\n\n1. Figure 2, the Violin plots seem confused and it is not cited in the paper. Can the authors explain more about this figure?\n\n2. Will some of the 25 public volumetric medical segmentation datasets share the same data? Can the authors provide more detailed information about the datasets?\n\n3. How do the authors process the collected data with inconsistent annotations? For example, different hospitals will have different annotations of the Aorta. Will this influence the results?\n\n### Limitations\n\nThe authors adequately addressed their limitations in the manuscript as well as the appendix.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        }
      }
    ],
    "authors": [
      "Bo Zhao",
      "Fan BAI",
      "Tiejun Huang",
      "Yuxin Du"
    ],
    "url": "pdfs/neurips.cc-2024-conference_be1c6fd41b6a35d009d999195ed528afb6b69f1b.pdf",
    "remote_url": "https://openreview.net/pdf/be1c6fd41b6a35d009d999195ed528afb6b69f1b.pdf",
    "venue_id": "neurips.cc-2024-conference"
  }
]