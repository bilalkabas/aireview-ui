[
  {
    "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical Dataset",
      "Breast Cancer Bone Metastasis",
      "Diagnosis",
      "Prognosis",
      "Sparse CT reconstruction",
      "CT",
      "X-ray",
      "Large language model",
      "AI for Science"
    ],
    "abstract": "Breast cancer bone metastasis (BCBM) affects women‚Äôs health globally, calling\n for the development of effective diagnosis and prognosis solutions. While deep\n learning has exhibited impressive capacities across various healthcare domains, its\n applicability in BCBM diseases is consistently hindered by the lack of an open,\n large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis\n (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical\n resource, which is derived from a well-accepted murine BCBM model. The unique\n advantage of BoneMet over existing human datasets is repeated sequential scans\n per subject over the entire disease development phases. The dataset consists of\n over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D\n CT scans, and detailed biological data (e.g., medical records and bone quantitative\n analysis), collected from more than five hundreds mice spanning from 2019 to\n 2024. Our BoneMet dataset is well-organized into six components, i.e., Rotation\nX-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. We further\n show that BoneMet can be readily adopted to build versatile, large-scale AI models\n for managing BCBM diseases in terms of diagnosis using 2D or 3D images, prognosis of bone deterioration, and sparse-angle 3D reconstruction for safe long-term\n disease monitoring. Our preliminary results demonstrate that BoneMet has the\n potentials to jump-start the development and fine-tuning of AI-driven solutions\n prior to their applications to human patients. To facilitate its easy access and\n wide dissemination, we have created the BoneMet package, providing three APIs\n that enable researchers to (i) flexibly process and download the BoneMet data\n filtered by specific time frames; and (ii) develop and train large-scale AI models for\n precise BCBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets at https://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) at https://pypi.org/project/BoneMet. Code and tutorials are available at https://github.com/Tiankuo528/BoneMet.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a novel large-scale dataset (named BoneMet) of Breast tumor bone metastasis for disease diagnosis, prognosis, and treatment management. It consists of six components: Rotational X-Ray Imagery, Reconstructed CT Imagery, Segmented CT Imagery, Registered CT Imagery, Region of Interest CT Imagery, and Mice Medical Records. Besides, the author conducted a series of experiments on the BoneMet dataset by developing various deep learning models to exhibit its applicability and efficiency in managing BTBM disease. The enormous open-source dataset has significant implications for the development of new algorithms in this field beyond doubt.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n(1) The motivation of this study is clear and the background is well presented in the manuscript.\nThe main contribution of this work is to release a large-scale breast tumor bone metastasis dataset BoneMet, for the research community. Upon the subsets of the BoneMet, the researchers are allowed to develop some novel approaches to solve the task of breast tumor bone metastasis diagnosis and prognosis, and thus facilitating the automated analysis for breast tumor bone metastasis.\n(2) Apart from the information of the collected dataset, the authors also developed several deep learning models to validate the applicability and efficacy of BoneMet.\n(3) Dataset, code and tutorials are made available for free use.\n\n### Weaknesses\n\n(1) The technical contributions of this study are quite marginal, as most of the deep learning models are built upon existing methods.\n(2) Some details about the dataset preparation and experiments are missing.\n(3) Some settings and analyses in the experiment may not support the objectives.\n(4) The quality of the labels, particularly the pixel-wise annotation of the bone, might be questionable.\n\n### Questions\n\n1. Authors describe that Seg-CT and Recon-CT are independent. What is the difference between 3D CT scans in Recon-CT and Seg-CT? What is the meaning of the adjective 'segmented'?\n2. Line 341 \"Second, we observe a significant training-test accuracy gap of 20.4% with ViT (w/o STA), indicating a pronounced overfitting issue inherent to the ViT architecture.\"\nI think ViT, as a visual backbone network, is less overfitting compared to other structures such as CNN. In addition, is the decrease in accuracy necessarily overfitting?\n3. Section 3.1, the purpose and analysis of experiments is confusing. Why comparing two ViT models could demonstrate the applicability of the Rotation-X-Ray dataset to manage BTBM disease? What is the relation between the effectiveness of\nSTA and the applicability of the Rotation-X-Ray dataset? All subsections 3.2-4 have similar questions.\n4. The experiment did not report data partitioning.\n5. Table 2 reported metrics on the training and testing data. Is there any evaluation/dev data?\n6. The experiment reported many indicator values, but their actual clinical significance has not been evaluated. For example, what are the Precision/Recall/F1-Score/Accuracy of clinical experts on the BTBM diagnosis? Besides, the 3D-GAN, T-VAE, and ST-VAE methods achieve SSIM values of 0.767, 0.817, and 0.860. So, what role can they play in clinical practice?\n7. The authors used many existing softwares as the processing tools to generate the sub-datasets of BoneMet. Please provide the corresponding reference and specify whether it is free of charge or not. Also, please accurately explain in which case the datasets are processed by the Seg-API, Regist-API and RoI-API that were developed by the authors.\n8. Are Pre-Op and Post-Op refer to pre-operation and post-operation? Please give the full spelling.\n9. For RoI-CT, the tibiae ROI is assigned with different pixel values. As shown in Figure 4, for each CT example, is the processed CT image contains the values of 180, 240, 60 and 0 only? If yes, the original CT turns to a segmentation mask instead, and lots of information would be lost.\n10. In the whole preparation procedure, it seems that the manual annotation of the organs or tumors is not involved, except for the manual cropping of fibular by  CTAn¬Æ. How could we ensure the effectiveness of the existing software?\n11. In Table 1, the column \"Spatial Resolution\" might be accurately filled out as most of them indicate the organs. Please revise it.\n12. Some details about the BoneMet dataset are missing. For instance, the number of mice is not introduced. Besides, in Table 1, the images/records are classified into two categories, i.e., positive and negative. Is the label assigned at the slice level or animal level? Besides, we noticed that the numbers of Seg-CT are larger than those of Recon-CT, which makes it confusing, as the Seg-CT is obtained from Recon-CT.\n13. I also have concerns about the MiceMediRec dataset. It mainly consists of the medical record and quantitative analysis results of 3D CT images, simulation and mechanical testing. The authors list only a part of the features in Table S2 in the supplementary file. However, all feature should be provided. The off-the-shelf software, such as CTAn¬Æ, was used to generate the quantitative parameters, which took about half an hour for each case of animal. In Section 3.3, the multi-modal prognostic assessment model first leveraged the GAN to produce the CT scan of the future point and use the quantitative data in the MiceMediRec dataset as the ground truth label to validate the predicted reaction force values. However, the details about how to obtain the predicted reaction force values are not clear.\n14. In my opinion, multi-modal learning might refer to a way to integrate the information of multi-source data to build a model. However, the application in Section 3.3 just used CT data as model input. Please consider revising it.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe authors have obtained ethical approval from their institution, and thus, so the ethics review is not needed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *BoneMet*, a large-scale, open-access dataset for studying breast tumor bone metastasis (BTBM). The resource encompasses six components, including rotational X-ray, reconstructed CT, segmented CT, registered CT, region of interest CT, and mice medical records. The authors further validate the dataset‚Äôs utility through experiments using several deep learning models. The paper‚Äôs motivation is clearly articulated, and its main strength lies in providing an openly available dataset that can facilitate further research in BTBM diagnosis, prognosis, and treatment management. Overall presentation is adequate, though some methodological and descriptive details are lacking.\n\n**Major Comments**  \n1. **Technical novelty**: The study‚Äôs technical contribution appears limited, as the applied deep learning models are primarily based on existing architectures without clear methodological innovation.  \n2. **Dataset details**: Important information about dataset preparation, labeling quality, and sample counts (e.g., number of mice, labeling granularity) is insufficient. Clarification is also needed on how different sub-datasets (e.g., Seg-CT and Recon-CT) are defined and generated.  \n3. **Experimental design**: The purpose and logic of the experiments are unclear. The relationship between specific model comparisons (e.g., ViT variants, STA component) and the dataset‚Äôs applicability to BTBM management requires explanation.  \n4. **Data management and evaluation**: The manuscript omits key details such as data partitioning schemes, use of validation sets, and the clinical interpretability of reported metrics.  \n5. **Annotation and labeling accuracy**: Concerns remain regarding the reliability of labels, particularly for pixel-level segmentation, given the apparent absence of manual annotation steps.  \n6. **Use of external software**: The manuscript should include references and licensing details for external tools used in dataset processing, and clarify when custom APIs were applied.  \n7. **MiceMediRec dataset**: The description of this sub-dataset is incomplete, lacking full feature lists and clear explanation of how predicted and ground-truth values were generated.  \n8. **Terminology and conceptual clarity**: Terms such as ‚Äúmulti-modal learning,‚Äù ‚ÄúPre-/Post-Op,‚Äù and ‚Äúsegmented CT‚Äù should be precisely defined and consistently applied.\n\n**Minor Comments**  \n- Correct inconsistent entries in Table‚ÄØ1, including spatial resolution fields.  \n- Specify whether RoI-CT values represent discrete masks or continuous CT intensity values.  \n- Provide complete spellings and clarify acronyms.  \n- Ensure figures and tables contain sufficient labeling and annotations for full interpretability.  \n\n**Summary Paragraph**  \nThis paper makes a valuable contribution by releasing a comprehensive dataset that may advance BTBM research. The provision of code and tutorials further supports reproducibility. However, the technical and methodological aspects of the experiments need clearer justification, and several dataset preparation and documentation details are incomplete. Improving the transparency of data generation, labeling, and evaluation would substantially strengthen the manuscript‚Äôs scientific soundness and usability.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The dataset release is significant, but the manuscript requires substantial clarification and methodological refinement before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces BoneMet, the first large-scale, open dataset specifically designed for BTBM research. This dataset offers over 50 terabytes of high-resolution, multi-modal data, including 2D X-rays, 3D CT scans, and comprehensive biological records collected from thousands of mice. Structured into six components, the dataset may be suitable for a range of AI tasks focused on BTBM diagnosis, prognosis, and treatment. The authors also conducted extensive experiments to demonstrate the usability of this dataset. The dataset, APIs for flexible data processing and retrieval, accompanying code, and tutorials are freely available. To some extent, this dataset makes a valuable contribution to this field.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.This paper presents a large-scale, open dataset for the diagnosis and prognosis of breast tumor bone metastasis (BTBM), with a total volume of over 50 terabytes, encompassing various modalities. To some extent, this dataset makes a valuable contribution to this field.\n2.The authors have developed a specialized toolkit for accessing and processing the dataset, facilitating its use for researchers in the field.\n3.The authors conducted detailed experiments that demonstrate the usability of this dataset for some tasks related to BTBM diagnosis and prognosis.\n\n### Weaknesses\n\n1.This dataset is derived from mice, and there are certain differences between mouse skeletons and human skeletons. Can models trained on this dataset be effectively transferred to the diagnosis and prognosis of human BTBM? If applicable, how is the performance? If not, what clinical value does this dataset or model hold?\n2.The authors do not provide sufficient details regarding the experimental setup, particularly how the data was partitioned. The experiments appear to be internal validations conducted solely within the dataset, with no external validation on other datasets. This raises concerns about the generalization ability of the models trained on this dataset. Therefore, it remains uncertain whether this dataset can be used to build versatile large-scale AI models or foundational models.\n\n### Questions\n\nrefer to the weaknesses.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **BoneMet**, an open, large-scale dataset specifically developed for breast tumor bone metastasis (BTBM) research. The dataset includes over 50 terabytes of high-resolution, multi-modal data‚Äî2D X-rays, 3D CT scans, and biological records from thousands of mice‚Äîorganized into six components supporting diverse AI tasks in diagnosis, prognosis, and treatment modeling. The authors also provide APIs, processing toolkits, code, and tutorials to promote usability. Overall, the work presents a useful resource for the BTBM research community, with experiments demonstrating the dataset‚Äôs potential applications.  \n\n**Major Comments**  \n1. **Translational Relevance**: The dataset is derived from mouse skeletons, which differ anatomically from human skeletons. The review questions whether models trained on this dataset can effectively transfer to human BTBM diagnosis and prognosis. The authors should clarify whether cross-species validation was attempted, what performance was observed, and what the resulting clinical value of the dataset or associated models might be.  \n2. **Experimental Details and Generalization**: The experimental setup lacks sufficient detail, particularly regarding data partitioning. Current experiments appear to involve only internal validations within the same dataset, without external testing. This limitation raises concerns about model generalization and the broader applicability of the dataset for developing large-scale or foundation models in BTBM research.  \n\n**Minor Comments**  \n- While the paper is generally informative, the presentation could be improved by elaborating on the experimental setup and providing explicit partition and validation schemes.  \n- The manuscript might benefit from improved organization and figure clarity to enhance accessibility for readers unfamiliar with BTBM datasets.  \n\n**Summary Paragraph**  \nThis work‚Äôs primary strength lies in creating a comprehensive, multi-modal, open-access dataset with supporting tools that could benefit future AI research in BTBM. However, its scope is limited by potential species differences and insufficient external validation. Clarifying these aspects would strengthen confidence in the dataset‚Äôs biomedical utility and the reproducibility of experimental findings.  \n\n**Decision Recommendation**: **Major Revision** ‚Äì The dataset is promising and valuable, but concerns regarding translational relevance and insufficient detail in the experimental validation must be thoroughly addressed before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors integrated a publicly available large-scale Bone Metastasis (BoneMet) dataset with multimodal medical data, including 2D X-ray images, 3D CT scans, and comprehensive biological data. The primary objective of this database is to facilitate the diagnosis, prognosis, and treatment management of bone metastasis associated with breast tumors.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well-organized and clearly written.\n\nOur collected dataset is open, large-scale, and consists of multi-modal resources including 2D X-ray images, 3D CT scans, and medical records and quantitative analysis.\n\nThe author provides a tool on the Python Package Index (PyPI) for assisting researchers and practitioners.\n\nThe authors have conducted multiple benchmarks, such as BTBM Diagnosis to exemplify the utilisation of the collated dataset.\n\n### Weaknesses\n\nIn the abstract, the author states, ‚ÄúBreast tumor bone metastasis (BTBM) affects women‚Äôs health globally, necessitating the development of effective solutions for its diagnosis and prognosis.‚Äù However, the dataset collected was derived from mice rather than human patients.\n\nThe author should perform a comprehensive comparison between the BoneMet dataset and previously available datasets, including those derived from human subjects, to elucidate the differences and scales involved.\n\nAdditionally, the methods employed in each benchmark were limited. The author should justify the selection of the included methods over others.\n\n### Questions\n\nThis paper comprises two main components: the dataset and the benchmark. Both sections could benefit from additional details and comparisons.\n\nThe authors should evaluate and clarify how the collected data can directly benefit human health. For example, they could discuss the potential for training models on their dataset and subsequently transfer the obtained model to human datasets.\n\nFurthermore, the figures in the manuscript require better organization, as the text is currently too small for effective readability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a publicly available Bone Metastasis (BoneMet) dataset that integrates multimodal medical data, including 2D X-ray images, 3D CT scans, and biological and clinical information. The stated goal is to support diagnosis, prognosis, and treatment management of bone metastasis in breast tumors. The paper is clearly written, well-organized, and supported by benchmark experiments demonstrating potential applications of the dataset.  \n\n**Major Comments**  \n1. **Data Source vs. Claimed Clinical Relevance:** The abstract emphasizes the global impact of breast tumor bone metastasis in women, yet the dataset is derived entirely from mice. This discrepancy should be clarified to avoid misinterpretation of the dataset‚Äôs direct clinical relevance.  \n2. **Comparative Analysis:** A more comprehensive comparison between BoneMet and existing datasets‚Äîparticularly those based on human data‚Äîis necessary to highlight similarities, differences, and relative scale. Such a comparison would strengthen the dataset‚Äôs positioning and potential utility to the research community.  \n3. **Benchmark Justification:** The manuscript includes limited benchmark experiments. The authors should justify their choice of methods and discuss why certain approaches were selected over others. A more thorough evaluation would better demonstrate the dataset‚Äôs capabilities.  \n4. **Detail on Translational Relevance:** The paper should explicitly discuss how models trained on BoneMet could be transferred or adapted to human clinical contexts, clarifying the potential impact on healthcare applications.  \n\n**Minor Comments**  \n- Improve figure organization and readability; current text sizes are too small.  \n- Provide additional implementation details where possible to better support reproducibility.  \n\n**Summary Paragraph**  \nOverall, the study presents a valuable multimodal and open-access resource with meaningful potential for preclinical research. Strengths include clear writing, comprehensive data integration, and public accessibility through a PyPI tool. However, limitations related to the dataset‚Äôs animal origin, limited benchmarking, and insufficient comparison with existing datasets reduce its immediate translational relevance. Clarifying these aspects would significantly improve the paper.  \n\n**Decision Recommendation:** Major Revision",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase 1 ‚Äì Analytical & Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript clearly falls within the methodological scope of *IEEE Transactions on Medical Imaging (TMI)*.  \nAlthough it primarily introduces a dataset rather than a new algorithmic framework, it directly supports medical imaging methodology by enabling development and benchmarking of imaging reconstruction, segmentation, registration, and prognosis models. The inclusion of multiple imaging modalities (2D X-ray, 3D CT) and temporal alignment further strengthens its relevance for imaging physics, reconstruction, and AI-based image analysis.\n\n**2. Novelty & Contribution Level**  \n‚ö†Ô∏è Moderate. The work‚Äôs central novelty is the assembly and dissemination of an exceptionally large, multi-modal, longitudinal murine imaging dataset specialized for breast cancer bone metastasis (BCBM). While the technical foundation (image acquisition and FBP-based CT reconstruction) is standard, the dataset‚Äôs scale, longitudinal structure, systematic processing, and accompanying APIs for segmentation, registration, and RoI cropping provide methodological value. However, most methodological demonstrations (e.g., ViT diagnosis, generative prognosis, sparse-angle reconstruction) reuse established algorithms to illustrate utility rather than introduce fundamentally new imaging methodology.\n\n**3. Technical and Experimental Rigor**  \nHigh in data collection, moderate in validation. The paper meticulously documents imaging parameters, animal model design, sample numbers, temporal alignment, data organization, and preprocessing pipelines‚Äîindicating strong engineering and reproducibility standards. The demonstration studies show feasibility (diagnosis, prognosis, sparse reconstruction) but are limited in depth of experimental comparison and statistical evaluation. Metrics are provided, but details on error analysis, uncertainty, or generalization to human data are minimal.\n\n**4. Clarity and Presentation**  \nGenerally clear and well-structured; grammar and formatting are acceptable though inconsistent spacing, capitalization, and symbol rendering (e.g., ‚Äú0¬∞0.8¬∞1.6¬∞...‚Äù) and minor typographical errors recur. Figures are informative but sometimes heavily compressed. Section organization and dataset tables are comprehensive.\n\n**5. Ethical and Reproducibility Compliance**  \nStrong compliance. IACUC approval is explicitly stated with details on anesthesia, radiation dose, and humane animal treatment. Data is publicly released (Hugging Face, PyPI, GitHub), ensuring reproducibility and accessibility. Ethical data handling appears compliant with TMI standards.\n\n---\n\n## **Phase 2 ‚Äì IEEE TMI Review Report**\n\n### **1. Summary**\nThe authors present **BoneMet**, a large-scale, longitudinal, multi-modal murine dataset designed for research on breast cancer bone metastasis. The dataset contains over **67 TB** of high-resolution rotational X-rays and 3D CT scans from **500+ mice** over five years, complemented by biological and mechanical metadata. Data are organized into six components (Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec). The accompanying **BoneMet Python package** provides APIs for CT segmentation, registration, and region-of-interest cropping. Demonstration studies validate dataset usability in (i) X-ray/CT-based disease diagnosis, (ii) prognosis via 3D generative modeling and biomechanical simulation, and (iii) sparse-angle CT reconstruction using NeRF variants.\n\n### **2. Strengths**\n- **Significant dataset contribution:** First open, large-scale, longitudinal murine imaging database addressing a major gap in preclinical BCBM research.  \n- **Comprehensive documentation:** Detailed acquisition, reconstruction, labeling, and ethical compliance, promoting reproducibility.  \n- **Multi-modality & temporal richness:** Enables diverse imaging tasks including segmentation, registration, longitudinal progression modeling, and reconstruction.  \n- **Practical infrastructure:** Public dataset hosting, APIs, and tutorials lower access barriers for the imaging community.  \n- **Clinical and translational potential:** Provides a valuable foundation for deep-learning approaches in BCBM diagnosis and mechanical property prediction.\n\n### **3. Weaknesses**\n- **Limited methodological novelty:** The imaging pipeline primarily employs established methods (FBP, simple segmentation/registration), and the deep-learning models are off-the-shelf architectures used for illustration.  \n- **Insufficient methodological benchmarking:** Experimental sections demonstrate feasibility but lack comparison with alternative datasets or statistical robustness (error bars, uncertainty analysis).  \n- **Scope‚Äìfit risk:** As a dataset paper, the contribution may align better with *TMI‚Äôs Dataset and Benchmark Notes* if intended primarily as a resource rather than a methodological advance.  \n- **Presentation polish:** Occasional formatting, typographic inconsistencies, and figure readability issues.  \n- **Limited generalization evidence:** No demonstration on human data or cross-modality adaptation, leaving translational claims speculative.\n\n### **4. Major Comments**\n1. **Clarify methodological innovation:** Articulate explicitly how BoneMet advances medical imaging *methodology* rather than serving solely as a data resource (e.g., novel CT alignment pipeline, automated longitudinal registration protocols, or quantitative validation routines).  \n2. **Benchmarking and comparative analysis:** Provide systematic evaluations across algorithms, not just proof-of-concept. For instance, compare STA-ViT, CNN, and other temporal models statistically over multiple folds; quantify uncertainty across subjects.  \n3. **Dataset citation and maintenance:** Include DOIs, version control plans, and data access statements consistent with IEEE reproducibility guidelines.  \n4. **Broaden generalization discussion:** While murine data are preclinical, briefly assess representativeness and potential transfer to human imaging scenarios.  \n5. **Condense or restructure the manuscript:** Some content is descriptive (e.g., operational minutiae) and could move to supplementary material to emphasize scientific insights within main text.\n\n### **5. Minor Comments**\n- Correct spacing and capitalization in headings (e.g., ‚ÄúA N OPEN LARGE -SCALE MULTI -MODAL‚Ä¶‚Äù).  \n- Figures 1‚Äì4: improve font sizes and legends for readability.  \n- Provide consistent units (¬µm vs. micron, Gy vs. mGy).  \n- Ensure reference formatting conforms to IEEE style (e.g., numbered citations with bracketed numbers).  \n- Minor grammar issues (use ‚Äúdata are‚Äù instead of ‚Äúdata is‚Äù, etc.).  \n- Verify that ‚ÄúMMST-ViT,‚Äù ‚ÄúST-VAE,‚Äù and other abbreviations are defined upon first use.\n\n### **6. Overall Recommendation**\n**Recommendation:** üî∏ *Reject (Out of Scope / Low Novelty for TMI)* *or* consider resubmission as a **‚ÄúTMI Dataset or Resource Note‚Äù** if such track exists.  \n*Rationale:* Scientifically valuable and methodologically rigorous dataset, but limited novelty in imaging methodology relative to regular TMI research articles. The contribution is infrastructural rather than algorithmic.\n\nIf reoriented toward method development (e.g., automated registration toolkit or sparse-angle reconstruction algorithm with novel design and analytical validation), the paper could qualify for Major Revision consideration.\n\n### **7. Confidence Level:** 5/5  \nAssessment based on strong familiarity with medical imaging datasets, quantitative imaging, and deep-learning-based reconstruction methods.\n\n---\n\n**In summary:** *BoneMet* represents a high‚Äìimpact community resource with careful technical execution and strong potential to catalyze methodological advances. However, in its current form it is more a **dataset introduction paper** than a **methodological innovation manuscript**, placing it marginally below the typical novelty threshold for IEEE *Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **BoneMet**, a large-scale, multi-modal, longitudinal murine imaging dataset focused on breast cancer bone metastasis research. The dataset comprises high-resolution rotational X-rays and 3D CT scans from over 500 mice collected over five years, accompanied by metadata and a Python package enabling segmentation, registration, and region-of-interest extraction. Its main goal is to provide a comprehensive resource for developing and benchmarking imaging reconstruction, segmentation, registration, and prognosis models. The paper is clearly written, well-organized, and ethically compliant, although its novelty lies primarily in dataset construction rather than in new imaging methodology.\n\n---\n\n**Major Comments**  \n1. **Methodological Innovation** ‚Äì The work should more explicitly articulate the methodological contributions beyond data collection, such as any original alignment, registration, or reconstruction pipelines. As presented, it functions primarily as a data resource rather than a methodological advancement.  \n2. **Benchmarking and Evaluation Depth** ‚Äì The demonstration studies (diagnosis, prognosis, sparse reconstruction) effectively illustrate dataset utility but lack comparative evaluation and statistical rigor. Systematic benchmarking with alternative methods and uncertainty analysis would strengthen the validation.  \n3. **Scope and Fit** ‚Äì Given its dataset-oriented focus, the submission may align better with a dataset or resource note format rather than a full research article.  \n4. **Generalization and Translational Relevance** ‚Äì The manuscript could briefly discuss how the murine data relate to human imaging contexts and whether such translation is feasible or currently limited.  \n5. **Manuscript Structure and Focus** ‚Äì Some highly descriptive sections could be condensed or moved to supplementary material to better highlight methodological insights and analytical design.  \n6. **Data Access and Maintenance** ‚Äì The dataset release should include DOIs, clear versioning, and reproducibility statements to ensure long-term accessibility and citation consistency.\n\n---\n\n**Minor Comments**  \n- Correct spacing, capitalization, and typographical inconsistencies in headings and figure labels.  \n- Improve figure readability by enlarging fonts and refining legends.  \n- Standardize units (¬µm vs. micron, Gy vs. mGy).  \n- Ensure all abbreviations (e.g., MMST-ViT, ST-VAE) are defined at first mention.  \n- Apply consistent reference formatting and minor grammar corrections (‚Äúdata are‚Äù instead of ‚Äúdata is‚Äù).\n\n---\n\n**Summary Paragraph**  \nThis work presents a well-documented, ethically approved, and publicly released imaging dataset that fills a substantial gap for preclinical bone metastasis research. Its strengths lie in the scale, multimodality, longitudinal design, and reproducible infrastructure. However, methodological novelty and analytical validation are limited, reducing its suitability as a full research article. The dataset‚Äôs clarity and accessibility suggest strong community value, but the lack of new algorithmic or analytical contributions constrains its scientific impact within methodological venues.\n\n---\n\n**Decision Recommendation**  \n**Recommendation: Reject (Resubmit as Dataset or Resource Note).**  \nWhile technically rigorous and valuable to the community, the contribution is primarily infrastructural rather than methodological. A reframing emphasizing novel analytical pipelines or validation frameworks could merit reconsideration under a different submission category.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces BoneMet, a large-scale, multi-modal dataset for breast cancer bone metastasis (BCBM) diagnosis and prognosis derived from murine models. The dataset comprises 67 terabytes of data from over 500 mice collected between 2019-2024, including 2D X-ray images, 3D CT scans, and medical records organized into six components: Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec (Section 2.1, Table 1). The authors demonstrate the dataset's applicability through experiments on BCBM diagnosis using Vision Transformers and CNNs, achieving accuracies ranging from 79.1% to 92.0% (Section 3.2-3.3), prognostic assessment of bone mechanical properties using generative models (Section 3.4), and sparse-angle CT reconstruction (Section 3.5). The dataset is publicly available through Hugging Face with accompanying APIs for image processing tasks.\n\n## Weaknesses\n\n‚Ä¢ **Limited experimental validation scope**: The evaluation demonstrates basic feasibility but lacks comprehensive benchmarking against established methods or clinical relevance assessment. The experiments primarily show that existing models can achieve reasonable performance on the dataset rather than demonstrating significant advances in BCBM diagnosis. Standard accuracy metrics (Table 3, Figures 5a-5d) do not establish clinical utility or compare against radiologist performance, limiting impact assessment.\n\n‚Ä¢ **Inadequate mathematical and technical rigor**: No mathematical formulations are provided for key processing steps, making reproducibility challenging. The segmentation threshold selection (75/255, Section 2.1, Page 4) lacks theoretical justification or validation studies. Registration methodology mentions mutual information maximization (Section A, Page 15) without presenting the optimization objective function or convergence criteria, hindering technical assessment and reproducibility.\n\n‚Ä¢ **Questionable generalizability to human clinical applications**: The murine model limitations are acknowledged but insufficiently addressed regarding translation potential. Significant physiological differences between mice and humans (disease progression timescales of weeks vs. years, Section G, Page 20) raise concerns about clinical relevance. The animal-level labeling approach (Section 3, Page 6) differs substantially from typical clinical diagnostic scenarios where lesion-level or time-specific predictions are needed.\n\n‚Ä¢ **Insufficient dataset quality assessment**: Missing critical analyses of data quality, annotation consistency, and potential biases. The 20.4% training-test accuracy gap with ViT (Table 3) suggests possible overfitting issues or data quality problems that remain unexplored. Inter-annotator agreement, segmentation accuracy validation, and systematic quality control measures are not reported (Section 2.1), raising concerns about dataset reliability.\n\n‚Ä¢ **Methodological limitations in experimental design**: The experimental setup lacks statistical rigor and proper controls. Cross-validation is mentioned (Section 3.1) but detailed statistical analysis, confidence intervals, or significance testing are absent from results (Tables 3-5). Comparison baselines are limited, and the choice of evaluation metrics may not reflect clinical decision-making requirements, reducing the validity of performance claims.\n\n## Suggestions for Improvement\n\n‚Ä¢ **Expand experimental validation comprehensively**: Conduct thorough benchmarking against state-of-the-art BCBM diagnosis methods and include radiologist performance comparisons. Evaluate clinical relevance through collaboration with medical professionals and assess diagnostic accuracy at clinically meaningful thresholds. Implement more diverse evaluation metrics including sensitivity/specificity analysis and receiver operating characteristic curves to better demonstrate clinical utility.\n\n‚Ä¢ **Enhance mathematical formalization and technical documentation**: Provide complete mathematical formulations for all image processing steps, including segmentation algorithms, registration optimization objectives, and reconstruction methods. Justify threshold selection through systematic validation studies and present convergence criteria for iterative procedures. Include detailed algorithmic descriptions and parameter sensitivity analyses to ensure reproducibility and technical soundness.\n\n‚Ä¢ **Address generalizability concerns systematically**: Conduct comprehensive analysis of murine-to-human translation challenges and provide validation strategies for clinical applications. Develop time-scale normalization approaches to bridge the temporal gap between mouse and human disease progression. Implement lesion-level labeling and evaluation protocols that better match clinical diagnostic scenarios and demonstrate translational potential through pilot human studies or existing human dataset comparisons.\n\n‚Ä¢ **Implement rigorous dataset quality assessment**: Establish comprehensive quality control protocols including inter-annotator agreement studies, systematic bias analysis, and data consistency validation. Address overfitting concerns through detailed data quality investigation and implement cross-dataset validation approaches. Develop standardized annotation guidelines and provide quantitative quality metrics for all dataset components to ensure reliability and scientific rigor.\n\n‚Ä¢ **Strengthen experimental methodology and statistical analysis**: Implement proper statistical testing frameworks with confidence intervals, significance tests, and multiple comparison corrections. Expand baseline comparisons to include more diverse and recent methods, and develop clinically-relevant evaluation protocols. Provide detailed cross-validation results, learning curve analyses, and failure case studies to better characterize model performance and limitations across different scenarios.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *BoneMet*, a large-scale, multimodal dataset for the diagnosis and prognosis of breast cancer bone metastasis (BCBM) derived from mouse models. Comprising 67 terabytes of imaging and medical record data from over 500 mice, the dataset includes multiple components encompassing X-ray, CT, segmentation, registration, and metadata modalities. The authors demonstrate the dataset‚Äôs utility across several tasks‚Äîdiagnosis using CNNs and Vision Transformers, prognostic modeling of bone mechanical properties, and sparse-angle CT reconstruction. The paper is clear in structure and provides a substantial data resource, but its experimental analyses and technical documentation raise multiple concerns regarding rigor, reproducibility, and clinical relevance.  \n\n**Major Comments**  \n1. **Limited Experimental Validation Scope** ‚Äì The experiments mainly confirm the dataset‚Äôs feasibility but do not offer comprehensive benchmarking against existing BCBM diagnosis methods or human expert baselines. Clinical utility is not demonstrated, and standard accuracy metrics fail to establish relevance for practical deployment.  \n2. **Insufficient Mathematical and Technical Rigor** ‚Äì Key image processing steps lack mathematical formulation. For example, segmentation threshold selection and registration via mutual information maximization are described qualitatively without objective functions, convergence criteria, or justification, limiting technical reproducibility.  \n3. **Concerns About Generalizability** ‚Äì The study uses murine models without adequately addressing translational limitations. Physiological and temporal differences between mice and humans undermine the relevance to clinical settings. The use of animal-level labeling diverges from lesion-level or time-resolved diagnostics typical in clinical workflows.  \n4. **Insufficient Dataset Quality Assessment** ‚Äì Data quality, annotation consistency, and bias evaluations are missing. The notable accuracy gap observed suggests potential overfitting or data reliability issues. No inter-annotator agreement or segmentation validation is reported.  \n5. **Weakness in Experimental Methodology** ‚Äì Lack of statistical testing, confidence intervals, and baseline variety reduces the credibility of reported performance. Cross-validation details are minimal, and evaluation metrics are not clinically grounded.  \n\n**Minor Comments**  \n- Clarify selection of segmentation thresholds and registration parameters.  \n- Specify training-validation splits in more detail.  \n- Improve methodological transparency in Appendices for reproducibility.  \n\n**Summary Paragraph**  \nOverall, *BoneMet* represents a potentially valuable data resource, well-organized and accessible for image-based oncology research. However, the current experimental validation, technical formalization, and quality control documentation are insufficient to establish scientific or clinical impact. The paper‚Äôs main strengths lie in dataset scale and accessibility; its main weaknesses concern reproducibility, technical clarity, and translational value to human medicine.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The dataset is promising, but substantial additional validation, methodological rigor, and quality assessment are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces the BoneMet dataset, a large-scale, publicly available, high-resolution medical imaging dataset derived from a murine model of breast cancer bone metastasis (BCBM). The dataset consists of over 67 terabytes of multi-modal medical data, including 2D X-ray images, 3D CT scans, and detailed biological data, collected from over 500 mice over five years. The dataset is structured into six components: Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. The authors demonstrate the utility of BoneMet in building versatile AI models for BCBM diagnosis and prognosis, showcasing preliminary results with various deep learning architectures. The manuscript is well-written, with clear motivations and descriptions of the dataset's potential applications.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: The BoneMet dataset is novel and addresses a significant gap in the availability of large-scale, high-resolution datasets for BCBM. However, the manuscript could better contextualize the dataset within existing literature, particularly highlighting how it differs from other publicly available medical imaging datasets. Additionally, the authors should provide a more detailed comparison with recent advancements in multi-modal datasets and their applications in BCBM.\n\n2. Evaluation Design: The experimental results are primarily focused on demonstrating the dataset's compatibility with various deep learning models, but they lack a thorough validation across different modalities and scenarios. The authors should expand the validation to include more diverse experimental setups, such as comparing the performance of models trained exclusively on the BoneMet dataset with those trained on human datasets. Furthermore, the inclusion of more extensive ablation studies and comparisons with state-of-the-art methods would strengthen the manuscript.\n\n3. Comparisons and Baselines: While the manuscript showcases promising results with different deep learning models, it lacks a comprehensive comparison with existing methods specifically designed for BCBM diagnosis and prognosis. Including these comparisons would provide a clearer picture of the dataset's contribution to advancing the field. Additionally, the manuscript should discuss the limitations of the existing methods and how BoneMet addresses these limitations.\n\n4. Reproducibility: The authors have made efforts to ensure reproducibility by providing APIs for data processing and making the dataset publicly available. However, the manuscript could benefit from more detailed documentation of the preprocessing pipelines, hyperparameters, and training protocols. This would help researchers replicate the results and build upon the dataset.\n\n###\n\n## Minor Comments\n1. Clarity of Figures: Figures 3 and 4 are somewhat cluttered. Showing fewer representative slices with zoomed-in regions would improve readability.\n2. Notation and Terminology: The introduction of notation in Section 2.1 is inconsistent and requires more explanation. Definitions for acronyms like \"R=4\" should be provided.\n3. Typographical Errors: Correct typographical issues such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7).\n4. Supplementary Materials: The supplementary materials could benefit from a more structured organization, with clear headings and subheadings to guide readers through the additional content.\n\n###\n\n## Summary Paragraph\nThe manuscript introduces the BoneMet dataset, addressing a significant challenge in the availability of high-resolution, multi-modal datasets for BCBM diagnosis and prognosis. The dataset's novelty and potential for advancing deep learning applications in this domain are clear. However, the evaluation design could be expanded to include more diverse scenarios and comparisons with existing methods. The reproducibility of the dataset is ensured through the provision of APIs and detailed documentation, though further refinement of the training protocols and preprocessing pipelines would enhance this aspect. Overall, the manuscript makes a valuable contribution to the field, but it requires additional validation and comparative analysis to fully meet the standards of TMI.\n\n###\n\n## Decision Recommendation\nMajor Revision: The authors should expand the comparative analysis, broaden the validation across different modalities and scenarios, and provide more detailed methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the **BoneMet dataset**, a large-scale, publicly available, high-resolution medical imaging resource derived from a murine model of breast cancer bone metastasis (BCBM). Encompassing over 67 terabytes of multi-modal data‚Äî2D X-ray, 3D CT scans, and biological measurements‚Äîcollected from more than 500 mice across five years, the dataset comprises six structured components (Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec). The authors illustrate its potential through preliminary deep learning experiments for BCBM diagnosis and prognosis. Overall, the manuscript is clearly written, well-structured, and effectively conveys the motivation and significance of this data resource.  \n\n**Major Comments**  \n1. **Novelty and Positioning**: The dataset is a valuable and novel contribution, addressing a notable gap in publicly available high-resolution datasets for BCBM. However, the manuscript should more explicitly contextualize BoneMet relative to existing medical imaging datasets, clarifying its distinctive aspects and advantages. A deeper review of related work on multi-modal datasets and their specific applications to BCBM would improve positioning.  \n2. **Evaluation Design**: Current experiments primarily demonstrate dataset compatibility with multiple deep learning architectures. The evaluation would be stronger if expanded to more diverse validation settings‚Äîsuch as cross-modality tests or comparisons between models trained solely on BoneMet versus human datasets. Additional ablation studies and benchmarking against state-of-the-art methods are recommended.  \n3. **Comparisons and Baselines**: Although initial results are promising, the paper lacks systematic comparisons with existing BCBM-specific models. Incorporating such baselines and discussing their limitations relative to BoneMet would help clarify the dataset‚Äôs contribution.  \n4. **Reproducibility**: Public release of the dataset and APIs is commendable, yet the manuscript should include fuller documentation of preprocessing, hyperparameters, and training settings to facilitate replication and extension.  \n\n**Minor Comments**  \n- **Figures**: Figures 3 and 4 appear cluttered; fewer, enlarged representative slices would improve clarity.  \n- **Notation and Terminology**: Section 2.1 includes inconsistent notation; acronym definitions (e.g., ‚ÄúR=4‚Äù) should be provided.  \n- **Typographical Errors**: Correct errors such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes.‚Äù  \n- **Supplementary Material**: Reorganize the supplementary materials with clear headings and subheadings for easier navigation.  \n\n**Summary Paragraph**  \nThe BoneMet dataset represents an important and timely contribution to the study of metastatic bone disease in murine models, offering a broad and well-structured collection of imaging and biological data. The work is well-presented and makes significant progress toward open, reproducible research in BCBM. However, broader validation, more comprehensive comparative analyses, and stronger documentation of methods are needed to fulfill its potential impact and ensure replicability.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The authors should augment comparative analyses, extend experimental validation across modalities and scenarios, and expand methodological documentation to support reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## BONEMET: AN OPEN LARGE-SCALE MULTI-MODAL MURINE DATASET FOR BREAST CANCER BONE METASTASIS DIAGNOSIS AND PROGNOSIS\n\n### Summary\n\nThe paper introduces BoneMet, a large-scale, open murine dataset for breast cancer bone metastasis (BCBM), comprising 67 TB of high-resolution multi-modal data: rotational 2D X-rays, reconstructed 3D CTs, segmented/registered/ROI CT volumes, and detailed longitudinal biological and mechanical records across >500 mice scanned weekly over 3‚Äì5 weeks. The authors release accompanying APIs for segmentation, registration, and ROI selection and demonstrate dataset utility through preliminary benchmarks for BCBM diagnosis on 2D/3D modalities, prognosis via future CT generation with FE-based mechanical assessment, and sparse-angle CT reconstruction from a single X-ray using NeRF-style methods. The central value proposition is the unique, repeated longitudinal imaging across the disease course, which is often unavailable in human datasets, enabling both supervised and self-supervised research directions.\n\n### Strengths\n\n- Technical novelty and innovationThe dataset‚Äôs longitudinal design (weekly repeated scans over multiple weeks per mouse) is a unique and valuable feature rarely available at this scale, enabling temporal modeling and prognosis tasks.Inclusion of multiple, well-aligned modalities (2D projections, 3D reconstructions, segmented/registered tibiae, ROI overlays) plus structured biological/mechanical measurements supports diverse AI tasks (diagnosis, prognosis, sparse-view reconstruction).The provision of processing APIs for segmentation, registration, and ROI extraction is practical and lowers barriers to entry, encouraging reproducible workflows.\n- Experimental rigor and validationInitial benchmarks span complementary tasks: 2D spatiotemporal diagnosis (ViTs), 3D CT diagnosis (CNNs/ViTs), generative temporal prognosis with FE-derived mechanical evaluation, and sparse-angle reconstruction (PixelNeRF/MedNeRF).Mouse-level data splits reduce subject-level leakage risks; five-fold cross-validation on training sets is a sound practice.\n- Clarity of presentationThe six-component dataset structure is well-articulated, with high-level workflows and per-component descriptions that help prospective users understand the data flow from acquisition to derived products.Supplementary information outlines acquisition parameters and pipeline steps (reconstruction, segmentation, registration, ROI generation).\n- Significance of contributionsAddresses a recognized bottleneck: lack of large, longitudinal, multimodal, high-resolution datasets for bone metastasis in breast cancer.Provides a substrate for label-efficient learning, longitudinal modeling, physics-based reconstruction, and mechanobiology-informed prediction‚Äîresearch directions of high current interest.Potential to seed foundation-model pretraining for orthopedics/oncology imaging and accelerate translation to human data.\n\n- The dataset‚Äôs longitudinal design (weekly repeated scans over multiple weeks per mouse) is a unique and valuable feature rarely available at this scale, enabling temporal modeling and prognosis tasks.\n- Inclusion of multiple, well-aligned modalities (2D projections, 3D reconstructions, segmented/registered tibiae, ROI overlays) plus structured biological/mechanical measurements supports diverse AI tasks (diagnosis, prognosis, sparse-view reconstruction).\n- The provision of processing APIs for segmentation, registration, and ROI extraction is practical and lowers barriers to entry, encouraging reproducible workflows.\n\n- Initial benchmarks span complementary tasks: 2D spatiotemporal diagnosis (ViTs), 3D CT diagnosis (CNNs/ViTs), generative temporal prognosis with FE-derived mechanical evaluation, and sparse-angle reconstruction (PixelNeRF/MedNeRF).\n- Mouse-level data splits reduce subject-level leakage risks; five-fold cross-validation on training sets is a sound practice.\n\n- The six-component dataset structure is well-articulated, with high-level workflows and per-component descriptions that help prospective users understand the data flow from acquisition to derived products.\n- Supplementary information outlines acquisition parameters and pipeline steps (reconstruction, segmentation, registration, ROI generation).\n\n- Addresses a recognized bottleneck: lack of large, longitudinal, multimodal, high-resolution datasets for bone metastasis in breast cancer.\n- Provides a substrate for label-efficient learning, longitudinal modeling, physics-based reconstruction, and mechanobiology-informed prediction‚Äîresearch directions of high current interest.\n- Potential to seed foundation-model pretraining for orthopedics/oncology imaging and accelerate translation to human data.\n\n### Weaknesses\n\n- Technical limitations or concernsLabel granularity: current labels are assigned at the animal level across weeks; positive means a lesion occurs sometime within weeks 0‚Äì5. This inexact labeling for early time points can confound diagnostic training and evaluation (e.g., week-0 images labeled positive despite no lesion), and limits early-detection claims.Image encoding and quantitative fidelity: CT data appear to be stored as PNGs; micro-CT is typically 16-bit and HU-calibrated. PNG (often 8-bit) risks dynamic range loss and weakens quantitative reproducibility and downstream mechanistic analysis. Clarification on bit depth and HU calibration preservation is needed.Sparse-angle ‚Äúsingle-view‚Äù reconstruction from a single X-ray is an extremely ill-posed problem; while NeRF-style priors can hallucinate plausible anatomy, clinical safety requires stronger validation and uncertainty quantification. Evaluation protocols must carefully avoid training-test leakage of subject-specific priors.Segmentation uses a fixed global threshold (75/255) without reported accuracy metrics or quality control; registration accuracy metrics (e.g., TRE, overlap) are not reported.\n- Experimental gaps or methodological issuesDataset accessibility is unclear: links show placeholders (XXX). For a dataset paper, immediate availability, licensing, and stable identifiers are essential for reproducibility.The diagnostic experiments report high training accuracy (‚âà99.5%) and sizable generalization gaps for some models, suggesting overfitting. There is no calibration analysis, ROC/PR curves, per-week performance, ablations on confounders, or external validation.Prognosis (FE simulation) results show a single reaction-force curve example and an R^2 statistic without sample size, confidence intervals, or details on material models/boundary conditions. Robustness and statistical significance are not established.Sparse-view reconstruction metrics are given, but evaluation protocols (e.g., whether test-time fine-tuning was allowed, angle dependence, subject-wise separation) are not specified in sufficient detail; no uncertainty maps or failure-case analyses are provided.\n- Clarity or presentation issuesMultiple typographical inconsistencies: image sizes (e.g., ‚Äú4,032 x 2.688 x 1‚Äù), ‚ÄúRol-CT‚Äù vs ‚ÄúRoI-CT,‚Äù mixed scan-duration claims (10 min vs 4 min), tool names (‚ÄúDataviewer‚Äù vs ‚ÄúDataviewer¬Æ‚Äù), and an unreadable R^2 symbol in the main text.Table and text occasionally diverge on dataset sizes and counts; some units are unclear or likely typos.\n- Missing related work or comparisonsSparse-view reconstruction literature has progressed beyond MedNeRF/PixelNeRF (e.g., structure-aware NeRFs, Gaussian representations for CT); these should be cited and discussed relative to the reported results.Recent murine micro-CT segmentation works and cross-domain generalization studies could contextualize the segmentation/registration pipelines and model choices.Label-efficient and weakly supervised learning in medical imaging is directly relevant given the animal-level labels; broader survey connections could strengthen methodological framing.\n\n- Label granularity: current labels are assigned at the animal level across weeks; positive means a lesion occurs sometime within weeks 0‚Äì5. This inexact labeling for early time points can confound diagnostic training and evaluation (e.g., week-0 images labeled positive despite no lesion), and limits early-detection claims.\n- Image encoding and quantitative fidelity: CT data appear to be stored as PNGs; micro-CT is typically 16-bit and HU-calibrated. PNG (often 8-bit) risks dynamic range loss and weakens quantitative reproducibility and downstream mechanistic analysis. Clarification on bit depth and HU calibration preservation is needed.\n- Sparse-angle ‚Äúsingle-view‚Äù reconstruction from a single X-ray is an extremely ill-posed problem; while NeRF-style priors can hallucinate plausible anatomy, clinical safety requires stronger validation and uncertainty quantification. Evaluation protocols must carefully avoid training-test leakage of subject-specific priors.\n- Segmentation uses a fixed global threshold (75/255) without reported accuracy metrics or quality control; registration accuracy metrics (e.g., TRE, overlap) are not reported.\n\n- Dataset accessibility is unclear: links show placeholders (XXX). For a dataset paper, immediate availability, licensing, and stable identifiers are essential for reproducibility.\n- The diagnostic experiments report high training accuracy (‚âà99.5%) and sizable generalization gaps for some models, suggesting overfitting. There is no calibration analysis, ROC/PR curves, per-week performance, ablations on confounders, or external validation.\n- Prognosis (FE simulation) results show a single reaction-force curve example and an R^2 statistic without sample size, confidence intervals, or details on material models/boundary conditions. Robustness and statistical significance are not established.\n- Sparse-view reconstruction metrics are given, but evaluation protocols (e.g., whether test-time fine-tuning was allowed, angle dependence, subject-wise separation) are not specified in sufficient detail; no uncertainty maps or failure-case analyses are provided.\n\n- Multiple typographical inconsistencies: image sizes (e.g., ‚Äú4,032 x 2.688 x 1‚Äù), ‚ÄúRol-CT‚Äù vs ‚ÄúRoI-CT,‚Äù mixed scan-duration claims (10 min vs 4 min), tool names (‚ÄúDataviewer‚Äù vs ‚ÄúDataviewer¬Æ‚Äù), and an unreadable R^2 symbol in the main text.\n- Table and text occasionally diverge on dataset sizes and counts; some units are unclear or likely typos.\n\n- Sparse-view reconstruction literature has progressed beyond MedNeRF/PixelNeRF (e.g., structure-aware NeRFs, Gaussian representations for CT); these should be cited and discussed relative to the reported results.\n- Recent murine micro-CT segmentation works and cross-domain generalization studies could contextualize the segmentation/registration pipelines and model choices.\n- Label-efficient and weakly supervised learning in medical imaging is directly relevant given the animal-level labels; broader survey connections could strengthen methodological framing.\n\n### Detailed Comments\n\n- Technical soundness evaluationDataset design: The longitudinal, multi-modal structure is a major strength. However, quantitative imaging data integrity needs clarification: if CT files are stored as PNGs with 8-bit depth, this undermines HU calibration, dynamic range, and quantitative reproducibility. Strongly recommend releasing 16-bit DICOM/TIFF/NRRD with preserved scanner geometry and calibration. Provide per-scan metadata (geometry, voxel size, kVp/mA, filters) in standardized form.Labeling: Animal-level labels are a practical start but limit early-detection claims. Consider curating timepoint-level labels (e.g., overt lesion presence, radiographic severity scores) and time-to-event labels for survival/hazard modeling. Even weak temporal labels (e.g., lesion appearance week) would enable more faithful evaluation of early detection and prognosis.Preprocessing and QC: Segmentation via a fixed threshold is brittle across ages/protocol variations; report segmentation/registration QC metrics (Dice/TRE against expert masks for a subset). Provide inter-rater variability where applicable and error rates for registration to baseline.APIs: Packaging is helpful; please include explicit versioning, dependency pinning, and unit tests. Consider adding checks for image bit depth/HU calibration and automated sanity checks (e.g., histogram stats per scan, geometry validation).\n- Experimental evaluation assessment2D/3D diagnosis: Report ROC-AUC/PR-AUC, calibration (ECE/Brier), and confidence intervals via bootstrapping. Provide per-week performance stratification to assess early vs late timepoints. Analyze overfitting gap (e.g., stronger regularization, augmentation, or longitudinal MIL baselines). Include simple radiomics baselines and logistic regression to establish lower bounds.Prognosis and FE validation: Expand beyond a single illustrative example. Specify n, report distributional statistics (MAE, RMSE, R^2 with CIs) across mice/timepoints. Detail FE pipeline (meshing, material models‚Äîhomogeneous vs inhomogeneous density‚Äìelasticity mapping, boundary conditions, validation against 3-point bending tests where available). Evaluate clinical relevance by correlating predicted mechanical metrics with fracture outcomes or ex vivo mechanical tests for a subset.Sparse-angle reconstruction: Clearly describe training/testing protocols, whether per-instance fine-tuning was used (MedNeRF typically does), and enforce mouse-level disjoint splits. Provide angle ablations (which single view?), uncertainty quantification, and failure-case analyses. Consider modern baselines (e.g., SAX-NeRF, 3D Gaussian CT frameworks) and model-based methods (e.g., DOLCE/iterative hybrids) for a fairer landscape.Reproducibility: Publish standardized splits (mice IDs), seeds, and scripts. Consider offering a smaller curated benchmark subset (e.g., 100 mice) to facilitate quick experimentation and fair comparison.\n- Comparison with related work (using the summaries provided)Temporal prognosis aligns with T-VAE work on murine tibiae; your ST-VAE results improve SSIM/PSNR but need broader statistical backing and uncertainty quantification. Highlight differences in dataset scale, ROI definition, and integration with FE analyses relative to prior T‚ÄëVAE.Sparse-view reconstruction: Beyond PixelNeRF/MedNeRF, recent advances (e.g., structure-aware NeRFs and 3D Gaussian representations for CT) show significant gains and faster convergence; discussing them would better position your baselines and suggest future directions (e.g., FBP-guided priors, line-segment attention along rays).Label-efficient learning surveys emphasize the utility of weak/inexact labels; explicitly framing the animal-level labeling as inexact labels and proposing label refinement or MIL approaches would connect your dataset to current methodology.Murine CT segmentation and cross-institution generalization studies (e.g., Swin UNETR variants) support your registration/segmentation use case; incorporating those findings could inform robust baselines and cross-domain validation.\n- Discussion of broader impact and significanceThe dataset‚Äôs longitudinal nature and multimodality can catalyze early-detection research, mechanobiology-informed modeling, and safe imaging protocols through sparse-view methods. However, claims about pretraining for human translation should be tempered with domain-shift considerations (morphology, acquisition physics, dose, noise) and aligned with proposed benchmarks for cross-species transfer learning.Ethical and governance aspects: include IACUC protocol references, data usage license, and an ethical use statement. Given the dataset‚Äôs size, provide access modalities (e.g., streaming via Hugging Face datasets, chunked Zarr/N5 storage) and clear data cards with bias/limitations sections.Safety: emphasize that single-view reconstructions are not for clinical decision-making without strong uncertainty calibration and external validation; include guidance in the dataset documentation.\n\n- Dataset design: The longitudinal, multi-modal structure is a major strength. However, quantitative imaging data integrity needs clarification: if CT files are stored as PNGs with 8-bit depth, this undermines HU calibration, dynamic range, and quantitative reproducibility. Strongly recommend releasing 16-bit DICOM/TIFF/NRRD with preserved scanner geometry and calibration. Provide per-scan metadata (geometry, voxel size, kVp/mA, filters) in standardized form.\n- Labeling: Animal-level labels are a practical start but limit early-detection claims. Consider curating timepoint-level labels (e.g., overt lesion presence, radiographic severity scores) and time-to-event labels for survival/hazard modeling. Even weak temporal labels (e.g., lesion appearance week) would enable more faithful evaluation of early detection and prognosis.\n- Preprocessing and QC: Segmentation via a fixed threshold is brittle across ages/protocol variations; report segmentation/registration QC metrics (Dice/TRE against expert masks for a subset). Provide inter-rater variability where applicable and error rates for registration to baseline.\n- APIs: Packaging is helpful; please include explicit versioning, dependency pinning, and unit tests. Consider adding checks for image bit depth/HU calibration and automated sanity checks (e.g., histogram stats per scan, geometry validation).\n\n- 2D/3D diagnosis: Report ROC-AUC/PR-AUC, calibration (ECE/Brier), and confidence intervals via bootstrapping. Provide per-week performance stratification to assess early vs late timepoints. Analyze overfitting gap (e.g., stronger regularization, augmentation, or longitudinal MIL baselines). Include simple radiomics baselines and logistic regression to establish lower bounds.\n- Prognosis and FE validation: Expand beyond a single illustrative example. Specify n, report distributional statistics (MAE, RMSE, R^2 with CIs) across mice/timepoints. Detail FE pipeline (meshing, material models‚Äîhomogeneous vs inhomogeneous density‚Äìelasticity mapping, boundary conditions, validation against 3-point bending tests where available). Evaluate clinical relevance by correlating predicted mechanical metrics with fracture outcomes or ex vivo mechanical tests for a subset.\n- Sparse-angle reconstruction: Clearly describe training/testing protocols, whether per-instance fine-tuning was used (MedNeRF typically does), and enforce mouse-level disjoint splits. Provide angle ablations (which single view?), uncertainty quantification, and failure-case analyses. Consider modern baselines (e.g., SAX-NeRF, 3D Gaussian CT frameworks) and model-based methods (e.g., DOLCE/iterative hybrids) for a fairer landscape.\n- Reproducibility: Publish standardized splits (mice IDs), seeds, and scripts. Consider offering a smaller curated benchmark subset (e.g., 100 mice) to facilitate quick experimentation and fair comparison.\n\n- Temporal prognosis aligns with T-VAE work on murine tibiae; your ST-VAE results improve SSIM/PSNR but need broader statistical backing and uncertainty quantification. Highlight differences in dataset scale, ROI definition, and integration with FE analyses relative to prior T‚ÄëVAE.\n- Sparse-view reconstruction: Beyond PixelNeRF/MedNeRF, recent advances (e.g., structure-aware NeRFs and 3D Gaussian representations for CT) show significant gains and faster convergence; discussing them would better position your baselines and suggest future directions (e.g., FBP-guided priors, line-segment attention along rays).\n- Label-efficient learning surveys emphasize the utility of weak/inexact labels; explicitly framing the animal-level labeling as inexact labels and proposing label refinement or MIL approaches would connect your dataset to current methodology.\n- Murine CT segmentation and cross-institution generalization studies (e.g., Swin UNETR variants) support your registration/segmentation use case; incorporating those findings could inform robust baselines and cross-domain validation.\n\n- The dataset‚Äôs longitudinal nature and multimodality can catalyze early-detection research, mechanobiology-informed modeling, and safe imaging protocols through sparse-view methods. However, claims about pretraining for human translation should be tempered with domain-shift considerations (morphology, acquisition physics, dose, noise) and aligned with proposed benchmarks for cross-species transfer learning.\n- Ethical and governance aspects: include IACUC protocol references, data usage license, and an ethical use statement. Given the dataset‚Äôs size, provide access modalities (e.g., streaming via Hugging Face datasets, chunked Zarr/N5 storage) and clear data cards with bias/limitations sections.\n- Safety: emphasize that single-view reconstructions are not for clinical decision-making without strong uncertainty calibration and external validation; include guidance in the dataset documentation.\n\n### Questions for Authors\n\n- Data availability: Are the dataset and code actually accessible now? The links contain ‚ÄúXXX‚Äù. Please provide working DOIs/URLs, licensing terms, and a data card detailing governance and permissible use.\n- Bit depth and calibration: In what format/bit depth are CT volumes released? Are HU values and scanner geometry preserved? If PNGs are 8-bit, can you release 16-bit DICOM/NRRD to maintain quantitative fidelity?\n- Labeling granularity: How are labels assigned across weeks for positive animals? Are early (pre-lesion) timepoints labeled positive? Do you plan to release timepoint-level lesion labels or time-to-event annotations?\n- Segmentation/registration QC: Do you have quantitative validation against expert annotations (Dice, HD95, TRE) for segmentation and registration? What proportion of scans required manual correction?\n- Confounders and splits: How are age, sex, strain, and interventions distributed between train/test? Did you stratify or control for these variables to avoid confounding?\n- Diagnosis metrics: Can you report ROC-AUC/PR-AUC, calibration (ECE), and per-week performance for the 2D/3D diagnosis tasks, with confidence intervals?\n- Prognosis scale: What is the number of mice/timepoints used to compute the ST-VAE and FE simulation metrics? Please report summary statistics (e.g., mean¬±SD) across the cohort and details of material models/boundary conditions.\n- Sparse-view protocol: For MedNeRF/PixelNeRF, was test-time fine-tuning used? Which single angle was provided? Can you provide angle-dependent performance and uncertainty maps?\n- Human translation: Do you have pilot results on human datasets (X-ray/CT) to illustrate transferability? If not, what is your plan for cross-species domain adaptation?\n- Dataset subsets: Given 67 TB size, will you provide curated, standardized benchmark subsets and streaming/chunked access to enable broader adoption?\n\n### Overall Assessment\n\nBoneMet is a timely, ambitious, and potentially field-shaping resource: longitudinal, high-resolution, multimodal murine data for BCBM is genuinely scarce, and the authors‚Äô organization into six interoperable components plus APIs is thoughtfully designed. The preliminary experiments convincingly suggest broad applicability, especially the advantage of spatiotemporal alignment for 2D diagnosis and the feasibility of temporal CT prediction with mechanobiology readouts. However, to meet top-tier standards for a dataset paper, several elements require strengthening: immediate and verifiable availability with stable links and licenses; preservation of quantitative CT fidelity (bit depth/HU); clearer, finer-grained labels and evaluation protocols aligned with early-detection and prognosis claims; stronger experimental rigor (statistical reporting, calibration, confounder control, external validation); and enriched baselines that reflect the state of the art in sparse-view CT and label-efficient learning. Addressing these points‚Äîwith added QC metrics for segmentation/registration, robust FE validation across cohorts, and comprehensive documentation‚Äîwould substantially increase the dataset‚Äôs impact and its suitability for publication at a top venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **BoneMet**, a large-scale open murine dataset designed for breast cancer bone metastasis (BCBM) diagnosis and prognosis. It compiles 67 TB of multimodal, high-resolution data, including rotational 2D X-rays, reconstructed and segmented 3D CTs, and associated biological and mechanical records from over 500 mice monitored longitudinally. The dataset is intended to support diverse machine learning tasks, from diagnosis to prognosis and sparse-view reconstruction, and includes accompanying APIs to standardize segmentation, registration, and ROI selection. The paper is clearly written, with systematic documentation of the dataset‚Äôs structure and benchmark demonstrations, although some methodological and data integrity issues limit its immediate reproducibility.  \n\n**Major Comments**  \n1. **Labeling Granularity** ‚Äì Labels are defined at the animal level, marking an entire sequence positive if any lesion appears within weeks 0‚Äì5, potentially confounding early detection analyses. Incorporating week-level or time-to-event labels would improve clinical interpretability.  \n2. **Quantitative Fidelity** ‚Äì CT data stored as 8-bit PNGs risk loss of dynamic range and HU calibration. Release of 16-bit calibrated DICOM/NRRD formats with full metadata would preserve quantitative validity.  \n3. **Evaluation and Overfitting** ‚Äì Diagnostic models show near-perfect training accuracy with notable generalization gaps, indicating overfitting. Additional analyses (ROC/PR curves, calibration metrics, temporal stratification, and external validation) are needed.  \n4. **Sparse-View Reconstruction** ‚Äì The ‚Äúsingle-view‚Äù NeRF-based reconstruction is highly ill-posed; stronger uncertainty quantification, clearer separation of train/test subjects, and reference to recent structure-aware NeRF approaches are advised.  \n5. **Experimental Protocols** ‚Äì The prognosis section lacks statistical rigor: very limited examples, no sample sizes or confidence intervals. Segmentation and registration steps lack reported accuracy metrics.  \n6. **Dataset Accessibility** ‚Äì The dataset links are placeholders (‚ÄúXXX‚Äù). Verified DOIs, licensing, and governance details are essential for reproducibility.  \n7. **Related Work and Context** ‚Äì The paper omits discussion of newer sparse-view reconstruction and murine CT segmentation methods; citing these would better position the dataset‚Äôs scope.  \n\n**Minor Comments**  \n- Several typographical inconsistencies in scan duration, image dimensions, and variable notation.  \n- Missing or inconsistent dataset counts between tables and text.  \n- Recommend including ethical approvals, data license information, and access modalities (e.g., streaming, benchmark subsets).  \n- APIs should include versioning, dependency management, and automated QC checks for bit depth and geometry.  \n\n**Summary Paragraph**  \nBoneMet represents a valuable and timely resource filling a clear data gap in longitudinal, multimodal bone metastasis imaging. The paper‚Äôs presentation is generally strong, and the dataset‚Äôs potential for AI-driven prognosis and mechanobiological modeling is evident. Nonetheless, key limitations‚Äîincluding insufficient label granularity, reduced quantitative fidelity, absent QC metrics, incomplete data access details, and underdeveloped evaluation protocols‚Äîmust be resolved before the work can serve as a definitive reference dataset. Strengthening data integrity, statistical rigor, and documentation will notably enhance both reproducibility and impact.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening paragraph summarizing the studied problem, proposed solution, and demonstrated results\n\nThe paper addresses a critical gap in breast cancer bone metastasis (BCBM) research: the scarcity of large-scale, high-resolution, longitudinal datasets needed to develop and validate AI models for early detection and prognosis. The authors introduce BoneMet, the first comprehensive multi-modal murine dataset specifically designed for BCBM research, comprising over 67 terabytes of data from more than 500 mice spanning five years (2019-2024). The dataset is systematically organized into six components: Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec, capturing 2D X-ray images, 3D CT scans, and detailed biological records at multiple time points. The authors demonstrate BoneMet's utility across several applications: BCBM diagnosis using 2D and 3D images (achieving up to 92% accuracy with Swin-B architecture), prognosis of bone deterioration via multi-modal analysis, sparse-angle 3D reconstruction from single X-ray images, and tibiae auto-segmentation. They also provide the BoneMet package with APIs for data processing to enhance reproducibility. The experimental results validate that BoneMet can effectively support the development of AI models for BCBM management, with potential applications in early diagnosis, disease monitoring, and reduced radiation exposure during imaging.\n\n## 2. Major and minor comments on strengths and limitations\n\n### Major strengths:\n- **Groundbreaking dataset**: BoneMet represents the first large-scale, longitudinal, high-resolution medical dataset specifically designed for BCBM research, addressing a critical bottleneck in the field. Its multi-modal nature and temporal alignment across multiple disease stages offer unprecedented value.\n- **Comprehensive organization**: The six-component structure (Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, MiceMediRec) provides a logical framework that supports diverse research applications from basic image analysis to complex multi-modal prognostic assessment.\n- **Strong experimental validation**: The authors demonstrate the dataset's utility across multiple applications with rigorous experiments, showing state-of-the-art results for BCBM diagnosis (92% accuracy) and promising outcomes for sparse-angle reconstruction and mechanical property prediction.\n- **Reproducibility focus**: The BoneMet package with well-documented APIs for segmentation, registration, and ROI cropping significantly lowers barriers for adoption and enhances reproducibility across the research community.\n\n### Major limitations:\n- **Translation to clinical practice**: While the murine model provides valuable data, the paper insufficiently addresses the specific physiological differences between mice and humans that may limit direct clinical translation. The authors should provide more concrete pathways for how findings from this dataset would translate to human applications.\n- **Accessibility challenges**: The enormous dataset size (67 TB) presents significant barriers for researchers without substantial computational resources. The paper needs to better address how researchers with limited infrastructure can access and utilize portions of the dataset.\n- **Model architecture details**: The experimental section lacks sufficient technical depth regarding the specific model architectures and their adaptations to the BoneMet dataset, which would strengthen the technical contribution.\n\n### Minor strengths:\n- **Clear visualization**: The figures effectively illustrate the data collection pipeline, processing workflow, and application examples, enhancing readability.\n- **Practical code examples**: Providing configuration code snippets for the APIs adds immediate practical utility for researchers.\n- **Ethical considerations**: The thorough ethical statement regarding animal welfare demonstrates responsible research practices.\n\n### Minor limitations:\n- **Lack of comparative analysis**: The paper would benefit from a more detailed comparison with existing smaller datasets to better highlight BoneMet's unique contributions.\n- **Incomplete data quality discussion: While the supplementary material mentions handling label noise and data imbalance, these critical challenges deserve more attention in the main text.\n- **Inconsistent figure referencing**: Some figures in the text lack corresponding captions in the layout, which could confuse readers.\n\n## 3. Summary paragraph evaluating the work along the 4 TMI editorial criteria\n\n**Significance**: The work addresses a significant and timely gap in BCBM research by providing the first large-scale, high-resolution, longitudinal dataset specifically designed for this application. The scarcity of quality datasets has been a major bottleneck in developing effective AI tools for BCBM, and this resource has substantial potential to catalyze progress in the field. The longitudinal nature of the data is particularly valuable for studying disease progression and treatment response.\n\n**Innovation**: The primary innovation lies in the systematic creation and organization of the BoneMet dataset itself, with its unique six-component structure designed to facilitate diverse research applications. The development of specialized APIs for data processing represents a valuable methodological contribution that enhances usability. While the paper focuses on resource creation rather than novel algorithms, this type of foundational work is essential for advancing medical imaging research.\n\n**Evaluation**: The evaluation is comprehensive across multiple applications with appropriate metrics for each task. The authors demonstrate the dataset's utility with multiple deep learning architectures and provide comparative results. However, the evaluation would be strengthened by more detailed ablation studies and comparisons with state-of-the-art methods on similar tasks, particularly regarding the performance of the sparse-angle reconstruction methods.\n\n**Reproducibility**: The work excels in reproducibility with the release of the BoneMet package containing well-documented APIs, code examples, and the complete dataset. The inclusion of detailed processing pipelines, configuration examples, and data organization specifications makes it straightforward for other researchers to adopt and build upon the work. The availability on Hugging Face and PyPI further enhances accessibility.\n\n## 4. Decision recommendation\n\n**Major revision (reject/resubmit)**\n\nWhile BoneMet represents a valuable resource that addresses a critical gap in BCBM research, several substantive issues need to be addressed before acceptance. The authors should provide more concrete discussion of the translation pathway from murine models to human clinical applications, including specific physiological differences and how they might impact model development. They must also address the practical challenges of the dataset's enormous size by providing clearer guidance on how researchers with limited computational resources can access and utilize the data. The experimental section requires additional technical depth regarding model architectures and comparative analyses with state-of-the-art methods. Strengthening the comparison with existing datasets and incorporating more discussion of data quality issues in the main text would also significantly enhance the paper's impact. Addressing these points would make this valuable resource even more impactful for the TMI community.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThis manuscript presents *BoneMet*, a large-scale, longitudinal, multi-modal murine dataset designed to advance research on breast cancer bone metastasis (BCBM). The dataset spans more than 500 mice and five years of imaging (2019‚Äì2024), totaling 67 TB across six structured components: Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. The authors demonstrate BoneMet‚Äôs value for multiple applications‚Äîincluding BCBM diagnosis, bone deterioration prognosis, sparse-angle 3D reconstruction, and tibiae segmentation‚Äîachieving strong quantitative performance (e.g., 92% diagnostic accuracy using Swin-B). The work also offers a reproducible software package with APIs for data processing, positioning BoneMet as a potentially foundational resource for the field.\n\n---\n\n**Major Comments**  \n1. **Clinical translation:** The paper insufficiently discusses physiological differences between murine and human bone metastasis, limiting clarity on how results would transfer to clinical practice. The authors should elaborate on translational pathways and potential human research extensions.  \n2. **Data accessibility:** The dataset‚Äôs size (67 TB) poses logistical and computational challenges. More guidance is needed on partial access, downsampled versions, or remote processing options for researchers with limited resources.  \n3. **Model architecture detail:** The experimental descriptions lack technical depth regarding model architectures and their adaptation for BoneMet tasks. Greater clarity on this aspect would improve reproducibility and highlight methodological contribution.  \n4. **Comparative context:** The manuscript would benefit from a detailed comparison to existing smaller-scale BCBM or bone datasets to better establish BoneMet‚Äôs distinct advantages.  \n5. **Data quality discussion:** Handling of label noise and data imbalance is mentioned only in supplementary materials but should be addressed more clearly in the main text.\n\n---\n\n**Minor Comments**  \n- Figures effectively illustrate the data pipeline and applications, but a few lack consistent referencing within the text.  \n- The inclusion of configuration code snippets is commendable and enhances usability.  \n- Ethical documentation regarding animal use is appropriately thorough and transparent.  \n\n---\n\n**Summary Paragraph**  \nOverall, the work fills a significant gap in BCBM research by providing a longitudinal, multi-modal dataset for preclinical imaging studies. Its innovation lies in the systematic organization and public release of a high-quality resource complemented by reproducible APIs. The evaluation is comprehensive but would benefit from additional architectural details, ablations, and comparisons with related studies. Reproducibility is a clear strength, facilitated by the openly shared dataset and code framework. Addressing issues of clinical translation, dataset accessibility, and comparative evaluation would substantially strengthen the paper‚Äôs contribution.\n\n---\n\n**Decision Recommendation**  \n**Major Revision (Reject/Resubmit)** ‚Äì The study offers a highly valuable dataset but requires more detailed discussion on translation to human application, data accessibility strategies, and methodological specificity before it is ready for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript introduces **BoneMet**, a large‚Äëscale, longitudinal murine imaging repository intended to address the paucity of high‚Äëresolution data for breast‚Äëcancer bone metastasis (BCBM). Over the period 2019‚Äë2024, more than five hundred mice were scanned, generating in excess of 67‚ÄØTB of multi‚Äëmodal data. The collection is organized into six components: (i) rotational 2‚ÄëD X‚Äëray images, (ii) reconstructed 3‚ÄëD CT volumes, (iii) segmented tibia CTs, (iv) CTs registered to baseline scans, (v) region‚Äëof‚Äëinterest CTs that isolate metastatic lesions, and (vi) comprehensive medical records containing quantitative bone measurements. A Python package accompanies the dataset, offering APIs for CT segmentation, registration, and ROI extraction to streamline preprocessing. The authors evaluate a set of off‚Äëthe‚Äëshelf models on three tasks‚Äîdiagnosis from 2‚ÄëD X‚Äëray, classification from 3‚ÄëD CT, and future‚Äëframe prediction and sparse‚Äëangle reconstruction. Reported results include up to 89‚ÄØ% accuracy for a spatial‚Äëtemporal vision transformer on X‚Äëray, 92‚ÄØ% accuracy for a Swin‚ÄëTransformer on registered CT, structural similarity indices of 0.86 for generative prediction, and PSNR/SSIM values of 30.2/0.81 for MedNeRF‚Äëbased reconstruction. The primary contribution is the public release of the dataset together with a processing toolkit to facilitate AI research on BCBM.\n\n---\n\n## General feedback  \n\n- **Significance:**  By providing longitudinal, high‚Äëresolution pre‚Äëclinical imaging, BoneMet fills a noticeable gap in BCBM research and has the potential to accelerate development of temporal modelling approaches.  \n- **Innovation:**  The novelty lies chiefly in the scale, multimodality, and temporal depth of the dataset, as well as the provision of a ready‚Äëto‚Äëuse API. No new algorithmic methodology is introduced.  \n- **Evaluation:**  Baseline experiments span diagnosis, prognosis, and reconstruction and demonstrate respectable performance (e.g., 89‚ÄØ% with STA‚ÄëViT on rotational X‚Äëray, 92‚ÄØ% with Swin‚ÄëB on registered CT). Nevertheless, the evaluation is limited to a handful of pre‚Äëexisting models, lacks strong competitive baselines, and does not incorporate statistical testing or cross‚Äëspecies validation.  \n- **Reproducibility:**  Although the authors intend to release the data on Hugging‚ÄØFace and the code on PyPI, the manuscript only contains placeholder URLs, omits licensing information, and provides insufficient detail on preprocessing, loss functions, and training schedules.\n\n---\n\n## Specific comments / critiques  \n\n1. **Label granularity:**  Positive and negative labels are defined at the animal level (‚Äúa metastatic bone lesion occurs between week‚ÄØ0 and week‚ÄØ5‚Äù) without per‚Äëscan or per‚Äëlesion annotation, which restricts the possibility of fine‚Äëgrained supervision (see Section‚ÄØ3.1).  \n2. **Absence of lesion‚Äëlevel ground truth:**  No segmentation masks or bounding‚Äëbox annotations for individual lesions are provided; only whole‚Äëanimal labels are used. This precludes assessment of detection or localisation performance (Section‚ÄØ3).  \n3. **Baseline selection:**  The comparison omits several state‚Äëof‚Äëthe‚Äëart 3‚ÄëD architectures (e.g., 3‚ÄëD ResNet, MedicalNet) and conventional CT reconstruction techniques, making it difficult to gauge whether the reported accuracies are competitive (Figures‚ÄØ5, Table‚ÄØ5).  \n4. **Statistical reporting:**  Accuracy figures are presented without confidence intervals or hypothesis testing, and the claimed training‚Äëtest gap for the ViT with and without STA is not quantified (Table‚ÄØ3).  \n5. **External validation:**  No experiments evaluate the transferability of models trained on BoneMet to human BCBM imaging, which would be necessary to substantiate the translational claims made in the Introduction and Section‚ÄØ6.  \n6. **Ethical documentation:**  The manuscript does not include a statement of IACUC approval nor reference to ARRIVE guidelines for the animal work (Supplementary Section‚ÄØA).  \n7. **Dataset access:**  URLs are placeholders (‚Äúhttps://huggingface.co/datasets/‚Äù, ‚ÄúXXX‚Äù), and no DOI, version identifier, or guidance on data storage is provided, limiting verification of availability (Abstract, Section‚ÄØ4).  \n8. **Licensing:**  The terms of use are unspecified, leaving uncertainty about permissible commercial or derivative applications (Section‚ÄØ4).  \n9. **Ablation analysis:**  The contribution of temporal information, individual imaging modalities, or specific data augmentations to the reported performance is not examined (Section‚ÄØ3).  \n10. **Pre‚Äëprocessing details:**  The description of 3‚ÄëD CT preprocessing and augmentation is sparse (e.g., only ‚ÄúRandAug (9,‚ÄØ0.5)‚Äù mentioned in Table‚ÄØ2), which hampers exact replication of the experiments.\n\n---\n\n## A suggested decision  \n\n**Reject**  \n\nThe manuscript presents a valuable dataset but, in its current form, does not satisfy the standards required for publication. Critical issues include insufficient annotation granularity, limited baseline comparisons, missing statistical analysis, lack of external validation, and incomplete documentation of data access and ethical compliance. Addressing these concerns would be necessary before the work could be considered for acceptance.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *BoneMet*, a large‚Äëscale longitudinal mouse imaging repository designed to support research on breast‚Äëcancer bone metastasis (BCBM). The dataset spans more than five hundred subjects and 67‚ÄØTB of multimodal data collected between 2019 and‚ÄØ2024, encompassing six organized components ranging from raw X‚Äëray and 3‚ÄëD CT volumes to registered and region‚Äëof‚Äëinterest scans, accompanied by detailed medical records. A Python package provides APIs for segmentation, registration, and ROI extraction. Baseline evaluations are conducted on diagnosis, classification, future‚Äëframe prediction, and sparse‚Äëangle reconstruction tasks, with reported accuracies up to 92‚ÄØ%. The paper‚Äôs main contribution lies in the creation and prospective public release of this comprehensive dataset and accompanying tools; no new algorithms are introduced.\n\n---\n\n**Major Comments**  \n1. **Label resolution:**  The use of mouse‚Äëlevel labels without per‚Äëscan or lesion annotations limits the capacity for fine‚Äëgrained supervision and downstream detection tasks.  \n2. **Lack of lesion‚Äëlevel ground truth:**  Absence of segmentation masks or bounding boxes prevents evaluation of localization accuracy.  \n3. **Restricted baseline comparisons:**  The evaluation omits several state‚Äëof‚Äëthe‚Äëart 3‚ÄëD models and standard reconstruction methods, making competitiveness uncertain.  \n4. **Insufficient statistical analysis:**  Accuracy metrics are reported without confidence intervals, hypothesis testing, or quantification of training‚Äìtest gaps.  \n5. **No external validation:**  There is no assessment of model transferability to human or other species imaging, weakening translational claims.  \n6. **Incomplete ethical documentation:**  IACUC compliance or adherence to ARRIVE guidelines is not stated.  \n7. **Dataset accessibility:**  Provided URLs are placeholders, lacking DOIs and storage or versioning details.  \n8. **Unspecified licensing:**  Terms of data use and redistribution are unclear.  \n9. **Missing ablation studies:**  The relative impact of temporal information, modality contributions, or augmentations is unexplored.  \n10. **Sparse preprocessing description:**  Limited detail on 3‚ÄëD CT preprocessing and augmentation hinders reproducibility.\n\n---\n\n**Minor Comments**  \n- Figures and tables are generally clear, though cross‚Äëreferencing could be improved (e.g., ensuring all tasks are linked to corresponding figures).  \n- Placeholder links and missing identifiers should be updated prior to release.  \n- Tables referencing methods (e.g., ‚ÄúRandAug‚ÄØ(9,‚ÄØ0.5)‚Äù) should be expanded with explicit parameter definitions.\n\n---\n\n**Summary Paragraph**  \nOverall, the work addresses an important need for longitudinal, high‚Äëresolution BCBM imaging data and demonstrates potential value for future research. Its main strengths are dataset scope, multimodality, temporal depth, and provision of a supporting toolkit. However, key weaknesses include insufficient annotation granularity, absence of lesion‚Äëlevel ground truth, limited benchmarking, missing statistical rigor, lack of external validation, and incomplete documentation of data access, ethics, and licensing. These deficiencies significantly limit reproducibility and evaluative credibility in the current state.\n\n---\n\n**Decision Recommendation**  \n**Reject.** While the dataset could become a meaningful resource, the present submission requires major revisions and substantial additional documentation before it meets publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Fudong Lin",
      "Jason Jiang",
      "Liyun Wang",
      "Shubo Wang",
      "Tiankuo Chu",
      "Wiley Jia-Wei Gong",
      "Xu Yuan"
    ],
    "url": "pdfs/iclr.cc-2025-conference_c17c3d4a8e2210abad05684756a47d326d0f63e8.pdf",
    "remote_url": "https://openreview.net/pdf/c17c3d4a8e2210abad05684756a47d326d0f63e8.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "unsupervised anomaly detection",
      "medical images",
      "contrastive reverse distillation",
      "student-teacher"
    ],
    "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors propose a new method for anomaly detection in medical data setting using contrastive reverse distillation. The method uses knowledge distillation as it's foundation, implementing pre-trained teachers models along with a student model trained to either minimize or maximize its distance from the teachers in a feature-map space. They additionally utilize a scale adaptive mechanism to allow for better generalizations across different sizes of anomaly.\n\nTheir method's specific construction utilizes two teacher models with the same weights, one \"bad\" and one \"good\". The good model is given benign images while the bad model is given images with synthetic anomalies. The student model is then trained to maximize its cosine similarity with the per-layer features of the good teacher while minimizing the cosine similarity with the bad teacher. At evaluation time anomaly scores are computed by comparing the scaled, per-layer feature spaces of the good teacher against those of the student. Inputs with high levels of dissimilarity between the good model and student are given high anomaly scores.\n\nTheir evaluation against prior work is very comprehensive, comparing against a total of 18 other detectors. They out perform all other detectors on all three datasets (lung, brain, and skin anomalies) in terms of their main metrics (AUC, F1, and Accuracy). The authors further perform ablations with respect to their main contributions of Contrastive Reverse Distilation and Scale Adaptive Mechanisms.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n* Anomaly detection is somewhat outside my area of expertise, but the paper seems to frame itself well within the current body of literature. \n* The paper is well written and easy to follow. Figures are also informative and easy to understand.\n* The idea of contrastive reverse distillation seems novel (to the best of my knowledge) and intuitive. I'd like to see how future works can iterate upon this framework in other settings or with improved constructions.\n* Motivation is well established for the usage synthetic anomalies, and the choice of simplex noise is clever.\n* Empirical analysis is very comprehensive in terms of datasets, baselines, and ablations.\n* The proposed method out-performs all baselines in terms of AUC, F1, and Accuracy.\n* The proposed method scales well with model architecture size, which is a good sign that the method is sensible.\n\n### Weaknesses\n\nIn general, within my somewhat limited knowledge of this sub-field, I like this paper. In spite of that I think there are a few things the authors may want to consider to further strengthen their work.\n\nI think when discussing detection and AUC scores it is important to give them context within the given domain. See section B of [1] for some example discussion on this topic from a different field. I'd like the authors to consider the following questions:\n\n* Is it more important to maximize the true positive rate in spite of inducing some false positives? \n* Is it better to minimize false positives to increase confidence in the case of a positive prediction?\n* How do you anticipate doctors using these tools in their analysis?\n\nGiven this, it would also be nice if the authors could plot some of the ROCs between their method and some of the baselines (maybe including all of them is too visually messy, but at least the top 5). This way people can compare their method to the baselines at different levels of false positive rate.\n\nI think it would be good if the authors could include some extended discussion, either in the main body or the supplementary material, analyzing the more subtle features of their results. See the next section of my review \"Questions\" for some examples of what I think they can further analyze.\n\nLastly, the results of table 1 are good, but a bit overwhelming. I think it would help if the second best score in each metric is underlined and if the methods are visually grouped based on detector type (e.g. RD4AD, RD++, and ReContrast could be grouped together). Optionally it would be nice if all experiments were averaged over a few runs and given standard deviations or $\\pm$ scores, at least in the supplementary material. I understand if these experiments are too extensive to replicate too many times, though.\n\n [1] \"Membership Inference Attacks From First Principles\" https://arxiv.org/abs/2112.03570\n\n### Questions\n\nHere I include some questions for the authors to consider.\n\n* Why does ISIC seem like the hardest dataset for all the detectors? Why is Brain Tumor the easiest?\n* Why is SAM more impactful in the Brain Tumor dataset while it seems minimally impactful in the RSNA dataset in table 2? Is this related at all to the results of table 3 - where Gaussian noise performs similar to simplex noise on RSNA, but much worse on Brain Tumor?\n* Can the method achieve better generalization by using a mixture of synthetic noise types, perhaps 10% gaussian, 90% simplex? \n* Can we improve performance by including some real world data in the training set, or will that harm generalization?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel method for anomaly detection in medical imaging using *contrastive reverse distillation* (CRD). Building upon knowledge distillation, the approach employs two teacher models‚Äîone trained on benign images (‚Äúgood‚Äù) and one on synthetic anomalies (‚Äúbad‚Äù)‚Äîand a student model trained to align with the good teacher and diverge from the bad teacher in feature-space similarity. A scale-adaptive mechanism is incorporated to improve generalization across anomaly sizes. The study offers an extensive evaluation over three datasets (lung, brain, skin) and demonstrates state-of-the-art results against 18 existing detectors. The paper is clearly written, well-motivated, and supported by comprehensive experiments and ablations.\n\n**Major Comments**  \n1. **Contextualization of Results**: While quantitative performance is strong, the discussion lacks context on the clinical implications of high AUC and detection rates. The authors could elaborate on trade-offs between maximizing true-positive rates versus minimizing false positives, particularly given the practical constraints of medical use.  \n2. **ROC Visualization**: Including receiver operating characteristic (ROC) plots comparing the proposed method with key baselines (e.g., top five) would help interpret performance across different false positive thresholds.  \n3. **Result Interpretation**: The results, particularly in Table‚ÄØ1, could benefit from additional analysis on dataset-specific behavior. For instance, ISIC appears hardest for all detectors, whereas Brain Tumor is easiest. The authors might discuss why this pattern occurs and how dataset characteristics influence performance.  \n4. **Presentation of Results**: Table‚ÄØ1 is dense and potentially overwhelming. Highlighting second-best results and visually grouping methods by detector type could improve readability. Reporting variability across runs (e.g., standard deviations) would further strengthen confidence in the reported results, though it is acknowledged this may be computationally intensive.  \n5. **Further Analysis**: The influence of the Scale Adaptive Mechanism (SAM) varies across datasets. The authors could clarify why SAM yields stronger effects on Brain Tumor data than on RSNA and consider whether different synthetic noise types or mixtures could enhance generalization.\n\n**Minor Comments**  \n- Figures are clear and informative, and overall presentation quality is high.  \n- Terminology such as ‚Äúcontrastive reverse distillation‚Äù and ‚Äúscale adaptive mechanism‚Äù are well introduced; minor labeling refinements in tables could improve clarity.  \n- Typographical quality and readability are good.\n\n**Summary Paragraph**  \nOverall, the paper is well-executed, clearly presented, and methodologically sound. The proposed use of CRD and synthetic anomalies is both intuitive and original, and the empirical evaluation is extensive. Primary suggestions concern deeper interpretation of the results, improved visualization and organization of performance tables, and additional domain-context discussion. The work represents a solid, well-motivated contribution that could be further strengthened through these refinements.\n\n**Decision Recommendation**: **Minor Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nTwo issues arise in reverse knowledge distillation for medical imaging, an inability to properly distinguish between different features and an inability to deal with different scales. This work proposes two methods to address these issues: a contrastive student-teacher learning approach that involves using both a ‚Äúgood‚Äù and a ‚Äúbad‚Äù teacher, as well as a scale adaptation mechanism. Results demonstrate that on almost every dataset the proposed approach is able to outperform existing techniques.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well written and the ideas are easy to follow. I appreciate how the authors identify the two problems facing medical imaging in this domain and then propose solutions. The new approach outperforms all other methods that the authors compare with in table 1.\n\n### Weaknesses\n\nIssue 1: This paper doesn‚Äôt have any formal proofs or theorems to guarantee its effectiveness, therefore the strengths of the paper must rely on the experiments and empirical results. For the datasets I will admit as a review I am not very familiar with them. For example, on the image dataset CIFAR-10, a few years ago a jump from 93% to 98% was considered significant. Today an increase from 99% to 99.9% on CIFAR-10 would not even be considered a major contribution. Likewise, for these datasets it is hard to determine whether a jump of 3.01% and 4.38% (as the authors show) is worthy of publication. Could the authors please clarify how much other papers pushed the accuracy forward in other works, as compared to the increase their method is offering?\n\nIssue 2: The authors say they will release their code upon publication, but I did not see any code given in the supplemental material. This might be an oversight on my part, but where is the code? I would like to see a good faith gesture of releasing an anonymous github for the code or at least a zip file of the code. Too many papers claim SOTA results and never release code. In this day and age it is simply unacceptable to continue this trend. \n\nIssue 3: The authors also don‚Äôt mention anything about the runtime of their algorithm. How does the student-teacher setup and training time compare to other existing methods in the field? While this is not a HUGE concern, I think it would be good to have some discussion and analysis of this. For example could you show the training time complexity of your algorithm and the next two best methods in the field? Or even an experimental runtime of the training in seconds of your algorithm and the next two best methods? Without this I feel we don‚Äôt have the complete story when comparing methods. \n\nIssue 4: Please see my minor comments about uncited claims in the papers. For acceptance I would like to see citations for all the claims you have made. \n\nIssue 5: I don‚Äôt like the terminology ‚Äúgood‚Äù and ‚Äúbad‚Äù teacher. It is very non-technical and hard to follow. Can you please change the terminology in your paper and name the teachers more appropriately? \n\nMinor Comments:\n\n=Line 36, ‚Äúanomalous samples is often prohibitively expensive and time-consuming‚Ä¶‚Äù I believe this is correct but I want to see a citation to backup this claim?\n\n=Line 45, ‚Äúthey tend to generalize too well, thereby risking the reconstruction of abnormal regions.‚Äù Citation for this claim? \n\n=Table 1 has many entries that are unsorted (aside from the bolded number indicating the best method). It would be better if this table was sorted in order of increasing AUC for the methods.\n\n=Line 167 ‚ÄúMedical images typically exhibit a power law distribution of frequencies, with lower frequency components dominating the image content.‚Äù Please give a citation for this bold claim?\n\n### Questions\n\nQ1: Please address each of my issues 1-5. If they are adequately addressed I would be willing to increase my score for the paper accordingly.\n\nEDIT: The reviewers have address my concerns adequately. I have update the score of my review accordingly.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses two challenges in reverse knowledge distillation for medical imaging: differentiating between feature types and managing multiple scales. The authors propose a contrastive student‚Äìteacher learning framework using both a ‚Äúgood‚Äù and a ‚Äúbad‚Äù teacher, along with a scale adaptation mechanism to improve performance. Experimental results across several datasets indicate consistent improvements over competing methods. The paper is clearly written and presents its ideas in an accessible manner.  \n\n**Major Comments**  \n1. **Lack of Theoretical Guarantees:** The paper does not include formal proofs or theoretical analysis to justify the effectiveness of the proposed approach. The contribution therefore relies entirely on empirical results, which could be insufficient without stronger justification.  \n2. **Significance of Reported Gains:** The reviewer questions whether the reported improvements (3‚Äì4% increases) constitute a meaningful advance compared to previous work, noting that for well-studied datasets such as CIFAR-10, incremental improvements at high accuracy levels may not be substantial. The authors should contextualize their gains relative to other recent methods.  \n3. **Code Availability:** The paper states that code will be released upon publication, but no code or supplementary materials are currently available. The reviewer emphasizes the importance of providing an anonymous code repository at submission to ensure transparency and reproducibility.  \n4. **Runtime Analysis:** The manuscript omits discussion of computational cost. A comparison of training time or complexity between the proposed method and baseline approaches would help evaluate efficiency.  \n5. **Terminology Choice:** The use of ‚Äúgood‚Äù and ‚Äúbad‚Äù teacher is viewed as overly informal. More descriptive, technical terms are recommended.  \n6. **Citation Completeness:** Several claims are unsupported by references (see minor comments). All such statements should be properly cited.  \n\n**Minor Comments**  \n- Line 36: Add a citation supporting the statement on the expense of anomalous sample collection.  \n- Line 45: Provide a reference regarding generalization risks.  \n- Line 167: Include a citation for the claim on power-law frequency distributions in medical images.  \n- Table 1: Consider sorting results in ascending AUC order for clarity.  \n\n**Summary Paragraph**  \nOverall, the paper proposes an intuitively motivated enhancement to student‚Äìteacher frameworks and demonstrates empirical improvements. Its strengths lie in clarity of presentation and solid experimental support. However, the lack of theoretical backing, contextualization of performance gains, limited reproducibility information, and missing runtime analysis reduce the perceived strength of the contribution. Addressing these points would considerably improve the manuscript‚Äôs rigor.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a twofold approach: on one hand, data augmentation for unlabeled information is achieved by injecting noise, which then serves as the basis for contrastive learning. On the other hand, parameters are extracted at each layer to achieve multi-scale information fusion, enabling the model to better distinguish between positive and negative samples.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe methodology section of this paper is clearly written and well-presented.\n\n### Weaknesses\n\n1. In terms of innovation, this article presents two components that are commonly seen in other works. A straightforward suggestion would be for the authors to explore a more adaptable data augmentation method to address the issue raised in ‚ÄúQuestion 1.‚Äù\n\n2. Regarding the writing, the **Introduction** section of this article is somewhat confusing. Firstly, the paragraphing is suboptimal. In the second paragraph of the introduction, the author discusses the shortcomings of anomaly detection models based on generation. The third paragraph explains existing work aimed at improving these models, followed by an exploration of the development of self-supervised learning. Although self-supervised learning indeed enriches unsupervised learning within deep learning, the third paragraph would be clearer if it unified these two parts, focusing on how self-supervised learning specifically addresses the issues raised in the second paragraph. Secondly, and more importantly, an excessive portion of the introduction is dedicated to discussing related work, lacking sufficient focus on the contributions of this paper. Maybe you could merging the discussion of existing work and self-supervised learning, followed by a clear transition to the paper's contributions. Additionally, you could recommend what key points about the paper's contributions should be emphasized earlier in the introduction.\n\n### Questions\n\n1. In the noise generation process, the intensity of each position is sampled from a uniform distribution. However, in real-world medical imaging, anomalous regions in disease cases appear in specific locations; for example, abnormalities in a chest image typically do not appear outside body regions. For instance, you could suggest incorporating prior knowledge about typical anomaly locations in different types of medical images.\n\n2. Additionally, is each test sample measured at the image level or at the patch level? If it is at the image level, the positive-to-negative anomaly ratio in the dataset is relatively high, which does not quite align with the extremely imbalanced nature of anomaly detection tasks.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a dual strategy for anomaly detection that combines noise-based data augmentation for contrastive learning with multi-scale feature fusion through parameter extraction at each network layer. The approach is intended to enhance the model‚Äôs ability to distinguish positive and negative samples. The methodology section is clearly written, though the overall clarity of the introduction and articulation of contributions could be improved.\n\n**Major Comments**  \n1. **Limited Novelty:** Both main components‚Äînoise-based augmentation and multi-scale feature fusion‚Äîare common in related literature. The reviewer suggests exploring a more adaptive or innovative augmentation strategy to better address the stated research question.  \n2. **Methodological Detail:** In the noise generation process, the intensity at each position is uniformly sampled. This assumption may not reflect the spatial priors of real medical anomalies, which typically appear in specific anatomical regions. Incorporating prior knowledge about expected anomaly locations could make the approach more realistic.  \n3. **Evaluation Design:** Clarify whether detection is performed at the image level or patch level. If evaluated at the image level, the dataset‚Äôs high positive-to-negative ratio may not represent the severe imbalance encountered in practical anomaly detection tasks.  \n\n**Minor Comments**  \n1. The **Introduction** requires restructuring for clarity. Currently, the sequence of paragraphs (limitations of generative models, improvements by others, and introduction of self-supervised learning) could be better unified to show how self-supervised learning directly addresses prior shortcomings.  \n2. The introduction devotes excessive space to related work, with insufficient emphasis on the paper‚Äôs unique contributions. A more explicit statement of contributions and a clearer transition from literature review to the paper‚Äôs focus would improve readability.  \n\n**Summary Paragraph**  \nThe paper presents a clearly written methodology and a reasonable approach to anomaly detection; however, its novelty is modest and certain design choices could better reflect real-world conditions. The introduction‚Äôs organization and articulation of contributions should also be refined for improved coherence and contextual framing.  \n\n**Decision Recommendation**  \n**Major Revision.** The study would benefit from stronger methodological innovation, clearer problem motivation, and improved presentation of contributions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚úÖ**  \n   The manuscript introduces a novel *scale-aware contrastive reverse distillation* framework for **unsupervised medical anomaly detection**. The innovation is methodological rather than clinical‚Äîit involves a new training paradigm combining contrastive learning and knowledge distillation with scale adaptation. This fits well within IEEE‚ÄØTMI‚Äôs scope in machine learning methodology and image analysis for medical imaging.\n\n2. **Novelty & Contribution Level ‚Äì Moderate to High**  \n   The work extends the reverse distillation concept with two methodological contributions: (1) a *contrastive student‚Äìteacher learning* mechanism that enhances representation discrimination through synthesized pseudo‚Äëabnormal data, and (2) a *scale‚Äëadaptation module* that dynamically weights multi‚Äëscale distillation losses. Prior art (RD4AD, RD++, ReContrast) did not combine these aspects. The novelty relative to RD++ and ReContrast appears incremental but non‚Äëtrivial‚Äîthe contrastive formulation and scale weighting address recognized shortcomings (feature indistinctness, scale sensitivity).  \n\n3. **Technical and Experimental Rigor ‚Äì High but with Some Gaps**  \n   Experiments are extensive across three imaging modalities (X‚Äëray, MRI, dermoscopy), comparing against over ten baselines with thorough ablations and backbone tests. The method appears technically sound and reproducible (code link provided). However, further clarification would aid rigor:  \n   - justification of simplex‚Äënoise realism relative to anatomical priors;  \n   - statistical significance of reported AUC improvements;  \n   - verification that no anomalous cases leak into training (dataset splits need confirmation).  \n\n4. **Clarity and Presentation ‚Äì Generally Clear**  \n   Writing is concise and logically structured.  \n   - Figures are informative (architecture, histograms, ROC curves) but could benefit from higher resolution and standardized color schemes for accessibility.  \n   - Some typographic and formatting inconsistencies remain (‚Äúf eature d iscriminability,‚Äù etc.).  \n\n5. **Ethical and Reproducibility Compliance ‚Äì Mostly Adequate**  \n   - All datasets are public; authors cite IRB/ethics sources appropriately.  \n   - Link to public code is provided.  \n   - No mention of anonymization within generated examples, but given datasets are de‚Äëidentified, risk is low.  \n   - Strong reproducibility practices (detailed algorithm pseudo‚Äëcode included).  \n\n---\n\n**Phase 2: IEEE‚ÄØTMI Review Report**\n\n### 1. Summary\nThe manuscript proposes a *Scale‚ÄëAware Contrastive Reverse Distillation (SACRD)* approach for unsupervised anomaly detection in medical images. Building on reverse distillation frameworks, the authors introduce (i) a contrastive student‚Äìteacher learning strategy that leverages a pair of ‚Äúclean‚Äù and ‚Äúnoisy‚Äù teachers to encourage discriminative representations, and (ii) a scale‚Äëadaptation module that dynamically weights loss contributions across feature scales. Synthetic anomalies are generated using *simplex noise* to produce structured perturbations. Evaluated on RSNA chest X‚Äërays, a brain‚Äëtumor MRI dataset, and the ISIC 2018 dermoscopy dataset, the proposed method achieves state‚Äëof‚Äëthe‚Äëart AUC, F1, and accuracy compared with 15+ recent baselines.\n\n### 2. Strengths\n- **Methodological innovation:** Combines contrastive and reverse‚Äëdistillation paradigms with scale weighting‚Äîan elegant enhancement addressing feature discriminability and scale variation.  \n- **Comprehensive experimentation:** Multiple datasets, ablation studies (contrastive loss, SAM module, noise type and intensity, backbone).  \n- **Strong results:** Reported AUC improvements of 3‚Äì4‚ÄØ% vs. best baselines and clear qualitative separation of normal vs. abnormal distributions.  \n- **Good reproducibility:** Public code, detailed architecture, and pseudo‚Äëcode.  \n- **Generalizability:** Demonstrated across modalities, reinforcing potential broad utility.  \n\n### 3. Weaknesses\n- **Incremental nature:** Builds directly upon RD4AD/RD++ with architectural tweaks rather than a fundamentally new learning principle; may be near TMI‚Äôs lower novelty bound.  \n- **Synthetic anomaly realism:** Simplex noise is a convenient proxy but lacks clinical plausibility; justification or qualitative examples of realism would strengthen claims.  \n- **Statistical validation:** No confidence intervals or significance analyses on AUC differences.  \n- **Limited discussion of failure cases:** Insight into missed anomalies or false‚Äëpositives is absent.  \n- **Figure/formatting issues:** Some blurry figures and inconsistent typography.  \n\n### 4. Major Comments\n1. **Novelty positioning:** Clearly delineate conceptual differences from RD++ and ReContrast in the Introduction and Discussion, ideally through a table or schematic emphasizing algorithmic and loss‚Äëfunction differences rather than implementation details.  \n2. **Statistical analysis:** Include significance testing (e.g., DeLong test) or bootstrapped confidence intervals for AUC to demonstrate robustness of reported improvements.  \n3. **Synthetic anomaly justification:** Provide qualitative comparison of simplex‚Äënoise anomalies versus realistic pathologies; discuss limitations regarding anatomical plausibility.  \n4. **Failure case analysis:** Present sample anomaly maps illustrating false detections or missed lesions to help readers understand boundary conditions of the model.  \n5. **Clinical interpretability:** Comment on how contrastive and scale‚Äëaware representations might translate to localizing pathology or assisting clinician trust.  \n\n### 5. Minor Comments\n- Correct text artifacts and spacing (e.g., ‚Äúf eature d iscriminability‚Äù).  \n- Improve figure legends and resolution.  \n- Standardize abbreviations (e.g., SAM vs. S.A.M.).  \n- Provide exact runtime environment in reproducibility statement (CUDA/cuDNN versions).  \n- Ensure that all referenced works have full bibliographic information (some abbreviations inconsistent).  \n\n### 6. Overall Recommendation  \n**Recommendation:** **Major Revision**  \n\n**Rationale:**  \nThe paper demonstrates strong technical execution and promising performance but remains a methodological refinement over existing reverse‚Äëdistillation models rather than a fundamentally novel concept. With clearer differentiation from prior art, additional statistical support, and expanded discussion of synthetic‚Äëanomaly validity, it could reach TMI‚Äôs publishable standard.\n\n### 7. Confidence Level  \n**4‚ÄØ/‚ÄØ5 ‚Äì High Confidence**  \nBased on familiarity with reverse‚Äëdistillation anomaly detection literature and medical imaging machine‚Äëlearning methods.\n\n---\n\n**Summary Judgment:**  \nA rigorous and well‚Äëexecuted study presenting an incremental yet meaningful enhancement to unsupervised anomaly detection. The work is within TMI scope and could be publishable after strengthened justification of novelty, statistical rigor, and interpretability.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a *Scale‚ÄëAware Contrastive Reverse Distillation (SACRD)* framework for unsupervised medical anomaly detection. This method integrates contrastive learning with a student‚Äìteacher reverse‚Äëdistillation design and introduces a scale‚Äëadaptation module that adjusts loss weighting across feature scales. Synthetic anomalies are generated using simplex noise to encourage discriminative representations. Experiments across X‚Äëray, MRI, and dermoscopy datasets show consistent performance gains over existing baselines. Overall, the paper is clearly written, technically sound, and well structured, though several aspects require clarification to fully establish novelty, statistical robustness, and interpretability.\n\n---\n\n**Major Comments**  \n1. **Novelty Positioning:** The approach extends RD4AD and RD++ with contrastive objectives and scale‚Äëaware loss weighting. The contribution appears incremental; the authors should more clearly delineate conceptual advances over these prior models, ideally with a comparative table or schematic summarizing algorithmic differences.  \n2. **Statistical Validation:** The reported AUC improvements lack confidence intervals or significance testing. Including statistical analyses (e.g., bootstrapped intervals or significance tests) would substantiate claims of superiority.  \n3. **Synthetic Anomaly Realism:** Simplex noise as a surrogate for real anomalies is not fully justified. Additional discussion or visual comparison demonstrating its plausibility relative to anatomical structures would strengthen the methodological argument.  \n4. **Failure‚ÄëCase Explanation:** The study omits examples of false positives or missed anomalies. Showing representative failure cases could help clarify the system‚Äôs limitations and boundary conditions.  \n5. **Clinical Interpretability:** The paper would benefit from comments on how scale‚Äëaware and contrastive representations could aid localization or clinician confidence, supporting potential downstream applicability.\n\n---\n\n**Minor Comments**  \n- Correct typographic artifacts (e.g., ‚Äúf eature d iscriminability‚Äù) and ensure consistent abbreviations (e.g., SAM).  \n- Improve figure resolution and standardize color schemes for accessibility.  \n- Provide complete bibliographic information for all cited works.  \n- Specify runtime environment details (e.g., CUDA/cuDNN versions) in reproducibility statements.\n\n---\n\n**Summary Paragraph**  \nThis work offers a technically solid and well‚Äëevaluated extension of reverse‚Äëdistillation frameworks for unsupervised anomaly detection. Its strengths lie in methodological integration, comprehensive experimentation, and strong reproducibility practices. However, the incremental nature of the innovation, the limited justification for synthetic anomalies, and the absence of statistical significance testing and failure analysis reduce the perceived conceptual impact. Addressing these issues would make the contribution clearer and more convincing.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The study demonstrates solid technical depth and promising empirical results but requires stronger evidence of novelty, statistical rigor, and explanation of methodological assumptions before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes a scale-aware contrastive reverse distillation model for unsupervised medical anomaly detection. The method addresses two limitations of existing reverse distillation approaches: insufficient feature discriminability and inability to handle anomaly scale variations. The approach introduces a contrastive student-teacher learning framework with dual encoding pathways - a \"clean\" teacher processing normal data and a \"noisy\" teacher processing synthesized anomalies created using simplex noise (Section 2.2.2). A scale adaptation mechanism generates input-specific weights to handle multi-scale anomalies (Equation 3-4). The method is evaluated on three medical datasets (RSNA, Brain Tumor, ISIC) and achieves state-of-the-art performance, with AUC improvements of 3.01%, 4.38%, and 3.08% respectively over second-best methods (Table 1).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and unclear notation**\n  - Equation 2 presents a ratio-based loss without theoretical justification for why this specific formulation enhances discrimination compared to additive or weighted alternatives\n  - The reshaping operation mentioned in Section 2.2.1 for converting uk and vk to 1D representations lacks mathematical specification of dimensions and aggregation method\n  - The small epsilon value in the denominator (Equation 2) is introduced without analysis of its impact on gradient stability or convergence properties\n  - Scale descriptor normalization in Equation 3 uses softmax, but no justification is provided for why this constraint improves performance over other normalizations\n\n‚Ä¢ **Limited experimental rigor and insufficient baseline comparisons**\n  - Table 1 shows inconsistent performance patterns across methods without statistical significance testing or confidence intervals to validate claimed improvements\n  - Ablation study in Table 2 tests only two components, missing analysis of individual contributions of simplex noise type, noise intensity, and architectural choices\n  - The comparison with RD++ in Section 2.5 lacks experimental validation on the same datasets to substantiate claimed fundamental differences\n  - Figure 3 shows noise intensity analysis only on RSNA dataset, missing validation on other datasets where scale variations differ significantly\n\n‚Ä¢ **Methodological limitations in synthetic anomaly generation**\n  - Section 2.2.2 uses fixed hyperparameters (Œ≥ = 0.6, Œª = 0.2, six octaves) for simplex noise without dataset-specific optimization or sensitivity analysis\n  - The assumption that medical anomalies follow power law distributions lacks empirical validation or citation support in the manuscript\n  - Table 3 compares only Gaussian vs. simplex noise, missing comparison with other structured noise types or learnable anomaly synthesis methods\n  - Algorithm 1 shows random spatial sampling for noise placement without consideration of anatomically relevant regions in medical images\n\n‚Ä¢ **Incomplete evaluation and limited generalizability analysis**\n  - The three datasets represent limited medical imaging diversity, missing common modalities like CT, ultrasound, or histopathology mentioned in related work\n  - Table 4 backbone comparison lacks analysis of computational trade-offs or performance scaling with model complexity\n  - Figure 2 qualitative analysis shows only score distributions without spatial anomaly localization examples or failure case analysis\n  - Discussion in Section 3.4 acknowledges dataset-specific performance differences but provides no systematic analysis of when the method succeeds or fails\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical foundations and provide clearer notation**\n  - Provide theoretical analysis or empirical justification for the ratio-based loss formulation in Equation 2, comparing against alternative contrastive loss designs\n  - Specify exact mathematical operations for feature reshaping, including dimensional analysis and impact on spatial information preservation\n  - Conduct sensitivity analysis of epsilon values and their effect on training stability, providing guidance for hyperparameter selection\n  - Compare softmax normalization against alternative constraints for scale descriptors, with ablation studies showing the impact on multi-scale performance\n\n‚Ä¢ **Enhance experimental rigor with comprehensive validation**\n  - Include statistical significance testing with confidence intervals for all performance comparisons in Table 1, using appropriate multiple testing corrections\n  - Expand ablation studies to isolate individual component contributions, including noise type, architectural choices, and loss function variants\n  - Provide direct experimental comparison with RD++ on identical datasets and evaluation protocols to substantiate claimed improvements\n  - Extend noise intensity analysis (Figure 3) to all datasets, analyzing the relationship between optimal noise levels and dataset characteristics\n\n‚Ä¢ **Improve synthetic anomaly generation methodology**\n  - Develop dataset-adaptive hyperparameter selection for simplex noise generation, with systematic optimization across different medical imaging modalities\n  - Provide empirical validation of power law assumptions in medical images or explore alternative frequency distribution models\n  - Compare against a broader range of anomaly synthesis approaches, including learnable methods and anatomically-informed generation strategies\n  - Incorporate anatomical priors or attention mechanisms to guide noise placement in medically relevant regions as suggested in the conclusion\n\n‚Ä¢ **Broaden evaluation scope and provide deeper analysis**\n  - Include additional medical imaging modalities (CT, ultrasound, histopathology) to demonstrate broader applicability and identify domain-specific limitations\n  - Provide comprehensive computational analysis for different backbones, including inference time, memory usage, and performance scaling relationships\n  - Add spatial anomaly localization examples and systematic failure case analysis to complement the score distribution analysis in Figure 2\n  - Develop systematic characterization of method performance across different anomaly types, scales, and dataset characteristics to guide practical deployment",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a scale‚Äêaware contrastive reverse distillation (CRD) model for unsupervised medical anomaly detection. The proposed framework seeks to overcome two main limitations of prior reverse distillation methods: limited feature discriminability and difficulty handling anomalies of varying scales. It introduces a dual‚Äêteacher contrastive learning architecture‚Äîusing a ‚Äúclean‚Äù teacher for normal data and a ‚Äúnoisy‚Äù teacher that processes simplex noise‚Äìbased synthetic anomalies‚Äîand a scale adaptation mechanism with input‚Äêspecific weighting. Evaluation on three medical datasets (RSNA, Brain Tumor, ISIC) demonstrates state‚Äêof‚Äêthe‚Äêart results with reported AUC improvements over prior work. The paper is clearly organized and addresses an important task, but several methodological, theoretical, and experimental issues require clarification or expansion.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity**:  \n   - The ratio‚Äêbased loss in Equation‚ÄØ2 is not theoretically justified relative to alternative additive or weighted formulations.  \n   - The reshaping operation for converting \\(u_k\\) and \\(v_k\\) to 1D vectors lacks precise dimensional specification.  \n   - The effect of the small epsilon term in Equation‚ÄØ2 on training stability and convergence is not analyzed.  \n   - Softmax normalization for the scale descriptor (Equation‚ÄØ3) is used without rationale or comparison to other possible normalizations.  \n\n2. **Experimental design and rigor**:  \n   - Reported improvements in Table‚ÄØ1 lack statistical significance testing or confidence intervals.  \n   - The ablation study covers only two factors, omitting the impact of simplex noise parameters and architectural variants.  \n   - The claimed distinction from RD++ is not empirically validated on the same datasets.  \n   - Noise intensity analysis (Figure‚ÄØ3) is confined to RSNA, limiting insights into scaling behavior.  \n\n3. **Synthetic anomaly generation methodology**:  \n   - Fixed noise parameters are used without dataset‚Äêadaptive tuning or sensitivity analysis.  \n   - The assumption of power law distributions for anomalies is unsubstantiated.  \n   - Comparisons are limited to Gaussian versus simplex noise, excluding other structured or learnable anomaly generators.  \n   - Noise placement is random, without regard to anatomical relevance.  \n\n4. **Evaluation scope and generalizability**:  \n   - The use of only three datasets limits coverage of medical imaging diversity; key modalities such as CT, ultrasound, and histopathology are absent.  \n   - Computational trade‚Äêoffs among backbones are not analyzed.  \n   - Qualitative results lack spatial anomaly localization and failure case examples.  \n   - Dataset‚Äêspecific differences are acknowledged but not systematically examined.  \n\n**Minor Comments**  \n- Clarify notation around Equations‚ÄØ2‚Äì4 for readability.  \n- Ensure consistent dimensional notation and symbol usage across sections.  \n- Improve figure captions (Figures‚ÄØ2‚Äì3) to describe axes and experimental settings.  \n\n**Summary Paragraph**  \nThe paper offers a creative extension of reverse distillation through contrastive and scale‚Äêaware mechanisms, supported by strong quantitative results. However, unclear mathematical definitions, limited ablation depth, and insufficient analysis of synthetic anomaly design reduce confidence in the methodological soundness and generalizability of the findings. Addressing these issues‚Äîparticularly through added theoretical justification, more comprehensive experimentation, and clearer empirical validation‚Äîwould significantly strengthen the contribution.  \n\n**Decision Recommendation**  \n**Major Revision.** The approach is promising, but substantial clarification and additional experimentation are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces a scale-aware contrastive reverse distillation model for unsupervised medical anomaly detection. The authors propose a novel framework that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. The model incorporates a noisy teacher encoder to generate synthetic anomalies and a scale adaptation mechanism to account for scale variations. The authors evaluate their approach on three datasets‚Äîchest X-rays, brain MRI, and dermoscopy‚Äîand claim state-of-the-art performance. The manuscript is well-written and the technical details are presented clearly.\n\n## Major Comments\n1. Novelty and Positioning: The proposed scale-aware contrastive reverse distillation model introduces innovative elements such as the noisy teacher encoder and the scale adaptation mechanism. However, the manuscript could benefit from a more detailed discussion on how these contributions differ from existing work, particularly in the context of reverse distillation and contrastive learning. The authors should provide a clearer distinction between their method and recent advancements in these areas.\n\n2. Evaluation Design: The evaluation is conducted on three datasets from different imaging modalities, which is commendable. However, the experimental setup is limited to a specific set of backbone architectures (WideResNet50, ResNet18, ResNet50). Including a broader range of backbones and testing on additional datasets would strengthen the generalizability claims. Additionally, the manuscript should consider providing more qualitative analyses beyond the anomaly score distributions, such as visual comparisons of detected anomalies.\n\n3. Comparisons: The baseline comparisons are thorough, covering a variety of recent methods. However, the manuscript should include a discussion on the computational efficiency and memory usage of the proposed method compared to the baselines. While the authors mention inference time and memory usage, a more detailed analysis would be beneficial, especially considering the clinical relevance of these metrics.\n\n4. Reproducibility: The authors state that the code will be made available, which is good practice. However, the manuscript lacks detailed descriptions of certain aspects of the training protocol, such as the exact hyperparameters used for different backbones and the specific data preprocessing steps. Providing a more comprehensive description of the training setup would enhance the reproducibility of the results.\n\n## Minor Comments\n1. Figures: Figures 2 and 3 are somewhat cluttered. Simplifying these figures by showing fewer representative samples or zoomed-in regions would improve readability.\n   \n2. Notation and Terminology: The notation for the forward operator is introduced without sufficient explanation, and some acronyms (e.g., \"R=4\") are used without definition. Clarifying these points would improve the clarity of the manuscript.\n\n3. Typographical Issues: Minor typographical errors such as \"k-spacce\" (page 6) and \"undersampling maskes\" (page 7) should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge in unsupervised medical anomaly detection, which is crucial for early diagnosis and intervention. The proposed scale-aware contrastive reverse distillation model is technically innovative, particularly in its handling of feature discriminability and scale variations. However, the evaluation could be strengthened by including a broader range of datasets and backbones. The manuscript provides a thorough comparison with state-of-the-art methods, but the reproducibility of the approach is somewhat compromised by incomplete methodological details. Overall, the proposed method shows promise, but the current evidence needs to be expanded to fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the evaluation to include a broader range of datasets and backbones, provide more detailed methodological descriptions for reproducibility, and offer a more comprehensive discussion on computational efficiency and memory usage.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a scale-aware contrastive reverse distillation framework for unsupervised medical anomaly detection. It seeks to address limitations of existing reverse distillation methods‚Äîspecifically, low feature discriminability and difficulty handling anomaly scale variations. The approach integrates a noisy teacher encoder to synthesize anomalies and a scale adaptation mechanism to manage multi-scale features. Experiments on chest X-ray, brain MRI, and dermoscopy datasets reportedly achieve state-of-the-art performance. The paper is clearly written and technically well organized.  \n\n**Major Comments**  \n1. **Novelty and Positioning:** The proposed combination of a noisy teacher encoder and scale adaptation mechanism is innovative. Nonetheless, the paper would benefit from a clearer comparison with prior work in reverse distillation and contrastive learning. The authors should explicitly differentiate their contributions from recent related methods to clarify novelty.  \n\n2. **Evaluation Design:** Evaluation across three imaging modalities is a strength, yet the experiments rely on a limited set of backbone architectures (WideResNet50, ResNet18, ResNet50). Extending to additional backbones and datasets would better support claims of generalizability. Qualitative analyses‚Äîsuch as visual examples of detected anomalies‚Äîwould also enhance interpretability beyond anomaly score distributions.  \n\n3. **Comparisons:** Baseline coverage is broad, but computational efficiency and memory usage merit more detailed analysis. Given clinical relevance, these aspects should be explicitly compared with baselines rather than only summarized.  \n\n4. **Reproducibility:** Although code release is mentioned, the manuscript omits details on some training procedures, including specific hyperparameters and preprocessing steps. Clarifying these would improve reproducibility and transparency.  \n\n**Minor Comments**  \n- **Figures:** Figures 2 and 3 could be simplified by reducing visual clutter or highlighting representative regions.  \n- **Notation and Acronyms:** Define the forward operator and undeclared acronyms (e.g., ‚ÄúR=4‚Äù) for clarity.  \n- **Typographical Errors:** Correct minor typos such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes.‚Äù  \n\n**Summary Paragraph**  \nThis study tackles an important and clinically relevant problem in unsupervised medical anomaly detection. The proposed model demonstrates technical novelty and promising results, yet stronger evidence through broader evaluations and clearer methodological details is needed. Despite thorough comparisons, claims of scalability and reproducibility would benefit from further substantiation. Overall, the work is promising but requires additional experiments and clarifications to reach the publication standard.  \n\n**Decision Recommendation**  \n**Major Revision.** The authors should enhance evaluation breadth, detail methodological settings for reproducibility, and expand discussion of computational efficiency and memory use.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection\n\n### Summary\n\nThe paper proposes a scale-aware contrastive reverse distillation framework for unsupervised medical anomaly detection. Building on reverse distillation, the method introduces a dual-teacher setup (clean/noisy inputs with shared weights), a contrastive ratio loss that pulls student features toward clean-teacher features and pushes them away from noisy-teacher features synthesized via simplex noise, and a scale adaptation mechanism that learns input-specific weights over multi-scale features. Experiments on RSNA (X-ray), Brain Tumor (MRI slices), and ISIC 2018 (dermoscopy) report state-of-the-art AUCs and show consistent gains from both contrastive distillation and scale weighting.\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a contrastive reverse distillation objective that explicitly exploits synthetic out-of-normal distributions via a noisy-teacher pathway, conceptually strengthening discriminability relative to standard RD.Proposes a simple, learnable scale adaptation mechanism (GMP + linear + softmax) to reweight per-scale contributions conditioned on the input, addressing a well-known challenge in medical AD (varying lesion sizes).The use of simplex noise as synthetic anomalies is justified by frequency statistics and prior work; ablations on noise type and intensity support the design choice.\n- Experimental rigor and validationCross-modality evaluation (X-ray, MRI, dermoscopy) demonstrates generality and robustness.Ablations isolate the contributions of contrastive reverse distillation and the scale adaptation mechanism; sensitivity to noise intensity is explored.Broad baseline coverage includes reconstruction, memory bank, normalizing flow, SSL-based, and KD/RD-based methods.\n- Clarity of presentationThe overall architecture and training flow are clearly depicted; the roles of clean and noisy teachers and SAM are easy to follow.The ablation tables and noise-sensitivity plot provide actionable insights on design choices (e.g., Œª=0.2).\n- Significance of contributionsAddresses two practical shortcomings of RD approaches‚Äîlimited discriminability and scale variation‚Äîboth highly relevant in medical imaging.Achieves strong results on three benchmarks with a comparatively simple and computationally efficient architecture, indicating potential for practical deployment.\n\n- Introduces a contrastive reverse distillation objective that explicitly exploits synthetic out-of-normal distributions via a noisy-teacher pathway, conceptually strengthening discriminability relative to standard RD.\n- Proposes a simple, learnable scale adaptation mechanism (GMP + linear + softmax) to reweight per-scale contributions conditioned on the input, addressing a well-known challenge in medical AD (varying lesion sizes).\n- The use of simplex noise as synthetic anomalies is justified by frequency statistics and prior work; ablations on noise type and intensity support the design choice.\n\n- Cross-modality evaluation (X-ray, MRI, dermoscopy) demonstrates generality and robustness.\n- Ablations isolate the contributions of contrastive reverse distillation and the scale adaptation mechanism; sensitivity to noise intensity is explored.\n- Broad baseline coverage includes reconstruction, memory bank, normalizing flow, SSL-based, and KD/RD-based methods.\n\n- The overall architecture and training flow are clearly depicted; the roles of clean and noisy teachers and SAM are easy to follow.\n- The ablation tables and noise-sensitivity plot provide actionable insights on design choices (e.g., Œª=0.2).\n\n- Addresses two practical shortcomings of RD approaches‚Äîlimited discriminability and scale variation‚Äîboth highly relevant in medical imaging.\n- Achieves strong results on three benchmarks with a comparatively simple and computationally efficient architecture, indicating potential for practical deployment.\n\n### Weaknesses\n\n- Technical limitations or concernsThe contrastive ratio loss L = (1 ‚àí sim(u, v)) / (1 ‚àí sim(z, v) + Œµ) is unusual; it may lead to optimization instability when the denominator becomes small, lacks temperature/margin control, and is not contrasted against more standard objectives (e.g., InfoNCE, triplet with margins).The paper states that features are ‚Äúreshaped into 1D representations‚Äù to compute cosine similarity for stability, which appears inconsistent with later inference-time use of per-pixel (vector-wise) similarity maps; this discrepancy needs clarification.Synthetic anomalies rely on spatially localized simplex noise; the risk of overfitting to a specific synthetic distribution is not deeply analyzed (e.g., robustness to different synthesis regimes or domain shifts).\n- Experimental gaps or methodological issuesResults are reported largely as single-run without variance or statistical testing; considering dataset variability and unsupervised sensitivity, reporting mean¬±std over multiple seeds is important.Threshold-dependent metrics (F1/ACC) are computed with the best test-time F1 threshold; this inflates scores. Threshold selection protocols more aligned with deployment (e.g., fixed FPR, validation-free heuristics, or AUCPR/AP) would strengthen claims.No pixel-level localization evaluation is provided despite generating anomaly maps; for MRI and dermoscopy, localization metrics (AUPRC, best-Dice) would be valuable and are increasingly standard.The Brain Tumor dataset merges sources for normal and abnormal slices, raising potential domain-shift confounds; no control experiments (e.g., intensity harmonization, source-balanced splits) are reported.\n- Clarity or presentation issuesConfusion between global vs. spatial cosine similarity during training vs. inference; the bottleneck/decoder dimensionalities and exact mapping to K scales could be more explicit.The definition and effect of the normalization constant Ck in Eq. (1) and how it relates to the proposed loss are not discussed; potential scale imbalance across layers remains unquantified beyond SAM.Some experimental figures/tables (e.g., inference-time/memory bubble chart) lack detailed measurement protocols and could be misinterpreted without standardization.\n- Missing related work or comparisonsRecent KD/distillation advances that explicitly separate normal vs. abnormal modeling (e.g., DMDD 2024) are not discussed; comparisons or conceptual positioning relative to decoupled student branches and more advanced synthesis strategies would contextualize the contribution.The broader MedIAnomaly benchmark observations (e.g., threshold-selection pitfalls, evaluation standards across modalities) are cited but not fully integrated into the evaluation protocol.\n\n- The contrastive ratio loss L = (1 ‚àí sim(u, v)) / (1 ‚àí sim(z, v) + Œµ) is unusual; it may lead to optimization instability when the denominator becomes small, lacks temperature/margin control, and is not contrasted against more standard objectives (e.g., InfoNCE, triplet with margins).\n- The paper states that features are ‚Äúreshaped into 1D representations‚Äù to compute cosine similarity for stability, which appears inconsistent with later inference-time use of per-pixel (vector-wise) similarity maps; this discrepancy needs clarification.\n- Synthetic anomalies rely on spatially localized simplex noise; the risk of overfitting to a specific synthetic distribution is not deeply analyzed (e.g., robustness to different synthesis regimes or domain shifts).\n\n- Results are reported largely as single-run without variance or statistical testing; considering dataset variability and unsupervised sensitivity, reporting mean¬±std over multiple seeds is important.\n- Threshold-dependent metrics (F1/ACC) are computed with the best test-time F1 threshold; this inflates scores. Threshold selection protocols more aligned with deployment (e.g., fixed FPR, validation-free heuristics, or AUCPR/AP) would strengthen claims.\n- No pixel-level localization evaluation is provided despite generating anomaly maps; for MRI and dermoscopy, localization metrics (AUPRC, best-Dice) would be valuable and are increasingly standard.\n- The Brain Tumor dataset merges sources for normal and abnormal slices, raising potential domain-shift confounds; no control experiments (e.g., intensity harmonization, source-balanced splits) are reported.\n\n- Confusion between global vs. spatial cosine similarity during training vs. inference; the bottleneck/decoder dimensionalities and exact mapping to K scales could be more explicit.\n- The definition and effect of the normalization constant Ck in Eq. (1) and how it relates to the proposed loss are not discussed; potential scale imbalance across layers remains unquantified beyond SAM.\n- Some experimental figures/tables (e.g., inference-time/memory bubble chart) lack detailed measurement protocols and could be misinterpreted without standardization.\n\n- Recent KD/distillation advances that explicitly separate normal vs. abnormal modeling (e.g., DMDD 2024) are not discussed; comparisons or conceptual positioning relative to decoupled student branches and more advanced synthesis strategies would contextualize the contribution.\n- The broader MedIAnomaly benchmark observations (e.g., threshold-selection pitfalls, evaluation standards across modalities) are cited but not fully integrated into the evaluation protocol.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe contrastive ratio formulation is intuitive but could be analytically grounded: gradients can explode when sim(z, v) approaches 1; adding only Œµ may not suffice. Consider adding temperature scaling, a margin, or switching to a bounded and well-studied loss (e.g., logistic/InfoNCE: ‚àílog exp(sim(u,v)/œÑ) / [exp(sim(u,v)/œÑ) + exp(sim(z,v)/œÑ)]) for stability and interpretability.Clarify whether ‚Äúreshaped into 1D‚Äù means flattening across spatial and channel dimensions, or only across channels per spatial location. If training is global but inference is local, explain why this mismatch improves performance and whether a local contrastive variant was tried.The SAM design is elegant; however, explain why GMP was preferred over GAP or mixed pooling (GMP is sensitive to outliers). Ablate or analyze alternatives and report the learned Œ± distributions and their correlation with lesion sizes across datasets.Anomaly synthesis via simplex noise is reasonable and supported by ablation. Provide qualitative visualizations of synthetic patches and their frequency spectra to further justify realism. Consider mixing scales or multiple procedural textures to reduce overfitting to one synthesis family.\n- Experimental evaluation assessmentReport results over multiple random seeds (mean¬±std). Include AUCPR/AP for imbalanced settings and operating-point metrics like FPR@95%TPR. For image-level scoring, avoid selecting thresholds on the test set; adopt validation-free heuristics or report both ‚Äúoracle‚Äù and ‚Äúpractical‚Äù thresholds.Since anomaly maps are produced, evaluate localization on at least one dataset with pixel-level labels (e.g., BraTS variants) or create proxy masks via expert annotations. Even if the focus is classification, localization is a critical clinical desideratum.The Brain Tumor setup risks domain confounds (normals and tumors from different repositories). Provide harmonization steps, source-balanced splits, or a cross-source control to confirm the model is detecting pathology rather than dataset bias.Fairness of baseline comparisons: clarify whether all baselines used the same backbone/input size and pretraining regime when possible. Some baselines (e.g., PatchCore) are sensitive to feature extractors and sampling; specify if you re-trained them or adopted published numbers, and align training budgets if re-running.Image-level score aggregation: you use the mean over the aggregated anomaly map, whereas RD4AD often uses the maximum. Provide an ablation on pooling functions (mean/max/top-k pooling) and post-processing (e.g., Gaussian smoothing) to confirm robustness.\n- Comparison with related work (using the summaries provided)Relative to RD4AD (Deng & Li 2022), your contrastive formulation and SAM are clear extensions; however, explain how your one-class bottleneck and multi-scale fusion relate to OCBE/MFF, and whether your bottleneck mirrors or modifies those modules.RD++ (Tien et al. 2023) uses dual pathways and synthetic anomalies but regularizes reconstruction differently. Your positioning is fair; it would be helpful to include a controlled comparison where both methods share identical backbones, training budgets, and synthesis (simplex) to confirm gains come from the contrastive objective and SAM.DMDD (2024) explicitly decouples normal/abnormal branches and uses foreground-aware synthesis and segmentation heads; while industrial, it is relevant to KD-based AD. Discuss how your simpler loss and SAM compare to dual-branch designs and whether combining SAM with decoupled branches could further improve scale sensitivity.MedIAnomaly (2024) emphasizes evaluation standards; integrating AP/AUCPR and avoiding test-time threshold selection would bring your evaluation in line with those recommendations and strengthen the claims, particularly on ISIC where class semantics can differ broadly.\n- Discussion of broader impact and significanceThe method is computationally efficient and could be practical in clinical pipelines, as suggested by the reported inference speeds. To support deployment, consider evaluating robustness to training-set contamination, scanner/site shifts, and calibration of anomaly scores.Ethical and clinical considerations: emphasize that anomaly detection is an assistive tool, not a diagnostic; discuss potential failure modes (e.g., rare pathologies outside the synthetic distribution), uncertainty quantification, and human-in-the-loop integration. Calibration analysis and case studies would increase clinical relevance.\n\n- The contrastive ratio formulation is intuitive but could be analytically grounded: gradients can explode when sim(z, v) approaches 1; adding only Œµ may not suffice. Consider adding temperature scaling, a margin, or switching to a bounded and well-studied loss (e.g., logistic/InfoNCE: ‚àílog exp(sim(u,v)/œÑ) / [exp(sim(u,v)/œÑ) + exp(sim(z,v)/œÑ)]) for stability and interpretability.\n- Clarify whether ‚Äúreshaped into 1D‚Äù means flattening across spatial and channel dimensions, or only across channels per spatial location. If training is global but inference is local, explain why this mismatch improves performance and whether a local contrastive variant was tried.\n- The SAM design is elegant; however, explain why GMP was preferred over GAP or mixed pooling (GMP is sensitive to outliers). Ablate or analyze alternatives and report the learned Œ± distributions and their correlation with lesion sizes across datasets.\n- Anomaly synthesis via simplex noise is reasonable and supported by ablation. Provide qualitative visualizations of synthetic patches and their frequency spectra to further justify realism. Consider mixing scales or multiple procedural textures to reduce overfitting to one synthesis family.\n\n- Report results over multiple random seeds (mean¬±std). Include AUCPR/AP for imbalanced settings and operating-point metrics like FPR@95%TPR. For image-level scoring, avoid selecting thresholds on the test set; adopt validation-free heuristics or report both ‚Äúoracle‚Äù and ‚Äúpractical‚Äù thresholds.\n- Since anomaly maps are produced, evaluate localization on at least one dataset with pixel-level labels (e.g., BraTS variants) or create proxy masks via expert annotations. Even if the focus is classification, localization is a critical clinical desideratum.\n- The Brain Tumor setup risks domain confounds (normals and tumors from different repositories). Provide harmonization steps, source-balanced splits, or a cross-source control to confirm the model is detecting pathology rather than dataset bias.\n- Fairness of baseline comparisons: clarify whether all baselines used the same backbone/input size and pretraining regime when possible. Some baselines (e.g., PatchCore) are sensitive to feature extractors and sampling; specify if you re-trained them or adopted published numbers, and align training budgets if re-running.\n- Image-level score aggregation: you use the mean over the aggregated anomaly map, whereas RD4AD often uses the maximum. Provide an ablation on pooling functions (mean/max/top-k pooling) and post-processing (e.g., Gaussian smoothing) to confirm robustness.\n\n- Relative to RD4AD (Deng & Li 2022), your contrastive formulation and SAM are clear extensions; however, explain how your one-class bottleneck and multi-scale fusion relate to OCBE/MFF, and whether your bottleneck mirrors or modifies those modules.\n- RD++ (Tien et al. 2023) uses dual pathways and synthetic anomalies but regularizes reconstruction differently. Your positioning is fair; it would be helpful to include a controlled comparison where both methods share identical backbones, training budgets, and synthesis (simplex) to confirm gains come from the contrastive objective and SAM.\n- DMDD (2024) explicitly decouples normal/abnormal branches and uses foreground-aware synthesis and segmentation heads; while industrial, it is relevant to KD-based AD. Discuss how your simpler loss and SAM compare to dual-branch designs and whether combining SAM with decoupled branches could further improve scale sensitivity.\n- MedIAnomaly (2024) emphasizes evaluation standards; integrating AP/AUCPR and avoiding test-time threshold selection would bring your evaluation in line with those recommendations and strengthen the claims, particularly on ISIC where class semantics can differ broadly.\n\n- The method is computationally efficient and could be practical in clinical pipelines, as suggested by the reported inference speeds. To support deployment, consider evaluating robustness to training-set contamination, scanner/site shifts, and calibration of anomaly scores.\n- Ethical and clinical considerations: emphasize that anomaly detection is an assistive tool, not a diagnostic; discuss potential failure modes (e.g., rare pathologies outside the synthetic distribution), uncertainty quantification, and human-in-the-loop integration. Calibration analysis and case studies would increase clinical relevance.\n\n### Questions for Authors\n\n- How exactly is the cosine similarity computed during training: per-spatial-location (channel vectors) or on fully flattened feature maps? If the latter, how do you reconcile this with the spatial anomaly maps used at inference?\n- Why choose the ratio loss rather than a temperature/margin-based contrastive loss (InfoNCE/triplet)? Did you observe instability with your ratio objective (e.g., when sim(z, v) ‚âà 1)? Any gradient clipping or auxiliary regularization used?\n- Can you report mean¬±std over multiple seeds and include AUCPR/AP, FPR@95%TPR, and calibration metrics? Also, how are thresholds chosen in a validation-free manner for F1/ACC in practice?\n- For the Brain Tumor dataset, what steps were taken to mitigate domain shift between normal and abnormal sources? Can you provide a control where normals and abnormals come from the same site/distribution, or show performance with histogram matching/intensity harmonization?\n- Could you share statistics/visualizations of learned Œ± (SAM) across datasets and per sample? Do Œ± distributions correlate with estimated lesion sizes? Have you tried GAP or mixed pooling in SAM?\n- For anomaly synthesis, do you vary patch sizes/shapes beyond uniform sampling up to h/8? Have you tried multi-scale textures, Poisson blending, or learned perturbations to diversify anomalies and reduce overfitting?\n- How were inference time and memory footprints measured across methods (same hardware/batch size/precision)? Were all baselines re-run under the same settings?\n- What is the effect of the image-level pooling choice (mean vs max vs top-k) and any post-processing on anomaly maps? Could you provide an ablation?\n- Have you evaluated pixel-level localization metrics on a dataset with masks (e.g., BraTS variants)? If not, what prevents including this, given your method produces anomaly maps?\n\n### Overall Assessment\n\nThis paper presents a well-motivated and practically useful extension of reverse distillation for medical anomaly detection. The contrastive dual-teacher design and the learned scale adaptation mechanism are conceptually sound and empirically effective across three imaging modalities. The method‚Äôs simplicity and efficiency are attractive for real-world settings. The main areas needing strengthening are the theoretical and empirical grounding of the ratio-based contrastive loss (including stability and alternatives), clarifying the spatial vs. global similarity computations, and broadening/standardizing the evaluation (multiple seeds, AUCPR, localization metrics, thresholding protocols) while controlling for dataset/domain confounds, especially in the Brain Tumor setup. Addressing these points would significantly enhance the paper‚Äôs rigor and increase its suitability for a top-tier venue. Overall, I view the contribution as novel and valuable, with clear potential after methodological clarifications and evaluation refinements.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a *scale-aware contrastive reverse distillation* framework for unsupervised medical anomaly detection. Extending standard reverse distillation (RD), the method employs a dual-teacher configuration (clean and noisy inputs sharing weights), a *contrastive ratio loss* that attracts student representations toward clean-teacher features while repelling them from noisy-teacher (synthetic anomaly) features, and a *Scale Adaptation Module* (SAM) that dynamically reweights multi-scale features for lesion-size variability. Experiments on X-ray (RSNA), MRI (Brain Tumor), and dermoscopy (ISIC 2018) datasets demonstrate state-of-the-art AUCs, with detailed ablations supporting each component. The presentation is clear, visualizations are effective, and the approach is conceptually coherent and computationally efficient.  \n\n**Major Comments**  \n1. **Loss formulation and stability** ‚Äì The proposed ratio loss \\((1‚àísim(u,v))/(1‚àísim(z,v)+Œµ)\\) may exhibit instability when the denominator is small, lacking temperature or margin parameters and comparison to standard contrastive objectives (e.g., InfoNCE or triplet). Analytical justification and experiments on training stability are needed.  \n2. **Inconsistency between training and inference similarity computation** ‚Äì The manuscript reports collapsing features into 1D for training while using per-pixel similarity during inference; this discrepancy requires clarification and potential evaluation of local contrastive alternatives.  \n3. **Synthetic anomaly design** ‚Äì Reliance on simplex noise makes the model potentially overfit to a specific synthetic distribution. Additional tests under different noise regimes or domains would help judge robustness.  \n4. **Experimental rigor** ‚Äì Results are single-run without variance reporting. Metrics dependent on optimal test thresholds (F1/ACC) are inflated; mean¬±std over seeds, AUCPR/AP, and threshold-free evaluations (e.g., fixed FPR) are recommended.  \n5. **Localization and dataset issues** ‚Äì Despite producing spatial anomaly maps, no localization metrics (AUPRC or Dice) are provided. The Brain Tumor dataset combines normal and abnormal images from different sources, introducing domain-shift confounds that require control experiments.  \n6. **Methodological and presentation clarity** ‚Äì Details such as the normalization constant \\(C_k\\), mapping to K scales, and dimensionalities are insufficiently specified. Some figure protocols (e.g., memory chart) need clearer definitions.  \n7. **Comparative context** ‚Äì Missing discussion of recent knowledge-distillation works (e.g., DMDD 2024) and benchmarks (MedIAnomaly 2024) prevents full contextualization. Controlled comparisons with RD++, DMDD, or similar methods sharing backbones and resources would clarify contributions.  \n\n**Minor Comments**  \n- Clarify whether SAM uses global or channel pooling and justify the choice of GMP over GAP; report distributions of learned weights and their correlation with lesion size.  \n- Provide visualizations of synthetic anomalies and frequency characteristics.  \n- Verify fairness of baseline settings (backbone, input size, pretraining) and describe hardware consistency for runtime comparisons.  \n- Add ablations on image-level pooling choices (mean vs. max vs. top-k) and evaluation post-processing.  \n\n**Summary Paragraph**  \nOverall, the paper makes a technically novel and practically meaningful contribution by combining contrastive distillation and scale adaptation for anomaly detection across multiple medical modalities. The work is well-motivated and experimentally comprehensive, with clear gains over baselines. However, its theoretical justification for the custom ratio loss, incomplete reporting practices (variance, threshold independence, localization), and limited discussion of domain and comparative contexts weaken the empirical rigor. Addressing stability analysis, expanding evaluation metrics, controlling dataset biases, and integrating newer comparative baselines would solidify the contribution and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The manuscript presents promising and innovative ideas but requires methodological clarification, additional analyses, and improved evaluation rigor before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for IEEE Transactions on Medical Imaging\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical challenge of unsupervised medical anomaly detection where labeled abnormal samples are scarce. The authors propose a scale-aware contrastive reverse distillation framework that introduces two key innovations: 1) a contrastive student-teacher learning approach that leverages synthetically generated abnormal samples to enhance feature discriminability, and 2) a scale adaptation mechanism that dynamically weights contrastive losses across different feature scales to handle the scale variation of anomalies. The framework demonstrates state-of-the-art performance across three diverse medical imaging datasets (RSNA chest X-rays, Brain Tumor MRI, and ISIC dermoscopy), with improvements of 3.01%, 4.38%, and 3.08% in AUC over the second-best methods on the respective datasets. The approach also shows computational efficiency with competitive inference times, achieving the highest AUROC while being significantly faster than competing methods like PatchCore (6x faster) and GAN Ensemble (4x faster).\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The contrastive reverse distillation paradigm is well-designed and addresses a genuine limitation in existing reverse distillation methods by exploring out-of-distribution representations, which is evident in the improved separation of normal and abnormal distributions shown in Figure 2.\n- The scale adaptation mechanism is a thoughtful addition that addresses the scale variation problem in medical anomalies, with the ablation study (Table 2) demonstrating its particular importance for datasets with highly variable anomaly sizes like Brain Tumor MRI.\n- The experimental validation is comprehensive across three diverse medical imaging datasets, showing consistent performance gains over 16 state-of-the-art methods, which strengthens the generalizability of the approach.\n- The computational efficiency analysis (Figure 4) is particularly valuable for clinical deployment considerations, demonstrating that the method achieves the highest AUROC while maintaining competitive inference times.\n\n**Limitations:**\n- The paper doesn't sufficiently justify why the contrastive loss formulation (Eq. 2) is superior to more standard contrastive learning approaches. The division-based formulation is unusual and lacks theoretical grounding or comparison to alternative contrastive learning implementations that might achieve similar results.\n- While the authors demonstrate that simplex noise outperforms Gaussian noise (Table 3), they don't provide a deeper theoretical or empirical justification for why simplex noise better mimics medical anomalies beyond the qualitative statement that it \"produces smooth, structured randomness.\" A more detailed analysis of noise characteristics would strengthen this key design choice.\n- The synthetic anomaly generation method uses arbitrary uniform sampling of noise position/size without considering anatomical structures or regions more likely to contain abnormalities, which may limit the clinical relevance of the synthesized anomalies. The paper would benefit from a more principled approach to anomaly placement.\n- The paper lacks failure case analysis or discussion of limitations when faced with certain types of anomalies (e.g., diffuse abnormalities, those with appearance similar to normal tissue).\n\n### Minor Comments\n- The description of the contrastive loss could be clearer regarding implementation details such as batch size considerations and how positive/negative pairs are constructed.\n- The ROC curve figures (Figures 5-7) would benefit from confidence intervals to better assess the statistical significance of the performance differences.\n- The comparison with ReContrast (Table 1) shows only marginal differences on some datasets; the paper could more clearly articulate the specific scenarios where the scale adaptation mechanism provides decisive advantages.\n- The bottleneck architecture (Figure 1) could benefit from more detailed description in the methodology section to clarify how multi-scale features are fused.\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a clinically important problem - unsupervised medical anomaly detection where labeled abnormal data is scarce. The proposed method demonstrates substantial improvements over existing approaches across multiple medical imaging modalities, suggesting potential clinical impact. The computational efficiency is particularly significant for real-world deployment in medical settings where processing speed matters.\n\n**Innovation:** The paper presents two key innovations - the contrastive reverse distillation framework that leverages synthetic anomalies to improve discrimination, and the scale adaptation mechanism for handling scale variation in anomalies. These address specific limitations in prior work and represent meaningful advances to the reverse distillation paradigm. The approach differs fundamentally from RD++ by using contrastive regularization to enhance feature discrimination rather than restricting anomalous information flow.\n\n**Evaluation:** The evaluation is thorough, with testing across three diverse medical imaging datasets, comprehensive ablation studies, and comparison to state-of-the-art methods. The paper also provides detailed analysis of the impact of noise type and intensity. However, the statistical significance of improvements could be more rigorously established (e.g., through statistical tests), and failure case analysis is limited.\n\n**Reproducibility:** The paper provides sufficient implementation details including network architecture, training parameters, and code availability. The appendix includes algorithms for data synthesis and training. However, more specific details about the scale adaptation module implementation and exact noise generation parameters would further enhance reproducibility.\n\n## 4. Decision Recommendation\n\nMinor Revision\n\nThe paper presents a solid contribution to medical anomaly detection with clear innovations and strong empirical results across multiple medical imaging modalities. The main limitations that require addressing are: 1) providing more theoretical justification for the contrastive loss formulation and noise selection, 2) improving the synthetic anomaly generation methodology with anatomical considerations, and 3) adding failure case analysis. With these revisions, the paper would make a valuable contribution to TMI. The authors should also consider adding statistical significance testing to their performance comparisons and clarifying the implementation details of the scale adaptation mechanism.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of unsupervised medical anomaly detection in scenarios where labeled abnormal data are scarce. It introduces a scale-aware contrastive reverse distillation framework featuring two primary innovations: (1) a contrastive student‚Äìteacher learning approach that uses synthetically generated anomalies to improve feature discriminability, and (2) a scale adaptation mechanism that dynamically reweights contrastive losses across multiple feature scales to account for variation in anomaly sizes. Experiments on chest X-rays, brain tumor MRIs, and dermoscopy images demonstrate consistent improvements over prior methods in AUC as well as substantial computational efficiency. Overall, the manuscript is clearly written, methodologically sound, and potentially impactful in clinical contexts where inference speed is crucial.  \n\n**Major Comments**  \n1. **Conceptual Strengths:** The proposed reverse distillation paradigm effectively enhances anomaly detection by exploring out-of-distribution representations, supported by clear empirical separation between normal and abnormal features. The scale adaptation mechanism is a meaningful addition, with ablation studies confirming its importance for datasets with varied anomaly sizes. Evaluation across three diverse modalities and 16 baselines reinforces both robustness and generalizability, while analysis of inference speed underscores practical utility.  \n2. **Conceptual Limitations:** The manuscript does not sufficiently justify the contrastive loss formulation; its division-based design is unconventional and lacks theoretical or comparative substantiation. Discussion of why simplex noise outperforms Gaussian noise remains qualitative, without analysis of underlying noise characteristics. Additionally, synthetic anomaly generation via random spatial sampling overlooks anatomical plausibility, limiting clinical realism. Finally, failure cases‚Äîparticularly diffuse or low-contrast anomalies‚Äîare not examined, leaving potential failure modes unexplored.  \n\n**Minor Comments**  \n- Clarify construction of positive/negative pairs and influence of batch size in the contrastive loss implementation.  \n- Include confidence intervals for ROC curves to support claims of statistical significance.  \n- Emphasize conditions under which the scale adaptation mechanism yields decisive improvements beyond marginal performance gains.  \n- Expand the methodological description of the bottleneck architecture to explain feature fusion across scales.  \n\n**Summary Paragraph**  \nThe study presents a significant advancement in unsupervised medical anomaly detection through an innovative combination of contrastive reverse distillation and scale adaptation. Its thorough evaluation and demonstrated efficiency highlight strong applicability, though the theoretical justification of design choices and robustness analysis could be stronger. Enhancing the discussion of loss formulation, noise modeling, and failure cases would address the main methodological gaps while maintaining the paper‚Äôs high overall quality and relevance.  \n\n**Decision Recommendation**  \n**Minor Revision.** The manuscript makes an important and well-substantiated contribution, needing clarification and additional theoretical and analytical depth before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe authors tackle unsupervised anomaly detection in medical imaging, motivated by the paucity of labeled abnormal cases. Their experiments span three publicly available datasets: chest X‚Äërays, brain MRI slices, and dermoscopic photographs. They propose a ‚Äúscale‚Äëaware contrastive reverse distillation‚Äù framework that consists of three components: (i) a clean teacher encoder, (ii) a noisy teacher encoder that receives synthetically perturbed images generated with simplex noise, and (iii) a student decoder that reconstructs multi‚Äëscale feature maps. A scale‚Äëadaptation module produces input‚Äëspecific weights that modulate the contrastive loss across encoder layers. At test time, cosine similarity between teacher and student features is weighted by the learned scale descriptors to yield an anomaly map and a global anomaly score. The reported results are AUC‚ÄØ=‚ÄØ91.01‚ÄØ% (X‚Äëray), 98.88‚ÄØ% (MRI), and 83.10‚ÄØ% (dermoscopy), together with corresponding F1 and accuracy figures. The claimed novelty lies in merging contrastive reverse‚Äëdistillation with a learnable, scale‚Äëaware weighting mechanism to handle size variability of anomalies.\n\n---\n\n## General feedback  \n\n- **Significance.** Detecting anomalies without supervision is indeed a pressing problem in medical imaging where annotated pathology is scarce.  \n- **Innovation.** The manuscript introduces two ideas that are not present in earlier reverse‚Äëdistillation works (e.g., RD4AD, RD++): a contrastive reverse‚Äëdistillation objective that incorporates a noisy teacher, and a data‚Äëdriven scale‚Äëweighting scheme. These concepts also differentiate the method from recent contrastive reconstruction approaches (ReContrast) and synthetic‚Äëanomaly generators (NSA, CutPaste).  \n- **Evaluation.** The authors present quantitative results on three modalities (Table‚ÄØ1, ROC curves in Figs‚ÄØ5‚Äë7) and provide ablation experiments that suggest each component contributes to performance (Table‚ÄØ2). However, the study lacks statistical significance testing, confidence intervals, and an analysis of failure cases. Moreover, the experimental protocol is based on a single train/validation split for each dataset, which raises concerns about the robustness of the reported numbers.  \n- **Reproducibility.** A public GitHub repository and a description of the training setup (backbone, optimizer, 4‚ÄØk iterations, batch size‚ÄØ16) are supplied (Section‚ÄØ3.1.3). Critical hyper‚Äëparameters‚Äîsuch as the noise intensity Œª, the simplex‚Äënoise octaves, and the architecture of the scale‚Äëadaptation module‚Äîare insufficiently detailed, and the provided pseudo‚Äëcode does not address random‚Äëseed handling.\n\n---\n\n## Specific comments/critiques  \n\n- **Loss formulation clarity.** Equation‚ÄØ(2) combines two similarity terms but does not include a margin, temperature scaling, or explicit normalization. The only justification for Œµ is to avoid division‚Äëby‚Äëzero, leaving the overall scaling of the loss ambiguous.  \n- **Scale‚Äëadaptation module details.** The description (‚Äúglobal max pooling ‚Üí linear layer ‚Üí softmax‚Äù) is too brief. The dimensionality of the bottleneck output, the number of scales‚ÄØK, and the gradient flow through the linear layer are not reported, making it difficult to assess the module‚Äôs capacity and stability.  \n- **Synthetic anomaly realism & parameter specification.** While simplex noise is presented as the sole perturbation method, no visual examples of the generated pseudo‚Äëanomalies are included. The manuscript omits the exact ranges for noise size/position ([a,‚ÄØb] bounds), the number of octaves, the persistence Œ≥, and the chosen intensity Œª‚ÄØ=‚ÄØ0.2, all of which are necessary for reproduction.  \n- **Dataset split reproducibility.** The paper does not fully describe whether patient‚Äëlevel or study‚Äëlevel splits were used for RSNA, Brain‚ÄëTumor, and ISIC. Without explicit leakage prevention measures, the reported performance may be over‚Äëoptimistic.  \n- **Statistical analysis.** Gains such as ‚Äú+3.01‚ÄØ% AUC on RSNA‚Äù are presented without confidence intervals or p‚Äëvalues, so it is unclear whether these improvements are statistically meaningful.  \n- **Ablation granularity.** Table‚ÄØ2 contrasts the full model with versions lacking either the contrastive reverse distillation (CRD) or the scale‚Äëadaptation module (SAM). Intermediate configurations (e.g., noisy teacher without contrastive loss, or contrastive loss without scale weighting) are absent, limiting insight into the independent contribution of each component.  \n- **Computational cost reporting.** Figure‚ÄØ4 shows inference time and memory usage qualitatively, but the manuscript does not provide absolute training time, GPU memory consumption during training, or FLOP counts. The claim of being ‚Äú6√ó faster than PatchCore‚Äù lacks hardware specifications and benchmark details.  \n- **Baseline fairness.** For several baselines (NSA, CutPaste, ReContrast) the authors appear to have used default hyper‚Äëparameters without any dataset‚Äëspecific tuning, which may compromise the fairness of the comparison.  \n- **Clinical relevance & visualisation.** Apart from a histogram in Figure‚ÄØ2, the paper does not present localized anomaly maps or qualitative assessments by clinical experts. Such visual evidence would be essential to substantiate the claimed clinical applicability.  \n- **Typographical and formatting errors.** The title contains broken words (‚ÄúCONTRASTIVE REVERSE DISTILLA##‚ÄØTION‚Äù, ‚ÄúDE‚Äë‚ÄØTECTION‚Äù), and numerous formatting issues are scattered throughout the manuscript, detracting from overall readability.\n\n---\n\n## A suggested decision  \n\n**Major Revision**  \n\n*Rationale:* The manuscript presents an interesting idea but falls short in several critical aspects: baseline comparisons are not sufficiently controlled, statistical analyses are absent, and many implementation details needed for reproducibility are missing. These deficiencies must be addressed before the work can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses unsupervised anomaly detection in medical imaging, motivated by the scarcity of labeled abnormal data. It introduces a ‚Äúscale‚Äëaware contrastive reverse distillation‚Äù framework composed of a clean teacher encoder, a noisy teacher encoder perturbed with simplex noise, and a student decoder reconstructing multi‚Äëscale features. A learnable scale‚Äëadaptation module assigns input‚Äëspecific weights to modulate contrastive losses, producing weighted anomaly maps and global anomaly scores. Experiments on chest X‚Äëray, brain MRI, and dermoscopy datasets demonstrate competitive AUCs. The claimed contribution lies in integrating contrastive reverse‚Äëdistillation with a data‚Äëdriven scale‚Äëweighting mechanism to handle anomaly size variability.\n\n---\n\n**Major Comments**  \n1. **Significance and novelty:** The problem is timely and relevant. Two novel aspects‚Äîcontrastive reverse distillation with a noisy teacher and a learnable scale‚Äëweighting scheme‚Äîdifferentiate the work from prior RD4AD, RD++, ReContrast, and CutPaste methods.  \n2. **Experimental evaluation:** Results on three modalities are promising and supported by ablation studies. However, the evaluation lacks statistical significance testing, confidence intervals, and an analysis of failure cases. Only a single train/validation split per dataset is used, raising concerns about robustness.  \n3. **Reproducibility:** Although code and training details are provided, essential hyper‚Äëparameters (e.g., noise intensity Œª, simplex‚Äënoise octaves, parameters of the scale‚Äëadaptation module) remain unspecified. Random‚Äëseed management is also not described.  \n4. **Loss formulation:** Equation‚ÄØ(2) is unclear regarding normalization, margin, and temperature scaling; the role of Œµ is insufficiently justified.  \n5. **Scale‚Äëadaptation module:** Description lacks dimensional specifications, number of scales‚ÄØK, and gradient flow details, limiting assessment of model capacity and stability.  \n6. **Synthetic anomaly realism:** No visual examples or parameter ranges for simplex noise are given, hindering reproducibility.  \n7. **Dataset splitting:** The paper does not clarify whether splits are patient‚Äëlevel or study‚Äëlevel, risking data leakage.  \n8. **Statistical support:** Reported AUC gains are not accompanied by confidence intervals or p‚Äëvalues.  \n9. **Ablation completeness:** Missing intermediate configurations prevent clear attribution of improvements.  \n10. **Computational cost:** Claims of speed and efficiency are not supported with quantitative training or inference metrics or hardware details.  \n11. **Baseline fairness:** Some baselines appear untuned, potentially biasing comparisons.  \n12. **Clinical validation:** Absence of localized anomaly maps or expert visual assessments limits evidence for clinical utility.\n\n---\n\n**Minor Comments**  \n- The title and text contain typographical and formatting issues (‚ÄúCONTRASTIVE REVERSE DISTILLA##‚ÄØTION,‚Äù ‚ÄúDE‚Äë‚ÄØTECTION‚Äù).  \n- Several figures and tables could benefit from clearer labels and descriptions for readability.\n\n---\n\n**Summary Paragraph**  \nThe manuscript offers an original framework for unsupervised anomaly detection by combining scale‚Äëaware weighting with contrastive reverse distillation. While the methodological idea is innovative and results appear strong, the study suffers from incomplete experimental validation, limited statistical rigor, unclear module specifications, and insufficient reproducibility details. The absence of comprehensive comparisons and clarity on dataset splits further weakens the reliability of conclusions. Addressing these issues would substantially strengthen the work.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The approach is promising but requires more rigorous evaluation, clearer methodological disclosure, and stronger reproducibility evidence before it can be recommended for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chunlei Li",
      "Jingliang Hu",
      "Lichao Mou",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_8f2f06936cc0020aac0ce13e60997a27679bffa6.pdf",
    "remote_url": "https://openreview.net/pdf/8f2f06936cc0020aac0ce13e60997a27679bffa6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning General-purpose Biomedical Volume Representations using Randomized Synthesis",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "synthetic data",
      "representation learning",
      "medical image analysis",
      "image registration",
      "image segmentation"
    ],
    "abstract": "Current volumetric biomedical foundation models struggle to generalize as public 3D datasets are small and do not cover the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. We address this by creating a representation learning method that instead anticipates strong domain shifts at training time itself. We first propose a data engine that synthesizes highly variable training samples that would enable generalization to new biomedical contexts. To then train a single 3D network for any voxel-level task, we develop a contrastive learning method that pretrains the network to be stable against nuisance imaging variation simulated by the data engine, a key inductive bias for generalization. This network's features can be used as robust representations of input images for downstream tasks and its weights provide a strong, _dataset-agnostic_ initialization for finetuning on new datasets. As a result, we set new standards across _both_ multimodality registration and few-shot segmentation, a first for any 3D biomedical vision model, all without (pre-)training on any existing dataset of real images.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes training a backbone that generalizes across different datasets using synthetically generated dataset. The proposed pre-training strategy has 3 main steps: 1) Given a large datasets of 104 annotated organs, randomly sample anatomies, deform them and create a volume by ensembling these anatomies, 2) add noise and other augmentations to the volumes that are sampled in the previous step to simulate realistic looking synthetic medical images from labels, 3) train a U-Net with a contrastive objective by sampling two volumes that share the same 3D semantic layout but differ in appearance, treating corresponding features at different encoder levels as positives and all others as negatives. The pre-trained backbone is validated on two different tasks: 3D registration and 3D few-shot segmentation; using multiple datasets.  The results show the effectiveness of the proposed backbone in the experiments compared to existing methods.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- Foundational models are showing promising performance lately; however, we lack of a 3D model that work across different modalities in medical imaging. The paper proposes a solution to this important problem using domain randomisation and contrastive learning.\n- The paper contain experiments on multiple datasets both for registration and few shot segmentation, and the results demonstrate the potential of the method.\n- The idea of combining the ideas of domain randomisation and local contrastive learning to train a generic 3D backbone is quite interesting and, to my knowledge, is novel.\n\n### Weaknesses\n\n- One issue I see in the paper is the convoluted description of the data engine step, especially the part creating the label ensemble model in section 3. I understand that this step is mainly based on the domain randomisation idea proposed in the literature. However, it is not really clear to me the steps between 201-207, especially the parts multiplying the masks with a randomly deformed sphere and randomly encasing half of the foreground-masked volumes.\n\n- The images generated in the data engine step do not seem like as real medical images. Do they look like this because the deformation is too large? It is not clear why one would prefer training the model using such unrealistic images.\n\n- The paper does not discuss the recent foundational models that show better generalization performance on many medical image datasets [1]. The downstream task performance of the representations obtained from the proposed backbone should be compared with the those obtained by the representations of a foundational model (e.g. DinoV2 [2]). For example, [3] is a recent paper that uses DinoV2 features for registration; but the same applies for the segmentation experiments. One can use the DinoV2 features for segmentation.\n\n[1] Cekmeceli et al. \"Do Vision Foundation Models Enhance Domain Generalization in Medical Image Segmentation?\"\n[2] Oquab et al. \"DINOv2: Learning Robust Visual Features without Supervision\"\n[3] Song et al. \"DINO-Reg: General Purpose Image Encoder for Training-Free Multi-modal Deformable Medical Image Registration\"\n\n### Questions\n\n1- What are the exact steps of the \"label ensemble model\" described in Section 3? Please write elaborate description of these steps.\n2- Why do the generated images not look like real medical images? Is it because the deformation is too large? Why such \"unrealistic\" looking images are preferred rather than more realistic ones obtained with smaller deformation?\n3- How does the quality of the representations obtained by the proposed backbone compares with SoTA foundational models such as DinoV2 or SAM2?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a method for pre-training a 3D backbone that generalizes across multiple medical imaging datasets using synthetically generated data. The approach involves domain randomization and a contrastive learning objective applied to 3D U-Nets. Specifically, the pipeline builds synthetic volumes from deformed annotated anatomies, augments them with noise to simulate realistic appearances, and trains the model by comparing features that share underlying 3D structure but differ in appearance. The pre-trained backbone is evaluated on two downstream tasks‚Äî3D registration and few-shot segmentation‚Äîacross several datasets, demonstrating improved performance relative to existing techniques. Overall, the paper is clearly written and addresses an important gap in 3D medical image representation learning.  \n\n**Major Comments**  \n1. **Clarity of Method Description**: The description of the ‚Äúdata engine‚Äù and ‚Äúlabel ensemble model‚Äù in Section 3 is difficult to follow, particularly steps 201‚Äì207 involving the deformation and masking procedures. The process of constructing the composite label volumes and the rationale behind these transformations require clearer explanation.  \n2. **Realism of Synthetic Data**: The generated images in the data engine step appear unrealistic, possibly due to excessive deformation or augmentation. The authors should comment on why such unrealistic images are beneficial and whether smaller or more controlled deformations would yield better generalization.  \n3. **Comparison to Recent Foundational Models**: The paper does not engage with recent advances in vision foundation models (e.g., DINOv2, SAM2) that show strong cross-domain performance. It would strengthen the paper to compare representations from the proposed model with those from such pre-trained backbones, especially in registration and segmentation tasks.  \n\n**Minor Comments**  \n- Improve clarity and flow in the description of the label ensemble generation process.  \n- Provide more visual examples of synthetic outputs to illustrate the data generation quality.  \n- Ensure consistent citation formatting for referenced works [1‚Äì3].  \n\n**Summary Paragraph**  \nThe study tackles a relevant and challenging problem‚Äîbuilding a generalizable 3D medical imaging backbone‚Äîand offers a creative combination of domain randomization and contrastive learning. The manuscript is generally sound and experimentally supported, though methodological clarity and contextualization against recent foundational models require improvement. Enhancing descriptions of the data generation process and including comparative baselines would substantially strengthen the work.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The concept is promising and results are encouraging, but key methodological clarifications and additional comparisons are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work proposes a pre-training approach for downstream tasks related to fine-grained volumetric medical data: image registration and semantic segmentation. The authors propose to learn appearance invariance and shapes of human anatomy through synthetic dense pixel volumes. In this process, 3D volumes are synthesized by randomly recomposing 3D anatomical shapes and assigning multiple sets of random pixel values, in together with synthetic noises and deformations. Pairs of synthetic volumes are used for multi-scale contrastive learning. The proposed approach demonstrates improved image registration and low-shot image segmentation results compared to some previous works. Detailed ablation studies on the pre-training configurations toward downstream performances are conducted.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe overall methodology is straightforward and easy to understand. It echoes with the classical computer vision concept of invariance learning in the deep neural network era (although learned through a data-driven approach).\n\nImproved image registration results on two public image registration benchmarks and image segmentation performance on six image segmentation datasets are shown, compared to those of some existing works. \n\nThe paper is well-written with sufficient clarity. The illustrations are self-explanatory. Readers will enjoy reading it.\n\n### Weaknesses\n\nTechnical novelty: The core idea behind the approach is to leverage data-driven approach to learn invariance to pixel values through paired synthetic shapes and different pixel values, and to learn semantic-independent shape representation through random geometric (real-world and pure synthetic) shapes ‚Äì both key ideas come from the well-established SynthMorph (random Ising shapes with random intensity + synthetic deformation for training registration networks. Hoffmann et al.) and SynthSeg (GMM-like pixel value model for repainting brain structures. Billot et al.) Despite leveraging more anatomical shapes beyond brain structures and applied to a contrastive framework, the essence remains unchanged.  \n\nMany medical decisions are made not only on shape but also on subtle textures, for example, differentiating subtypes of tumors/lesions ‚Äì toward which the proposed over-simplified appearance model by nature falls short. More sophisticated texture models need to be carefully studied beyond this manuscript.\n\nFor the same reason, high-level global semantic information such as relative locations between anatomical structures cannot be learned due to the nature of this approach. \n\nReal-world value: Given the increasing number of large-scale publicly accessible volumetric image datasets such as CT-RATE (Hamamci et al.), Totalsegmenter (Wasserthal et al.), and AbdomenAtlas (Li et al.), and the derived 3D foundation models, the real-world application of the proposed framework is unclear. Some of these large-scale public datasets come with fine-grain pixel-wise labels and associated radiological reports which provide additional supervision signals and text alignment potentials. The claimed generalization capability can be learned from multi-site large real-world datasets as well, through the intrinsic heterogeneity of big data and possibly through intense data augmentation.\n\n### Questions\n\nThe proposed workflow involves many hyper-parameters (Figure 12) controlling the properties of generated synthetic volumes -- what is the rule of thumb for choosing them?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a pre-training framework for downstream medical image analysis tasks, focusing on fine-grained volumetric data such as image registration and semantic segmentation. The method synthesizes 3D anatomical volumes by recombining shapes and assigning randomized intensities, deformations, and noise, enabling multi-scale contrastive learning to capture appearance invariance and shape representation. Experiments demonstrate improved results on registration and few-shot segmentation tasks compared to several existing methods. The paper is clearly written and well-illustrated, with a coherent presentation of goals, methods, and results.\n\n**Major Comments**  \n1. **Technical novelty**: The approach closely follows principles established by prior works such as SynthMorph and SynthSeg, which also learn shape- and appearance-invariant representations using synthetic data with randomized intensity and shape perturbations. Although the present framework extends these ideas to broader anatomical structures and a contrastive learning setting, the underlying concepts remain largely similar to those established models.  \n2. **Modeling limitations**: The proposed method primarily captures shape information, overlooking texture-level cues that are often critical in medical decision-making (e.g., differentiating lesion subtypes). The simplified appearance model limits the capacity to represent subtle texture variations.  \n3. **Representation limits**: Due to its design, the model cannot effectively capture higher-level spatial or semantic relationships between anatomical structures, constraining its ability to learn global context.  \n4. **Practical relevance**: With the increasing availability of large-scale labeled volumetric datasets (e.g., CT-RATE, TotalSegmenter, AbdomenAtlas) and emerging 3D foundation models, the real-world utility of this synthetic data‚Äìdriven approach is uncertain. Comparable generalization might be achievable using diverse real-world datasets with rich annotations and augmentation strategies.  \n5. **Hyperparameter selection**: The method involves numerous hyperparameters controlling the synthetic generation process. The paper would benefit from guidance or heuristics on choosing appropriate values.\n\n**Minor Comments**  \n- Figures are well-designed and self-explanatory.  \n- No ethical concerns were identified.\n\n**Summary Paragraph**  \nOverall, the work is clearly presented and conceptually coherent, offering a systematic exploration of pre-training via synthetic volumetric data. Its strengths lie in clarity, sound evaluation, and comprehensive ablation studies. However, the novelty over prior synthesis-based methods is moderate, and limitations in texture modeling and real-world applicability reduce its potential practical impact.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper presents interesting empirical results but requires stronger justification of novelty, discussion of applicability to real data, and clearer guidance on hyperparameter settings.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nAuthors present a method to generate highly variable synthetic imaging data which is then used to pre-train a 3D network using contrastive learning. The data generation method consists of drawing samples from a semantically labelled repository of biomedical shape templates to randomly populate an empty 3D volume. The volume is then deformed. Empty space and organ 'envelopes' are also simulated. To simulate different modalities and protocols, random intensity transformation is applied to the deformed 3D volume to yield 2 images. Typical imaging artifacts such a sbias field and blurring are simulated through random augmentations.The two images are fed into a UNet, and contrastive pre-training is performed on features at each decoder layer. An anchor point is chosen in one of the images, and all voxels of that label in both images are considered positive, and everything else negative pairs. The network yields features that can be used to finetune on other modalities and tasks. Importantly, the representations are modality-agnostic and anatomy-agnostic.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n* The paper is very well written - it lays out the prior work and puts the contirbute in context.\n* The approach yields representations that are both modality-agnostic and task-agnostic while removing the need for dataset-specific and anatomy-specific pre-training.\n* Authors present results of several downstream tasks using their features including multi-modality registration image registration and few-shot segmentation on which their method outperform the others compared.\n* Authors perform ablation studies on the various components of their pipeilne.\n* The Authors present extensive visualization and quantitative results in their main text, and supplementary material. Algorithms and parameters are clearly presented too which allows for further scrutiny and improved reproducability.\n* Authors are aware of the limitations of their approach and include these in the paper.\n\n### Weaknesses\n\n* The segmentation task performed using the Authors features may yield better results than the other methods that are compared, however the result still misses significant portions of the anatomical regions they aim to segment. The features require further adjustment and extensive fine-tuning to be useful in diagnosis and treatment.\n\n### Questions\n\n* Authors compare their randomized shape template-based synthetic data engine to one that uses data with no biomedical priors and one using brain regions. Can Authors elaborate more on the intuiton for why their randomly deformed template shapes are so effective? Is there some point at which the extent of the deformation causes the representations to be less useful?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a method for generating highly variable synthetic 3D biomedical imaging data used to pre-train a network via contrastive learning. The synthetic data are created by sampling and randomly deforming semantically labeled shape templates to populate a 3D volume, with additional steps simulating empty space, organ envelopes, and modality-specific intensity transformations. The resulting paired images are used in a UNet-based framework where voxel-level contrastive pre-training produces modality- and anatomy-agnostic representations. Overall, the paper is clearly written, methodologically detailed, and well-situated within prior work.\n\n**Major Comments**  \n1. **Performance and practical utility:** While the proposed features outperform comparison methods on several downstream tasks (e.g., multi-modality registration, few-shot segmentation), segmentation outputs still miss substantial portions of the intended anatomical structures. This limitation suggests that additional refinement and fine-tuning may be required before the method is suitable for diagnostic or therapeutic contexts.  \n2. **Theoretical explanation of deformation effects:** The reviewer seeks clarification on the intuition behind why randomly deformed shape templates are so effective in generating useful representations. It is also unclear whether extreme deformations degrade representation quality or whether there is an optimal range that maintains anatomical plausibility and learning utility.  \n\n**Minor Comments**  \n- The work is well organized with clear algorithms, parameters, and extensive visual and quantitative results that support reproducibility.  \n- The authors explicitly discuss the limitations of their approach, which contributes to transparency and a balanced presentation.  \n- Minor typographical errors are present (e.g., ‚Äúsbias‚Äù instead of ‚Äúbias,‚Äù ‚Äúpipeilne‚Äù instead of ‚Äúpipeline‚Äù) and should be corrected for final submission.  \n- Ethical review is not required for this study as stated by the reviewer.\n\n**Summary Paragraph**  \nThis paper offers a structured and clearly presented framework for modality-agnostic representation learning from synthetic biomedical images. Strengths include clarity of exposition, methodological transparency, comprehensive evaluation, and strong attention to reproducibility. However, the main limitation lies in incomplete segmentation performance and limited discussion on deformation limits and representational stability. Overall, the contribution is sound and promising, though certain results warrant further refinement to improve applicability to real-world tasks.\n\n**Decision Recommendation**: **Minor Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1: Analytical and‚ÄØScope‚ÄØAudit**\n\n1. **Scope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI‚ÄØ‚úÖ**  \n   The manuscript presents a methodological contribution directly aligned with TMI‚Äôs remit. It proposes a new framework for volumetric biomedical representation learning that uses a synthetic‚Äêdata engine and contrastive pretraining to produce general‚Äëpurpose 3‚ÄëD features applicable to registration and segmentation. The emphasis is on algorithmic design, theory, and validation‚Äînot on a single clinical application.\n\n2. **Novelty‚ÄØ&‚ÄØContribution‚ÄØLevel‚ÄØ‚úÖ**  \n   The work introduces a distinctive combination of (a)‚ÄØdomain‚Äërandomized volumetric label synthesis informed by biomedical priors, and‚ÄØ(b)‚ÄØa voxel‚Äëwise multi‚Äëpositive contrastive objective that leverages perfect label supervision from those synthetic maps. The claim that the model generalizes across modalities and tasks without seeing real data is novel relative to current 3‚ÄëD medical‚Äëfoundation models, which are mostly trained on pooled real datasets.\n\n3. **Technical‚ÄØand‚ÄØExperimental‚ÄØRigor‚ÄØ‚úÖ/‚ö†Ô∏è**  \n   Implementation details are extensive (data‚Äëgeneration pseudocode, UNet configuration, ablation tables). Experiments span multiple benchmarks and tasks with quantified uncertainty. However, (i)‚ÄØstatistical significance tests beyond Dice/SDs are not shown, (ii)‚ÄØgeneration pipeline realism or possible bias is not formally analyzed, and (iii)‚ÄØresults rely primarily on two downstream tasks. Overall rigor is high but could be strengthened by clearer validation of the synthetic‚Äëto‚Äëreal domain gap.\n\n4. **Clarity‚ÄØand‚ÄØPresentation‚ÄØ‚úÖ**  \n   The paper is well‚Äëstructured, grammatically sound, and rich in figures. Given length and appendix volume, a condensed graphical overview of the pipeline and main results would improve readability.\n\n5. **Ethical‚ÄØ&‚ÄØReproducibility‚ÄØCompliance‚ÄØ‚úÖ**  \n   All used public datasets are cited with appropriate licenses; synthetic generation avoids patient data. The authors state code and pretrained weights will be released. The in‚Äëhouse dataset is anonymized and IRB‚Äëapproved.\n\n---\n\n**Phase‚ÄØ2:‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n### 1.‚ÄØSummary\nThe authors propose **a synthetic‚Äëdata‚Äìdriven framework for general‚Äëpurpose 3‚ÄëD biomedical representation learning**.  \nA ‚Äúdata engine‚Äô‚Äô first composes label ensembles from randomly deformed anatomical templates and assigns diverse textures via randomized Gaussian‚Äëmixture intensity models and physics‚Äëbased augmentations.  \nA 3‚ÄëD‚ÄØUNet is then pretrained using a supervised contrastive loss at voxel level so that features from the same label remain close despite large appearance variation.  \nWithout real‚Äëimage pretraining, the resulting model improves over state‚Äëof‚Äëthe‚Äëart methods on **unsupervised multimodality registration** and **few‚Äëshot segmentation** across MRI and‚ÄØCT datasets.\n\n### 2.‚ÄØStrengths\n- Addresses a long‚Äëstanding limitation of 3‚ÄëD foundation models‚Äîthe paucity of large, diverse volumes.  \n- Elegant integration of domain randomization and contrastive learning; full algorithmic transparency.  \n- Demonstrates genuine multi‚Äëtask generalization (registration‚ÄØ+‚ÄØsegmentation).  \n- Extensive ablation and comparison to leading baselines (SynthMorph,‚ÄØuniGradICON,‚ÄØSMIT,‚ÄØPrimGeoSeg).  \n- Fully reproducible with planned code release, aligning with TMI‚Äôs open‚Äëscience priorities.\n\n### 3.‚ÄØWeaknesses\n- The ‚Äúsynthetic‚Äëto‚Äëreal‚Äô‚Äô transfer remains empirically, not theoretically, justified; lacks quantitative diversity metrics or domain‚Äëgap analysis.  \n- Validation scope limited to registration‚ÄØ/‚ÄØsegmentation; testing on additional volumetric tasks (e.g., detection, quantitative mapping) would strengthen generality claims.  \n- Some performance gains (few‚Äëshot segmentation) are modest and may depend on heavy fine‚Äëtuning.  \n- Discussion of computational cost and training efficiency is minimal.  \n- Presentation could be condensed to improve accessibility.\n\n### 4.‚ÄØMajor‚ÄØComments\n1. **Quantify Synthetic Data Diversity:** Provide statistical metrics (e.g., intensity histograms, structural similarity) comparing synthetic and real datasets to substantiate ‚Äúbroad variability.‚Äô‚Äô  \n2. **Generalization Analysis:** Include cross‚Äëmodality transfer or out‚Äëof‚Äëdistribution validation beyond MRI/CT (e.g., ultrasound, microscopy) within the main paper rather than appendix only.  \n3. **Ablation of Contrastive Objective:** Expand discussion of why multi‚Äëpositive supervision outperforms self‚Äësupervised or reconstructive losses; possibly include qualitative embedding visualizations or clustering metrics.  \n4. **Computation‚ÄØ&‚ÄØThroughput:** Report pretraining time, GPU resources, and scalability relative to standard self‚Äësupervised pipelines.  \n5. **Limitations‚ÄØ&‚ÄØClinical‚ÄØOutlook:** Strengthen the discussion regarding tasks where intensity invariance may harm performance and how those could be mitigated.\n\n### 5.‚ÄØMinor‚ÄØComments\n- Improve figure captions for readability; some multi‚Äëpanel results lack color‚Äëmap legends.  \n- Ensure consistent notation between equations and algorithm listings.  \n- Typographical cleanup: occasional spacing errors (‚Äúbiomed‚Äë ical‚Äô‚Äô).  \n- Consider moving lengthy dataset details to supplementary material with summarized statistics in the main text.\n\n### 6.‚ÄØOverall‚ÄØRecommendation  \n**Major‚ÄØRevision** ‚Äì The paper offers clear methodological innovation and strong preliminary evidence, meeting TMI‚Äôs scope and novelty bar.  However, before acceptance the authors should (i)‚ÄØstrengthen quantitative validation of the synthetic‚Äëto‚Äëreal generalization,‚ÄØ(ii)‚ÄØbroaden evaluation beyond two downstream tasks, and‚ÄØ(iii)‚ÄØclarify computational and methodological limits.\n\n### 7.‚ÄØConfidence‚ÄØLevel  \n**4‚ÄØ/‚ÄØ5** ‚Äì High confidence in assessing technical merit and alignment with TMI standards; moderate uncertainty regarding potential unseen limitations of large‚Äëscale synthetic pretraining.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a methodological framework for volumetric biomedical representation learning driven entirely by synthetic data. A synthetic‚Äëdata engine generates randomized anatomical label maps and intensity patterns, while a 3‚ÄëD UNet is pretrained using a voxel‚Äëwise multi‚Äëpositive contrastive loss to learn general‚Äëpurpose volumetric features. The study aims to produce representations transferable to various downstream tasks, such as registration and segmentation, without requiring real imaging data. The work is clearly written, technically detailed, and well structured, with comprehensive algorithmic transparency.\n\n---\n\n**Major Comments**  \n1. **Quantitative Validation of Synthetic Data Diversity:** The paper should include statistical measures comparing synthetic and real data (e.g., intensity distributions, structural similarity) to substantiate claims of broad variability and realism in the generated dataset.  \n2. **Assessment of Generalization Scope:** Current validation is limited to registration and segmentation on MRI and CT. Additional experiments, or clearer integration of those available in the appendix, on out‚Äëof‚Äëdistribution modalities such as ultrasound or microscopy would strengthen claims of general applicability.  \n3. **Analysis of the Contrastive Objective:** The rationale for the proposed multi‚Äëpositive contrastive formulation versus standard self‚Äësupervised or reconstruction losses requires further discussion, supported by visual or quantitative embedding analyses.  \n4. **Computational Efficiency and Scalability:** Report pretraining time, GPU usage, and resource requirements relative to comparable self‚Äësupervised methods to allow fair assessment of practicality.  \n5. **Limitations and Application Boundaries:** Expand the discussion on potential drawbacks of enforced intensity invariance, particularly for modalities or tasks where absolute intensity carries diagnostic meaning.\n\n---\n\n**Minor Comments**  \n- Some figure captions lack sufficient detail and color‚Äëmap legends.  \n- Notation should be made consistent between equations and procedural descriptions.  \n- Minor typographical corrections (e.g., spacing issues).  \n- Condensing dataset descriptions in the main text and moving extended details to supplementary material would improve readability.\n\n---\n\n**Summary Paragraph**  \nThis work introduces an innovative synthetic‚Äëdata‚Äëdriven approach addressing data scarcity in 3‚ÄëD biomedical representation learning. Strengths include methodological novelty, integration of domain randomization with contrastive pretraining, strong transparency, and promising cross‚Äëtask generalization. The main weaknesses concern insufficient quantitative analysis of the synthetic‚Äëto‚Äëreal domain gap, narrow validation scope, and limited information on computational cost. Addressing these issues would considerably strengthen the empirical and practical foundation of the contribution.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The manuscript demonstrates significant innovation and technical depth but requires additional quantitative and generalization analyses, along with improved reporting of computational aspects, before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses the challenge of limited generalization in volumetric biomedical foundation models due to small and domain-specific 3D datasets. The authors propose a representation learning framework combining: (1) a synthetic data engine that generates highly variable training samples using randomized compositions of biomedical shape templates from TotalSegmentator (Section 3, Figure 2), and (2) a contrastive learning method that pretrains a 3D UNet to be stable against nuisance imaging variation while preserving semantic consistency (Equation 1, Figure 3). The method trains exclusively on synthetic data without using any real biomedical images. Experiments demonstrate state-of-the-art performance on unsupervised multimodality registration across L2RAb and MM-WHS datasets (Figure 5, Table 2) and competitive few-shot segmentation results across six diverse datasets (Table 1), establishing the first 3D biomedical foundation model capable of both registration and segmentation tasks.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation lacks precision and completeness**\n  - The contrastive loss definition in Equation 1 uses ambiguous notation where spatial indices are pooled from both volumes (I = {1,...,2HWD}) but the relationship between indices across V1 and V2 is unclear, making reproducibility difficult\n  - The positive set P(i) definition assumes label correspondence across volumes but doesn't specify how spatial alignment is maintained during geometric augmentations, creating potential training inconsistencies\n  - Temperature parameter œÑ = 0.33 deviates significantly from standard contrastive learning values (typically 0.07-0.1) but lacks theoretical justification or sensitivity analysis in the main text\n\n‚Ä¢ **Experimental evaluation has significant methodological limitations**\n  - Registration baselines exclude recent deep learning methods like VoxelMorph variants that could provide stronger comparisons, limiting the scope of performance claims (Section 4.1)\n  - Few-shot segmentation uses only 1-3 training volumes per dataset (Table 1), which may not reflect realistic clinical deployment scenarios where more data is typically available\n  - Cross-validation or multiple random splits are not reported, raising questions about statistical significance and generalizability of the Dice improvements shown in Table 1\n  - The MM-WHS dataset required substantial preprocessing including custom spine annotations (Appendix B.4.4), potentially introducing evaluation bias\n\n‚Ä¢ **Limited technical novelty and insufficient analysis of design choices**\n  - The synthetic data engine primarily combines existing techniques (Dead Leaves model, domain randomization) with biomedical templates, offering incremental rather than fundamental innovation (Section 2, Page 3)\n  - Ablation studies in Table 3 show relatively small performance differences between design choices, questioning the necessity of the proposed complexity\n  - The choice of TotalSegmentator templates introduces potential bias toward CT-based anatomical structures, but impact on MRI-heavy datasets is not thoroughly analyzed\n  - Feature visualization in Figure 1 lacks quantitative stability metrics, relying only on qualitative assessment of cross-modal consistency\n\n‚Ä¢ **Insufficient baseline comparisons and evaluation scope**\n  - Comparison with 2D foundation models like DINOv2 is relegated to appendix (Table 8) despite their widespread adoption in medical imaging, understating competitive landscape\n  - Large-scale medical foundation models comparison (SuPreM in Table 9) shows the proposed method's advantages are modest, questioning practical significance\n  - Registration experiments lack comparison with learning-based multimodal registration methods that could leverage similar synthetic training strategies\n  - The multitask evaluation in Table 2 tests only one additional task (registration), insufficient to claim general-purpose capabilities\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and strengthen mathematical formulations**\n  - Provide explicit spatial correspondence mechanism between V1 and V2 indices in Equation 1, including how geometric augmentations preserve label alignment during training\n  - Add theoretical analysis or empirical justification for the œÑ = 0.33 choice, including comparison with standard contrastive learning temperatures\n  - Include algorithmic pseudocode for the complete contrastive training procedure to ensure reproducibility\n\n‚Ä¢ **Enhance experimental rigor and scope**\n  - Include recent deep learning registration baselines (e.g., VoxelMorph variants, TransMorph) to provide comprehensive performance comparison\n  - Expand few-shot evaluation to include 5-10 training volumes scenarios that better reflect clinical reality\n  - Report confidence intervals and statistical significance tests across multiple random dataset splits for all major results\n  - Validate registration performance on standard datasets without custom preprocessing to ensure fair comparison\n\n‚Ä¢ **Strengthen technical contributions and analysis**\n  - Conduct systematic ablation study comparing synthetic vs. real label sources with quantitative stability metrics beyond visual inspection\n  - Analyze the impact of TotalSegmentator's CT bias on different imaging modalities with dedicated experiments\n  - Provide theoretical framework connecting synthetic data diversity to downstream generalization performance\n  - Include quantitative feature stability analysis (e.g., mutual information, correlation) to supplement Figure 1 visualizations\n\n‚Ä¢ **Expand baseline comparisons and evaluation comprehensiveness**\n  - Move DINOv2 and large-scale foundation model comparisons to main text with detailed analysis of when synthetic pretraining outperforms real data approaches\n  - Include learning-based multimodal registration methods trained on synthetic data for direct comparison\n  - Evaluate on additional tasks (e.g., classification, detection) to substantiate general-purpose claims beyond registration and segmentation\n  - Provide computational cost analysis comparing synthetic pretraining with real data collection and annotation efforts",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a representation learning framework designed to improve generalization of volumetric biomedical foundation models trained on limited, domain-specific 3D datasets. The approach integrates a synthetic data engine‚Äîgenerating diverse training samples from randomized biomedical shape templates‚Äîwith a contrastive learning method that pretrains a 3D UNet to maintain semantic consistency across imaging variations. Training uses only synthetic data, and experiments show competitive or state-of-the-art performance on multimodal registration and few-shot segmentation benchmarks. Overall, the paper is ambitious and addresses an important problem, but several methodological, mathematical, and evaluative aspects limit its clarity, reproducibility, and novelty.  \n\n**Major Comments**  \n1. **Mathematical formulation and reproducibility issues**  \n   - Equation (1) lacks clear definition of spatial indices across paired volumes, making implementation details ambiguous.  \n   - The positive set \\(P(i)\\) presumes voxel correspondence during augmentations without specifying alignment mechanisms.  \n   - The temperature parameter (\\(\\tau = 0.33\\)) differs from standard practice and lacks theoretical or empirical justification.  \n\n2. **Experimental limitations**  \n   - Registration baselines omit strong and recent deep learning methods such as VoxelMorph variants, reducing comparative validity.  \n   - Few-shot segmentation experiments with only 1‚Äì3 training volumes per dataset may not reflect realistic clinical scenarios, and no cross-validation or significance testing is reported.  \n   - Custom preprocessing of MM-WHS data could introduce bias in evaluation.  \n\n3. **Limited novelty and design analysis**  \n   - The synthetic data engine builds mainly on existing domain randomization methods, yielding incremental innovation.  \n   - Ablation results indicate only modest gains across variants, and the bias from CT-based templates toward MRI datasets is not examined.  \n   - Feature visualizations rely on qualitative assessment without quantitative stability metrics.  \n\n4. **Restricted evaluation scope**  \n   - Comparisons with prominent 2D and large-scale foundation models are relegated to appendices, understating relevance to the broader field.  \n   - Registration experiments exclude learning-based multimodal methods and assess only one additional task beyond segmentation, limiting claims of general-purpose capability.  \n\n**Minor Comments**  \n- Clarify notation in Equation (1) and define all parameters explicitly.  \n- Include algorithmic pseudocode for reproducibility.  \n- Improve figure captions and move key comparisons (e.g., DINOv2) to the main manuscript.  \n- Provide standard deviations or confidence intervals for reported metrics.\n\n**Summary Paragraph**  \nThe paper‚Äôs strengths lie in addressing a critical data limitation by employing synthetic pretraining for volumetric models and demonstrating cross-task applicability. However, unclear mathematical definitions, narrow evaluation baselines, limited analysis of design choices, and modest performance gains constrain its impact and reproducibility. Strengthening methodological clarity, expanding baselines, and including quantitative analyses would substantially improve the work‚Äôs scholarly contribution.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### Comprehensive Peer Review\n\n:\nThe manuscript addresses the critical issue of the limited generalizability of biomedical foundation models due to their reliance on narrowly defined training datasets. To tackle this challenge, the authors propose a novel approach that utilizes a synthetic data engine combined with contrastive learning to develop a versatile 3D network capable of performing various voxel-level tasks without requiring real-world biomedical images. Demonstrated results show that this method achieves superior performance in tasks such as multimodality registration and few-shot segmentation, outperforming existing methods.\n\n## Major Comments\n:\n\n- Strengths:\n  - Clear Problem Statement and Novel Solution: The manuscript effectively communicates the limitations of current biomedical foundation models and proposes a unique solution leveraging synthetic data and contrastive learning, which addresses the identified shortcomings.\n  - Versatility and Empirical Evidence: The proposed method demonstrates broad applicability across different biomedical tasks, supported by empirical evidence showing state-of-the-art results in key areas.\n  - Technical Rigor and Relevance: The manuscript is grounded in advanced techniques and recent advancements in medical imaging and machine learning, as evidenced by the comprehensive references section.\n\n- Limitations:\n  - Lack of Specific Technical Details: The abstract and main text could benefit from more detailed descriptions of the synthetic data engine and contrastive learning methodology, which are essential for readers to fully grasp the innovation.\n  - Insufficient Discussion of Limitations: The manuscript does not sufficiently discuss potential limitations or challenges associated with the proposed approach, which could offer a more balanced perspective.\n  - Overemphasis on Specific Techniques and Limited Clinical Validation: The references heavily emphasize certain techniques and predominantly focus on technical aspects, lacking a broader inclusion of traditional methods and clinical validation studies.\n\n## Minor Comments\n:\n\n- Abstract:\n  - Provide more specific technical details about the synthetic data engine and contrastive learning method.\n  - Explicitly discuss the practical implications and potential impact on clinical practice.\n- References Section:\n  - Include a broader range of traditional methodologies alongside advanced techniques.\n  - Incorporate more references related to clinical validation studies and diverse imaging modalities beyond brain imaging and MRI.\n\n## Summary Paragraph\nEvaluating the manuscript against the TMI editorial criteria:\n\n- Significance: The proposed method addresses a significant gap in the field of biomedical imaging, offering a promising solution to enhance the generalizability of foundation models.\n- Innovation: The use of a synthetic data engine and contrastive learning represents a novel approach that has the potential to advance the field.\n- Evaluation: The empirical evidence provided supports the effectiveness of the proposed method, though additional technical details and a broader scope of validation are recommended.\n- Reproducibility: While the availability of code and tutorials is mentioned, explicit details on reproducibility are needed to ensure transparency and scientific rigor.\n\n## Decision Recommendation\n:\nMajor Revision\n\nThe manuscript presents a significant and innovative approach to enhancing the generalizability of biomedical foundation models. However, it requires substantial revisions to address the lack of specific technical details, a more balanced discussion of limitations, and a broader inclusion of traditional methodologies and clinical validation studies. These revisions will strengthen the manuscript's technical rigor and practical applicability, making it suitable for publication in a top-tier medical imaging journal.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the limited generalizability of biomedical foundation models arising from their dependence on narrowly defined training datasets. To address this limitation, the authors propose a synthetic data engine combined with contrastive learning to create a 3D network capable of performing various voxel-level biomedical imaging tasks without requiring real-world training data. Empirical results demonstrate superior performance in multimodality registration and few-shot segmentation, surpassing competing methods. Overall, the paper presents a clear problem statement and a technically innovative solution supported by strong empirical evidence.  \n\n**Major Comments**  \n1. **Novelty and Contribution**: The proposed combination of synthetic data generation and contrastive learning is a creative approach that meaningfully addresses the problem of dataset bias in biomedical foundation models. The manuscript articulates its motivation and contributions clearly.  \n2. **Methodological Detail**: The description of the synthetic data engine and the contrastive learning framework lacks sufficient technical depth. Additional information on architecture design, data synthesis pipeline, and training procedures would enhance understanding and reproducibility.  \n3. **Discussion of Limitations**: The paper would benefit from a more balanced discussion acknowledging potential weaknesses or constraints of the method, such as generalizability to other modalities or computational demands.  \n4. **Scope of Evaluation**: Although the quantitative results are strong, the experiments are limited primarily to specific technical benchmarks. Broader evaluations incorporating clinical validation or comparisons with traditional approaches would strengthen the study‚Äôs practical relevance.  \n5. **Literature Context**: The references emphasize recent technical advances but give limited attention to traditional methodologies and clinically validated studies. Expanding the literature review could situate the contribution within a wider biomedical context.  \n\n**Minor Comments**  \n- The abstract could include more technical details about both the synthetic data generation and contrastive learning components, as well as their potential clinical implications.  \n- Clarify notation consistency throughout the text.  \n- Expand the references to include traditional and clinical studies across diverse imaging modalities beyond MRI.  \n\n**Summary Paragraph**  \nThe manuscript offers an innovative and potentially significant contribution to biomedical image analysis by addressing the generalizability challenge of foundation models through synthetic data and contrastive learning. The presented empirical results substantiate the method‚Äôs effectiveness. However, the lack of detailed methodological explanation, limited discussion of limitations, and narrow experimental or clinical validation reduce the manuscript‚Äôs completeness. Enhanced technical transparency and a broader contextual discussion would improve its impact and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The study presents a compelling and well-motivated approach but requires additional methodological details, a balanced discussion of limitations, and broader contextual and clinical coverage before being suitable for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## LEARNING GENERAL-PURPOSE BIOMEDICAL VOLUME REPRESENTATIONS USING RANDOMIZED SYNTHESIS\n\n### Summary\n\nThe paper proposes a synthetic-data-driven framework to pretrain a single 3D U-Net that yields general-purpose, voxelwise features robust to modality, appearance, and field-of-view variations. A data engine composes randomly deformed anatomical label templates into 3D ‚Äúlabel ensembles‚Äù and synthesizes paired volumes with shared geometry but independently randomized appearances; a multi-positive, multi-scale supervised contrastive loss trains the network to be invariant to nuisance variation. Without using any real images for pretraining, the learned features substantially improve training-free multimodality registration when plugged into ANTs and ConvexAdam, and the pretrained weights consistently boost few-shot 3D segmentation across diverse datasets.\n\n### Strengths\n\n- Technical novelty and innovationThe label ensemble model that composes and deforms real anatomical templates to create diverse 3D layouts is a compelling twist on domain randomization beyond brain-only settings.The multi-positive, label-supervised, dense contrastive pretraining across two synthetic ‚Äúviews‚Äù with shared geometry directly targets appearance invariance and pose equivariance at voxel level.Training a single 3D network to serve both registration (as a feature metric) and segmentation (via fine-tuning) represents a meaningful, multitask contribution for 3D biomedical vision.\n- Experimental rigor and validationStrong gains in multimodality registration on L2RAb MR-CT and MM-WHS using standard solvers, replacing intensities with learned features.Consistent few-shot segmentation improvements over diverse baselines across multiple datasets and modalities, including an out-of-distribution fetal BOLD set.Ablations examining temperature, label sources, loss design, and augmentations enhance confidence in design choices.\n- Clarity of presentationMethod and data-engine components are described clearly and intuitively, with figures that help visualize invariance properties of learned features.A succinct loss formulation and a clean pipeline (paired synthesis, shared-geometry augmentations, multi-scale contrast) help reproducibility.\n- Significance of contributionsDemonstrates that purely synthetic pretraining can yield useful 3D features for disparate downstream biomedical tasks, reducing reliance on scarce annotated or even unannotated real data.Provides a practical path to plug learned features into classical registration solvers, which can have immediate impact.\n\n- The label ensemble model that composes and deforms real anatomical templates to create diverse 3D layouts is a compelling twist on domain randomization beyond brain-only settings.\n- The multi-positive, label-supervised, dense contrastive pretraining across two synthetic ‚Äúviews‚Äù with shared geometry directly targets appearance invariance and pose equivariance at voxel level.\n- Training a single 3D network to serve both registration (as a feature metric) and segmentation (via fine-tuning) represents a meaningful, multitask contribution for 3D biomedical vision.\n\n- Strong gains in multimodality registration on L2RAb MR-CT and MM-WHS using standard solvers, replacing intensities with learned features.\n- Consistent few-shot segmentation improvements over diverse baselines across multiple datasets and modalities, including an out-of-distribution fetal BOLD set.\n- Ablations examining temperature, label sources, loss design, and augmentations enhance confidence in design choices.\n\n- Method and data-engine components are described clearly and intuitively, with figures that help visualize invariance properties of learned features.\n- A succinct loss formulation and a clean pipeline (paired synthesis, shared-geometry augmentations, multi-scale contrast) help reproducibility.\n\n- Demonstrates that purely synthetic pretraining can yield useful 3D features for disparate downstream biomedical tasks, reducing reliance on scarce annotated or even unannotated real data.\n- Provides a practical path to plug learned features into classical registration solvers, which can have immediate impact.\n\n### Weaknesses\n\n- Technical limitations or concernsThe pretraining label bank is derived from TotalSegmentator (adult torso CT), which may bias shape statistics toward certain anatomies; fetal and neuroanatomy are less represented.Potential class-imbalance during voxel sampling (e.g., background dominance) is not discussed; no explicit positive/negative balancing strategy is described.The use of only shared geometric transforms may limit training-induced equivariance to a subset of transformations; explicit equivariance regularization is not explored.\n- Experimental gaps or methodological issuesRegistration validation relies on small test suites with minimal validation data (e.g., 1/7 split on L2RAb), raising risk of overfitting solver hyperparameters.Baseline comparisons for registration omit strong modern 3D feature extractors (e.g., SAM‚ÄëMed3D or TotalSegmentator features) and recent training-free semantic metrics (e.g., IMPACT-style features), which would further contextualize the advantage.Few-shot segmentation baselines have heterogeneous parameter counts and training recipes; while the authors tune augmentations and iterations, fairness controls (e.g., matched backbones) are limited to a subset of baselines.\n- Clarity or presentation issuesImportant implementation details are scattered to the appendix: number of output channels used across tasks, sampling strategy across labels/voxels, and distributions of ‚Äútemplates per volume.‚ÄùRuntime/compute footprint for pretraining (600k iters), and inference overhead when integrating features into registration are not reported.\n- Missing related work or comparisonsLimited discussion of recent general-purpose 3D promptable segmenters (e.g., SAM‚ÄëMed3D) and of repurposing pretrained segmentation features for registration (as in recent semantic registration metrics).Connections to related volumetric pretraining frameworks (e.g., MACL) and to fetal-specific domain randomization methods (e.g., DRIFTS/FetalSynthSeg) could be expanded to situate contributions relative to nearest neighbors.\n\n- The pretraining label bank is derived from TotalSegmentator (adult torso CT), which may bias shape statistics toward certain anatomies; fetal and neuroanatomy are less represented.\n- Potential class-imbalance during voxel sampling (e.g., background dominance) is not discussed; no explicit positive/negative balancing strategy is described.\n- The use of only shared geometric transforms may limit training-induced equivariance to a subset of transformations; explicit equivariance regularization is not explored.\n\n- Registration validation relies on small test suites with minimal validation data (e.g., 1/7 split on L2RAb), raising risk of overfitting solver hyperparameters.\n- Baseline comparisons for registration omit strong modern 3D feature extractors (e.g., SAM‚ÄëMed3D or TotalSegmentator features) and recent training-free semantic metrics (e.g., IMPACT-style features), which would further contextualize the advantage.\n- Few-shot segmentation baselines have heterogeneous parameter counts and training recipes; while the authors tune augmentations and iterations, fairness controls (e.g., matched backbones) are limited to a subset of baselines.\n\n- Important implementation details are scattered to the appendix: number of output channels used across tasks, sampling strategy across labels/voxels, and distributions of ‚Äútemplates per volume.‚Äù\n- Runtime/compute footprint for pretraining (600k iters), and inference overhead when integrating features into registration are not reported.\n\n- Limited discussion of recent general-purpose 3D promptable segmenters (e.g., SAM‚ÄëMed3D) and of repurposing pretrained segmentation features for registration (as in recent semantic registration metrics).\n- Connections to related volumetric pretraining frameworks (e.g., MACL) and to fetal-specific domain randomization methods (e.g., DRIFTS/FetalSynthSeg) could be expanded to situate contributions relative to nearest neighbors.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe loss design is sound: positives are all voxels sharing the same label across two views with shared geometry and independently randomized appearances, thereby driving appearance invariance and spatial coherence. Using the loss at multiple decoder layers encourages multiscale consistency.The choice of a higher contrastive temperature (œÑ ‚âà 0.33) is empirically justified; ablations show lower œÑ leads to aliasing/degenerate features, aligning with intuition that negatives are uniformly ‚Äúhard‚Äù under strong randomization.Two aspects merit clarification: (i) sampling fairness across labels and background in the dense InfoNCE (to avoid dominance of large classes), and (ii) whether any spatial mines or hard-negative sampling were used to stabilize training.\n- Experimental evaluation assessmentRegistration: Using features in ANTs and ConvexAdam is a convincing testbed; consistent Dice gains while keeping fold rates under standard thresholds suggest the features deliver semantically meaningful similarity. However, the datasets are small (especially L2RAb validation), so adding more pairs or cross-dataset validation would strengthen claims.Segmentation: Few-shot regimes across six datasets and modalities, with bootstrapped means/SDs, provide breadth. The consistent top-2 performance, despite using fewer parameters, supports generalization claims. Reporting per-structure breakdowns (at least in the appendix) and multi-seed variability would further strengthen the analysis.Ablations: The comparisons against alternative label sources (smshapes, Brains), loss variants (denoising, intensity-only SSL), and augmentation removals are valuable and generally support the design. It would help to also ablate the number of label templates per volume and envelope layers to directly tie structure richness to downstream performance.Efficiency: The paper would benefit from reporting training wall-clock, inference latency for feature extraction, and registration runtime overhead compared to standard intensity-based runs.\n- Comparison with related work (using the summaries provided)Relative to SynthSeg/DRIFTS-style domain randomization (brain-focused, segmentation-centric), this work generalizes label composition to broader anatomy and learns task-agnostic features that transfer to registration. DRIFTS‚Äô insights on randomized GMMs and weight-space interpolation could inspire future extensions that balance OOD robustness with in-domain accuracy.Compared to ContraReg and other registration-specific representation learning, this method is dataset-agnostic and training-free at deployment, while ContraReg/related approaches are dataset-specific and tuned per task. The trade-off is fewer task-specific optimizations but broader applicability.MACL targets volumetric segmentation with multi-level contrast, but relies on real unlabeled upstream data and remains task-focused; here, the synthetic-only, multitask nature is a differentiator.IMPACT-style recent work demonstrates that repurposed pretrained segmentation encoders can serve as universal registration metrics; directly comparing your features to those extracted from large 3D segmentation foundation models (e.g., TotalSegmentator network or SAM‚ÄëMed3D) within the same ANTs/ConvexAdam pipelines would provide a valuable reference point.\n- Discussion of broader impact and significanceThe approach can materially reduce dependence on scarce or siloed medical data for pretraining, encouraging wider sharing and reproducibility. It also provides a path to robust, plug-and-play features for classical registration pipelines.A limitation the authors note is important: intensity- or texture-driven tasks may not benefit from intensity-invariant pretraining. Explicitly documenting such boundaries will help practitioners select appropriate initializations.Ethically, training without real images mitigates privacy concerns. However, shape priors derived from public labels may still reflect population biases (e.g., adult anatomy predominance), warranting careful extension to pediatric, fetal, or non-human domains.\n- Additional suggestionsReport channel counts used for registration and whether per-voxel feature normalization (e.g., L2) or whitening is applied; consider PCA or feature selection to analyze which channels drive gains.Explore complementary equivariance regularizers (e.g., small SE(3) perturbations) to further solidify pose behavior.Provide a qualitative failure analysis for cases where registration or segmentation underperforms, to guide future refinement of the data engine (e.g., augmentations, label mixing strategies).Consider testing features on ultrasound or challenging artifact-heavy domains to evaluate robustness beyond MR/CT.\n\n- The loss design is sound: positives are all voxels sharing the same label across two views with shared geometry and independently randomized appearances, thereby driving appearance invariance and spatial coherence. Using the loss at multiple decoder layers encourages multiscale consistency.\n- The choice of a higher contrastive temperature (œÑ ‚âà 0.33) is empirically justified; ablations show lower œÑ leads to aliasing/degenerate features, aligning with intuition that negatives are uniformly ‚Äúhard‚Äù under strong randomization.\n- Two aspects merit clarification: (i) sampling fairness across labels and background in the dense InfoNCE (to avoid dominance of large classes), and (ii) whether any spatial mines or hard-negative sampling were used to stabilize training.\n\n- Registration: Using features in ANTs and ConvexAdam is a convincing testbed; consistent Dice gains while keeping fold rates under standard thresholds suggest the features deliver semantically meaningful similarity. However, the datasets are small (especially L2RAb validation), so adding more pairs or cross-dataset validation would strengthen claims.\n- Segmentation: Few-shot regimes across six datasets and modalities, with bootstrapped means/SDs, provide breadth. The consistent top-2 performance, despite using fewer parameters, supports generalization claims. Reporting per-structure breakdowns (at least in the appendix) and multi-seed variability would further strengthen the analysis.\n- Ablations: The comparisons against alternative label sources (smshapes, Brains), loss variants (denoising, intensity-only SSL), and augmentation removals are valuable and generally support the design. It would help to also ablate the number of label templates per volume and envelope layers to directly tie structure richness to downstream performance.\n- Efficiency: The paper would benefit from reporting training wall-clock, inference latency for feature extraction, and registration runtime overhead compared to standard intensity-based runs.\n\n- Relative to SynthSeg/DRIFTS-style domain randomization (brain-focused, segmentation-centric), this work generalizes label composition to broader anatomy and learns task-agnostic features that transfer to registration. DRIFTS‚Äô insights on randomized GMMs and weight-space interpolation could inspire future extensions that balance OOD robustness with in-domain accuracy.\n- Compared to ContraReg and other registration-specific representation learning, this method is dataset-agnostic and training-free at deployment, while ContraReg/related approaches are dataset-specific and tuned per task. The trade-off is fewer task-specific optimizations but broader applicability.\n- MACL targets volumetric segmentation with multi-level contrast, but relies on real unlabeled upstream data and remains task-focused; here, the synthetic-only, multitask nature is a differentiator.\n- IMPACT-style recent work demonstrates that repurposed pretrained segmentation encoders can serve as universal registration metrics; directly comparing your features to those extracted from large 3D segmentation foundation models (e.g., TotalSegmentator network or SAM‚ÄëMed3D) within the same ANTs/ConvexAdam pipelines would provide a valuable reference point.\n\n- The approach can materially reduce dependence on scarce or siloed medical data for pretraining, encouraging wider sharing and reproducibility. It also provides a path to robust, plug-and-play features for classical registration pipelines.\n- A limitation the authors note is important: intensity- or texture-driven tasks may not benefit from intensity-invariant pretraining. Explicitly documenting such boundaries will help practitioners select appropriate initializations.\n- Ethically, training without real images mitigates privacy concerns. However, shape priors derived from public labels may still reflect population biases (e.g., adult anatomy predominance), warranting careful extension to pediatric, fetal, or non-human domains.\n\n- Report channel counts used for registration and whether per-voxel feature normalization (e.g., L2) or whitening is applied; consider PCA or feature selection to analyze which channels drive gains.\n- Explore complementary equivariance regularizers (e.g., small SE(3) perturbations) to further solidify pose behavior.\n- Provide a qualitative failure analysis for cases where registration or segmentation underperforms, to guide future refinement of the data engine (e.g., augmentations, label mixing strategies).\n- Consider testing features on ultrasound or challenging artifact-heavy domains to evaluate robustness beyond MR/CT.\n\n### Questions for Authors\n\n- How many output channels does F produce, and how many are used in registration (the text mentions 16)? Are features per-voxel L2-normalized or otherwise standardized before being passed to the solver?\n- How are anchor/positive/negative voxels sampled with respect to class balance? Is the background label down-weighted or excluded to avoid dominating the loss?\n- What is the distribution of ‚Äútemplates per synthetic label ensemble‚Äù and the typical number/widths of envelope labels? Did downstream results vary with these hyperparameters?\n- Can you report compute footprint: pretraining wall-clock/GPU-hours, and average inference time and memory overhead when integrating your features into ANTs/ConvexAdam?\n- Did you evaluate registration performance using features from large 3D segmentation models (e.g., TotalSegmentator or SAM‚ÄëMed3D) within the same ANTs/ConvexAdam pipelines? How do your features compare?\n- Are there any overlaps in subjects between the TotalSegmentator label bank used for pretraining and the evaluation datasets (e.g., AMOS-CT organs overlap at class level)? While images are not used, could label set overlap bias results?\n- How sensitive are results to the number of sampled indices per decoder layer (512) and to the decoder layers chosen? Would increasing samples or adding encoder-level losses change the trade-off between registration and segmentation performance?\n- Have you explored explicit equivariance regularization (e.g., enforcing consistency under small rotations/translations) in addition to shared geometric augmentations?\n- For tasks reliant on intensity or texture, can the framework be adapted (e.g., via curriculum or mixed objectives) to retain some intensity sensitivity?\n\n### Overall Assessment\n\nThis is a thoughtfully designed and timely contribution that demonstrates the surprising breadth of what synthetic, domain-randomized volumetric pretraining can deliver: robust, task-agnostic 3D features that meaningfully improve multimodality registration and few-shot segmentation without using real images for pretraining. The data engine and the multi-positive, multi-scale contrastive loss are well-aligned with the invariances required for cross-domain generalization, and the empirical results are compelling across multiple datasets with solid ablations. The main areas to strengthen before top-tier publication are (i) broader and more contemporary baselines for registration that use pretrained semantic features (e.g., SAM‚ÄëMed3D/TotalSegmentator encoders or IMPACT-style metrics), (ii) deeper reporting on compute and runtime, and (iii) clarifications on sampling, class balance, and sensitivity to pretraining hyperparameters. Addressing these points would significantly bolster the paper‚Äôs positioning and reproducibility. Overall, I view this work as a strong step toward general-purpose volumetric biomedical representations and recommend acceptance after the suggested enhancements.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a synthetic-data-driven pretraining framework for learning general-purpose 3D biomedical volume representations. A data engine synthesizes paired 3D ‚Äúviews‚Äù by randomly composing and deforming anatomical label templates, while a multi-positive, multi-scale supervised contrastive loss encourages voxelwise invariance to appearance and modality variations. The pretrained 3D U-Net serves both registration and segmentation applications, demonstrating strong cross-task generalization despite not using real images in pretraining. The paper is clearly written and well-organized, detailing both the methodological design and empirical evaluation.  \n\n**Major Comments**  \n1. **Technical Scope and Limitations** ‚Äî The label bank relies primarily on adult torso CT labels (TotalSegmentator), possibly biasing shape statistics toward certain anatomies. Less representation of fetal or neuroanatomy and potential voxel class imbalance are unaddressed. Only shared geometric transforms are used, limiting induced equivariance; no explicit regularization is applied.  \n2. **Experimental Gaps** ‚Äî Registration experiments employ small validation splits (e.g., 1/7 on L2RAb), posing overfitting risks. Comparisons exclude strong modern baselines such as SAM‚ÄëMed3D, TotalSegmentator, or IMPACT-style features. Few-shot segmentation baselines differ in architecture and settings, limiting fairness.  \n3. **Clarity and Reproducibility** ‚Äî Key details (output channels, voxel sampling, template distributions) are deferred to the appendix. Runtime, training cost, and inference latency are unreported.  \n4. **Related Work Coverage** ‚Äî Discussion of general-purpose 3D segmenters, volumetric pretraining (MACL), and relevant domain-randomization works (DRIFTS/FetalSynthSeg) could be expanded.  \n5. **Further Analysis and Suggestions** ‚Äî Clarify sampling and loss balancing strategies. Report per-structure segmentation breakdowns and multi-seed variability. Include compute footprint, and analyze which feature channels drive improvement. Consider adding equivariance regularizers and evaluating robustness on modality-divergent domains such as ultrasound.  \n\n**Minor Comments**  \n- Ensure consistent reporting of feature dimensionality and normalization (e.g., L2).  \n- Figures are clear; adding examples of failure cases would enhance interpretability.  \n- Expand acronyms at first use for accessibility.  \n\n**Summary Paragraph**  \nThe paper offers a technically strong and well-validated approach to synthetic pretraining for 3D biomedical imaging. Its key strengths lie in the originality of the data engine, the multi-scale contrastive design, and the demonstrated cross-task generalization to registration and segmentation. Main weaknesses concern incomplete baseline coverage, limited reporting of computational aspects, and the need for clarification on label sampling and equivariance. Overall, the work effectively advances the case for synthetic-only pretraining in volumetric biomedical vision and has potential for broad applicability if the suggested clarifications are addressed.  \n\n**Decision Recommendation**  \n**Minor Revision** ‚Äî The contribution is technically sound and significant, but requires additional comparisons, fuller methodological detail, and minor revisions for completeness and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThe paper addresses a critical challenge in medical imaging: the limited generalizability of current volumetric biomedical foundation models due to small and narrow public 3D datasets that fail to capture the broad diversity of medical procedures, conditions, anatomical regions, and imaging protocols. The authors propose a novel framework that learns general-purpose biomedical volume representations using a synthetic data engine that generates highly diverse training samples combined with a contrastive learning approach that trains networks to be stable against nuisance imaging variations. The key innovation is that the method requires no pretraining on real medical images, instead leveraging randomized synthesis of biomedical structures. The authors demonstrate state-of-the-art results across multiple tasks (multimodality registration and few-shot segmentation) and diverse datasets (abdominal MRI/CT, cardiac MRI/CT, fetal MRI), showing that their synthetic-data-pretrained network provides robust, domain-agnostic features that outperform existing foundation models in multitask capabilities.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The synthetic data engine is exceptionally well-designed with medical considerations, effectively generating diverse training samples that capture the variability in biomedical imaging while avoiding dependence on specific real datasets\n- The paper demonstrates impressive multitask capabilities, showing strong performance across both registration (Figs. 4-5) and segmentation (Table 1, Fig. 6) with the same pretrained model\n- The extensive ablation studies (Table 3, Figs. 7, 10) provide valuable insights into design choices and validate the importance of each component of the framework\n\n**Limitations:**\n- The claim of \"no pretraining on any existing dataset of real images\" needs clarification since the TotalSegmentator dataset (whose segmentations are used as templates) originates from real CT images, though the authors correctly note they don't use the actual intensity images\n- The computational requirements of the synthetic data generation and training process are not discussed, which is important for practical adoption in resource-constrained medical settings\n- The paper would benefit from more discussion about failure cases or specific limitations beyond the brief mention of tasks relying on relative intensities (e.g., Nakamura et al., 2017)\n\n### Minor Comments\n\n**Strengths:**\n- The visualization of network features across different modalities (Fig. 1) effectively demonstrates the stability of the learned representations to domain shifts\n- The comparison with multiple baselines across different datasets provides strong evidence of the method's generalizability, particularly the consistent outperformance in multitask settings (Table 2)\n- The detailed implementation of the data engine (Fig. 2, Appendix B.1) enables understanding of the synthetic generation process\n\n**Limitations:**\n- Some additional architectural details beyond Table 10 (e.g., specific hyperparameters for the MLP projection heads) would strengthen reproducibility\n- The paper could better contextualize how this approach compares to adapting natural image foundation models (e.g., DINOv2) for medical imaging, beyond the brief Appendix A.7\n- The \"temperature\" hyperparameter analysis (Fig. 11) shows interesting tradeoffs between registration and segmentation performance that warrant more discussion\n\n## 3. Evaluation Along TMI Editorial Criteria\n\n**Significance (High):** The work addresses a fundamental limitation in medical AI - the lack of generalizable models due to narrow datasets - with potential to transform how biomedical foundation models are developed. The approach could significantly reduce the need for extensive reannotation and retraining when adapting to new clinical contexts, directly addressing a major bottleneck in medical AI deployment.\n\n**Innovation (High):** The paper presents highly innovative concepts: using synthetic data with medical priors as the sole pretraining source, a data engine specifically designed for biomedical variability, and contrastive learning with inductive biases for medical domain shifts. The demonstration that synthetic-data-only pretraining can outperform models trained on real medical images is particularly novel.\n\n**Evaluation (High):** The evaluation is comprehensive, spanning multiple tasks (registration and segmentation), diverse datasets (6+ distinct biomedical contexts), and extensive ablation studies. The comparisons with appropriate baselines (Table 1, Fig. 5) are well-designed, and the visualization of feature stability (Fig. 1) provides qualitative support for the quantitative results. The inclusion of both registration and segmentation tasks demonstrates true multitask capabilities.\n\n**Reproducibility (Good):** The paper provides detailed methodology, including the synthetic data generation process (Figs. 2, 12), training procedures (Appendix B), and ablation studies. The authors' commitment to releasing code, tutorials, and model weights (stated in the abstract) should enable reproduction. However, some additional implementation details (e.g., exact training time, GPU requirements) would further strengthen reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nWhile this paper presents a highly innovative and significant contribution to biomedical foundation models, several issues require addressing before acceptance. The most critical is clarifying the \"no real images\" claim to more precisely characterize the relationship between the synthetic data engine and the source segmentations. The paper should also include computational requirements and expand the discussion of limitations and failure cases. Additional details about model architecture and training would strengthen reproducibility. These revisions would enhance an already strong paper that demonstrates substantial advances in creating truly general-purpose biomedical vision models. The core methodology and results are sound, and with these clarifications, the paper would represent an important contribution to the field.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript targets the challenge of limited generalizability in volumetric biomedical foundation models, which arises from the scarcity and narrow scope of public 3D medical datasets. It introduces a synthetic data engine for diverse volume generation and a contrastive learning strategy to produce domain-agnostic biomedical representations without pretraining on real images. The study demonstrates that this synthetic-data-pretrained model achieves state-of-the-art results across multiple tasks‚Äîmultimodal registration and few-shot segmentation‚Äîspanning abdominal, cardiac, and fetal MRI/CT datasets. The presentation is clear, well-organized, and supported by extensive experimental evidence.\n\n---\n\n**Major Comments**  \n\n**Strengths**  \n1. The synthetic data engine is thoughtfully designed with medical realism, creating diverse training samples that capture anatomical and procedural variability without reliance on real datasets.  \n2. The approach achieves strong multitask performance‚Äîusing the same pretrained model for both registration and segmentation‚Äîdemonstrating robust generalization capabilities.  \n3. Ablation studies are extensive and informative, validating the effect of individual components and illustrating how design choices influence performance.  \n\n**Limitations**  \n1. The assertion of ‚Äúno pretraining on real images‚Äù requires clarification, as the generation process incorporates segmentations derived from real CT data (TotalSegmentator), even though intensity images are not directly used.  \n2. The paper does not discuss computational costs associated with the synthetic data generation and training pipeline, an important factor for practical adoption.  \n3. Discussion of failure modes or performance limitations should be expanded, especially for tasks depending on relative intensity information.  \n\n---\n\n**Minor Comments**  \n- Visualizations effectively highlight the stability of learned representations across domains.  \n- Broader comparison to natural image foundation models (e.g., DINOv2) would strengthen contextual understanding.  \n- Including more architectural details (e.g., hyperparameters for projection heads) would enhance reproducibility.  \n- The effects of the ‚Äútemperature‚Äù hyperparameter merit fuller discussion, as results indicate notable tradeoffs between tasks.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper offers a highly innovative contribution by demonstrating that exclusively synthetic pretraining can yield versatile biomedical representations competitive with real-image-trained models. Its significance lies in addressing a fundamental bottleneck in medical AI: limited data diversity. Evaluation is rigorous across tasks and datasets, and reproducibility is generally strong though incomplete regarding computational and training specifics. Addressing the noted clarifications and technical details would further strengthen an already impactful and well-executed study.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper is technically sound and innovative, but requires clarification on data provenance, computational requirements, and additional discussion of limitations before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**  \n\nThe manuscript tackles the well‚Äëknown problem that volumetric biomedical models often fail to generalise because the available 3‚ÄëD datasets are small and narrowly focused. To address this, the authors construct a synthetic data engine that creates a large collection of label ensembles from thousands of anatomical templates and then samples paired volumes with a wide range of appearances, resolutions, and imaging physics. A 3‚ÄëD UNet is pretrained with a multi‚Äëpositive supervised contrastive loss that forces voxels sharing the same synthetic label to produce consistent feature representations across the two generated views. This pretraining yields features that are invariant to appearance, equivariant to pose, and largely independent of any particular dataset. Experimental results indicate that the learned representations improve multimodal MRI‚ÄìCT registration Dice scores by up to 11‚ÄØpercentage points while keeping folding rates low, and that fine‚Äëtuning the pretrained weights enables few‚Äëshot segmentation performance comparable to, or exceeding, that of larger foundation models on several organ and modality benchmarks. The central contribution is a contrastive pretraining framework driven entirely by synthetic data, which produces general‚Äëpurpose 3‚ÄëD biomedical features without needing real images.  \n\n---  \n\n**## General feedback**  \n\n- **Significance** ‚Äì The work confronts a crucial bottleneck: the scarcity of annotated 3‚ÄëD medical data. Demonstrating that synthetic‚Äëonly pretraining can generate representations that transfer to both registration and segmentation suggests a potentially broad impact across downstream clinical tasks.  \n\n- **Innovation** ‚Äì The paper merges large‚Äëscale label‚Äëensemble generation (a ‚Äúdead‚Äëleaves‚Äù style composition of organ masks) with dense, multi‚Äëlayer contrastive learning that exploits exact synthetic voxel labels. This combination is, to my knowledge, new in the 3‚ÄëD biomedical literature, where prior methods have either relied on real label maps or focused on a single downstream objective.  \n\n- **Evaluation** ‚Äì The authors present an extensive experimental suite covering two multimodal registration benchmarks (Learn2Reg‚ÄëAbdomen MR‚ÄëCT‚ÄØ[L2RAb] and MM‚ÄëWHS) and six few‚Äëshot segmentation datasets (MSD‚ÄëHeart, PROMISE12, L2RAb‚ÄëMRI, FeTA, AMOS‚ÄëCT, WUFetal). Their results consistently place the proposed method in the top‚Äëone or top‚Äëtwo positions (see Table‚ÄØ1, Fig.‚ÄØ6), and the ablation studies explore label sources, temperature‚ÄØœÑ, and alternative pretraining losses (Table‚ÄØ3, Fig.‚ÄØ7). However, the manuscript does not provide statistical significance testing (e.g., p‚Äëvalues or confidence intervals), and it omits comparisons with the most recent 2‚ÄëD foundation models.  \n\n- **Reproducibility** ‚Äì Implementation details are described thoroughly (Algorithm‚ÄØ1, Tables‚ÄØ10 and‚ÄØ12, hyper‚Äëparameter settings). The code repository and pretrained weights are made available, yet the lack of exact random seeds, training duration, hardware specifications, and licensing information makes exact replication of the synthetic data pipeline difficult.  \n\n---  \n\n**## Specific comments / critiques**  \n\n1. **Synthetic label diversity** ‚Äì The label ensembles are built exclusively from organ masks supplied by TotalSegmentator. The manuscript does not assess how the absence of pathological or rare anatomical structures might affect performance; the only related ablation compares ‚ÄúBrains‚Äù with ‚Äúsmshapes‚Äù (Table‚ÄØ3).  \n\n2. **Quantifying feature invariance** ‚Äì Figure‚ÄØ1 offers a qualitative illustration of feature stability across modalities, but no quantitative metric (e.g., intra‚Äëclass variance, mutual information) is reported to substantiate the claim of modality‚Äëinvariant representations.  \n\n3. **Contrastive temperature analysis** ‚Äì While Figure‚ÄØ7 and Table‚ÄØ3 show that varying œÑ degrades feature quality, downstream effects on registration and segmentation are only presented for œÑ‚ÄØ=‚ÄØ0.07,‚ÄØ0.20,‚ÄØ0.33. A broader sweep would help identify the optimal temperature range more confidently.  \n\n4. **Scaling of registration channels** ‚Äì When the 16‚Äëchannel features are incorporated into ANTs and ConvexAdam, they are multiplied by a factor of 0.1 (see B.4.2) without justification or sensitivity analysis. Different scaling factors or channel selections could influence the reported gains.  \n\n5. **Ablation of augmentations** ‚Äì The synthetic volume pipeline includes many augmentations (bias fields, Gibbs ringing, gamma shifts, etc.). Table‚ÄØ3 reports only cumulative ablations (removing the foreground mask, offline augmentations, or all augmentations). The individual contribution of each augmentation remains unclear.  \n\n6. **Few‚Äëshot fine‚Äëtuning protocol** ‚Äì All segmentation baselines are fine‚Äëtuned for 37.5‚ÄØk iterations with every layer unfrozen. The impact of fewer training iterations, linear probing, or varying the number of annotated volumes is not examined, despite a brief mention that linear probing was considered (Sec.‚ÄØ4.2).  \n\n7. **Negative result on neurite‚ÄëOASIS** ‚Äì Table‚ÄØ6 shows no improvement on the OASIS adult brain dataset. The manuscript does not analyse why the synthetic pretraining fails in this high‚Äëresolution, low‚Äëvariability scenario.  \n\n8. **Multitask registration comparison** ‚Äì Table‚ÄØ2 evaluates only ANTs‚ÄëMutualInfo as a baseline for multitask capability; other foundation models (e.g., SMIT, PrimGeoSeg) are not tested with the same registration solver, limiting the fairness of the comparison.  \n\n9. **Licensing and availability** ‚Äì The code URL (https://www.neeldey.com/anatomix/) is provided, but the repository link, the precise license, and confirmation that the synthetic data generation scripts and pretrained weights are publicly downloadable are not verified.  \n\n---  \n\n**## A suggested decision**  \n\n**Major Revision** ‚Äì The manuscript presents a promising approach, but it currently lacks essential baselines, statistical validation, and several clarifications needed for reproducibility. Substantial revisions are required before the work can be accepted.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the limited generalizability of volumetric biomedical models caused by small and narrowly scoped 3‚ÄëD datasets. To mitigate this, the authors propose a synthetic data engine that generates large ensembles of labeled anatomical volumes with varied appearance, resolution, and imaging physics. A 3‚ÄëD UNet is pretrained using a multi‚Äëpositive supervised contrastive loss, encouraging voxel features to remain consistent across paired synthetic views. Experiments indicate that this synthetic pretraining improves multimodal MRI‚ÄìCT registration and few‚Äëshot segmentation performance, producing features invariant to appearance and pose. The work‚Äôs central contribution is a fully synthetic, contrastive pretraining framework that yields broadly transferable 3‚ÄëD biomedical representations without requiring real data.  \n\n---\n\n**Major Comments**  \n1. **Significance and novelty** ‚Äì The approach tackles data scarcity in 3‚ÄëD medical imaging by demonstrating that synthetic‚Äëonly pretraining can yield transferable features. The combination of large‚Äëscale label‚Äëensemble generation and dense voxel‚Äëwise contrastive learning appears novel for 3‚ÄëD biomedical applications.  \n2. **Evaluation scope** ‚Äì Experiments cover multiple registration and segmentation benchmarks and include meaningful ablations. However, no statistical significance testing is reported, and comparisons with recent 2‚ÄëD foundation models are missing.  \n3. **Reproducibility** ‚Äì While architecture, hyperparameters, and ablation details are extensive, replication is hindered by absent information on random seeds, hardware, training time, and software licensing.  \n4. **Synthetic label diversity** ‚Äì All label ensembles are derived from TotalSegmentator masks, with limited assessment of how this restricts coverage of pathological or atypical anatomies.  \n5. **Feature invariance assessment** ‚Äì The claim of modality‚Äëinvariant representations is supported visually but lacks quantitative validation metrics.  \n6. **Contrastive temperature** ‚Äì The analysis of the temperature parameter œÑ is restricted to a few values; broader exploration would clarify sensitivity.  \n7. **Feature scaling** ‚Äì The fixed scaling of 16‚Äëchannel registration features is not justified or tested for sensitivity.  \n8. **Augmentation ablations** ‚Äì Only grouped augmentation ablations are shown; the contribution of individual augmentations remains unclear.  \n9. **Fine‚Äëtuning protocol** ‚Äì Few‚Äëshot segmentation experiments use a fixed training schedule without testing alternatives such as linear probing or varying labeled volumes.  \n10. **Negative result** ‚Äì The performance drop on neurite‚ÄëOASIS is reported but not explained, leaving unclear why pretraining fails in this setting.  \n11. **Baseline fairness** ‚Äì Multitask registration comparisons exclude several relevant foundation models, limiting comparative validity.  \n12. **Code and data availability** ‚Äì Although a project URL is given, the accessibility of scripts, pretrained weights, and license details is uncertain.  \n\n---\n\n**Minor Comments**  \n- Clarify the influence of missing anatomical subclasses in the label ensembles.  \n- Provide quantitative measures for feature stability (e.g., variance or mutual information).  \n- Specify randomization control and computational setup for reproducibility.  \n- Include sensitivity analyses for temperature œÑ and channel scaling in registration.  \n- Clarify whether the repository ensures complete access to code and datasets.  \n\n---\n\n**Summary Paragraph**  \nOverall, the study presents a potentially impactful contribution by establishing a synthetic data‚Äìdriven contrastive pretraining framework for volumetric biomedical learning. The experiments are comprehensive and methodologically sound, demonstrating notable gains across several benchmarks. Nonetheless, the work would benefit from additional statistical testing, expanded baseline comparisons, and clearer documentation of synthetic data generation and code availability to ensure transparency and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The approach is promising and innovative but requires further experimental validation, reproducibility clarification, and additional comparative analysis before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Adrian V Dalca",
      "Benjamin Billot",
      "Clinton Wang",
      "Ellen Grant",
      "Mengwei Ren",
      "Neel Dey",
      "Polina Golland",
      "Hallee E. Wong"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2d77a9ccf261321c494f73f45590632c7fc2bce6.pdf",
    "remote_url": "https://openreview.net/pdf/2d77a9ccf261321c494f73f45590632c7fc2bce6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Time-to-Event Pretraining for 3D Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Multimodal learning",
      "medical imaging",
      "Electronic Health Records"
    ],
    "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify biomarkers correlated with disease progression, as they rely on supervision derived only from images and concurrent text descriptions. To address this, we introduce time-to-event pretraining, a pretraining framework for 3D medical imaging models that leverages large-scale temporal supervision from paired, longitudinal electronic health records (EHRs). Using a dataset of 18,945 CT scans (4.2 million 2D images) and time-to-event distributions across thousands of EHR-derived tasks, our method improves outcome prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell‚Äôs C-index across 8 benchmark tasks. Importantly, these gains are achieved without sacrificing diagnostic classification performance. This study lays the foundation for integrating longitudinal EHR and 3D imaging data to advance clinical risk prediction.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\n*Edit: Score increased from 6 to 8 during discussion period.*\n\nThis paper presents a self-supervised learning (SSL) method for 3D medical imaging data that leverages electronic health records (EHR) to provide extra sources of supervision via time-to-event modeling. The proposed method, future-guided pretraining, performs time-to-event (TTE) survival modeling of various medical events in the longitudinal EHR associated with each 3D scan. The authors show that future-guided pretraining consistently improves downstream TTE modeling and prognostic classification tasks ‚Äì also improving data efficiency ‚Äì without degrading standard diagnostic classification performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The presentation quality is very high. Care has been taken to logically organize the paper, clearly articulate key points, and straightforwardly present results with concise figures and tables.\n- The core idea is creative, making use of the wealth of longitudinal EHR data associated with each 3D volume for pretraining.\n- Discussion or related work and background is particularly strong.\n- Experiments are sufficiently thorough and easy to interpret ‚Äì results are convincing.\n\n### Weaknesses\n\n- The actual description of the TTE pretraining approach is brief (lines 184-191) and somewhat unclear. I would advise the authors to flesh out this section. See specific questions below.\n- A description or list of the 8,192 EHR pretraining tasks is never provided. I‚Äôm aware there may not be a convenient place to list this many items, but a general description of categories of events or a few illustrative examples would be helpful. Without this information, it‚Äôs impossible to assess whether, e.g., one the TTE pretraining tasks is *also* used as a downstream TTE modeling task. In this case, there may be concerns of ‚Äúlabel leakage‚Äù.\n\nI‚Äôm happy to increase my score once these issues are addressed ‚Äì this is an otherwise strong submission.\n\n### Questions\n\n- What exactly does it mean that Steinberg et al.‚Äôs method was used to ‚Äú[sample tasks to maximize entropy given the frequency distribution of medical codes populating the DAG‚Äù? I feel that a basic plain-language description of the motivation for this procedure is needed first: why is this method being applied at all? Are there way more than 8k events and the goal is to settle on a subset of 8k ‚Äúmeaningful‚Äù/common ones for pretraining? I don‚Äôt understand the motivation.\n- Unless I am misunderstanding, this is the only description of the TTE pretraining procedure and labels used: ‚ÄúWe define our TTE task labels by predicting the time until the next occurrence of a medical code.‚Äù The previous Section 3 described deep survival modeling in the abstract, so I expected Section 4 to more concretely describe how TTE pretraining works. Is this a ‚Äúcompeting risks‚Äù approach, where multiple events are being modeled simultaneously (in ‚Äúmulti-label‚Äù fashion)?\n- What are the 8,192 EHR tasks/events? I‚Äôm aware it would be cumbersome or impossible to list and define them all, but any reasonable attempt to convey information about them would be useful. What kinds of ‚Äúevents‚Äù are they? What are some examples?\n- Related to the above point, are the downstream labels *also* present in the set of TTE pretraining tasks? If so, isn‚Äôt there concern of ‚Äúlabel leakage‚Äù, where the model has been pretrained on label information present in the downstream training dataset? Please clarify this.\n\n**Minor comments/questions:**\n- Line 13: Maybe ‚Äúbuild‚Äù instead of ‚Äúcapture‚Äù since you use this word in the next sentence.\n- In-text citation style seems off ‚Äì should be parenthetical (\\pcite{}) in most cases when used at end of sentence/clause: ‚ÄúSox et al. (2024)‚Äù -> ‚Äú(Sox et al., 2024)‚Äù\n- Change ‚Äúe.g.‚Äù -> ‚Äúe.g.,‚Äù throughout\n- Would include more recent references [1,2] when discussing deep prognosis models on longitudinal medical imaging (first paragraph of Section 2)\n- ‚Äúi.e. 8192‚Äù -> ‚Äúi.e., 8.192‚Äù\n- ‚ÄúOur approach improves training data efficiency, increasing training labels by an average of 3x over labels assigned to patients based on their current EHR visit.‚Äù This is a bit unusual to highlight as a main contribution ‚Äì I don‚Äôt think readers will understand what ‚Äúincreasing training labels‚Äù means without having read the entire paper (nor why this impact data efficiency). Perhaps clarify language here to indicate that your approach provides 3x as many sources of supervision during SSL + that this is what provides data efficiency benefits.\n- ‚ÄúPretraining task labels as assigned per-CT scan and vary in density based on pretraining approach, see Figure 2.‚Äù Perhaps ‚Äúas assigned‚Äù is meant to be ‚Äúare assigned‚Äù? Also change ‚Äú, see Figure 2‚Äù -> ‚Äú(Figure 2)‚Äù.\n- Be consistent with ‚Äúc-statistic‚Äù vs. ‚ÄúC-statistic‚Äù\n\n**References**\n[1] Holste, Gregory, et al. \"Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling.\" NPJ Digital Medicine 7.1 (2024): 216.\n[2] Sriram, Anuroop, et al. \"Covid-19 prognosis via self-supervised representation learning and multi-image prediction.\" arXiv preprint arXiv:2101.04909 (2021).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a self-supervised learning (SSL) framework for 3D medical imaging that integrates electronic health records (EHR) as additional supervisory signals through time-to-event (TTE) modeling. This ‚Äúfuture-guided pretraining‚Äù approach aims to enhance downstream survival and prognostic prediction tasks while maintaining diagnostic classification performance. The paper is well written and logically structured, with clear exposition, effective figures and tables, and a thorough experimental section. Its presentation and discussion of related work are particularly strong.\n\n**Major Comments**  \n1. **Clarity of Method Description:** The section describing the TTE pretraining procedure (lines 184‚Äì191) is brief and somewhat unclear. A fuller explanation is needed to help readers understand how time-to-event pretraining is implemented.  \n2. **Specification of Pretraining Tasks:** The paper mentions 8,192 EHR-based pretraining tasks but does not describe them or provide representative categories or examples. Without this, the reader cannot evaluate whether any pretraining tasks overlap with downstream targets, which could raise concerns about potential ‚Äúlabel leakage.‚Äù  \n3. **Motivation and Sampling Approach:** The description of the task-sampling method attributed to Steinberg et al.‚Äîintended to ‚Äúmaximize entropy given the frequency distribution of medical codes populating the DAG‚Äù‚Äîrequires a clearer, plain-language explanation. The motivation and mechanics of this sampling step remain ambiguous.  \n4. **Pretraining Objectives and Modeling Setup:** Clarify whether the TTE pretraining involves ‚Äúcompeting risks‚Äù or multi-label modeling, and how labels are defined for each 3D scan. More detail on how TTE labels are assigned and interpreted is needed.\n\n**Minor Comments**  \n- Replace ‚Äúcapture‚Äù with ‚Äúbuild‚Äù (line 13) for stylistic consistency.  \n- Use parenthetical citation format (e.g., ‚Äú(Sox et al., 2024)‚Äù).  \n- Add missing commas after ‚Äúe.g.‚Äù throughout the text.  \n- Incorporate updated references on deep prognosis modeling ([Holste et al., 2024]; [Sriram et al., 2021]).  \n- Correct ‚Äúi.e. 8192‚Äù to ‚Äúi.e., 8,192.‚Äù  \n- Clarify phrasing around ‚Äúincreasing training labels by 3√ó,‚Äù explaining this as providing additional supervisory signals during pretraining.  \n- Revise ‚ÄúPretraining task labels as assigned‚Ä¶‚Äù to ‚ÄúPretraining task labels are assigned‚Ä¶‚Äù; parenthesize the figure reference.  \n- Ensure consistent use of ‚ÄúC-statistic.‚Äù\n\n**Summary Paragraph**  \nThis is a creative and technically solid contribution that leverages EHR-derived TTE information to guide SSL for 3D imaging. Strengths include clarity of presentation, sound experimental design, and a strong related work discussion. However, key methodological details are underspecified, particularly regarding pretraining task definitions, sampling rationale, and possible label overlap with downstream tasks. Addressing these points would greatly strengthen the paper‚Äôs transparency and reproducibility.\n\n**Decision Recommendation**  \n**Minor Revision.** The manuscript is strong overall but requires clarification of the pretraining methodology and task descriptions before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors proposed to utilize the time-to-event information in EHR that paired with the imaging data as a form of supervision for the pre-training purpose. A public dataset with both 3D images and EHR notes is employed for the pre-training and downstream applications. Another dataset without the time events is also used for the evaluation of model adaptation. The manuscript is easy to follow. However, it also suffers from several critical flaws, which are detailed below.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- Propose utilizing the time events as pre-training tasks specially designed for prognosis tasks in downstream applications. \n- The manuscript is overall easy to follow\n\n### Weaknesses\n\n- The proposed method is limited in generalization since it will require longitudinal time-to-event EHR data as the supervision for the pre-training. In comparison to the common self-supervised pre-training, the proposed methods are harder to scale up.\n\n- There is no comparison evaluation between the proposed method and prior methods in model pre-training. Only the results of the proposed method with different model architectures are reported. It will be difficult to appreciate the benefits of the proposed method.\n\n- The selected model architecture also raises questions since there are many popular model networks in medical imaging, e.g., 3D-UNet, ViT, etc. It will be helpful to see their performance compared to the vanilla ResNet. \n\n- Baselines without the pre-training process should also be reported.\n\n- The current setting utilizes public data for both pre-training and downstream applications. Having a separate evaluation dataset of a prognosis task will be helpful. \n\n- The proposed method is limited in technical innovation, though utilizing the time-to-event data as a form of supervision is relatively new in the pre-training. Mostly existing techniques are adopted for the pre-training.\n\n### Questions\n\nSee above\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a pre-training approach that leverages time-to-event information from electronic health records (EHRs) paired with medical imaging data to improve downstream prognosis tasks. A public dataset containing 3D images and EHR notes is employed for pre-training and further adaptation is examined on another dataset lacking time-event data. The paper is clearly written and easy to follow. However, the study exhibits several substantial methodological and experimental limitations that weaken its overall contribution.\n\n**Major Comments**  \n1. **Generalization and scalability** ‚Äì The proposed approach depends on longitudinal EHR data with time-to-event annotations, which limits its general applicability and scalability compared to standard self-supervised methods that use unlabeled data.  \n2. **Lack of comparative evaluation** ‚Äì The study does not include comparisons with prior pre-training strategies. Only variations of the proposed method with different model architectures are shown, making it difficult to evaluate the relative benefit of the proposed approach.  \n3. **Model architecture choice** ‚Äì The exclusive use of a vanilla ResNet architecture raises questions, as other established models in medical imaging (e.g., 3D-UNet, Vision Transformer) could offer relevant benchmarks. Performance comparisons with such architectures would strengthen the evaluation.  \n4. **Missing baselines** ‚Äì Results for models trained from scratch (without pre-training) are not presented, which would provide a necessary baseline for assessing the effectiveness of the proposed pre-training.  \n5. **Evaluation data limitations** ‚Äì Both pre-training and downstream tasks rely on public data. Including an independent dataset for prognosis evaluation would better demonstrate model adaptability.  \n6. **Technical novelty** ‚Äì While using time-to-event data as a pre-training signal is relatively new, the method largely builds on existing techniques and offers limited technical innovation.\n\n**Minor Comments**  \n- The manuscript is well organized and readable.  \n- No ethical concerns are identified.\n\n**Summary Paragraph**  \nIn summary, the study is clearly presented and introduces an interesting idea of integrating EHR-based time-to-event supervision into medical imaging pre-training. Nevertheless, restricted generalizability, lack of comparative baselines, limited architectural exploration, and modest methodological novelty collectively reduce the paper‚Äôs impact.\n\n**Decision Recommendation**  \n**Major revision.** The concept is promising but requires additional experiments, broader comparisons, and stronger validation to substantiate its contributions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a future-guided pretraining approach using time-to-event supervision to enhance the prognostic capabilities of 3D medical imaging models. By incorporating longitudinal EHR data into the pretraining process and predicting time-until-event, the model outperforms traditional methods across multiple standard tasks, as demonstrated by thorough experiments.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. Innovative Approach: The method creatively leverages EHR data following a medical scan to assist model pretraining, demonstrating better performance compared to imaging-only pretraining.\n2. Comprehensive Evaluation: Extensive comparisons across multiple tasks validate the robustness and efficiency of the TTE-based approach across different architectures.\n\n### Weaknesses\n\n1. Dependence on Large EHR Datasets: This approach relies on extensive, high-quality EHR data, which many medical datasets do not include.\n2. Limited Modality Scope: Tested only on CT images; broader modality testing could validate versatility across imaging types.\n3. Interpretability: The TTE pretraining‚Äôs impact on specific pixel-level biomarkers is less clear; additional analysis on feature attribution could help.\n\n### Questions\n\n1. Why start from 3D image scans instead of 2D medical images? Is this due to the dataset choice, or has similar work already been done on 2D data?\n2. How does the choice of time segmentation for EHR data affect model results during pretraining? Specifically, my understanding is that the model predicts the probability of a patient experiencing a certain event at intervals like 1, 2, or 3 years post-scan. How does the granularity of these time segments impact the performance of the pretrained encoder?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a future-guided pretraining framework that leverages time-to-event (TTE) supervision derived from longitudinal electronic health record (EHR) data to improve the prognostic ability of 3D medical imaging models. By integrating downstream clinical outcomes into the pretraining stage, the approach aims to enhance representation learning compared to imaging-only methods. The work is clearly written and presents comprehensive experiments demonstrating superior performance across several standard tasks and model architectures.\n\n**Major Comments**  \n1. **Dependence on Large EHR Datasets:** The proposed approach requires rich, high-quality longitudinal EHR data linked to imaging studies, which may not be available for many institutions or datasets. This dependency could limit the method‚Äôs applicability and reproducibility.  \n2. **Limited Modality Evaluation:** The experiments focus solely on CT imaging, leaving the generalizability of the method to other modalities (e.g., MRI or X-ray) uncertain. Expanding the validation to other imaging types would strengthen claims of versatility.  \n3. **Interpretability of Learned Features:** While results demonstrate improved performance, it remains unclear how the TTE pretraining influences feature learning at the pixel or biomarker level. Additional analysis on feature attribution or explanatory visualization could enhance understanding of the model‚Äôs behavior.  \n\n**Minor Comments**  \n- Clarify the rationale for focusing exclusively on 3D image scans rather than including 2D modalities; it is unclear whether this choice is due to dataset availability or prior work in 2D.  \n- Discuss how the granularity of time segmentation in EHR-derived outcomes (e.g., predicting events at 1-, 2-, or 3-year intervals) affects pretraining performance, as varying temporal resolution may influence the learned representations.  \n- The manuscript is clearly written and figures appear readable; no major presentation issues were noted.  \n\n**Summary Paragraph**  \nOverall, this paper introduces an innovative and well-executed method that connects longitudinal EHR information with imaging-based pretraining, yielding measurable improvements in prognostic modeling. The study‚Äôs thorough evaluation is a strength, though questions remain about generalizability, data dependency, and interpretability. Addressing these limitations would improve both the practical relevance and scientific depth of the work.  \n\n**Decision Recommendation**  \n**Recommendation:** Minor Revision. The study is novel and technically sound, but would benefit from clarifications and additional discussion regarding dataset requirements, modality coverage, and interpretability.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**IEEE TMI Peer Review Report**\n\n---\n\n### Phase 1: Analytical and Scope Audit\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a *methodological innovation* in medical image modeling ‚Äî a new *time‚Äëto‚Äëevent (TTE) pretraining* framework for 3‚ÄëD imaging encoders that integrates longitudinal EHR‚Äëderived temporal supervision. This lies firmly within the scope of IEEE *Transactions on Medical Imaging*, which emphasizes new imaging algorithms, reconstruction and modeling techniques, and machine‚Äëlearning methods grounded in imaging science rather than purely clinical application.\n\n**2. Novelty & Contribution Level**  \nThe study introduces the first large‚Äëscale *TTE pretraining paradigm for 3‚ÄëD medical images*. It bridges survival analysis and large‚Äëscale representation learning, moving beyond per‚Äëexam or static self‚Äësupervised schemes. The idea of using TTE distributions as supervision signals in pretraining is distinctly new for imaging foundation models. The contribution level is high, provided the implementation and validation are rigorous.\n\n**3. Technical and Experimental Rigor**  \nThe technical formulation of the piecewise exponential neural network and the PEANN‚Äëbased loss is mathematically consistent; the authors also present ablations, metrics (AUROC, Harrell‚Äôs C‚Äëindex, Brier score), and comparisons to strong baselines. However, parts of the methodology‚Äîsuch as details of task sampling from 8,192 events, data harmonization, and how censorship handling interacts with multitask pretraining‚Äîshould be explained more precisely. Experiments are extensive, use publicly available datasets (INSPECT and RSPECT), and include statistical confidence intervals. Overall rigor is good, though the theoretical justification for why TTE pretraining improves representation quality could be expanded.\n\n**4. Clarity and Presentation**  \nThe paper is clearly structured with detailed tables, appendices, and well‚Äëcommented figures. The terminology (‚Äúmissing context problem,‚Äù ‚Äúpiecewise exponential neural network‚Äù) is appropriate for the field. Minor language polishing could improve readability; currently, the manuscript is densely written.\n\n**5. Ethical and Reproducibility Compliance**  \n‚úÖ The authors use publicly released, de‚Äëidentified datasets, describe compliance with HIPAA requirements, and provide open code and checkpoints. Ethical considerations and subgroup bias analyses are included, aligning with reproducibility and fairness expectations of *TMI*.\n\n---\n\n### Phase 2: IEEE TMI Review Report\n\n#### 1. Summary\nThis paper proposes **time‚Äëto‚Äëevent (TTE) pretraining** for 3‚ÄëD medical image foundation models. The method links each imaging study to long‚Äëterm EHR outcomes to learn representations predictive not only of current disease but of *future events*. Using a **piecewise exponential neural network (PEANN)** as the survival objective, 8,192 EHR‚Äëderived TTE tasks are generated across ~19k CT exams. The pretrained encoder is evaluated by linear and Cox‚Äêbased probes on eight outcome‚Äëprediction tasks and eight diagnostic tasks. Results show mean gains of roughly 20‚Äì30‚ÄØ% in AUROC and Harrell‚Äôs‚ÄØC‚Äëindex on prognostic prediction without loss of diagnostic accuracy.\n\n#### 2. Strengths\n- **Methodological novelty**: introduces TTE modeling as a large‚Äëscale pretraining signal for 3‚ÄëD imaging.  \n- **Technical soundness**: clearly formulated PEANN loss and comprehensive experimental design.  \n- **Empirically broad**: multi‚Äëarchitecture validation (SwinUNETR, DenseNet, ResNet), multiple performance metrics, and subgroup fairness checks.  \n- **Reproducibility and openness**: code, model weights, and dataset identifiers are supplied.  \n- **Clinical relevance**: connects imaging features to long‚Äëterm outcomes, addressing a key gap in prognostic imaging.\n\n#### 3. Weaknesses\n- Limited theoretical explanation of *why* survival‚Äëbased supervision yields more transferable representations beyond increased label density.  \n- EHR‚Äìimaging alignment and task‚Äësampling procedure depend on several heuristic design choices; robustness to those choices is not explored.  \n- Evaluation restricted to chest CT; generalizability to other modalities, body regions, and tasks remains untested.  \n- Compute cost is extremely high (hundreds of GPU‚Äëhours), potentially limiting reproducibility for typical labs.  \n- Writing is occasionally repetitive; the message could be clearer with condensation of tables/figures.\n\n#### 4. Major Comments\n1. **Clarify the pretraining objective integration:** Expand Section‚ÄØ4 describing how multiple TTE tasks are optimized jointly ‚Äî whether losses are summed, weighted, or sampled per batch. Include sensitivity analysis for the number of time bins and task selection method (entropy‚Äëranked vertex cover).  \n2. **Provide deeper theoretical motivation:** Connect the survival likelihood to representation learning theory‚Äîwhy should time‚Äëdependent hazard supervision improve prognostic encoding?  \n3. **Generalization assessment:** Consider at least one non‚Äëthoracic dataset or 2‚ÄëD modality to test transferability beyond chest CT.  \n4. **Compute and scalability discussion:** Include a quantitative comparison of performance versus compute cost or label density to highlight efficiency gains.  \n5. **Bias and censoring:** Elaborate on how competing risks and varying censoring distributions affect training stability; include discussion of possible biases introduced by EHR‚Äëderived event times.\n\n#### 5. Minor Comments\n- Proofread for minor grammar inconsistencies (‚Äúpretrianing,‚Äù ‚Äúinspsect‚Äù).  \n- Some figures (e.g., Grad‚ÄëCAM montages, Appendix‚ÄØQ) could be resized or compressed; the main text may summarize key examples instead of showing all.  \n- Consolidate identical numeric results presented across tables/appendices; highlight essential findings in the primary text.  \n- Add consistent notation for survival time variables; some equations reuse symbols (T,‚ÄØÀúT) inconsistently.  \n- Cite recent TMI relevant works on self‚Äësupervised risk prediction if available.\n\n#### 6. Overall Recommendation  \n**Major Revision** ‚Äì The study is promising and methodologically innovative, clearly within TMI‚Äôs scope, but requires stronger justification, clearer methodological exposition, and possibly an additional demonstration of generalizability. With these revisions, it could reach TMI‚Äôs publication standard.\n\n#### 7. Confidence Level  \n**4/5** ‚Äì High confidence in assessment; the reviewer has expertise in medical imaging machine learning and time‚Äëto‚Äëevent modeling.\n\n---\n\n**Summary Decision:**  \nThe paper offers *substantive methodological advancement* with strong empirical evidence and reproducibility. Addressing the above major comments‚Äîparticularly on theoretical motivation, cross‚Äëmodality robustness, and detailed implementation clarity‚Äîwould likely make it a strong candidate for publication in *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a methodological innovation in medical image modeling‚Äîa new *time‚Äëto‚Äëevent (TTE) pretraining* framework for 3‚ÄëD imaging encoders that integrates longitudinal EHR‚Äëbased temporal supervision. The approach combines survival analysis with large‚Äëscale representation learning, aiming to produce image encoders that capture prognostic rather than static features. The work is clearly presented, well structured, and includes extensive experiments on publicly available datasets using multiple architectures. The manuscript demonstrates methodological novelty, strong empirical validation, and commitment to reproducibility, though some aspects of theoretical justification, experimental scope, and methodological clarity require further elaboration.  \n\n**Major Comments**  \n1. **Integration of TTE objectives:** Clarify how the multiple TTE tasks are optimized jointly‚Äîwhether losses are summed, weighted, or sampled per batch‚Äîand include sensitivity analyses for time‚Äëbinning and task‚Äëselection strategies (e.g., entropy‚Äëranked vertex cover).  \n2. **Theoretical motivation:** Provide a more explicit discussion linking survival‚Äëbased likelihood learning to representation learning theory, explaining why time‚Äëdependent hazard supervision strengthens feature transferability.  \n3. **Generalization across datasets:** The study is limited to chest CT; an additional experiment on a different modality or anatomical region would strengthen claims of generalizability.  \n4. **Compute and efficiency considerations:** Quantitatively compare performance gains with computational cost or label density to provide perspective on the method‚Äôs scalability and efficiency.  \n5. **Bias and censoring effects:** Expand discussion of how varying censoring distributions and competing risks influence model stability and bias, particularly given the heuristic nature of EHR‚Äëevent alignment and sampling.  \n\n**Minor Comments**  \n- Proofread for typographical errors (‚Äúpretrianing,‚Äù ‚Äúinspsect‚Äù) and streamline repetitive phrasing.  \n- Compress or summarize redundant tables and large figures (e.g., Grad‚ÄëCAM montages) to improve readability.  \n- Maintain consistent notation for survival variables (T,‚ÄØÀúT).  \n- Consider citing recent related self‚Äësupervised risk‚Äëprediction studies.  \n\n**Summary Paragraph**  \nOverall, this study represents a substantial and timely contribution linking prognostic modeling with image representation learning. It is technically sound, empirically comprehensive, and reproducible through open code and models. The principal weaknesses lie in limited theoretical exposition, untested generalizability, and heavy computational demands. With clearer motivation and additional validation, the paper could achieve high impact within medical imaging methodology.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is novel and well executed but requires enhanced theoretical context, methodological clarity, and broader evaluation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces time-to-event (TTE) pretraining for 3D medical imaging models to address the \"missing context problem\" in current self-supervised learning approaches. The authors argue that existing methods fail to learn prognostic biomarkers because they rely only on imaging data and concurrent text descriptions, lacking temporal information about disease progression. Their method leverages longitudinal electronic health records (EHRs) to create 8,192 TTE pretraining tasks using piecewise exponential neural networks (PEANN). The approach is evaluated on 18,945 CT scans from the INSPECT dataset and tested on three architectures (SwinUNETR, DenseNet-121, ResNet-152). Results show average improvements of 23.7% in AUROC and 29.4% in Harrell's C-index across 8 benchmark tasks for prognostic prediction, while maintaining diagnostic classification performance on the RSPECT dataset.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and notation clarity issues**\n  - Equation 1 uses inconsistent notation where the survival function Si(t) depends on both patient i and time piece p, but the relationship between image-specific representations Mip and the hazard calculation is unclear (Section 3, Equation 1)\n  - The derivation of the loss function references Appendix M but the main text lacks sufficient mathematical detail to verify the PEANN implementation independently (Section 3, page 5)\n  - The piecewise exponential model assumes constant hazard rates within intervals, but the paper doesn't adequately justify this assumption for medical events or discuss potential violations (Section 3, Equation 1)\n\n‚Ä¢ **Limited experimental scope and generalizability concerns**\n  - The evaluation is restricted to chest CT scans from a single institution (INSPECT dataset), limiting generalizability across different imaging modalities, anatomical regions, and healthcare systems (Table 1, Section 5)\n  - Only three architectures are tested, with two (DenseNet, ResNet) using 2D weight inflation rather than native 3D pretraining, potentially confounding the comparison (Section 5, page 7)\n  - The baseline comparisons lack strong SSL methods specifically designed for 3D medical imaging, making it difficult to assess the true benefit of TTE supervision over state-of-the-art alternatives (Section 5, page 7)\n\n‚Ä¢ **Methodological limitations in task selection and evaluation design**\n  - The entropy-ranked vertex cover approach for selecting 8,192 pretraining tasks from 4.3 million candidates lacks sufficient detail and validation of task relevance (Section 4, page 5)\n  - The frozen encoder evaluation strategy may not reflect real-world deployment scenarios where full fine-tuning is often preferred (Section 4, page 6)\n  - Statistical significance testing uses bootstrap confidence intervals but doesn't adequately address multiple comparisons across numerous tasks and metrics (Section 5, Tables 2-4)\n\n‚Ä¢ **Insufficient analysis of temporal assumptions and competing risks**\n  - The method treats patient death as the only competing risk and censors at death, but medical events often have complex competing risk structures that could bias results (Section 4, page 6)\n  - The choice of uniform time bins for the piecewise exponential model lacks justification and sensitivity analysis across different temporal discretizations (Section 5, page 6)\n  - No evidence found in the manuscript for handling time-varying covariates or validating the proportional hazards assumption underlying the evaluation metrics\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation consistency**\n  - Provide complete mathematical derivations in the main text rather than relegating them to appendices, ensuring all notation is clearly defined and consistently used throughout\n  - Include formal justification for the piecewise exponential assumption with empirical validation on the medical event data\n  - Add sensitivity analyses examining how different choices of time intervals and hazard rate assumptions affect model performance\n\n‚Ä¢ **Expand experimental validation scope**\n  - Evaluate the approach on multiple datasets from different institutions and imaging modalities to demonstrate broader applicability beyond chest CT scans\n  - Include comparisons with recent 3D SSL methods specifically designed for medical imaging rather than relying primarily on 2D weight inflation baselines\n  - Conduct experiments using native 3D pretrained models for all architectures to ensure fair comparisons\n\n‚Ä¢ **Strengthen methodological foundations and evaluation protocols**\n  - Provide detailed algorithmic descriptions and validation studies for the entropy-ranked vertex cover task selection method, including ablation studies on different numbers of selected tasks\n  - Compare frozen encoder results with full fine-tuning approaches to better understand the trade-offs and practical deployment considerations\n  - Implement proper multiple comparison corrections and provide more comprehensive statistical analysis addressing the numerous hypothesis tests conducted\n\n‚Ä¢ **Address temporal modeling limitations comprehensively**\n  - Extend the competing risk analysis beyond death censoring to include other relevant competing events and validate these assumptions empirically\n  - Conduct systematic sensitivity analyses across different temporal discretization schemes and validate the impact on downstream performance\n  - Incorporate methods for handling time-varying covariates and provide empirical validation of key survival analysis assumptions using the available longitudinal data",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a time-to-event (TTE) pretraining framework for 3D medical imaging models, aiming to address the lack of temporal information in current self-supervised learning (SSL) approaches. By leveraging longitudinal electronic health records (EHRs), the authors define 8,192 TTE pretraining tasks using piecewise exponential neural networks (PEANN) and evaluate the method on 18,945 CT scans from the INSPECT dataset across three architectures (SwinUNETR, DenseNet-121, ResNet-152). The approach reports substantial improvements in prognostic metrics while maintaining diagnostic performance. The paper presents an interesting idea connecting survival analysis and representation learning but exhibits several methodological and evaluation weaknesses that limit interpretability and generalizability.\n\n---\n\n**Major Comments**  \n1. **Mathematical and Notational Clarity**  \n   - Equation (1) shows inconsistent notation regarding the survival function \\( S_i(t) \\) and its dependence on patient and time piece, with unclear linkage to image-based representations.  \n   - The loss formulation references Appendix M without sufficient mathematical derivations in the main text, hindering independent verification of PEANN.  \n   - The assumption of constant hazard rates within time intervals is not justified or empirically validated for medical events.\n\n2. **Experimental Scope and Generalizability**  \n   - Evaluation is restricted to chest CT scans from a single institution, limiting generalization to other modalities or patient populations.  \n   - Comparisons use two architectures with inflated 2D weights instead of native 3D pretraining, which may confound performance differences.  \n   - Baselines omit contemporary 3D SSL methods, making relative benefits of TTE unclear.\n\n3. **Task Selection and Evaluation Design**  \n   - The entropy-ranked vertex cover method lacks detail and validation regarding the relevance of selected pretraining tasks.  \n   - Reliance on frozen encoder evaluations may not represent realistic downstream applications involving full fine-tuning.  \n   - Statistical analysis does not address multiple comparisons across numerous tasks and metrics.\n\n4. **Temporal Modeling and Competing Risks**  \n   - Treating death as the sole competing risk risks biasing event modeling; other potential competing outcomes are not considered.  \n   - Uniform time binning is used without justification or sensitivity testing.  \n   - Time-varying covariates and proportional hazards assumptions are not empirically validated.\n\n---\n\n**Minor Comments**  \n- Clarify mathematical notation and define all variables consistently.  \n- Provide cross-references between equations in the main text and appendices.  \n- Expand figure and table captions to clearly specify datasets, models, and evaluation settings.  \n- Check typographical consistency in equations and section numbering.\n\n---\n\n**Summary Paragraph**  \nOverall, the manuscript addresses an important gap by integrating temporal EHR-derived supervision into SSL for 3D imaging, with promising reported performance improvements. However, limitations in mathematical rigor, incomplete methodological details, and narrow experimental validation raise concerns about reproducibility and generalizability. Statistical methodology and temporal assumptions also require more rigorous analysis. Addressing these issues through expanded derivations, broader experiments, and improved evaluation protocols would significantly strengthen the work.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The paper presents an innovative concept but requires substantial methodological, analytical, and experimental improvements before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### Comprehensive Peer Review\n\n:\nThe manuscript introduces a novel time-to-event (TTE) pretraining framework for 3D medical imaging models that leverages longitudinal electronic health records (EHRs) to improve prognostic performance without compromising diagnostic accuracy. The authors demonstrate significant enhancements in outcome prediction metrics, such as AUROC and Harrell‚Äôs C-index, showcasing the potential of integrating temporal context into medical imaging analysis.\n\n## Major Comments\n:\n\nStrengths:\n1. Clear Problem Statement and Solution: The manuscript effectively identifies the limitations of current self-supervised methods in 3D medical imaging and proposes a TTE pretraining framework that addresses these gaps by incorporating temporal context.\n2. Quantitative Results: The study provides compelling quantitative evidence of the proposed method's effectiveness, with notable increases in AUROC and Harrell‚Äôs C-index, demonstrating superior prognostic performance.\n3. Comprehensive References: The reference section covers a broad spectrum of relevant topics, including deep learning, medical imaging, and survival analysis, with a strong emphasis on recent publications (2022-2024).\n4. Detailed Appendix: The appendix offers extensive detail on data preprocessing, model configurations, and performance metrics, supporting transparency and reproducibility.\n\nLimitations:\n1. Abstract Lack of Specific Details: The abstract could benefit from more specific details about the methodology, such as the exact nature of the pretraining tasks and technical implementation.\n2. Balanced Reference Coverage: While the reference section is robust, it slightly overemphasizes deep learning and lacks clinical trial studies, which could provide additional validation and practical insights.\n3. Appendix Complexity: The abundance of tables and metrics in the appendix might overwhelm readers unfamiliar with the specific methodologies and terminologies used. Simplifying or summarizing some of the key points could improve readability.\n\n## Minor Comments\n:\n1. Formatting Consistency: Both the reference section and appendix suffer from inconsistent formatting, particularly in the inclusion of page numbers and DOI links.\n2. Visual Presentation: The absence of figures and tables within the text of the appendix reduces clarity. Including these elements would greatly enhance readability.\n3. Technical Explanation Clarity: Certain technical explanations, such as those for the piecewise exponential loss function, could be simplified for better understanding.\n\n## Summary Paragraph\nThe manuscript meets the TMI editorial criteria as follows:\n- Significance: The work addresses a critical gap in prognostic biomarker identification using 3D medical imaging, with substantial improvements in outcome prediction metrics.\n- Innovation: The introduction of TTE pretraining represents a novel approach that integrates temporal context into medical imaging analysis, offering significant advancements in prognostic performance.\n- Evaluation: The study employs rigorous evaluation methods, including multiple performance metrics and confidence intervals, demonstrating robustness and reliability.\n- Reproducibility: The comprehensive appendix and detailed descriptions of data preprocessing and model configurations significantly enhance the reproducibility of the research.\n\n## Decision Recommendation\n:\nMinor Revision: The manuscript is strong overall but requires minor revisions to address inconsistencies in formatting, enhance clarity in technical explanations, and include additional visual aids in the appendix. These adjustments will further enhance the manuscript's comprehensiveness and appeal to a broader audience.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a novel time‚Äëto‚Äëevent (TTE) pretraining framework for 3D medical imaging models that incorporates longitudinal electronic health records (EHRs) to improve prognostic performance while maintaining diagnostic accuracy. The study demonstrates substantial gains in outcome prediction metrics such as AUROC and Harrell‚Äôs C‚Äëindex, supporting the potential of integrating temporal contextual information into medical image analysis. Overall, the paper is clearly presented and addresses an important methodological gap in prognostic modeling.  \n\n**Major Comments**  \n**Strengths**  \n1. **Clear problem statement and solution:** The manuscript effectively identifies limitations in current self‚Äësupervised learning methods for 3D medical imaging and presents the TTE pretraining framework as a solution that integrates temporal dynamics.  \n2. **Quantitative evidence:** Experimental results show convincing improvements in prognostic metrics (AUROC, Harrell‚Äôs C‚Äëindex), emphasizing the framework‚Äôs effectiveness.  \n3. **Comprehensive literature coverage:** References are wide‚Äëranging and current, covering deep learning, medical imaging, and survival analysis, with a focus on recent work from 2022‚Äì2024.  \n4. **Detailed supplementary material:** The appendix thoroughly documents data preprocessing, model configurations, and evaluation procedures, contributing to transparency and reproducibility.  \n\n**Limitations**  \n1. **Abstract specificity:** The abstract could more clearly summarize key methodological details, including the pretraining objectives and technical implementation.  \n2. **Reference balance:** The bibliography, while extensive, favors deep learning literature and includes fewer clinical validation studies that could strengthen practical relevance.  \n3. **Appendix readability:** The extensive tables and metrics in the appendix may overwhelm some readers; summarizing or streamlining key content would improve accessibility.  \n\n**Minor Comments**  \n1. Ensure consistent formatting in references and appendix, including page numbers and DOI links.  \n2. Incorporate figures or tables directly within the appendix to improve clarity of presentation.  \n3. Simplify explanations of complex components, such as the piecewise exponential loss function, to aid reader comprehension.  \n\n**Summary Paragraph**  \nThe paper makes a meaningful contribution to prognostic modeling in 3D medical imaging by introducing a TTE pretraining framework that significantly enhances outcome prediction. Its innovation and evaluation rigor are clear, and reproducibility is supported through detailed methodological documentation. Most issues concern presentation‚Äîclarity in the abstract, formatting consistency, and improved visual support‚Äîrather than substantive scientific limitations.  \n\n**Decision Recommendation**  \n**Minor Revision:** The study is strong and ready for publication pending minor revisions to improve clarity, formatting, and accessibility of supplementary materials.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## TIME-TO-EVENT PRETRAINING FOR 3D MEDICAL IMAGING\n\n### Summary\n\nThis paper proposes time-to-event (TTE) pretraining for 3D medical imaging encoders, using longitudinal EHR-derived supervision to learn prognostic representations that link imaging features to future clinical outcomes. The method attaches a piecewise exponential neural network (PEANN) survival head to a 3D CT encoder and continues pretraining on 8,192 EHR-selected tasks, then evaluates frozen encoders with lightweight survival or classification heads. Using the public INSPECT dataset (18,945 CTs with ~5-year follow-up) and external RSPECT, the authors report substantial gains on prognostic tasks (e.g., AUROC and Harrell‚Äôs C-index) without degrading diagnostic classification performance.\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a scalable, EHR-driven TTE pretraining paradigm for 3D imaging, addressing the ‚Äúmissing context‚Äù problem by leveraging longitudinal outcome supervision rather than contemporaneous labels only.Sensible survival modeling choice (piecewise-exponential hazard) that is stable, censoring-aware, and computationally feasible at scale; pragmatic adaptation with a CoxPH (DeepSurv) probe for downstream TTE.Task selection via an ontology-aware, entropy-ranked vertex cover leverages the structure of medical code graphs to curate a large, diverse set of TTE tasks.Demonstrates that temporal outcome supervision can be used as a pretraining signal to embed prognostic information into 3D encoders.\n- Experimental rigor and validationUses only public datasets (INSPECT, RSPECT) with clear splits; includes multiple 3D backbones (SwinUNETR, ResNet-152, DenseNet-121) and continued-pretraining baselines (MTL, per-visit labels).Reports bootstrap CIs, significance tests, and multiple metrics (AUROC, Harrell‚Äôs C-index; mentions IBS and time-dependent C-statistics in appendices).Provides an informative visit-vs-TTE ablation with the same 8,192 tasks to isolate the value of future temporal supervision.\n- Clarity of presentationThe problem framing and motivation (missing temporal context in standard SSL) are clear and clinically relevant, with helpful schematics.The pipeline overview (task creation, PEANN pretraining, frozen adaptation) is understandable and aligns with the stated hypotheses.\n- Significance of contributionsPrognostic modeling from imaging is a high-impact, underexplored setting; integrating EHR-derived TTE supervision into 3D encoders advances the field.The reported gains on risk prediction with no degradation on diagnostic tasks suggests practical utility for clinical decision support.\n\n- Introduces a scalable, EHR-driven TTE pretraining paradigm for 3D imaging, addressing the ‚Äúmissing context‚Äù problem by leveraging longitudinal outcome supervision rather than contemporaneous labels only.\n- Sensible survival modeling choice (piecewise-exponential hazard) that is stable, censoring-aware, and computationally feasible at scale; pragmatic adaptation with a CoxPH (DeepSurv) probe for downstream TTE.\n- Task selection via an ontology-aware, entropy-ranked vertex cover leverages the structure of medical code graphs to curate a large, diverse set of TTE tasks.\n- Demonstrates that temporal outcome supervision can be used as a pretraining signal to embed prognostic information into 3D encoders.\n\n- Uses only public datasets (INSPECT, RSPECT) with clear splits; includes multiple 3D backbones (SwinUNETR, ResNet-152, DenseNet-121) and continued-pretraining baselines (MTL, per-visit labels).\n- Reports bootstrap CIs, significance tests, and multiple metrics (AUROC, Harrell‚Äôs C-index; mentions IBS and time-dependent C-statistics in appendices).\n- Provides an informative visit-vs-TTE ablation with the same 8,192 tasks to isolate the value of future temporal supervision.\n\n- The problem framing and motivation (missing temporal context in standard SSL) are clear and clinically relevant, with helpful schematics.\n- The pipeline overview (task creation, PEANN pretraining, frozen adaptation) is understandable and aligns with the stated hypotheses.\n\n- Prognostic modeling from imaging is a high-impact, underexplored setting; integrating EHR-derived TTE supervision into 3D encoders advances the field.\n- The reported gains on risk prediction with no degradation on diagnostic tasks suggests practical utility for clinical decision support.\n\n### Weaknesses\n\n- Technical limitations or concernsThe survival head is piecewise-constant hazard with a fixed number of uniform time bins (P=8). This may be suboptimal for heterogeneous hazards; no ablations on bin count/placement (e.g., quantile/log time) are presented.Competing risks are simplified to ‚Äúdeath only,‚Äù which is often inadequate for multi-outcome EHR settings; no cause-specific or multi-state modeling during pretraining.The mismatch between a PEANN head during pretraining and a CoxPH head during adaptation could lead to inconsistencies; limited discussion on why PEANN wasn‚Äôt also used for adaptation.Each task is modeled independently; any shared structure across outcomes (e.g., via multi-task survival heads) is not exploited.\n- Experimental gaps or methodological issuesExternal generalization is limited to a single diagnostic dataset (RSPECT) and only CT modality; no multi-institution evaluation for prognostic TTE transfer.Overlap between pretraining tasks and evaluation tasks is likely (e.g., PH). While the split prevents leakage across sets, it would be more convincing to show results when excluding evaluation-identical codes from pretraining.No ablation on the number of TTE tasks, selection heuristic variants, or label-density controls (e.g., holding label counts constant across visit vs TTE).Multiple comparisons are performed, but it is unclear whether corrections (e.g., FDR) were applied; some tables include repeated placeholder-like values (e.g., 0.505) that merit clarification.Calibration claims (IBS improvements) are relegated to the appendix; inclusion of main-text calibration plots/tables would strengthen the argument.\n- Clarity or presentation issuesSeveral technical details are under-specified: precise definition of time origin, treatment of left truncation, interval construction, and handling of irregular follow-up.The statement ‚Äúwe apply censorship at patient death‚Äù is ambiguous; for non-mortality tasks death is a competing risk, not simple censoring‚Äîplease clarify the exact handling.Code links in the text appear as placeholders; explicit, functioning links are crucial for reproducibility.\n- Missing related work or comparisonsLimited discussion of recent progression-aware SSL and survival heads for neural models beyond Cox/PE (e.g., DeepHit, discrete-time/PLH/PLD survival heads; survival modeling with piecewise-linear hazards).Recent imaging-time-aware SSL approaches (e.g., TE-SSL for MRI, 3DTINC for OCT) are not positioned relative to this work; a brief comparison would strengthen context.Survival models that couple interpretability and deep components (DeepPAMM/DeepPAM) are relevant and could inform head design and calibration.\n\n- The survival head is piecewise-constant hazard with a fixed number of uniform time bins (P=8). This may be suboptimal for heterogeneous hazards; no ablations on bin count/placement (e.g., quantile/log time) are presented.\n- Competing risks are simplified to ‚Äúdeath only,‚Äù which is often inadequate for multi-outcome EHR settings; no cause-specific or multi-state modeling during pretraining.\n- The mismatch between a PEANN head during pretraining and a CoxPH head during adaptation could lead to inconsistencies; limited discussion on why PEANN wasn‚Äôt also used for adaptation.\n- Each task is modeled independently; any shared structure across outcomes (e.g., via multi-task survival heads) is not exploited.\n\n- External generalization is limited to a single diagnostic dataset (RSPECT) and only CT modality; no multi-institution evaluation for prognostic TTE transfer.\n- Overlap between pretraining tasks and evaluation tasks is likely (e.g., PH). While the split prevents leakage across sets, it would be more convincing to show results when excluding evaluation-identical codes from pretraining.\n- No ablation on the number of TTE tasks, selection heuristic variants, or label-density controls (e.g., holding label counts constant across visit vs TTE).\n- Multiple comparisons are performed, but it is unclear whether corrections (e.g., FDR) were applied; some tables include repeated placeholder-like values (e.g., 0.505) that merit clarification.\n- Calibration claims (IBS improvements) are relegated to the appendix; inclusion of main-text calibration plots/tables would strengthen the argument.\n\n- Several technical details are under-specified: precise definition of time origin, treatment of left truncation, interval construction, and handling of irregular follow-up.\n- The statement ‚Äúwe apply censorship at patient death‚Äù is ambiguous; for non-mortality tasks death is a competing risk, not simple censoring‚Äîplease clarify the exact handling.\n- Code links in the text appear as placeholders; explicit, functioning links are crucial for reproducibility.\n\n- Limited discussion of recent progression-aware SSL and survival heads for neural models beyond Cox/PE (e.g., DeepHit, discrete-time/PLH/PLD survival heads; survival modeling with piecewise-linear hazards).\n- Recent imaging-time-aware SSL approaches (e.g., TE-SSL for MRI, 3DTINC for OCT) are not positioned relative to this work; a brief comparison would strengthen context.\n- Survival models that couple interpretability and deep components (DeepPAMM/DeepPAM) are relevant and could inform head design and calibration.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe PEANN objective is appropriate for large-scale TTE with censoring; using a hazard-based piecewise exponential model avoids normalization pitfalls and provides a proper survival function.Uniform binning and a fixed P=8 may limit fidelity for tasks with early/late hazard spikes; consider data-driven binning (quantiles/log-time) or piecewise-linear hazards to improve flexibility and calibration.Treating death as the only competing risk for all tasks can bias estimates for outcomes where other competing events exist; consider cause-specific or Fine‚ÄìGray modeling in pretraining if computationally feasible.Modeling tasks independently overlooks cross-outcome correlations; multi-task survival heads or shared low-rank task parameterizations could yield further gains.\n- Experimental evaluation assessmentThe continued-pretraining baselines are well-chosen, especially base/visit which isolates the effect of future temporal supervision. However, a stronger label-density ablation is needed to rule out that gains are solely from having more labels (e.g., subsample TTE labels to match per-visit density).Provide an ablation on (i) number of TTE tasks (e.g., 2k/4k/8k), (ii) bin count/placement, and (iii) excluding any evaluation-identical codes from the pretraining set to assess transfer to unseen outcomes.Prognostic gains are compelling, but calibration is central for clinical use; move IBS results into the main text and add reliability diagrams and time-dependent calibration error (e.g., Nam‚ÄìD‚ÄôAgostino).Consider adding end-to-end fine-tuning on downstream TTE tasks to show that the pretrained representation continues to help when capacity is allowed to adapt, and report sample-efficiency curves.Statistical testing across many settings should include multiple-comparison correction. Clarify the seemingly repeated values (e.g., many 0.505s) and ensure reported CIs reflect non-parametric bootstrap variability per task.\n- Comparison with related work (using the summaries provided)Relative to TE-SSL (time/event-aware SSL on MRI) and 3DTINC (time-aware SSL on OCT), this work differs by using EHR-derived TTE supervision rather than purely imaging-based temporal pairing. A short discussion of these lines would position your contributions more clearly and suggest hybrid objectives (contrastive + TTE) for future work.Survival head alternatives (e.g., DeepHit, piecewise-linear hazard/density heads) could be referenced as potential replacements for PEANN that may improve calibration and flexibility.DeepPAMM/DeepPAM highlight semi-structured survival models that combine interpretable time terms with deep features; such designs could be relevant for adding smooth baseline/time-varying effects or for competing risks.MOTOR (structured EHR TTE foundation model) is properly cited; consider discussing how its low-rank task parameterization or task-selection safeguards against leakage could translate to imaging‚ÄìEHR pretraining.\n- Discussion of broader impact and significanceThe work addresses a meaningful clinical gap: prognostic imaging models often lack the temporal supervision needed to discover risk-related imaging biomarkers. Demonstrating large gains on C-index while preserving diagnostic performance is impactful.Ethical and equity considerations deserve more emphasis. EHR-derived supervision can encode access, utilization, and coding biases (e.g., disparate censoring, undertesting). Please report subgroup analyses (by age, sex, race/ethnicity, insurance) and fairness metrics, and discuss mitigation strategies.Releasing reproducible code/models on public data is valuable to the community; ensure links function and include scripts for EHR processing, task selection, and label construction to enable independent verification.\n\n- The PEANN objective is appropriate for large-scale TTE with censoring; using a hazard-based piecewise exponential model avoids normalization pitfalls and provides a proper survival function.\n- Uniform binning and a fixed P=8 may limit fidelity for tasks with early/late hazard spikes; consider data-driven binning (quantiles/log-time) or piecewise-linear hazards to improve flexibility and calibration.\n- Treating death as the only competing risk for all tasks can bias estimates for outcomes where other competing events exist; consider cause-specific or Fine‚ÄìGray modeling in pretraining if computationally feasible.\n- Modeling tasks independently overlooks cross-outcome correlations; multi-task survival heads or shared low-rank task parameterizations could yield further gains.\n\n- The continued-pretraining baselines are well-chosen, especially base/visit which isolates the effect of future temporal supervision. However, a stronger label-density ablation is needed to rule out that gains are solely from having more labels (e.g., subsample TTE labels to match per-visit density).\n- Provide an ablation on (i) number of TTE tasks (e.g., 2k/4k/8k), (ii) bin count/placement, and (iii) excluding any evaluation-identical codes from the pretraining set to assess transfer to unseen outcomes.\n- Prognostic gains are compelling, but calibration is central for clinical use; move IBS results into the main text and add reliability diagrams and time-dependent calibration error (e.g., Nam‚ÄìD‚ÄôAgostino).\n- Consider adding end-to-end fine-tuning on downstream TTE tasks to show that the pretrained representation continues to help when capacity is allowed to adapt, and report sample-efficiency curves.\n- Statistical testing across many settings should include multiple-comparison correction. Clarify the seemingly repeated values (e.g., many 0.505s) and ensure reported CIs reflect non-parametric bootstrap variability per task.\n\n- Relative to TE-SSL (time/event-aware SSL on MRI) and 3DTINC (time-aware SSL on OCT), this work differs by using EHR-derived TTE supervision rather than purely imaging-based temporal pairing. A short discussion of these lines would position your contributions more clearly and suggest hybrid objectives (contrastive + TTE) for future work.\n- Survival head alternatives (e.g., DeepHit, piecewise-linear hazard/density heads) could be referenced as potential replacements for PEANN that may improve calibration and flexibility.\n- DeepPAMM/DeepPAM highlight semi-structured survival models that combine interpretable time terms with deep features; such designs could be relevant for adding smooth baseline/time-varying effects or for competing risks.\n- MOTOR (structured EHR TTE foundation model) is properly cited; consider discussing how its low-rank task parameterization or task-selection safeguards against leakage could translate to imaging‚ÄìEHR pretraining.\n\n- The work addresses a meaningful clinical gap: prognostic imaging models often lack the temporal supervision needed to discover risk-related imaging biomarkers. Demonstrating large gains on C-index while preserving diagnostic performance is impactful.\n- Ethical and equity considerations deserve more emphasis. EHR-derived supervision can encode access, utilization, and coding biases (e.g., disparate censoring, undertesting). Please report subgroup analyses (by age, sex, race/ethnicity, insurance) and fairness metrics, and discuss mitigation strategies.\n- Releasing reproducible code/models on public data is valuable to the community; ensure links function and include scripts for EHR processing, task selection, and label construction to enable independent verification.\n\n### Questions for Authors\n\n- How did you ensure that pretraining did not include tasks identical to the downstream evaluation tasks (e.g., PH, readmission), or if included, that this did not unduly advantage TTE over baselines? Can you report results excluding any overlapping codes from pretraining?\n- Why choose uniform time bins and P=8 for PEANN? Did you try alternative bin counts or placements (e.g., quantile/log spacing) and piecewise-linear hazards? Please provide ablations and any effects on calibration (IBS).\n- How exactly are label counts in Figure 3 computed (per code, per interval, or per code-interval pair)? Could differences in label-density alone explain some of the gains versus per-visit? Can you include a control where TTE labels are subsampled to match per-visit density?\n- For non-mortality outcomes, how are death and other competing events handled? The phrase ‚Äúcensorship at patient death‚Äù suggests treating death as censoring, but clinically it is a competing risk. Please clarify and justify this choice.\n- What safeguards prevent information leakage from EHR-derived labels into the test split (e.g., overlapping longitudinal windows across patients, transfers across encounters)? Please detail the split strategy relative to patient timelines.\n- Could you share more details on the entropy-ranked vertex cover selection: inputs, hyperparameters, and any sensitivity to the task budget? How stable are the selected 8,192 tasks across random seeds?\n- Why did CoxPH (DeepSurv) outperform PEANN as a downstream probe, given PEANN was used in pretraining? Do you hypothesize a mismatch in objectives or implementation details?\n- Can you report main-text calibration results (IBS, calibration plots) and time-dependent C statistics to substantiate the claim of improved calibration?\n- Did you evaluate end-to-end fine-tuning of the encoder on downstream tasks, and few-shot regimes to test label efficiency? If so, how do results compare to frozen-probe evaluations?\n- What are the exact preprocessing steps for CT volumes (resampling, windowing, cropping) and how sensitive are results to these choices?\n\n### Overall Assessment\n\nThis paper addresses an important and timely problem: enabling 3D imaging encoders to learn prognostic biomarkers by injecting longitudinal temporal supervision from EHRs via a time-to-event objective. The approach is well-motivated, technically sound in broad strokes, and the empirical results on public datasets are encouraging, with sizable gains on prognostic metrics and no observed harm to diagnostic classification. The work is likely to be valuable to the community and pushes the field toward clinically useful imaging foundation models. To reach top-tier standards, I recommend strengthening the methodological clarity (binning, competing risks), adding key ablations (task overlap, label-density controls, task number/binning), reporting calibration in the main text, and broadening context against recent time-aware SSL and survival-head alternatives. With these additions and polished reproducibility (working code links), the contribution would be compelling for publication.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a time-to-event (TTE) pretraining strategy for 3D medical imaging encoders that leverages longitudinal supervision from electronic health records (EHRs) to learn prognostic representations. A piecewise exponential neural network (PEANN) survival head is attached to a 3D CT encoder and pretrained on 8,192 automatically selected TTE tasks derived from EHR data. The pretrained encoders are evaluated‚Äîwithout further fine-tuning‚Äîon public CT datasets (INSPECT and RSPECT), showing improved prognostic performance (AUROC, Harrell‚Äôs C-index) without loss in diagnostic accuracy. The paper is clearly written, well-motivated, and presents a rigorous empirical evaluation using fully public data, suggesting meaningful advances in prognostic imaging representation learning.  \n\n**Major Comments**  \n1. **Modeling Limitations:** The PEANN uses a fixed number (P=8) of uniform time bins, potentially limiting flexibility for heterogeneous hazards. No ablation on bin number or placement (e.g., quantile/log-time) is provided.  \n2. **Handling of Competing Risks:** Pretraining simplifies competing risks to a single ‚Äúdeath‚Äù event, which is insufficient for multi-outcome EHR settings. Cause-specific or multi-state modeling should be discussed.  \n3. **Objective Mismatch:** The shift from PEANN during pretraining to CoxPH (DeepSurv) during adaptation could cause inconsistencies; rationale for not using PEANN downstream is limited.  \n4. **Task Independence:** Each TTE task is modeled separately; shared-structure modeling via multi-task or low-rank survival heads could be advantageous.  \n5. **Experimental Scope:** Generalization is evaluated only on one external CT dataset. No ablation on task number, heuristic variants, or label-density control is shown; possible overlap between pretraining and evaluation tasks should be ruled out.  \n6. **Statistical and Calibration Reporting:** Multiple comparisons lack correction details (e.g., FDR). Calibration evidence is relegated to the appendix; main-text calibration plots and reliability diagrams are strongly recommended.  \n7. **Clarity and Reproducibility:** Definitions of time origin, left truncation handling, and death censorship are unclear. Code references are placeholders and must be functional for reproducibility.  \n8. **Contextualization:** The discussion omits related temporal SSL and survival-head methods (e.g., TE-SSL, 3DTINC, DeepHit, DeepPAMM). A concise positioning relative to these would improve context.  \n9. **Fairness Considerations:** EHR-derived supervision may encode demographic or access biases; subgroup and fairness analyses should be added to assess equity impacts.  \n\n**Minor Comments**  \n- Clarify label count computation and pretraining task selection stability.  \n- Correct repeated placeholder numeric values (e.g., 0.505) in tables.  \n- Explicitly describe CT preprocessing steps and parameter sensitivities.  \n- Consider including end-to-end fine-tuning and few-shot experiments to verify representation utility.  \n\n**Summary Paragraph**  \nOverall, this paper presents a technically interesting and well-motivated approach that bridges imaging and prognosis through TTE supervision. Its strengths lie in the innovative formulation, use of public datasets, and careful evaluation design. However, areas needing improvement include survival head flexibility, handling of competing risks, ablation completeness, calibration reporting, and clarity in methodological details. Addressing these issues would considerably enhance confidence in the contribution‚Äôs rigor and generalizability.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"TIME-TO-EVENT PRETRAINING FOR 3D MEDICAL IMAGING\"\n\n## 1. Opening Paragraph\n\nThe paper addresses a critical limitation in current medical imaging foundation models: the inability to capture temporal disease progression patterns needed for identifying prognostic pixel biomarkers. The authors identify what they term a \"missing context problem\" where existing self-supervised learning methods focus on local structural features but fail to link pixel information with future health outcomes due to the absence of longitudinal temporal context. To solve this, they propose a novel time-to-event (TTE) pretraining framework that leverages longitudinal electronic health records (EHRs) to provide rich temporal supervision signals. Using 18,945 chest CT scans (4.2 million 2D images) linked to EHR data with 225 million clinical events, the authors create 8,192 TTE pretraining tasks that capture when clinical events occur. Their approach demonstrates substantial improvements in prognostic prediction, achieving an average AUROC increase of 23.7% and a 29.4% gain in Harrell's C-index across 8 benchmark tasks, while maintaining diagnostic classification performance. The method also increases label density by 3x compared to prior approaches, enhancing data efficiency without requiring additional annotation effort.\n\n## 2. Major and Minor Comments\n\n**Major Strengths:**\n- The paper addresses a genuine and significant gap in medical imaging foundation models by incorporating temporal context from longitudinal EHR data, which is essential for clinical prognosis but largely overlooked in current approaches\n- The methodology is well-designed and grounded in established survival analysis techniques, with a thoughtful selection of 8,192 diverse pretraining tasks from medical ontologies\n- The comprehensive evaluation across three architectures (SwinUNETR, DenseNet, ResNet) and multiple metrics (AUROC, Harrell's C-index, Integrated Brier Score) provides compelling evidence of the approach's effectiveness\n- The authors have made significant contributions to reproducibility by releasing code and model checkpoints, and using public datasets that others can replicate\n\n**Major Limitations:**\n- The paper lacks comparison to more recent medical vision foundation models beyond Merlin (e.g., Med3D, MedCLIP), which limits the understanding of how TTE pretraining compares to state-of-the-art approaches\n- The computational requirements (80GB GPUs for SwinUNETR) may limit accessibility for many researchers, particularly in resource-constrained settings\n- The study is limited to chest CT scans only, with no validation on other imaging modalities or body regions, raising questions about generalizability\n- The ethical considerations regarding potential biases in the EHR data and how they might affect model performance across subgroups could be more thoroughly addressed\n\n**Minor Strengths:**\n- The GradCAM visualizations effectively demonstrate improved biomarker localization in the TTE-pretrained models compared to baselines\n- The detailed ablation studies (including subgroup performance analysis) strengthen the validity of the claims and show consistent improvements across demographic categories\n- The thorough statistical analysis with confidence intervals and significance testing adds substantial credibility to the findings\n\n**Minor Limitations:**\n- Some figures could be better labeled (e.g., Figure 3 lacks y-axis labels, making interpretation difficult)\n- The description of medical code selection criteria could be more detailed to help readers understand the representativeness of the 8,192 tasks\n- The explanation of why piecewise exponential neural networks were chosen over other survival models could be expanded for readers less familiar with survival analysis\n\n## 3. Summary Evaluation Against TMI Criteria\n\n**Significance:** This work addresses an important gap in medical imaging AI - the inability of current foundation models to capture temporal disease progression despite its clinical importance. By linking imaging biomarkers to future outcomes, this approach could significantly advance clinical risk prediction tools. The demonstrated 23.7% AUROC improvement across diverse prognostic tasks represents a meaningful advance with potential clinical impact, particularly for early intervention in disease progression.\n\n**Innovation:** The key innovation is the novel integration of time-to-event analysis with 3D medical image pretraining at scale, which has not been previously explored in this manner. The method effectively converts longitudinal EHR data into rich temporal supervision signals, creating 8,192 pretraining tasks that capture complex temporal patterns. The approach also innovatively handles right-censoring, a common challenge in clinical data that is often overlooked in imaging studies.\n\n**Evaluation:** The evaluation is exceptionally thorough across multiple architectures, metrics, and task types (prognostic classification, time-to-event prediction, diagnostic classification). The authors provide extensive statistical analysis with confidence intervals and significance testing, and the subgroup analysis demonstrates consistent improvements across demographic categories. The comparison against multiple baselines (Random Init., Base, MT, Visit) provides clear evidence of the specific contribution of TTE pretraining.\n\n**Reproducibility:** The paper provides excellent reproducibility with public datasets (INSPECT, RSPECT), detailed hyperparameters in Appendix F, and release of code and model checkpoints. The authors' commitment to transparency is further demonstrated through comprehensive appendices covering data preprocessing, model architectures, and evaluation metrics. The use of established medical datasets facilitates replication by other researchers.\n\n## 4. Decision Recommendation\n\nI recommend **acceptance with minor revisions**. This paper makes a significant contribution to the field of medical imaging foundation models by addressing a critical limitation in current approaches. The methodology is sound, the evaluation is exceptionally thorough, and the results are compelling across multiple metrics and architectures. While there are minor limitations regarding comparisons to more recent models and scope of validation, these do not undermine the core contribution. The work is well-positioned for TMI as it addresses a key challenge in medical imaging AI with practical clinical implications. The authors should address the minor limitations noted above, particularly improving figure labeling and providing more detail about medical code selection, but the core contributions and findings are strong enough for acceptance with these relatively straightforward revisions. The paper's emphasis on reproducibility and open science further strengthens its suitability for TMI.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript, *‚ÄúTime-to-Event Pretraining for 3D Medical Imaging,‚Äù* aims to address a key limitation of current medical imaging foundation models‚Äîthe lack of temporal context necessary to capture disease progression and prognostic biomarkers. The authors introduce a time-to-event (TTE) pretraining framework that integrates longitudinal electronic health record (EHR) data with 3D chest CT scans to provide temporal supervision signals. Using 18,945 studies linked to 225 million clinical events, they create 8,192 TTE pretraining tasks representing clinical event timing. The approach shows substantial improvement in prognostic prediction (23.7% AUROC, 29.4% Harrell‚Äôs C-index gains) across multiple benchmarks while maintaining diagnostic accuracy. Overall, the study presents a technically detailed and empirically strong contribution that is clearly written and reproducible.  \n\n---\n\n**Major Comments**  \n1. **Comparative Scope:** The paper does not include comparisons with several recent medical imaging foundation models beyond Merlin (e.g., Med3D, MedCLIP), which limits understanding of relative performance against the state-of-the-art.  \n2. **Computational Demand:** The method requires substantial GPU resources (e.g., 80 GB), potentially reducing accessibility for smaller research groups.  \n3. **Domain Generalization:** Validation is limited to chest CT scans, leaving generalizability to other imaging modalities untested.  \n4. **Ethical Considerations:** Potential biases in EHR data and their impact on model performance across demographic subgroups are only briefly addressed.  \n\n---\n\n**Minor Comments**  \n- Improve figure labeling, particularly missing y-axis information in Figure 3.  \n- Clarify medical code selection criteria to illustrate how the 8,192 tasks were derived and whether they represent diverse clinical conditions.  \n- Expand on the rationale for using piecewise exponential neural networks over alternative survival models.  \n- GradCAM visualizations effectively demonstrate enhanced biomarker localization; this strength could be briefly highlighted in the main text.  \n- Ablation and subgroup analyses are robust but could be summarized more concisely for readability.  \n\n---\n\n**Summary Paragraph**  \nThis paper provides a substantial contribution by incorporating temporal disease progression into self-supervised 3D imaging pretraining, a gap in current foundation models. Its methodological innovation‚Äîlinking imaging features with time-to-event targets from large-scale EHRs‚Äîis well-executed and convincingly evaluated across multiple architectures and performance metrics. The work is further strengthened by detailed statistical analyses, transparent reporting, and publicly available code and data resources. Key limitations involve restricted model comparisons, computational cost, modality scope, and limited discussion of potential bias. Nonetheless, these do not detract from the paper‚Äôs core scientific value.  \n\n---\n\n**Decision Recommendation**  \n**Recommendation: Accept with Minor Revisions.**  \nThe manuscript offers a rigorous, innovative, and reproducible contribution to medical imaging AI. Minor revisions should focus on expanding baseline comparisons, improving figure clarity, and providing additional detail on task selection and model explanation.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the ‚Äúmissing context problem‚Äù in 3‚ÄëD medical imaging, namely that most self‚Äësupervised approaches ignore the longitudinal information required to learn prognostic pixel‚Äëlevel biomarkers. Using a publicly available cohort of 18,945 chest CT scans (‚âà4.2‚ÄØmillion 2‚ÄëD slices) linked to electronic health records that contain 225‚ÄØmillion clinical events and a median follow‚Äëup of five years, the authors introduce a pre‚Äëtraining strategy termed time‚Äëto‚Äëevent (TTE) pre‚Äëtraining. In this framework, EHR codes are transformed into thousands of survival tasks and a 3‚ÄëD vision encoder is trained with a piecewise‚Äëexponential neural network loss that naturally handles right‚Äëcensoring. After continued pre‚Äëtraining, the frozen encoder is fine‚Äëtuned with classification or survival heads for downstream tasks. The reported results show average gains of 23.7‚ÄØ% in AUROC and 29.4‚ÄØ% in Harrell‚Äôs C‚Äëindex across eight benchmark outcome‚Äëprediction tasks, while preserving performance on diagnostic classification. The primary contribution is the first large‚Äëscale evaluation of survival‚Äëbased pre‚Äëtraining for 3‚ÄëD medical imaging models.\n\n---\n\n## General feedback  \n\n*Significance*: Integrating long‚Äëterm EHR outcomes into image pre‚Äëtraining directly addresses a key limitation of current prognosis‚Äëoriented AI and could enable discovery of pixel‚Äëlevel risk biomarkers.  \n\n*Innovation*: The combination of (i) massive multi‚Äëtask TTE supervision through a piecewise‚Äëexponential approach, (ii) ontology‚Äëaware conditional‚Äëentropy task selection, and (iii) continued pre‚Äëtraining of a 3‚ÄëD encoder is novel relative to existing self‚Äësupervised or contrastive methods.  \n\n*Evaluation*: Experiments are conducted on two public datasets (INSPECT, RSPECT) and eight downstream TTE tasks, with results reported as AUROC, C‚Äëindex, and Integrated Brier Score (Figures‚ÄØ6‚Äì8, Tables‚ÄØ2‚Äë5). However, the description of baseline configurations, statistical significance testing, and external validation lacks sufficient detail.  \n\n*Reproducibility*: The authors provide code and model checkpoints and refer to preprocessing details in Appendix‚ÄØA. Nonetheless, crucial training hyper‚Äëparameters, the exact list of tasks, and data‚Äësplitting procedures are not fully disclosed, which hampers full replication.\n\n---\n\n## Specific comments/critiques  \n\n* The manuscript does not identify the precise self‚Äësupervised baselines (e.g., MAE, contrastive, Merlin) nor their training regimes and hyper‚Äëparameters. *(Figure‚ÄØ6‚Äë8)*  \n* No confidence intervals or statistical tests (e.g., paired t‚Äëtests) accompany the reported AUROC and C‚Äëindex improvements, leaving their significance unclear. *(Section‚ÄØ5)*  \n* The entropy‚Äëbased selection of the 8‚ÄØ192 TTE tasks is described only at a high level; the final code list, budget, entropy threshold, and frequency distribution are omitted. *(Figure‚ÄØ2, Appendix‚ÄØB)*  \n* There is no ablation study on the number of piecewise‚Äëexponential bins (currently set to eight), so the effect of this design choice remains unknown. *(Section‚ÄØ4)*  \n* Calibration gains are presented via Integrated Brier Score, but reliability diagrams or other quantitative calibration analyses are absent. *(Figure‚ÄØ8)*  \n* The ‚Äúexternal tasks‚Äù (eight diagnostic tasks) are mentioned without indicating whether they come from a different institution, modality, or temporal split, which limits assessment of generalisation. *(Figure‚ÄØ6)*  \n* Details on training/validation/test splits, potential patient overlap between INSPECT and RSPECT, and the exact INSPECT version used are not provided, impeding reproducibility. *(Table‚ÄØ1, not shown)*  \n* Computational cost is limited to node specifications; wall‚Äëclock time, GPU‚Äëhours, batch size, and learning‚Äërate schedule are not reported. *(Section‚ÄØ5)*  \n* Only death is treated as a competing risk; many clinical events have additional competing risks that are ignored, potentially biasing survival estimates. *(Section‚ÄØ5)*  \n* Grad‚ÄëCAM visualisations (Figure‚ÄØ20) are offered as interpretability evidence, yet no quantitative analysis of biomarker relevance or validation of the highlighted regions is presented. *(Figure‚ÄØ20)*\n\n---\n\n## A suggested decision  \n\nMajor Revision ‚Äì The manuscript currently lacks essential baseline comparisons and statistical analyses; substantial revisions are required before it can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the ‚Äúmissing context problem‚Äù in three‚Äëdimensional medical imaging by proposing a pre‚Äëtraining strategy that incorporates longitudinal electronic health record (EHR) information to learn prognostic, pixel‚Äëlevel biomarkers. Using nearly 19,000 chest CT scans linked to extensive clinical data, the authors introduce a time‚Äëto‚Äëevent (TTE) pre‚Äëtraining framework in which EHR codes are transformed into survival tasks and a 3‚ÄëD encoder is trained with a piecewise‚Äëexponential neural network loss. After pre‚Äëtraining, the frozen encoder is fine‚Äëtuned for various prognostic and diagnostic tasks, showing substantial gains in AUROC and C‚Äëindex across multiple benchmarks. Overall, the manuscript presents a technically ambitious approach with potentially high clinical relevance, though aspects of evaluation and reproducibility remain insufficiently documented.  \n\n**Major Comments**  \n1. **Baseline Definition and Comparisons** ‚Äì The manuscript does not clearly specify which self‚Äësupervised baselines (e.g., MAE, contrastive, Merlin) were included or their training configurations, making it difficult to interpret comparative performance.  \n2. **Statistical Significance** ‚Äì Reported improvements in AUROC and C‚Äëindex lack confidence intervals or hypothesis testing (e.g., paired t‚Äëtests), leaving the robustness of gains uncertain.  \n3. **Task Selection Detail** ‚Äì The process for entropy‚Äëbased selection of the 8,192 TTE tasks is inadequately described; key elements such as final code list, thresholds, and distribution are missing.  \n4. **Ablation on Bin Number** ‚Äì No ablation is conducted on the number of piecewise‚Äëexponential bins, preventing assessment of this modeling choice.  \n5. **Calibration and Reliability** ‚Äì Calibration performance is summarized only via Integrated Brier Score, without reliability diagrams or other quantitative analyses.  \n6. **External Task Definition** ‚Äì The ‚Äúexternal‚Äù diagnostic tasks are insufficiently described; the dataset origins, modalities, and time splits are unclear, limiting claims of generalization.  \n7. **Data and Split Transparency** ‚Äì Precise training/validation/test splits, overlap between INSPECT and RSPECT, and dataset versions are omitted, impeding reproducibility.  \n8. **Computational Reporting** ‚Äì Resource details are limited; wall‚Äëclock time, GPU‚Äëhours, batch sizes, and learning‚Äërate schedules should be provided.  \n9. **Competing Risks** ‚Äì Only death is modeled as a competing risk, though many clinical outcomes may have additional competing events that could bias survival estimates.  \n10. **Interpretability Analysis** ‚Äì Grad‚ÄëCAM visualizations are presented but lack quantitative assessment of biomarker relevance or validation of highlighted regions.  \n\n**Minor Comments**  \n- Clarify figure and table references corresponding to major analyses (Figures‚ÄØ6‚Äì8,‚ÄØ20; Tables‚ÄØ1‚Äì5).  \n- Verify consistent terminology for datasets and TTE task descriptions.  \n- Provide precise hyper‚Äëparameter and preprocessing details in the Appendix for full reproducibility.  \n\n**Summary Paragraph**  \nThis work introduces an original survival‚Äëbased pre‚Äëtraining paradigm that meaningfully integrates longitudinal EHR data with 3‚ÄëD imaging, representing a promising contribution toward prognosis‚Äëaware representations. However, the current version omits key experimental details, lacks statistical validation of results, and provides limited transparency in reproducibility and calibration reporting. These omissions weaken confidence in the empirical claims despite the conceptual innovation and apparent performance improvements.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial revisions addressing baseline specification, statistical rigor, methodological transparency, and reproducibility are necessary prior to consideration for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Alejandro Lozano",
      "Curtis Langlotz",
      "Ethan Steinberg",
      "Jason Alan Fries",
      "Jeya Maria Jose Valanarasu",
      "Louis Blankemeier",
      "Nigam Shah",
      "Akshay S Chaudhari",
      "Zepeng Frazier Huo"
    ],
    "url": "pdfs/iclr.cc-2025-conference_ab67303155dc14cfbb1febaf43b5bf38ae9bd5b0.pdf",
    "remote_url": "https://openreview.net/pdf/ab67303155dc14cfbb1febaf43b5bf38ae9bd5b0.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "Multimodal Large Language Model",
      "Biomedicine",
      "Region-Text"
    ],
    "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. \nMost current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence.\nTo mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans.\nTo achieve it, we first formulate \\textbf{Region-Centric tasks} and construct a \\textbf{large-scale dataset, MedRegInstruct,} to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a \\textbf{Region-Aware medical MLLM, MedRegA}, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. The codes and model will be made publicly available.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a new large-scale, multi-site, medical visual-question answering (VQA) dataset, called MedRegInstruct, with fine-grained region-specific annotation and multiple corresponding region-centric tasks. The proposed dataset contains multiple region-centric evaluation tasks including, region-to-text generation, text-to-region detection, and grounded medical report generation. The paper further proposed a new region-aware multi-modal large language model (MLLM), called, MedRegA, pre-trained with the proposed dataset and regional chain of thought (CoT). The proposed method outperforms multiple existing SOTA medical VQA models on the regional-centric evaluation.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n1. The proposed new region-centric medical dataset MedRegInstruct addressed the need for fine-grained, region-level, training and evaluation datasets in the medical domain. It is novel to the field and will help the development of the field as provides a more fine-grained training and evaluation annotation. It is proven that the method pre-trained with the new dataset can demonstrate more robust performance in the targeted region-specific tasks and also other related medical tasks. \n2. The paper has provided enough details about the data curation and evaluation, including the process of question-answering pair generation from medical data with region annotation, a prompt for each downstream evaluation, and detailed settings for each task. The author also seems to claim they will release the code and pre-trained model later.\n3. The paper is overall well written with nice figures and a clean presentation. It is easy to follow even if there are many details in it.\n\n### Weaknesses\n\n1. While the proposed dataset is novel and important to the field, the corresponding MedRegA model is not that novel, though I hate to say that, in comparison. The overall model design is basically the same as the base InternVL model and the major change is the training scheme and prompt formulation with the new dataset, which can be extended to other medical VQL models as well. While the author further proposed single-step regional CoT in the paper, it is still relatively straightforward. **Yet**, this is not a major drawback as the performance of this model demonstrates the superiority of training with the proposed dataset.\n2. Similarly, it seems that the baselines used in the evaluation are not all fine-tuned with the proposed dataset, which enables the model to understand regional information. Comparing the model trained on the proposed dataset with baselines that are not trained on this dataset is not very fair, to some extent. It would be interesting to see how well each baseline would improve if they were fine-tuned on the proposed dataset, even if just in a few-shot manner.\n3. Another concern about this paper is the quality of the data. Most of the regional bounding box-text pairs in the dataset were taken from the existing dataset and generated from an automated rule-based system and an LLM. The evaluation of the data quality is missing in this paper, which is very critical as the main purpose is **fine-grained regional evaluation,** where the quality of the bounding box and the correctness of the corresponding text description are very important. It would be better if some sort of data quality check could be done before releasing it to the public.\n\n### Questions\n\nOverall, this is a good paper with solid contributions and it has a significant meaning to the field. Still, the reviewer has a few questions here.\n\n1. How is the English version of the text report/caption generated for the in-house Chinese data? What kind of translation was used here?\n2. According to the ‚ÄúStructure Detection‚Äù section on page 5, the structure bounding box in the dataset was generated with a fine-tuned MLLM on the existing Region-Text dataset given the corresponding organ prompt. The reviewer wonders if any kind of evaluation was done on this fine-tuned MLLM, how accurate is the extract bounding box? And why not train a more straightforward detection model for this purpose? Considering that the specific detection model usually performs better than a general MLLM.\n3. Also, as mentioned above, the reviewer wonders how much will the other baseline medical MLLMs improve if they were trained with the proposed dataset. The comparison between InternVL and MedRegA in the paper is not very fair as the InternVL was only trained with natural image-text pairs. The reviewer understands it may take tons of extra time/money to conduct such an evaluation, but it would still be an interesting question to explore.\n4. According to the abstract, the code and model will be released to the public, but the reviewer wonders if the data will be released as well. This is pretty critical as the dataset is the major contribution of the paper. The answer to this question will influence the reviewer's final score for the paper.\n\n### Flag For Ethics Review\n\n- Yes, Privacy, security and safety\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe paper proposed a new medical visual question-answering dataset with in-house medical imaging data from a Chinese hospital. Though the author has an ethics statement section in the paper, further ethics evaluation may be needed to ensure there is no critical ethical issue.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegInstruct*, a large-scale, multi-site medical visual question answering (VQA) dataset designed for fine-grained, region-level annotation and multiple region-centric tasks such as region-to-text generation, text-to-region detection, and grounded report generation. It also presents *MedRegA*, a region-aware multi-modal large language model (MLLM) pre-trained on the proposed dataset with a regional chain of thought approach. The paper reports that MedRegA surpasses several state-of-the-art medical VQA systems on region-centric benchmarks. Overall, the manuscript is clearly written, with thorough methodological details and high-quality figures that support readability.\n\n---\n\n**Major Comments**  \n1. **Novelty of the Model:** While the dataset is genuinely novel and valuable, the MedRegA model itself shows limited architectural innovation compared to the base InternVL model. The primary differences lie in the training scheme and prompt design using the new dataset. The proposed single-step regional chain-of-thought remains conceptually straightforward.  \n2. **Fairness of Baseline Comparisons:** The reported performance comparisons may not be fully equitable, as baseline models were not fine-tuned on the new dataset. Evaluating those baselines after fine-tuning or in a few-shot setting would provide a more balanced comparison.  \n3. **Data Quality and Evaluation:** The dataset integrates bounding box‚Äìtext pairs derived from existing datasets and automated generation using a rule-based pipeline and an LLM, yet it lacks explicit data quality assessment. Given the paper‚Äôs focus on fine-grained regional evaluation, validation of bounding box accuracy and textual correctness is essential before public release.  \n4. **Data Release and Ethics:** The manuscript notes forthcoming release of code and model but does not confirm data release. Clarification on data availability is critical, as the dataset constitutes the core contribution. Furthermore, due diligence regarding privacy and ethical handling of in-house medical data from a Chinese hospital should be ensured.\n\n---\n\n**Minor Comments**  \n- Clarify how English captions were generated from Chinese medical reports, specifying the translation process.  \n- Provide additional details on the fine-tuned MLLM used for structure detection, including any accuracy evaluation.  \n- Figures and descriptions are generally clear; minor improvements could include explicit labeling of region annotations.  \n\n---\n\n**Summary Paragraph**  \nThe work substantially contributes a well-structured, region-centric medical VQA dataset and demonstrates its usefulness through strong performance gains. The technical innovation of the accompanying model is incremental, and issues of data quality, fairness in comparisons, and dataset release remain unresolved. Despite these limitations, the paper is methodologically solid, carefully written, and addresses an important gap in medical AI evaluation at the regional level.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe paper makes a meaningful contribution through dataset creation, but major concerns regarding baseline fairness, data quality validation, and ethical/data release aspects must be addressed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this work, the authors tackle the current problem in MLLMs (multimodal large language models) of how they don't necessarily \"focus\" on particular regions, but rather take the entire image as context and then have to solve a variety of downstream tasks. To address this problem they offer two main sets of contributions. The first, is the formulation of several region specific tasks, namely region-to-text identification, text-to-region detection, and grounded report generation, which they combine into a dataset called MedRegInstruct. The second is the training strategy and modeling paradigm for a MLLM they propose, which they refer to as MedRegA. MedRegA is a bilingual, as it was trained on both English and Chinese paired datapoints, model that they apply to this larger dataset they prepare in comparison to currently available medical MLLMs.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n‚Ä¢ The paper really introduces another method of interaction, in the bounding boxes, that can be quite useful as MedRegA is able to reason about specific regions. This is an immensely useful tool for narrowing of problems as opposed to current trained MLLMs have to do.\n\n‚Ä¢ The proposed training strategy for learning how to incorporate the bounding boxes is simple and scalable, seems like it could be generally applied to other strategies that want to apply bounding boxes.\n\n‚Ä¢ Regional CoT makes a lot of sense intuitively and provides a nice bias for solving region specific problems.\n\n‚Ä¢ MedRegA has very strong performance on the chosen set of benchmarks and metrics in comparison to existing state of the art methods.\n\n### Weaknesses\n\n‚Ä¢ The section of the report grounding dataset is quite confusing as to how its constructed, and how much it relies on the models it uses for automation for providing correct labels/groundtruth. For example, is InternLM perfect at sub-select different parts of the reports? \n\n‚Ä¢ The paper claims to work across a broad set of modalities, but when considering the grounding dataset it seems that it only considers, as other MLLMs do, Chest Xrays?\n\n‚Ä¢ It's unclear if the comparison to existing baselines are exactly fair as the trained MedRegA has access to more data. In fact, compared to the baseline of InternLM, it should be strictly better because MedRegA is trained with InternLM as an initialization. While it's true that the baselines don't provide a mechanism for highlighting particular regions, they still should be adapted to the new data that the authors are evaluating on.\n\n### Questions\n\n‚Ä¢ What was the filtering criterion for reducing SA-Med2D-20M?\n\n‚Ä¢ Is there a noticeable performance difference between English and Chinese tasks? Is there paired data that this can be evaluated on? ‚Ä¢ ‚Ä¢ ‚Ä¢ \n\n‚Ä¢ Overall, it is unclear from the paper why exactly these two particular languages are used, other than being able to source data for both, rather than just arbitrary languages with medical report data.\n\n‚Ä¢ Is the MedRegInstruct dataset planning to be release?\nFor the Region-Text dataset, why is the case that there needs to be this split into half for region-to-text and half for text-to-region? Can't every datapoint server the purpose of being either during training?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MedRegA**, a bilingual multimodal large language model (MLLM) designed to enable region-specific reasoning in medical imaging. To address the limitation that current MLLMs process entire images without explicit spatial focus, the authors define several localized tasks‚Äîregion-to-text identification, text-to-region detection, and grounded report generation‚Äîcollectively forming the **MedRegInstruct** dataset. They further propose a training strategy and modeling paradigm that incorporate bounding-box‚Äìbased interaction and region-aware chain-of-thought (CoT) reasoning. The manuscript is generally clear and well-presented, with a coherent motivation and well-defined contributions.  \n\n**Major Comments**  \n1. **Dataset construction and clarity:** The section describing the report grounding dataset is confusing. The process for dataset creation, especially the degree of reliance on automated models for annotation (e.g., InternLM), should be clarified. It is not evident how accurate or reliable these automated annotations are.  \n2. **Scope of modalities:** Although the paper claims to address a broad range of modalities, it appears that the dataset and evaluations are limited primarily to chest X-rays, similar to other medical MLLMs. This discrepancy between claims and actual scope should be addressed.  \n3. **Fairness of comparisons:** The comparison to baseline models may not be entirely fair since MedRegA benefits from additional data and from initialization with InternLM, one of the baselines. This advantage could partially account for the reported performance gains.  \n4. **Language choice and bilingual evaluation:** The motivation for choosing English and Chinese as the two training languages is not clearly justified beyond data availability. It would be helpful to provide evidence of bilingual performance or cross-lingual evaluation.  \n\n**Minor Comments**  \n- Clarify the filtering criteria used for reducing the SA‚ÄëMed2D‚Äë20M dataset.  \n- Specify whether the MedRegInstruct dataset will be released publicly.  \n- Explain why region‚Äëtext pairs are split evenly into region‚Äëto‚Äëtext and text‚Äëto‚Äëregion samples rather than using a unified setup.  \n- Minor issues of clarity exist in the dataset description section; reorganization could improve readability.  \n\n**Summary Paragraph**  \nOverall, this work offers a valuable contribution by introducing region-focused interaction mechanisms in medical MLLMs and demonstrating strong benchmark performance. The approach is intuitively appealing and methodologically clear, particularly in the simplicity of its bounding-box integration. However, the paper would benefit from clarification of dataset construction, justification of modality and language scope, and a more balanced evaluation setup.  \n\n**Decision Recommendation**  \n**Minor Revision** ‚Äî The paper presents solid contributions but requires additional clarification and justification on dataset and evaluation aspects before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work introduces a novel region-centric task and presents a large-scale Chinese medical dataset named MedRegInstruct. To address this new task, the authors developed a bilingual multimodal large language model, MedRegA, which outperforms baseline methods across most evaluation metrics.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.\tThe paper is well-organized.\n2.\tThis work introduces a novel task alongside a large-scale medical dataset.\n3.\tThe authors propose a new model that significantly outperforms other models across the majority of datasets and evaluation metrics, particularly for the newly proposed task.\n\n### Weaknesses\n\n1.\tThe article‚Äôs description of the model structure in the main text is incomplete, relying heavily on Figure 4 to convey the model‚Äôs architecture. For example, the structure of the encoder and tokenizer remains ambiguous. The authors should clarify which components utilize existing models, which are fine-tuned, and which are trained from scratch.\n2.\tThe fairness of the model comparison is questionable. In Table 1, only a subset of models has been fine-tuned based on the test dataset, leaving others unmodified. Is this due to an inability to fine-tune certain models, or for another reason? The authors should provide detailed criteria for choosing to fine-tune specific models and address the fairness of this experimental design.\n3.\tOn the newly proposed task, the authors‚Äô model demonstrates a substantial performance advantage. They mention that existing open-source models cannot accurately capture coordinates, but it would be insightful to know if fine-tuning on the authors‚Äô proposed dataset could improve this. If fine-tuning other models would yield better results, the authors should do so for a more persuasive comparison. Conversely, if fine-tuning has no impact, the authors should explain the model‚Äôs unique advancements to provide deeper insights.\n4.\tThe role of the CoT component is somewhat unclear, with insufficient ablation experiments to support its effectiveness. The authors should conduct a more specific ablation study to illustrate the impact of CoT rather than merely reporting improved results with its inclusion.\n5.\tWhile the authors claim that the proposed model can perform diagnostic tasks, there is no clear evaluation or user study provided. A brief evaluation or explanation of the diagnostic results would add clarity.\n6.\tSeveral prior studies focus on medical image segmentation, which aligns well with the proposed region-centric task. Have the authors considered using such methods as feature extractors, backbones, or even as comparative models?\n7.\tThe experimental results on report generation are insufficient, as the authors rely solely on descriptive analysis without quantitative evaluation or user studies. Similar to point 5, if the authors wish to validate effectiveness without explicit evaluation metrics, a user study would provide valuable support.\n\n### Questions\n\n1.\tPlease explain why certain models were fine-tuned in the comparison while others were not.\n2.\tThe authors state that adding CoT improves model performance; however, no comprehensive ablation experiments were conducted to confirm this.\n3.\tHave the authors considered comparisons using other large models as the backbone?\n4.\tDo the authors plan to open-source this dataset?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel region-centric medical imaging task and presents a corresponding large-scale Chinese dataset, *MedRegInstruct*. To address this task, the authors propose *MedRegA*, a bilingual multimodal large language model that reportedly surpasses baseline methods across most evaluation metrics. The paper is well-organized and presents both a new problem definition and technical solution with promising experimental outcomes.  \n\n**Major Comments**  \n1. **Model Architecture Clarity** ‚Äì The description of the model structure is incomplete, with excessive reliance on Figure 4. Key details about the encoder and tokenizer remain vague. The authors should specify which components use pre-trained models, which are fine-tuned, and which are trained from scratch.  \n2. **Fairness of Model Comparison** ‚Äì The comparison in Table 1 appears inconsistent: only some baseline models were fine-tuned on the test dataset, while others were not. The rationale for this selective fine-tuning should be explained, addressing whether certain models could not be fine-tuned or were intentionally left unchanged.  \n3. **Evaluation of Fine-Tuning Effects** ‚Äì For the new task, the proposed model shows a strong advantage, but it is unclear if fine-tuning existing open-source models on the proposed dataset would narrow this gap. The authors should either conduct such experiments or justify why fine-tuning others is infeasible.  \n4. **Role of Chain-of-Thought (CoT)** ‚Äì The contribution of the CoT component is unclear. More detailed ablation studies are needed to isolate and verify its impact beyond reporting overall performance improvements.  \n5. **Evaluation of Diagnostic Capability** ‚Äì The paper claims diagnostic functionality without supporting evaluations or user studies. Including even a brief diagnostic analysis or user study would help substantiate this claim.  \n6. **Comparison with Segmentation-Based Methods** ‚Äì As many related works address medical image segmentation, it would be valuable to explore these as backbones, feature extractors, or comparative baselines.  \n7. **Report Generation Evaluation** ‚Äì The report generation results rely on qualitative descriptions without quantitative or user-based evaluation. Incorporating such analyses would strengthen claims of effectiveness.  \n\n**Minor Comments**  \n- Ensure that figure-based information (e.g., Figure 4) is fully supported by explanatory text.  \n- Clarify dataset release plans and accessibility.  \n\n**Summary Paragraph**  \nOverall, the paper proposes an innovative dataset and model for a new medical imaging task and demonstrates promising performance gains. However, issues with experimental fairness, incomplete model description, limited ablation studies, and insufficient evaluation for diagnostic and generative capabilities substantially limit interpretability and persuasiveness. Addressing these would solidify the contribution and reproducibility of the work.  \n\n**Decision Recommendation**  \n**Major Revision.** The contribution is novel and potentially impactful, but the methodological and evaluative shortcomings require substantial clarification and additional experiments before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase 1 ‚Äì Analytical and Scope Audit**\n\n### **1. Scope & Relevance to TMI**\n‚úÖ **Within scope.**  \nThis paper presents a **novel methodological contribution** centered on enhancing medical multimodal large language models (MLLMs) with *region-centric understanding*. The work integrates spatial localization (region-to-text, text-to-region, grounded report generation) into a large bilingual medical foundation model. The emphasis on interpretability, spatial grounding, structured dataset design, and model formulation positions it squarely within **medical imaging methodology** ‚Äî particularly image understanding and vision-language modeling. Suitable for IEEE *TMI*.\n\n### **2. Novelty & Contribution Level**\nThe introduction of a **region-aware medical MLLM** and the formulation of **region-centric tasks** (MedRegInstruct dataset, Region-CoT mechanism, and Region-Aligned evaluation scheme) represent a step beyond prior ‚Äúglobal‚Äù or holistic MLLMs such as Med-Flamingo, RadFM, and MedDr.  \nKey conceptual advances:\n- Structured modeling of image-region-text alignment;\n- Semi-automated curation pipeline integrating bounding-box annotations into bilingual reports;\n- Introduction of interpretable, region-aware grounding in multimodal tasks.  \nThe novelty is incremental relative to major foundation-model efforts but methodologically strong and more interpretable ‚Äî at or just above *TMI‚Äôs novelty threshold.*\n\n### **3. Technical and Experimental Rigor**\n- **Dataset construction:** detailed description, clear provenance (public + institutional data), semi-automatic annotation, and expert validation (‚âà93% textual accuracy, 72% bounding-box accuracy).  \n- **Modeling:** builds upon InternVL foundation; clearly stated two-stage training (alignment ‚Üí instruction tuning); includes mathematical formulations of tasks and explicit representation (<box> tags).  \n- **Evaluation:** extensive benchmarking across >15 datasets, region-specific metrics, and comparisons with six baselines.  \n- Potential gaps: limited discussion of generalization to unseen modalities, ablation of dataset quality, and clinical verification of generated regions.\n\n### **4. Clarity and Presentation**\n- Generally **well written and organized**, with coherent narrative from motivation through experiments.\n- Minor grammatical issues, overuse of long sentences, and inconsistent figure captions (e.g., Figure 5/6 labeling).\n- Results tables are comprehensive but dense ‚Äî clarity could improve via summarizing key metrics.\n\n### **5. Ethical and Reproducibility Compliance**\n- Proper anonymization, IRB-like statement provided, clear delineation of public vs. private data.\n- Ethical practices well documented.\n- Reproducibility: model and code availability implied but not confirmed; specifying release plan for MedRegInstruct (public vs. derived subsets) would be helpful.\n\n---\n\n## **Phase 2 ‚Äì IEEE TMI Review Report**\n\n### **1. Summary**\nThe manuscript introduces **MedRegA**, a bilingual multimodal large language model for medical imaging that explicitly models **region-level understanding**. The authors define three region-centric tasks ‚Äî region-to-text identification, text-to-region detection, and grounded report generation ‚Äî and create the **MedRegInstruct** dataset (‚âà790K samples). Leveraging a hybrid of public and in-house clinical data, the model integrates bounding-box‚Äìbased regional encoding, a *Regional Chain-of-Thought* reasoning strategy, and a *Region-Aligned evaluation framework*. Extensive experiments demonstrate superior performance over prior medical MLLMs (MedDr, RadFM, BiomedGPT, etc.) across vision-language and classification tasks, with enhanced interpretability via spatial grounding.\n\n### **2. Strengths**\n- **Methodological novelty:** clear transition from global to region-aware MLLMs in medicine.\n- **Dataset innovation:** semi-automatic bilingual dataset that supports grounding and interpretability.\n- **Comprehensive evaluation:** large experimental suite across multiple modalities and languages.\n- **Clinical interpretability:** bounding-box reasoning improves trust and transparency.\n- **Ethical & bilingual scope:** multilingual component broadens utility for global medical datasets.\n\n### **3. Weaknesses**\n- **Novelty breadth:** approach builds atop existing LLM backbones; novelty lies in dataset and task definition rather than underlying architecture.\n- **Evaluation limitations:** limited human expert validation of clinical correctness and no reader study.\n- **Annotation accuracy:** automated bounding-box localization (72%) may limit reliability for fine-grained tasks.\n- **Reproducibility:** unclear public release of weights or dataset subsets; details on computing resources could be expanded.\n- **Writing:** some redundancy; figures occasionally crowded.\n\n### **4. Major Comments**\n1. **Clarify methodological novelty** relative to prior grounded-reporting models (e.g., Maira‚Äë2, AutoRG‚ÄëBrain). Explicit comparison or ablation without region-centric training would strengthen claims.\n2. **Validation of regional accuracy**‚ÄîConsider radiologist-level qualitative evaluation of MedRegA‚Äôs localized predictions to substantiate interpretability and clinical realism.\n3. **Dataset transparency**‚ÄîProvide explicit plan for releasing de‚Äëidentified subsets and annotation tools, consistent with IEEE TMI reproducibility.\n4. **Generalization analysis**‚ÄîInclude experiments on unseen modalities or pathologies to assess model robustness and domain shift sensitivity.\n5. **Ablation and explainability**‚ÄîQuantify how each component (MedRegInstruct, Regional¬†CoT) contributes to improvements on VQA and report generation.\n\n### **5. Minor Comments**\n- Improve caption clarity in Figures¬†2,¬†4,¬†6.\n- Correct minor typos (e.g., ‚Äúr egions t hey‚Äù, ‚Äúinterpertability‚Äù).\n- Provide parameter counts and inference speed.\n- Consider compressing large tables or moving secondary results to supplementary material.\n- Clarify whether ‚ÄúRegion-Aligned Accuracy‚Äù ‚â•0.5 denotes threshold or normalized metric.\n\n### **6. Overall Recommendation**\n**Recommendation:** ‚òÖ‚òÖ‚òÖ‚òÜ‚òÜ¬†**Major Revision**  \nWhile solid and relevant, the paper‚Äôs publication in *IEEE Transactions on Medical Imaging* would benefit from strengthened empirical validation (clinical/qualitative) and refinement of the novelty claims. The methodological framework is promising and aligns with *TMI‚Äôs* emphasis on interpretable medical imaging AI.\n\n### **7. Confidence Level**\n**4 / 5** ‚Äì High confidence based on manuscript clarity, literature familiarity, and reproducible details.\n\n---\n\n**Summary Judgment:**  \nAn ambitious and well‚Äëstructured paper introducing a region‚Äëaware paradigm for bilingual medical MLLMs. Methodologically interesting and well executed, yet requiring stronger experimental justification of clinical interpretability and fuller comparative discussion before meeting *IEEE‚ÄØTMI* acceptance standards.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedRegA*, a bilingual multimodal large language model designed for medical imaging that explicitly models region-level understanding. It defines three region-centric tasks‚Äîregion-to-text identification, text-to-region detection, and grounded report generation‚Äîand presents the accompanying *MedRegInstruct* dataset consisting of approximately 790K samples derived from a combination of public and institutional data. The method integrates bounding-box‚Äìbased regional encoding, a Regional Chain-of-Thought reasoning strategy, and a Region-Aligned evaluation framework. The paper is clearly written and technically detailed, though the clarity of figures and tables could be improved.  \n\n**Major Comments**  \n1. The methodological novelty should be clarified relative to earlier grounded-reporting and region-aware vision-language models. Comparative experiments or ablations omitting region-centric training would help establish the unique contribution.  \n2. The evaluation would benefit from a radiologist-level qualitative assessment of regional localization accuracy to confirm the clinical interpretability of generated results.  \n3. Dataset transparency requires attention. The authors should specify clear plans for releasing de-identified subsets, annotation tools, and related assets to ensure reproducibility.  \n4. The current validation focuses primarily on known datasets; testing on unseen imaging modalities or pathologies would provide insight into robustness under domain shifts.  \n5. An ablation study isolating contributions of key components, such as *MedRegInstruct* and Regional Chain-of-Thought, should quantify their individual effects on VQA and report generation performance.  \n\n**Minor Comments**  \n- Improve the clarity and labeling of Figures 2, 4, and 6.  \n- Correct typographical errors (e.g., ‚Äúr egions t hey,‚Äù ‚Äúinterpertability‚Äù).  \n- Report parameter counts and inference speed for transparency.  \n- Summarize key quantitative results in main tables and move secondary metrics to supplementary materials.  \n- Clarify the definition and thresholding of ‚ÄúRegion-Aligned Accuracy.‚Äù  \n\n**Summary Paragraph**  \nOverall, the work presents a methodologically interesting and well-organized framework advancing region-aware multimodal modeling for medical imaging. Strengths include its structured dataset design, interpretability emphasis, and extensive experimental benchmarking. Remaining issues involve limited verification of clinical validity, somewhat incremental novelty, and incomplete reproducibility details. Addressing these concerns through expanded validation, clearer dataset release plans, and concise presentation would significantly strengthen the paper‚Äôs impact.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MedRegA, a bilingual multimodal large language model designed to address region-centric tasks in medical imaging. The authors identify that existing medical MLLMs are region-agnostic, treating entire images holistically without focusing on specific anatomical regions. To address this limitation, they propose three region-centric tasks: Region-to-Text Identification, Text-to-Region Detection, and Grounded Report Generation. The authors construct MedRegInstruct, a large-scale dataset containing 550K region-text triplets and 240K grounded reports from both public datasets and 25K clinical scans from Sun Yat-Sen Memorial Hospital. MedRegA is built upon InternVL 1.2 and trained using alignment training followed by instruction tuning. Experimental results show improvements over existing methods across visual question answering, report generation, and medical image classification tasks, with the model achieving bilingual capabilities in both English and Chinese.\n\n## Weaknesses\n\n‚Ä¢ **Dataset Construction and Validation Concerns**\n  - The automatic labeling system for structure detection achieves only 72% accuracy compared to human annotations (Page 5), which may introduce systematic errors that propagate through training and evaluation\n  - Human validation is limited to only 50 samples with 2 experts (Page 5), providing insufficient statistical power to assess dataset quality across the claimed 790K total samples\n  - The rule-based strategy for organ summarization and LLM-based report segmentation lacks detailed validation metrics beyond sentence-level accuracy of 93.33% (Page 5)\n\n‚Ä¢ **Mathematical Formulation and Technical Specification Issues**\n  - Bounding box coordinates are normalized to integers within [0, 1000) (Page 6) but the paper lacks explanation of why this specific range was chosen or how it affects model performance\n  - The Region-Aligned evaluation framework introduces multiple complex metrics (Figure 6, Algorithm 1) but provides no theoretical justification or convergence analysis for the Hungarian Matching algorithm application\n  - Loss function specification is vague, stating only \"language model loss is applied\" (Page 6) without mathematical formulation or discussion of how region-specific losses are weighted\n\n‚Ä¢ **Experimental Design and Evaluation Limitations**\n  - Baseline comparisons are problematic as most existing models cannot perform region-centric tasks (Table 1 shows \"‚úó\" for most baselines), making performance comparisons potentially misleading\n  - The Regional CoT evaluation (Table 14, Figure 19) lacks proper ablation studies to isolate the contribution of regional information versus improved prompting strategies\n  - Test set construction methodology is unclear, particularly for the bilingual evaluation where translation quality could significantly impact results (Tables 8, 10)\n\n‚Ä¢ **Reproducibility and Implementation Concerns**\n  - Training details are relegated to appendix with insufficient hyperparameter specification in the main paper (Page 6 references \"Appendix B\" for critical implementation details)\n  - The semi-automatic pipeline for dataset construction involves multiple complex steps (Figure 3) but lacks sufficient detail for reproduction, particularly regarding the \"Doctor Finetuned MLLM\" component\n  - Clinical data collection protocol and ethical approval details are mentioned only briefly in ethics statement (Page 11) without adequate description of data preprocessing and anonymization procedures\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen Dataset Validation and Quality Assessment**\n  - Expand human validation to at least 500 samples across different modalities and anatomical regions to provide statistically meaningful quality assessment\n  - Provide detailed inter-annotator agreement scores (e.g., Cohen's kappa) for both bounding box accuracy and report segmentation quality\n  - Include analysis of how the 72% structure detection accuracy affects downstream model performance and implement error correction mechanisms\n\n‚Ä¢ **Enhance Mathematical Rigor and Technical Clarity**\n  - Provide mathematical justification for the [0, 1000) normalization range choice and include ablation studies comparing different coordinate encoding schemes\n  - Formalize the loss function with explicit mathematical notation, including how region-specific and text-specific losses are combined and weighted\n  - Add theoretical analysis of the Hungarian Matching algorithm's computational complexity and convergence properties within the evaluation framework\n\n‚Ä¢ **Improve Experimental Design and Analysis**\n  - Develop region-capable versions of baseline models through fine-tuning on region-centric tasks to enable fair comparison, or clearly acknowledge this limitation in the evaluation\n  - Conduct comprehensive ablation studies separating the effects of regional information, improved prompting, and bilingual training\n  - Implement cross-validation procedures for test set construction and provide confidence intervals for all reported metrics\n\n‚Ä¢ **Enhance Reproducibility and Documentation**\n  - Move critical implementation details from appendix to main paper, including complete hyperparameter specifications, training procedures, and convergence criteria\n  - Provide detailed algorithmic descriptions and code snippets for the semi-automatic dataset construction pipeline\n  - Include comprehensive supplementary materials with dataset statistics, preprocessing steps, and detailed experimental protocols to ensure full reproducibility",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedRegA*, a bilingual multimodal large language model (MLLM) designed for region-centric medical imaging analysis. The authors identify a gap in existing medical MLLMs, which typically process whole images without anatomical localization. To address this, they define three new region-level tasks‚ÄîRegion-to-Text Identification, Text-to-Region Detection, and Grounded Report Generation‚Äîand introduce *MedRegInstruct*, a large-scale dataset combining 550K region-text triplets and 240K grounded reports derived from public datasets and 25K clinical scans. The model is built upon InternVL 1.2 and trained through alignment and instruction tuning, showing improved performance across several tasks and supporting both English and Chinese input. While the paper is ambitious and well-motivated, several issues concerning dataset validation, technical formulation, experimental rigor, and reproducibility limit its current impact.  \n\n**Major Comments**  \n1. **Dataset Construction and Validation**  \n   - The automatic labeling system attains only 72% accuracy relative to human annotations, raising concerns about error propagation.  \n   - Human validation is limited to 50 samples with two experts, insufficient for assessing the 790K-sample dataset‚Äôs overall quality.  \n   - The rule-based organ summarization and report segmentation methods lack comprehensive validation beyond sentence-level accuracy metrics.  \n\n2. **Mathematical and Technical Specifications**  \n   - The choice to normalize bounding box coordinates to integers within [0,1000) is unexplained.  \n   - The Region-Aligned evaluation framework uses complex matching algorithms without theoretical or convergence analysis.  \n   - The loss function is described vaguely without explicit formulation or weighting strategy for region-specific components.  \n\n3. **Experimental Design and Evaluation**  \n   - Baseline comparisons may be misleading since most prior models cannot perform region-centric tasks.  \n   - The Regional CoT evaluation lacks ablation studies to separate contributions of region information and prompting strategies.  \n   - Test set construction and bilingual evaluation procedures are insufficiently detailed, particularly regarding translation quality.  \n\n4. **Reproducibility and Implementation**  \n   - Critical hyperparameters and training configurations appear only in the appendix.  \n   - The semi-automatic dataset pipeline and the ‚ÄúDoctor Finetuned MLLM‚Äù component are under-described.  \n   - Clinical data preprocessing, anonymization, and ethical approval processes require clearer documentation.  \n\n**Minor Comments**  \n- Clarify figure and algorithm references (e.g., Figure 3, Algorithm 1).  \n- Ensure consistent use of technical terms and notation.  \n- Move essential implementation details to the main text for clarity.  \n\n**Summary Paragraph**  \nOverall, the paper introduces a potentially valuable region-aware bilingual medical MLLM, with strong motivation and an extensive dataset contribution. However, the work currently suffers from gaps in dataset validation, incomplete mathematical specification, limited ablation and baseline analysis, and insufficient reproducibility information. Strengthening validation experiments, formal derivations, and implementation transparency would substantially enhance credibility and impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The study is promising but requires significant improvements in dataset validation, technical rigor, and reproducibility before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### Comprehensive Peer Review\n\n:\nThe manuscript addresses the issue of region-agnostic behavior in current medical Multimodal Language Learning Models (MLLMs) and proposes MedRegA, a region-aware model designed to enhance interpretability and usability across various medical modalities. Through the introduction of the MedRegInstruct dataset and a region-centric approach, the authors demonstrate improved performance in tasks such as medical image classification and grounded report generation. The study highlights the importance of regional information in enhancing the diagnostic capabilities of AI models in medical applications.\n\n## Major Comments\n:\n\nStrengths:\n1. Clear Problem Statement and Solution Overview: The manuscript effectively articulates the limitations of current MLLMs and introduces MedRegA with a well-defined region-centric approach. The problem formulation is clear, and the solution is well-supported with the development of the MedRegInstruct dataset.\n2. Extensive Benchmarking and Detailed Metrics: The results section provides comprehensive benchmarking against multiple datasets and models, using a wide array of metrics such as BLEU, F1, Recall, Accuracy, and BertScore. This thorough evaluation showcases the versatility and robustness of MedRegA.\n3. Region-Aligned Evaluation Framework: The introduction of the Region-Aligned evaluation framework and the regional chain-of-thought (Regional CoT) concept demonstrates innovative approaches to medical image analysis, enhancing the model's diagnostic capabilities.\n4. Comprehensive References: The references section includes a broad range of recent studies and datasets relevant to medical imaging and multimodal large language models, ensuring the paper is up-to-date with the latest research trends.\n\nLimitations:\n1. Technical Depth and Comparative Analysis: The abstract and conclusion sections lack sufficient technical depth and comparative analysis with existing models, which could provide a clearer picture of MedRegA's superiority.\n2. Quantitative Evidence and Specific Metrics: While the results are presented comprehensively, there is a need for more specific quantitative evidence and metrics to support claims about the model's performance and the essential nature of region-centric capabilities.\n3. Clarity and Structure: The writing quality and structure vary across sections, with some parts suffering from awkward phrasing and abrupt transitions, affecting readability. Additionally, the transition from the conclusion to the ethics statement is abrupt and could benefit from smoother integration.\n4. Relevance and Critical Analysis: The references section lacks explicit contextualization and critical analysis of how cited works support or contrast with the authors' approach, which could enhance the depth of the paper.\n\n## Minor Comments\n:\n1. Inconsistent Presentation: Some parts of the results section are not well-structured, leading to minor readability issues. The integration of textual descriptions with tables and figures could be improved for better flow.\n2. Ambiguous Descriptions: Certain descriptions lack clarity, particularly in explaining how the models handle multi-label classification and the specifics of the grounded report generation task.\n3. Formatting Consistency: Ensure all URLs in the references section are fully formatted and consider adding DOIs where available for better accessibility.\n\n## Summary Paragraph\nEvaluating the work based on the four TMI editorial criteria:\n- Significance: The manuscript addresses a significant gap in the current medical MLLM landscape by introducing region-aware capabilities, which is highly relevant for improving interpretability and usability.\n- Innovation: The introduction of the Region-Aligned evaluation framework and the Regional CoT concept represents innovative contributions to medical image analysis.\n- Evaluation: The evaluation is rigorous, employing comprehensive metric analysis and fine-grained regional evaluations, although more specific quantitative evidence and comparative analysis would strengthen the results.\n- Reproducibility: The technical details provided are generally sufficient, but enhancing the clarity and structure of the writing would improve reproducibility.\n\n## Decision Recommendation\n:\nMajor Revision: The manuscript requires significant improvements in technical depth, comparative analysis, and clarity to fully meet the editorial criteria. Addressing the recommendations outlined will enhance the overall quality and impact of the work.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the challenge of region-agnostic behavior in current medical Multimodal Language Learning Models (MLLMs) and introduces _MedRegA_, a region-aware architecture aimed at improving interpretability and usability across different medical modalities. Supported by the new _MedRegInstruct_ dataset, the study demonstrates enhanced performance in medical image classification and grounded report generation. Overall, the paper clearly identifies a relevant gap and proposes a promising region-centric solution, though aspects of clarity and depth could be strengthened.\n\n**Major Comments**  \n**Strengths:**  \n1. **Clear Problem Formulation and Approach:** The authors clearly articulate the limitations of existing MLLMs and present _MedRegA_ through a well-defined region-aware framework, supported by the creation of the _MedRegInstruct_ dataset.  \n2. **Comprehensive Benchmarking:** The evaluation employs numerous datasets and metrics, including BLEU, F1, Recall, Accuracy, and BERTScore, providing strong evidence of performance robustness and versatility.  \n3. **Innovative Regional Evaluation:** The introduction of the Region-Aligned framework and the Regional Chain-of-Thought (CoT) method represents a novel approach that enhances the interpretability and diagnostic relevance of the results.  \n4. **Current and Well-Sourced References:** The reference list is extensive and reflects up-to-date developments in medical imaging and multimodal modeling.\n\n**Limitations:**  \n1. **Insufficient Technical Depth:** The abstract and conclusions would benefit from deeper technical discussion and stronger comparative analysis with existing models to better support claims of superiority.  \n2. **Need for Quantitative Support:** Although the evaluation is broad, more specific quantitative evidence and meaningful statistical comparisons are needed to substantiate key findings.  \n3. **Variable Writing Quality:** Differences in linguistic clarity and abrupt transitions‚Äîespecially between the conclusion and ethics statement‚Äîdetract from overall readability.  \n4. **Limited Contextualization of References:** The reference section lacks systematic discussion of how prior work informs or contrasts with the proposed approach.\n\n**Minor Comments**  \n1. Improve the structural integration of text, tables, and figures for smoother presentation of results.  \n2. Clarify ambiguous explanations, particularly regarding multi-label classification and grounded report generation tasks.  \n3. Ensure consistent formatting in references, including complete URLs and DOIs where available.\n\n**Summary Paragraph**  \nThe manuscript makes a significant contribution by addressing an important gap in region-aware medical MLLMs. Its innovations‚Äîparticularly the Region-Aligned framework and Regional CoT‚Äîare notable and timely. Evaluation practices are comprehensive, though the paper would benefit from stronger comparative analysis, deeper quantitative justification, and improved structural coherence to enhance clarity and reproducibility.\n\n**Decision Recommendation**  \n**Major Revision:** The work is promising but requires substantial improvement in technical depth, comparative discussion, and writing clarity before it can meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## INTERPRETABLE BILINGUAL MULTIMODAL LARGE LANGUAGE MODEL FOR DIVERSE BIOMEDICAL TASKS\n\n### Summary\n\nThe paper proposes MedRegA, a bilingual region-aware medical multimodal large language model that unifies image-level and region-level tasks across multiple modalities. The authors construct MedRegInstruct, a large-scale dataset spanning three ‚ÄúRegion-Centric‚Äù tasks (Region-to-Text identification, Text-to-Region detection, and Grounded Report Generation), and introduce a Region-Aligned evaluation framework and a Regional Chain-of-Thought (CoT) inference strategy. Experiments suggest that MedRegA improves on general medical VQA, report generation, classification, and the newly defined region-centric tasks, with qualitative gains in interpretability via bounding-box grounding.\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a coherent suite of region-centric tasks (identify, detect, ground) for medical MLLMs and encodes regions with explicitandtokens to steer spatial reasoning.Proposes a practical Regional CoT strategy (localize first, then reason) that is intuitive for clinical workflows and shows measurable gains in multi-label settings.Presents a Region-Aligned evaluation protocol decomposed into object-, region-, and alignment-level metrics, which is a step beyond purely textual evaluation.\n- Experimental rigor and validationEvaluates MedRegA on a broad task set (VQA, report generation, classification, three region-centric tasks) and in both English and Chinese settings.Provides qualitative examples showing improved localization and more specific, grounded descriptions.\n- Clarity of presentationClearly delineates the three region-centric tasks and the associated tokenization scheme for region representation.The pipeline for data construction (report refinement, structure detection, and human validation) is explained with reasonable transparency.\n- Significance of contributionsAddresses a pressing and widely recognized gap in medical MLLMs: weak spatial grounding and limited interpretability for region-specific findings.Bilingual capability and cross-modality coverage broaden potential utility and suggest a path toward more generalist, interpretable assistants.\n\n- Introduces a coherent suite of region-centric tasks (identify, detect, ground) for medical MLLMs and encodes regions with explicitandtokens to steer spatial reasoning.\n- Proposes a practical Regional CoT strategy (localize first, then reason) that is intuitive for clinical workflows and shows measurable gains in multi-label settings.\n- Presents a Region-Aligned evaluation protocol decomposed into object-, region-, and alignment-level metrics, which is a step beyond purely textual evaluation.\n\n- Evaluates MedRegA on a broad task set (VQA, report generation, classification, three region-centric tasks) and in both English and Chinese settings.\n- Provides qualitative examples showing improved localization and more specific, grounded descriptions.\n\n- Clearly delineates the three region-centric tasks and the associated tokenization scheme for region representation.\n- The pipeline for data construction (report refinement, structure detection, and human validation) is explained with reasonable transparency.\n\n- Addresses a pressing and widely recognized gap in medical MLLMs: weak spatial grounding and limited interpretability for region-specific findings.\n- Bilingual capability and cross-modality coverage broaden potential utility and suggest a path toward more generalist, interpretable assistants.\n\n### Weaknesses\n\n- Technical limitations or concernsThe grounded components rely heavily on bounding boxes with only moderate human validation (72% accuracy over 50 samples), raising concerns about noise in supervision and evaluation.Central-slice extraction for 3D CT/MRI inputs discards volumetric context, which is critical for many clinical tasks and localization fidelity.The ‚Äúfirst bilingual generalist‚Äù claim should be nuanced in light of concurrent region-aware or expert-augmented systems and large-scale grounding frameworks (e.g., VILA-M3, Med-GLIP), and CXR-specific grounded VQA/report pipelines (e.g., GEMeX, AutoRG-Brain).\n- Experimental gaps or methodological issuesRegion-centric evaluation is largely based on constructed/derived data from SA-Med2D and in-house/MIMIC pipelines; external region-grounding benchmarks (e.g., GEMeX V-score, Med-GLIP-style AP/AP50) are missing.Many baselines are not region-capable by design and are marked ‚ÄúX‚Äù; comparisons to recent region-aware medical models and grounding-first frameworks are limited.VQA reporting relies on generic NLG metrics (BLEU-1, BertScore) instead of standardized answer accuracy for each dataset; the averaging across datasets and metrics reduces interpretability of results.Minimal ablation on the contribution of each dataset component, the/tokenization, and the Regional CoT steps; no calibration or hallucination analysis.\n- Clarity or presentation issuesSome tables contain formatting artifacts and unusual metric choices (e.g., ‚ÄúCloseAccuracy,‚Äù ‚ÄúOpenRecall‚Äù) without clear definitions or dataset-specific protocols, making reproducibility and judgment difficult.The definition of evaluation thresholds and matching strategies for multi-object multi-region detection could be further clarified with examples and sensitivity analyses.\n- Missing related work or comparisonsLimited engagement with sequence/temporal and multi-view grounding benchmarks (e.g., MedSG-Bench) that are clinically relevant.Absent comparisons to large, region-level pretraining frameworks (e.g., Med-GLIP) and grounded VQA/report datasets (e.g., GEMeX, GEMeX-RMCoT; AutoRG-Brain for MRI; ChestX-Reasoner for process supervision).No analysis of leveraging or integrating external expert modules (e.g., segmentation experts as in VILA-M3) vs. end-to-end region tokenization.\n\n- The grounded components rely heavily on bounding boxes with only moderate human validation (72% accuracy over 50 samples), raising concerns about noise in supervision and evaluation.\n- Central-slice extraction for 3D CT/MRI inputs discards volumetric context, which is critical for many clinical tasks and localization fidelity.\n- The ‚Äúfirst bilingual generalist‚Äù claim should be nuanced in light of concurrent region-aware or expert-augmented systems and large-scale grounding frameworks (e.g., VILA-M3, Med-GLIP), and CXR-specific grounded VQA/report pipelines (e.g., GEMeX, AutoRG-Brain).\n\n- Region-centric evaluation is largely based on constructed/derived data from SA-Med2D and in-house/MIMIC pipelines; external region-grounding benchmarks (e.g., GEMeX V-score, Med-GLIP-style AP/AP50) are missing.\n- Many baselines are not region-capable by design and are marked ‚ÄúX‚Äù; comparisons to recent region-aware medical models and grounding-first frameworks are limited.\n- VQA reporting relies on generic NLG metrics (BLEU-1, BertScore) instead of standardized answer accuracy for each dataset; the averaging across datasets and metrics reduces interpretability of results.\n- Minimal ablation on the contribution of each dataset component, the/tokenization, and the Regional CoT steps; no calibration or hallucination analysis.\n\n- Some tables contain formatting artifacts and unusual metric choices (e.g., ‚ÄúCloseAccuracy,‚Äù ‚ÄúOpenRecall‚Äù) without clear definitions or dataset-specific protocols, making reproducibility and judgment difficult.\n- The definition of evaluation thresholds and matching strategies for multi-object multi-region detection could be further clarified with examples and sensitivity analyses.\n\n- Limited engagement with sequence/temporal and multi-view grounding benchmarks (e.g., MedSG-Bench) that are clinically relevant.\n- Absent comparisons to large, region-level pretraining frameworks (e.g., Med-GLIP) and grounded VQA/report datasets (e.g., GEMeX, GEMeX-RMCoT; AutoRG-Brain for MRI; ChestX-Reasoner for process supervision).\n- No analysis of leveraging or integrating external expert modules (e.g., segmentation experts as in VILA-M3) vs. end-to-end region tokenization.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe region tokenization scheme and integer-normalized box coordinates are reasonable and easy to implement; however, the conversion from masks to boxes (SA-Med2D) and reliance on auto-detected regions in in-house data likely introduce systematic bias (e.g., oversized boxes).The two-stage inference (Regional CoT) aligns with clinical reading workflows. A more formalized failure-mode analysis (missed small lesions, multi-focal disease, diffuse patterns) would strengthen claims, especially since boxes are lossy for irregular pathology.Freezing the vision encoder and focusing updates on the language model may limit representation learning for fine-grained localization; an ablation on partial vision finetuning could clarify trade-offs.\n- Experimental evaluation assessmentStrong breadth across tasks, but depth is uneven: region-centric tasks are primarily evaluated on internal/derived data; external, standardized region-aware benchmarks are needed to substantiate generalization.VQA metrics: BLEU-1 and BertScore are not standard for answer evaluation; use dataset-specific accuracy or semantic answer matching, and report per-dataset scores rather than averages.Grounded report generation: reporting both NLG and clinical efficacy (CheXpert, RadGraph, RadCliQ) for CXR is good; including human grading (error taxonomies, RaTE) and inter-rater agreement would make the clinical significance clearer.The 72% box ‚Äúaccuracy‚Äù on a small validation (n=50) suggests meaningful annotation noise. Provide IoU distributions and sensitivity to IoU thresholds; quantify how noise propagates to model performance.\n- Comparison with related work (using the summaries provided)Med-GLIP demonstrates large-scale, modality-aware region supervision that boosts downstream VQA and MRG; comparing MedRegA‚Äôs region detection to Med-GLIP-style AP/AP50 on shared subsets would contextualize progress.GEMeX provides chest X-ray VQA with explicit textual explanations and grounding (V-score); evaluating MedRegA on GEMeX would test transfer beyond your constructed data and directly assess visual grounding.AutoRG-Brain (grounded report generation for brain MRI) and MedRegion-CT (region-focused chest CT reports) explore pixel/region grounding with 3D context; the present 2D central-slice strategy is a limitation relative to these methods‚Äô volumetric reasoning.ChestX-Reasoner and GEMeX-RMCoT highlight process supervision and verifiable rewards to align reasoning with evidence; MedRegA‚Äôs Regional CoT could be extended with verifiable or consistency-based rewards to reduce hallucinations and improve robustness.MedSG-Bench emphasizes clinically motivated sequence grounding; the present single-image grounding could be extended and evaluated in multi-view/temporal settings for greater clinical fidelity.\n- Discussion of broader impact and significanceExplicit localization tied to generated text improves interpretability and supports clinician verification, which is vital for trust and adoption.Bilingual capability is valuable; however, reliance on non-released in-house Chinese data limits reproducibility and external validation; consider releasing de-identified evaluation-only subsets or synthetic-but-validated proxies.A region-centric paradigm is well aligned with safety goals but should be complemented with calibration, uncertainty estimation, and guardrails against confident but wrong localizations.\n- Data quality, ethics, and reproducibilityMore extensive human validation of the auto-annotations is needed (larger sample size, multiple raters, inter-rater agreement). Present per-organ/per-lesion performance to identify where auto-labeling is most/least reliable.Clarify what portions of MedRegInstruct will be released, with licensing and patient privacy safeguards. Without the in-house portion, claims for Chinese/bilingual performance are hard to reproduce.\n- Metrics and evaluation designFor detection, add standard detection metrics (AP, AP50, AP75) and recall/precision curves; for grounding, report mIoU with Hungarian matching for multi-box cases (as in GEMeX) and alignment accuracy.For VQA, use answer accuracy or GPTScore-like semantic measures for open-ended items; reduce reliance on BLEU-1 for short answers.Provide per-dataset, per-modality breakdowns rather than only averages; report statistical significance or confidence intervals.\n- Model design and ablationsInclude ablations on: with/without region tokens; with/without Region-Text vs grounded-report subsets; proportion of Chinese vs English data; Regional CoT‚Äôs stage-1 detector accuracy vs final task performance; and partial finetuning of the vision encoder.Explore alternatives to box-only grounding (mask tokens or referring segmentation) for multi-focal/diffuse pathologies and small lesions.\n- Interpretability and clinical utilityAdd clinician-centered evaluations: time-to-review, correction effort, error taxonomy (omission, wrong location/description), and user studies on whether grounded outputs improve trust and workflow.Provide examples of failure cases and discuss mitigation (e.g., uncertainty flags when localization confidence is low).\n- Bilingual and cross-domain considerationsShow cross-lingual transfer (train English ‚Üí test Chinese and vice versa) and error analyses by language. Clarify how bilingual prompts and tokenization handle medical synonyms and nomenclature.\n\n- The region tokenization scheme and integer-normalized box coordinates are reasonable and easy to implement; however, the conversion from masks to boxes (SA-Med2D) and reliance on auto-detected regions in in-house data likely introduce systematic bias (e.g., oversized boxes).\n- The two-stage inference (Regional CoT) aligns with clinical reading workflows. A more formalized failure-mode analysis (missed small lesions, multi-focal disease, diffuse patterns) would strengthen claims, especially since boxes are lossy for irregular pathology.\n- Freezing the vision encoder and focusing updates on the language model may limit representation learning for fine-grained localization; an ablation on partial vision finetuning could clarify trade-offs.\n\n- Strong breadth across tasks, but depth is uneven: region-centric tasks are primarily evaluated on internal/derived data; external, standardized region-aware benchmarks are needed to substantiate generalization.\n- VQA metrics: BLEU-1 and BertScore are not standard for answer evaluation; use dataset-specific accuracy or semantic answer matching, and report per-dataset scores rather than averages.\n- Grounded report generation: reporting both NLG and clinical efficacy (CheXpert, RadGraph, RadCliQ) for CXR is good; including human grading (error taxonomies, RaTE) and inter-rater agreement would make the clinical significance clearer.\n- The 72% box ‚Äúaccuracy‚Äù on a small validation (n=50) suggests meaningful annotation noise. Provide IoU distributions and sensitivity to IoU thresholds; quantify how noise propagates to model performance.\n\n- Med-GLIP demonstrates large-scale, modality-aware region supervision that boosts downstream VQA and MRG; comparing MedRegA‚Äôs region detection to Med-GLIP-style AP/AP50 on shared subsets would contextualize progress.\n- GEMeX provides chest X-ray VQA with explicit textual explanations and grounding (V-score); evaluating MedRegA on GEMeX would test transfer beyond your constructed data and directly assess visual grounding.\n- AutoRG-Brain (grounded report generation for brain MRI) and MedRegion-CT (region-focused chest CT reports) explore pixel/region grounding with 3D context; the present 2D central-slice strategy is a limitation relative to these methods‚Äô volumetric reasoning.\n- ChestX-Reasoner and GEMeX-RMCoT highlight process supervision and verifiable rewards to align reasoning with evidence; MedRegA‚Äôs Regional CoT could be extended with verifiable or consistency-based rewards to reduce hallucinations and improve robustness.\n- MedSG-Bench emphasizes clinically motivated sequence grounding; the present single-image grounding could be extended and evaluated in multi-view/temporal settings for greater clinical fidelity.\n\n- Explicit localization tied to generated text improves interpretability and supports clinician verification, which is vital for trust and adoption.\n- Bilingual capability is valuable; however, reliance on non-released in-house Chinese data limits reproducibility and external validation; consider releasing de-identified evaluation-only subsets or synthetic-but-validated proxies.\n- A region-centric paradigm is well aligned with safety goals but should be complemented with calibration, uncertainty estimation, and guardrails against confident but wrong localizations.\n\n- More extensive human validation of the auto-annotations is needed (larger sample size, multiple raters, inter-rater agreement). Present per-organ/per-lesion performance to identify where auto-labeling is most/least reliable.\n- Clarify what portions of MedRegInstruct will be released, with licensing and patient privacy safeguards. Without the in-house portion, claims for Chinese/bilingual performance are hard to reproduce.\n\n- For detection, add standard detection metrics (AP, AP50, AP75) and recall/precision curves; for grounding, report mIoU with Hungarian matching for multi-box cases (as in GEMeX) and alignment accuracy.\n- For VQA, use answer accuracy or GPTScore-like semantic measures for open-ended items; reduce reliance on BLEU-1 for short answers.\n- Provide per-dataset, per-modality breakdowns rather than only averages; report statistical significance or confidence intervals.\n\n- Include ablations on: with/without region tokens; with/without Region-Text vs grounded-report subsets; proportion of Chinese vs English data; Regional CoT‚Äôs stage-1 detector accuracy vs final task performance; and partial finetuning of the vision encoder.\n- Explore alternatives to box-only grounding (mask tokens or referring segmentation) for multi-focal/diffuse pathologies and small lesions.\n\n- Add clinician-centered evaluations: time-to-review, correction effort, error taxonomy (omission, wrong location/description), and user studies on whether grounded outputs improve trust and workflow.\n- Provide examples of failure cases and discuss mitigation (e.g., uncertainty flags when localization confidence is low).\n\n- Show cross-lingual transfer (train English ‚Üí test Chinese and vice versa) and error analyses by language. Clarify how bilingual prompts and tokenization handle medical synonyms and nomenclature.\n\n### Questions for Authors\n\n- How robust are your region outputs to annotation noise? Can you provide IoU histograms, per-class box quality, and sensitivity analyses to different IoU thresholds beyond 0.5?\n- Can you evaluate MedRegA‚Äôs grounding on external benchmarks such as GEMeX (V-score) and report detection AP metrics in the style of Med-GLIP, to better assess generalization?\n- What exact definitions and dataset-specific protocols underlie ‚ÄúCloseAccuracy,‚Äù ‚ÄúOpenRecall,‚Äù and other nonstandard metrics in Table 1? Please provide per-dataset results and accuracy metrics commonly used by those benchmarks.\n- How do results change when the vision encoder is partially finetuned vs. frozen? Is the strong region localization driven mostly by the language head learning coordinate patterns?\n- For CT/MRI, what is the impact of using central slices vs. 3D context on region detection and report grounding? Have you explored 2.5D or lightweight volumetric context?\n- Regional CoT adds an extra inference pass; what is the computational overhead and how does it scale with multiple regions? Any batching or caching strategies to keep latency acceptable?\n- How do you handle multi-focal and diffuse abnormalities where rectangular boxes are inadequate? Have you considered mask-based grounding or referring segmentation?\n- Could you provide more extensive human evaluation for grounded report quality (e.g., RaTE, radiologist scoring, inter-rater agreement), especially in the Chinese setting?\n- What parts of MedRegInstruct (especially grounded subsets) will be released, and how can others reproduce your Chinese evaluation?\n- How does MedRegA compare to region-aware or expert-augmented systems like Med-GLIP and VILA-M3 on shared tasks when those systems‚Äô outputs are adapted to your evaluation (or vice versa)?\n- Do you have calibration or uncertainty estimates for localization and descriptions, and can the model abstain when uncertain about a region?\n- Have you analyzed error modes where incorrect localization leads to downstream diagnostic mistakes, and can Regional CoT be extended with verifiable rewards to mitigate this?\n\n### Overall Assessment\n\nThis paper tackles an important and understudied aspect of medical MLLMs: explicit, interpretable regional grounding aligned with textual outputs. The introduction of a unified set of region-centric tasks, a region-aligned evaluation protocol, and a pragmatic Regional CoT strategy is meaningful, and the breadth of bilingual, multi-modality experiments is promising. However, several issues limit the strength of the claims: heavy reliance on auto-annotated boxes with modest human validation, limited external benchmarking against region-aware datasets/frameworks, and evaluation choices that obscure per-dataset accuracy and fairness relative to baselines. The technical novelty is moderate in view of closely related contemporary efforts (e.g., Med-GLIP, GEMeX, AutoRG-Brain, VILA-M3), though the bilingual generalist framing and unification of tasks are valuable. With stronger external evaluations, clearer metrics, deeper ablations, larger-scale human validation, and more cautious claims contextualized against recent region-aware systems, this work could make a solid contribution to the field and be competitive at a top venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MedRegA**, a bilingual, region-aware multimodal large language model designed to unify image- and region-level medical tasks across multiple modalities. It is supported by **MedRegInstruct**, a large-scale dataset covering region-to-text identification, text-to-region detection, and grounded report generation, alongside a **Region-Aligned evaluation** protocol and a **Regional Chain-of-Thought (CoT)** inference strategy. The study demonstrates improvements in medical VQA, report generation, classification, and newly defined region-centric tasks, with notable gains in interpretability through spatial grounding. The paper is clearly written and well organized, offering a coherent technical framework with broad empirical evaluation.\n\n---\n\n**Major Comments**  \n1. **Data Quality and Validation:** The reliance on auto-generated bounding boxes with limited human verification (72% accuracy on 50 samples) introduces significant annotation noise. Broader validation with multiple raters, IoU analyses, and inter-rater agreement is needed.  \n2. **Evaluation Design:** Region-centric benchmarks are primarily in-house or derived from SA-Med2D; external validations on public datasets such as GEMeX or Med-GLIP are missing. Detection metrics (AP, AP50) and standardized VQA accuracy measures should replace or complement BLEU-based text metrics.  \n3. **Comparative Analysis:** Claims of being the ‚Äúfirst bilingual generalist‚Äù should be tempered given concurrent systems (e.g., VILA-M3, Med-GLIP). Direct quantitative comparisons on shared benchmarks would clarify the contribution.  \n4. **Ablations and Model Analysis:** Additional ablations on region tokens, dataset components, vision encoder finetuning, and CoT components would clarify source contributions. Current omission limits interpretability of results.  \n5. **3D and Spatial Context:** The central-slice approach discards volumetric information critical for CT and MRI interpretation; comparisons with 3D or multi-view alternatives are warranted.  \n6. **Reproducibility and Release:** The Chinese component depends on non-released data. Clear statements about dataset availability, licensing, and privacy protections are essential for reproducibility.  \n7. **Metric Definitions and Clarity:** Several metrics (e.g., ‚ÄúCloseAccuracy,‚Äù ‚ÄúOpenRecall‚Äù) are nonstandard and insufficiently defined, complicating result interpretation.  \n8. **Interpretability and Clinical Utility:** The grounding strategy enhances explainability, but more clinician-centered evaluations (trust, correction effort, uncertainty estimation) would substantiate clinical relevance.\n\n---\n\n**Minor Comments**  \n- Improve table clarity, correct formatting artifacts, and define all metrics.  \n- Include per-dataset and per-modality results with confidence intervals.  \n- Add failure-case visualization and sensitivity analysis for region matching thresholds.  \n- Show bilingual transfer performance (English‚ÜîChinese) and language-specific error analysis.  \n- Consider mask-based grounding for irregular or diffuse pathologies.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper presents a well-motivated effort toward interpretable, bilingual medical MLLMs via explicit regional grounding. Strengths include the unified region-centric formulation, intuitive Regional CoT design, and broad task coverage across languages and modalities. However, methodological limitations‚Äîparticularly limited human validation of ground truth, restricted external benchmarking, and nonstandard evaluation metrics‚Äîundermine the robustness of the conclusions. With more extensive validation, standardized comparisons, and clarified metrics, this work could make a meaningful contribution to interpretable medical AI.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial revisions are recommended to improve external validation, clarify metrics, strengthen ablations, and enhance reproducibility before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThis paper addresses a critical limitation in current medical multimodal large language models (MLLMs) ‚Äì their region-agnostic nature that prevents them from identifying which specific regions they focus on when generating medical reports or answers. The authors propose MedRegA, a bilingual region-aware medical MLLM that simultaneously handles both image-level and region-level medical vision-language tasks across eight different modalities. The key innovation is the introduction of Region-Centric tasks (Region-to-Text Identification, Text-to-Region Detection, and Grounded Report Generation) along with a large-scale dataset, MedRegInstruct, containing over 790,000 region-annotated medical samples. The authors demonstrate that MedRegA significantly outperforms existing models across multiple benchmarks, achieving 67.65 BLEU-1 in visual question answering (3.91% improvement over MedDr), 40.46 BLEU-1 in English report generation (5.97% improvement), and 47.97 F1 score in medical image classification (15.32% improvement). Most notably, the model excels in region-centric tasks with 69.72 BLEU-1 in Region-to-Text Identification and 76.59% region accuracy in Grounded Report Generation, while providing interpretable outputs that associate generated text with specific anatomical regions in medical images.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- **Critical Problem Addressed**: The paper identifies and addresses a fundamental limitation in current medical MLLMs ‚Äì their inability to specify which regions they focus on, which severely impacts clinical interpretability and trustworthiness.\n- **Comprehensive Dataset**: The MedRegInstruct dataset (550K region-text pairs and 240K grounded reports) covers 8 modalities, multiple body regions, and bilingual (English/Chinese) capabilities, addressing a significant gap in existing medical vision-language resources.\n- **Strong Performance Improvements**: The model demonstrates substantial performance gains across diverse medical tasks, with particularly impressive results in region-centric capabilities where previous models fail completely.\n- **Innovative Regional CoT**: The proposed Chain-of-Thought approach that first detects regions before answering questions provides a practical method to leverage the model's region-aware capabilities for improved clinical reasoning.\n\n### Major Limitations\n- **Limited Clinical Validation**: While the model shows strong quantitative results, there is no evaluation with actual clinicians regarding the practical usefulness of region-centric outputs for clinical decision-making.\n- **Lack of Failure Analysis**: The paper does not systematically analyze failure cases, particularly for complex scenarios with subtle abnormalities or when region identification fails.\n- **Computational Resources**: There is no discussion of the computational cost and resource requirements for training/deploying the model, which could impact clinical adoption.\n- **Medical Specificity**: The paper doesn't clearly articulate how the region-centric approach specifically addresses clinical workflow needs beyond improving metrics.\n\n### Minor Strengths\n- **Well-Structured Evaluation**: The comprehensive evaluation across multiple tasks and modalities provides strong evidence of the model's versatility.\n- **Effective Visualizations**: The figures effectively demonstrate the model's capabilities, particularly the side-by-side comparisons showing region-specific reasoning.\n- **Bilingual Capability**: The inclusion of Chinese language processing significantly extends the potential global impact of the work.\n\n### Minor Limitations\n- **Figure Clarity**: Some figures (e.g., Figure 8) could be simplified for better readability while maintaining their informative value.\n- **Length Management**: The paper is quite lengthy; certain sections in the appendix could be condensed or moved to supplementary materials.\n- **Architectural Details**: More specific information about the architectural modifications made to the base model would enhance technical understanding.\n\n## 3. Evaluation Along TMI Editorial Criteria\n\n**Significance**: High. The region-centric approach addresses a fundamental limitation in medical AI interpretability that directly impacts clinical trust and adoption. By enabling models to specify which regions they're analyzing, this work moves medical MLLMs closer to mimicking clinician reasoning patterns. The bilingual capability further enhances global relevance, and the demonstrated improvements across multiple clinical tasks suggest meaningful potential for real-world impact.\n\n**Innovation**: High. The paper introduces three novel Region-Centric tasks specifically designed for medical MLLMs, along with a comprehensive dataset to support them. While region grounding has been explored in general vision-language models, this work is the first to systematically apply it across diverse medical modalities with clinical relevance. The Regional CoT approach is also an innovative application of the model's capabilities.\n\n**Evaluation**: Good. The evaluation is comprehensive across multiple dimensions (VQA, report generation, classification, and region-centric tasks) and modalities. The region-aligned evaluation framework is well-designed for the proposed tasks. However, the clinical utility evaluation is lacking ‚Äì the paper would benefit from clinician feedback on the practical value of region-specific outputs for diagnostic reasoning.\n\n**Reproducibility**: Good. The paper provides extensive details on dataset construction, model architecture, and training procedures in both the main text and appendix. The authors mention releasing code and dataset (via a project page), and the detailed descriptions enable replication. More specific information about hyperparameters and computational requirements would further enhance reproducibility.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThis paper presents a significant advancement in medical multimodal AI by addressing the critical limitation of region-agnostic processing in current models. The comprehensive dataset, strong performance improvements, and innovative region-centric approach make this a valuable contribution to the field. The paper is well-structured and thoroughly evaluated, with clear demonstrations of the model's capabilities.\n\nHowever, the paper would benefit from revisions addressing the clinical relevance of the region-centric approach and providing more analysis of failure cases. The authors should include:\n1) A discussion of how the region-specific outputs align with actual clinical workflow needs\n2) Analysis of failure cases, particularly where region identification impacts diagnostic accuracy\n3) Information on computational requirements for potential clinical deployment\n\nThese revisions would strengthen the clinical significance of the work without requiring additional major experiments. The current version is already strong, but addressing these points would make it an exemplary contribution to medical imaging AI research.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes **MedRegA**, a bilingual region-aware medical multimodal large language model (MLLM) designed to overcome the region-agnostic limitation of existing medical MLLMs. The model jointly performs image-level and region-level vision-language tasks across eight imaging modalities. Its main contributions include three novel region-centric tasks‚ÄîRegion-to-Text Identification, Text-to-Region Detection, and Grounded Report Generation‚Äîand the introduction of a large-scale dataset, **MedRegInstruct**, containing approximately 790,000 region-annotated medical samples. Experimental results show that MedRegA achieves notable improvements over prior methods across multiple benchmarks, particularly in region-centric tasks, while producing interpretable outputs linking generated text to anatomical regions.\n\n---\n\n**Major Comments**  \n1. **Significance of the Problem**: The paper tackles a crucial limitation in current medical MLLMs related to region interpretability, which affects clinical trust and transparency.  \n2. **Dataset Contribution**: The MedRegInstruct dataset, encompassing eight imaging modalities and bilingual annotations, represents a valuable and comprehensive resource filling a notable gap in the field.  \n3. **Performance and Innovation**: The model achieves consistent gains across several benchmarks, with substantial advantages in region-aware tasks. The Regional Chain-of-Thought (CoT) mechanism that detects image regions before generating answers is both practical and innovative.  \n4. **Clinical Validation**: Despite impressive quantitative results, the work lacks qualitative evaluation with clinicians to assess whether region-centric outputs meaningfully support diagnostic reasoning.  \n5. **Failure Analysis**: The paper does not include analysis of error or failure cases, especially where subtle abnormalities or incorrect region localization occur.  \n6. **Computational Efficiency**: The computational cost and resource footprint are not discussed, which limits understanding of real-world deployment feasibility.  \n7. **Clinical Workflow Integration**: The discussion should clarify how the proposed method specifically fits into or improves clinical workflows beyond improved performance metrics.\n\n---\n\n**Minor Comments**  \n- Some figures (e.g., Figure 8) could be streamlined for better readability.  \n- The manuscript is lengthy; certain appendix sections could be condensed or relegated to supplementary materials.  \n- Additional architectural details would help clarify the exact modifications to the base model.  \n- The paper‚Äôs bilingual capability and well-structured evaluation are commendable, and the visual examples effectively demonstrate region-specific reasoning.\n\n---\n\n**Summary Paragraph**  \nOverall, this is a technically strong and comprehensive paper that presents a significant advancement in region-aware medical vision-language modeling. Its contributions‚Äîparticularly the new dataset, novel region-centric tasks, and bilingual capability‚Äîenhance the interpretability and versatility of medical MLLMs. The work is highly significant and innovative, though its current limitations include the absence of clinical validation, limited discussion of computational demands, and missing failure analysis. Addressing these issues would further solidify the study‚Äôs clinical and practical relevance.\n\n---\n\n**Decision Recommendation**  \n**Minor Revision** ‚Äì The manuscript is a valuable and well-executed contribution that primarily requires expanded discussion on clinical applicability, computational cost, and error analysis to strengthen its impact and completeness.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThis work tackles a notable shortcoming of current medical multimodal large language models: they treat an image as a single, undifferentiated entity and cannot indicate which anatomical region is responsible for a particular textual output. To endow the models with spatial awareness, the authors propose three region‚Äëcentric tasks‚Äîregion‚Äëto‚Äëtext identification, text‚Äëto‚Äëregion detection, and grounded report generation. They also build a large‚Äëscale bilingual dataset, **MedRegInstruct**, that pairs image regions with textual descriptions and region‚Äëgrounded reports across a wide range of modalities (X‚Äëray, CT, MRI, pathology, dermatology, ophthalmology). Using this dataset together with other medical multimodal corpora, they fine‚Äëtune the vision‚Äëlanguage model InternVL, resulting in **MedRegA**, a multilingual medical MLLM that can reason about specific image regions. Experimental results demonstrate that MedRegA produces accurate region annotations, improves performance on visual question answering, report generation, and image classification, and can generate reports that are explicitly linked to image regions. The main contribution is the integration of region‚Äëcentric capabilities into a bilingual, general‚Äëpurpose medical AI system.\n\n---\n\n**## General feedback**\n\n*Significance.*  \nThe introduction of region grounding together with bilingual support addresses a real clinical demand for interpretable AI that can point to the precise anatomical structures underlying its predictions. Figure‚ÄØ2(a) nicely illustrates how this capability could increase trust and usability in practice.\n\n*Innovation.*  \nThe novelty of the paper is three‚Äëfold: (1) defining the three region‚Äëcentric tasks and assembling a sizable, partially automatically annotated dataset that spans many imaging modalities; (2) proposing a box‚Äëtoken encoding together with a ‚ÄúRegional‚ÄØCoT‚Äù inference scheme (see Figure‚ÄØ5); and (3) embedding both English and Chinese report generation within a single unified model. While grounding has been studied for natural‚Äëimage datasets, applying it systematically to diverse medical imaging modalities is a fresh contribution.\n\n*Evaluation.*  \nThe authors evaluate on a broad spectrum of benchmarks‚ÄîVQA (SLAKE, VQA‚ÄëRAD, PathVQA, PMC‚ÄëVQA), report generation (MIMIC‚ÄëCXR, IU‚ÄëXray, an in‚Äëhouse Chinese set), image classification (VinDr series, CheXpert, ISIC, etc.)‚Äîas well as the newly introduced region‚Äëcentric tasks (Tables‚ÄØ2‚Äë5). Reported gains are impressive (e.g., +8‚ÄØ% BLEU‚Äë1 on MIMIC‚ÄëCXR compared with MedDr, +27‚ÄØ% IoU on lesion‚Äëgrounded reports versus InternVL). A limitation, however, is that most tables present only point estimates; confidence intervals or statistical tests are missing, and baseline comparisons sometimes juxtapose zero‚Äëshot results with fine‚Äëtuned ones (Table‚ÄØ13).\n\n*Reproducibility.*  \nThe manuscript provides high‚Äëlevel information about data sources, hardware, and hyper‚Äëparameters (Section‚ÄØB). Important details such as the exact train/validation splits of MedRegInstruct, the LoRA rank and learning‚Äërate, the full set of prompt templates (only Figure‚ÄØ12 is shown), and the implementation of the Region‚ÄëAligned evaluation (Algorithm‚ÄØ1) are not released. Moreover, the MedRegInstruct dataset (particularly the in‚Äëhouse portion) and the trained checkpoints are unavailable, which hampers independent verification.\n\n---\n\n**## Specific comments / critiques**\n\n1. **Annotation quality** ‚Äì The automatically generated bounding boxes attain a modest 72‚ÄØ% agreement with human annotations (Section‚ÄØ3.2). The manuscript does not explore how this level of noise influences downstream performance.\n\n2. **Dataset split clarity** ‚Äì A 90‚ÄØ%/10‚ÄØ% split is mentioned for the Chinese in‚Äëhouse data, but the patient‚Äëlevel partitioning for the full MedRegInstruct corpus is not disclosed. Potential leakage across tasks therefore remains unclear (Section‚ÄØA).\n\n3. **Baseline fairness** ‚Äì Comparisons to existing medical MLLMs (MedFlamingo, LLaVA‚ÄëMed, MedDr) are largely zero‚Äëshot, whereas MedRegA benefits from fine‚Äëtuning on MedRegInstruct. A controlled experiment where the baselines are also fine‚Äëtuned on the same data would strengthen the claims; Table‚ÄØ13 only includes a limited 5‚ÄØK subset.\n\n4. **Bilingual evaluation limited** ‚Äì Chinese report generation is evaluated solely on an internal dataset (Table‚ÄØ8). No external Chinese benchmark is used, making it difficult to assess generalisation beyond the collected hospital data.\n\n5. **Statistical significance absent** ‚Äì All performance tables (e.g., Tables‚ÄØ1‚Äë7,‚ÄØ14) present single numbers without confidence intervals or hypothesis testing, limiting confidence in the robustness of the reported gains.\n\n6. **External grounding baselines omitted** ‚Äì Recent vision‚Äëlanguage models that support region grounding (e.g., RegionGPT, Kosmos‚Äë2) are not adapted for comparison. Consequently it is unclear whether the improvements stem primarily from the new dataset or from architectural choices.\n\n7. **Region‚ÄëAligned evaluation novelty not benchmarked** ‚Äì The proposed evaluation (Algorithm‚ÄØ1, Figure‚ÄØ6) is novel but is not compared against established grounding metrics such as the pointing game or mean IoU that are common in the vision‚Äëlanguage literature.\n\n8. **Ablation of model components missing** ‚Äì The impact of freezing the vision encoder and language model during alignment training versus jointly fine‚Äëtuning them is not quantified. Likewise, the benefit of the `<box>` token relative to a simple concatenation is not isolated.\n\n9. **Regional‚ÄØCoT details scarce** ‚Äì Table‚ÄØ14 shows substantial gains on multi‚Äëlabel classification and VQA, yet the exact prompting format, detection thresholds, and the mechanism by which region detections are fed back to the language model are not fully described (Figure‚ÄØ5 offers only a schematic).\n\n10. **Clinical validation only qualitative** ‚Äì The paper includes illustrative examples (Figures‚ÄØ2(a),‚ÄØ13,‚ÄØ17) but lacks a prospective or retrospective user study with radiologists that would substantiate the claimed interpretability improvements.\n\n---\n\n**## A suggested decision**\n\n**Reject**\n\n*Rationale.* While the manuscript presents an interesting direction‚Äîadding region‚Äëaware, bilingual capabilities to medical MLLMs‚Äîit falls short of the journal‚Äôs standards for methodological rigor, reproducibility, and comprehensive evaluation. Addressing the points above would be necessary before the work could be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a bilingual medical multimodal large language model designed to achieve region-level understanding and grounding within medical images. To overcome the limitation of current models that process an image as a single entity, the authors define three region-centric tasks‚Äîregion-to-text identification, text-to-region detection, and grounded report generation. They assemble a large bilingual dataset, *MedRegInstruct*, spanning multiple imaging modalities, and fine-tune the InternVL model to create *MedRegA*, a bilingual system capable of spatially grounded reasoning. The work is clearly structured and presents promising results, though important methodological and reproducibility issues remain.  \n\n**Major Comments**  \n1. **Annotation quality** ‚Äì Automatically generated bounding boxes show only 72‚ÄØ% agreement with human annotations, yet the effect of this noise on downstream tasks is not analyzed.  \n2. **Dataset splitting** ‚Äì Dataset partitioning is not fully specified, raising possible concerns about data leakage across tasks.  \n3. **Baseline fairness** ‚Äì Comparisons to previous MLLMs (MedFlamingo, LLaVA‚ÄëMed, MedDr) rely largely on zero‚Äëshot performance, while the proposed model benefits from fine‚Äëtuning. More balanced experiments would improve fairness.  \n4. **Limited bilingual evaluation** ‚Äì Chinese report generation is tested only on an internal dataset, leaving generalization across institutions unverified.  \n5. **Lack of statistical significance** ‚Äì Reported metrics lack confidence intervals or hypothesis testing, reducing support for claimed gains.  \n6. **Missing comparisons with external grounding models** ‚Äì No evaluation is provided against recent region‚Äëgrounded vision‚Äëlanguage baselines (e.g., RegionGPT, Kosmos‚Äë2).  \n7. **Unverified evaluation protocol** ‚Äì The new Region‚ÄëAligned metric (Algorithm‚ÄØ1) is not benchmarked against standard grounding metrics such as the pointing game or mean IoU.  \n8. **Missing ablation studies** ‚Äì The impacts of fine‚Äëtuning strategy and the `<box>` token are not isolated.  \n9. **Incomplete Regional‚ÄØCoT details** ‚Äì The prompting format and feedback mechanisms are insufficiently described.  \n10. **Limited clinical validation** ‚Äì Interpretability claims are supported only by qualitative examples; no study with clinical experts is presented.  \n\n**Minor Comments**  \n- Figures could better illustrate the technical pipeline, and additional details on hyperparameters, prompt templates, and LoRA settings would support reproducibility.  \n- Some tables combine zero‚Äëshot and fine‚Äëtuned baselines, and the rationale for this should be clarified.  \n- Dataset and checkpoint release would enable independent verification.  \n\n**Summary Paragraph**  \nThe paper makes a meaningful attempt to integrate spatial grounding and bilingual capabilities within a large medical vision‚Äëlanguage model, addressing an important interpretability gap. Its contributions in defining new region‚Äëcentric tasks and building a multimodal dataset are noteworthy. However, issues related to dataset transparency, statistical rigor, limited bilingual evaluation, and absent ablation analyses significantly weaken the empirical support for the proposed claims. The absence of publicly released data and checkpoints further restricts reproducibility.  \n\n**Decision Recommendation**  \n**Reject.** The work is conceptually interesting but does not meet expected standards for experimental rigor, transparency, and validation; substantial revision and additional analyses would be required before reconsideration.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Haonan Wang",
      "Honglong Yang",
      "Jiaji Mao",
      "Jun Shen",
      "Lehan Wang",
      "Xiaomeng Li",
      "Zehong Yang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2b1c7dbf52c032b412c014b02b84f27ac218e3ee.pdf",
    "remote_url": "https://openreview.net/pdf/2b1c7dbf52c032b412c014b02b84f27ac218e3ee.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
    "status": "completed",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Medical Foundation Model",
      "Multimodal Dataset",
      "Vision-Language Pretraining."
    ],
    "abstract": "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with multigranular annotations for more than 65 diseases. These multigranular annotations encompass both global information, such as modality and organ detection, and local information like ROI analysis, lesion texture, and region-wise correlations. Unlike the existing multimodal datasets, which are limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations in the form of image-ROI-description triplets without the need for any paired text descriptions. Specifically, data from over 30 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. We propose LLaVA-Tri by pretraining LLaVA on MedTrinity-25M, achieving state-of-the-art performance on VQA-RAD, SLAKE, and PathVQA, surpassing representative SOTA multimodal large language models. Furthermore, MedTrinity-25M can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. We will make our dataset available. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces MedTrinity-25M, a comprehensive and large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities with detailed annotations for more than 65 diseases. The dataset provides the most enriched annotations for various multimodal tasks, such as captioning, classification, and segmentation, and supports the pre-training of advanced multimodal medical AI models, achieving state-of-the-art performance.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe paper introduces MedTrinity-25M, the largest multimodal medical dataset to date, featuring multigranular annotations and containing over 25 million triplets (image-ROI-description). The development of an automated data annotation pipeline significantly scales up medical image-text data.\n2.\tWith the support of MedTrinity-25M, the pretrained CLIP and LLaVA models demonstrate better performance compared to previous methods.\n3.\tThe release of this dataset contributes to the medical AI community, providing researchers and practitioners with a valuable resource for advancing multimodal tasks and improving healthcare applications.\n\n### Weaknesses\n\n1.\tThe quality of the generated image-text data may not be sufficiently high. It is advisable to review the questions associated with this data. In the image-captioning constructed from MedTrinity-25M, we found that many basic imaging modalities were incorrectly identified. For instance, in the CT-RATE data used as the source, over 60,000 images were misidentified as MRI, more than 90,000 as X-ray, and even a small number were identified as endoscopy. Below, we provide some captions to illustrate this phenomenon present in MedTrinity-25M.\n‚Äú‚Äù‚Äù\n{\"image_path\":\"seg_train_126_a_2-right lung.nii-34.jpg\",\"id\":\"b573995e-2d11-11ef-bbea-f02f74942466\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the thoracic region, showing the heart, part of the lung, and the spine. The heart is centrally located with the lung on either side and the spine running vertically in the background. The region of interest, located in the left-center at the bottom of the image, appears to have a different texture and intensity compared to the surrounding tissue, suggesting a possible abnormality. This region's relative position is towards the lower left side of the image, adjacent to the lower part of the lung. The content within this region may indicate a disease process, which could be related to or affecting the adjacent lung tissue. Given the proximity to the lung, it is possible that the abnormality could be influencing or being influenced by the pulmonary structures.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_126_a_2-right lung.nii-11.jpg\",\"id\":\"b57345c6-2d11-11ef-a899-f02f74942466\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the thoracic region, showing the heart, part of the lung, and the spine. The heart is centrally located with the lung on either side and the spine visible posteriorly. The region of interest, located in the lower-middle left-center of the image, shows an area with altered signal intensity, which is indicative of a pathological condition. This area is situated within the lung tissue and is characterized by a texture and signal intensity that differ from the surrounding healthy lung tissue, suggesting the presence of a disease process such as consolidation, infection, or a mass.\\n\\nThe region of interest may be related to the surrounding lung tissue in that it could represent a localized disease process affecting the lung, potentially leading to or resulting from changes in the adjacent lung parenchyma. Given the nature of MRI imaging and the appearance of the region, it is possible that this area could be associated with a process such as inflammation, demyelination, or a neoplastic growth, which may have implications for the function of the adjacent lung tissue.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_14684_a_1-covid-19 infection.nii-76.jpg\",\"id\":\"a77ee972-2d78-11ef-bdc4-f02f74942576\",\"caption\":\"The image is a radiographic scan, likely a chest X-ray, showing the thoracic region with the trachea and main bronchi appearing open, indicating no occlusive pathology. The lungs exhibit minimal emphysematous changes, and there are pleuroparenchymal sequelae changes at the apex of both lungs. Additionally, there are linear atelectasis in the middle lobe of the right lung and the lingular segment of the left lung upper lobe. The heart contour and size are normal, and there are no pleural or pericardial effusions. The mediastinal structures are not optimally evaluated due to the absence of contrast material, and the mediastinal main vascular structures are of normal width. No enlarged lymph nodes or pathological wall thickness increase are observed in the esophagus. The thoracic vertebral corpus shows normal heights, alignment, and densities, with osteophytes at the vertebral corpus corners, and the neural foramina are open.\\n\\nThe region of interest, located centrally and in the upper-middle area of the image, occupying approximately 0.3% of the area, corresponds to the lung apices where pleuroparenchymal sequelae changes are noted. These changes are characterized by alterations in lung parenchyma texture, which may appear as irregularities or areas of increased density compared to the surrounding lung tissue.\\n\\nThe pleuroparenchymal sequelae changes in the lung apices may be related to the emphysematous changes in the lungs, as both conditions can result from chronic inflammatory processes, suggesting a possible pathophysiological connection between the two findings.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_9381_a_2-covid-19 infection.nii-145.jpg\",\"id\":\"9880af92-2c1e-11ef-b9dd-f02f74942466\",\"caption\":\"The image is a chest X-ray showing a cross-sectional view of the thoracic cavity, including the lungs, heart, and part of the spine. A region of interest is located at the periphery of the thoracic cavity, likely within the lung tissue, which appears as a darker area compared to the surrounding lung parenchyma. The region of interest, which is abnormal, shows an area of increased opacity that suggests the presence of a pathological condition, possibly a mass or lesion within the lung tissue. This abnormal area is indicative of a disease process, which could be related to the surrounding lung tissue either as a primary pathology or as a secondary effect of a systemic condition affecting the lung. The abnormality's proximity to other structures within the thoracic cavity, such as the pleura or lung tissue could imply a relationship where the disease process in the region of interest may have originated from or is affecting adjacent areas, potentially impacting nearby structures due to its location within the thoracic cavity.\",\"source\":\"ct_rate\"}\n{\"image_path\":\"seg_train_7973_a_1-right lung.nii-84.jpg\",\"id\":\"88af61aa-2d11-11ef-959e-f02f74942466\",\"caption\":\"The image is a sagittal section of an endoscopic view of the thoracic region, showing the spine, part of the lung, and the surrounding thoracic structures. A region of interest is located at the lower-middle part of the image, horizontally left-center, occupying approximately 0.6% of the area. The region of interest, located in the lower-middle left-center of the image, displays an abnormality in the lung tissue, which appears to be a small, localized area with a different texture and possibly altered density compared to the surrounding lung parenchyma, suggesting a pathological change. This abnormality could be related to the adjacent lung tissue, potentially indicating a localized disease process or lesion that may be affecting or resulting from the surrounding lung tissue, given the proximity and the nature of the disease knowledge provided.\",\"source\":\"ct_rate\"}\n‚Äú‚Äù‚Äù\nMoreover, there are also issues with modality misidentification in the image-captioning derived from the quilt-1m dataset. For example, pathology images were misidentified as X-ray and MRI, as follow, \n‚Äú‚Äù‚Äù\n{\"image_path\":\"G-tdJ0oZxJ4_image_e74e3372-a40e-40e3-ac06-e3d53eeab845.jpg\",\"id\":\"f5ce616b-89ca-41f6-b820-b480bb3327af\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the lung, showing a cross-sectional view of the thoracic cavity. A region of interest is located at the top of the image, which appears to be in the upper part of the lung field, likely within the upper lobe, given the position of the lung's anatomy. The region of interest, which is abnormal, exhibits an unusual appearance compared to the surrounding lung tissue, possibly indicating a neoplastic process. This abnormality is characterized by a difference in texture and density, which may suggest a pathological change, such as a mass or lesion within the lung tissue. The affected area's relationship to the rest of the lung tissue could imply a localized pathological process that may be the primary site of the disease, potentially impacting or being impacted by adjacent lung structures due to its proximity to other regions, although the exact relationship depends on the nature of the pathology and its progression.\",\"source\":\"quilt_1m\"}\n{\"image_path\":\"895313503048712192_0.jpg\",\"id\":\"0b53a8c8-5661-449a-b39a-f92887fceb86\",\"caption\":\"The image is a magnetic resonance imaging (MRI) scan of the brain, showing a cross-sectional view that includes brain tissue with various shades of gray indicating different structures and densities. A region of interest is located at left-center part of the image, which appears to be in the cerebral hemisphere, likely within the white matter of the brain. The region of interest exhibits an abnormality that differs in texture and possibly size from the surrounding brain tissue, suggesting the presence of a pathological change. This abnormal area could be related to the surrounding brain structures, potentially affecting or being affected by them due to its proximity and the nature of brain tissue, which may indicate a pathological process such as a tumor, edema, or other brain abnormalities. The abnormality's characteristics, such as altered signal intensity, could be indicative of the disease process affecting the brain tissue's function or structure.\",\"source\":\"quilt_1m\"} \n{\"image_path\":\"962043872649011205_0.jpg\",\"id\":\"f94e8ba4-7ddf-450e-9616-d4681e9dcf02\",\"caption\":\"The image is a chest X-ray image of a 15-year-old boy's hand, showing the left side of the image is a chest X-ray showing the hand's anatomy, including the bones of the hand, with the focus on the epiphysis and possibly the bones of the hand's proximal and distal parts such as the clavicles, ribs, clavicles, and parts of the spine, which are essential for various activities like running, throwing, and punching, along with the presence of the epiphyseal plate and the development of the trapezius, which are crucial for a 15-year-old boy's hand, suggesting a focus on the skeletal structure and function of the hand, including the development of the hand's bones such as the clavicles and the development of the wrist bones, which are crucial for various activities like walking, running, and running, as well as the development of the wrist bones of the hand, which are essential for a growing hand and are indicative of a developing hand and are crucial for a growing hand and are typically found in activities such as playing with play and sports, suggesting a comprehensive evaluation of the hand's anatomy and function.\",\"source\":\"quilt_1m\"}\n{\"image_path\":\"1301506707483439105_1.jpg\",\"id\":\"7b0161c9-1451-4f27-9278-aff87726bacb\",\"caption\":\"The image is a lateral chest X-ray image of a 15-year-old boy's hand, showcasing the epiphysis of the wrist bones, which are the bones of the hand, including the radius of the wrist bones, clavicles and the bones of the hand's growth plate are visible, such as the clavicles, ribs, and vertebrae are the primary focus of the image is a close-up view of a lateral chest X-ray image, showcasing the epiphysis of the hand's anatomy, which are essential for various stages of development and growth patterns. The image is a detailed X-ray of the hand's anatomy, displaying the epiphyseal growth plate and development of the wrist bones of a 15-year-old boy's hand, with the bones of the epiphyseal plate visible in the image are the primary organ of interest, which includes the epiphyseal plate and epiphyseal plate, as well as the epiphyseal plate's development is crucial for diagnosing the stage of developmental stage of the wrist bones, typically found in the wrist bones of a 15-year-old boy's hand, which is crucial for understanding the zone of developmental development, and the image is likely to be a lateral chest X-ray image showing the epiphyseal plate's development is crucial for diagnosing the stage of developmental stage of the wrist, with the epiphyseal plate being the primary focus of the image.\",\"source\":\"quilt_1m\"}.\n2.\tThe experiments lack comprehensiveness. The comparison of multimodal large models is limited to VQA-RAD, SLAKE, and PathVQA. A broader range of specialized benchmarks, such as the health subset of MMMU, could provide a more robust comparison of the multimodal large models‚Äô performance.\n\n### Questions\n\n1.\tThe data construction pipeline consists of two main steps: first, generating 200,000 multigranular textual descriptions via GPT-4V, and then fine-tuning LLaVA with this dataset. In my opinion, the performance of the trained model LLaVA-MedCap is significantly influenced by the quality of the data generated by GPT-4V. However, in Table 3, GPT-4V's performance falls far short of LLaVA-Tri. Why did you choose to use GPT-4V to generate the supervised fine-tuning data for the fine-tuning process? In my recent review of the publicly released MedTrinity-25M data, I found that many of the generated data instances were of average quality, with even simple modalities being incorrect.\n2.\tIn Section 4.1, it is stated that ‚ÄúThe model is fine-tuned for three epochs on each VQA dataset and evaluated accordingly.‚Äù However, this evaluation setup is not fair when comparing with the methods presented in the table. Some results in Table 3 appear to be directly extracted from llava-med‚Äôs table, but these methods were not fine-tuned on the training set of the VQA benchmark. In contrast, the accuracy of the chosen llava-med method was achieved after fine-tuning for 15 epochs on the corresponding VQA benchmark training set, making this comparison inappropriate.\nCurrently, the evaluation paradigm for multimodal large models (MLLM) typically involves assessing the model directly on multiple benchmarks after two stages of pretraining and supervised fine-tuning. Therefore, I suggested distinguishing the results presented: there should be a comparison of the results not fine-tuned on the corresponding training set against the models that were not fine-tuned. Additionally, when comparing with methods fine-tuned on the corresponding training set, the number of fine-tuning epochs (including for the comparison methods) should be clearly indicated. If possible, please also conduct an ablation study regarding the number of fine-tuning epochs.\n3.\tIn Table 3, comparing CLIP-like models with LLaVA, a multimodal large model, seems inappropriate. It would be better to categorize them into two groups: one for CLIP models and another for MLLM.\n4.\tThe evaluation details in Table 3&4 are not clearly explained. What do \"open\" and \"close\" specifically refer to?  Please add a brief explanation of the \"open\" and \"close\" terms in the table caption or in the text describing the results.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n- Yes, Other reasons (please specify below)\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "human",
          "comment": "There is overlap between major and minor comments, such as whether automated annotations have been manually verified."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **MedTrinity‚Äë25M**, a large‚Äëscale multimodal medical dataset comprising over 25‚ÄØmillion image‚ÄìROI‚Äìdescription triplets across ten modalities and more than sixty‚Äëfive disease categories. It aims to provide a comprehensive resource for multimodal tasks such as captioning, classification, and segmentation, thereby facilitating pretraining of advanced multimodal AI models. Overall, the work is ambitious and potentially valuable for the medical AI community. However, concerns arise regarding the quality of the automatically generated annotations, the fairness and completeness of experimental evaluations, and unclear methodological details in the comparison design.\n\n---\n\n**Major Comments**  \n1. **Data Quality Issues:** The reviewer finds that many image‚Äìtext pairs in MedTrinity‚Äë25M exhibit incorrect modality identification. Examples indicate CT images classified as MRI, X‚Äëray, or even endoscopy, and similar misclassifications occur in samples derived from the Quilt‚Äë1M dataset. These raise questions about the accuracy of the automatic caption generation process and the reliability of the resulting dataset.  \n2. **Limited Experimental Scope:** Model comparisons are restricted to three datasets (VQA‚ÄëRAD, SLAKE, PathVQA). Inclusion of more specialized or diverse benchmarks, such as the health subset of MMMU, would enhance the robustness of the evaluation.  \n3. **Choice of GPT‚Äë4V for Data Generation:** Since GPT‚Äë4V‚Äôs own performance is lower than that of LLaVA‚ÄëTri, it is unclear why GPT‚Äë4V was used to generate supervision data for fine‚Äëtuning. The reviewer questions whether this affects data quality and overall model performance.  \n4. **Fairness of Evaluation Setup:** The comparison in Table‚ÄØ3 may be inappropriate, as some baseline results were obtained under different fine‚Äëtuning regimes (e.g., 15‚ÄØepochs versus 3). The reviewer suggests distinguishing results obtained with and without fine‚Äëtuning, clearly stating the number of epochs used, and conducting an ablation on fine‚Äëtuning duration.  \n5. **Comparison Structure:** It would be more appropriate to separate CLIP‚Äëlike models from large language‚Äìvision models (LLaVA) rather than comparing them directly.  \n6. **Unclear Terminology:** The meaning of the ‚Äúopen‚Äù and ‚Äúclose‚Äù evaluation setups in Tables‚ÄØ3‚ÄØ&‚ÄØ4 is not explained and should be clarified in the captions or main text.\n\n---\n\n**Minor Comments**  \n- The captioned examples reveal inconsistent modality terminology and possible mislabeling that require correction.  \n- Some descriptions could be streamlined for clarity when presenting evaluation procedures or dataset construction steps.  \n\n---\n\n**Summary Paragraph**  \nThis submission introduces a uniquely large multimodal medical dataset with potentially substantial community benefit. Strengths include dataset scale, diversity, and demonstrated improvements in model performance. Nevertheless, the review identifies significant weaknesses: questionable data quality due to annotation inaccuracies, limited and possibly unfair comparative analysis, and insufficient reporting of experimental settings. Addressing these issues‚Äîparticularly improving annotation reliability and expanding the evaluation to more benchmarks‚Äîwould substantially strengthen the paper.\n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million images across 10 imaging modalities and covering more than 65 diseases. The dataset features multigranular annotations that include both global information and local information. The authors develop an automated pipeline that generates image-ROI-description triplets without the need for paired textual descriptions. This pipeline utilizes domain-specific expert models to identify ROIs and prompts multimodal large language models (MLLMs) to produce detailed annotations through retrieval-augmented generation (RAG).\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- Dataset Comprehensiveness: The dataset is exceptionally extensive, encompassing a wide array of imaging modalities, diseases, and anatomical structures. This diversity significantly enhances its comprehensiveness and utility for various medical AI applications.\n- Construction Pipeline through Advanced Models: The utilization of domain-specific expert models and multimodal large language models (MLLMs) for annotation substantially enriches the dataset. This approach adds multigranular details that improve the quality and depth of the annotations, making the dataset more valuable for training sophisticated models.\n\n### Weaknesses\n\n- Limited Applicability for Visual Data without ROIs: The proposed pipeline relies heavily on regions of interest (ROIs) for constructing multimodal data pairs. For visual data that lack explicit ROI annotations, it is unclear how the pipeline can be effectively applied. This limitation may restrict the dataset's usability across the full spectrum of medical images, particularly those where ROI determination is challenging or subjective.\n- Impact of Multigranular Information Not Fully Explored: While Tables 3 and 4 validate the effectiveness of the benchmark in general terms, the paper does not specifically assess how the inclusion of multigranular information, such as ROIs, influences the performance of medical MLLMs. Given that ROI is a pivotal component of the dataset, a focused evaluation comparing models trained with and without multigranular information would strengthen the claims about its benefits.\n- Insufficient Discussion of Related Benchmarks: The paper lacks a thorough discussion of existing med-MLLM benchmarks that involve multigranular information. Notably, works like \"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI\" and \"A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models\" are not cited or compared against. Including a comparative analysis would provide context and clarify how MedTrinity-25M advances the field relative to existing resources.\n- Lack of Evaluation on Multigranular-Specific Tasks: The dataset is primarily used for training and is evaluated only on VQA-RAD, SLAKE, and PathVQA. These evaluations may not fully capture the effectiveness of the dataset's multigranular annotations since they do not specifically target tasks designed to leverage such detailed information. This omission raises questions about the practical benefits of the multigranular data provided.\n- Challenges with Unique Medical Image Descriptions in RAG: The Retrieval-Augmented Generation (RAG) technique used for annotation relies on a general medical knowledge base. However, medical images often have unique features and presentations, even within the same disease category. For example, Atelectasis can manifest differently in chest X-rays depending on the affected lobe. The paper does not address how the RAG system accounts for these unique variations when the initial visual datasets typically offer only generic classification labels. This could impact the accuracy and specificity of the generated annotations.\n\n### Questions\n\n- Handling of Visual Data Without ROIs: For medical images that lack explicit ROI annotations, how does your pipeline construct the corresponding multimodal data pairs? Is there a mechanism to generate or infer ROIs in such cases, or is the pipeline limited to images where ROIs are predefined?\n- Assessing the Impact of Multigranular Information on Med-MLLMs: Considering that ROIs and multigranular annotations are central to your dataset, have you conducted experiments to evaluate how these features specifically affect the performance of medical MLLMs? Can you provide insights or results that demonstrate the advantages of including multigranular information in model training?\n- Comparison with Existing Multigranular Benchmarks: Could you elaborate on how MedTrinity-25M compares with existing benchmarks that involve multigranular information? What distinguishes your dataset from these, and how does it contribute uniquely to the advancement of general medical AI?\n- Evaluation on Multigranular-Specific Tasks: Given that the dataset is evaluated only on VQA-RAD, SLAKE, and PathVQA, which may not fully utilize multigranular annotations, do you have plans to test your dataset on tasks specifically designed for multigranular information? How can you demonstrate the effectiveness of your dataset's detailed annotations in improving model performance on such tasks?\n- Addressing Unique Medical Image Presentations in RAG: Medical images often present unique and variable features even when categorized under the same disease label. How does your RAG approach handle the specificity and variability of individual medical image descriptions? For instance, with conditions like Atelectasis that can manifest differently in imaging, how does the system ensure that the generated annotations accurately reflect these variations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "ai",
          "comment": "The short headings in the major comments did not adequately reflect the key points of the subsequent comments."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity-25M*, a large-scale multimodal medical dataset comprising over 25 million images across 10 imaging modalities and spanning more than 65 diseases. The dataset includes multigranular annotations at both global and local levels, generated through an automated pipeline that integrates domain-specific expert models with multimodal large language models (MLLMs) using a retrieval-augmented generation (RAG) framework. The work is clearly described, and the dataset‚Äôs scale and diversity represent significant potential value for medical AI research.\n\n**Major Comments**  \n1. **Dependence on ROI Annotations:** The annotation pipeline relies heavily on regions of interest (ROIs), limiting applicability to images without explicit ROI information. It remains unclear whether the approach can be adapted for such data or if its use is restricted to images with predefined ROIs.  \n2. **Limited Evaluation of Multigranular Benefits:** Although general validation results are provided, the paper does not explicitly assess the effect of multigranular annotations‚Äîsuch as ROIs‚Äîon model performance. Comparative experiments with and without such annotations would strengthen claims about their contribution.  \n3. **Lack of Comparison with Related Benchmarks:** The work does not contextualize its contributions relative to existing multigranular medical benchmarks, such as GMAI-MMBench or Spectrum Evaluation Benchmark for Medical MLLMs. A comparative discussion would clarify the unique advances brought by MedTrinity-25M.  \n4. **Evaluation Scope:** The dataset is evaluated only on VQA-RAD, SLAKE, and PathVQA, which may not fully leverage detailed, multigranular information. Evaluating on tasks designed specifically for such annotations would better demonstrate their utility.  \n5. **Specificity in RAG-Generated Descriptions:** Because the RAG technique relies on general medical knowledge bases, it may not capture case-specific variations common in medical imaging (e.g., differing manifestations of the same disease). The paper does not address how this issue is mitigated, which may affect annotation accuracy.\n\n**Minor Comments**  \n- Clarify whether the pipeline includes mechanisms to infer or generate ROIs for images without them.  \n- Expand discussion of existing MLLM datasets to contextualize the dataset‚Äôs novelty.  \n- Ensure references to tables and figures explicitly connect to the claims about dataset utility.\n\n**Summary Paragraph**  \nOverall, the paper introduces a highly comprehensive dataset supported by a technically sophisticated construction pipeline. The main strengths lie in dataset scale and the combination of expert and MLLM-based annotation. However, evaluation limits, lack of comparative benchmarking, and uncertain generalization to images without explicit ROIs reduce the clarity of its demonstrated impact. Addressing these points would significantly enhance the rigor and utility of the work.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The contribution is promising, but additional evaluation and comparative analysis are needed to substantiate the claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper presents MedTrinity-25M, a large-scale multimodal medical dataset with 25 million images across 10 modalities and detailed annotations for over 65 diseases. Enhanced by automated multigranular annotations, it supports tasks like image captioning and report generation. The LLaVA-Tri model, pre-trained on MedTrinity-25M, achieved state-of-the-art results on multiple medical VQA benchmarks.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The dataset uses an automated process with medical knowledge retrieval and domain-specific models, greatly reducing manual labeling needs.\n2. With over 25 million image-ROI-description triplets, the dataset supports classification, segmentation, report generation, and spans multiple medical domains.\n3. The LLaVA-Tri model, pre-trained on MedTrinity-25M, performs exceptionally well across multiple benchmarks, showcasing the dataset‚Äôs potential to enhance medical AI applications.\n\n### Weaknesses\n\n1. Although the labeling process is automated, its reliance on domain-specific models may limit scalability when handling new modalities or emerging diseases.\n2. The accuracy of automated labeling may fall short of expert-labeled datasets, potentially impacting performance in critical medical applications. How can a high standard of automated labeling be ensured?\n3. With data from over 30 sources, potential biases in image quality, demographics, or disease distribution call for a deeper integration analysis.\n4. Performance depends on external medical knowledge bases, risking inconsistency if not updated, and the LLaVA-Tri comparison may be biased due to overlap with MedTrinity-25M's data sources.\n\n### Questions\n\n1. Given the reliance on automated processes and external knowledge sources, how is labeling consistency ensured in the data generation process? Additionally, has it been validated by human experts?\n2. Has a comparison with expert-labeled datasets been considered to further quantify the quality of automated labeling?\n3. How does the dataset address potential biases in source data? For example, is there a mechanism to prevent overrepresentation of certain demographics?\n4. What is the expandability of the labeling process for new, unrepresented modalities or diseases? Does it offer strong scalability?\n5. Although LLaVA-Tri achieved good performance on RAD, PathVQA, and SLAKE, how do the authors ensure that MedTrinity-25M does not contain data from these datasets? If these datasets were included, the model may have seen the questions and answers during training, leading to artificially high accuracy.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 3,
          "constructiveness": 4,
          "stance": 4,
          "source": "human",
          "comment": "Some comments are not accurate, e.g., the seventh comment (\"Key pre‚Äëtraining parameters for LLaVA‚ÄëTri (epochs, learning rate, batch size) are undisclosed\"), where the authors have claimed that \"The model is fine-tuned for three epochs on each of the three VQA datasets and evaluated accordingly\" in the manuscript."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MedTrinity‚Äë25M**, a large‚Äëscale multimodal medical dataset containing 25‚ÄØmillion images from ten imaging modalities and detailed annotations for over 65 diseases. The dataset is built through automated multi‚Äëgranular annotation that supports a variety of tasks, including classification, segmentation, report generation, and captioning. In addition, the authors present the **LLaVA‚ÄëTri** model, pre‚Äëtrained on this dataset, which reportedly achieves state‚Äëof‚Äëthe‚Äëart performance on several medical visual question answering (VQA) benchmarks. Overall, the paper is clearly written and conveys a significant scaling effort in medical imaging data creation.  \n\n**Major Comments**  \n1. **Automation and Scalability:** The automated labeling process substantially reduces manual annotation but may not scale easily to new modalities or emerging diseases that were not covered by domain‚Äëspecific models.  \n2. **Annotation Quality:** Automated labels may not achieve the precision of expert annotations, raising concerns about reliability in clinical or safety‚Äëcritical uses. The paper should explain how labeling quality is validated and maintained.  \n3. **Dataset Biases:** With data aggregated from more than 30 sources, potential biases in image quality, demographic distribution, and disease prevalence need deeper analysis and mitigation strategies.  \n4. **Knowledge Base Dependence:** Since the approach draws upon external medical knowledge bases, its performance could vary with updates to these sources. The paper should clarify versioning and update practices.  \n5. **Benchmark Overlap:** There is a risk that MedTrinity‚Äë25M overlaps with benchmark datasets (e.g., RAD, PathVQA, SLAKE), which could inflate evaluation results. Explicit verification of non‚Äëoverlap is necessary.  \n\n**Minor Comments**  \n- Clarify whether human experts have reviewed a subset of automated annotations.  \n- Provide more detail on expandability to unrepresented modalities or diseases.  \n- Include quantitative comparison with expert‚Äëlabeled datasets to assess annotation accuracy.  \n\n**Summary Paragraph**  \nThis work presents an ambitious and potentially impactful dataset that could advance multimodal medical AI research. Strengths include the scale of data collection, broad domain coverage, and demonstrated utility in downstream tasks. However, concerns remain regarding annotation reliability, possible dataset bias, dependence on external knowledge sources, and evaluation fairness. Addressing these points would enhance confidence in both the dataset‚Äôs reproducibility and the robustness of the reported model performance.  \n\n**Decision Recommendation:** **Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript proposes a methodological framework for automated, large‚Äêscale generation of multimodal medical datasets. Though the main product is a dataset (MedTrinity‚Äë25M), the work also introduces a structured pipeline‚Äîintegrating expert segmentation models, retrieval‚Äëaugmented generation, and multimodal large language models‚Äîto synthesize ‚Äúimage‚ÄìROI‚Äìdescription‚Äù triplets. This development directly concerns imaging data curation and quantitative annotation methodology, falling within TMI‚Äôs methodological scope.\n\n2. **Novelty & Contribution Level**  \nThe pipeline departs from existing datasets (e.g., MIMIC‚ÄëCXR, PMC‚ÄëOA, Quilt‚Äë1M) by removing the need for expert text pairing. Generative annotation of multigranular information (global + local + regionwise correlations) constitutes a fresh methodological idea. While some components rely on existing tools (e.g., pretrained LLMs, segmentation networks), their orchestration for scalable medical data generation and evaluation offers an original contribution. The empirical section on re‚Äëtraining LLaVA provides proof‚Äëof‚Äëconcept rather than foundational modeling advance, so the novelty centers on dataset construction and validation.\n\n3. **Technical and Experimental Rigor**  \nThe description is detailed: data sources (30+, 25‚ÄØM images), clear pipeline steps (metadata integration, ROI localization, knowledge retrieval), and both expert and LLM evaluations are provided. However, reproducibility would benefit from explicit release of processing code, model prompts, and quality control statistics per modality. The medical knowledge retrieval and privacy safeguards need slightly more elaboration (e.g., IRB considerations for reused clinical data).\n\n4. **Clarity and Presentation**  \nThe paper is mostly clear though dense. Occasional grammar and formatting issues (e.g., duplicated spaces, capitalization) should be corrected. Figures effectively illustrate qualitative comparisons. The flow between Sections‚ÄØ3.2.1‚Äì3.2.2 could be tightened for readability.\n\n5. **Ethical and Reproducibility Compliance**  \nSources appear to derive from publicly available datasets, and data are said to be de‚Äëidentified. The manuscript asserts public release but should include a clear statement on license compliance and absence of protected health information. Code and generation prompts are outlined but not yet openly documented‚Äîimportant for transparency and validation.\n\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThe paper presents *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical image dataset (‚âà25‚ÄØM samples across‚ÄØ10‚ÄØmodalities). It introduces an automated pipeline that combines metadata integration, ROI localization (via ensemble of domain‚Äëspecific models), and retrieval‚Äëaugmented generation with medically tuned LLMs to produce structured ‚Äúimage‚ÄìROI‚Äìdescription‚Äù triplets. Using this dataset, the authors pre‚Äëtrain an enhanced multimodal model (*LLaVA‚ÄëTri*) that demonstrates improved accuracy on standard medical VQA benchmarks.\n\n**2. Strengths**\n- First systematic framework to scale medical multimodal data generation without human text pairing.  \n- Detailed integration of vision and language modules, enabling multigranular annotations (global/local/relational).  \n- Extensive comparative evaluation against existing datasets and validation using both expert and LLM scoring.  \n- Demonstrated utility through downstream performance gains on multiple benchmarks.  \n\n**3. Weaknesses**\n- Limited methodological novelty in model architecture (fine‚Äëtuning existing components).  \n- Ethical and data‚Äëuse clarifications are brief; legal compliance for redistributed data is not fully specified.  \n- Evaluation relies partly on automatic scoring by GPT‚Äë4V, which may introduce bias; more human validation would strengthen claims.  \n- Quantitative quality metrics (e.g., inter‚Äërater consistency, uncertainty) are limited.  \n- Some figures and tables could be condensed or moved to supplementary to improve readability.\n\n**4. Major Comments**\n1. **Reproducibility and Transparency** ‚Äì Please provide all code, prompts, and configuration for the data‚Äëgeneration pipeline to ensure evaluators can verify annotation quality.  \n2. **Quality Control & Validation** ‚Äì Expand human evaluation beyond‚ÄØ200‚ÄØsamples, report inter‚Äëobserver agreement, and discuss potential propagation of LLM hallucination or bias.  \n3. **Ethical/Data Licensing Statement** ‚Äì Explicitly describe data governance (licenses of reused datasets, patient consent status, IRB considerations).  \n4. **Scientific Positioning** ‚Äì Clarify the methodological core: is the novelty primarily the scalable triplet‚Äëgeneration algorithm or the resultant dataset? Explicit theoretical or algorithmic framing would help position it within imaging methodology rather than data release.  \n5. **Clinical Relevance** ‚Äì Discuss how multigranular annotations concretely benefit clinical imaging tasks (e.g., lesion quantification, multimodal fusion).  \n\n**5. Minor Comments**\n- Correct typographical inconsistencies (e.g., ‚ÄúLLaV‚ÄØA‚ÄëTri by pretraining LLaV‚ÄØA‚Äù ‚Üí ‚ÄúLLaVA‚ÄëTri, pretrained on‚Ä¶‚Äù).  \n- Ensure references are consistent in style (some duplicates and capitalization issues).  \n- Improve figure legends for self‚Äëcontainment, particularly Figs.‚ÄØ3‚Äì6.  \n- Include dataset sampling statistics by disease and modality in the main text or supplement for quick reference.  \n\n**6. Overall Recommendation**  \n**Major Revision** ‚Äî The work has high potential impact as an infrastructure and methodology paper but requires stronger demonstration of data quality assurance, clearer ethical governance, and more explicit methodological framing to reach IEEE‚ÄØTMI standards.\n\n**7. Confidence Level:**‚ÄØ4‚ÄØ/‚ÄØ5  \n\n---\n\n**Guidance Summary**  \nTo reach TMI publishability, the authors should:  \n- Strengthen reproducibility and ethics documentation,  \n- Substantiate annotation validity with larger expert review,  \n- Emphasize methodological advances (automation framework, retrieval‚Äìgeneration coupling) rather than purely dataset scale.  \nWith these refinements, the paper could represent a valuable contribution to medical imaging data methodology.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 3,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 2,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical imaging dataset comprising approximately‚ÄØ25‚ÄØmillion samples across ten modalities. It describes an automated pipeline that integrates metadata aggregation, region‚Äëof‚Äëinterest (ROI) detection, knowledge retrieval, and multimodal large language models to generate structured ‚Äúimage‚ÄìROI‚Äìdescription‚Äù triplets. The study demonstrates the pipeline‚Äôs capability through the training of a model (*LLaVA‚ÄëTri*) that shows improved performance on several medical visual question answering benchmarks. Overall, the submission is within the methodological scope of medical imaging, well documented in many parts, and potentially impactful, though several aspects require strengthening for clarity, reproducibility, and ethical transparency.  \n\n**Major Comments**  \n1. **Reproducibility and Transparency** ‚Äì The pipeline is carefully described, yet full reproducibility requires public release of generation code, prompts, and configuration files. Clear quality‚Äëcontrol statistics for each modality would further support validation.  \n2. **Annotation Quality and Human Evaluation** ‚Äì Current validation relies heavily on automatic scoring by GPT‚Äë4V and a limited expert sample (~200 cases). Broader human evaluation, together with inter‚Äëobserver consistency metrics, is needed to confirm annotation reliability and mitigate possible model hallucination or bias.  \n3. **Ethical and Data Governance** ‚Äì While the manuscript claims use of de‚Äëidentified public data, licensing, consent status, and institutional approval (IRB) should be explicitly clarified to ensure legal reusability of redistributed data.  \n4. **Methodological Positioning** ‚Äì The paper should clearly articulate whether its novelty lies in the scalable triplet‚Äëgeneration algorithm or primarily in dataset assembly. A more formal framing of the underlying methodological innovation would strengthen its scientific contribution.  \n5. **Clinical and Practical Relevance** ‚Äì The discussion could better elaborate how the produced multigranular annotations translate to practical clinical or research applications, such as lesion quantification or multimodal feature fusion.  \n\n**Minor Comments**  \n- Correct minor typographical and formatting issues (spacing, capitalization, inconsistent terminology such as ‚ÄúLLaV‚ÄØA‚ÄëTri‚Äù).  \n- Ensure consistent reference formatting and remove duplicates.  \n- Improve figure legends and streamline figures/tables for readability; move less critical visuals to supplementary material.  \n- Provide concise modality‚Äë and disease‚Äëlevel dataset statistics in the main manuscript or supplement.  \n\n**Summary Paragraph**  \nThe work offers a promising contribution to large‚Äëscale medical image‚Äìtext data generation by integrating several pretrained components into a coherent, automated framework. Its strengths include comprehensive pipeline design, broad modality coverage, and demonstration of downstream model benefits. However, reproducibility, ethical documentation, and rigorous validation require further elaboration before the dataset and methodology can be fully trusted and reused. Addressing these concerns will enhance both credibility and impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The study is methodologically significant and potentially impactful but needs stronger evidence of data quality control, explicit ethical governance, and clearer articulation of its scientific novelty before it can be recommended for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MedTrinity-25M, a large-scale multimodal medical dataset containing over 25 million image-ROI-description triplets across 10 modalities and 65+ diseases. The authors develop an automated pipeline that generates multigranular annotations without requiring paired text descriptions, using metadata integration, ROI localization via expert models, medical knowledge retrieval, and multimodal large language models (MLLMs) for text generation. The resulting dataset provides structured descriptions covering five attributes: modality, organ detection, ROI analysis, lesion texture, and region-wise correlations. To demonstrate effectiveness, they propose LLaVA-Tri, achieving state-of-the-art performance on VQA-RAD (81.6%), SLAKE (87.8%), and PathVQA (82.8%) benchmarks. The dataset aims to support various medical AI tasks including captioning, report generation, classification, and segmentation.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical and Technical Formulation Gaps**\n  - The paper lacks mathematical formalization of the ROI localization process and coordinate system transformations mentioned in Section 3.2.1, making reproducibility difficult\n  - No quantitative metrics are provided for the \"left-right reversal in image representation\" for X-ray and MRI modalities (Section 3.2.1, Figure 6)\n  - The vector similarity search methodology using Med-CPT encoder and Faiss indexing lacks technical specifications such as distance metrics, dimensionality, and retrieval thresholds (Section 3.2.1)\n\n‚Ä¢ **Insufficient Evaluation Methodology**\n  - The quality evaluation relies on only 200 samples from a 25M dataset (Table 2), representing 0.0008% coverage, which may not be statistically representative\n  - Expert evaluation methodology lacks details about evaluator qualifications, inter-rater agreement, or evaluation protocols (Section 3.3)\n  - No comparison with human-generated descriptions or established medical text quality metrics beyond word count (Figure 8d)\n\n‚Ä¢ **Dataset Construction Pipeline Reliability**\n  - Heavy dependence on metadata extraction from diverse sources (30+ datasets) without validation of metadata accuracy or consistency across sources (Section 3.2.1, Table 5)\n  - The automated ROI generation using different expert models for different modalities (Table 6) lacks cross-validation or quality assessment\n  - LLaVA-Medcap training details are relegated to appendix, yet this model generates all 25M descriptions, raising concerns about potential systematic biases\n\n‚Ä¢ **Limited Experimental Validation**\n  - Evaluation restricted to three VQA benchmarks without testing on other medical tasks like classification, segmentation, or report generation despite claims of broad applicability (Abstract, Section 5)\n  - Ablation studies focus only on presence/absence of MedTrinity-25M pretraining without analyzing individual pipeline components' contributions\n  - No analysis of how multigranular annotations specifically contribute to performance improvements compared to simpler annotation schemes\n\n‚Ä¢ **Scalability and Reproducibility Concerns**\n  - The pipeline's computational requirements, processing time, and resource costs for generating 25M descriptions are not reported, limiting practical adoption assessment\n  - No direct evidence found in the manuscript regarding the availability timeline or access procedures for the dataset despite public availability claims\n  - Quality control mechanisms for detecting and correcting errors in automated annotation generation are not described\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance Mathematical and Technical Documentation**\n  - Provide formal mathematical definitions for ROI coordinate transformations and localization algorithms, including pseudocode for the pipeline components\n  - Specify technical parameters for vector similarity search including distance metrics, embedding dimensions, and retrieval confidence thresholds\n  - Include detailed algorithmic descriptions of the metadata integration process with validation procedures\n\n‚Ä¢ **Strengthen Evaluation Framework**\n  - Expand quality evaluation to at least 1000-2000 samples (0.004-0.008% of dataset) with stratified sampling across modalities and disease types\n  - Implement comprehensive inter-rater agreement studies with multiple medical experts and provide detailed evaluator qualification criteria\n  - Compare generated descriptions against human-written references using established medical text quality metrics such as medical concept coverage and clinical accuracy\n\n‚Ä¢ **Improve Pipeline Validation and Transparency**\n  - Implement cross-validation procedures for metadata accuracy across the 30+ source datasets with systematic quality checks\n  - Provide comprehensive quality assessment of ROI generation across different expert models including precision/recall metrics\n  - Move LLaVA-Medcap training details to main paper and include systematic bias analysis of generated descriptions\n\n‚Ä¢ **Expand Experimental Scope**\n  - Evaluate MedTrinity-25M on medical classification, segmentation, and report generation tasks beyond VQA to validate broad applicability claims\n  - Conduct detailed ablation studies analyzing individual pipeline components (metadata integration, ROI localization, knowledge retrieval) contributions\n  - Compare multigranular annotations against simpler annotation schemes to demonstrate specific value of the proposed approach\n\n‚Ä¢ **Address Practical Implementation Issues**\n  - Report computational requirements, processing times, and estimated costs for dataset generation to enable practical adoption assessment\n  - Provide concrete dataset access procedures, file formats, and expected availability timeline with reproducibility guidelines\n  - Implement and describe quality control mechanisms for error detection and correction in the automated annotation pipeline",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical dataset comprising over 25 million image‚ÄìROI‚Äìdescription triplets spanning 10 imaging modalities and more than 65 diseases. The authors introduce an automated pipeline that integrates metadata extraction, ROI localization using expert models, medical knowledge retrieval, and multimodal large language models to generate multigranular textual annotations. They further propose *LLaVA‚ÄëTri*, achieving strong results on several VQA benchmarks. Overall, the paper is ambitious and addresses a timely challenge in constructing comprehensive medical multimodal datasets. However, important gaps in technical formulation, evaluation methodology, and reproducibility limit the paper‚Äôs current rigor.  \n\n**Major Comments**  \n1. **Mathematical and Technical Formulation** ‚Äì The ROI localization process and coordinate transformations lack formal mathematical definitions, hampering reproducibility. Metrics for the reported ‚Äúleft‚Äìright reversal‚Äù issue are absent, and the vector similarity search (Med‚ÄëCPT + Faiss) lacks technical detail such as distance metrics, dimensionality, or retrieval thresholds.  \n2. **Evaluation Methodology** ‚Äì Quality assessment uses only 200 examples (‚âà0.0008% of the dataset), which is statistically insufficient. The expert evaluation lacks information on reviewer qualifications, inter‚Äërater reliability, or evaluation procedures. There is no quantitative comparison with human‚Äëwritten text or established medical text metrics.  \n3. **Dataset Construction Reliability** ‚Äì Metadata derived from 30+ heterogeneous sources are not validated for accuracy or consistency. ROI generation by modality‚Äëspecific expert models lacks cross‚Äëvalidation. Training details for LLaVA‚ÄëMedcap, which generates all text descriptions, are only in the appendix, raising transparency concerns.  \n4. **Experimental Validation** ‚Äì Testing is limited to three VQA datasets, with no experiments on classification, segmentation, or report generation despite claims of broader applicability. Ablation studies are minimal and do not isolate contributions of individual pipeline components or annotation granularity.  \n5. **Scalability and Reproducibility** ‚Äì Computational cost, processing time, and resource requirements for creating 25M annotations are unspecified. Details on dataset release, access, and quality‚Äëcontrol mechanisms are missing.  \n\n**Minor Comments**  \n- Clarify details of figures (e.g., coordinate diagrams, data flow).  \n- Define all acronyms at first use and standardize notation.  \n- Ensure consistent section references and table labeling.  \n\n**Summary Paragraph**  \nThe paper proposes an impressive and potentially valuable resource for multimodal medical AI research, but key methodological and reporting issues constrain its impact. The main weaknesses concern inadequate technical documentation, insufficient evaluation coverage, limited experimental scope, and lack of reproducibility details. Strengthening these aspects would substantially improve the manuscript‚Äôs credibility and utility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The contribution is promising, but substantial revisions are needed to address methodological transparency, evaluation robustness, and reproducibility before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million image-ROI-description triplets across 10 imaging modalities and over 65 diseases. The dataset is unique in its automated generation of multigranular annotations, which include both global and local information, using an ensemble of domain-specific expert models and multimodal large language models (MLLMs). The authors claim that MedTrinity-25M provides the most enriched annotations and supports a broad range of multimodal tasks, including captioning, report generation, and vision-centric tasks. They also propose LLaV A-Tri, a model pretrained on MedTrinity-25M, which achieves state-of-the-art performance on several medical VQA benchmarks.\n\n## Major Comments\n1. Novelty and Positioning: While MedTrinity-25M represents a significant advancement in automating the generation of multigranular annotations, the novelty of the approach needs to be better contextualized within the existing literature. The authors should provide a clearer distinction between their method and recent advancements in automated medical image annotation and multimodal large language models.\n\n2. Evaluation Design: The evaluation of LLaV A-Tri is conducted on three medical VQA benchmarks, but the dataset's broader applicability and generalizability remain unclear. Additional evaluations on a wider variety of datasets and tasks (e.g., segmentation, classification) would strengthen the manuscript's claims of versatility and utility.\n\n3. Comparisons: The manuscript should include a more comprehensive comparison with other recent multimodal medical datasets and models. Current comparisons lack depth, particularly in how MedTrinity-25M's annotations compare to those of other datasets in terms of quality and utility for downstream tasks.\n\n4. Reproducibility: Although the authors mention that the dataset is publicly available, the manuscript lacks detailed descriptions of the preprocessing pipelines, expert models used, and the specific parameters and protocols employed in generating the annotations. Providing these details is crucial for ensuring the reproducibility of the dataset and the proposed methods.\n\n## Minor Comments\n1. Clarity in Descriptions: Some sections, such as the data processing pipeline, could benefit from clearer explanations. For instance, the distinction between the metadata integration and medical knowledge retrieval steps could be made more explicit.\n\n2. Consistency in Terminology: There are inconsistencies in the use of terminologies, particularly around the definition of ROIs and their roles in the annotation process. Standardizing these definitions would improve clarity.\n\n3. Figures and Tables: Figures and tables, especially those showcasing the dataset statistics and comparisons, could be enhanced for better readability and clarity. For example, Figure 8 could benefit from a more detailed legend explaining the axes and data points.\n\n4. Typographical Issues: Minor typographical errors, such as inconsistent spacing and punctuation, should be corrected for a polished presentation.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical imaging: the creation of large-scale, multigranularly annotated datasets that can support a wide range of multimodal tasks. MedTrinity-25M represents a novel and valuable contribution to the field, offering rich annotations generated through an innovative automated pipeline. However, the evaluation is somewhat limited in scope, and the manuscript would benefit from a more thorough comparison with existing datasets and models. Additionally, while the dataset's public availability is promising, the reproducibility of the methods requires more detailed documentation. Overall, the work demonstrates promise but needs additional validation and clarity to fully meet the standards of significance, innovation, evaluation, and reproducibility expected by TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis, broaden the evaluation of the dataset across a wider range of tasks and datasets, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 1,
          "source": "ai",
          "comment": "The comments regarding the dataset size and automated annotation process were overly positive. They did not adequately address the errors inherent in the dataset itself, such as the modality inconsistency issue mentioned by other reviewers."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity‚Äë25M*, a large-scale multimodal medical dataset comprising over 25 million image‚ÄìROI‚Äìdescription triplets covering 10 imaging modalities and more than 65 diseases. The dataset introduces automated, multigranular annotations that integrate global and local information via domain-specific expert models and multimodal large language models. The authors further introduce *LLaVA‚ÄëTri*, a model pretrained on MedTrinity‚Äë25M, which reportedly attains state‚Äëof‚Äëthe‚Äëart results on several medical VQA benchmarks. Overall, the paper is clear in presentation and proposes a promising resource for multimodal medical research, though certain areas require clarification and additional validation.  \n\n**Major Comments**  \n1. **Novelty and Contextualization** ‚Äì Although the automated generation of multigranular annotations is a valuable addition, the manuscript should more explicitly situate its novelty with respect to existing automated annotation frameworks and recent developments in multimodal large language models. A clearer articulation of what differentiates this approach would help define its contribution.  \n2. **Evaluation Design** ‚Äì The evaluation of LLaVA‚ÄëTri is restricted to three medical VQA benchmarks. Broader empirical assessment on additional tasks (e.g., segmentation, classification) and datasets is needed to substantiate claims about the dataset‚Äôs generalizability and versatility.  \n3. **Comparative Analysis** ‚Äì Current comparisons with related multimodal medical datasets and models are limited. More detailed benchmarking and analysis of annotation quality relative to other resources would strengthen the claims of superiority and utility.  \n4. **Reproducibility and Transparency** ‚Äì While public release of the dataset is noted, the paper does not provide comprehensive details of preprocessing, expert models employed, or parameter settings. Full transparency in these aspects is necessary to ensure reproducibility.  \n\n**Minor Comments**  \n1. Clarify the description of the data processing pipeline‚Äîparticularly the distinction between metadata integration and medical knowledge retrieval.  \n2. Standardize terminology, especially definitions of ROIs and their role in the annotation process.  \n3. Improve figure and table clarity; for instance, expand legends and axis explanations in Figure‚ÄØ8.  \n4. Correct minor typographical and formatting inconsistencies.  \n\n**Summary Paragraph**  \nThe study tackles an important need for richly annotated multimodal medical datasets and offers a notable attempt through MedTrinity‚Äë25M. Its automated annotation pipeline and accompanying pretrained model represent promising directions, but current evaluation and comparative analyses are insufficient for fully supporting the claimed scope and performance. Enhanced methodological detail and expanded benchmarking are essential for establishing robustness and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The manuscript should broaden comparative and empirical evaluations and provide detailed methodological documentation to substantiate its contributions and ensure reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## MEDTRINITY-25M: A LARGE-SCALE MULTIMODAL DATASET WITH MULTIGRANULAR ANNOTATIONS FOR MEDICINE\n\n### Summary\n\nThe paper presents MedTrinity-25M, a large-scale, multimodal medical dataset comprising over 25 million image‚ÄìROI‚Äìdescription triplets spanning 10 imaging modalities and 65+ diseases. The authors propose an automated pipeline that integrates metadata-derived coarse captions, expert-model-guided ROI localization, and retrieval-augmented generation (RAG) to produce multigranular structured descriptions, then demonstrate the dataset‚Äôs utility by pretraining LLaVA-Tri (a multiscale LLaVA variant with LLaMA3), achieving state-of-the-art results on VQA-RAD, SLAKE, and PathVQA. The work aims to address the scarcity of paired image‚Äìtext data and the lack of region-level grounding by programmatically generating region-aware annotations at unprecedented scale.\n\n### Strengths\n\n- Technical novelty and innovationThe pipeline that combines metadata integration, automated ROI localization with domain-specific expert models, and RAG-augmented MLLMs to generate multigranular image-ROI-description triplets at scale is an original and ambitious contribution.The focus on region-wise correlations and structured, multigranular attributes (modality, organ detection, ROI analysis, lesion texture, region-wise correlations) goes beyond typical captioning/report datasets.Leveraging both expert segmentation/grounding models and knowledge retrieval to standardize terminology and enrich local analysis demonstrates a thoughtful system design.\n- Experimental rigor and validationClear empirical demonstration that pretraining with MedTrinity-25M improves multiple models, not just the authors‚Äô own LLaVA-Tri (e.g., InternVL2, MiniCPM-V, PubMedCLIP).Strong performance on three widely used Med-VQA benchmarks, including open-ended and closed formats, with consistent gains.\n- Clarity of presentationThe overall pipeline is explained in a stepwise manner with helpful qualitative comparisons elucidating the role of coarse captions, ROIs, and external knowledge.Dataset statistics (modality and anatomy distributions, word counts) help convey scale and coverage.\n- Significance of contributionsIf released as described, MedTrinity-25M would be among the largest, most richly annotated open resources for medical multimodal research, potentially accelerating work on grounding, captioning, VQA, and foundation model pretraining.The ability to generate region-guided text from unpaired images at scale addresses a central bottleneck in medical VLM research.\n\nTechnical novelty and innovation\n\n- The pipeline that combines metadata integration, automated ROI localization with domain-specific expert models, and RAG-augmented MLLMs to generate multigranular image-ROI-description triplets at scale is an original and ambitious contribution.\n- The focus on region-wise correlations and structured, multigranular attributes (modality, organ detection, ROI analysis, lesion texture, region-wise correlations) goes beyond typical captioning/report datasets.\n- Leveraging both expert segmentation/grounding models and knowledge retrieval to standardize terminology and enrich local analysis demonstrates a thoughtful system design.\n\nExperimental rigor and validation\n\n- Clear empirical demonstration that pretraining with MedTrinity-25M improves multiple models, not just the authors‚Äô own LLaVA-Tri (e.g., InternVL2, MiniCPM-V, PubMedCLIP).\n- Strong performance on three widely used Med-VQA benchmarks, including open-ended and closed formats, with consistent gains.\n\nClarity of presentation\n\n- The overall pipeline is explained in a stepwise manner with helpful qualitative comparisons elucidating the role of coarse captions, ROIs, and external knowledge.\n- Dataset statistics (modality and anatomy distributions, word counts) help convey scale and coverage.\n\nSignificance of contributions\n\n- If released as described, MedTrinity-25M would be among the largest, most richly annotated open resources for medical multimodal research, potentially accelerating work on grounding, captioning, VQA, and foundation model pretraining.\n- The ability to generate region-guided text from unpaired images at scale addresses a central bottleneck in medical VLM research.\n\n### Weaknesses\n\n- Technical limitations or concernsHeavy reliance on metadata-derived disease labels and RAG may propagate dataset-level priors into generated descriptions, risking bias and label leakage; regional attributions could be spurious if ROIs or retrievals are imperfect.ROI localization consolidates masks into bounding boxes in some cases, potentially losing shape detail; clarity is needed on whether original masks are retained.Conversion of 3D volumes into 2D slices at large scale risks redundancy, reduced 3D context, and increased near-duplicate content.\n- Experimental gaps or methodological issuesThe evaluation of dataset quality is limited to a small sample (n=200) with both expert and LLM scoring, which is insufficient to assess reliability at 25M scale. No inter-rater agreement or cross-modality breakdown is reported.No quantitative ablations isolate the contribution of coarse captions, ROIs, and RAG on downstream tasks; evidence is largely qualitative.Potential training‚Äìtest leakage is not addressed: deduplication against VQA-RAD, SLAKE, and PathVQA (and their source datasets) is critical given the massive, aggregated collection.\n- Clarity or presentation issuesImportant implementation details for LLaVA-Tri (multiscale integration specifics, training compute, dataset sampling strategy from 25M) are insufficiently described to ensure reproducibility.The dataset availability is stated as a placeholder URL; release details, licensing, and usage permissions across 30+ sources are not clarified.\n- Missing related work or comparisonsWhile Table 1 covers several datasets, more direct comparisons to fine-grained alignment efforts (e.g., fVLM‚Äôs anatomy-aligned CT pretraining) and recent, large instruction-tuning resources (e.g., MedMax) would better contextualize novelty; discussion versus region-grounding datasets (e.g., SA-Med2D and other ROI-centric resources) could be expanded.The work would benefit from positioning relative to datasets emphasizing grounded region-level supervision, and reporting on grounding/segmentation metrics to substantiate ROI quality.\n\nTechnical limitations or concerns\n\n- Heavy reliance on metadata-derived disease labels and RAG may propagate dataset-level priors into generated descriptions, risking bias and label leakage; regional attributions could be spurious if ROIs or retrievals are imperfect.\n- ROI localization consolidates masks into bounding boxes in some cases, potentially losing shape detail; clarity is needed on whether original masks are retained.\n- Conversion of 3D volumes into 2D slices at large scale risks redundancy, reduced 3D context, and increased near-duplicate content.\n\nExperimental gaps or methodological issues\n\n- The evaluation of dataset quality is limited to a small sample (n=200) with both expert and LLM scoring, which is insufficient to assess reliability at 25M scale. No inter-rater agreement or cross-modality breakdown is reported.\n- No quantitative ablations isolate the contribution of coarse captions, ROIs, and RAG on downstream tasks; evidence is largely qualitative.\n- Potential training‚Äìtest leakage is not addressed: deduplication against VQA-RAD, SLAKE, and PathVQA (and their source datasets) is critical given the massive, aggregated collection.\n\nClarity or presentation issues\n\n- Important implementation details for LLaVA-Tri (multiscale integration specifics, training compute, dataset sampling strategy from 25M) are insufficiently described to ensure reproducibility.\n- The dataset availability is stated as a placeholder URL; release details, licensing, and usage permissions across 30+ sources are not clarified.\n\nMissing related work or comparisons\n\n- While Table 1 covers several datasets, more direct comparisons to fine-grained alignment efforts (e.g., fVLM‚Äôs anatomy-aligned CT pretraining) and recent, large instruction-tuning resources (e.g., MedMax) would better contextualize novelty; discussion versus region-grounding datasets (e.g., SA-Med2D and other ROI-centric resources) could be expanded.\n- The work would benefit from positioning relative to datasets emphasizing grounded region-level supervision, and reporting on grounding/segmentation metrics to substantiate ROI quality.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe automated pipeline is conceptually sound: combining metadata, expert models, and RAG to scaffold an MLLM‚Äôs structured description generation is a reasonable approach to scale without paired reports.Risks arise from compounding errors: (1) metadata may be noisy or too coarse; (2) expert models may localize normal anatomy rather than pathology or mislocalize lesions, depending on the modality and disease; (3) RAG content may be tangential or overfit to disease labels seeded by metadata; (4) the MLLM may hallucinate correlations or overinterpret ROIs. A more extensive quality-control strategy and error taxonomy would strengthen technical credibility.Given that segmentation masks are reportedly converted to bounding boxes at times, preserving both ROI masks and boxes (and indicating provenance) would maximize utility for detection and segmentation tasks.\n- Experimental evaluation assessmentModel performance gains on three VQA benchmarks are impressive and consistent, and cross-backbone improvements suggest general utility.However, the dataset claims broad task coverage (classification, segmentation, report generation), yet evaluations are limited to VQA. Adding experiments in grounding/segmentation (e.g., IoU/AP on datasets with gold ROIs) and report/caption quality (e.g., CheXbert/RadGraph F1, clinical efficacy metrics) would better validate the claimed multigranular benefits.The dataset quality assessment with 200 samples is underpowered; no measure of inter-expert reliability, per-modality error rates, or failure mode analysis is provided. Scaling to at least a few thousand examples with modality/disease stratification, along with standardized metrics (e.g., location agreement between described region and actual ROI coordinates), would be more convincing.Potential overlap between training data and evaluation benchmarks must be rigorously ruled out. A deduplication and leakage audit (e.g., perceptual hashing, dataset ID matching, and patient-level exclusion if applicable) is essential; without it, SOTA gains could be inflated.\n- Comparison with related work (using the summaries provided)Compared to BiomedCLIP (PMC-15M), this work goes beyond image‚Äìcaption pairs by producing region-level grounding with structured, ROI-guided descriptions. However, BiomedCLIP demonstrated broad downstream gains with carefully filtered captions; MedTrinity-25M should similarly emphasize filtering and quality controls at scale.fVLM focuses on anatomy-level fine-grained alignment for CT with explicit per-anatomy contrastive learning and careful false-negative handling. In contrast, this work uses automated ROI identification plus MLLM-based description generation across many modalities; an ablation comparing fine-grained explicit alignment versus generated multigranular supervision on CT would be informative.MISS and MedMax convert unimodal data to captions/instructions, but typically lack region-level grounded triplets at this scale. MedTrinity-25M‚Äôs ROI grounding is a distinguishing feature; however, the paper would benefit from comparisons on grounding-sensitive tasks and a discussion of failure modes in generated rationales relative to datasets with human-verified rationales (e.g., MedThink).SAT demonstrates scalable anatomy-grounded 3D segmentation; positioning MedTrinity-25M relative to 3D/volumetric grounding and discussing limitations of 2D slicing could clarify the scope.OmniMedVQA highlights broad multi-modality VQA challenges; given the dataset‚Äôs diversity, a zero-shot evaluation on OmniMedVQA could showcase generalization beyond radiology/pathology-bounded datasets.\n- Discussion of broader impact and significanceA publicly released, region-grounded, multigranular dataset of this size could meaningfully advance medical VLM research and catalyze more transparent, localization-aware reasoning models.Ethical and legal aspects require careful handling: licensing heterogeneity across sources, privacy safeguards, and documentation of de-identification must be explicit; otherwise, widespread adoption might be hindered.The use of LLMs for medical text generation raises risks of subtle hallucinations, outdated knowledge, or confabulated correlations. Mitigations could include provenance tagging, confidence estimation, blacklist/whitelist term filters, and human-in-the-loop spot checks on high-risk modalities/diseases.\n\nTechnical soundness evaluation\n\n- The automated pipeline is conceptually sound: combining metadata, expert models, and RAG to scaffold an MLLM‚Äôs structured description generation is a reasonable approach to scale without paired reports.\n- Risks arise from compounding errors: (1) metadata may be noisy or too coarse; (2) expert models may localize normal anatomy rather than pathology or mislocalize lesions, depending on the modality and disease; (3) RAG content may be tangential or overfit to disease labels seeded by metadata; (4) the MLLM may hallucinate correlations or overinterpret ROIs. A more extensive quality-control strategy and error taxonomy would strengthen technical credibility.\n- Given that segmentation masks are reportedly converted to bounding boxes at times, preserving both ROI masks and boxes (and indicating provenance) would maximize utility for detection and segmentation tasks.\n\nExperimental evaluation assessment\n\n- Model performance gains on three VQA benchmarks are impressive and consistent, and cross-backbone improvements suggest general utility.\n- However, the dataset claims broad task coverage (classification, segmentation, report generation), yet evaluations are limited to VQA. Adding experiments in grounding/segmentation (e.g., IoU/AP on datasets with gold ROIs) and report/caption quality (e.g., CheXbert/RadGraph F1, clinical efficacy metrics) would better validate the claimed multigranular benefits.\n- The dataset quality assessment with 200 samples is underpowered; no measure of inter-expert reliability, per-modality error rates, or failure mode analysis is provided. Scaling to at least a few thousand examples with modality/disease stratification, along with standardized metrics (e.g., location agreement between described region and actual ROI coordinates), would be more convincing.\n- Potential overlap between training data and evaluation benchmarks must be rigorously ruled out. A deduplication and leakage audit (e.g., perceptual hashing, dataset ID matching, and patient-level exclusion if applicable) is essential; without it, SOTA gains could be inflated.\n\nComparison with related work (using the summaries provided)\n\n- Compared to BiomedCLIP (PMC-15M), this work goes beyond image‚Äìcaption pairs by producing region-level grounding with structured, ROI-guided descriptions. However, BiomedCLIP demonstrated broad downstream gains with carefully filtered captions; MedTrinity-25M should similarly emphasize filtering and quality controls at scale.\n- fVLM focuses on anatomy-level fine-grained alignment for CT with explicit per-anatomy contrastive learning and careful false-negative handling. In contrast, this work uses automated ROI identification plus MLLM-based description generation across many modalities; an ablation comparing fine-grained explicit alignment versus generated multigranular supervision on CT would be informative.\n- MISS and MedMax convert unimodal data to captions/instructions, but typically lack region-level grounded triplets at this scale. MedTrinity-25M‚Äôs ROI grounding is a distinguishing feature; however, the paper would benefit from comparisons on grounding-sensitive tasks and a discussion of failure modes in generated rationales relative to datasets with human-verified rationales (e.g., MedThink).\n- SAT demonstrates scalable anatomy-grounded 3D segmentation; positioning MedTrinity-25M relative to 3D/volumetric grounding and discussing limitations of 2D slicing could clarify the scope.\n- OmniMedVQA highlights broad multi-modality VQA challenges; given the dataset‚Äôs diversity, a zero-shot evaluation on OmniMedVQA could showcase generalization beyond radiology/pathology-bounded datasets.\n\nDiscussion of broader impact and significance\n\n- A publicly released, region-grounded, multigranular dataset of this size could meaningfully advance medical VLM research and catalyze more transparent, localization-aware reasoning models.\n- Ethical and legal aspects require careful handling: licensing heterogeneity across sources, privacy safeguards, and documentation of de-identification must be explicit; otherwise, widespread adoption might be hindered.\n- The use of LLMs for medical text generation raises risks of subtle hallucinations, outdated knowledge, or confabulated correlations. Mitigations could include provenance tagging, confidence estimation, blacklist/whitelist term filters, and human-in-the-loop spot checks on high-risk modalities/diseases.\n\n### Questions for Authors\n\n- Dataset release and licensing: Can you provide a concrete release URL and a comprehensive license matrix for all source datasets, including usage permissions, de-identification status, and redistribution terms?\n- Quality control at scale: Beyond the 200-sample evaluation, do you have larger-scale audits (e.g., thousands of samples) with inter-rater agreement and modality/disease breakdowns? How do you quantify hallucinations (e.g., mismatched lesion location vs. ROI, incorrect side, fabricated correlations)?\n- ROI fidelity: For datasets with segmentation masks, are masks retained alongside bounding boxes? What proportion of the 25M triplets have original masks vs. auto-generated ROIs, and what is the measured ROI accuracy (IoU/AP) on subsets with gold annotations?\n- Leakage analysis: What steps did you take to deduplicate MedTrinity-25M against VQA-RAD, SLAKE, PathVQA, and their source images? Can you share a leakage audit methodology and results?\n- Ablations: Can you quantify the contribution of each pipeline component (coarse captions, ROIs, RAG) to downstream improvements? For example, training on data generated without ROIs or without RAG and measuring VQA/open-ended performance differences?\n- 3D considerations: How do you handle redundancy and near-duplicate slices from volumetric studies? Are there heuristics to avoid oversampling near-identical slices, and do you preserve slice indices/positions for potential 3D-aware training?\n- LLaVA-Tri details: Please provide architecture specifics for the multiscale integration, pretraining schedule on the 25M (epochs, sampling, batch sizes, compute), and whether you trained on all 25M or a subset.\n- External knowledge: How do you ensure retrieved knowledge is appropriate, up-to-date, and not misleading given metadata biases? Do you filter for study type/quality and standardize terminology (e.g., ontology mapping) beyond free-text snippets?\n- Evaluation breadth: Do you plan to report results on grounding/segmentation and report-generation clinical efficacy metrics (e.g., RadGraph/ CheXpert F1), to align with the dataset‚Äôs multigranular claims?\n- Human-verified subset: Will you release a smaller, high-quality, multi-modality, expert-verified subset (with gold ROIs and textual rationales) to serve as a benchmark for factuality and localization?\n\n### Overall Assessment\n\nThis paper addresses a pressing need for scalable, region-aware multimodal supervision in medicine and proposes a compelling pipeline that could materially benefit the field. The dataset‚Äôs scope and the demonstrated cross-model gains on VQA tasks support its potential impact. However, given the scale and automated nature of annotation, the work would be significantly strengthened by a more rigorous quality and leakage audit, quantitative ablations isolating pipeline components, and broader evaluations beyond VQA (especially grounding/segmentation and clinical report metrics). Clarifying licensing and data release logistics is also crucial. With these issues addressed‚Äîparticularly around quality control, deduplication, and reproducibility‚Äîthe contribution would be a strong candidate for a top-tier venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MedTrinity‚Äë25M**, a large‚Äëscale multimodal medical dataset containing over 25‚ÄØmillion image‚ÄìROI‚Äìdescription triplets across ten imaging modalities and more than sixty‚Äëfive diseases. The authors present an automated pipeline that fuses metadata‚Äëbased captioning, expert‚Äëmodel‚Äëguided ROI localization, and retrieval‚Äëaugmented generation (RAG) for multigranular annotation. They further demonstrate the dataset‚Äôs utility by pretraining **LLaVA‚ÄëTri**, a multiscale LLaVA variant built on LLaMA3, achieving strong results on VQA‚ÄëRAD, SLAKE, and PathVQA. The paper is clearly organized and technically ambitious, aiming to alleviate the bottleneck of region‚Äëaware supervision in medical vision‚Äëlanguage modeling.\n\n---\n\n**Major Comments**  \n1. **Technical limitations and potential biases:** Heavy dependence on metadata‚Äëderived disease labels and RAG may reinforce dataset priors, leading to biased or spurious region attributions. Conversion of masks to bounding boxes can degrade shape fidelity, and 3D‚Äëto‚Äë2D slicing risks redundancy and loss of volumetric context.  \n2. **Evaluation design and quality assessment:** Dataset quality evaluation is restricted to 200 samples, lacking inter‚Äërater agreement, modality breakdown, and standardized error taxonomy. This small‚Äëscale audit does not establish reliability at 25‚ÄØM scope.  \n3. **Ablation and leakage analysis:** No quantitative ablations isolate the roles of coarse captions, ROI localization, or RAG. Potential training‚Äìtest overlap with VQA‚ÄëRAD, SLAKE, and PathVQA is unaddressed, raising concerns about inflated benchmark gains.  \n4. **Reproducibility and implementation clarity:** Key details‚Äîsampling strategy from 25‚ÄØM entries, multiscale integration for LLaVA‚ÄëTri, compute budget, and training setup‚Äîare insufficiently specified. Dataset release, license compliance, and usage rights across 30+ sources remain unclear.  \n5. **Comparative positioning:** Discussion against concurrent datasets (e.g., fVLM, MedMax, SA‚ÄëMed2D, BiomedCLIP) should be expanded to substantiate novelty. Reporting grounding‚Äëlevel metrics would better validate ROI quality.  \n6. **Scope of evaluation:** Despite claims of broader task coverage, experiments focus solely on VQA. Validation on grounding, segmentation, and report generation metrics would strengthen the evidence base.  \n7. **Ethical and legal considerations:** Heterogeneous licensing, de‚Äëidentification practices, and generation biases merit explicit documentation to ensure responsible release.\n\n---\n\n**Minor Comments**  \n- Provide explicit dataset URL, release plan, and source‚Äëspecific license matrix.  \n- Clarify whether original segmentation masks are preserved alongside bounding boxes.  \n- Quantify ROI accuracy (IoU/AP) on subsets with gold annotation.  \n- Add zero‚Äëshot evaluation on broader benchmarks (e.g., OmniMedVQA) to test generalization.  \n- Include detailed training schedule and sampling choices for LLaVA‚ÄëTri.  \n- Discuss provenance tagging or filtering to mitigate hallucinations in generated text.  \n- Verify 3D redundancy handling and slice index retention for volumetric data.  \n\n---\n\n**Summary Paragraph**  \nOverall, the work presents a technically innovative dataset and pipeline with convincing empirical gains on multiple Med‚ÄëVQA benchmarks. The large‚Äëscale, region‚Äëgrounded supervision addresses an important gap in medical multimodal learning and could substantially advance the field if released with robust quality control. However, the manuscript requires stronger evidence on annotation fidelity, dataset de‚Äëduplication, and component ablations, as well as clearer release conditions. Addressing reproducibility, comparison breadth, and ethical compliance would materially increase confidence in the contribution‚Äôs reliability and long‚Äëterm value.\n\n---\n\n**Decision Recommendation:** **Major Revision** ‚Äì Promising and impactful concept, but substantial methodological clarification and validation are needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical challenge of limited scale and granularity in existing medical multimodal datasets by introducing MedTrinity-25M, a comprehensive dataset containing over 25 million image-ROI-description triplets spanning 10 medical imaging modalities and 65+ diseases. The authors propose an innovative automated pipeline that leverages multimodal large language models with retrieval-augmented generation to create multigranular annotations without requiring paired text descriptions. The pipeline incorporates domain-specific expert models for ROI localization and medical knowledge retrieval from authoritative sources to generate detailed descriptions covering both global information (modality, organ detection) and local information (ROI analysis, lesion texture, region-wise correlations). The authors demonstrate the value of MedTrinity-25M through LLaVA-Tri, a model pre-trained on their dataset that achieves state-of-the-art results across three medical visual question answering benchmarks (81.6% on VQA-RAD, 87.8% on SLAKE, and 82.8% on PathVQA), significantly outperforming existing approaches.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- The unprecedented scale of MedTrinity-25M (25 million samples) represents a significant advancement over existing medical multimodal datasets, which typically contain orders of magnitude fewer samples.\n- The automated pipeline for generating multigranular annotations without human-labeled text is highly innovative and addresses the scalability limitations of traditional medical dataset construction.\n- The comprehensive multigranular annotation schema (covering both global and local information) provides richer context than conventional medical reports, enabling more sophisticated model training.\n- The thorough experimental validation across multiple model architectures (LLaVA-Tri, MiniCPM-V, InternVL2-8B, PubMedCLIP) consistently demonstrates performance improvements, validating the dataset's utility.\n\n### Major Limitations\n- The paper lacks sufficient discussion of potential demographic, geographic, and clinical biases in the dataset, which is critical for medical applications where representation bias can have serious clinical consequences.\n- While the retrieval-augmented approach helps, the pipeline's reliance on general-purpose LLMs introduces risks of medical misinformation that aren't adequately addressed through failure case analysis.\n- The evaluation focuses almost exclusively on VQA benchmarks without demonstrating value for clinical decision support tasks, limiting the assessment of real-world clinical relevance.\n- The description of medical knowledge integration lacks detail about how potentially conflicting information from different sources was resolved.\n\n### Minor Comments\n- Several figures (e.g., Figure 1) would benefit from more consistent labeling and formatting to enhance clarity of the comparison.\n- The paper would benefit from computational details about the dataset generation pipeline to better assess practical implementation requirements.\n- The comparison with existing datasets could be strengthened with more quantitative metrics beyond word count and sample size.\n- The clinical significance section could be expanded to better articulate how this work might translate to improved patient outcomes.\n\n## 3. Evaluation Against TMI Criteria\n\n**Significance:** The work is highly significant as it addresses a critical bottleneck in medical AI research‚Äîthe scarcity of large-scale, high-quality multimodal medical datasets. The 25 million samples in MedTrinity-25M represent a substantial increase over existing resources, and the multigranular annotations provide richer information than traditional medical reports. The demonstrated improvements across multiple medical AI tasks suggest this dataset could substantially accelerate research in medical foundation models and improve model performance for clinical applications.\n\n**Innovation:** The paper introduces several innovative elements: 1) The first automated pipeline for generating multigranular medical annotations without human-labeled text, 2) The integration of medical knowledge retrieval to enhance the quality of generated descriptions, and 3) The comprehensive multigranular annotation schema that captures both global and local medical information. The approach represents a significant methodological advancement over existing approaches that rely on manually annotated reports, which inherently limit scalability.\n\n**Evaluation:** The evaluation is thorough in demonstrating performance improvements on three medical VQA benchmarks with detailed comparisons to state-of-the-art methods. The authors provide ablation studies showing consistent improvements across different model architectures. However, the evaluation would be strengthened by more comprehensive dataset quality analysis beyond the LLM and expert evaluations, additional validation on clinical tasks beyond VQA, and a more detailed analysis of failure cases and limitations of the generated annotations.\n\n**Reproducibility:** The paper provides a clear description of the dataset construction pipeline and includes example prompt templates. The authors indicate the dataset will be publicly available, which is essential for reproducibility. However, the paper would benefit from more specific implementation details, such as complete prompt templates, precise instructions for replicating the knowledge base construction, and more detailed information about the model variants and training procedures to ensure full reproducibility.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThis paper represents a significant contribution to the field of medical imaging with a novel approach to large-scale dataset construction. The methodology is sound, the results are compelling, and the work addresses an important need in the medical AI community. However, before acceptance, I recommend addressing the limitations noted above, particularly expanding the discussion of potential biases in the dataset, providing more detailed information about failure cases, and enhancing the evaluation with additional clinical relevance analysis. These revisions would strengthen the paper and better position it for impact in the medical imaging community.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *MedTrinity-25M*, a large-scale multimodal medical imaging dataset comprising over 25 million image‚ÄìROI‚Äìdescription triplets across ten imaging modalities and more than sixty-five diseases. The authors propose an automated pipeline that uses multimodal large language models with retrieval-augmented generation to produce multigranular annotations without requiring paired text supervision. This framework integrates domain-specific expert models for ROI localization and medical knowledge retrieval to generate both global and local descriptions. The accompanying model, *LLaVA-Tri*, pre-trained on this dataset, achieves state-of-the-art results on several medical VQA benchmarks, demonstrating the dataset‚Äôs potential impact. Overall, the paper is ambitious, clearly written, and addresses a key limitation in medical AI‚Äîscalability of high-quality multimodal datasets.  \n\n**Major Comments**  \n1. **Significance and Scope:** The unprecedented dataset scale and automated annotation pipeline constitute major strengths. The proposed approach effectively overcomes the limitations of manual labeling and introduces a rich multigranular annotation schema.  \n2. **Experimental Validation:** The performance improvements across multiple architectures (LLaVA-Tri, MiniCPM-V, InternVL2-8B, PubMedCLIP) are clearly demonstrated and convincingly support the dataset‚Äôs utility.  \n3. **Bias and Ethical Considerations:** The manuscript lacks a detailed examination of demographic, geographic, and clinical biases‚Äîan essential issue for medical datasets where representation can directly impact patient outcomes.  \n4. **Model Reliability and Misinformation Risks:** Despite retrieval augmentation, reliance on general-purpose LLMs poses risks of medical inaccuracies. The absence of in-depth failure case analysis weakens confidence in data quality.  \n5. **Clinical Relevance:** Evaluation is confined to VQA benchmarks, offering limited insight into practical clinical applicability or diagnostic support tasks.  \n6. **Knowledge Integration:** Greater clarity is needed on how conflicts among multiple medical knowledge sources are resolved during annotation synthesis.\n\n**Minor Comments**  \n- Figures (e.g., Fig.‚ÄØ1) would benefit from consistent labeling and improved formatting.  \n- Additional computational details of dataset generation should be included to assess feasibility.  \n- Comparative analysis with existing datasets could include more quantitative measures beyond sample size and word count.  \n- The discussion of clinical significance could be expanded to connect dataset capabilities with potential impacts on patient care.\n\n**Summary Paragraph**  \nThis manuscript delivers a highly significant and innovative contribution by introducing a scalable automated framework for multimodal medical dataset creation. The technical novelty and extensive evaluation support the claimed advantages, yet several concerns remain regarding dataset bias assessment, validation on clinically meaningful tasks, and transparency in knowledge integration and dataset generation details. Addressing these issues would enhance both the reproducibility and practical relevance of the work.\n\n**Decision Recommendation**  \n**Minor Revision.** The paper is strong and impactful but requires improvements in bias analysis, failure case discussion, and expanded evaluation to ensure robustness and clinical pertinence before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper\n\nThe manuscript tackles the scarcity of multigranular annotations in current medical multimodal datasets, a limitation that hinders vision‚Äëlanguage models from linking fine‚Äëgrained image details to broader disease contexts. To address this, the authors construct MedTrinity‚Äë25M, a large‚Äëscale collection comprising 25‚ÄØ016‚ÄØ845 image‚ÄëROI‚Äëdescription triplets spanning ten imaging modalities and more than 65 disease categories. Their automated pipeline first assembles coarse captions from metadata, then localizes regions of interest using either expert segmentation models or grounding networks, and finally retrieves relevant medical knowledge from a vector‚Äëbased knowledge base. These elements are integrated into prompts that drive a multimodal large language model to produce detailed, multigranular textual descriptions. The resulting dataset is used to pre‚Äëtrain LLaVA‚ÄëTri, which the authors report achieves 81.6‚ÄØ% accuracy on VQA‚ÄëRAD, 87.8‚ÄØ% on SLAKE, and 82.8‚ÄØ% on PathVQA. The main contribution is the release of MedTrinity‚Äë25M together with the pipeline that enables scalable generation of multigranular visual‚Äëtextual annotations.\n\n## General feedback\n\n*Significance.*  The size (‚âà25‚ÄØM samples) and diversity (10 modalities, >65 diseases) directly address a notable gap in large‚Äëscale multimodal medical data and could facilitate a broad range of downstream applications.  \n\n*Innovation.*  The proposed pipeline that merges ROI‚Äëguided, retrieval‚Äëaugmented text generation without relying on paired clinical reports is a novel element (see Fig.‚ÄØ2).  Nevertheless, the principal novelty lies in the dataset rather than in the modeling approach; LLaVA‚ÄëTri‚Äôs contribution is principally the pre‚Äëtraining on the new corpus.  \n\n*Evaluation.*  Performance gains are demonstrated on three VQA benchmarks (Tables‚ÄØ3‚ÄØ&‚ÄØ4) with modest improvements over strong baselines.  The manuscript does not present results for other claimed downstream tasks such as report generation, segmentation, or classification.  Annotation quality is examined on a very limited random subset (200 samples) using expert and LLM assessments (Table‚ÄØ2), which is insufficient to assess a dataset of this magnitude.  \n\n*Reproducibility.*  Several essential details are missing, including the exact train/validation/test splits of MedTrinity‚Äë25M, the full prompting templates and generation hyper‚Äëparameters, ROI model performance on each source dataset, and the pre‚Äëtraining hyper‚Äëparameters for LLaVA‚ÄëTri.  The dataset link in the abstract is a placeholder (‚Äúhttps://XXX‚Äù), and no information on licensing, de‚Äëidentification, or usage policy is provided.\n\n## Specific comments/critiques\n\n- **Dataset composition and filtering** ‚Äì While Table‚ÄØ5 lists the source datasets, the paper does not disclose the final modality distribution, how duplicates were removed, or what quality‚Äëcontrol criteria were applied to the 25‚ÄØM samples.  \n\n- **ROI generation reliability** ‚Äì ROIs are derived from a heterogeneous set of expert models (Table‚ÄØ6), yet the manuscript lacks quantitative validation (e.g., IoU against ground‚Äëtruth masks), leaving uncertainty about the visual annotation quality.  \n\n- **Prompting and generation details** ‚Äì Fig.‚ÄØ14 illustrates the hierarchical prompt, but the exact prompt text, temperature settings, number of sampled candidates per image, and any post‚Äëprocessing steps are omitted, which hampers reproducibility.  \n\n- **Limited annotation evaluation** ‚Äì Only 200 samples were examined for alignment (Table‚ÄØ2).  No inter‚Äërater agreement, disease‚Äëlevel correctness analysis, or statistical significance testing is presented, which is inadequate for a corpus of this scale.  \n\n- **Ablation of pipeline components** ‚Äì Qualitative comparisons with and without coarse captions, ROIs, or knowledge retrieval are shown (Figs.‚ÄØ3‚Äì5), but systematic ablations quantifying each component‚Äôs impact on description quality are missing.  \n\n- **Comparison to existing multimodal datasets** ‚Äì Table‚ÄØ1 summarizes annotation types across datasets, but the manuscript does not provide empirical comparisons (e.g., training a model on MedTrinity versus MIMIC‚ÄëCXR or SAMMed‚Äë20M) to substantiate the claimed superiority.  \n\n- **LLaVA‚ÄëTri training specifics** ‚Äì The pre‚Äëtraining schedule (epochs, batch size, learning rate) on MedTrinity‚Äë25M is not disclosed; only a brief reference to ‚Äú600‚ÄØK image‚Äëtext pairs from PMC‚Äë15M‚Äù appears in Section‚ÄØ4.1.  \n\n- **Downstream task validation** ‚Äì Despite assertions that the dataset supports report generation, classification, and segmentation, no experiments (e.g., BLEU/Rouge for report generation or standard segmentation metrics) are provided.  \n\n- **Ethical and legal considerations** ‚Äì The manuscript does not discuss consent, de‚Äëidentification, or licensing for the aggregated data drawn from more than 30 public sources, which is essential for public release.  \n\n- **Accessibility of the dataset** ‚Äì The placeholder URL and the lack of clear data format specifications, example scripts, or licensing information impede immediate adoption by the community.\n\n## A suggested decision\n\n**Major Revision**  \nThe work has clear potential, but substantial revisions are needed to address methodological gaps, provide fuller evaluation, and improve reproducibility and ethical reporting.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *MedTrinity‚Äë25M*, a large‚Äëscale multimodal medical dataset comprising over 25‚ÄØmillion image‚Äëregion‚Äëtext triplets across ten imaging modalities and more than sixty‚Äëfive disease categories. The proposed automated pipeline generates multigranular annotations by integrating metadata‚Äëbased captions, region localization using segmentation or grounding models, and knowledge retrieval from a vector database. These are combined into prompts for a multimodal large language model, producing detailed captions subsequently used to pre‚Äëtrain *LLaVA‚ÄëTri*. Reported results show moderate improvements on three standard VQA benchmarks. Overall, the manuscript addresses a recognized data scarcity problem and offers an ambitious resource, though clarity, reproducibility, and evaluation depth require strengthening.  \n\n**Major Comments**  \n1. **Dataset Characterization and Quality Control** ‚Äì The paper lists source datasets but omits the final modality distribution, duplicate handling, and filtering criteria used to construct the 25‚ÄØM samples.  \n2. **Reliability of ROI Generation** ‚Äì Results from the heterogeneous expert segmentation and grounding models are not quantitatively validated (e.g., with IoU), limiting confidence in the quality of the visual annotations.  \n3. **Prompting and Generation Details** ‚Äì The exact text prompts, hyper‚Äëparameters, sampling settings, and post‚Äëprocessing procedures are missing, impeding reproducibility.  \n4. **Annotation Quality Assessment** ‚Äì Only 200 samples were manually evaluated without inter‚Äërater statistics or significance testing, which is insufficient for a corpus of this magnitude.  \n5. **Pipeline Ablation** ‚Äì The qualitative comparisons presented do not quantify the contribution of each pipeline component to annotation quality.  \n6. **Comparative Evaluation** ‚Äì No empirical comparison against other large multimodal datasets is provided to substantiate claims of superior coverage or performance.  \n7. **Model Training Information** ‚Äì Key pre‚Äëtraining parameters for LLaVA‚ÄëTri (epochs, learning rate, batch size) are undisclosed, complicating reproducibility.  \n8. **Downstream Validation** ‚Äì Although report generation, segmentation, and classification are mentioned as target applications, corresponding experiments are not included.  \n9. **Ethical and Legal Aspects** ‚Äì The manuscript does not discuss data consent, de‚Äëidentification, licensing, or usage permissions, critical for public dataset release.  \n10. **Data Accessibility** ‚Äì The dataset link remains a placeholder, and data format, example scripts, and license details are not specified.  \n\n**Minor Comments**  \n- Several tables and figures (e.g., Figs.‚ÄØ2‚Äì5,‚ÄØ14) would benefit from clearer captions and consistent notation.  \n- Explicitly state the train/validation/test splits and the models used for each imaging modality.  \n- Check for typographical consistency in table headings and acronyms.  \n\n**Summary Paragraph**  \nThe submission makes a significant infrastructural contribution by assembling an unprecedentedly large multimodal medical dataset and detailing an automated multigranular annotation pipeline. While the idea and scale are commendable, the study lacks comprehensive validation of dataset quality, sufficient methodological transparency, and thorough ethical documentation. Reported VQA improvements are modest, and broader downstream utility remains unverified. Addressing these issues‚Äîparticularly reproducibility, quantitative evaluation, and responsible data release‚Äîwould greatly enhance the manuscript‚Äôs impact and credibility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial revision is needed to supply missing methodological details, expand evaluation, and clarify data governance before the work can be considered for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Ce Zhou",
      "James Zou",
      "Juncheng Wu",
      "Lang Gao",
      "Lei Xing",
      "Sheng Liu",
      "Xianhang Li",
      "Yunfei Xie",
      "Yuyin Zhou",
      "Cihang Xie",
      "Hong-Yu Zhou"
    ],
    "url": "pdfs/iclr.cc-2025-conference_d35cf31b42d0db969be38ced93f86fe655f141fa.pdf",
    "remote_url": "https://openreview.net/pdf/d35cf31b42d0db969be38ced93f86fe655f141fa.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement",
    "status": "completed",
    "evaluators": [
      "Yixuan",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Human-aligned models",
      "robust neural networks",
      "visual perception",
      "perceptual learning",
      "medical machine learning"
    ],
    "abstract": "The currently leading artificial neural network models of the visual ventral stream - which are derived from a combination of performance optimization and robustification methods - have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. We show that image perturbations generated by these models can enhance the ability of humans to accurately report the ground truth class. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) applying image perturbations that aid recognition for novice learners. We find that combining these model-based strategies leads to categorization accuracy gains of 33-72% relative to control subjects without these interventions, on unmodified, randomly selected held-out test images. Beyond the accuracy gain, the training time for the augmented learning group was also shortened by 20-23%, despite both groups completing the same number of training trials. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as two tasks in clinically relevant image domains - histology and dermoscopy - where visual learning is notoriously challenging. To the best of our knowledge, our work is the first application of artificial neural networks to increase visual learning performance in humans by enhancing category-specific image features.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents L-WISE, a framework leveraging adversarially robust ANNs to estimate image difficulty and apply nuanced perturbations that facilitate human learning in visual categorization tasks. By selecting challenging images and amplifying category-specific features, L-WISE improves human categorization accuracy by 33-72% and reduces training time by 20-23% across both general and clinical domains (e.g., dermoscopy and histology). The authors also discuss ethical implications, emphasizing the benefits of enhanced medical training and cautioning against potential biases that may arise from reliance on model-derived guidance.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The application of Robustified ANNs for improving human visual performance on image categorization seems like an interesting avenue. \n2. L-Wise empirically demonstrate gains in categorization accuracy and training efficiency. \n3. The paper addresses ethics concerns. Since the work mentions use of clinical data ethics discussion is of critical importance.\n\n### Weaknesses\n\n1. The paper focused on the performance of ventral stream. But we know that the human visual stream has a dorsal stream (where) that locates an object and the ventral stream (what) stream. And the interplay of these two streams forms the basis of human visual system. In this work the authors mainly focused on the ventral stream. From only quantified data, we can see the gains but it is very hard to trace this back to the nuanced perturbations the ANN produces. Hence, the suggestion is to use human gaze. The human gaze will precisely pin-point the \"where\" aspect and then will truly help us understand if at all the model perturbations are helping improve human performance. Can the authors please explain this? \n2. A robust DNN actually has worse performance on nominal data points. Data points that have not been corrupted adversarially. What was the motivation of the authors to select such a model for their experiments? \n3. The perturbations -  The perturbations if I am not mistaken are very subtle ones. For fine grained classifications, humans do follow curriculum learning but learning structures gradually from simpler to harder concepts. No experiments have shown this. It would be great if the authors can provide some empirical results/ explanation that can explain how will their method occur when you focus on structural cues rather than model perturbations that will benefit fine grained categorization. \n4. I am providing some citations related to Gaze and dual stream hypothesis that can help authors clarify my concerns \n\na. A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains, NeurIPS 2023.\nb. Literature related to papers accepted in NeurIPS Gaze Meets ML workshop. That workshop accepted papers will provide intuition of how human gaze can be used in coherence with DL models.\n\n### Questions\n\n1. For model perturbations, can the authors please provide heat maps or any qualitative results that will help us track the ANN perturbations to human visual learning? \n2. Is there a way to show if the study scales/generalizes to other ANNs as well other than Adversarially trained ones? Since human subjects have been used here, I am not sure how feasible experiments will be. This can be a general neural network or a network trained by the CutMix [a] loss that provides robustness benefits as well. \n3. I feel generating perturbations based of dual stream networks and then using human gaze to track these will be a much stronger claim to the work. Can the authors please address this question? \n4. What about adversarially trained transformers? The attention maps are different from CNN feature maps. How will the study be applicable for perturbations based of a transformer backbone? \n5. Please also address concerns raised in the weakness sections. \n\na. CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019,\n\n### Flag For Ethics Review\n\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 6\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **L-WISE**, a framework that employs adversarially robust artificial neural networks (ANNs) to estimate image difficulty and apply calculated perturbations aimed at enhancing human learning in visual categorization tasks. By selecting challenging images and amplifying category-specific features, L-WISE reports notable improvements in human categorization accuracy (33‚Äì72% increases) and reduced training time (20‚Äì23%) across general and clinical domains, including dermoscopy and histology. The authors also discuss ethical considerations, highlighting both the potential benefits for medical training and the risks of bias arising from model-guided human learning. The paper is generally clear, though the rationale and scope of experiments could be more thoroughly justified.  \n\n**Major Comments**  \n1. **Human Visual Processing and Model Interpretation:** The study focuses primarily on the ventral stream (‚Äúwhat‚Äù pathway) of human vision, overlooking the dorsal stream (‚Äúwhere‚Äù pathway) and their interaction. Since human visual recognition depends on both, the reviewer suggests incorporating **human gaze data** to clarify how ANN perturbations influence perceptual processes and to substantiate claims about improved human performance.  \n2. **Choice of Robust Models:** Adversarially trained networks are known to perform worse on nominal (non-perturbed) data. The rationale for using such models as the foundation for human teaching experiments needs clearer motivation.  \n3. **Perturbation Design and Learning Structure:** The perturbations appear very subtle. Given that humans typically learn through curriculum-like progression from simple to complex stimuli, empirical or theoretical justification is needed for how structural cues or perturbations benefit fine-grained categorization.  \n4. **Generality and Comparison:** The reviewer requests clarification on whether the approach generalizes beyond adversarially trained models‚Äîe.g., to standard ANNs or networks trained with regularization methods like CutMix‚Äîand whether results might differ using transformer architectures with attention maps.  \n5. **Qualitative Analysis:** Including visualizations such as heatmaps could help trace model perturbations and relate them to human learning patterns.  \n6. **Supporting Literature:** The reviewer points to works on dual-stream vision (e.g., *Dual-Stream Neural Network*, NeurIPS 2023) and research from the *Gaze Meets ML* workshop as valuable for addressing the interplay of gaze and deep learning.  \n\n**Minor Comments**  \n- Clarify the motivation for robustness-based training in non-adversarial contexts.  \n- Ensure proper citation of referenced works (e.g., CutMix, NeurIPS 2023, workshop papers).  \n- Further details on ethical data handling would strengthen the responsible research section.  \n\n**Summary Paragraph**  \nOverall, the paper presents an interesting and potentially impactful framework linking human learning and robust machine vision models. Strengths include the novel application to human training, empirical performance improvements, and attention to ethical implications. However, the conceptual grounding in human visual neuroscience, justification for model choice, and qualitative substantiation of perturbation effects require more depth. Addressing these points would significantly enhance the study‚Äôs scientific rigor and interpretability.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a method to enhance human visual learning by designing a model-based selection and enhancement algorithm to improve classification accuracy during testing. First, the authors select images to present to novice learners based on a model‚Äôs estimated recognition difficulty for each image. Next, they apply image perturbations intended to aid recognition for novice learners. The authors conduct experiment on the three benchmark datasets, including the natural image and the clinical image to verify the effectiveness of their proposal.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper is well-motivated, and a decent amount of technical details are given.\n2. The idea of improving the categorization performance of the novice learner by leveraging the capacity of the robustified artificial neural network is both interesting and practical.\n3. The reported improvement in novice learners' performance is notable, with gains in both test accuracy and reduced training time.\n\n### Weaknesses\n\n1. The establishment of the empirical observations is somewhat unconvincing. Do these observations hold in more complex classification tasks or when applied to medical imaging?\n2. The related work section lacks discussion of both the machine teaching and human-machine vision alignment methods.\n3. The size of the particants is somewhat small. \n4. The perception of enhanced images may be altered due to perturbations.\n\n### Questions\n\n1. The empirical observations are derived from a 16-way animal categorization task on natural images, which seems somewhat simplistic. It would be valuable to examine how these observations hold up in more complex categorization tasks or with different types of images, especially medical images. Given the typically limited availability of medical images, the proposed method could have promising applications in the medical imaging field.\n\n2. Beyond the empirical observations, is there any physiological insight or analysis on why the proposed model-based selection and enhancement method could improve novice learners‚Äô performance in categorization tasks?\n\n3. The authors do not discuss the related machine teaching literature. An in-depth comparison with machine teaching methods, particularly with \"Teaching Categories to Human Learners with Visual Explanations\" (CVPR 2018), would be valuable. This work similarly considers image difficulty; an introduction and comparison with it are beneficial.\n\n4. The authors should also discuss the connection to human-machine vision alignment methods, such as \"Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans\" (NeurIPS 2022).\n\n5. The sample size of participants is relatively small, and expanding the participant pool is recommended to enhance the reliability of the results; also, recruiting participants from diverse backgrounds would improve the generalizability of the findings. If expanding the participant pool is impractical due to time or budget constraints, performing a power analysis or discussing effect sizes could help strengthen the reliability of the analysis.\n\n6. When using perturbations to enhance images, how does the method ensure that essential image details remain unchanged, particularly when using a large œµ (e.g., 20)? This concern is especially pertinent for medical images, where even slight pixel changes may alter critical information. I recommend involving medical experts to review the enhanced images or using quantitative similarity measures (such as SSIM or FSIM) to verify that essential details are preserved.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 2,
          "constructiveness": 2,
          "stance": 2,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a model-based method for enhancing human visual learning, aiming to improve novice learners‚Äô classification accuracy through selective image presentation and targeted image perturbations. The approach involves estimating recognition difficulty using a model to choose images and then applying perturbations to facilitate recognition. Experiments on three benchmark datasets, including both natural and clinical images, are used to validate the method. Overall, the paper is clearly motivated, technically detailed, and addresses an interesting and practical problem in human-in-the-loop visual learning.\n\n**Major Comments**  \n1. The empirical validation is somewhat limited. The observations are derived primarily from a 16-way animal categorization task, which may not generalize to more complex settings or medical imaging applications. Further experiments or discussions addressing these contexts would strengthen the work.  \n2. The related work section is incomplete. The authors should include and compare to literature on machine teaching and human‚Äìmachine vision alignment, such as *Teaching Categories to Human Learners with Visual Explanations* (CVPR 2018) and *Harmonizing the Object Recognition Strategies of Deep Neural Networks with Humans* (NeurIPS 2022).  \n3. The small participant sample limits the robustness of the results. Expanding the participant pool or performing statistical power analysis and reporting effect sizes would improve the credibility and generalizability of the findings.  \n4. The effects of image perturbations raise concerns about potential alteration of essential visual information. For sensitive applications such as medical imaging, the authors should consider validation by domain experts or include quantitative measures (e.g., SSIM, FSIM) to ensure that crucial details remain unaltered.  \n5. The paper lacks physiological or cognitive explanation of why the proposed selection and enhancement strategy aids human learners. A brief theoretical or empirical discussion could help clarify the mechanism underlying the observed performance improvements.\n\n**Minor Comments**  \n- Some typographical errors (e.g., ‚Äúparticants‚Äù) should be corrected.  \n- Clarify notation for perturbation magnitude (œµ) and ensure examples correspond clearly to figures or tables.\n\n**Summary Paragraph**  \nThis paper presents a well-motivated and methodically described approach to improving novice categorization performance by leveraging model-based selection and enhancement of learning images. Its strengths lie in a practical problem formulation and reported improvements in learner performance and training efficiency. However, the study would benefit from broader experimental validation, deeper engagement with related literature, larger participant samples, and stronger assurances regarding image integrity after perturbation. Addressing these points would considerably strengthen the empirical and conceptual foundations of the work.\n\n**Decision Recommendation**  \n**Major Revision.** The study is promising and well-presented but requires additional experimentation, literature integration, and methodological clarification before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes a novel approach to augment human learning in image categorization tasks. By leveraging robustified ANNs, the study introduces model-guided image selection and enhancement strategies that increase human test-time categorization accuracy by up to 72% and reduce training duration by around 20-23%. L-WISE employs selecting images based on predicted difficulty levels and enhancing images with pixel perturbations.The proposed approach is tested on natural images, dermoscopy, and histology images. The results demonstrates efficacy of L-WISE in aiding novice learners in fine-grained categorization tasks. This research represents one of the first applications of ANNs in optimizing human visual learning in clinically relevant domains.\n\n### Soundness: 1\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- Presents an innovative use of robustified ANNs to predict task difficulty and enhance images, aiding human perception and learning.\n- Shows broad applicability by successfully testing across diverse domains, such as natural image classification, dermoscopy, and histology.\n- Achieves practical efficiency by reducing training time and improving test-time accuracy, beneficial for fields requiring rapid, accurate human image categorization training.\n\n### Weaknesses\n\n- Lacks a dedicated related work section, which would help contextualize the research.\n- Both low and high logits from ANNs show significant variation in human accuracy, making predictions less reliable in certain logit intervals.\n- Uses only the ResNet-50 architecture, limiting generalization; further testing with models like vision transformers (ViT) is needed to support broader conclusions.\n- Image enhancement may introduce biases, potentially improving accuracy only for certain major classes; additional metrics like precision and recall per class, rather than just mean accuracy, should be reported to provide a clearer assessment.\n\n### Questions\n\n1. How to choose $\\epsilon$ for different tasks and image domains?\n2. What criteria determine if a model is \"robustified\" enough for use? Have you considered specific metrics to evaluate the robustness of guide models, and how do these metrics correlate with human learning outcomes?\n3. Did you collect qualitative feedbacks from participants? Did the new curriculum and enhanced images increase mental stress of human learners? Additional learning costs beyond training time should be considered, such as cognitive load and emotional well-being.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nThe use of ANN-guided models in human learning may introduce unintended biases, potentially affecting participants' learning outcomes in ways that favor certain demographic groups over others. For instance, if the training of ANNs inadvertently enhances accuracy for only a subset of the population (such as specific genders, ages, or races), it could lead to biased learning outcomes. Such biases could potentially skew related job opportunities, ultimately reinforcing inequities. Ensuring that ANNs benefit all demographic groups equitably requires further investigation and ongoing evaluation to mitigate these risks.\n\nThe above concern is addressed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *L-WISE*, a novel framework for augmenting human learning in image categorization by leveraging robustified artificial neural networks (ANNs). The system selects and enhances training images based on predicted difficulty, thereby improving human test-time categorization accuracy (up to 72% increase) and reducing training time by approximately 20‚Äì23%. Experiments span natural, dermoscopy, and histology images, demonstrating promising results, particularly for novice learners in fine-grained classification tasks. Overall, the paper is clearly written, methodologically sound, and offers an original contribution linking human visual learning and model-based task optimization.  \n\n**Major Comments**  \n1. **Context and Related Work** ‚Äì The paper lacks a dedicated related work section, making it difficult to situate the research within the broader literature on human-in-the-loop learning and model-guided training.  \n2. **Predictive Reliability** ‚Äì Variation in human accuracy for both low and high ANN logits suggests limited reliability of the difficulty predictions in certain confidence ranges. Clarification and quantitative analysis of these intervals would strengthen conclusions.  \n3. **Model Architecture Limitation** ‚Äì The study relies solely on ResNet-50; extending experiments to other architectures, such as vision transformers, would improve claims of generality.  \n4. **Evaluation Metrics** ‚Äì By reporting only mean accuracy, the analysis may obscure class-specific biases introduced by image enhancement. Additional metrics (e.g., per-class precision and recall) are recommended to assess fairness and robustness.  \n5. **Ethical and Cognitive Considerations** ‚Äì While no ethics review is formally needed, potential biases in ANN-guided learning outcomes across demographic groups merit attention. The study should also assess learner well-being and cognitive load given the modified training regimen.  \n\n**Minor Comments**  \n- Clarify criteria for choosing the perturbation parameter (Œµ) across tasks and domains.  \n- Define what constitutes an adequately ‚Äúrobustified‚Äù model and specify metrics used to quantify robustness in relation to learning outcomes.  \n- Consider reporting qualitative participant feedback to complement quantitative metrics.  \n\n**Summary Paragraph**  \nThis work presents a creative integration of robustified ANNs and human learning, offering both practical benefits and conceptual insights. Strengths include the novelty of model-guided human training, methodological clarity, and broad applicability. Main limitations concern missing contextualization, limited model diversity, incomplete performance metrics, and open ethical considerations regarding potential bias and learner strain. Addressing these issues would substantially enhance the paper‚Äôs rigor and impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper makes a strong and innovative contribution but requires additional experiments, analysis, and contextual discussion to fully substantiate its conclusions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚ö†Ô∏è Borderline**  \n   The manuscript, *L‚ÄëWISE: Boosting Human Visual Category Learning through Model‚ÄëBased Image Selection and Enhancement*, explores how adversarially robust neural networks can predict human image recognition difficulty and generate subtle ‚Äúenhancements‚Äù that facilitate visual category learning for humans.  \n   Although the paper includes experiments in dermoscopy and histology, the primary methodological innovation lies in **human learning augmentation** and **human‚ÄëAI perceptual alignment**, rather than in imaging physics, reconstruction, or quantitative medical image analysis per se. These goals fall closer to human‚Äìmachine interaction or cognitive vision than to core TMI topics (e.g., image formation, quantitative biomarkers, or clinical imaging pipelines). The contribution is therefore *marginally relevant* to TMI‚Äôs methodological orientation.\n\n2. **Novelty & Contribution Level ‚Äì Moderate**  \n   The concept of using robustified ANNs to both estimate human recognition difficulty and generate category‚Äëspecific perturbations to aid learning is novel in the context of perceptual augmentation. The combination of difficulty‚Äëweighted image selection and enhancement (‚ÄúL‚ÄëWISE‚Äù) is an original integration. However, within medical imaging, the methodological novelty is indirect: the work shows utility of an existing robust adversarial training framework rather than introducing a new imaging algorithm.  \n\n3. **Technical and Experimental Rigor ‚Äì Generally strong, with caveats**  \n   ‚Ä¢ Clear explanation of model training, datasets (HAM10000, MHIST, moth data), and subject recruitment.  \n   ‚Ä¢ Multiple ablation and cross‚Äëdomain validations reinforce internal consistency.  \n   ‚Ä¢ The human‚Äësubject experiments are well powered for behavioral analysis, not for clinical validation.  \n   ‚Ä¢ No quantitative evaluation against radiologist performance or diagnostic endpoints‚Äîhence limited medical imaging rigor.  \n   ‚Ä¢ Statistical treatment (œá¬≤ tests, bootstrapped CIs, logistic regressions) is appropriate.\n\n4. **Clarity and Presentation ‚Äì Excellent**  \n   Manuscript is well structured, well illustrated, and grammatically polished. Figures are clear and annotated; methods and appendices provide reproducibility details.\n\n5. **Ethical and Reproducibility Compliance ‚Äì Adequate**  \n   IRB approval, informed consent, fair participant compensation, and code availability are indicated. The authors discuss dataset biases (e.g., skin‚Äëtone imbalance in HAM10000) responsibly.\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \n   The authors propose *Logit‚ÄëWeighted Image Selection and Enhancement* (L‚ÄëWISE), a model‚Äëbased method to accelerate and improve human visual category learning. Using adversarially trained (‚Äúrobustified‚Äù) neural networks, they (a) predict each image‚Äôs recognition difficulty for humans from the ground‚Äëtruth logit output, and (b) apply pixel‚Äëbounded perturbations that highlight class‚Äëspecific features. These signals define an adaptive curriculum of progressively harder images and fading enhancement strength. Human participants trained with L‚ÄëWISE attained 33‚Äì72‚ÄØ% higher test‚Äëphase accuracy and ~20‚ÄØ% shorter training duration across natural, dermoscopy, and histology images.\n\n2. **Strengths**  \n   ‚Ä¢ Creative cross‚Äëdisciplinary idea linking adversarial robustness to human perceptual learning.  \n   ‚Ä¢ Extensive behavioral validation with >500 participants.  \n   ‚Ä¢ Strong statistical analysis and reproducibility documentation.  \n   ‚Ä¢ Ethical and bias awareness sections are exemplary.  \n   ‚Ä¢ Potential relevance for educational tools in pathology and dermatology training.\n\n3. **Weaknesses**  \n   ‚Ä¢ The methodological innovation pertains to **human learning augmentation**, not directly to **medical imaging methodology** as defined by TMI.  \n   ‚Ä¢ The medical tasks used are illustrative small‚Äëscale classification problems; no evidence of translation to clinical diagnostic improvement.  \n   ‚Ä¢ Enhancement mechanism is largely heuristic‚Äîno formal modeling of perceptual or imaging physics.  \n   ‚Ä¢ The perturbations may distort image realism; risk of misleading visual cues is insufficiently quantified.  \n   ‚Ä¢ Sparse comparison with existing curriculum‚Äëlearning or active‚Äëlearning frameworks in medical imaging.\n\n4. **Major Comments**  \n   1. **Clarify TMI relevance.** The paper should explicitly discuss how L‚ÄëWISE contributes to *medical imaging science* beyond cognitive training‚Äîfor instance, could similar methodology optimize annotation, reader training, or quality control in radiology workflows?  \n   2. **Quantify perturbation realism.** Provide perceptual or image‚Äëquality metrics (e.g., SSIM, color fidelity) on medical data to ensure that enhancements do not produce artificial or non‚Äëphysiologic appearances.  \n   3. **Ablation within clinical tasks.** The ablation study is mainly on natural or dermoscopy data; it would strengthen the translational argument to repeat at least one ablation in histology.  \n   4. **Statistical generalization.** Randomization and participant pooling across experiments should be formalized‚Äîreport per‚Äësubject variance and number of images per condition.  \n   5. **Potential biases.** Include quantitative assessment of whether perturbations exacerbate known dataset biases (e.g., skin tone).  \n\n5. **Minor Comments**  \n   ‚Ä¢ Correct occasional typographical issues (‚Äúrobustu‚Äëfied‚Äù, ‚Äúprecariousness‚Äù).  \n   ‚Ä¢ Provide actual number of images per class and dataset partition in main text rather than only in appendix.  \n   ‚Ä¢ Clarify computational cost for generating perturbations and whether code supports GPU inference.  \n   ‚Ä¢ Suggest renaming sections on ‚Äúhistology‚Äîmalignant vs‚ÄØbenign‚Äù to specify true class labels (hyperplastic polyp vs‚ÄØsessile serrated adenoma).  \n   ‚Ä¢ Ensure all figures use consistent font sizes and color legends.\n\n6. **Overall Recommendation** ‚Äì **Reject (Out‚ÄØof‚ÄØScope / Low‚ÄØNovelty for TMI)**  \n   The study is methodologically careful and creative, but its primary focus‚Äîaugmenting human visual learning via robustified networks‚Äîis outside the central scope of *IEEE‚ÄØTransactions on Medical Imaging*, which prioritizes new imaging or analytic methodologies rather than cognitive‚Äëtraining paradigms. A venue emphasizing human‚ÄìAI interaction, neuro‚Äëinspired vision, or medical education technology (e.g., *IEEE‚ÄØTransactions on Cognitive and Developmental Systems* or *Frontiers in Digital Health*) would be a better fit.\n\n7. **Confidence Level:**‚ÄØ4‚ÄØ/‚ÄØ5  \n   High confidence in assessment of scientific quality and scope alignment; moderate uncertainty about editorial policy boundaries.\n\n---\n\n**Summary Judgment:**  \nInnovative and well‚Äëexecuted behavioral research with interesting implications for visual training, but its contributions lie more in human‚Äìmachine perceptual alignment than in the methodological advancement of medical imaging. Recommend rejection for TMI due to limited scope fit, while encouraging submission to a journal focused on human‚ÄìAI perception or educational applications.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 3,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript titled *L‚ÄëWISE: Boosting Human Visual Category Learning through Model‚ÄëBased Image Selection and Enhancement* proposes a model‚Äëbased method to accelerate human visual category learning. The approach leverages adversarially trained robust neural networks to estimate image recognition difficulty for humans and to generate subtle, class‚Äëspecific ‚Äúenhancements‚Äù that aid visual learning. Experiments include natural, dermoscopy, and histology image sets, showing substantial gains in participant accuracy and reduced training time. The manuscript is clearly written, well organized, and includes strong experimental and ethical documentation. However, its main innovation lies in perceptual and behavioral learning rather than in the development of new medical imaging methodologies, placing it at the boundary of the field‚Äôs traditional scope.  \n\n**Major Comments**  \n1. **Relevance to Medical Imaging:** The work primarily focuses on human learning augmentation rather than advancing core medical imaging methods. The authors should more directly discuss how their framework contributes to imaging science beyond cognitive training‚Äîfor example, potential use in annotation, reader training, or workflow optimization.  \n2. **Perturbation Realism:** The enhancement process may alter image fidelity. Include quantitative perceptual or image‚Äëquality assessments (e.g., SSIM, color fidelity) to confirm the images remain clinically realistic.  \n3. **Domain‚ÄëSpecific Validation:** The ablation analysis emphasizes natural and dermoscopy data; it would strengthen translational value to replicate one ablation within histology tasks.  \n4. **Experimental Rigor and Generalization:** Clarify participant randomization and pooling, and report per‚Äësubject variance and the number of images per condition to validate statistical generalization.  \n5. **Dataset Biases:** Quantitatively assess whether perturbations amplify known dataset biases such as skin‚Äëtone imbalance in HAM10000.  \n\n**Minor Comments**  \n- Correct small typographical errors (‚Äúrobustu‚Äëfied,‚Äù ‚Äúprecariousness‚Äù).  \n- Provide explicit counts of images per class and data split in the main text.  \n- Clarify the computational cost and GPU requirements for generating perturbations.  \n- Refine section titles for medical tasks to specify true histologic class names.  \n- Standardize figure fonts and color legends for consistency.  \n\n**Summary Paragraph**  \nOverall, this is a creative and technically well‚Äëexecuted study linking adversarial robustness with human perceptual learning. The behavioral experiments are thoughtfully designed and statistically sound, with appropriate ethical safeguards. Nevertheless, the central contribution pertains to human learning enhancement rather than to imaging physics, analysis methodology, or clinical evaluation. As such, while the work is scientifically solid and potentially valuable for medical education or human‚ÄëAI interaction research, it falls outside the core methodological scope typically expected for medical imaging studies.  \n\n**Decision Recommendation:** **Reject (Out of Scope / Limited Methodological Novelty)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents L-WISE (Logit-Weighted Image Selection and Enhancement), a method that uses robustified artificial neural networks to augment human visual category learning. The authors establish that ground truth logit values from adversarially-trained models can predict human image recognition difficulty (Figure 1A) and that perturbations maximizing these logits enhance human categorization accuracy (Figure 1B). L-WISE combines two strategies: (1) selecting training images based on model-estimated difficulty with gradually increasing complexity, and (2) applying image enhancements that decrease over time (Figure 2). The method was tested across three domains: moth species classification, dermoscopy lesion classification, and histology tissue classification. Results show 33-72% relative accuracy gains above chance on unmodified test images compared to controls, while reducing training time by 20-23% (Figure 4A). Ablation studies indicate both image selection and enhancement contribute to performance improvements (Table 1).\n\n## Weaknesses\n\n‚Ä¢ **Limited Mathematical Foundation for Enhancement Optimization** \n  - Equation 1 lacks theoretical justification for the choice of Œ± parameter and its relationship to enhancement effectiveness, with Œ± set to different values (0 for ImageNet, 1 for fine-grained tasks) without principled reasoning\n  - The projection operation in Equation 2 uses a simple L2 constraint but no analysis is provided on why this norm is optimal for human perception compared to other constraint formulations\n  - The step size Œ∑ and number of steps formula (ceil(2œµ) for ResNet-50, ceil(4œµ) for XCiT) appear ad-hoc without convergence analysis or theoretical grounding (Page 10, Appendix S1)\n\n‚Ä¢ **Insufficient Statistical Power and Multiple Comparisons Control**\n  - Sample sizes of ~30 participants per condition (Page 10) may be inadequate for detecting meaningful effect sizes, particularly for class-specific analyses where the authors acknowledge being \"statistically underpowered\" (Page 26)\n  - No correction for multiple comparisons is applied across the numerous statistical tests performed, including comparisons between conditions in Table 1 and across different image domains\n  - The logistic regression analysis in Figure 1A pools data from different experimental conditions without accounting for potential confounding factors or participant-level random effects\n\n‚Ä¢ **Questionable Generalizability of Enhancement Schedules**\n  - The enhancement schedules (exponential tapering from Œµ=8, difficulty selection increasing by 0.15 per block) were not optimized systematically and appear to be based on limited pilot testing (Page 10, Figure S12H-M)\n  - No evidence provided that the fixed schedules work across different participant populations, learning rates, or task complexities beyond the three tested domains\n  - The authors acknowledge not conducting \"exhaustive search for optimal curriculum design strategies\" (Page 9) but provide no framework for how practitioners could adapt these schedules to new domains\n\n‚Ä¢ **Methodological Concerns in Human Experiments**\n  - The 17ms presentation time for ImageNet animal classification (Page 10) is extremely brief and may not reflect natural viewing conditions, potentially limiting ecological validity\n  - Assignment of Greek mythology aliases to categories (Page 10) introduces an additional layer of abstraction that may not generalize to real-world learning scenarios where meaningful category names are available\n  - Dropout analysis (Appendix S9) suggests systematic differences between groups that could indicate selection bias, with control participants withdrawing at significantly higher rates\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen Mathematical Framework for Enhancement**\n  - Provide theoretical analysis or empirical justification for the choice of Œ± parameter in Equation 1, including systematic evaluation across different Œ± values to determine optimal weighting\n  - Compare L2 projection constraints against other norms (L‚àû, L1) and provide principled guidelines for selecting constraint types based on image domain characteristics\n  - Develop convergence criteria and theoretical bounds for the optimization procedure in Equation 2, replacing ad-hoc step size formulas with principled optimization schedules\n\n‚Ä¢ **Improve Statistical Rigor and Power Analysis**\n  - Conduct proper power analysis to determine adequate sample sizes for detecting meaningful effects, particularly for class-specific comparisons mentioned in Table S1\n  - Apply appropriate multiple comparisons corrections (Bonferroni, FDR) across all statistical tests and clearly report adjusted p-values\n  - Implement mixed-effects models that account for participant-level variability and potential confounding factors in the pooled analyses shown in Figure 1A\n\n‚Ä¢ **Develop Systematic Schedule Optimization Framework**\n  - Implement systematic hyperparameter optimization (grid search, Bayesian optimization) for enhancement and difficulty selection schedules across different domains\n  - Provide adaptive scheduling algorithms that adjust enhancement and difficulty parameters based on individual learner performance during training\n  - Establish guidelines or decision trees for practitioners to customize schedules based on domain characteristics, task complexity, and target population\n\n‚Ä¢ **Enhance Experimental Methodology and Controls**\n  - Test multiple presentation durations for image classification tasks to establish the robustness of findings across realistic viewing conditions\n  - Include conditions with meaningful category names alongside Greek aliases to assess the impact of semantic knowledge on learning outcomes\n  - Implement balanced randomization procedures and intention-to-treat analyses to address potential selection bias indicated by differential dropout rates across conditions",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **L-WISE (Logit-Weighted Image Selection and Enhancement)**, a framework that leverages adversarially-trained neural networks to augment human visual category learning. The method uses model-derived logits to estimate the visual difficulty of images, applying progressive selection and enhancement strategies across multiple domains including moth species, dermoscopy lesions, and histology tissues. Experiments show consistent improvements in categorization accuracy and training efficiency. The paper is well structured and clearly presented, with thoughtful motivation and comprehensive experimental validation. However, important concerns remain regarding the theoretical foundation, statistical robustness, and generalizability of the proposed approach.\n\n---\n\n**Major Comments**  \n\n1. **Limited Mathematical Foundation for Enhancement Optimization**  \n   - The rationale for key parameters such as the Œ± weight in Equation‚ÄØ1 and the step size and iteration count in Equation‚ÄØ2 is not theoretically justified.  \n   - The use of an L2 projection constraint lacks discussion of its perceptual appropriateness relative to alternative norms.  \n   - Parameter choices appear ad‚ÄØhoc, and no convergence or optimization analysis is provided.\n\n2. **Insufficient Statistical Power and Lack of Multiple Comparisons Control**  \n   - Participant sample sizes (~30 per condition) may be inadequate for detecting meaningful effects, particularly in class-specific analyses acknowledged as underpowered.  \n   - The study does not correct for multiple comparisons across numerous conditions and domains.  \n   - Logistic regression analyses pool data across experiments without accounting for participant-level variability or confounders.\n\n3. **Questionable Generalizability of Enhancement and Difficulty Schedules**  \n   - Enhancement and difficulty schedules are fixed and based on limited pilot testing without systematic optimization.  \n   - No evidence is presented that these schedules transfer to new populations, tasks, or learning rates, and no framework is offered for adapting them.  \n\n4. **Methodological Concerns in Human Experiments**  \n   - The 17‚ÄØms presentation time for ImageNet tasks may limit ecological validity.  \n   - Using arbitrary category aliases adds unnecessary abstraction compared to meaningful labels.  \n   - Differential dropout rates across conditions may indicate selection bias that is not fully addressed.\n\n---\n\n**Minor Comments**  \n- Clarify parameter definitions (Œ±,‚ÄØŒ∑,‚ÄØŒµ) in the main text for easier reference.  \n- Figure and appendix references could be streamlined; some details (e.g., step-size formulas) could move to supplementary materials.  \n- Ensure all reported p-values correspond to specified tests after any corrections.\n\n---\n\n**Summary Paragraph**  \nOverall, the study offers an interesting and potentially impactful method for improving human perceptual learning via machine-guided curricula. Its experimental scope and observed performance gains are notable strengths. However, the theoretical underpinnings of the enhancement mechanism, statistical treatment of human study data, and generalizability of scheduling parameters require substantial reinforcement. Addressing these weaknesses would significantly strengthen the paper‚Äôs scientific rigor and applicability.\n\n---\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### Comprehensive Peer Review\n\n#### The manuscript investigates the use of artificial neural networks (ANNs) to enhance human visual categorization accuracy through image perturbations and model-based image selection, specifically in the context of medical imaging tasks such as dermoscopy and histology. The authors propose a method called Logit-Weighted Image Selection and Enhancement (L-WISE), which aims to improve categorization accuracy and reduce training time. The results demonstrate significant improvements in human learning outcomes across various domains, supported by empirical evidence from multiple datasets.\n\n####\n\n## Major Comments\nStrengths\n1. Comprehensive Scope: The abstract and main text cover a wide range of aspects, including motivation, methodology, results, and implications, making the study informative and relevant to a broad audience.\n2. Methodological Depth: The Methods section and Appendix provide detailed descriptions of the techniques used, including robustified ANNs, image enhancement techniques, and the L-WISE approach. This depth supports the reproducibility and validation of the findings.\n3. Statistical Rigor: The Results section employs appropriate statistical methods, such as chi-square tests, logistic regression, and ANOVA, to analyze the data, adding credibility to the conclusions.\n4. Wide Application: The study applies the proposed method across multiple domains, including natural images and clinically relevant image sets like histology and dermoscopy, demonstrating its generalizability.\n\nLimitations\n1. Terminology Complexity: The technical terminology used in the Methods section and Appendix might be challenging for readers without a strong background in machine learning and image processing.\n2. Limited Ablation Study: The ablation study is limited to moth photographs and dermoscopy images, lacking similar analysis for the histology task, which could weaken the generalizability of the findings.\n3. Ambiguity in Training Time Reduction: The abstract mentions a 20-23% reduction in training time, but the specific factors contributing to this reduction are not clearly explained, leading to potential misinterpretation.\n4. Inadequate Exploration of Hallucinations: While the phenomenon of \"hallucinations\" in enhanced images is discussed, its implications in educational settings are not fully explored, leaving room for further investigation.\n\n####\n\n## Minor Comments\n1. Formatting Inconsistencies: Some references lack page numbers or have inconsistent formatting, which should be corrected to comply with the journal's citation style guidelines.\n2. Redundancy in Appendix: Some technical descriptions in the Appendix are repetitive of the main text and could be streamlined to improve readability.\n3. Clarification Needed: The dropout analysis in the Results section is somewhat convoluted and could benefit from clearer explanations or simplified calculations to aid reader comprehension.\n4. Demographic Data: More detailed breakdowns of participant demographics, particularly across different tasks, would enhance the robustness of the study.\n\n####\n\n## Summary Paragraph\nThe manuscript meets the significance criterion by addressing a relevant problem with practical applications in medical imaging. The innovation criterion is met through the development of the L-WISE method, which shows promise in enhancing human visual learning. The evaluation criterion is satisfied with comprehensive statistical analyses and empirical validation across multiple datasets. However, the reproducibility criterion could be strengthened by providing more detailed explanations of hyperparameters and addressing potential biases more comprehensively.\n\n####\n\n## Decision Recommendation\nMajor Revision\n\nThe manuscript has substantial merit and addresses an important problem in medical imaging. However, several areas require refinement to ensure clarity, address potential biases, and fully explore the implications of the findings. Specifically, the authors should simplify technical descriptions, conduct a more comprehensive ablation study, and provide a more detailed exploration of the hallucination phenomenon. These revisions will enhance the manuscript's overall quality and impact.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript explores the use of artificial neural networks (ANNs) to augment human visual categorization through image perturbations and data selection strategies. The proposed method, Logit-Weighted Image Selection and Enhancement (L-WISE), is applied to domains such as dermoscopy and histology, aiming to improve human learning accuracy and reduce training time. Results indicate meaningful improvements across multiple datasets. Overall, the paper is comprehensive in scope and technically detailed, though certain methodological and interpretive aspects require clarification and deeper investigation.  \n\n**Major Comments**  \n**Strengths**  \n1. **Comprehensive Scope:** The paper effectively covers motivation, methodology, results, and implications, making it accessible and relevant to various research audiences.  \n2. **Methodological Depth:** The Methods and Appendix contain substantial detail on the approaches used, including robustified ANNs and image enhancement pipelines, supporting reproducibility.  \n3. **Statistical Rigor:** Analyses employ appropriate tests (e.g., chi-square, logistic regression, ANOVA), lending credibility to the conclusions.  \n4. **Broad Applicability:** The method is tested on natural, dermoscopic, and histological images, demonstrating potential generalizability.  \n\n**Limitations**  \n1. **High Terminological Complexity:** The technical language in methodological descriptions may challenge readers without a strong background in machine learning or image processing.  \n2. **Insufficient Ablation Coverage:** The ablation analysis is limited to moth and dermoscopy datasets, omitting evaluation on histology, which restricts claims of generality.  \n3. **Unclear Explanation of Time Reduction:** The reported 20‚Äì23% reduction in training time lacks a clear account of its causes or contributing factors.  \n4. **Underexplored Hallucination Effects:** While ‚Äúhallucinations‚Äù in enhanced images are noted, their educational significance is not thoroughly examined.  \n\n**Minor Comments**  \n1. Reference formatting should be standardized, ensuring consistent inclusion of page numbers.  \n2. The Appendix repeats methodological details from the main text and could be streamlined.  \n3. The dropout analysis is difficult to follow; clearer explanation of its procedure and interpretation is needed.  \n4. Participant demographic details, particularly across tasks, should be more fully reported.  \n\n**Summary Paragraph**  \nThe study makes a meaningful and innovative contribution through L-WISE, addressing the problem of improving human categorization in medical imaging. Its analyses are rigorous and empirically supported, strengthening the validity of findings. Nonetheless, improvements are needed in methodological clarity, ablation breadth, and interpretation of certain phenomena to enhance reproducibility and transparency.  \n\n**Decision Recommendation**  \n**Major Revision.** The manuscript is strong and promising but requires additional clarification, broader experimentation, and refinement of explanations before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## L-WISE: Boosting Human Visual Category Learning Through Model-Based Image Selection and Enhancement\n\n### Summary\n\nThe paper proposes L-WISE, a model-based curriculum for human visual category learning that combines two components derived from robustified ANNs: (i) using the ground-truth logit to predict image difficulty for humans and select training examples on an easy-to-hard schedule, and (ii) applying pixel-space perturbations that maximize the ground-truth logit to enhance category-relevant features, with enhancement tapered over time. Across three domains (fine-grained moth species, dermoscopy, histology), L-WISE improves test accuracy on unmodified, randomly sampled held-out images by 33‚Äì72% over controls (relative to chance margin) while reducing training time by ~20‚Äì23%, and the difficulty predictor achieves AUC ~0.72 on a 16-way basic animal task.\n\n### Strengths\n\n- Technical novelty and innovationUses adversarially trained (robustified) models‚Äô ground-truth logits as an actionable, human-aligned difficulty score for curriculum design, and leverages perceptually aligned gradients for category-specific enhancement aimed at helping humans (rather than fooling models).The idea of tapering enhancement while increasing difficulty is a simple, principled, and model-agnostic schedule that is likely to generalize.Clear separation between model-guided enhancement and human evaluation on unmodified test images mitigates concerns about learning artifacts unique to perturbed images.\n- Experimental rigor and validationMultiple image domains (natural fine-grained, dermoscopy, histology) with consistent improvements and statistical testing; includes ablations disentangling selection vs. enhancement and the role of ordering.Comparison against off-the-shelf enhancement baselines that are stronger in L2 magnitude yet fail to improve human accuracy, supporting the specificity of the ANN-guided method.Large-scale 16-way task for difficulty-prediction validation and logistic-regression analysis with cross-validation.\n- Clarity of presentationThe conceptual pipeline and the two mechanisms are explained clearly with helpful figures; the curriculum schedules and ablation conditions are described at a level enabling reproduction.Ethical and limitations sections are explicit and thoughtful.\n- Significance of contributionsDemonstrates that robustified ANNs can be used to actively facilitate human learning, not just explain or predict human behavior, with implications for education and clinical training.Advances a practical state-of-the-art difficulty predictor for human image recognition and introduces a concrete, deployable pipeline for web-based learning experiments.\n\n- Uses adversarially trained (robustified) models‚Äô ground-truth logits as an actionable, human-aligned difficulty score for curriculum design, and leverages perceptually aligned gradients for category-specific enhancement aimed at helping humans (rather than fooling models).\n- The idea of tapering enhancement while increasing difficulty is a simple, principled, and model-agnostic schedule that is likely to generalize.\n- Clear separation between model-guided enhancement and human evaluation on unmodified test images mitigates concerns about learning artifacts unique to perturbed images.\n\n- Multiple image domains (natural fine-grained, dermoscopy, histology) with consistent improvements and statistical testing; includes ablations disentangling selection vs. enhancement and the role of ordering.\n- Comparison against off-the-shelf enhancement baselines that are stronger in L2 magnitude yet fail to improve human accuracy, supporting the specificity of the ANN-guided method.\n- Large-scale 16-way task for difficulty-prediction validation and logistic-regression analysis with cross-validation.\n\n- The conceptual pipeline and the two mechanisms are explained clearly with helpful figures; the curriculum schedules and ablation conditions are described at a level enabling reproduction.\n- Ethical and limitations sections are explicit and thoughtful.\n\n- Demonstrates that robustified ANNs can be used to actively facilitate human learning, not just explain or predict human behavior, with implications for education and clinical training.\n- Advances a practical state-of-the-art difficulty predictor for human image recognition and introduces a concrete, deployable pipeline for web-based learning experiments.\n\n### Weaknesses\n\n- Technical limitations or concernsEnhancement via logit maximization can homogenize class appearances and potentially bias learners toward a subset of within-class features; risks of dataset-induced biases are acknowledged but not systematically quantified.The number of classes in the learning tasks is small (2‚Äì4), leaving open whether gains persist in higher-cardinality, more ecologically complex settings.The enhancement objective sometimes includes minimizing non-target logits (Œ±=1); this could suppress shared features in multi-class settings in non-obvious ways and might introduce artifacts.\n- Experimental gaps or methodological issuesThe ‚Äústate-of-the-art‚Äù difficulty prediction claim relies on an AUC comparison to prior methods reported in the appendix; a main-text quantitative comparison (with identical data splits) would strengthen the claim.The ablation finding that DS (shuffled) ‚â• DS suggests that ordering may be less critical than distributional easing, but the paper does not fully analyze why, nor quantify the respective contributions using a unified model (e.g., mixed-effects modeling of trial difficulty, order, and enhancement).No explicit controls to rule out class-specific ‚Äúwatermarking‚Äù or low-level artifact learning (e.g., matched-norm sham perturbations that preserve power spectrum but are label-agnostic).Difficulty-prediction performance is shown on the 16-way animal task; analogous predictive evaluation is not reported for the dermoscopy and histology datasets, despite using logits to schedule curricula there.\n- Clarity or presentation issuesSome statistical details are brief (e.g., effect sizes in absolute terms, multiple comparison corrections, power analyses); reliance on ‚Äúrelative to chance margin‚Äù can overstate perceived improvements.Limited detail in the main text about PGD settings, clipping, color management, and display timing fidelity (web-based 17 ms exposures) leaves potential reproducibility or interpretability questions.\n- Missing related work or comparisonsA direct empirical comparison to educational baselines that highlight where to look (e.g., EXPLAIN, CAM/bounding-box overlays) would contextualize the added value of implicit, model-crafted pixel perturbations.Additional alignment literature (e.g., off-/on-manifold robustness equivalence and PAG mechanisms) could be more tightly integrated into the narrative motivating why logits and gradients should help human learning.\n\n- Enhancement via logit maximization can homogenize class appearances and potentially bias learners toward a subset of within-class features; risks of dataset-induced biases are acknowledged but not systematically quantified.\n- The number of classes in the learning tasks is small (2‚Äì4), leaving open whether gains persist in higher-cardinality, more ecologically complex settings.\n- The enhancement objective sometimes includes minimizing non-target logits (Œ±=1); this could suppress shared features in multi-class settings in non-obvious ways and might introduce artifacts.\n\n- The ‚Äústate-of-the-art‚Äù difficulty prediction claim relies on an AUC comparison to prior methods reported in the appendix; a main-text quantitative comparison (with identical data splits) would strengthen the claim.\n- The ablation finding that DS (shuffled) ‚â• DS suggests that ordering may be less critical than distributional easing, but the paper does not fully analyze why, nor quantify the respective contributions using a unified model (e.g., mixed-effects modeling of trial difficulty, order, and enhancement).\n- No explicit controls to rule out class-specific ‚Äúwatermarking‚Äù or low-level artifact learning (e.g., matched-norm sham perturbations that preserve power spectrum but are label-agnostic).\n- Difficulty-prediction performance is shown on the 16-way animal task; analogous predictive evaluation is not reported for the dermoscopy and histology datasets, despite using logits to schedule curricula there.\n\n- Some statistical details are brief (e.g., effect sizes in absolute terms, multiple comparison corrections, power analyses); reliance on ‚Äúrelative to chance margin‚Äù can overstate perceived improvements.\n- Limited detail in the main text about PGD settings, clipping, color management, and display timing fidelity (web-based 17 ms exposures) leaves potential reproducibility or interpretability questions.\n\n- A direct empirical comparison to educational baselines that highlight where to look (e.g., EXPLAIN, CAM/bounding-box overlays) would contextualize the added value of implicit, model-crafted pixel perturbations.\n- Additional alignment literature (e.g., off-/on-manifold robustness equivalence and PAG mechanisms) could be more tightly integrated into the narrative motivating why logits and gradients should help human learning.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe use of adversarially trained ResNet-50 models is well-motivated given known perceptual alignment of gradients; the optimization in pixel space with L2-constrained PGD is standard and appropriate for controlled perturbations.The dual objective with Œ±=1 (max ground-truth, min others) is reasonable for fine-grained tasks but could over-suppress shared or contextual features; an ablation on Œ± would be informative.The logistic model predicting correctness from Lgt with cross-validated AUC=0.72 is encouraging; adding calibration curves and per-class random effects (mixed models) could validate robustness across categories.\n- Experimental evaluation assessmentThe 16-way basic animal experiment provides a clear human-behavioral validation for both the difficulty metric and enhancement efficacy. The progressive accuracy increase with Œµ and saturation is plausible; the negative results for CLAHE/MSRCR/Lightroom are a strong control.In learning experiments, reporting absolute accuracy gains (not only relative to chance margin) and standardized effect sizes (Cohen‚Äôs d, odds ratios) would clarify practical impact. Confusion matrices and per-class breakdowns (Fig. 4B is a step in this direction) are welcome; full matrices in the appendix would help.The reduced training duration is interesting; however, since trial counts are fixed, time gains may reflect faster decisions. Report median RTs by correctness and condition, and whether instructions emphasized speed; consider speed‚Äìaccuracy trade-off analyses.The ablations are useful but still leave open questions: DS (shuffled) outperforming DS suggests that the distributional ‚Äúeasing‚Äù dominates ordering. A factorial analysis (enhancement yes/no √ó selection yes/no √ó ordering structured/shuffled) with mixed-effects models on trial-level accuracy would help quantify interactions and control for inter-participant variability.\n- Comparison with related work (using the summaries provided)The paper extends the stream of work showing that adversarial robustness yields perceptually aligned gradients (e.g., 2207.11378, 2305.19101) by demonstrating direct benefits to human learning, which is novel compared to prior uses in interpretability or synthesis.Relative to EXPLAIN (Mac Aodha et al., 2018) and teaching-set approaches (Singla et al., Johns et al.), L-WISE‚Äôs implicit highlighting complements explicit region guidance; a side-by-side human study would strengthen claims about relative efficacy and learner experience.The result that off-the-shelf enhancement methods fail aligns with evidence that generic low-level changes do not map to human-aligned, category-relevant gradients (and with 2204.01099/2306.16805 showing the special role of robust/PAG models).\n- Discussion of broader impact and significanceThe clinical-domain demonstrations are promising but amplify concerns about bias and homogenization; the ethics section appropriately acknowledges skin-tone representation and class-mode collapse risks. Before deployment, bias audits and stratified analyses (e.g., skin tone, imaging devices) are essential.The framework could generalize beyond images (e.g., pathology whole-slide tiles, radiology slices) and beyond classification (e.g., localization), provided robust models with aligned gradients exist; discussing how to extend L-WISE to structured outputs would be valuable.Personalization is a natural next step: adapting difficulty and enhancement to individual learning curves (e.g., via response-time-based adaptation, as per Mettler & Kellman) may yield larger gains and fairer outcomes.\n\n- The use of adversarially trained ResNet-50 models is well-motivated given known perceptual alignment of gradients; the optimization in pixel space with L2-constrained PGD is standard and appropriate for controlled perturbations.\n- The dual objective with Œ±=1 (max ground-truth, min others) is reasonable for fine-grained tasks but could over-suppress shared or contextual features; an ablation on Œ± would be informative.\n- The logistic model predicting correctness from Lgt with cross-validated AUC=0.72 is encouraging; adding calibration curves and per-class random effects (mixed models) could validate robustness across categories.\n\n- The 16-way basic animal experiment provides a clear human-behavioral validation for both the difficulty metric and enhancement efficacy. The progressive accuracy increase with Œµ and saturation is plausible; the negative results for CLAHE/MSRCR/Lightroom are a strong control.\n- In learning experiments, reporting absolute accuracy gains (not only relative to chance margin) and standardized effect sizes (Cohen‚Äôs d, odds ratios) would clarify practical impact. Confusion matrices and per-class breakdowns (Fig. 4B is a step in this direction) are welcome; full matrices in the appendix would help.\n- The reduced training duration is interesting; however, since trial counts are fixed, time gains may reflect faster decisions. Report median RTs by correctness and condition, and whether instructions emphasized speed; consider speed‚Äìaccuracy trade-off analyses.\n- The ablations are useful but still leave open questions: DS (shuffled) outperforming DS suggests that the distributional ‚Äúeasing‚Äù dominates ordering. A factorial analysis (enhancement yes/no √ó selection yes/no √ó ordering structured/shuffled) with mixed-effects models on trial-level accuracy would help quantify interactions and control for inter-participant variability.\n\n- The paper extends the stream of work showing that adversarial robustness yields perceptually aligned gradients (e.g., 2207.11378, 2305.19101) by demonstrating direct benefits to human learning, which is novel compared to prior uses in interpretability or synthesis.\n- Relative to EXPLAIN (Mac Aodha et al., 2018) and teaching-set approaches (Singla et al., Johns et al.), L-WISE‚Äôs implicit highlighting complements explicit region guidance; a side-by-side human study would strengthen claims about relative efficacy and learner experience.\n- The result that off-the-shelf enhancement methods fail aligns with evidence that generic low-level changes do not map to human-aligned, category-relevant gradients (and with 2204.01099/2306.16805 showing the special role of robust/PAG models).\n\n- The clinical-domain demonstrations are promising but amplify concerns about bias and homogenization; the ethics section appropriately acknowledges skin-tone representation and class-mode collapse risks. Before deployment, bias audits and stratified analyses (e.g., skin tone, imaging devices) are essential.\n- The framework could generalize beyond images (e.g., pathology whole-slide tiles, radiology slices) and beyond classification (e.g., localization), provided robust models with aligned gradients exist; discussing how to extend L-WISE to structured outputs would be valuable.\n- Personalization is a natural next step: adapting difficulty and enhancement to individual learning curves (e.g., via response-time-based adaptation, as per Mettler & Kellman) may yield larger gains and fairer outcomes.\n\n### Questions for Authors\n\n- Difficulty prediction beyond ImageNet animals: Did you evaluate the Lgt-vs-human correctness relationship (AUC/calibration) on the dermoscopy and histology tasks as well? If not, can you report these to justify the use of logit-based schedules in those domains?\n- Artifact controls: Did you try label-agnostic or class-mismatched perturbations with matched L2 norms during training to check whether any ‚Äúwatermark-like‚Äù artifacts drive learning? Similarly, did you test enhancement using non-robust models (or robust models trained on a different dataset) to probe specificity?\n- Enhancement objective: How sensitive are results to the Œ± parameter in Eq. (1) and to PGD hyperparameters (step size/steps, initialization, clipping, color gamut)? Any qualitative differences between Œ±=0 vs Œ±=1 in terms of homogenization?\n- Curriculum ordering: Given DS (shuffled) ‚â• DS, can you provide a trial-level mixed-effects analysis to disentangle the effects of ordering from distributional easing and enhancement magnitude? Do you observe interactions between difficulty and enhancement?\n- Reaction times and speed‚Äìaccuracy: Can you report RT distributions by condition and correctness, and analyze whether reduced training duration reflects a beneficial shift (faster and more accurate) rather than a speed‚Äìaccuracy trade-off?\n- Generalization to higher cardinality: Have you piloted L-WISE on tasks with more classes (e.g., 8‚Äì16-way fine-grained), and do gains scale? Any evidence of diminishing returns with increasing class count?\n- Baselines with explicit guidance: How would L-WISE compare to CAM/bounding-box highlighting (EXPLAIN-style) or hybrid methods (explicit where + implicit what)? Could you include such a baseline in a follow-up?\n- Individualization: Did you attempt adaptive schedules based on ongoing learner performance (accuracy/RT)? If not, what obstacles did you face, and do you expect larger effects with closed-loop adaptation?\n- Cross-model robustness: If enhancement is generated by one robust model and difficulty is scored by another robust model (or a robust ViT), do you retain learning benefits and difficulty prediction accuracy?\n- Statistical reporting: Can you provide absolute accuracy improvements, standardized effect sizes, and multiple-comparison adjustments for the main hypothesis tests?\n\n### Overall Assessment\n\nThis is a timely and creative paper that moves beyond model‚Äìhuman alignment metrics to demonstrate a tangible application: using robustified ANNs to accelerate and improve human visual category learning. The methodology is conceptually sound, the experiments are well-motivated and diversified across domains, and the inclusion of ablations adds credibility. While several aspects would benefit from deeper analysis‚Äîparticularly artifact controls, comprehensive difficulty-prediction validation in clinical tasks, refined statistics, and stronger baselines with explicit visual guidance‚Äîthe work presents a compelling proof-of-concept with clear scientific and practical significance. With additional controls and expanded comparisons, this paper has strong potential for publication at a top-tier venue and for catalyzing follow-up research on personalized, model-informed curricula for human learning.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 2,
          "constructiveness": 3,
          "stance": 2,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **L‚ÄëWISE**, a model‚Äëbased curriculum framework for human visual category learning. It leverages robustified neural networks to (i) predict human image difficulty via ground‚Äëtruth logits and (ii) generate pixel‚Äëspace enhancements that amplify category‚Äërelevant features, with modifications tapered over training. Tested across moth species, dermoscopy, and histology domains, L‚ÄëWISE reportedly improves human test accuracy by 33‚Äì72% (relative to chance margin) and reduces training time by roughly 20%. The paper is clearly written, methodologically detailed, and positions its contributions within both machine‚Äëlearning and cognitive‚Äëtraining research.  \n\n**Major Comments**  \n1. **Technical limitations.** Enhancement via logit maximization may homogenize class appearances and bias learners toward a limited feature subset; dataset‚Äëinduced biases are acknowledged but not quantified. The small number of training classes (2‚Äì4) limits generalizability, and minimizing non‚Äëtarget logits (Œ±‚ÄØ=‚ÄØ1) could inadvertently suppress shared features in multi‚Äëclass settings.  \n2. **Evaluation gaps.** The claim of state‚Äëof‚Äëthe‚Äëart difficulty prediction relies on appendix‚Äëlevel comparisons; direct quantitative main‚Äëtext results would strengthen it. The ablation showing DS(shuffled)‚ÄØ‚â•‚ÄØDS raises unresolved questions about the relative roles of ordering versus distributional easing. Controls for class‚Äëspecific artifacts or watermarking and predictive‚Äëmodel validation on dermoscopy and histology data are missing.  \n3. **Statistical and methodological detail.** More explicit reporting of absolute accuracy gains, standardized effect sizes, and multiple‚Äëcomparison corrections would clarify effect magnitude. Insufficient detail about PGD parameters, color management, and display timing may hinder reproducibility.  \n4. **Related work and baselines.** Direct human‚Äëstudy comparisons with educational or region‚Äëhighlighting baselines (e.g., EXPLAIN) and greater integration with alignment literature (robustness vs. perceptual alignment) would provide stronger context.  \n5. **Experimental analysis.** The reviewer suggests additional ablations on Œ±, factorial designs to separate enhancement, selection, and ordering effects, and inclusion of calibration curves or mixed‚Äëeffects models to assess robustness of the difficulty predictor. Reaction‚Äëtime analyses could test potential speed‚Äìaccuracy trade‚Äëoffs.  \n\n**Minor Comments**  \n- Figures and curriculum schematics are clear; ensuring full confusion matrices and PGD hyperparameter details in the appendix would aid reproducibility.  \n- Clarify terminology such as ‚Äúrelative to chance margin.‚Äù  \n- Confirm consistency of statistical testing, AUC computation, and domain‚Äëspecific evaluation.  \n- Minor editorial issues: ensure uniform notation for Œµ and Œ± parameters, and define all acronyms at first use.  \n\n**Summary Paragraph**  \nOverall, this is a technically sound and innovative study that demonstrates how robustified networks can guide **human learning** rather than merely interpret it. The approach is conceptually strong, experimentally diversified, and complemented by thoughtful discussion of ethics and limitations. Main weaknesses lie in incomplete statistical transparency, limited artifact controls, and a need for more direct comparative baselines. Addressing these analytic and reporting issues would substantively reinforce the paper‚Äôs claims and broaden its applicability.  \n\n**Decision Recommendation**  \n**Major revision.** The work is promising and potentially impactful but requires stronger empirical validation, expanded analyses, and fuller reporting before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for IEEE Transactions on Medical Imaging\n\n## 1. Opening Paragraph\n\nThe paper addresses the critical challenge of improving human visual learning for complex image categorization tasks, particularly in medical imaging domains where experts require extensive training to achieve proficiency. The authors propose L-WISE (Logit-Weighted Image Selection and Enhancement), a novel approach that leverages robustified artificial neural networks (ANNs) to both predict image recognition difficulty and generate targeted image perturbations that enhance category perception for human learners. The method combines two key components: (1) selecting images based on model-predicted recognition difficulty to create an adaptive curriculum that begins with \"easier\" images and gradually increases difficulty, and (2) applying pixel-level perturbations that maximize ground truth logit activation to make images more discriminable. The authors demonstrate that L-WISE significantly improves test-time accuracy (33-72% relative gains) and reduces training time (20-23%) across three diverse image domains: fine-grained moth classification, dermoscopy skin lesion classification, and histology image classification. Notably, the most substantial improvements were observed in the clinically relevant dermoscopy task, suggesting practical value for medical education.\n\n## 2. Major and Minor Comments\n\n### Major Comments:\n\n**Strengths:**\n- The work represents a significant conceptual advance by applying robustified ANNs not just to improve model performance but to directly enhance human visual learning‚Äîa novel direction in the field.\n- The experimental validation across three distinct domains, including two clinically relevant medical imaging tasks, provides strong evidence of the approach's generalizability.\n- The comprehensive ablation study (Table 1) clearly demonstrates the relative contributions of image enhancement versus difficulty-based selection, showing both components provide meaningful benefits.\n- The human-subject experimental design is rigorous, with appropriate control conditions, sufficient statistical power, and thoughtful attention to potential confounding variables (e.g., randomized category naming).\n\n**Limitations:**\n- The paper insufficiently addresses potential safety concerns when applying this to medical diagnostics, particularly the risk that enhanced images might teach learners to focus on features that aren't clinically relevant or that enhancement might create \"hallucinations\" that could lead to misdiagnosis in real-world settings.\n- While the authors acknowledge the homogenizing effect of logit maximization (Section 5.1), they don't provide concrete mitigation strategies to prevent the loss of important heterogeneity in medical image features that might be clinically significant.\n- The clinical applicability is currently limited as the approach was only tested with non-clinicians in controlled settings; testing with actual medical trainees would strengthen the medical relevance.\n- The mechanism behind why image enhancement works isn't thoroughly explored‚Äîdoes it highlight real diagnostic features or create artificial ones? The discussion of \"hallucinations\" (Appendix S10) acknowledges this concern but doesn't provide sufficient investigation.\n\n### Minor Comments:\n\n**Strengths:**\n- The visualizations are exceptionally clear and effectively demonstrate the method's effects across multiple domains.\n- The paper is well-structured with a logical flow from problem statement to methodology to results.\n- The comparison with off-the-shelf enhancement methods (Figure 1B) provides compelling evidence that conventional approaches don't yield similar benefits.\n- The ethical considerations section thoughtfully addresses potential risks, particularly regarding bias in medical applications.\n\n**Limitations:**\n- More detailed explanation of how the \"shuffled\" conditions in Table 1 were implemented would improve reproducibility.\n- Some figures (e.g., Figure S12) could benefit from more detailed captions explaining the differences between the pilot and main experiments.\n- The relationship between enhancement magnitude (Œµ) and accuracy could be explored more systematically across different image difficulty levels.\n\n## 3. Evaluation Along TMI Editorial Criteria\n\n**Significance:** The work has high significance for medical imaging as it addresses the critical need to improve training efficiency for medical image interpretation. The demonstrated improvements in learning speed and accuracy could translate to substantial time savings in medical education while potentially improving diagnostic accuracy. The successful application to dermoscopy and histology‚Äîtwo domains where visual expertise is essential but difficult to acquire‚Äîmakes this particularly relevant to the TMI audience.\n\n**Innovation:** The approach is highly innovative in applying robustified ANNs to enhance human learning rather than just model performance. While individual components have been studied previously, their integration into a learning curriculum for humans represents a novel conceptual advance with strong practical implications. The work bridges computational neuroscience, machine learning, and medical education in a meaningful way.\n\n**Evaluation:** The evaluation is thorough and well-designed. The authors conducted rigorous human-subject experiments with appropriate controls, statistical analysis, and sufficient power across multiple domains. The ablation studies help isolate the contributions of different components, and the comparison with conventional image enhancement methods strengthens the claims. However, evaluation with actual medical trainees rather than general participants would have strengthened the clinical relevance.\n\n**Reproducibility:** The paper provides extensive methodological details in both the main text and appendices. The reproducibility statement indicates comprehensive sharing of code and experimental frameworks, which should facilitate replication. The detailed description of the enhancement algorithm (Appendix S1) and the inclusion of all hyperparameters for training robustified networks are particularly helpful for reproduction.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThe paper makes a valuable contribution to the field of medical image analysis by demonstrating a novel application of robustified ANNs to improve human learning. The methodology is sound, the evaluation is thorough, and the potential clinical applications are promising. However, I recommend minor revisions to address the safety concerns more thoroughly and to provide additional insight into the mechanism by which image enhancement aids learning. Specifically, the authors should:\n1) Expand the discussion of clinical safety considerations with concrete suggestions for mitigating potential risks\n2) Provide more detailed analysis of whether enhancements highlight real diagnostic features or create artificial ones\n3) Clarify the implementation details of the \"shuffled\" conditions in Table 1\n\nThese revisions would strengthen the paper's clinical relevance and ensure responsible development of this promising approach. With these modifications, this work would be an excellent addition to TMI.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 3,
          "specificity": 2,
          "correctness": 2,
          "constructiveness": 2,
          "stance": 2,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces L-WISE (Logit-Weighted Image Selection and Enhancement), a method designed to improve human visual learning in complex image categorization tasks such as dermoscopy and histology. L-WISE leverages robustified artificial neural networks to predict image difficulty and apply logit-based perturbations that make visual patterns more discriminable for learners. The proposed adaptive training framework demonstrates significant improvements in human test accuracy and reduced training time across multiple domains. The paper is well-written, logically organized, and presents a clear narrative from motivation to results, with high potential relevance to medical education.\n\n---\n\n**Major Comments**  \n\n1. **Conceptual Contribution and Evaluation Strengths**  \n   - The work offers a meaningful conceptual advance by applying robustified ANNs to enhance human learning rather than model accuracy.  \n   - Validation across three domains, two with direct clinical relevance, supports generalizability.  \n   - Ablation studies effectively isolate contributions of image enhancement and difficulty-based selection.  \n   - The human-subject experiments are well-controlled, statistically powered, and consider potential confounders such as randomized category naming.\n\n2. **Limitations and Areas for Improvement**  \n   - Clinical safety concerns need further discussion. Image perturbations could potentially emphasize features irrelevant or misleading for diagnostics, raising risks of misinterpretation.  \n   - The acknowledged homogenizing effect of logit maximization lacks proposed mitigation strategies to preserve clinically important heterogeneity.  \n   - Experiments were conducted with non-clinicians; validation with medical trainees would improve applicability.  \n   - The mechanism underlying why enhancement improves recognition remains unclear‚Äîwhether it accentuates meaningful features or introduces artifacts should be more thoroughly examined.\n\n---\n\n**Minor Comments**  \n\n- Visualizations are clear and effectively illustrate the method‚Äôs effects.  \n- Figures S12 and 1B could benefit from expanded captions clarifying experimental differences.  \n- The description of \"shuffled\" conditions in Table 1 requires more detail for reproducibility.  \n- The relationship between enhancement magnitude (Œµ) and accuracy across difficulty levels could be analyzed more systematically.  \n- Ethical considerations are thoughtfully addressed, particularly regarding fairness and bias.\n\n---\n\n**Summary Paragraph**  \nOverall, the submission is significant, innovative, and experimentally rigorous. It contributes a new perspective on using robustified neural networks to enhance human perceptual learning in medical imaging. Strengths include clear methodology, thorough human evaluation, and meaningful application to clinically relevant domains. The primary weaknesses concern unresolved questions about clinical safety, generalization to medical learners, and the theoretical basis for enhancement effectiveness. Addressing these points will enhance the manuscript‚Äôs translational value and reliability.\n\n---\n\n**Decision Recommendation**  \n**Minor Revision** ‚Äî The paper is strong and nearly ready for publication. The authors should expand discussion of safety considerations, clarify whether enhancements amplify genuine diagnostic cues or artificial patterns, and detail implementation aspects of the ‚Äúshuffled‚Äù conditions to ensure reproducibility and clinical robustness.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe authors examine whether robustly trained artificial neural networks (ANNs) that emulate ventral‚Äëstream processing can be leveraged to accelerate human visual category learning. Participants classified images in three medically relevant domains‚Äîfine‚Äëgrained moth species, dermoscopic skin lesions, and colon histology. The proposed L‚ÄëWISE curriculum combines (i) a difficulty estimate derived from the ground‚Äëtruth logit of a robustified ANN and (ii) pixel‚Äëlevel perturbations that maximize that logit, thereby amplifying task‚Äërelevant features. Across the three tasks, L‚ÄëWISE purportedly raises test accuracy by 33‚Äì72‚ÄØ% and cuts training time by 20‚Äì23‚ÄØ% while using an identical number of trials. The central claim is that a model‚Äëdriven curriculum can predict image difficulty and generate perceptual enhancements that speed human visual learning.\n\n---\n\n## General feedback  \n\n- **Significance:** The manuscript tackles an under‚Äëexplored junction between adversarially trained vision models and human learning, a topic that could bear on medical education and clinician training.  \n- **Innovation:** Melding a logit‚Äëbased difficulty estimator with model‚Äëdriven image enhancement is an original idea, even though curriculum learning and adversarial perturbations have each been investigated separately.  \n- **Evaluation:** The human experiments are extensive and report statistically significant effects (œá¬≤, *p*‚ÄØ<‚ÄØ0.001). Gains are displayed for all three domains (Fig.‚ÄØ4A) and reductions in training time are quantified (Table‚ÄØ1). Nevertheless, the authors compare L‚ÄëWISE only against a na√Øve control. Established curriculum‚Äëlearning schemes or saliency‚Äëhighlighting approaches are absent as baselines, and the paper provides no power analysis or effect‚Äësize metrics beyond confidence intervals.  \n- **Reproducibility:** The authors state that code and hyper‚Äëparameters will be released and include a reproducibility statement. Yet the main text omits crucial details about the adversarial training (e.g., attack type, Œµ budget schedule per task) and the participant randomisation procedure, which limits the ability of an independent group to replicate the study fully.\n\n---\n\n## Specific comments/critiques  \n\n- **Normalization of the difficulty metric:** The ground‚Äëtruth logit is employed as a difficulty score (Fig.‚ÄØ1A), but the manuscript does not clarify how logits are normalized across classes or datasets. Without this information it is unclear whether difficulty scores are comparable between tasks.  \n- **Baseline image‚Äëenhancement comparison:** The authors claim that conventional methods (CLAHE, MSRCR, Lightroom ‚ÄúAuto‚Äù) are ineffective (Fig.‚ÄØ1B) but present no quantitative effect sizes or statistical testing. Moreover, the parameter settings used for these baselines are not described, raising doubts about the fairness of the comparison.  \n- **Justification of the curriculum schedule:** The linear increase in maximal difficulty and the exponential decay of Œµ (Fig.‚ÄØ2E‚ÄëF) appear to be chosen heuristically. The manuscript lacks any hyper‚Äëparameter sweep or sensitivity analysis, leaving open the possibility that the reported gains depend on these particular schedules.  \n- **Statistical rigor of ablations:** Table‚ÄØ1 lists mean accuracies and training durations with 95‚ÄØ% confidence intervals, yet it does not provide pairwise significance tests (e.g., post‚Äëhoc *p*‚Äëvalues) for each ablated condition versus the control. This omission weakens the claim of a ‚Äúsignificant additive benefit.‚Äù  \n- **Bias analysis in the clinical domains:** Section‚ÄØ5.1 mentions a potential skin‚Äëtone bias in the dermoscopy task, but the authors do not empirically examine whether the logit‚Äëmaximizing perturbations mitigate or exacerbate class imbalances (see Fig.‚ÄØS12H). Such an analysis is essential before considering clinical deployment.  \n- **Participant demographics and attention checks:** The ethics statement refers to a demographic breakdown (Table‚ÄØS2) that is not included in the manuscript. Without these data it is difficult to assess sample diversity, prior experience with medical images, or the effectiveness of attention‚Äëcheck procedures.  \n- **Generalization to novel categories:** Test images are unmodified but drawn from the same set of classes used during training. The study does not explore whether L‚ÄëWISE improves transfer to unseen categories or out‚Äëof‚Äëdistribution samples, limiting the scope of the authors‚Äô generalizability claim.  \n- **Comparison with existing difficulty metrics:** Figure‚ÄØ1A shows that the logit‚Äëbased estimator outperforms the metric of Mayo et‚ÄØal.‚ÄØ(2023), yet no statistical test (e.g., DeLong comparison) is reported, and other plausible baselines are omitted. Consequently, the improvement remains only marginally substantiated.\n\n---\n\n## A suggested decision  \n\n**Major Revision** ‚Äì The manuscript currently lacks essential baseline comparisons, thorough statistical analyses, and detailed methodological information, which together prevent a reliable assessment of the claimed improvements. Addressing these gaps is necessary before the work can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 2,
          "specificity": 2,
          "correctness": 2,
          "constructiveness": 2,
          "stance": 2,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether robustly trained artificial neural networks (ANNs) that approximate ventral‚Äëstream processing can accelerate human visual category learning. The proposed L‚ÄëWISE curriculum combines a logit‚Äëbased image‚Äëdifficulty measure with pixel‚Äëlevel perturbations intended to emphasize task‚Äërelevant features. Human participants learned to classify medical images (moth species, dermoscopic lesions, and colon histology), and the authors report notable improvements in accuracy and reduced training time compared with a na√Øve control. The paper addresses a promising and under‚Äëstudied intersection of adversarially trained vision models and human perceptual learning, though several aspects of the evaluation and reporting warrant further clarification.  \n\n**Major Comments**  \n1. **Baseline Comparisons:** L‚ÄëWISE is evaluated only against a na√Øve control. The absence of established curriculum‚Äëlearning or saliency‚Äëbased baselines limits the strength of the comparative claims.  \n2. **Difficulty Metric Normalization:** The use of the ground‚Äëtruth logit as a difficulty score is insufficiently explained‚Äîspecifically, whether logits are normalized across classes or tasks remains unclear.  \n3. **Curriculum Schedule Justification:** The linear and exponential schedules for difficulty and perturbation magnitude appear heuristically chosen. No sensitivity analysis or hyperparameter sweep is provided to determine robustness.  \n4. **Statistical Rigor:** Reported confidence intervals are informative yet lack corresponding pairwise significance or post‚Äëhoc tests across ablations. The assertion of a significant additive benefit therefore remains weakly supported.  \n5. **Experimental and Demographic Transparency:** Details about adversarial‚Äëtraining settings (attack type, Œµ schedule) and participant randomization are missing, impeding reproducibility. Likewise, demographic information and attention‚Äëcheck data referenced in supplements are not available.  \n6. **Bias and Generalizability Analyses:** Potential biases, such as skin‚Äëtone effects in the dermoscopy task, are acknowledged but untested. Additionally, generalization to unseen or out‚Äëof‚Äëdistribution categories is not examined.  \n7. **Comparison with Other Difficulty Metrics:** The superiority of the proposed metric over prior methods is asserted without accompanying statistical tests, leaving this comparison inconclusive.  \n\n**Minor Comments**  \n- Clarify parameter settings for conventional image‚Äëenhancement baselines and include corresponding effect sizes or significance tests.  \n- Ensure all referenced tables and supplemental materials (e.g., demographic data) are provided.  \n- Define acronyms and normalize notation where appropriate for clarity across figures and sections.  \n\n**Summary Paragraph**  \nOverall, the study presents an innovative model‚Äëdriven approach to facilitating human visual learning, supported by extensive experimental effort. However, the lack of established baselines, incomplete statistical analyses, limited methodological transparency, and unaddressed bias or generalization questions reduce confidence in the reported improvements. Strengthening these elements would substantially enhance the work‚Äôs credibility and reproducibility.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Gabriel Kreiman",
      "Guy Gaziv",
      "James J. DiCarlo",
      "Morgan Bruce Talbot"
    ],
    "url": "pdfs/iclr.cc-2025-conference_f92d03707a21b34104857495b389885b58b8ecc6.pdf",
    "remote_url": "https://openreview.net/pdf/f92d03707a21b34104857495b389885b58b8ecc6.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Diffusion based models",
      "Self-supervised MRI denoising"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI), including diffusion MRI (dMRI), serves as a ``microscope'' for anatomical structures and routinely mitigates the influence of low signal-to-noise ratio scans by compromising temporal or spatial resolution. However, these compromises fail to meet clinical demands for both efficiency and precision. Consequently, denoising is a vital preprocessing step, particularly for dMRI, where clean data is unavailable. In this paper, we introduce Di-Fusion, a fully self-supervised denoising method that leverages the latter diffusion steps and an adaptive sampling process. Unlike previous approaches, our single-stage framework achieves efficient and stable training without extra noise model training and offers adaptive and controllable results in the sampling process. Our thorough experiments on real and simulated data demonstrate that Di-Fusion achieves state-of-the-art performance in microstructure modeling, tractography tracking, and other downstream tasks. Code is available at https://github.com/FouierL/Di-Fusion.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a new self-supervised learning-based denoising method for diffusion MRI (dMRI). The proposed method leveraged the diffusion modeling concept, but instead of training a diffusion model with ‚Äúclean images‚Äù as x_0 and noise as x_T, it utilized two diffusion weighted images (DWIs) with different diffusion encodings at both ends of a ‚Äúdiffusion-like‚Äù process. A denoising network was trained by predicting one DWI using a linear combination of two DWIs and an added noise term. The linear combination coefficients are time-dependent and determined via a scheduling strategy similar to training a diffusion model. The network was then used for a conditional sampling step for generating the final denoised images. The idea to utilize images acquired with different diffusion encodings to denoise one of them is interesting and the training strategy is an interesting approach to leverage the diffusion modeling concept, especially with training only latter diffusion steps to reduce hallucinations. However, several key assumptions made are questionable and the overall methodology and presentation lacks clarity. Evaluation using only dMRI signal model goodness of fit is limited and can be biased. There are a few overstatements that can mislead the readers. Detailed comments can be found below.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nA diffusion-like modeling that learns the relationship between two DWI volumes with different diffusion encodings to denoise one or each other.\n\nTraining only later step diffusion to avoid hallucination\n\nA fusion strategy that exploits linear combination of two DWIs with different contrasts with time-dependent coefficients and iterative refinement.\n\nExtensive evaluations using both simulations that exactly followed the assumptions for the proposed methodology and practical magnitude DWI data.\n\n### Weaknesses\n\nThere are statements that can be misleading in the context of MR physics (aka domain knowledge). For example, \"the noise predominantly originates from physical interferences (Fadnavis et al., 2020a)\". This statement about physical interferences is  both vague and inaccurate. This work is dealing with thermal noise or noise resulting from thermal noise in the measurements, which is not really physical interferences depending on how ones interpret them. Another example, \"Different clinical applications require varying numbers of diffusion vectors and acquisition strategies, which makes modeling the noise distribution and further implementing denoising techniques challenging\". Acquiring DWIs with varying numbers of diffusion vectors had nothing to do with the difficulty of  modeling noise distribution.\n\nMany key assumptions for the proposed method was built on do not hold which made the theoretical/mathematical foundations questionable, e.g.,\na) It seems that the authors assumed DWIs acquired with different diffusion encodings had the same underlying ‚Äúclean‚Äù image and were corrupted by independent noise. This is inaccurate. In fact, two DWIs can have rather different contrasts due to the diffusion encoding effects, e.g., different diffusion encoding directions. More specifically, x and x‚Äô cannot be simply modeled as the same y plus different noise. What are the implications of this assumption not met?\n\nb) Line 111: The authors claimed that that the proposed method does not require an explicit noise model. This is an overstatement. The J-invariance assumption, which formed the basis of the training objective in Eq. (9) implicitly requires that the noise distribution be zero-means and conditionally independent. Furthermore, additive noise model was assumed, x = y + n1 (Line 200). In dMRI, the magnitude images with higher b-values (stronger diffusion weightings) can have lower SNR for which additive noise may not hold. These need to be clarified.\n\n-  Overall, the presentation lacks clarity and there seem to be some concerning inaccuracies.\na) The linear combination relationship claimed in Section 3.1 does not seem accurate. I checked the derivation. Eq. 31 is correct which is known (so this is not a contribution of the authors), but I'm not sure about going from Eq. 31 to 32 as F_theta predicts x_0, but they are not equal, and there is also an additional term of sigma_t^2*z. Therefore, I don't think it's a correct statement to say x_(t-1) is a linear interpolation between x_out and x_t. But is this really needed for the proposed method? I really don‚Äôt see a connection between what‚Äôs argued theoretically and what‚Äôs actually being implemented.\n\nb) There are a few other inaccurate mathematical statements and notations which are confusing. For example, Eq. 7, the left side has q(x1:T |xt*) which is a joint distribution for x1 to xT, and the right side is a Gaussian distribution for xt. \nOn Line 160: {xt}1:T was described as‚Äùobtained from the reverse process.‚Äù However, in\nFigure 1 and on the right side of Equation (7) on Line 186, it appears that xt is a corrupted version of xt*.  This interpretation, along with the notation in the Fig. 1, implies that {xt}1:T would represent a forward process. It appears to this reviewer the authors had not been using a consistent definition of forward and reverse diffusion which made the overall description rather confusing. These are just examples of inconsistencies found.\n\nc) According to the J-invariance property, the noise should ideally have zero mean\nand be conditionally independent of the target output. This requirement is necessary to ensure that the expected loss for self-supervised training asymptotical approaching the supervised loss. However, the input to F(.) in Eq. (9) includes xt*, which is a linear combination of x and x‚Äô (Eq. (6)). Given that x serves as the supervision signal for the loss, this implies a correlation between the input x‚àót and the target x, which would violate the conditional independence requirement for J-invariance.\n\n### Questions\n\nIn Eq. (5) on Line 155, the authors highlighted a specific term as the ‚Äùmajor difference‚Äù between xt‚àí1 and x^bar_t‚àí1. Could the authors clarify why this particular term is considered the primary source of difference? Furthermore, can the authors elaborate on the underlying reason(s) for the ‚Äúdrift‚Äù in the model and how it emerges during the reverse diffusion process?\n\nAccording to the definition of the Fusion process in Eq. (6)  and the ‚Äúforward process‚Äù in Eq. (7), it appears that the starting point for the forward process changes based on t, as x_t* is dependent on t. This dependence implies that the Fusion process dynamically adjusts the starting point of the forward process at each step, which is unconventional compared to typical diffusion models. Could the authors clarify the rationale behind this design?\n\nOther more recent self-supervised denoising methods should be compared, if not for all, e.g., Noise2Score and Recorrupted2Recorrupted etc.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a self-supervised denoising framework for diffusion MRI (dMRI) that leverages concepts from diffusion probabilistic models. Instead of treating clean/noisy image pairs as two endpoints of a diffusion process, the authors formulate a ‚Äúdiffusion-like‚Äù process connecting two diffusion-weighted images (DWIs) with different diffusion encodings. The model predicts one DWI from a time-dependent linear combination of two DWIs and noise, and uses the trained network for conditional sampling to reconstruct denoised images. The approach is conceptually interesting and offers a creative adaptation of diffusion modeling to MR denoising. However, key assumptions are questionable, and both theoretical formulation and presentation lack rigor and clarity. Evaluation is based mainly on model fitting metrics, which limits empirical validation and may bias conclusions.  \n\n**Major Comments**  \n1. **Questionable Assumptions**: The method assumes that DWIs with different diffusion encodings share the same underlying clean image, differing only by independent noise. This assumption does not hold in practice, as diffusion contrast varies substantially across gradient directions and b-values. The consequences of violating this assumption are not discussed.  \n2. **Implicit Noise Modeling**: Although the authors claim the method does not require an explicit noise model, the J-invariance assumption and additive noise formulation effectively impose constraints on noise properties (zero-mean, conditional independence). These should be explicitly acknowledged and justified, especially since additive noise may not hold for magnitude MR data with low SNR.  \n3. **Mathematical and Conceptual Inconsistencies**: Several derivations and notations are inaccurate or unclear. For example, the relationship between Equations (31) and (32) and the interpretation of \\( x_{t-1} \\) as a linear interpolation appear incorrect. Equation (7) conflates a joint and a marginal distribution. The inconsistent labeling of ‚Äúforward‚Äù versus ‚Äúreverse‚Äù processes further confuses the formulation.  \n4. **Violation of J-invariance Conditions**: The training input \\( x_t^* \\), being a mixture of \\( x \\) and \\( x' \\), correlates with the target \\( x \\), contrary to the conditional independence required for valid self-supervised training.  \n5. **Limited and Potentially Biased Evaluation**: The assessment relies solely on goodness-of-fit measures under simulated conditions favorable to the model. Comparisons to more recent self-supervised methods (e.g., Noise2Score, Recorrupted2Recorrupted) are missing.  \n6. **Misleading Physical Statements**: Some descriptions related to MR physics are vague or inaccurate, such as characterizing MR noise as originating from ‚Äúphysical interferences.‚Äù Clarification and revision are needed.  \n\n**Minor Comments**  \n- Clarify why the highlighted term in Eq. (5) is described as the primary difference between \\( x_{t-1} \\) and \\( \\bar{x}_{t-1} \\), and explain the ‚Äúdrift‚Äù observed in the reverse process.  \n- Explain the rationale for redefining the forward process starting point within the Fusion process (Eq. 6‚Äì7).  \n- Ensure consistency in notation and variable definitions throughout figures and text.  \n- Several notational inaccuracies and ambiguities (e.g., in Eq. 7 and Fig. 1) should be corrected.  \n- Revise overly strong claims regarding model independence from noise assumptions.  \n\n**Summary Paragraph**  \nOverall, the paper introduces an innovative idea of using diffusion-like modeling to relate DWIs with different encodings for denoising, and the training strategy to limit hallucinations is interesting. However, the theoretical basis is weakened by invalid assumptions, inconsistent mathematical formulations, and inaccurate claims about noise modeling. The evaluation framework is narrow and lacks comparison to relevant baselines. Substantial clarification of theoretical justifications and presentation is required before the method‚Äôs validity can be properly assessed.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces Di-Fusion, a fully self-supervised diffusion MRI (dMRI) denoising method designed to enhance the signal-to-noise ratio (SNR) of MRI data without requiring clean reference data. The authors leverage novel late diffusion steps and an adaptive sampling process to create a single-stage framework that operates without an explicit noise model. Di-Fusion demonstrates superior performance over state-of-the-art denoising methods in tasks such as microstructure modeling and tractography. The method‚Äôs efficacy is validated through extensive quantitative and qualitative evaluations on real and simulated data.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- **Flexibility with data and noise models**: Instead of relying on explicit noise models or clean training data, the method relies on an N2N training strategy and pixel shuffling to reorganize the noise, providing strong generalization potential across different noise distributions. This suggests that the method has the potential to be applied to a wider range of denoising scenarios, such as cryo-EM.\n- Compared to the current state-of-the-art method, DDM^2, this approach demonstrates comprehensive improvements. Not only does it outperform in terms of performance, but it is also simpler to implement. Notably, this method does not require additional denoiser training, significantly enhancing its practical usability.\n- As a study on dMRI denoising, this paper conducts thorough and comprehensive experiments, including extensive comparisons and analyses on downstream task performance. This renders the work methodologically and experimentally well-rounded.\n\n### Weaknesses\n\nPlease refer to the **Questions** section for details.\n\n### Questions\n\n- To my understanding, the primary goal of dMRI denoising is to reduce the number of gradients required during acquisition, thus accelerating DWI scanning. In downstream tasks based on DTI, the authors compare DTI metrics computed from noisy images with those from denoised images. Why did the authors not use more DWI data to compute a clean DTI metric as a reference for comparison?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Di-Fusion*, a fully self-supervised diffusion MRI (dMRI) denoising framework aimed at enhancing signal-to-noise ratio (SNR) without the need for clean reference data or an explicit noise model. The approach introduces late diffusion steps and an adaptive sampling process to achieve single-stage denoising. The method is validated through extensive experiments on both simulated and real-world data, demonstrating improved quantitative and qualitative performance compared with existing techniques. Overall, the paper is clearly presented and methodologically solid.  \n\n**Major Comments**  \n1. The primary conceptual contribution lies in removing the dependence on clean data and explicit noise models via N2N training and pixel shuffling. This flexibility enhances generalization across varied noise distributions. The reviewer emphasizes the broad applicability of this framework, potentially extending to other imaging modalities such as cryo-EM.  \n2. When compared to the current state-of-the-art method (DDM¬≤), the proposed approach provides consistent improvements while simplifying implementation. Unlike DDM¬≤, it avoids additional denoiser training, which strengthens its practical usability.  \n3. The experimental design is thorough, encompassing extensive evaluations and downstream analyses in microstructure modeling and tractography. This breadth supports the claimed methodological soundness.  \n4. However, the reviewer raises a methodological concern: since one major motivation for dMRI denoising is to accelerate diffusion-weighted image (DWI) acquisition by reducing the number of gradient directions, the authors should clarify why they did not use more DWI data to compute a cleaner DTI metric as a reference in their downstream evaluations.  \n\n**Minor Comments**  \n- The paper is well organized and readable. There are no specific issues regarding figures, notation, or terminology noted.  \n- No ethical concerns were identified.  \n\n**Summary Paragraph**  \nThis paper provides a well-executed and practically valuable framework for self-supervised dMRI denoising. Its main strengths lie in methodological simplicity, flexibility across noise conditions, and rigorous experimental validation. The main area requiring clarification concerns the design choice in reference metric computation for downstream comparisons.  \n\n**Decision Recommendation**  \n**Accept (minor revision)** ‚Äì The work is strong and well supported, with only minor clarification requested.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes a method for denoising diffusion MRI data sets.\n\nThis is a well-studied problem with many solutions in the literature. It is an important problem, as diffusion MRI is widely used for neuroscience and for clinical medicine. Recent years have seen a trend towards using self-supervised approaches to characterise the noise distribution and separate noise from the underlying signal.  This submission falls very much in this category, but proposes a different algorithm to those that are popular in the literature.\n\nExperiments compare against five baselines and results appear competitive with other methods, sometimes surpassing them.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe algorithm appears novel, although I found it hard to tell from the literature review how novel it is - whether it takes ideas from other areas and repurposes them for this problem, or if this is an algorithm specifically designed for diffusion MRI.\n\nThe problem is an important one with widespread application.\n\nResults appear competitive on a few example images shown in the figures.\n\n### Weaknesses\n\nThe baselines chosen do not include the most widely used denoising methods.  A clear omission is the random-matrix theory approaches proposed by Veraart et al in a series of very highly cited papers starting with Neuroimage 2016.\n\nThe only quantitative results use simulations, which seem likely to be skewed towards to capabilities of the proposed algorithm.\n\nThe qualitative results on actual human data are questionable as to whether they show improvement over baselines.  Even if they do, these are single cherry-picked examples and it is not clear whether these are advantages that manifest over large collections of images/scenarios.\n\n### Questions\n\nCorresponding to weaknesses listed above.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a new algorithm for denoising diffusion MRI datasets, a problem of clear relevance for both neuroscience research and clinical imaging. The topic is well established, with numerous existing solutions, including recent self‚Äësupervised methods that model and separate noise from signal. The submission fits within this context while proposing a distinct algorithmic approach. The writing is generally clear, and results are shown against several baselines, indicating competitive performance in some cases.\n\n**Major Comments**  \n1. **Novelty and context:** While the algorithm appears to be novel, the literature review does not make the degree of originality clear. It is uncertain whether the method represents a repurposing of established ideas from other areas or a new approach developed specifically for diffusion MRI.  \n2. **Baseline selection:** The evaluation omits comparison with widely used denoising techniques, particularly the random‚Äëmatrix theory (RMT) approaches developed by Veraart et‚ÄØal. (Neuroimage 2016 and related works), which are considered standard in the field. Including these would strengthen the claim of competitiveness.  \n3. **Quantitative evaluation:** All quantitative results rely on simulations that may be biased toward the capabilities of the proposed method. Robust experimental design would require inclusion of real‚Äëdata quantitative analyses.  \n4. **Qualitative assessment:** Qualitative results on human data are limited to a few example images, whose improvements are uncertain. These appear selected and may not reflect generalizable advantages across larger datasets or varied imaging conditions.  \n\n**Minor Comments**  \n- The literature review could be expanded to position the work more clearly among existing self‚Äësupervised noise modeling approaches.  \n- Figures showing qualitative comparisons should be supplemented with quantitative metrics for real data if available.  \n\n**Summary Paragraph**  \nOverall, the paper addresses an important and timely denoising problem in diffusion MRI and demonstrates potential competitiveness of the proposed method. However, the work‚Äôs novelty is not fully established, and the experimental validation is incomplete, particularly regarding baseline selection and real‚Äëdata evaluation. Without stronger evidence and contextualization, the contribution remains uncertain in significance.\n\n**Decision Recommendation**  \n**Major revision.** The paper has potential but requires clearer positioning, more comprehensive comparisons, and additional quantitative validation on real data before it can be adequately assessed.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚úÖ**  \n   The submission introduces a novel self‚Äësupervised denoising framework (**Di‚ÄëFusion**) for diffusion MRI based on modifications to denoising diffusion probabilistic models (DDPMs). The paper focuses on imaging methodology: forward/reverse diffusion processes, statistical self‚Äësupervision, and controllable inference ‚Äî directly aligned with TMI‚Äôs scope in image reconstruction and quantitative image analysis.\n\n2. **Novelty & Contribution Level ‚Äì ‚úÖ**  \n   The authors propose several conceptually new elements:  \n   * ‚ÄúFusion process‚Äù to reduce sampling drift by aligning forward trajectories.  \n   * ‚ÄúDi‚Äëprocess‚Äù to empirically model real‚Äëworld noise without learned priors.  \n   * Restriction to latter diffusion steps to improve stability.  \n   * ‚ÄúRun‚ÄìWalk accelerated sampling‚Äù with adaptive termination for controllable denoising.  \n   This combination goes beyond incremental application and constitutes methodological innovation compared with prior self‚Äësupervised diffusion MRI denoising (e.g., DDM2, Patch2Self).\n\n3. **Technical & Experimental Rigor ‚Äì High**  \n   Mathematical derivations are detailed, describing both forward and reverse processes. Experiments span multiple real dMRI datasets and simulated MRI data, with ablation, assessment on downstream tractography and microstructure modeling, and quantitative SNR/CNR, PSNR, SSIM analyses. Reproducibility is covered (code link, hyperparameters, open datasets). However, more quantitative uncertainty analysis and statistical testing would strengthen claims.\n\n4. **Clarity & Presentation ‚Äì Moderate‚ÄëHigh**  \n   The manuscript is extensive and mathematically thorough. Figures aid intuition but text could be condensed; notation density may challenge general readers. English usage is good with minor grammatical inconsistencies. Structuring is consistent with IEEE style.\n\n5. **Ethical & Reproducibility Compliance ‚Äì ‚úÖ**  \n   All data are publicly available; privacy not an issue. The authors release source code and describe training details, implying strong reproducibility.\n\n---\n\n**Phase 2: IEEE‚ÄØTMI Review Report**\n\n### 1. Summary\nThe paper proposes **Di‚ÄëFusion**, a fully self‚Äësupervised denoising framework for diffusion MRI. It integrates self‚Äësupervised statistical denoising theory (Noise2Self) with diffusion probabilistic models, introducing a *Fusion process* to avoid sampling drift and a *Di‚Äëprocess* to estimate data‚Äëdependent noise. Training focuses on latter diffusion steps for stable learning, while inference applies an adaptive ‚ÄúRun‚ÄìWalk‚Äù sampling and adjustable stopping criterion to control denoising strength. Tested on multiple public datasets and simulations, Di‚ÄëFusion yields higher SNR/CNR, improved tractography coherence, and better microstructure fitting (CSD/DTI R¬≤) than existing denoisers (Patch2Self, DDM2, etc.).\n\n### 2. Strengths\n- **Methodological novelty:** Combines self‚Äësupervision, adaptive diffusion sampling, and real‚Äënoise modeling in a unified single‚Äëstage framework.  \n- **Technical rigor:** Mathematical derivations, algorithmic clarity, and ablation studies identify each component‚Äôs role.  \n- **Robust validation:** Evaluations across in‚Äëvivo and simulated data; clinical impact examined through tractography and DTI metrics.  \n- **Open‚Äësource release** enhances transparency and reproducibility.\n\n### 3. Weaknesses\n- The theoretical justification of stability improvements (e.g., training only latter steps) is largely empirical.  \n- Quantitative uncertainty or statistical significance of performance gains is missing.  \n- Heavy notation and supplementary length reduce accessibility.  \n- Practical runtime issues remain: diffusion models are computationally intensive, and inference time, though mitigated, may limit clinical use.\n\n### 4. Major Comments\n1. **Quantitative support for stability claims:**  \n   Provide statistical variance or confidence intervals across multiple subjects/runs to confirm stability improvements from latter‚Äëstep training.\n2. **Complexity and runtime:**  \n   Report detailed training/inference time compared to DDM2 and classical methods; discuss feasibility for clinical pipelines.\n3. **Ablation depth:**  \n   Current ablations are qualitative; include numerical metrics (PSNR/SSIM/SNR) for each component removal.\n4. **Notation clarity:**  \n   Streamline notation in Sections‚ÄØ3.1‚Äì3.3 to enhance readability; summarize equations in a table or schematic.\n5. **Clinical translation:**  \n   Elaborate on how parameter tuning (e.g., CSNR threshold) could be standardized for routine deployment.\n\n### 5. Minor Comments\n- Fix minor typos (‚ÄúDi-‚Äù sometimes written inconsistently, spacing before figures).  \n- When referencing appendices, denote key equations in main text more clearly.  \n- Improve figure captions (especially Fig.‚ÄØ1) for self‚Äëcontained explanation.  \n- Confirm that all datasets cite proper usage licenses and IRB waivers.\n\n### 6. Overall Recommendation\n**Major Revision** ‚Äì Strong potential for publication in *IEEE‚ÄØTMI*. The paper introduces a meaningful methodological advance in self‚Äësupervised diffusion MRI denoising but would benefit from clearer theoretical justification, additional quantitative comparisons, and improved presentation before acceptance.\n\n### 7. Confidence Level\n**4 / 5** ‚Äì High confidence in technical assessment; moderate confidence in clinical utility judgment awaiting clarified experiments.\n\n---\n\n**Reviewer summary judgment:**  \nThe work offers a significant step toward self‚Äësupervised diffusion MRI denoising within the diffusion‚Äëmodel framework. With enhanced quantitative validation and streamlined presentation, it would be a valuable contribution to *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Di‚ÄëFusion*, a self‚Äësupervised denoising framework for diffusion MRI built upon denoising diffusion probabilistic models. The method integrates self‚Äësupervised statistical denoising with diffusion modeling to reconstruct clean MR signals without external ground truth. Its main contributions include the introduction of a fusion process to align forward trajectories, a data‚Äëdriven Di‚Äëprocess for real‚Äëworld noise modeling, restriction of training to latter diffusion steps for improved stability, and a ‚ÄúRun‚ÄìWalk‚Äù adaptive sampling scheme for controllable inference. The paper is technically detailed and well aligned with the field of image reconstruction and analysis, though it is mathematically dense and could be made more accessible.  \n\n**Major Comments**  \n1. **Quantitative validation of stability claims:** The improvement from focusing training on later diffusion steps is justified mainly qualitatively. Statistical variance or confidence intervals across subjects or runs should be reported to substantiate stability claims.  \n2. **Runtime and computational complexity:** While adaptive sampling improves efficiency, diffusion models remain resource‚Äëintensive. Providing detailed training and inference times, and comparisons to prior methods such as DDM2 and Patch2Self, would clarify practical feasibility.  \n3. **Ablation study depth:** Current ablations are primarily qualitative; quantitative metrics (e.g., PSNR, SSIM, SNR) are needed for each model component to confirm its contribution.  \n4. **Notation and presentation:** The manuscript‚Äôs heavy mathematical notation may hinder readability. Condensing or summarizing lengthy equations‚Äîperhaps in a table or schematic‚Äîwould improve clarity.  \n5. **Clinical translation:** Discussion on standardizing parameters (e.g., CSNR threshold) could clarify applicability to clinical and routine research pipelines.  \n\n**Minor Comments**  \n- Ensure consistent formatting of ‚ÄúDi‚Äë‚Äù terms and correct minor typographical errors.  \n- Clarify references to appendices by indicating key equations in the main text.  \n- Enhance figure captions, particularly Figure‚ÄØ1, to be self‚Äëcontained.  \n- Verify citation of dataset usage licenses and institutional approvals.  \n\n**Summary Paragraph**  \nOverall, the study demonstrates methodological novelty by merging self‚Äësupervision, adaptive diffusion sampling, and real‚Äënoise modeling into one coherent framework. The experiments are broad and rigorous, showing improvements in SNR/CNR, tractography, and microstructure modeling across public datasets. Strengths lie in algorithmic detail, open‚Äësource release, and reproducibility. Weaknesses concern limited quantitative uncertainty analysis, dense notation, and incomplete runtime reporting. Addressing these points would significantly strengthen the contribution‚Äôs clarity, empirical support, and practical relevance.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is methodologically strong and innovative but requires additional quantitative validation, expanded runtime analysis, and clearer presentation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces Di-Fusion, a self-supervised denoising method for diffusion MRI (dMRI) that leverages diffusion probabilistic models without requiring clean training data. The method addresses three key challenges: (1) a \"Fusion process\" that creates linear interpolations between noisy volumes to avoid drift during sampling, (2) a \"Di-\" process that characterizes real-world noise using differences between independent noisy measurements, and (3) training only the latter diffusion steps (Tc ‚â§ T) to reduce generative diversity and improve stability. The approach enables adaptive sampling termination based on a controllable SNR threshold (CSNR). Experiments on three dMRI datasets (Stanford HARDI, Sherbrooke 3-Shell, PPMI) and simulated data demonstrate state-of-the-art performance in downstream tasks including tractography, microstructure modeling, and DTI fitting compared to methods like DDM2, Patch2Self, and traditional approaches.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Equation (5) introduces notation ¬Øxt‚àí1 without clear definition until much later in the text (Section 3.1), creating confusion about the relationship between different trajectory sets {xt} and {¬Øxt}\n  - The \"mess(¬∑)\" operation in Equation (8) is defined only as \"spatial shuffling\" without mathematical rigor, yet it's critical for the noise characterization claim\n  - Appendix C.1 derivations contain algebraic steps that appear to conflate different probability distributions without proper justification for substituting x‚àót in place of x‚Ä≤\n\n‚Ä¢ **Insufficient theoretical justification for key design choices**\n  - The claim that training only latter diffusion steps (Tc = 300) reduces \"diverse generative capabilities\" lacks theoretical grounding beyond intuitive explanations (Section 3.2)\n  - No formal analysis of why the Fusion process mathematically prevents drift, despite this being a central contribution (Section 3.1, Figure 1a)\n  - The variance preservation claim for Œæx‚àíx‚Ä≤ in Appendix C.2 assumes independence that may not hold in practice for dMRI noise\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - Competing methods use different architectures while claiming \"fair comparison\" - only DDM2, Nr2N, and DIP adopt Di-Fusion's U-Net architecture (Appendix D.1)\n  - The primary evaluation metric (R¬≤ for microstructure fitting, Table S2) shows marginal improvements (0.967 vs 0.959 for Nr2N in CSD-CC) that may not be clinically significant\n  - Simulated experiments (Table 1) show high standard deviations for DDM2 and Nr2N but not for Di-Fusion, suggesting potential implementation or hyperparameter optimization bias\n\n‚Ä¢ **Limited scope and generalizability concerns**\n  - Method requires specific dMRI acquisition with adjacent volume pairs (j and j-1), which may not be available in many clinical protocols\n  - Noise model assumptions explicitly target only additive Gaussian thermal noise, excluding physiological artifacts common in real dMRI (Section 5, limitations)\n  - Computational overhead of diffusion model sampling (5+ hours training, Section D.1) limits practical clinical adoption compared to methods like Patch2Self\n\n‚Ä¢ **Presentation and reproducibility issues**\n  - Critical implementation details scattered across appendices make reproduction difficult (noise schedule in D.1, hyperparameters in multiple sections)\n  - Figure quality issues with small text and overlapping elements in Figure 2 and supplementary figures\n  - Inconsistent notation switching between FŒ∏ and F without subscripts throughout equations\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and presentation clarity**\n  - Provide formal mathematical definition of the \"mess(¬∑)\" operation with properties and theoretical justification for variance preservation\n  - Reorganize Section 3.1 to define all trajectory notations (xt, ¬Øxt, x‚àót) upfront before presenting equations\n  - Include rigorous proofs for drift prevention claims and theoretical analysis of why latter-step training reduces generative capacity\n\n‚Ä¢ **Strengthen theoretical foundations with formal analysis**\n  - Develop theoretical framework explaining why Tc = 300 is optimal, including analysis of training stability vs. generative capacity trade-offs\n  - Provide mathematical proof that the Fusion process prevents drift under stated assumptions\n  - Analyze conditions under which independence assumptions for noise hold in real dMRI scenarios\n\n‚Ä¢ **Improve experimental rigor and fairness**\n  - Implement all competing methods with identical U-Net architectures and report hyperparameter optimization procedures for fair comparison\n  - Include statistical significance testing for R¬≤ improvements and establish clinical significance thresholds\n  - Provide error bars and multiple runs for all methods in simulated experiments, not just selected ones\n\n‚Ä¢ **Expand applicability and address limitations**\n  - Evaluate performance on datasets without adjacent volume pairs by developing alternative pairing strategies\n  - Extend noise model to handle physiological artifacts beyond additive Gaussian noise\n  - Compare computational efficiency more comprehensively, including memory requirements and inference time comparisons with all baselines\n\n‚Ä¢ **Improve presentation and ensure reproducibility**\n  - Consolidate all implementation details into main paper or single appendix section with complete hyperparameter tables\n  - Enhance figure quality with larger fonts and clearer layouts, particularly Figure 2 and supplementary visualizations\n  - Standardize notation throughout and provide a symbols table for complex mathematical expressions",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Di‚ÄëFusion*, a self‚Äësupervised denoising framework for diffusion MRI (dMRI) based on diffusion probabilistic models that operate without clean target data. The method tackles three primary challenges through novel design elements: a ‚ÄúFusion‚Äù process to mitigate drift during sampling, a ‚ÄúDi‚Äë‚Äù process to model real‚Äëworld noise from differences between independent measurements, and selective training of the latter diffusion steps to enhance stability while reducing excessive generative variability. The authors further propose an adaptive sampling termination criterion linked to a controllable signal‚Äëto‚Äënoise threshold. Empirical evaluation on three public dMRI datasets and simulated data reports state‚Äëof‚Äëthe‚Äëart results in reconstruction and downstream tasks. The work is ambitious and technically detailed, though several theoretical and methodological aspects require clarification and strengthening.\n\n**Major Comments**  \n1. **Mathematical formulation and clarity** ‚Äì Some definitions, such as the notation ¬Øx‚Çú‚Çã‚ÇÅ in Eq.‚ÄØ(5) and the operation *mess(¬∑)* in Eq.‚ÄØ(8), are introduced without sufficient rigor or timely explanation. Appendix‚ÄØC.1 conflates probability distributions without clear justification for substituting x*‚Çú in place of x‚Ä≤.  \n2. **Theoretical justification of design choices** ‚Äì Claims regarding training only later diffusion steps (Tc‚ÄØ=‚ÄØ300) and the drift‚Äëprevention effect of the Fusion process are presented intuitively but lack formal analysis, proofs, or theoretical grounding. Assumed noise independence in variance preservation may not hold for real dMRI.  \n3. **Experimental design and fairness** ‚Äì Comparisons across competing methods may not be architecturally consistent. Reported gains in R¬≤ are marginal and possibly not clinically meaningful. Variation in standard deviations across models suggests potential implementation bias.  \n4. **Scope and generalizability** ‚Äì The method depends on paired adjacent volumes and assumes additive Gaussian noise, limiting applicability to certain acquisition protocols. The reported computation time (over‚ÄØ5‚ÄØhours) raises practical concerns for clinical use.  \n5. **Presentation and reproducibility** ‚Äì Implementation details are fragmented across appendices, figures contain small or overlapping text, and notation is inconsistently presented, reducing reproducibility and readability.\n\n**Minor Comments**  \n- Define all trajectory notations (x‚Çú, ¬Øx‚Çú, x*‚Çú) before use.  \n- Provide complete explanations of all operators and hyperparameters within the main text or a consolidated appendix.  \n- Improve figure readability and maintain consistent notation (FŒ∏ vs‚ÄØF).  \n- Include a formal table of symbols.\n\n**Summary Paragraph**  \nThe paper presents an original approach with promising empirical outcomes but suffers from gaps in mathematical rigor, theoretical explanation, and experimental fairness. Clarifying core derivations, justifying key assumptions, and reorganizing presentation for transparency would substantially enhance the credibility and reproducibility of the contribution. Computational efficiency and broader applicability also merit further exploration before clinical deployment.\n\n**Decision Recommendation**  \n**Major Revision.** The study is conceptually interesting but requires significant theoretical and methodological strengthening prior to acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nPeer Review\n\n:\nThe manuscript presents Di-Fusion, a novel fully self-supervised denoising method specifically designed for diffusion MRI (dMRI) data. Leveraging later diffusion steps and an adaptive sampling process, Di-Fusion aims to improve the quality of dMRI images, demonstrating state-of-the-art performance across various downstream tasks. The authors emphasize the method‚Äôs innovative approach and its potential impact in the field of medical imaging, with the added benefit of code availability for reproducibility.\n\n## Major Comments\n:\nStrengths:\n1. Clarity and Conciseness: The abstract effectively communicates the core concept, motivation, and key achievements of the research in a concise manner.\n2. Reproducibility: The availability of code through a GitHub link significantly enhances the transparency and reproducibility of the study.\n3. Comprehensive Contributions: The manuscript clearly delineates the three main contributions of the work, providing a clear roadmap for the reader.\n4. Technical Depth: The abstract touches upon technical aspects such as the use of diffusion models and adaptive sampling, adding depth to the content.\n5. Financial Transparency: The Acknowledgments section clearly lists the funding agencies and includes specific grant numbers, enhancing the transparency of the research.\n\nLimitations:\n1. Lack of Specific Performance Metrics: While the abstract mentions state-of-the-art performance, it lacks specific metrics or comparisons to provide a quantitative sense of the improvement.\n2. Insufficient Context for Non-Specialists: Some technical terms and concepts might be challenging for readers who are not specialists in MRI or machine learning.\n3. Limited Scope: The abstract could benefit from a brief mention of the broader implications of the method beyond dMRI, to highlight its potential impact.\n4. Acknowledgment Brevity: The Acknowledgments section lacks detail about individual contributors or institutions that provided additional support beyond financial backing.\n5. Potential Overlap in References: Some references might overlap in terms of the methodologies they describe, leading to redundancy in the citation list.\n\n## Minor Comments\n:\n1. Abstract: Include specific performance metrics and comparisons to give a quantitative sense of the improvement over existing methods.\n2. Abstract: Provide broader contextual implications of the method beyond dMRI to highlight its potential impact.\n3. Acknowledgments: Include acknowledgment of additional forms of support and provide more detailed context about the role of each funding agency.\n4. References: Conduct a thorough review to minimize overlapping references and ensure each citation adds unique value.\n5. References: Ensure completeness in citations by including all necessary details such as page numbers and URLs.\n\n## Summary Paragraph\nEvaluating the work against the TMI editorial criteria:\n- Significance: The manuscript addresses a significant problem in MRI denoising and demonstrates substantial improvements in image quality using a novel self-supervised method.\n- Innovation: Di-Fusion represents an innovative approach to MRI denoising, particularly in leveraging diffusion steps and adaptive sampling.\n- Evaluation: Comprehensive experiments on both real and simulated data support the robustness of the findings, though more specific performance metrics would enhance the evaluation.\n- Reproducibility: The inclusion of a code repository link significantly enhances reproducibility, aligning with high standards for transparency and verification.\n\n## Decision Recommendation\n:\nMinor Revision\n\nThe manuscript presents a valuable contribution to the field of MRI denoising with Di-Fusion. Minor revisions to address the lack of specific performance metrics, broader contextual implications, and the completeness of references would strengthen the manuscript and better align it with the journal‚Äôs standards.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Di-Fusion*, a fully self-supervised denoising approach tailored for diffusion MRI (dMRI). By utilizing later diffusion steps and an adaptive sampling strategy, the method aims to enhance dMRI image quality and demonstrates state-of-the-art results across multiple tasks. The work emphasizes technical innovation, practical relevance, and reproducibility through publicly available code. Overall, the manuscript is clearly written, well structured, and presents its contributions effectively.  \n\n**Major Comments**  \n**Strengths:**  \n1. **Clarity and Conciseness:** The abstract communicates the motivation, methodology, and contributions clearly and succinctly.  \n2. **Reproducibility:** Provision of a public GitHub repository strengthens transparency and replicability.  \n3. **Defined Contributions:** The paper explicitly enumerates its three main contributions, offering a clear research narrative.  \n4. **Technical Depth:** Integration of diffusion models and adaptive sampling introduces meaningful technical advancement.  \n5. **Transparency in Funding:** The Acknowledgments section appropriately identifies funding sources and grants, reinforcing research integrity.  \n\n**Limitations:**  \n1. **Absence of Quantitative Detail:** The claim of state‚Äëof‚Äëthe‚Äëart performance would be stronger with explicit performance metrics and comparative results.  \n2. **Accessibility:** Some terminology may be too specialized for non‚Äëexperts in MRI or machine learning.  \n3. **Scope Limitation:** A brief discussion of potential applications outside dMRI could broaden the perceived impact.  \n4. **Acknowledgment Detail:** The Acknowledgments could mention specific non‚Äëfinancial contributors or institutional support.  \n5. **Reference Overlap:** Some citations appear redundant and could be streamlined for conciseness.  \n\n**Minor Comments**  \n1. Add quantitative metrics and comparative references in the abstract.  \n2. Include a short discussion of broader implications beyond dMRI.  \n3. Expand the Acknowledgments to include all forms of support and clarify roles of funding agencies.  \n4. Review references to remove redundancies.  \n5. Verify completeness of reference details such as page numbers and URLs.  \n\n**Summary Paragraph**  \nThe study presents a significant contribution to MRI denoising through a novel self‚Äësupervised diffusion framework. It offers strong technical innovation and sound experimental validation, further enhanced by accessible code for reproducibility. However, the manuscript would benefit from added quantitative evidence of performance, a broader contextual framing, and refined references and acknowledgments.  \n\n**Decision Recommendation**  \n**Minor Revision.** The paper is solid in concept and execution but requires modest improvements in quantitative reporting, contextual scope, and documentation details before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## SELF-SUPERVISED DIFFUSION MRI DENOISING VIA ITERATIVE AND STABLE REFINEMENT\n\n### Summary\n\nThe paper proposes Di-Fusion, a self-supervised denoising framework for diffusion MRI that integrates two key ideas into a single-stage diffusion-model pipeline: a Fusion process that mixes the target slice with a neighboring volume to align forward‚Äìreverse trajectories and mitigate drift, and a ‚ÄúDi-‚Äù process that replaces Gaussian noise with a shuffled, zero-meaned difference between adjacent volumes to better approximate real noise statistics. Training is restricted to the latter diffusion steps to reduce generative diversity and improve stability, and inference uses a Run‚ÄìWalk accelerated sampler and an adaptive termination criterion based on a normalized discrepancy metric. Experiments across multiple in-vivo datasets and simulations suggest improved downstream microstructure fitting, tractography quality, and SNR/CNR compared to several self-supervised baselines.\n\n### Strengths\n\n- Technical novelty and innovationThe Fusion process to align forward trajectories with the reverse chain is a concrete, intuitive mechanism to mitigate drift and stabilize sampling in a conditional denoising setting.The ‚ÄúDi-‚Äù process that builds a data-driven noise source from spatially shuffled inter-volume differences is an interesting alternative to fitting an explicit noise model.Training only the latter diffusion steps is a pragmatic design choice to constrain generative diversity for a restoration task while improving training stability.The Run‚ÄìWalk nonuniform reverse-time schedule plus an adaptive stopping rule adds a practical control knob to tune denoising strength versus runtime.\n- Experimental rigor and validationEvaluation spans three public dMRI datasets (HARDI, Sherbrooke 3-Shell, PPMI) and simulated fastMRI experiments across multiple noise levels.Downstream validation includes microstructure model fits (DTI, CSD), tractography (OR bundles, FBC metric), and ROI-based SNR/CNR‚Äîmetrics that are clinically meaningful for diffusion analysis.Ablation studies analyze contributions of Fusion, ‚ÄúDi-‚Äù noise, latter-steps training, and the sampling schedule.\n- Clarity of presentationThe paper clearly motivates why fully supervised denoisers are impractical in dMRI and situates the approach relative to Noise2Self-like methods and DDM2.Figures effectively communicate the core mechanisms (Fusion trajectory alignment, training only latter steps, Run‚ÄìWalk sampling).\n- Significance of contributionsSelf-supervised dMRI denoising with improved tractography and microstructure fitting is clinically relevant.Reducing dependence on many diffusion directions (relative to Patch2Self) can expand applicability to time-constrained clinical protocols.\n\nTechnical novelty and innovation\n\n- The Fusion process to align forward trajectories with the reverse chain is a concrete, intuitive mechanism to mitigate drift and stabilize sampling in a conditional denoising setting.\n- The ‚ÄúDi-‚Äù process that builds a data-driven noise source from spatially shuffled inter-volume differences is an interesting alternative to fitting an explicit noise model.\n- Training only the latter diffusion steps is a pragmatic design choice to constrain generative diversity for a restoration task while improving training stability.\n- The Run‚ÄìWalk nonuniform reverse-time schedule plus an adaptive stopping rule adds a practical control knob to tune denoising strength versus runtime.\n\nExperimental rigor and validation\n\n- Evaluation spans three public dMRI datasets (HARDI, Sherbrooke 3-Shell, PPMI) and simulated fastMRI experiments across multiple noise levels.\n- Downstream validation includes microstructure model fits (DTI, CSD), tractography (OR bundles, FBC metric), and ROI-based SNR/CNR‚Äîmetrics that are clinically meaningful for diffusion analysis.\n- Ablation studies analyze contributions of Fusion, ‚ÄúDi-‚Äù noise, latter-steps training, and the sampling schedule.\n\nClarity of presentation\n\n- The paper clearly motivates why fully supervised denoisers are impractical in dMRI and situates the approach relative to Noise2Self-like methods and DDM2.\n- Figures effectively communicate the core mechanisms (Fusion trajectory alignment, training only latter steps, Run‚ÄìWalk sampling).\n\nSignificance of contributions\n\n- Self-supervised dMRI denoising with improved tractography and microstructure fitting is clinically relevant.\n- Reducing dependence on many diffusion directions (relative to Patch2Self) can expand applicability to time-constrained clinical protocols.\n\n### Weaknesses\n\n- Technical limitations or concernsThe J-invariance justification appears inconsistent with the actual training input, which includes x via x_t*; this breaks the input‚Äìtarget independence assumption underpinning the Noise2Self argument used to motivate Eq. (9).The ‚ÄúDi-‚Äù process assumes x and x‚Ä≤ are independent corrupted measurements of the same underlying signal y, but in dMRI different gradient directions generally have different clean signals; x ‚àí x‚Ä≤ will include signal differences, not only noise.Replacing Gaussian noise with Œæx‚àíx‚Ä≤ throughout the chain breaks standard DDPM assumptions; the resulting process lacks theoretical guarantees about consistency or unbiasedness.\n- Experimental gaps or methodological issuesThe adaptive termination criterion d_x and the brain-tissue coefficient b_x are heuristic, with limited sensitivity analyses and unclear cross-dataset calibration (fixed œÅ1, œÅ2 and a single CSNR).The Run‚ÄìWalk acceleration is not compared against widely used fast samplers and ODE solvers (e.g., DPM-Solver++, DEIS, multistep samplers), or recent ‚Äútrain-on-few-steps‚Äù designs (e.g., Fast-DDPM); wall-clock and quality trade-offs remain underexplored.Main-text comparisons omit some strong practical baselines (e.g., MP-PCA, Noise2Score) that are relegated to the appendix; statistical significance and subject-level variability are not consistently reported.\n- Clarity or presentation issuesNotation is occasionally inconsistent (e.g., Œª superscripts/subscripts; ‚ÄúŒªt1‚Äù vs ‚ÄúŒª1t‚Äù), and some equations are terse or ambiguous, making it hard to verify derivations (especially around Eq. (5), (6), (10), (11)).The role of x vs x‚Ä≤ is described as ‚Äúmapping from x‚Ä≤ to x,‚Äù but both volumes are used to form inputs in training and sampling; this conceptual pivot deserves a clearer and earlier statement.\n- Missing related work or comparisonsLimited empirical comparison against modern accelerated diffusion sampling schemes and recent conditional few-step training strategies.The relationship to DDM2‚Äôs noise-estimation stage and the statistical assumptions it relaxes (vs the stronger assumption that x ‚àí x‚Ä≤ ‚âà noise) would benefit from deeper analysis and quantitative contrasts.\n\nTechnical limitations or concerns\n\n- The J-invariance justification appears inconsistent with the actual training input, which includes x via x_t*; this breaks the input‚Äìtarget independence assumption underpinning the Noise2Self argument used to motivate Eq. (9).\n- The ‚ÄúDi-‚Äù process assumes x and x‚Ä≤ are independent corrupted measurements of the same underlying signal y, but in dMRI different gradient directions generally have different clean signals; x ‚àí x‚Ä≤ will include signal differences, not only noise.\n- Replacing Gaussian noise with Œæx‚àíx‚Ä≤ throughout the chain breaks standard DDPM assumptions; the resulting process lacks theoretical guarantees about consistency or unbiasedness.\n\nExperimental gaps or methodological issues\n\n- The adaptive termination criterion d_x and the brain-tissue coefficient b_x are heuristic, with limited sensitivity analyses and unclear cross-dataset calibration (fixed œÅ1, œÅ2 and a single CSNR).\n- The Run‚ÄìWalk acceleration is not compared against widely used fast samplers and ODE solvers (e.g., DPM-Solver++, DEIS, multistep samplers), or recent ‚Äútrain-on-few-steps‚Äù designs (e.g., Fast-DDPM); wall-clock and quality trade-offs remain underexplored.\n- Main-text comparisons omit some strong practical baselines (e.g., MP-PCA, Noise2Score) that are relegated to the appendix; statistical significance and subject-level variability are not consistently reported.\n\nClarity or presentation issues\n\n- Notation is occasionally inconsistent (e.g., Œª superscripts/subscripts; ‚ÄúŒªt1‚Äù vs ‚ÄúŒª1t‚Äù), and some equations are terse or ambiguous, making it hard to verify derivations (especially around Eq. (5), (6), (10), (11)).\n- The role of x vs x‚Ä≤ is described as ‚Äúmapping from x‚Ä≤ to x,‚Äù but both volumes are used to form inputs in training and sampling; this conceptual pivot deserves a clearer and earlier statement.\n\nMissing related work or comparisons\n\n- Limited empirical comparison against modern accelerated diffusion sampling schemes and recent conditional few-step training strategies.\n- The relationship to DDM2‚Äôs noise-estimation stage and the statistical assumptions it relaxes (vs the stronger assumption that x ‚àí x‚Ä≤ ‚âà noise) would benefit from deeper analysis and quantitative contrasts.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe Fusion process offers a practical way to inject the target signal into the latent xt path, narrowing the reverse trajectory and mitigating drift. However, since xt is constructed using both x and x‚Ä≤, the input to FŒ∏ is no longer independent of the target x, and the Noise2Self-style equivalence argument does not apply directly. The paper should either provide a revised justification (e.g., showing minimization of Eq. (9) still yields a valid denoiser under the new construction) or avoid over-claiming J-invariance.The ‚ÄúDi-‚Äù noise Œæx‚àíx‚Ä≤ is only a valid surrogate for noise if x and x‚Ä≤ share the same underlying clean signal y. In dMRI, measurements across different b-vectors are not replicates; thus x ‚àí x‚Ä≤ contains signal terms. While the mess(¬∑) spatial shuffle reduces structural leakage, it does not remove intensity-distribution bias induced by signal differences. A theoretical analysis of the bias and an empirical sensitivity study (e.g., pairing volumes within the same shell or repeated directions, or using b0‚Äìb0 pairs where available) would strengthen the case.Substituting Gaussian noise distributions with Œæx‚àíx‚Ä≤ at both forward and reverse steps departs from the DDPM theory (which relies on Gaussian perturbations to derive closed-form posteriors). The paper treats FŒ∏ as a generic denoiser and proceeds empirically; that is acceptable pragmatically, but the absence of any consistency guarantee or fixed-point analysis warrants caution, especially in clinical applications.Training only the latter steps is a defensible design that reduces model uncertainty for conditional restoration tasks. The ablation over T_c is helpful; still, a brief discussion connecting this choice to recent few-step training/sampling literature would offer a stronger methodological context.\n- Experimental evaluation assessmentThe choice of metrics‚ÄîSNR/CNR in ROIs, microstructure model R¬≤, FBC for tractography‚Äîaligns with dMRI practice. Results are promising, particularly the R¬≤ gains and the qualitative tract coherence.The adaptive termination and Run‚ÄìWalk gains on runtime are asserted; it would be beneficial to report per-volume wall-clock reductions versus DDPM, DDIM, and DPM-Solver++, alongside quality deltas.The tractography analysis focuses on OR and highlights fewer spurious streamlines while retaining high FBC. Please add subject-level statistics, inter-subject variance, and hypothesis tests to support robustness claims. If possible, include other bundles and tracking settings to show generality.Simulations show stronger performance at higher noise. However, numerical tables use percentages for SSIM, which is acceptable, but ensure units are consistent and that median ¬± IQR are also reported, given non-Gaussian distributions typical in SSIM/PSNR.Provide sensitivity curves for CSNR and œÅ1/œÅ2, and an analysis of their transfer across datasets (HARDI, PPMI, Sherbrooke) to support the ‚Äúuniversal‚Äù threshold claim.\n- Comparison with related work (using the summaries provided)Patch2Self depends on multiple volumes and J-invariance and avoids assuming identical clean signals across directions; Di-Fusion‚Äôs appeal is reducing volume dependency and avoiding a separate noise-model stage. However, Patch2Self‚Äôs theoretical footing remains strong under the independence-of-noise assumption, whereas Di-Fusion assumes near-replication between x and x‚Ä≤ at points (e.g., for Œæx‚àíx‚Ä≤) that may not hold for differing gradients. Please clarify acquisition conditions or pairing strategies that make your assumption reasonable in practice (e.g., within-shell pairing, repeated directions).DDM2 uses a three-stage pipeline; your work simplifies to a single stage and proposes Œæx‚àíx‚Ä≤ instead of fitting a Gaussian noise level. A direct, controlled head-to-head should include runtime, denoising strength, preservation of high-frequency details, hallucination checks, and sensitivity to mismatch between noise statistics and Œæx‚àíx‚Ä≤. It would also be informative to compare against conditional few-step training (e.g., Fast-DDPM) that directly aligns training and inference time steps.Recent accelerated samplers (DPM-Solver++, DEIS) or learned schedulers are not empirically compared. Given your Run‚ÄìWalk schedule, a baseline against these methods would contextualize your speed‚Äìquality trade-offs.\n- Discussion of broader impact and significanceIf robust, the approach could broaden denoising benefits to low-direction or time-constrained dMRI protocols, improving tractography and microstructural estimates where clinical constraints dominate.The paper appropriately acknowledges hallucination and runtime concerns. Given the clinical setting, consider adding simple sanity checks: residual maps across many subjects, lesion retention tests, and test‚Äìretest reproducibility on public datasets.Since Œæx‚àíx‚Ä≤ can be influenced by motion and physiological artifacts (not just thermal noise), the method may inadvertently model artifacts; articulating pre-processing requirements (motion/eddy correction, coil combination strategy) would help practitioners.\n\nTechnical soundness evaluation\n\n- The Fusion process offers a practical way to inject the target signal into the latent xt path, narrowing the reverse trajectory and mitigating drift. However, since xt is constructed using both x and x‚Ä≤, the input to FŒ∏ is no longer independent of the target x, and the Noise2Self-style equivalence argument does not apply directly. The paper should either provide a revised justification (e.g., showing minimization of Eq. (9) still yields a valid denoiser under the new construction) or avoid over-claiming J-invariance.\n- The ‚ÄúDi-‚Äù noise Œæx‚àíx‚Ä≤ is only a valid surrogate for noise if x and x‚Ä≤ share the same underlying clean signal y. In dMRI, measurements across different b-vectors are not replicates; thus x ‚àí x‚Ä≤ contains signal terms. While the mess(¬∑) spatial shuffle reduces structural leakage, it does not remove intensity-distribution bias induced by signal differences. A theoretical analysis of the bias and an empirical sensitivity study (e.g., pairing volumes within the same shell or repeated directions, or using b0‚Äìb0 pairs where available) would strengthen the case.\n- Substituting Gaussian noise distributions with Œæx‚àíx‚Ä≤ at both forward and reverse steps departs from the DDPM theory (which relies on Gaussian perturbations to derive closed-form posteriors). The paper treats FŒ∏ as a generic denoiser and proceeds empirically; that is acceptable pragmatically, but the absence of any consistency guarantee or fixed-point analysis warrants caution, especially in clinical applications.\n- Training only the latter steps is a defensible design that reduces model uncertainty for conditional restoration tasks. The ablation over T_c is helpful; still, a brief discussion connecting this choice to recent few-step training/sampling literature would offer a stronger methodological context.\n\nExperimental evaluation assessment\n\n- The choice of metrics‚ÄîSNR/CNR in ROIs, microstructure model R¬≤, FBC for tractography‚Äîaligns with dMRI practice. Results are promising, particularly the R¬≤ gains and the qualitative tract coherence.\n- The adaptive termination and Run‚ÄìWalk gains on runtime are asserted; it would be beneficial to report per-volume wall-clock reductions versus DDPM, DDIM, and DPM-Solver++, alongside quality deltas.\n- The tractography analysis focuses on OR and highlights fewer spurious streamlines while retaining high FBC. Please add subject-level statistics, inter-subject variance, and hypothesis tests to support robustness claims. If possible, include other bundles and tracking settings to show generality.\n- Simulations show stronger performance at higher noise. However, numerical tables use percentages for SSIM, which is acceptable, but ensure units are consistent and that median ¬± IQR are also reported, given non-Gaussian distributions typical in SSIM/PSNR.\n- Provide sensitivity curves for CSNR and œÅ1/œÅ2, and an analysis of their transfer across datasets (HARDI, PPMI, Sherbrooke) to support the ‚Äúuniversal‚Äù threshold claim.\n\nComparison with related work (using the summaries provided)\n\n- Patch2Self depends on multiple volumes and J-invariance and avoids assuming identical clean signals across directions; Di-Fusion‚Äôs appeal is reducing volume dependency and avoiding a separate noise-model stage. However, Patch2Self‚Äôs theoretical footing remains strong under the independence-of-noise assumption, whereas Di-Fusion assumes near-replication between x and x‚Ä≤ at points (e.g., for Œæx‚àíx‚Ä≤) that may not hold for differing gradients. Please clarify acquisition conditions or pairing strategies that make your assumption reasonable in practice (e.g., within-shell pairing, repeated directions).\n- DDM2 uses a three-stage pipeline; your work simplifies to a single stage and proposes Œæx‚àíx‚Ä≤ instead of fitting a Gaussian noise level. A direct, controlled head-to-head should include runtime, denoising strength, preservation of high-frequency details, hallucination checks, and sensitivity to mismatch between noise statistics and Œæx‚àíx‚Ä≤. It would also be informative to compare against conditional few-step training (e.g., Fast-DDPM) that directly aligns training and inference time steps.\n- Recent accelerated samplers (DPM-Solver++, DEIS) or learned schedulers are not empirically compared. Given your Run‚ÄìWalk schedule, a baseline against these methods would contextualize your speed‚Äìquality trade-offs.\n\nDiscussion of broader impact and significance\n\n- If robust, the approach could broaden denoising benefits to low-direction or time-constrained dMRI protocols, improving tractography and microstructural estimates where clinical constraints dominate.\n- The paper appropriately acknowledges hallucination and runtime concerns. Given the clinical setting, consider adding simple sanity checks: residual maps across many subjects, lesion retention tests, and test‚Äìretest reproducibility on public datasets.\n- Since Œæx‚àíx‚Ä≤ can be influenced by motion and physiological artifacts (not just thermal noise), the method may inadvertently model artifacts; articulating pre-processing requirements (motion/eddy correction, coil combination strategy) would help practitioners.\n\n### Questions for Authors\n\n- How do you reconcile the J-invariance argument with the fact that the model input includes x via x_t*? Can you provide a theoretical justification (or an alternative argument) showing that minimizing Eq. (9) remains valid under your Fusion input construction?\n- What acquisition pairing is used when constructing Œæx‚àíx‚Ä≤? Are x and x‚Ä≤ from the same shell or repeated directions? Please quantify how much signal difference leaks into Œæx‚àíx‚Ä≤ and its effect on bias; a controlled experiment with known repeated directions or b0 pairs would clarify this.\n- How sensitive is the adaptive termination to CSNR and to the thresholds œÅ1, œÅ2? Can you provide calibration curves and cross-dataset transfer results, and guidance for practitioners on setting these parameters?\n- Could you compare Run‚ÄìWalk to DPM-Solver++/DEIS and to a few-step training baseline (e.g., Fast-DDPM) in both runtime and quality? This would better contextualize the practical advantages of your sampling strategy.\n- Can you report statistical tests (e.g., paired t-tests or nonparametric tests) across subjects for the SNR/CNR and microstructure R¬≤ improvements, and for tractography metrics (FBC, streamline counts)? How many subjects per dataset were used for each analysis?\n- Do you observe hallucinations or structure removal under extreme settings (e.g., very small or very large CSNR)? Please include failure cases and a quantitative hallucination check (e.g., residual structure metrics).\n- What architectural details (network, capacity, normalization) and training hyperparameters (T_c, T_r, p, Œ∑, œÉ_t schedule, learning rate, epochs) were used? Will you release code and trained models?\n- How is the first volume (j=1) handled when x‚Ä≤ is unavailable? If wrap-around or b0 pairing is used, please describe any implications for performance.\n- How does performance change when Œæx‚àíx‚Ä≤ is replaced by Gaussian z or by residuals estimated via a blind-spot denoiser (as in DDM2 Stage I)? A quantitative ablation would isolate the benefit of the ‚ÄúDi-‚Äù process.\n- Can you provide guidance on pre-processing (motion/eddy correction, coil combination) required to satisfy your independence assumptions and to avoid embedding structured artifacts into Œæx‚àíx‚Ä≤?\n\n### Overall Assessment\n\nThis paper tackles an important and challenging problem‚Äîself-supervised denoising for dMRI‚Äîwhere clean references are unavailable and downstream clinical utility matters. The proposed Fusion and ‚ÄúDi-‚Äù processes, the focus on latter diffusion steps, and the adaptive Run‚ÄìWalk sampling together form a cohesive, single-stage pipeline with encouraging empirical results on in-vivo datasets and simulations. However, the theoretical justification has gaps: the J-invariance equivalence is invoked despite inputs that include the target x, and the construction of Œæx‚àíx‚Ä≤ assumes replicate-like measurements that typically do not hold across differing diffusion gradients. These points do not invalidate the method per se, but they weaken claims and suggest a risk of bias or artifact leakage. On the experimental side, the evaluation is broad and uses relevant metrics, yet several crucial comparisons (modern fast samplers, few-step training, stronger baselines in main text) and statistical analyses are missing, and parameter sensitivity for the adaptive stopping is under-characterized. With a strengthened theoretical discussion (or reframing), additional empirical controls and comparisons, and clearer guidance on parameterization and pre-processing, this work could be a valuable addition to self-supervised MRI denoising literature. As it stands, I view the paper as promising and practically interesting but in need of clarification and additional validation to reach a top-tier standard.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**\n\nThe manuscript introduces **Di-Fusion**, a self-supervised denoising framework for diffusion MRI that builds on diffusion models through two main innovations: (1) a *Fusion* mechanism aligning forward and reverse trajectories to reduce drift, and (2) a data-driven *‚ÄúDi‚Äù-noise* process replacing Gaussian perturbations with shuffled inter-volume differences to approximate empirical noise statistics. Training is limited to later diffusion steps to enhance stability, while inference applies a Run‚ÄìWalk sampling schedule and adaptive stopping criterion. Experiments on several public dMRI datasets and simulations indicate improvements in microstructure fitting, tractography, and SNR/CNR over existing self-supervised baselines. The paper is clearly written, well motivated, and experimentally detailed.\n\n---\n\n**Major Comments**\n\n1. **Methodological Justification** ‚Äì The J‚Äëinvariance argument is inconsistent with the model input, which includes the target \\(x\\). This undermines the independence assumption used to justify the objective. Similarly, the ‚ÄúDi‚Äù-noise formulation assumes paired volumes share identical underlying signals, an assumption that may not hold across diffusion directions, potentially introducing bias.  \n2. **Theoretical Soundness** ‚Äì Replacing Gaussian noise with \\(\\xi_{x-x'}\\) departs from DDPM theory and removes guarantees of consistency or unbiasedness. A revised justification or discussion of implications is needed.  \n3. **Experimental Coverage** ‚Äì The adaptive termination rule and Run‚ÄìWalk schedule are heuristic and only lightly analyzed; cross-dataset sensitivity is unclear. Comparisons with recent accelerated or few-step diffusion samplers (e.g., DPM‚ÄëSolver++, DEIS, Fast‚ÄëDDPM) are missing, and strong baselines (MP‚ÄëPCA, Noise2Score) appear only in the appendix.  \n4. **Statistical Reporting** ‚Äì The experiments show promising results, yet statistical significance and inter-subject variability are not systematically reported.  \n5. **Clarity and Presentation** ‚Äì Some equations have inconsistent notation and terseness that hinder verification. The conceptual role of \\(x\\) versus \\(x'\\) should be clarified earlier.  \n6. **Related Work** ‚Äì The relationship to DDM2 and Patch2Self‚Äôs theoretical assumptions should be elaborated. Head‚Äëto‚Äëhead runtime and quality comparisons would better contextualize Di-Fusion.  \n7. **Broader Validity** ‚Äì Since \\(\\xi_{x-x'}\\) may capture motion or physiological artifacts, guidance on preprocessing requirements and sanity checks (e.g., residual maps, test‚Äìretest reproducibility) would strengthen reliability claims.\n\n---\n\n**Minor Comments**\n\n- Ensure consistent notation (e.g., Œª superscripts/subscripts).  \n- Report units and dispersion (median ¬± IQR) for SSIM/PSNR metrics.  \n- Provide sensitivity analyses for the adaptive thresholds (œÅ‚ÇÅ, œÅ‚ÇÇ, CSNR).  \n- Clarify how the first volume is handled when no \\(x'\\) is available.  \n- Include additional tract bundles and subject‚Äëlevel statistics in tractography results.  \n\n---\n\n**Summary Paragraph**\n\nOverall, the paper presents a creative and potentially impactful self‚Äësupervised denoising approach for diffusion MRI, combining trajectory fusion, data‚Äëdriven noise modeling, and adaptive inference. Its strengths lie in the originality of the design, breadth of experiments, and practical motivation. Nonetheless, theoretical inconsistencies in the J‚Äëinvariance and noise assumptions, limited benchmarking against the latest diffusion sampling methods, and partial reporting of statistical analyses weaken the current evidential basis. Clarifying these theoretical foundations, expanding validation, and refining methodological exposition would substantially strengthen the work.\n\n---\n\n**Decision Recommendation**\n\n**Major Revision.** The contribution is promising but requires stronger theoretical justification, broader comparisons, and improved empirical and statistical validation to fully support its claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for IEEE Transactions on Medical Imaging\n\n## 1. Opening Paragraph\n\nThe paper addresses the critical challenge of denoising diffusion MRI (dMRI) data, where low signal-to-noise ratio (SNR) compromises clinical utility without requiring clean reference data. The authors propose Di-Fusion, a fully self-supervised denoising framework that leverages diffusion models with several key innovations: (1) a Fusion process to align the forward diffusion trajectory and prevent drift, (2) a \"Di-\" process to better characterize real-world noise distributions without extra noise modeling, and (3) training only the latter diffusion steps with an adaptive sampling process for stable and controllable refinement. The method is validated across three public dMRI datasets (Stanford HARDI, Sherbrooke 3-Shell, and PPMI) and simulated data, demonstrating superior performance over state-of-the-art methods in multiple clinically relevant downstream tasks. Specifically, Di-Fusion achieves the highest R¬≤ values in microstructure modeling (CSD and DTI), better fiber bundle coherency in tractography with minimal streamline count, and improved diffusion signal estimates (FA, MD, AD, RD) while maintaining anatomical fidelity. The approach requires only one additional volume for denoising, making it clinically practical compared to methods needing 10+ volumes.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- **Clinically practical framework**: The single-stage architecture eliminates the need for clean reference data or multiple training stages (unlike DDM2), making it feasible for clinical deployment where clean data is unavailable\n- **Innovative Fusion process**: Effectively addresses the drift problem common in diffusion-based denoising by aligning the forward process with the target slice\n- **Comprehensive clinical validation**: Demonstrates superiority not just on standard metrics but across multiple downstream clinical tasks (microstructure modeling, tractography, diffusion signal estimation)\n- **\"Di-\" process**: Provides a more accurate characterization of real-world noise compared to standard Gaussian assumptions, validated through statistical analysis\n\n### Major Limitations\n- **Computational cost**: While the authors note the sampling time is reduced through adaptive termination, the overall computational burden compared to non-diffusion methods like Patch2Self could be better quantified and contextualized for clinical deployment\n- **Hyperparameter justification**: Limited exploration of why T_c=300 (for latter diffusion steps) is optimal, with only a brief comment that performance is consistent when T_c<500\n- **Clinical parameter guidance**: The CSNR threshold for adaptive termination is not sufficiently linked to clinical outcomes; how clinicians should select this parameter for specific diagnostic purposes is unclear\n\n### Minor Strengths\n- **Clear visualization**: The figures effectively illustrate the diffusion process, drift prevention, and adaptive sampling mechanism\n- **Superior noise characterization**: The \"Di-\" process is convincingly shown to capture noise properties better than Gaussian assumptions (Fig. S20)\n- **Reduced volume dependency**: Only one additional volume needed compared to Patch2Self's 10+ requirement, expanding clinical applicability\n\n### Minor Limitations\n- **Figure organization**: Some supplementary figures (e.g., Fig. S24) combine multiple ablation studies, making interpretation challenging\n- **Inconsistent terminology**: Occasional inconsistent use of terms like \"latter diffusion steps\" vs \"latter diffusion steps\"\n- **Limited b-value analysis**: While the method shows robustness to mixed b-values, this could be explored more deeply\n\n## 3. Evaluation Summary\n\n**Significance (High)**: The work addresses a critical clinical need for dMRI denoising where clean reference data is unavailable. The demonstrated improvements in downstream tasks (microstructure modeling, tractography) directly impact diagnostic accuracy and clinical decision-making. The method's ability to work with minimal additional volumes (one vs ten required by Patch2Self) significantly enhances its clinical applicability, potentially enabling better diagnostic quality from existing dMRI protocols without requiring longer scan times or reduced spatial resolution.\n\n**Innovation (High)**: The paper presents several novel contributions: (1) the Fusion process that aligns the diffusion trajectory to prevent drift, (2) the \"Di-\" process for more accurate noise characterization, and (3) the adaptive Run-Walk sampling with controllable refinement. These components form a cohesive single-stage framework that fundamentally differs from prior multi-stage approaches like DDM2. The creative integration of statistical self-supervised techniques with diffusion models represents a meaningful advancement in the field, with potential applicability beyond dMRI.\n\n**Evaluation (High)**: The evaluation is exceptionally thorough, with comparisons against multiple state-of-the-art methods across three public dMRI datasets and simulated data. The authors go beyond standard metrics to validate performance on clinically relevant downstream tasks, providing R¬≤ scores for microstructure modeling, fiber bundle coherency for tractography, and diffusion signal estimates. The ablation studies are comprehensive, testing each component's contribution. However, the main paper could benefit from including more comparisons with diffusion-based approaches (currently largely in supplementary materials).\n\n**Reproducibility (High)**: The paper provides sufficient detail for reproduction, including complete algorithms for training and sampling, noise schedule details, and implementation parameters. The supplementary materials offer extensive ablation studies and additional comparisons. The authors mention code availability, and the computational requirements (RTX 3090, 5 hours for training) are clearly stated. Including more specific guidance on selecting CSNR for different clinical applications would further enhance reproducibility.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThe paper presents a significant and innovative contribution to self-supervised dMRI denoising with strong clinical relevance. The technical approach is sound, the evaluation is comprehensive, and the clinical validation is particularly compelling. The method demonstrates clear advantages over existing approaches across multiple metrics and downstream tasks.\n\nThe following relatively straightforward revisions would strengthen the paper:\n1. Provide more detailed justification for key hyperparameters (especially T_c=300)\n2. Elaborate on clinical guidance for selecting the CSNR threshold\n3. Streamline some supplementary material figures for better readability\n4. Include a brief computational comparison with non-diffusion methods to contextualize the computational cost\n\nThese revisions are relatively minor and would not require additional experiments. The paper is already of high quality and would benefit from these refinements before final acceptance. The contribution represents a meaningful advancement in the field that would be of interest to the TMI readership.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *Di-Fusion*, a self-supervised framework for denoising diffusion MRI (dMRI) data without requiring clean reference volumes, thereby addressing the low signal-to-noise ratio challenge that limits clinical usability. The method introduces three main innovations: a Fusion process to align diffusion trajectories and avoid drift, a ‚ÄúDi-‚Äù process for modeling realistic noise distributions without extra noise simulation, and an adaptive sampling strategy that trains only the latter diffusion steps for controllable refinement. Validation across three public dMRI datasets (Stanford HARDI, Sherbrooke 3-Shell, and PPMI) and simulated data demonstrates superior performance to current state-of-the-art methods in downstream tasks such as microstructure modeling, tractography, and diffusion metric estimation. The framework maintains anatomical fidelity while requiring only one additional volume, enhancing its clinical practicality.  \n\n**Major Comments**  \n1. **Computational cost**: Although adaptive termination mitigates sampling time, the relative computational burden compared with non-diffusion approaches like Patch2Self should be quantified and discussed in a clinical context.  \n2. **Hyperparameter justification**: The choice of \\(T_c = 300\\) for training the latter diffusion steps lacks detailed rationale; only a short comment indicates stable performance for \\(T_c < 500\\).  \n3. **Clinical parameter interpretation**: The relationship between the CSNR threshold used for adaptive termination and its clinical implications is insufficiently explained. Guidance for clinicians on parameter selection would improve applicability.  \n4. **Comparative design**: Most comparisons with other diffusion-based methods are confined to supplementary materials; including key results in the main text would enhance readability and fairness of evaluation.  \n\n**Minor Comments**  \n- Several supplementary figures (e.g., Fig.‚ÄØS24) present multiple ablation analyses simultaneously, which may hinder interpretation.  \n- Occasional inconsistencies appear in terminology (e.g., ‚Äúlatter diffusion steps‚Äù phrased variably).  \n- The analysis of mixed b-values could be elaborated for completeness.  \n- Figures are generally clear and effectively illustrate the workflow, drift prevention, and adaptive sampling process.  \n\n**Summary Paragraph**  \nThe study offers a well-motivated and technically innovative contribution to self-supervised dMRI denoising. Strengths include comprehensive validation across clinical datasets, an elegant single-stage framework eliminating the need for clean training data, and detailed supplementary material supporting reproducibility. Remaining issues concern better quantification of computation time, additional context for hyperparameter choices, and clearer clinical guidance on adaptive parameters. With minor refinements, the paper would substantially enhance its impact and clarity.  \n\n**Decision Recommendation**  \n**Minor Revision** ‚Äî The work is strong in methodology, evaluation, and practical relevance. Addressing the noted clarifications on computational cost, parameter choices, and presentation of supplementary content would fully prepare the paper for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe manuscript tackles the challenge of denoising diffusion‚Äëweighted magnetic resonance imaging (dMRI) in situations where low signal‚Äëto‚Äënoise ratio hampers clinical use and clean reference data are unavailable. The authors evaluate their approach on three publicly available in‚Äëvivo brain dMRI datasets (Stanford HARDI, Sherbrooke multi‚Äëshell, PPMI) and on simulated fastMRI k‚Äëspace data subjected to various noise levels. Their method, named Di‚ÄëFusion, is a fully self‚Äësupervised diffusion‚Äëmodel framework that incorporates (i) a Fusion step to align forward trajectories, (ii) a ‚ÄúDi‚Äë‚Äù component that extracts noise statistics without a separate noise‚Äëmodel training phase, and (iii) an adaptive Run‚ÄëWalk sampling scheme that iteratively refines the output and permits early termination. Training is limited to the later diffusion steps, which the authors argue reduces generative variance. Reported results show improvements in peak‚Äësignal‚Äëto‚Äënoise ratio, structural similarity index, and downstream measures such as micro‚Äëstructure model fitting and tractography coherence, indicating that the method can provide stable, iterative dMRI denoising without paired clean images.\n\n---\n\n**## General feedback**\n\n- **Significance:** Denoising dMRI without access to clean data is an important problem for both clinical and research workflows; a self‚Äësupervised diffusion‚Äëmodel that promises stable training and controllable sampling could have wide applicability.  \n- **Innovation:** The paper introduces a combination of (i) Fusion alignment, (ii) the Di‚Äë noise estimator, (iii) restriction of training to the later diffusion steps, and (iv) Run‚ÄëWalk adaptive sampling. This set of contributions is novel relative to earlier self‚Äësupervised methods such as Patch2Self and DDM2.  \n- **Evaluation:** The authors provide an extensive quantitative and downstream assessment (PSNR/SSIM, tractography FBC, R¬≤ of micro‚Äëstructure fits, SNR/CNR). However, the presented results lack measures of variance or statistical significance, and several recent diffusion‚Äëbased denoisers (e.g., score‚Äëbased, cold‚Äëdiffusion approaches) are not included in the comparison.  \n- **Reproducibility:** Training details (optimizer, learning rate, batch size) are reported in Section‚ÄØ3.2 and Appendix‚ÄØD.1, yet many hyper‚Äëparameters (Œ≤‚Äëschedule,‚ÄØTc,‚ÄØTr,‚ÄØp, CSNR thresholds) are only briefly described or confined to supplemental figures. Moreover, the code link is a placeholder (‚Äúgithub.com/XXX‚Äù), which prevents verification of the implementation.\n\n---\n\n**## Specific comments/critiques**\n\n- **Method clarity:** The derivations in Sections‚ÄØ3.1‚Äì3.3 and Appendix‚ÄØC are dense. A high‚Äëlevel schematic illustrating the Fusion and Di‚Äë processes would aid comprehension; presently only Fig.‚ÄØ1‚ÄØ(a) serves this purpose.  \n- **Justification of later‚Äëstep training:** The authors claim that limiting training to the later diffusion steps reduces uncertainty, but no ablation study comparing full‚Äëstep versus later‚Äëstep training is provided to substantiate this assertion.  \n- **Noise representation limitations:** The Di‚Äë component relies on shuffling the difference between two independent noisy volumes (Eq.‚ÄØ8). It is unclear how the method would perform when only a single volume is available or when noise exhibits spatial correlation (e.g., physiological noise).  \n- **Adaptive termination sensitivity:** The CSNR threshold used for early termination (Eq.‚ÄØ13) is fixed across experiments. A sensitivity analysis showing the effect of varying CSNR values on denoising quality and runtime would be informative.  \n- **Baseline coverage:** Recent diffusion‚Äëbased denoisers such as score‚Äëbased denoising and cold diffusion are omitted from the comparative study, despite being mentioned in the background (Section‚ÄØ2.2). Including these baselines would strengthen the claim of state‚Äëof‚Äëthe‚Äëart performance.  \n- **Statistical analysis:** Tables (e.g., Table‚ÄØ1, Table‚ÄØS2, Table‚ÄØS5) present only mean values; standard deviations, confidence intervals, or p‚Äëvalues are absent, leaving the statistical significance of the reported gains uncertain.  \n- **Runtime reporting:** Training time (~5‚ÄØh on an RTX‚ÄØ3090) is reported, but inference speed (sampling time per volume, impact of Run‚ÄëWalk acceleration) is not quantified, even though the manuscript discusses ‚Äúrelatively longer inference‚Äù in Section‚ÄØ5.  \n- **Code availability:** The repository link is a placeholder, which prevents reviewers and readers from reproducing the experiments or inspecting implementation details.  \n- **Typographical issues:** Numerous LaTeX rendering problems (e.g., misplaced brackets, ‚Äú_‚àö_ ~~Œ±~~‚Äù) impede readability of equations and figure captions; a thorough proof‚Äëread is needed.  \n- **Downstream clinical relevance:** The manuscript demonstrates improvements in tractography FBC and R¬≤ fits, yet it does not discuss the practical clinical impact of these gains (e.g., expert evaluation, patient‚Äëlevel outcomes).\n\n---\n\n**## A suggested decision**\n\n**Major Revision** ‚Äì The manuscript requires substantial revisions, particularly concerning baseline comparisons, statistical analysis, and reproducibility provisions, before it can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the problem of denoising diffusion‚Äëweighted MRI (dMRI) data when clean references are unavailable and signal‚Äëto‚Äënoise ratios are low. The proposed self‚Äësupervised approach, Di‚ÄëFusion, integrates three main components: a Fusion step aligning forward trajectories, a ‚ÄúDi‚Äë‚Äù noise extraction module that avoids separate noise‚Äëmodel training, and an adaptive Run‚ÄëWalk sampling strategy permitting early termination and stable refinement. Training is restricted to later diffusion steps to minimize generative variance. The method is evaluated on several public brain dMRI datasets and simulated fastMRI data, showing quantitative gains in PSNR, SSIM, and downstream analyses such as micro‚Äëstructure fitting and tractography coherence. Overall, the study presents a potentially valuable contribution but requires additional validation and clarification to support its claims.  \n\n**Major Comments**  \n1. **Evaluation completeness:** While the quantitative and downstream analyses are extensive, measures of variance or statistical significance are missing, and recently proposed diffusion‚Äëbased denoisers (e.g., score‚Äëbased, cold‚Äëdiffusion) are not included in comparisons.  \n2. **Justification of training strategy:** The claim that training only on later diffusion steps reduces uncertainty is made without ablation experiments contrasting full‚Äë versus partial‚Äëstep training.  \n3. **Noise model assumptions:** The Di‚Äë component depends on differencing two independent noisy volumes; the approach‚Äôs robustness when only a single volume is available or under spatially correlated noise is unclear.  \n4. **Adaptive termination analysis:** The CSNR threshold governing early termination is fixed; sensitivity to this parameter should be explored to confirm its stability and effect on output quality and runtime.  \n5. **Reproducibility:** Although some training details are documented, important hyper‚Äëparameters are only briefly described or confined to supplements, and the code repository link is non‚Äëfunctional, hindering verification.  \n6. **Statistical rigor and runtime reporting:** Tables report only mean values without deviations or significance testing. Inference time and effectiveness of the Run‚ÄëWalk acceleration are not quantified.  \n7. **Clinical interpretation:** Improvements in tractography and micro‚Äëstructure metrics are reported but not contextualized in terms of clinical benefit or patient‚Äëlevel relevance.  \n\n**Minor Comments**  \n- Several equations exhibit LaTeX rendering issues and misplaced formatting that impede readability.  \n- A clearer schematic illustrating the Fusion and Di‚Äë processes would aid understanding.  \n- The manuscript would benefit from proofreading and inclusion of standard deviations or confidence intervals in result tables.  \n\n**Summary Paragraph**  \nThis work introduces a novel self‚Äësupervised diffusion‚Äëmodel framework for dMRI denoising with thoughtful algorithmic innovations and promising empirical results. Its strengths lie in methodological creativity and comprehensive quantitative evaluation. However, limitations in baseline coverage, statistical validation, and reproducibility documentation weaken the evidential support. Clarifying experimental sensitivities, providing accessible code, and improving methodological and presentation clarity would considerably strengthen the paper.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial revisions addressing evaluation completeness, statistical analysis, and reproducibility are required before the manuscript can be reconsidered.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chenxu Wu",
      "Qingpeng Kong",
      "S Kevin Zhou",
      "Zihang Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_6bd7c307edad6542cdf2945234d9af92905ebf8f.pdf",
    "remote_url": "https://openreview.net/pdf/6bd7c307edad6542cdf2945234d9af92905ebf8f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models",
    "status": "completed",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Large Language Models",
      "Large Vision Models",
      "Semantic Evaluation",
      "Computational Pathology",
      "Medical Imaging"
    ],
    "abstract": "Do Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts? Currently, only biased, qualitative approaches with limited, small-scale expert evaluations are available to answer this question. In this study, we propose the Boltzmann Semantic Score (BSS), a novel method inspired by state space modeling, to evaluate the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. Through extensive experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, we first establish a baseline of LLMs' performance in digital pathology and show that LLMs' encoding can be linked to patient outcomes. Then, we compared seven LVMs with BSS and showed that LVMs suffer from poor semantic capability when compared with encoded expert knowledge from pathology reports.\nWe also found statistically significant correlations between BSS (as a measure of structural similarity) and performance in two downstream tasks: information retrieval and survival prediction tasks. Our study also investigates the consensus among LLMs in evaluating LVMs using BSS, indicating that LLMs generally reach substantial consensus in rating LVMs, with some variation dependant on the cancer type. We believe the BSS metric proposed here holds significant potential for application in other domains with similar contexts. Data and code can be found in \\footnotesize \\url{ https://github.com/AIMLab-UBC/Boltzmann}",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a novel semantic metric called Boltzmann Semantic Score (BSS), which is inspired by state space modeling, to evaluate the semantic capability of large vision models (LVMs) in medical image processing. The authors demonstrate the effectiveness of this metric through experiments, revealing that LVMs exhibit low semantic capabilities. Additionally, BSS shows a strong correlation with the performance of LVMs on two clinical tasks: information retrieval and survival prediction.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1.The paper is well-structured and clearly presented, which significantly improves its readability.\n2.The introduction of the Boltzmann Semantic Score (BSS) is an innovative approach inspired by state space modeling, providing a fresh perspective on evaluating the semantic capabilities of LVMs in medical image processing.\n3.The experiments demonstrate significant correlations between BSS and performance on the clinical tasks of information retrieval and survival prediction. Additionally, the experiments show LLMs' capabilities in these two key tasks and provide a quantitative comparison of LLM consistency. This consistency further supports BSS as an effective metric for evaluating the semantic capabilities of LVMs.\n\n### Weaknesses\n\n1.The computational complexity of BSS may be high in practical applications, particularly when applied to large-scale datasets.\n2.While the experiments show strong performance of BSS in the information retrieval task, its correlation with survival prediction is weaker. This may indicate that BSS lacks robustness across different types of tasks, especially in more complex medical applications. Therefore, its effectiveness as a general semantic metric remains to be further validated.\n3.The experiments focus on the tasks of information retrieval and survival prediction, but these tasks may differ in nature from other potential tasks. The consistency of LLMs and the effectiveness of BSS in other semantic tasks require further experimental validation across a broader range of tasks.\n4.The paper focuses on evaluating the semantic capabilities of existing LVMs, but it lacks concrete suggestions on how to improve their semantic performance. Although the limitations of LVMs are highlighted, there is little discussion on how to optimize or modify their architectures to overcome these shortcomings.\n\n### Questions\n\n1.Could the authors suggest ways to optimize BSS for large-scale datasets, or clarify if any tests on smaller subsets were conducted for comparative analysis?\n2. Since BSS performs better on information retrieval than survival prediction, could the authors elaborate on the reasons for this difference? Is there evidence BSS might generalize to other medical tasks?\n3. The paper notes limitations in LVMs' semantic capabilities. Do the authors have ideas on potential architectural or training adjustments that might address these limitations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 2",
        "reviewer": "human",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 2,
          "constructiveness": 3,
          "stance": 4,
          "source": "ai",
          "comment": "To the third minor comment (The choice of k‚ÄØ=‚ÄØ5 in k‚ÄëNN graphs is arbitrary and insufficiently analyzed), the authors have analyzed the Boltzmann Semantic Score for each dataset for all `k' in {1, 3, 5, 20, 50} in the supplementary materials. \n\nThe `‚Äî' is commonly used in an AI review."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Boltzmann Semantic Score (BSS)*, a novel semantic metric inspired by state space modeling, designed to evaluate the semantic capabilities of large vision models (LVMs) in medical image processing. The study presents experimental evidence showing that LVMs have limited semantic understanding and that BSS correlates with model performance on two key clinical tasks‚Äîinformation retrieval and survival prediction. The paper is clearly written, well-organized, and generally accessible.\n\n**Major Comments**  \n1. **Computational Complexity** ‚Äì The proposed BSS may be computationally intensive for real-world applications, particularly on large-scale datasets. Further clarification on computational feasibility or optimization approaches would strengthen the work.  \n2. **Task Robustness** ‚Äì Although BSS correlates well with performance in information retrieval, its weaker correlation with survival prediction suggests limited robustness across task types. The metric‚Äôs generalizability to more complex or diverse medical tasks remains uncertain.  \n3. **Experimental Scope** ‚Äì The evaluation is limited to two specific tasks. The consistency of LVMs and the validity of BSS across broader medical or semantic tasks should be verified through additional experiments.  \n4. **Lack of Improvement Strategies for LVMs** ‚Äì While the paper highlights limitations in LVM semantic performance, it provides little guidance on methods to enhance these models. Suggesting potential architectural or training-based improvements could enhance the paper‚Äôs practical impact.\n\n**Minor Comments**  \n- Consider clarifying whether BSS was tested on smaller subsets for computational comparison.  \n- Further discuss possible reasons for differences in BSS performance between the two evaluated tasks and whether the metric could extend to other medical domains.  \n- No ethical issues are identified in this study.\n\n**Summary Paragraph**  \nOverall, the paper presents a clear and well-structured contribution through a novel metric for assessing semantic capability in LVMs. The concept of BSS is original and empirically supported, yet concerns remain regarding computational scalability, robustness across different task domains, and limited discussion of how findings could inform LVM improvement.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is innovative and clearly presented but requires broader validation, computational analysis, and discussion of LVM improvement strategies before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes the Boltzmann Semantic Score (BSS) as a novel metric to evaluate the semantic performance of latent visual models (LVMs) by leveraging large language models (LLMs). The idea behind using BSS is to quantify how well the visual representations align with text expert annotations. The authors show that BSS could be used as a measure of semantic similarity for LVMs. This paper include applications to pathology reports and whole slide images from  The Cancer Genome Atlas (TCGA), a large publicly available cancer genome dataset. Evaluation on various tasks such as information retrieval and survival prediction is included. This paper suggests high correlations for certain cancer between BSS and performance in both survival prediction and information retrieval.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n‚Ä¢\tBoltzmann Semantic Score is a novel approach to evaluate the semantic perspective of LVMs.\n‚Ä¢\tThe work leverages a large dataset (TCGA), and experiments are performed on several benchmarks.\n‚Ä¢\tThe work provides interesting insights on the model performance using BSS based on observed results.\n‚Ä¢\tHigh correlation between BSS and two downstream tasks i.e information retrieval and survival prediction, highlighting the significance of the results.\n‚Ä¢\tInteresting experiments on clinical tasks showing correspondence between LLMs and patient survival.\n\n### Weaknesses\n\n‚Ä¢\tThe mathematics for the explanation of the Boltzmann Score and its application is rather heavy. A more concise and clearer explanation would enable to understand better the intuition behind the usefulness of BSS as an evaluation metric for LVMs.\n‚Ä¢\tThe authors could develop more on the clinical implication and real-word use of BSS in decision-making.\n‚Ä¢\tSome insights are provided to explain the differences between LVMs and LLMs performance, but the paper could investigate more thoroughly those differences and inherent variations.\n‚Ä¢\tA discussion of the limitations of this work in terms of generalization under different contexts is lacking.\n\n### Questions\n\n‚Ä¢\tDid you visualize the semantic similarity and qualitatively assess the use of BSS as evaluation metric?\n‚Ä¢\tHow reliable is Boltzmann Semantic Score ? \n‚Ä¢\tWhat preprocessing was applied to the medical reports?\n‚Ä¢\tCould you explain the differences observed in Table 3 a) for the two-sided Pearon's Correlation Test? \n‚Ä¢\tWhat is the effect of bias originating from the datasets ?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Boltzmann Semantic Score (BSS)*, a new metric intended to assess the semantic performance of latent visual models (LVMs) through the use of large language models (LLMs). The goal is to quantify the degree of alignment between visual representations and expert text annotations. The study applies BSS to pathology reports and whole slide images from The Cancer Genome Atlas (TCGA), evaluating its relationship to downstream tasks such as information retrieval and survival prediction. The authors report high correlations between BSS and performance outcomes in certain cancer types, suggesting the metric captures meaningful semantic aspects of model behavior. Overall, the work is original and addresses an interesting question, but the presentation and depth of analysis could be improved for clarity and interpretability.  \n\n**Major Comments**  \n1. The mathematical explanation of the Boltzmann Score and its implementation is dense and difficult to follow. A more streamlined and intuitive discussion would help readers grasp the motivation and practical application of BSS.  \n2. The manuscript would benefit from a more developed discussion of the clinical relevance and potential real-world utility of BSS in decision-making contexts.  \n3. While the paper mentions observed differences between LVM and LLM performance, a deeper analysis of these discrepancies and their underlying causes is needed to strengthen the interpretation of results.  \n4. The generalization of BSS across different datasets or clinical contexts is not discussed, leaving uncertainty about its broader applicability.  \n\n**Minor Comments**  \n- Clarify whether qualitative visualization or semantic similarity plots were used to assess BSS.  \n- Provide details on preprocessing of medical reports.  \n- Explain the results in Table‚ÄØ3(a) for the two-sided Pearson correlation test.  \n- Address possible dataset biases influencing the correlations.  \n\n**Summary Paragraph**  \nThe study proposes an innovative metric with promising empirical support and potential significance for evaluating semantic alignment in LVMs. Strengths include the novelty of BSS, its grounding in a large medical dataset, and the reported correlation to downstream clinical tasks. However, the clarity of mathematical formulation, discussion of clinical implications, and examination of limitations need refinement. Addressing these issues would substantially improve the work‚Äôs accessibility and robustness.  \n\n**Decision Recommendation**  \n**Major Revision.** The concept is strong and relevant, but methodological clarity and broader contextual discussion must be strengthened before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a semantic metric, BBS, to evaluate LVMs from a medically semantic perspective.\n\nThe paper also leverages LLMs and a large and collective database of medical reports across more than 30 cancer types that represent more than 9,500 patients and it also establishes a baseline of LLMs' performance in two large-scale digital pathology tasks.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n* Originality: Noval evaluation metric which evaluates the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. \n\n* Quality: Extensive experiments including experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, comparison of seven LVMs with BSS, and two correlation analyses between BSS and performance in two downstream tasks.\n\n* Clarity: Well-painted figures and clear formulas.\n\n* Significance: Well-designed metric is important for the community, especially for the evaluation of latent embedding space.\n\n### Weaknesses\n\nI cannot find significant Weaknesses in this paper.\n\n### Questions\n\nWould it be possible to make the BSS a training loss to guide and supervise vision encoder embeddings to align with the strong LLM embeddings? Will BSS have additional advantages over contrastive learning loss, such as smaller batch size requirements?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai",
          "comment": "The code link is a placeholder, and key implementation aspects (e.g., pseudo code) are insufficiently described, limiting reproducibility."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a semantic evaluation metric, BBS, designed to assess latent visual models (LVMs) from a medical semantic perspective by leveraging large language models (LLMs) and a substantial corpus of medical reports. The study examines data from over 9,500 patients across more than 30 cancer types, establishing a baseline of LLM performance on two large-scale digital pathology tasks. Overall, the paper is clear, provides solid experimental support, and contributes a novel perspective on evaluating embedding spaces in medical imaging.\n\n**Major Comments**  \n1. **Novelty and Contribution**: The approach of measuring semantic alignment between LVM embeddings and LLM embeddings derived from medical reports is innovative and may represent a meaningful step toward a new class of evaluation metrics for medical image understanding.  \n2. **Experimental Validation**: The authors have conducted extensive experiments involving 32 Cancer Genome Atlas datasets, five state-of-the-art LLMs, and seven LVMs, along with two correlation analyses between the BBS metric and downstream task performance. The comprehensive evaluation supports the proposed metric‚Äôs robustness.  \n3. **Future Work Question**: The reviewer raises an open question about whether the BBS metric could be extended beyond evaluation‚Äîspecifically, whether BBS could be adapted as a training loss to guide visual encoder embeddings to align with LLM embeddings, and whether it would offer advantages such as reduced batch size requirements compared to traditional contrastive losses.\n\n**Minor Comments**  \n- Figures and formulas are well-presented and easy to follow.  \n- The text is clear and well-written; no major issues with presentation were identified.  \n- No ethical concerns are noted.  \n\n**Summary Paragraph**  \nThis paper is recognized for its originality, clear presentation, and thorough experimental design. The metric appears well-motivated and potentially significant for the community, especially for evaluating latent embedding spaces in medical image models. The reviewer found no major weaknesses and viewed the contribution as solid and experimentally supported.\n\n**Decision Recommendation**: **Accept**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI:**  \n‚úÖ The manuscript sits squarely within the methodological scope of IEEE TMI. It introduces a new metric‚Äî**the Boltzmann Semantic Score (BSS)**‚Äîfor evaluating large vision models (LVMs) by leveraging the embedding spaces of large language models (LLMs) applied to paired image‚Äìtext data from medical pathology. The paper focuses on foundational mathematical formulation, algorithmic definition, and quantitative validation using large-scale medical datasets (TCGA), rather than an application case study. The contribution is methodological and generalizable to medical imaging domains.\n\n2. **Novelty & Contribution Level:**  \nModerate to high novelty. BSS introduces a formal probabilistic framework (based on Boltzmann factors and state-space analogy) for quantifying cross-modal semantic alignment between medical image and text embeddings. Unlike prior comparative studies of LVM‚ÄìLLM correlations, this work defines a differentiable and theoretically motivated metric. The notion of ‚Äúsemantic similarity‚Äù between modalities grounded in energy-based modeling is new within medical imaging evaluation. The theoretical innovation is meaningful, though empirical validation still relies on standard dataset/architecture comparisons.\n\n3. **Technical and Experimental Rigor:**  \nThe manuscript is strong in theoretical exposition, with clear equations and algorithmic outlines. Experimental design is extensive: five LLMs, seven LVMs, 32 TCGA cohorts, and two downstream tasks (retrieval and survival). The statistical methods (Pearson correlation tests, cross-validation, standard retrieval and survival evaluation metrics) are appropriate. Reproducibility is aided by public code and detailed appendices.  \nPotential gaps:  \n- The description of hyperparameters and data preprocessing, while extensive in the appendix, could still be summarized in the main text for clarity.  \n- Limited ablation on the sensitivity of BSS hyperparameters (e.g., *k* selection, influence of LLM choice).  \n- The baseline comparison to existing semantic or alignment metrics (e.g., CCA-based measures, mutual information) is missing.  \n\n4. **Clarity and Presentation:**  \nVery well organized and clearly written for a technically complex topic. Figures are numerous and generally informative. The mathematical narrative (from intuitive derivation to formal algorithm) is sound, though slightly dense in mid-section 3. Minor language and formatting inconsistencies (e.g., spacing, figure callouts). Overall readability is high.\n\n5. **Ethical and Reproducibility Compliance:**  \nDataset usage (public TCGA), de-identification, and preprocessing are transparently described. Code availability is promised. Ethical statements about patient privacy and bias are explicitly included in the appendix. The study meets TMI reproducibility and compliance expectations.\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \nThe authors propose the **Boltzmann Semantic Score (BSS)**, a theoretically derived cross-modal metric to evaluate the semantic alignment between visual features from large vision models (LVMs) and textual encodings from large language models (LLMs) in medical imaging. Using TCGA pathology images and reports, they validate BSS as a measure of semantic correspondence that correlates with downstream task performance (retrieval, survival prediction). The study argues that BSS provides a quantitative, scalable alternative to expert-based qualitative evaluations.\n\n2. **Strengths**  \n- Clear methodological innovation linking statistical mechanics with multimodal embedding evaluation.  \n- Large-scale empirical validation across dozens of cancer datasets.  \n- Transparent code/data availability and detailed experiments.  \n- Conceptually bridges LLM-based reasoning with visual representation quality assessment in medicine.  \n- Potential extensibility beyond pathology to other imaging‚Äìtext modalities.  \n\n3. **Weaknesses**  \n- Limited comparative analysis with existing quantitative alignment metrics (e.g., cosine similarity baselines, cross-modal contrastive losses).  \n- The connection between BSS magnitude and specific model behaviors is descriptive rather than mechanistically analyzed.  \n- Writing occasionally conflates ‚Äúsemantic‚Äù with ‚Äúclinical‚Äù relevance‚Äîclarify interpretability versus outcome prediction.  \n- Some figures (e.g., dense multipanel charts) could be simplified for readability.  \n- Statistical significance is shown for correlations but effect size interpretation is modest.\n\n4. **Major Comments**  \n- **Contextual Comparison:** Include comparison or discussion against established metrics for modality alignment (e.g., CCA, linear alignment, information-theoretic similarity), situating BSS within quantitative literature.  \n- **Sensitivity and Robustness:** Provide a brief ablation or analytic justification of the choice of *k* in neighborhood graphs and of the Boltzmann formulation constants.  \n- **Interpretability / Usability:** Discuss what a specific BSS value implies operationally‚Äîhow practitioners might use it to compare or tune LVMs.  \n- **Generalization / Future Work:** Summarize in main text (not only appendix) how BSS could inform model training (e.g., as a loss), since this is important for TMI‚Äôs methodological emphasis.  \n\n5. **Minor Comments**  \n- Correct minor typographical spacing (‚ÄúLVMs,‚Äù ‚ÄúBoltzmann‚Äôs,‚Äù etc.).  \n- Improve axis labeling readability in Figs.‚ÄØ5‚Äì8.  \n- Condense the introduction by merging some background on foundation models.  \n- Ensure consistent citation formatting (some ‚Äúet‚ÄØal.‚Äù listings inconsistent).  \n\n6. **Overall Recommendation**  \n**Major Revision.**  \nThe paper introduces an interesting and potentially impactful metric, with sound theory and strong experiments. However, to meet TMI‚Äôs threshold for methodological completeness, the authors should (a) include direct comparison with alternative semantic similarity metrics, (b) perform a limited ablation to confirm robustness, and (c) clarify interpretability and practical relevance. Addressing these points would elevate the contribution from exploratory to mature methodological advancement.\n\n7. **Confidence Level:**‚ÄØ**5‚ÄØ/‚ÄØ5** ‚Äî reviewer is confident in evaluation given background in imaging‚ÄìAI methodology.\n\n---\n\n**Summary Statement for Editors:**  \nThe submission offers a novel, theoretically grounded metric that could become a useful tool for assessing and aligning multimodal medical foundation models. It is methodologically innovative, scientifically rigorous, and well executed. With additional comparative and robustness analyses, it is likely to reach the publication standard of IEEE‚ÄØTMI.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the **Boltzmann Semantic Score (BSS)**, a probabilistic metric for quantifying semantic alignment between visual embeddings from large vision models (LVMs) and textual embeddings from large language models (LLMs) using paired image‚Äìtext data in medical pathology. The work formulates BSS through an energy-based framework inspired by Boltzmann statistics and validates it on large-scale public datasets (TCGA) across multiple models and downstream tasks. The study emphasizes methodological development rather than application, offering a theoretically grounded tool for evaluating multimodal medical models. Overall, the paper is clearly written, technically sound, and well organized, though several methodological and comparative gaps remain.  \n\n**Major Comments**  \n1. **Comparative Analysis:** The paper lacks direct comparison with established quantitative alignment metrics such as canonical correlation analysis (CCA), linear alignment, mutual information, or cosine similarity baselines. Including these would contextualize the novelty and empirical performance of BSS.  \n2. **Sensitivity and Robustness:** The influence of hyperparameters (e.g., neighborhood size *k*, constants in the Boltzmann formulation, and choice of LLMs) should be analyzed more extensively. A limited ablation or justification would strengthen confidence in the robustness of the metric.  \n3. **Interpretability and Practical Utility:** The manuscript could better articulate how specific BSS values should be interpreted operationally‚Äîhow practitioners might apply the score to compare or optimize model configurations.  \n4. **Generalization and Application:** Discussion of how BSS might be incorporated into model training (e.g., as a loss function) should be summarized in the main text, not relegated to the appendix.  \n5. **Presentation of Experimental Details:** While appendices are comprehensive, essential information about hyperparameters and data preprocessing should be condensed in the main body for clarity.  \n\n**Minor Comments**  \n- Minor typographical and spacing corrections are needed (‚ÄúLVMs,‚Äù ‚ÄúBoltzmann‚Äôs,‚Äù etc.).  \n- Improve readability of dense multipanel figures (e.g., Figs.‚ÄØ5‚Äì8) through clearer axis labeling.  \n- Clarify distinction between ‚Äúsemantic‚Äù and ‚Äúclinical‚Äù relevance to avoid interpretational ambiguity.  \n- Streamline introductory background and ensure consistent citation formatting.  \n\n**Summary Paragraph**  \nThe study presents a theoretically innovative and potentially influential metric connecting statistical mechanics with multimodal medical representation evaluation. Strengths include mathematical rigor, large-scale benchmarking, and transparent reproducibility practices. However, the contribution remains partially limited by the absence of explicit baseline comparisons, limited sensitivity analysis, and incomplete discussion of interpretability. Addressing these issues would transform the work from an exploratory demonstration into a fully substantiated methodological framework.  \n\n**Decision Recommendation**  \n**Major Revision.** The work is promising and well executed but requires additional comparative experiments, robustness analysis, and clearer interpretative framing before being ready for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces the Boltzmann Semantic Score (BSS), a novel metric to evaluate Large Vision Models (LVMs) using Large Language Models (LLMs) as semantic references. The method addresses the question of whether LVMs extract medically relevant features similar to human experts by comparing structural similarity between LVM and LLM embedding spaces. The authors first establish LLM baselines on 32 TCGA datasets using five state-of-the-art LLMs for information retrieval and survival prediction tasks. They then propose BSS, inspired by statistical mechanics and Boltzmann distributions, to measure semantic alignment between vision and text encodings. The method constructs k-nearest neighbor graphs in both modalities and computes semantic scores based on matching and non-matching states. Experiments on seven LVMs show generally low BSS values, indicating poor semantic capability. The authors demonstrate statistically significant correlations between BSS and downstream task performance, and show substantial consensus among LLMs in ranking LVMs using Cohen's kappa scores.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - The transition from Boltzmann factor in Equation 2 to the constant ‚àöd1 replacing kT lacks proper justification (Section 3.3, Page 5), making the physical analogy questionable\n  - The definition of sets A and D in Equations 4-5 (Page 5) contains unclear notation where Li = Lj relationships are not precisely defined\n  - Algorithm 1 (Page 19) lacks mathematical rigor in describing how \"corresponding nodes\" are identified in the observer graph\n  - The normalization in Equation 6 (Page 5) assumes non-zero denominators but provides no handling for edge cases\n\n‚Ä¢ **Experimental design and validation limitations**\n  - BSS correlations with downstream tasks show high variability across datasets (Table 3a-3b, Page 9) with some non-significant results, questioning the metric's reliability\n  - The choice of k=5 neighbors (Figures 5-6, Pages 8-9) appears arbitrary with insufficient ablation studies on this critical hyperparameter\n  - Survival prediction experiments use only 8 cancer types (Table 2, Page 8) while information retrieval uses all 32, creating inconsistent evaluation scope\n  - The supervised pooling methodology using AbMIL (Section J.2, Page 19) introduces task-specific bias that may confound BSS measurements\n\n‚Ä¢ **Methodological assumptions and theoretical foundation concerns**\n  - The fundamental assumption that LLM representations serve as ground truth for medical semantics lacks empirical validation beyond retrieval tasks (Section 3.1, Page 3)\n  - The physical analogy to statistical mechanics appears forced, with energy differences defined as L2 distances without theoretical justification (Equation 2, Page 5)\n  - The method assumes semantic similarity should correspond to spatial proximity in embedding spaces, which may not hold across different model architectures\n  - No comparison with existing semantic evaluation metrics or human expert validation is provided to establish BSS validity\n\n‚Ä¢ **Reproducibility and generalization challenges**\n  - Critical implementation details for graph construction and nearest neighbor computation are missing from the main text\n  - The consensus analysis (Figure 7, Page 9) shows moderate agreement for some LLM pairs, suggesting potential inconsistencies in BSS computation\n  - Dataset-specific performance variations (Figures 5-6, Pages 8-9) raise questions about BSS generalizability across different medical domains\n  - The method's dependence on specific LLM architectures may limit its applicability as foundation models evolve\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical rigor and theoretical foundation**\n  - Provide rigorous justification for the choice of ‚àöd1 as the temperature equivalent, including sensitivity analysis across different dimensionalities\n  - Clarify the mathematical definition of matching states in sets A and D with explicit algorithmic descriptions\n  - Add formal complexity analysis and convergence properties of the BSS computation\n  - Include theoretical analysis of BSS behavior under different embedding space properties\n\n‚Ä¢ **Enhance experimental validation and consistency**\n  - Conduct comprehensive ablation studies on the k parameter across multiple values and datasets to justify the choice of k=5\n  - Expand survival prediction experiments to all 32 cancer types for consistent comparison with retrieval tasks\n  - Include comparison with existing semantic similarity metrics and human expert evaluations as validation benchmarks\n  - Separate analysis of supervised vs unsupervised pooling effects on BSS to isolate methodological confounds\n\n‚Ä¢ **Validate core methodological assumptions**\n  - Provide empirical evidence that LLM representations truly capture medical semantics beyond simple keyword matching\n  - Compare BSS with direct human expert assessments of LVM semantic quality on a subset of cases\n  - Investigate the relationship between embedding space geometry and semantic meaning across different model architectures\n  - Include analysis of failure cases where BSS may not reflect true semantic alignment\n\n‚Ä¢ **Improve reproducibility and broader applicability**\n  - Provide complete algorithmic specifications and open-source implementation for all BSS computation steps\n  - Address the consensus variability issue by investigating sources of disagreement between different LLMs\n  - Test BSS stability across different medical domains and imaging modalities beyond pathology\n  - Develop guidelines for selecting appropriate reference LLMs and handling cases where LLM consensus is low",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Boltzmann Semantic Score (BSS)*, a new metric for evaluating Large Vision Models (LVMs) using Large Language Models (LLMs) as semantic references. The approach aims to determine whether LVMs capture medically relevant semantics comparable to human experts by measuring structural alignment between LVM and LLM embeddings. The study establishes LLM baselines across 32 TCGA datasets and proposes the BSS metric, inspired by Boltzmann distributions, to quantify semantic coherence between modalities. Experimental results on seven LVMs show generally low BSS scores but significant correlations with downstream task performance. The paper is ambitious and explores an important question in multimodal model evaluation but raises clarity, methodological, and validation concerns that limit its current impact.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity** ‚Äì Several equations lack justification or precise definitions, notably the transition in Equation‚ÄØ2 using ‚àöd‚ÇÅ as a temperature analogue, unclear specification of sets‚ÄØA and‚ÄØD, and ambiguous node correspondence in Algorithm‚ÄØ1. Normalization in Equation‚ÄØ6 omits edge-case handling.  \n2. **Experimental design and validation** ‚Äì Reported correlations between BSS and downstream tasks vary substantially, undermining robustness. The fixed choice of *k‚ÄØ=‚ÄØ5* lacks ablation studies, and inconsistent dataset coverage between retrieval and survival experiments complicates interpretation. The inclusion of supervised pooling (AbMIL) may introduce bias.  \n3. **Methodological assumptions and theoretical grounding** ‚Äì The assumption that LLM embeddings serve as accurate medical semantic references is insufficiently supported. The analogy to statistical mechanics and use of energy-based L‚ÇÇ distances are weakly justified. The premise that semantic similarity equates to embedding proximity may not generalize, and comparisons to existing metrics or human expert evaluations are missing.  \n4. **Reproducibility and generalization** ‚Äì The paper omits detailed implementation procedures for graph construction and neighbor computation. Variability in consensus analyses and dataset-specific results raise concerns about consistency. Dependence on specific LLMs may further limit transferability.\n\n**Minor Comments**  \n- Clarify notation and symbol definitions in Equations‚ÄØ4‚Äì5.  \n- Improve figure referencing and ensure all parameters are described in captions.  \n- Provide explicit dataset counts and sample sizes where mentioned.  \n\n**Summary Paragraph**  \nOverall, the work presents an innovative direction for assessing LVM semantic capability through cross-modal alignment with LLMs. Its strengths lie in conceptual novelty and exploration of semantic metrics in medical imaging. However, weaknesses include limited theoretical justification, incomplete mathematical rigor, inconsistent evaluation design, and insufficient validation of assumptions. Strengthening the theoretical basis, broadening empirical evaluation, and enhancing reproducibility measures would substantially improve the study‚Äôs credibility and general impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The concept is promising but currently requires stronger methodological foundation, theoretical coherence, and experimental validation before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### Comprehensive Peer Review\n\n#### The manuscript presents a novel metric, the Boltzmann Semantic Score (BSS), designed to evaluate the semantic capability of Large Vision Models (LVMs) in medical imaging contexts, specifically in digital pathology. By leveraging Large Language Models (LLMs) and a large-scale dataset (TCGA), the authors demonstrate that BSS can effectively quantify LVMs' ability to extract medically relevant features, showing a correlation with performance in downstream tasks such as information retrieval and survival prediction. The study aims to address the limitations of current evaluation methods, which often rely on small-scale expert evaluations, and proposes BSS as a more systematic and objective alternative.\n\n####\n\n## Major Comments\nStrengths\n1. Novelty and Relevance: The introduction of BSS as a new metric for evaluating LVMs is a significant contribution to the field of medical imaging, particularly in digital pathology. The study's focus on enhancing the semantic capabilities of LVMs is highly relevant and addresses a critical gap in current evaluation methods.\n2. Comprehensive Evaluation: The use of multiple LLMs and LVMs across a large dataset (TCGA) adds robustness to the findings. The authors' approach to comparing the performance of LLMs and LVMs across various tasks demonstrates a thorough experimental setup.\n3. Clinical Relevance: The correlation of BSS with downstream clinical tasks, such as information retrieval and survival prediction, underscores the practical importance of the proposed metric. This alignment with clinical outcomes strengthens the manuscript's argument for the utility of BSS.\n4. Commitment to Open Science: The authors‚Äô commitment to releasing their code and processed data on GitHub enhances the credibility and potential impact of the research. Detailed documentation and the inclusion of algorithms and datasets in the appendix further support reproducibility.\n\nLimitations\n1. Complexity and Accessibility: The manuscript introduces several technical terms and concepts (LVMs, LLMs, BSS) without providing detailed definitions, which could confuse readers unfamiliar with these terms. The abstract, in particular, lacks clarity in explaining these concepts, potentially limiting the accessibility of the research.\n2. Specificity in Resources: While the authors commit to releasing their code and data, the lack of specific timelines and URLs makes it challenging for readers to access these resources promptly. Additionally, the reference to \"section E\" in the appendix is vague, requiring clarification.\n3. Formatting and Consistency: The references section exhibits inconsistencies in formatting, including the absence of DOIs and incomplete citations. These issues could detract from the manuscript's overall professionalism and readability.\n\n####\n\n## Minor Comments\n1. Abstract Clarity: Provide brief definitions or explanations for key terms (LVMs, LLMs, BSS) within the abstract to enhance accessibility.\n2. Reproducibility Details: Specify timelines and URLs for accessing the released code and data to facilitate immediate reproducibility.\n3. Reference Standardization: Ensure all citations follow a consistent style guide and include DOIs where possible to improve accessibility and reliability.\n4. Appendix Organization: Group related sections together in the appendix for better readability. Clarify transitions between theoretical explanations and empirical results.\n\n####\n\n## Summary Paragraph\nEvaluating the manuscript against the TMI editorial criteria:\n\n- Significance: The manuscript addresses a significant gap in the evaluation of LVMs in medical imaging, proposing a novel metric (BSS) that aligns with clinical outcomes.\n- Innovation: The introduction of BSS as a new metric for assessing semantic capabilities of LVMs is innovative and offers a promising direction for future research.\n- Evaluation: The comprehensive experimental setup involving multiple LLMs and LVMs across a large dataset (TCGA) demonstrates strong methodological rigor. The correlation with downstream clinical tasks adds to the validity of the proposed metric.\n- Reproducibility: The authors' commitment to open science and detailed documentation in the appendix enhances reproducibility. However, improvements in resource specificity and citation formatting are needed.\n\nOverall, the manuscript is a valuable contribution to the field, with significant potential for impact. Addressing the minor comments and improving the accessibility of key concepts and resources will further strengthen the manuscript.\n\n####\n\n## Decision Recommendation\nMinor Revision\n\nThe manuscript is recommended for minor revision to address the minor comments and enhance the clarity and accessibility of the key concepts and resources. Once these revisions are completed, the manuscript should be suitable for publication in a top-tier medical imaging journal.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 2,
          "specificity": 1,
          "correctness": 4,
          "constructiveness": 2,
          "stance": 1,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel evaluation metric, the *Boltzmann Semantic Score* (BSS), intended to assess the semantic capabilities of Large Vision Models (LVMs) in medical imaging, particularly digital pathology. By integrating Large Language Models (LLMs) and leveraging the TCGA dataset, the authors demonstrate that BSS correlates with performance in downstream clinical tasks such as information retrieval and survival prediction. The study addresses limitations of existing evaluation methods that rely heavily on expert judgments, proposing BSS as a more systematic and objective approach. Overall, the manuscript is well-motivated and presents a potentially important contribution to the evaluation of LVMs in medical imaging.  \n\n---\n\n**Major Comments**  \n\n**Strengths**  \n1. **Novelty and Relevance:** The introduction of BSS as a metric for LVM evaluation represents a meaningful and timely contribution, filling a key methodological gap in digital pathology research.  \n2. **Comprehensive Evaluation:** Multiple LLMs and LVMs are tested on a large-scale dataset (TCGA), supporting the robustness of the findings and the generality of the conclusions.  \n3. **Clinical Relevance:** Demonstrated correlations between BSS and clinically meaningful tasks enhance confidence in the metric‚Äôs practical value.  \n4. **Open Science Practices:** The authors‚Äô intention to release code and processed data on GitHub, supported by detailed documentation and appendices, promotes reproducibility and transparency.  \n\n**Limitations**  \n1. **Accessibility of Technical Concepts:** The manuscript introduces terms such as LVMs, LLMs, and BSS without sufficient definition, limiting accessibility for readers less familiar with these concepts. The abstract is particularly dense and could benefit from clarification.  \n2. **Resource Availability:** Although the authors commit to open release of data and code, no specific timelines or URLs are provided, and references to ‚Äúsection E‚Äù in the appendix are vague.  \n3. **Formatting Consistency:** The references section contains incomplete citations and missing DOIs, reducing coherence and professionalism.  \n\n---\n\n**Minor Comments**  \n1. Define or briefly explain key terms (LVMs, LLMs, BSS) in the abstract.  \n2. Include direct links and expected release timelines for code and datasets.  \n3. Standardize reference formatting and ensure inclusion of DOIs.  \n4. Improve appendix organization by grouping related sections and clarifying transitions.  \n\n---\n\n**Summary Paragraph**  \nThe manuscript makes a significant contribution by proposing an innovative and clinically relevant metric for evaluating semantic performance in LVMs. Its strengths lie in methodological rigor, relevance to clinical outcomes, and commitment to reproducible research. Minor weaknesses relate to limited accessibility of key technical definitions, lack of specificity in resource sharing, and inconsistency in formatting. Addressing these issues will enhance readability and professional presentation.  \n\n---\n\n**Decision Recommendation**  \n**Minor Revision.** The manuscript is suitable for publication pending clarifications on terminology, resource accessibility, and reference formatting.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## BOLTZMANN SEMANTIC SCORE: A SEMANTIC METRIC FOR EVALUATING LARGE VISION MODELS USING LARGE LANGUAGE MODELS\n\n### Summary\n\nThis paper introduces the Boltzmann Semantic Score (BSS), a training-free metric to evaluate the semantic alignment of large vision models (LVMs) in digital pathology by comparing their neighborhood structures to those induced by large language model (LLM) embeddings of paired pathology reports. The authors establish baselines for five LLMs on text-only retrieval and survival prediction across TCGA cohorts, then compute BSS for seven LVMs under unsupervised and supervised pooling, and analyze correlations between BSS and downstream performance as well as inter-LLM consensus via Cohen‚Äôs kappa. The results suggest (i) LLM embeddings capture clinically relevant information linked to patient outcomes, (ii) current LVMs exhibit limited semantic alignment to text-defined structures, and (iii) BSS is positively correlated with downstream retrieval and survival metrics in several cohorts.\n\n### Strengths\n\n- Technical novelty and innovationProposes a simple, training-free metric (BSS) for cross-modal structural alignment that does not require joint training or architectural coupling between LVMs and LLMs.The ‚Äúsecond-order‚Äù degeneracy idea attempts to account for non-matching neighbors by incorporating relative structure within the reference (text) space rather than only 1:1 k-NN overlap.Positions BSS as a semantically grounded alternative to purely qualitative assessments (e.g., attention maps), aiming for scalable, quantitative evaluation.\n- Experimental rigor and validationWide coverage across 32 TCGA datasets for text-only tasks and 8 cohorts for BSS/LVM analyses, with multiple LLMs and LVMs considered.Perturbed-text retrieval experiments help mitigate the concern that LLM performance is driven only by keyword matching, showing retained performance after removing key terms.Correlation analysis between BSS and downstream metrics (retrieval accuracy and survival C-index) provides initial evidence that BSS captures task-relevant structure.Inter-LLM consensus analysis is a thoughtful attempt to quantify agreement across reference encoders, rarely reported in qualitative evaluation settings.\n- Clarity of presentationHigh-level motivation is clear: replace small-sample, subjective expert inspections with scalable, objective semantics inferred from reports.The pipeline components (LLM text embedding, LVM image pooling, k-NN construction, summary statistics) are explained at an intuitive level.\n- Significance of contributionsAddresses an important gap in computational pathology: how to assess whether vision encoders capture human-relevant semantics without training a new multimodal model.If refined, BSS could become a useful diagnostic/evaluation tool during model development and selection, complementing standard downstream task leaderboards.\n\n- Proposes a simple, training-free metric (BSS) for cross-modal structural alignment that does not require joint training or architectural coupling between LVMs and LLMs.\n- The ‚Äúsecond-order‚Äù degeneracy idea attempts to account for non-matching neighbors by incorporating relative structure within the reference (text) space rather than only 1:1 k-NN overlap.\n- Positions BSS as a semantically grounded alternative to purely qualitative assessments (e.g., attention maps), aiming for scalable, quantitative evaluation.\n\n- Wide coverage across 32 TCGA datasets for text-only tasks and 8 cohorts for BSS/LVM analyses, with multiple LLMs and LVMs considered.\n- Perturbed-text retrieval experiments help mitigate the concern that LLM performance is driven only by keyword matching, showing retained performance after removing key terms.\n- Correlation analysis between BSS and downstream metrics (retrieval accuracy and survival C-index) provides initial evidence that BSS captures task-relevant structure.\n- Inter-LLM consensus analysis is a thoughtful attempt to quantify agreement across reference encoders, rarely reported in qualitative evaluation settings.\n\n- High-level motivation is clear: replace small-sample, subjective expert inspections with scalable, objective semantics inferred from reports.\n- The pipeline components (LLM text embedding, LVM image pooling, k-NN construction, summary statistics) are explained at an intuitive level.\n\n- Addresses an important gap in computational pathology: how to assess whether vision encoders capture human-relevant semantics without training a new multimodal model.\n- If refined, BSS could become a useful diagnostic/evaluation tool during model development and selection, complementing standard downstream task leaderboards.\n\n### Weaknesses\n\n- Technical limitations or concernsThe Boltzmann formalism is only loosely justified: mapping ‚Äúenergy‚Äù to Euclidean distance and setting kT = sqrt(d1) is ad hoc; no principled calibration or sensitivity analysis is provided.The ‚Äúsecond-order‚Äù factor b_{i;j|q} uses only distances within the text space; visual distances influence only membership in near(Vq, k), not quantitative scoring, which may under-utilize visual geometry.A key inconsistency: for matching pairs (i, j) ‚àà A with Li = Lj, b_{i;j|q} should equal p_i/p_q (not 1), yet the text states b = 1; this affects the normalization in Eq. (6) and casts doubt on the correctness of the derivation.Lack of invariance considerations: BSS uses L2 distances in high-dimensional spaces; cosine similarity or a learned/estimated temperature might be more appropriate and robust.\n- Experimental gaps or methodological issuesMissing comparisons with established structural similarity baselines (e.g., k-NN overlap/Jaccard, Spearman/Kendall correlation of distance/rank lists, Procrustes alignment, CKA/local-CKA) make it hard to assess whether BSS offers advantages beyond simpler measures.The retrieval evaluation description is ambiguous: at times it reads as label-based retrieval using LLM text embeddings; at others, it appears to involve visual features. The exact query/target setup, train/test separation (leave-one-out vs within-set), and label usage should be clarified to rule out leakage or trivial cues.The inter-LLM agreement on ‚Äúrankings‚Äù is measured using Cohen‚Äôs kappa (for categorical labels), whereas rank correlation (Kendall tau/Spearman) would be more appropriate for ordinal/full ranking data.Multiple-testing correction is absent for extensive correlation analyses (LLMs √ó cohorts √ó tasks), raising the risk of false positives.\n- Clarity or presentation issuesSeveral typographical/formatting artifacts, equation misalignments, and duplicated captions (likely from PDF extraction) hinder readability; the formal definitions of sets A and D and the mapping between non-matching (i, j) pairs require clearer algorithmic specification.The survival analysis setup is not fully detailed (e.g., covariate usage beyond embeddings, how censored data are handled beyond RSF defaults, per-cohort sample sizes after filtering).\n- Missing related work or comparisonsNo direct comparison to representation-similarity approaches (e.g., CKA/local-CKA for cross-modal alignment without training) that target a closely related question.Limited engagement with recent pathology vision‚Äìlanguage alignment works (e.g., PathAlign, TITAN, SlideChat) that explicitly target slide‚Äìtext semantic alignment; these could serve as reference points to contextualize what ‚Äúlow BSS‚Äù means when text‚Äìimage are jointly trained.\n\n- The Boltzmann formalism is only loosely justified: mapping ‚Äúenergy‚Äù to Euclidean distance and setting kT = sqrt(d1) is ad hoc; no principled calibration or sensitivity analysis is provided.\n- The ‚Äúsecond-order‚Äù factor b_{i;j|q} uses only distances within the text space; visual distances influence only membership in near(Vq, k), not quantitative scoring, which may under-utilize visual geometry.\n- A key inconsistency: for matching pairs (i, j) ‚àà A with Li = Lj, b_{i;j|q} should equal p_i/p_q (not 1), yet the text states b = 1; this affects the normalization in Eq. (6) and casts doubt on the correctness of the derivation.\n- Lack of invariance considerations: BSS uses L2 distances in high-dimensional spaces; cosine similarity or a learned/estimated temperature might be more appropriate and robust.\n\n- Missing comparisons with established structural similarity baselines (e.g., k-NN overlap/Jaccard, Spearman/Kendall correlation of distance/rank lists, Procrustes alignment, CKA/local-CKA) make it hard to assess whether BSS offers advantages beyond simpler measures.\n- The retrieval evaluation description is ambiguous: at times it reads as label-based retrieval using LLM text embeddings; at others, it appears to involve visual features. The exact query/target setup, train/test separation (leave-one-out vs within-set), and label usage should be clarified to rule out leakage or trivial cues.\n- The inter-LLM agreement on ‚Äúrankings‚Äù is measured using Cohen‚Äôs kappa (for categorical labels), whereas rank correlation (Kendall tau/Spearman) would be more appropriate for ordinal/full ranking data.\n- Multiple-testing correction is absent for extensive correlation analyses (LLMs √ó cohorts √ó tasks), raising the risk of false positives.\n\n- Several typographical/formatting artifacts, equation misalignments, and duplicated captions (likely from PDF extraction) hinder readability; the formal definitions of sets A and D and the mapping between non-matching (i, j) pairs require clearer algorithmic specification.\n- The survival analysis setup is not fully detailed (e.g., covariate usage beyond embeddings, how censored data are handled beyond RSF defaults, per-cohort sample sizes after filtering).\n\n- No direct comparison to representation-similarity approaches (e.g., CKA/local-CKA for cross-modal alignment without training) that target a closely related question.\n- Limited engagement with recent pathology vision‚Äìlanguage alignment works (e.g., PathAlign, TITAN, SlideChat) that explicitly target slide‚Äìtext semantic alignment; these could serve as reference points to contextualize what ‚Äúlow BSS‚Äù means when text‚Äìimage are jointly trained.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe adoption of a Boltzmann factor exp(‚àíŒîE/kT) with ŒîE defined as L2 distance and kT fixed to sqrt(d1) lacks theoretical grounding. In high dimensions, distance concentration can distort probabilities; a data-driven temperature (e.g., percentiles of distance, median absolute deviation) or cosine distance could improve calibration and interpretability.The second-order factor b_{i;j|q} := (p_i/p_j)¬∑(p_i/p_q) is intended to penalize observer-only neighbors V_j that do not appear among reference neighbors. However, as defined, it ignores visual-space distances beyond membership and relies fully on L-space geometry between Li, Lj, and Lq. Consider incorporating a term reflecting how strongly V_j is favored in visual space (e.g., weight by similarity rank or distance to V_q) to better capture the degree of mismatch.The mismatch for (i, j) ‚àà A: b_{i;j|q} equals p_i/p_q, not 1, unless p_i = p_q. The text explicitly states b = 1, which contradicts Eq. (3). This should be corrected, as it affects normalization in Eq. (6) and may change empirical values of BSS.Definitions of A and D (Eqs. 4‚Äì5) are hard to parse programmatically. The procedure for pairing non-matching sets (i, j) is under-specified (e.g., is i the closest unmatched reference neighbor to Lj, or a fixed index pairing by position?). Please provide a precise algorithm and complexity analysis for building A and D.Invariance and stability: BSS depends on k, distance metric, normalization, and neighbor search implementation. An ablation on k, metric (cosine vs Euclidean), feature normalization (e.g., whitening), and temperature would strengthen claims of robustness.\n- Experimental evaluation assessmentLLM retrieval: The organ-specific and organ-independent protocols need clearer definitions of query/target pools. If ‚Äúcancer type‚Äù is the retrieval label, the evaluation might reflect class clustering rather than nuanced semantic retrieval. Clarify whether retrieval is leave-one-out over the full corpus of reports and whether any splitting avoids trivial self-retrieval or near-duplicate leakage.Perturbation tests are valuable; however, more systematic perturbations (e.g., selectively removing micro- vs macro- sections, shuffling sentence order, masking site identifiers) could isolate which parts of pathology reports primarily drive LLM performance.Survival prediction with RSF shows promising C-indices for certain cancers; please clarify whether additional clinical covariates were included (or not), the handling of class imbalance/censoring, and whether external validation cohorts were available. Given the field‚Äôs variability, external validation or robustness checks (e.g., site-stratified CV) would materially raise confidence.BSS on LVMs: Reporting only eight cohorts limits generalization; a rationale for cohort selection and an analysis of cohort-specific factors (e.g., morphological signal vs report content mismatch) would help interpret low BSS. Additionally, reporting variance across seeds/folds for AbMIL is important.Statistical testing: The numerous Pearson tests warrant multiple-comparison correction (e.g., Benjamini‚ÄìHochberg FDR). Some p-values in Table 3b appear near 0.96, consistent with non-significance; please mark significant tests post-correction and discuss the overall pattern cautiously.\n- Comparison with related work (using the summaries provided)Training-free alignment methods (e.g., local-CKA and QAP-based matching) explicitly target the same hypothesis‚Äîwhether independently trained vision and language encoders share latent semantics‚Äîand provide quantitative baselines. Including CKA/local-CKA or simpler k-NN-overlap metrics as baselines would contextualize BSS and demonstrate its added value.Recent WSI‚Äìtext alignment frameworks (PathAlign, TITAN, SlideChat) show that slide‚Äìtext semantics can be brought into closer concordance when jointly trained; their presence suggests that ‚Äúlow BSS‚Äù for single-modality SSL vision encoders may reflect absence of cross-modal supervision rather than fundamental semantic deficiency. A useful experiment would compare BSS for (i) vision-only SSL encoders vs (ii) vision‚Äìlanguage aligned encoders (where available) to test whether BSS reflects known improvements from multimodal alignment.QwenCLIP highlights the value of dedicated LLM embedding models for long clinical text. Given long TCGA reports, comparing BSS computed with an embedding-oriented LLM (or biomedical sentence encoders) versus generative LLMs used as encoders could be informative and might increase inter-LLM agreement and stability.\n- Discussion of broader impact and significanceThe central question‚Äîdo vision encoders capture clinically meaningful semantics?‚Äîis highly relevant. However, pathology reports contain information not fully visible in WSIs (e.g., gross descriptions, staging, ancillary tests). Low BSS might thus partially reflect modality mismatch. An analysis restricting reports to microscopy sections (when available) or projecting onto a controlled set of morphological concepts could better isolate visual semantics.The consensus analysis is a practical contribution. Still, rank correlation would be more appropriate than kappa for ranking agreement. If high agreement persists with Kendall tau/Spearman, it strengthens the claim that BSS-based rankings are not idiosyncratic to one LLM.If refined (baselines, calibration, invariance), BSS could provide a domain-agnostic template for evaluating representation alignment across modalities, potentially beyond medicine.\n\n- The adoption of a Boltzmann factor exp(‚àíŒîE/kT) with ŒîE defined as L2 distance and kT fixed to sqrt(d1) lacks theoretical grounding. In high dimensions, distance concentration can distort probabilities; a data-driven temperature (e.g., percentiles of distance, median absolute deviation) or cosine distance could improve calibration and interpretability.\n- The second-order factor b_{i;j|q} := (p_i/p_j)¬∑(p_i/p_q) is intended to penalize observer-only neighbors V_j that do not appear among reference neighbors. However, as defined, it ignores visual-space distances beyond membership and relies fully on L-space geometry between Li, Lj, and Lq. Consider incorporating a term reflecting how strongly V_j is favored in visual space (e.g., weight by similarity rank or distance to V_q) to better capture the degree of mismatch.\n- The mismatch for (i, j) ‚àà A: b_{i;j|q} equals p_i/p_q, not 1, unless p_i = p_q. The text explicitly states b = 1, which contradicts Eq. (3). This should be corrected, as it affects normalization in Eq. (6) and may change empirical values of BSS.\n- Definitions of A and D (Eqs. 4‚Äì5) are hard to parse programmatically. The procedure for pairing non-matching sets (i, j) is under-specified (e.g., is i the closest unmatched reference neighbor to Lj, or a fixed index pairing by position?). Please provide a precise algorithm and complexity analysis for building A and D.\n- Invariance and stability: BSS depends on k, distance metric, normalization, and neighbor search implementation. An ablation on k, metric (cosine vs Euclidean), feature normalization (e.g., whitening), and temperature would strengthen claims of robustness.\n\n- LLM retrieval: The organ-specific and organ-independent protocols need clearer definitions of query/target pools. If ‚Äúcancer type‚Äù is the retrieval label, the evaluation might reflect class clustering rather than nuanced semantic retrieval. Clarify whether retrieval is leave-one-out over the full corpus of reports and whether any splitting avoids trivial self-retrieval or near-duplicate leakage.\n- Perturbation tests are valuable; however, more systematic perturbations (e.g., selectively removing micro- vs macro- sections, shuffling sentence order, masking site identifiers) could isolate which parts of pathology reports primarily drive LLM performance.\n- Survival prediction with RSF shows promising C-indices for certain cancers; please clarify whether additional clinical covariates were included (or not), the handling of class imbalance/censoring, and whether external validation cohorts were available. Given the field‚Äôs variability, external validation or robustness checks (e.g., site-stratified CV) would materially raise confidence.\n- BSS on LVMs: Reporting only eight cohorts limits generalization; a rationale for cohort selection and an analysis of cohort-specific factors (e.g., morphological signal vs report content mismatch) would help interpret low BSS. Additionally, reporting variance across seeds/folds for AbMIL is important.\n- Statistical testing: The numerous Pearson tests warrant multiple-comparison correction (e.g., Benjamini‚ÄìHochberg FDR). Some p-values in Table 3b appear near 0.96, consistent with non-significance; please mark significant tests post-correction and discuss the overall pattern cautiously.\n\n- Training-free alignment methods (e.g., local-CKA and QAP-based matching) explicitly target the same hypothesis‚Äîwhether independently trained vision and language encoders share latent semantics‚Äîand provide quantitative baselines. Including CKA/local-CKA or simpler k-NN-overlap metrics as baselines would contextualize BSS and demonstrate its added value.\n- Recent WSI‚Äìtext alignment frameworks (PathAlign, TITAN, SlideChat) show that slide‚Äìtext semantics can be brought into closer concordance when jointly trained; their presence suggests that ‚Äúlow BSS‚Äù for single-modality SSL vision encoders may reflect absence of cross-modal supervision rather than fundamental semantic deficiency. A useful experiment would compare BSS for (i) vision-only SSL encoders vs (ii) vision‚Äìlanguage aligned encoders (where available) to test whether BSS reflects known improvements from multimodal alignment.\n- QwenCLIP highlights the value of dedicated LLM embedding models for long clinical text. Given long TCGA reports, comparing BSS computed with an embedding-oriented LLM (or biomedical sentence encoders) versus generative LLMs used as encoders could be informative and might increase inter-LLM agreement and stability.\n\n- The central question‚Äîdo vision encoders capture clinically meaningful semantics?‚Äîis highly relevant. However, pathology reports contain information not fully visible in WSIs (e.g., gross descriptions, staging, ancillary tests). Low BSS might thus partially reflect modality mismatch. An analysis restricting reports to microscopy sections (when available) or projecting onto a controlled set of morphological concepts could better isolate visual semantics.\n- The consensus analysis is a practical contribution. Still, rank correlation would be more appropriate than kappa for ranking agreement. If high agreement persists with Kendall tau/Spearman, it strengthens the claim that BSS-based rankings are not idiosyncratic to one LLM.\n- If refined (baselines, calibration, invariance), BSS could provide a domain-agnostic template for evaluating representation alignment across modalities, potentially beyond medicine.\n\n### Questions for Authors\n\n- Can you clarify the exact definition and computation of sets A and D and the pairing (i, j) for non-matching neighbors? A step-by-step algorithm (with pseudocode) would help resolve ambiguities.\n- For (i, j) ‚àà A with Li = Lj, why is b_{i;j|q} set to 1 instead of p_i/p_q as implied by Eq. (3)? If this is a typo, how does using p_i/p_q change BSS empirically?\n- How sensitive is BSS to the choice of k, distance metric (L2 vs cosine), feature scaling, and the temperature (kT)? Have you explored data-driven or learned temperatures?\n- In the retrieval experiments, what exactly is the query‚Äîan embedded report or a label? Is retrieval leave-one-out over the entire corpus, and how are near-duplicates or multiple reports per patient handled?\n- Did you evaluate BSS against simpler baselines (k-NN overlap/Jaccard, Kendall/Spearman correlation of rankings, CKA/local-CKA, Procrustes alignment)? If not, can you add these to quantify the incremental value of BSS?\n- For consensus, why use Cohen‚Äôs kappa for rankings instead of Kendall tau or Spearman? Do qualitative conclusions hold under rank correlation?\n- In survival prediction, were any clinical covariates used beyond text embeddings? How were site effects handled (e.g., site-stratified CV) and were there external validation cohorts?\n- Given that reports include non-visual information, have you tested BSS using only microscopy sections or pathology-specific concept embeddings to better match the visual modality?\n- Could you provide details for LLM embedding extraction (model mode, pooling strategy, context window limits, truncation policy, tokenization) and whether any domain-specific sentence encoders were tested?\n- What is the runtime and memory profile for computing BSS on 9k+ samples, and how does it scale with larger datasets (e.g., 100k+ reports/slides)?\n\n### Overall Assessment\n\nThe paper tackles an important and underexplored problem: quantitatively assessing whether vision encoders in computational pathology capture clinically meaningful semantics absent multimodal training. The proposed BSS is novel and the experimental scope is commendable, including multi-LLM baselines, perturbation tests, correlations to downstream tasks, and consensus analysis. However, the current formulation suffers from theoretical ambiguities (most notably, an inconsistency in the second-order factor for matched neighbors, heuristic temperature selection, and limited use of visual geometry), missing comparisons to established structural-similarity baselines, and some methodological clarity issues (retrieval protocol details, appropriate rank-agreement metrics, multiple-testing correction). I view this work as a promising direction with substantial potential impact if the metric is better grounded, more thoroughly validated against baselines, and evaluated with modality-appropriate controls (e.g., microscopy-only text). Addressing these concerns would substantially improve confidence in the conclusions and elevate the paper‚Äôs suitability for a top-tier venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Boltzmann Semantic Score (BSS)*, a training-free metric designed to assess semantic alignment between large vision models (LVMs) and large language models (LLMs) in computational pathology. BSS measures correspondence between neighborhood structures in visual and textual embedding spaces, aiming to quantify ‚Äúsemantic alignment‚Äù without joint training. The study benchmarks several LLMs and LVMs across TCGA datasets, examining correlations between BSS and downstream retrieval/survival outcomes, as well as inter-LLM agreement. The paper is well motivated and clearly written overall, offering a novel quantitative approach to a relevant gap in multimodal model evaluation.  \n\n**Major Comments**  \n1. **Conceptual and Theoretical Justification** ‚Äì The Boltzmann formalism is only loosely grounded; defining energy as Euclidean distance and fixing the temperature term heuristically lacks theoretical clarity. The derivation includes an inconsistency where matched pairs are incorrectly normalized (b=1 instead of p_i/p_q). Clarifying and potentially recalibrating this formulation is critical.  \n2. **Use of Visual Geometry** ‚Äì The ‚Äúsecond-order‚Äù factor depends solely on text-space distances, under‚Äêutilizing visual-space structure. Incorporating a weighting that reflects visual similarity could improve representational faithfulness.  \n3. **Experimental Design Clarity** ‚Äì Retrieval evaluations are ambiguously described, particularly concerning whether retrieval is label-based or feature-based and how data splits prevent leakage. Details on survival analysis (covariates, censoring, and cohort sizes) are also insufficient.  \n4. **Comparisons and Baselines** ‚Äì The absence of standard structural similarity baselines (e.g., CKA/local-CKA, rank correlation, k-NN overlap) makes it difficult to assess BSS‚Äôs distinct advantages.  \n5. **Statistical and Methodological Rigor** ‚Äì No multiple-testing correction is applied despite numerous correlations, inflating significance risk. Using Cohen‚Äôs kappa for rank-based agreement is questionable; Kendall or Spearman correlation would be more appropriate.  \n6. **Reproducibility and Specification** ‚Äì The definitions of neighbor sets A and D, pairing strategy, and computational complexity should be precisely described, ideally with pseudocode. The sensitivity of BSS to metric choice, k, and normalization should be evaluated.  \n7. **Missing Contextualization** ‚Äì Related multimodal studies (PathAlign, TITAN, SlideChat) are not discussed; direct comparisons would contextualize what ‚Äúlow BSS‚Äù indicates and test whether BSS tracks improvements from explicit multimodal alignment.  \n\n**Minor Comments**  \n- Several typos, equation misalignments, and duplicated captions hinder readability.  \n- Figures and tables could highlight statistically significant correlations after correction.  \n- Provide details of LLM embedding extraction (model configuration, pooling, truncation).  \n- Include runtime and scalability discussion for computing BSS on large corpora.  \n\n**Summary Paragraph**  \nThe paper offers an innovative evaluation concept for cross-modal semantic alignment in pathology and demonstrates thoughtful experimentation across multiple datasets and encoders. Its novelty and breadth are clear strengths, but theoretical rigor and methodological transparency require improvement. Ambiguities in the mathematical formulation, insufficient baselines, and incomplete statistical controls currently limit interpretability of correlations linking BSS to downstream performance. Resolving these issues and situating BSS against established alignment metrics would significantly enhance reliability and impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is promising and potentially impactful, but it requires stronger theoretical justification, clearer methodological exposition, additional baseline comparisons, and comprehensive statistical validation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical challenge of evaluating whether Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts. Current evaluation methods rely on biased, qualitative approaches with limited expert assessments. The authors propose the Boltzmann Semantic Score (BSS), a novel metric inspired by state space modeling that leverages Large Language Models (LLMs) to evaluate LVMs by comparing their embedding spaces with those of LLMs applied to pathology reports. Through extensive experiments on 32 cancer datasets from The Cancer Genome Atlas (TCGA) using five state-of-the-art LLMs and seven LVMs, the authors first establish LLMs' capability in digital pathology tasks (achieving 0.871 top-1 accuracy in information retrieval) and demonstrate that LLM representations correlate with patient outcomes. Most significantly, they reveal that current LVMs exhibit poor semantic capability (BSS typically below 0.4), indicating a substantial gap between the features extracted by LVMs and the medically relevant information captured in expert pathology reports. The authors further demonstrate statistically significant correlations between BSS and performance in downstream clinical tasks, providing evidence that structural similarity in visual embeddings relates to clinical utility.\n\n## 2. Major and Minor Comments\n\n**Major Strengths:**\n- The paper addresses a fundamental and previously underserved problem in medical AI: how to quantitatively evaluate whether LVMs capture medically relevant knowledge rather than just achieving high accuracy on specific tasks\n- The Boltzmann Semantic Score is a theoretically grounded, novel contribution that provides a systematic, quantitative approach to measuring semantic alignment between visual representations and expert medical knowledge\n- The extensive experimental validation across 32 diverse cancer datasets provides robust evidence that current LVMs have poor semantic capability, revealing an important limitation that has implications for clinical deployment\n- The demonstration that LLM representations correlate with patient outcomes (survival prediction) is significant and novel for the field\n- The finding that multiple LLMs show substantial consensus in evaluating LVMs adds credibility to the proposed metric\n\n**Major Limitations:**\n- The paper does not sufficiently explain why the semantic gap exists (why LVMs fail to capture medical semantics) or provide concrete pathways for addressing this limitation\n- The theoretical foundation of BSS would benefit from more rigorous mathematical justification, particularly regarding the choice of k=5 and the specific formulation of the Boltzmann factors\n- The paper lacks direct comparison with existing LVM evaluation methods to establish the relative advantages of BSS as an evaluation metric\n- The clinical implications of the findings could be strengthened with more specific discussion about how BSS could be integrated into practical LVM development and validation workflows\n\n**Minor Strengths:**\n- The experimental design is comprehensive, evaluating multiple LLMs and LVMs across diverse cancer types with multiple validation strategies\n- The paper includes well-designed visualizations and qualitative examples that effectively illustrate key findings\n- The authors provide detailed implementation information and commit to releasing code and data for reproducibility\n- The study establishes for the first time that LLM-processed pathology reports can predict patient outcomes\n\n**Minor Limitations:**\n- Some sections (particularly the methodology) could be more concise to improve readability\n- The paper would benefit from expanded discussion of related work on model interpretability in medical imaging\n- Figure 1 would benefit from more explicit labeling to clarify the relationship between cancer types and clusters\n- The paper could more explicitly identify which specific aspects of medical knowledge LVMs are failing to capture\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a critical challenge in medical AI with high potential impact. The finding that current LVMs exhibit poor semantic alignment with expert knowledge (BSS typically below 0.4) reveals a fundamental limitation that must be addressed before these models can be trusted in clinical settings. This has significant implications for the field's direction, suggesting that current LVMs may capture statistical patterns rather than medically meaningful features.\n\n**Innovation:** The paper introduces the novel Boltzmann Semantic Score, a theoretically grounded metric that represents a significant conceptual innovation. By leveraging LLMs to evaluate LVMs' semantic capability, it moves beyond traditional task-specific performance metrics to assess whether models capture the underlying medical knowledge. The cross-modal evaluation approach using Boltzmann distribution principles is particularly innovative.\n\n**Evaluation:** The paper provides extensive and rigorous evaluation across 32 TCGA cancer datasets using five LLMs and seven LVMs. The experiments include multiple validation strategies (information retrieval, survival prediction, correlation analysis) and demonstrate substantial consensus among different LLMs. However, the evaluation would be strengthened by direct comparison with existing LVM evaluation methods to establish BSS's relative advantages.\n\n**Reproducibility:** The paper provides comprehensive implementation details, including code availability, data sources, and experimental procedures. The methodology is described with sufficient detail to enable replication, and the use of standard datasets (TCGA) and publicly available models enhances reproducibility. The authors' commitment to releasing code and processed data further supports reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision (Reject/Resubmit)**\n\nThis paper makes significant contributions to the field by introducing a novel metric for evaluating whether LVMs capture medically relevant knowledge. However, the manuscript requires substantial revisions to address several critical limitations before it can be accepted. The authors should: (1) strengthen the theoretical foundation of BSS with more rigorous mathematical justification, (2) provide direct comparisons with existing evaluation methods to establish BSS's relative advantages, (3) expand the discussion of why the semantic gap exists and how it might be bridged, and (4) provide more concrete pathways for how BSS could be integrated into practical LVM development workflows. The paper's core contributions are valuable and potentially transformative for the field, but these revisions are necessary to fully realize that potential and establish the metric's broader utility. I encourage the authors to address these concerns in a revised submission.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 3,
          "specificity": 2,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 2,
          "source": "ai",
          "comment": "Some claims are weakly grounded, e.g., \"the manuscript introduces terms such as LVMs, LLMs, and BSS without sufficient definition\". Truly, the authors have explained them on page 1."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the *Boltzmann Semantic Score (BSS)*, a new metric designed to evaluate whether Large Vision Models (LVMs) capture medically and semantically meaningful features aligned with expert pathology knowledge. Drawing on principles of state space modeling and Boltzmann distributions, the BSS employs Large Language Models (LLMs) applied to pathology reports to assess the semantic alignment of LVM embeddings. The study uses 32 cancer datasets from The Cancer Genome Atlas (TCGA), evaluates five LLMs and seven LVMs, and establishes that LLM-derived representations correlate with patient outcomes. The authors find that current LVMs show low semantic capability (BSS < 0.4), highlighting a substantial performance gap. Overall, the manuscript is clear, comprehensive, and proposes an original, quantitative framework for assessing semantic fidelity in medical AI models.  \n\n**Major Comments**  \n1. **Conceptual Contribution:** The work tackles an important, underexplored question‚Äîhow to measure whether LVMs encode medically relevant semantics beyond task accuracy. The proposed BSS is theoretically motivated and potentially impactful.  \n2. **Theoretical Foundation:** The metric‚Äôs formal justification could be expanded. The rationale behind specific choices (e.g., k=5 and the Boltzmann factor formulation) should be better explained to reinforce mathematical soundness.  \n3. **Comparative Evaluation:** The paper lacks direct comparisons with existing LVM evaluation metrics or benchmarks. Including such analysis would clarify BSS‚Äôs relative advantages.  \n4. **Explanatory Analysis:** While the findings demonstrate a semantic gap, the paper does not discuss why this gap arises or how it might be addressed in model training or design.  \n5. **Clinical Relevance:** The discussion could more explicitly describe how BSS can be integrated into practical workflows for model validation or development within clinical research.  \n\n**Minor Comments**  \n- The methods section could be more concise to improve readability.  \n- Expanded discussion of related work on interpretability in medical imaging would provide better context.  \n- Figure 1 should label cancer types and clusters more clearly.  \n- The paper could more precisely indicate which semantic aspects LVMs fail to capture.  \n- Implementation details and visualizations are strong; release of code and data supports reproducibility.  \n\n**Summary Paragraph**  \nThis study introduces a notable and innovative metric for assessing semantic alignment between vision and language models in medical imaging. Its experimental breadth‚Äîcovering diverse datasets, multiple LLMs, and validation tasks‚Äîprovides solid evidence of current LVM limitations and suggests a meaningful direction for refining future models. However, theoretical and comparative analyses remain limited, and the discussion of underlying causes and practical application could be expanded. Overall, the work is significant and promising but requires further theoretical rigor and broader empirical context for full impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The paper makes an important and original contribution but needs additional theoretical clarification, comparison with existing methods, and more practical discussion of its implications before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript explores whether large vision models (LVMs) capture medically and semantically relevant information that is comparable to what is recorded in expert pathology reports. The authors assemble a corpus of more than 9,500 pathology reports drawn from 32 TCGA cancer cohorts and encode the text with five state‚Äëof‚Äëthe‚Äëart large language models (LLMs), obtaining dense embeddings. They then propose a new metric ‚Äì the Boltzmann Semantic Score (BSS) ‚Äì to assess structural similarity between the LLM embedding space (treated as the reference) and the LVM embedding space (the observer). BSS is built from k‚Äënearest‚Äëneighbor graphs together with Boltzmann factors. The experimental portion first evaluates the LLMs on information‚Äëretrieval and survival‚Äëprediction tasks, demonstrating that the resulting textual embeddings correlate with patient outcomes. Afterwards, seven LVMs are examined using BSS, which generally reveals low semantic alignment with the text space. Correlation analyses show that BSS values relate to downstream performance, and the authors also quantify the consensus among the LLMs when ranking the LVMs. The main contribution is the introduction of BSS as a modality‚Äëagnostic, systematic measure of semantic fidelity for visual models using language‚Äëmodel encodings.  \n\n---  \n\n## General feedback  \n\n- **Significance:** The work offers a scalable, quantitative alternative to the labor‚Äëintensive expert visual inspection that is often used to evaluate medical vision models. By providing a way to benchmark LVMs against large collections of pathology reports, the paper addresses a genuine need in digital pathology.  \n\n- **Innovation:** The Boltzmann‚Äëbased similarity measure, which operates on k‚Äënearest‚Äëneighbor graphs of the text and image embedding spaces (see Section‚ÄØ3.3, Figures‚ÄØ2‚Äì3), is an original idea. While the formulation is inventive, a stronger theoretical grounding and a clearer comparison with more straightforward cross‚Äëmodal metrics (e.g., cosine similarity) would help to highlight its practical advantages.  \n\n- **Evaluation:** The authors make good use of a substantial multi‚Äëcancer dataset (TCGA) and evaluate a diverse set of baselines (five LLMs and seven LVMs). The experiments span two downstream tasks, report Pearson correlations (Table‚ÄØ3), and examine consensus among the LLMs (Figure‚ÄØ7). Nonetheless, many methodological details‚Äîsuch as hyper‚Äëparameter choices, query construction, and statistical correction procedures‚Äîare omitted, and the reported BSS values are modest without an explicit interpretation of what constitutes a meaningful magnitude in practice.  \n\n- **Reproducibility:** Although the manuscript states that code and data will be released (Abstract, ‚ÄúReproducibility Statement‚Äù), the provided URL is a placeholder (https://github.com/XXX). The appendix lists software versions (Section‚ÄØE) but does not include complete pipelines, random‚Äëseed specifications, or a detailed description of the report‚Äëperturbation protocol described in Section‚ÄØ4.3.1.  \n\n---  \n\n## Specific comments / critiques  \n\n- **Mathematical clarity:** In Equation‚ÄØ2 the Boltzmann factor combines Euclidean distance with a constant ‚Äú‚àö‚ÄØd‚ÇÅ‚Äù without justification; the statistical or physical interpretation of this term is not clear (Section‚ÄØ3.3).  \n\n- **Second‚Äëorder factor notation:** Equation‚ÄØ3 repeats the term *p·µ¢¬∑p·µ¢* and does not differentiate between matching and non‚Äëmatching states, leading to ambiguous notation and interpretation (Section‚ÄØ3.3).  \n\n- **Choice of *k* in k‚ÄëNN graphs:** The default setting *k*‚ÄØ=‚ÄØ5 appears arbitrary. Although Figure‚ÄØ8 hints at an ablation over *k*, the manuscript does not present how varying *k* influences BSS values or the correlations reported in Table‚ÄØ3.  \n\n- **Perturbation protocol:** Table‚ÄØ1 presents results for ‚Äúperturbed‚Äù reports, yet the manuscript does not specify which keywords are removed, how many deletions occur per report, or whether the perturbation preserves report length (Section‚ÄØ4.3.1).  \n\n- **Baseline comparisons missing:** No experiments compare BSS against existing cross‚Äëmodal similarity measures (e.g., CLIP cosine similarity or simple Euclidean distance). Providing such baselines would make it possible to assess whether BSS truly offers a performance advantage.  \n\n- **Statistical reporting:** The correlation tables (Table‚ÄØ3a,‚ÄØ3b) include p‚Äëvalues but omit confidence intervals, effect‚Äësize interpretation, and any correction for multiple testing across the 32 cancer types. This omission raises concerns about potential false‚Äëpositive findings.  \n\n- **Formatting and readability:** Several tables contain placeholder entries such as ‚Äú0‚ÄØ._‚ÄØ._‚ÄØ871,‚Äù and several figures lack axis labels, legends, and fully informative captions (Tables‚ÄØ1‚Äë8, Figures‚ÄØ4‚Äë7). Improving these elements would greatly aid readers in interpreting the results.  \n\n- **Survival‚Äëprediction baselines:** The reported C‚Äëindices for LLM embeddings (~0.6, Table‚ÄØ2) are not benchmarked against standard clinical prognostic models (e.g., stage‚Äëbased models). Consequently, the added predictive value of the embeddings remains uncertain.  \n\n- **Consensus analysis details:** Figure‚ÄØ7 displays pairwise Cohen‚Äôs Œ∫ values but does not provide absolute numbers, confidence intervals, or statistical testing, which makes the claim of ‚Äúsubstantial agreement‚Äù harder to evaluate.  \n\n- **Code availability:** The repository link is currently a dummy placeholder. Without access to the BSS implementation, preprocessing scripts, and training pipelines, the reproducibility claim cannot be verified.  \n\n---  \n\n## A suggested decision  \n\nReject  \n\n*Rationale:* The manuscript lacks essential baseline comparisons and comprehensive statistical reporting, which limit confidence in the evaluation (evaluation‚ÄØ‚â§‚ÄØ1). Consequently, the current submission does not meet the criteria for acceptance.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether large vision models (LVMs) encode medical and semantic information comparable to that contained in expert pathology reports. Using 9,500 reports from 32 TCGA cancer cohorts, the authors obtain text embeddings from five large language models (LLMs) and propose a new metric‚Äîthe Boltzmann Semantic Score (BSS)‚Äîto quantify alignment between the LLM and LVM embedding spaces. Through retrieval and survival‚Äëprediction tasks, they show textual embeddings relate to patient outcomes, and then assess seven LVMs, observing generally low semantic concordance. The study‚Äôs main contribution is introducing BSS as a modality‚Äëagnostic, systematic measure of semantic fidelity. Overall, the topic is important and the approach ambitious, but the manuscript suffers from incomplete methodological details, missing baselines, and limited statistical clarity.  \n\n**Major Comments**  \n1. **Significance:** The work offers a scalable alternative to manual expert inspection for evaluating medical vision models, addressing a real need in digital pathology.  \n2. **Innovation:** The use of a Boltzmann‚Äëbased similarity built on k‚Äënearest‚Äëneighbor graphs is original. However, the method lacks strong theoretical grounding and should be compared more transparently to simpler cross‚Äëmodal similarity measures such as cosine or Euclidean metrics.  \n3. **Evaluation Design:** While the dataset and model diversity are strengths, essential methodological information (hyper‚Äëparameters, query construction, statistical corrections) is missing. Reported BSS values are modest and their practical interpretation remains unclear.  \n4. **Reproducibility:** The code link is a placeholder, and key implementation aspects‚Äîpipelines, random seeds, and the report‚Äëperturbation procedure‚Äîare insufficiently described, limiting reproducibility.  \n5. **Baselines and Statistical Reporting:** No results are provided against established cross‚Äëmodal metrics. Correlation tables omit confidence intervals or multiple‚Äëtesting corrections, raising uncertainty about significance claims.  \n6. **Survival and Consensus Analyses:** Survival results lack comparison with clinical baselines, and the consensus analysis reports Œ∫ values without uncertainty estimates, weakening validity of conclusions.  \n\n**Minor Comments**  \n- Equation‚ÄØ2 includes an unexplained ‚àö‚ÄØd‚ÇÅ term, and Equation‚ÄØ3 contains ambiguous notation (*p·µ¢¬∑p·µ¢*).  \n- The choice of *k*‚ÄØ=‚ÄØ5 in k‚ÄëNN graphs is arbitrary and insufficiently analyzed.  \n- Perturbation details (keywords removed, number per report) are unspecified.  \n- Several tables and figures contain placeholder numbers, missing labels, or unclear captions, reducing readability.  \n\n**Summary Paragraph**  \nThe study contributes an inventive metric and a large‚Äëscale multimodal analysis framework, but critical issues‚Äîabsence of meaningful baselines, incomplete statistical treatment, and lacking methodological transparency‚Äîundermine confidence in its conclusions. While conceptually promising, the work requires substantial clarification and experimental strengthening before it can be considered robust.  \n\n**Decision Recommendation**  \n**Reject.** The manuscript‚Äôs novelty is evident, but deficiencies in evaluation design, baseline comparisons, and reproducibility prevent acceptance in its current form.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Ali Bashashati",
      "Ali Khajegili Mirabadi",
      "Hossein Farahani",
      "Katherine Rich"
    ],
    "url": "pdfs/iclr.cc-2025-conference_4a62e78f3be27e86a9498d083dc2dcb4d2136f2f.pdf",
    "remote_url": "https://openreview.net/pdf/4a62e78f3be27e86a9498d083dc2dcb4d2136f2f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding",
    "status": "completed",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Vision-language model",
      "fine-grained alignment",
      "large-scale pre-training",
      "CT image"
    ],
    "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.",
    "decision": "Accept (Spotlight)",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes a fine-grained vision-language model (fVLM) to improve CT image interpretation by associating specific anatomical regions in CT scans with corresponding descriptions in radiology reports, addressing limitations of global image-report contrastive methods. It introduces a large dataset, MedVL-CT69K, encompassing 272,124 CT scans and achieves state-of-the-art performance. Key contributions include anatomy-level contrastive learning and a method to handle false negatives.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. Fine-Grained Anatomy-Level Alignment: The paper introduces a novel, fine-grained vision-language model (fVLM) that aligns anatomical regions in CT images with corresponding report descriptions.\n2. Large and Comprehensive Dataset: Experiments were conducted on a range of datasets including the largest CT dataset to date (MedVL-CT69K)\n3. Effectively tackling shortcomings of their method by introducing the dual false-negative reduction approach.\n\n### Weaknesses\n\n1. Experiments are incomplete: Table 1 doesn't include a performance evaluation of the methods from Table 3, namely CT-CLIP, CT-VocabFine, CT-LiPro. Table 3 doesn't include performance evaluation of the methods from Table 1. In both cases it's not argued why those experiments were not conducted. For Table 1, and Table 2 it's also unclear how the 2D approaches were adapted for 3D and how their evaluation was then done. Additionally, the evaluation metrics in Table 1 and Table 3 differ. Why? Furthermore, the performance comparison in Table 2 should also include comparisons to other CT-CLIP approaches, not only natural image counterparts (he it's again not clear how the 2D models were adapted to 3D).  \n2. Report generation uses only a single NLU metric. Other NLU metrics such as ROUGE would be interesting. Furthermore, using the GREEN metric[1] or RadFact[2] for evaluation would allow assessing clinical relevance.\n3. T-SNE visualization: An (additional) comparison of the embeddings to CT-CLIP would be be interesting to see if they have the same clustering behavior. \n4. Ablations study on masking missing: The masking approach is not well investigate and no ablation studies are done. One intuitive baseline would be the anatomical cropping of the image. What's the advantage of the masking approach? \n5. The authors approach only works for CT scans, since it relies on anatomy segmentations from TotalSegmentator. Furthermore, only on the classes which are available in TotalSegmentator.\n\n[1] Ostmeier, S., Xu, J., Chen, Z., Varma, M., Blankemeier, L., Bluethgen, C., ... & Delbrouck, J. B. (2024). GREEN: Generative Radiology Report Evaluation and Error Notation. arXiv preprint arXiv:2405.03595.\n\n[2] Bannur, S., Bouzid, K., Castro, D. C., Schwaighofer, A., Bond-Taylor, S., Ilse, M., ... & Hyland, S. L. (2024). MAIRA-2: Grounded Radiology Report Generation. arXiv preprint arXiv:2406.04449.\n\n### Questions\n\n1. What's the authors reproducibility statement? Will the dataset, code and weights be released? Especially publishing the dataset would be of great value for the community.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a fine-grained vision‚Äìlanguage model (fVLM) that enhances CT image interpretation by aligning specific anatomical regions in scans with corresponding text descriptions from radiology reports. It addresses the limitations of previous global image‚Äìreport contrastive learning approaches and introduces a large-scale dataset, MedVL-CT69K, comprising over 272,000 CT scans. The paper reports state-of-the-art performance and proposes techniques for anatomy-level contrastive learning and false-negative reduction. The work is clearly presented, though some experimental design and evaluation aspects require clarification.\n\n**Major Comments**  \n1. **Incomplete Experimental Evaluation:** The experiments are not comprehensive. Table 1 omits evaluation of methods from Table 3 (CT-CLIP, CT-VocabFine, CT-LiPro), and vice versa, without justification. The rationale for differing evaluation metrics across tables (Tables 1 and 3) is not provided. In addition, the adaptation of 2D models for 3D CT data is unclear, as is their evaluation strategy. Comparison in Table 2 should also include CT-CLIP-based methods rather than focusing solely on natural image baselines.  \n2. **Limited Evaluation Metrics:** Report generation is assessed using only a single natural language understanding (NLU) metric. It would strengthen the evaluation to include metrics such as ROUGE, GREEN, or RadFact to better assess clinical relevance.  \n3. **Visualization Analysis:** The t-SNE embedding visualization could be extended to include CT-CLIP for comparative clustering analysis.  \n4. **Ablation Studies on Masking:** The masking strategy lacks investigation. No ablation studies are provided, and the advantage over straightforward anatomical cropping is not established.  \n5. **Scope Limitation:** The approach depends on TotalSegmentator-based anatomy segmentations, restricting applicability to CT scans and to the anatomical classes provided by that tool.  \n\n**Minor Comments**  \n- The reproducibility statement is missing. Clarify whether dataset, code, and model weights will be released, as open availability would substantially benefit the community.  \n- Minor clarifications in figure legends and metric naming would help the reader.  \n\n**Summary Paragraph**  \nThis study presents a promising and well-motivated approach to fine-grained vision‚Äìlanguage alignment for CT interpretation, supported by a valuable large-scale dataset. However, experimental completeness, consistency in comparisons, absence of reproduction details, and limited evaluation metrics reduce the clarity of the results and the assessability of claims. Addressing these issues would significantly strengthen the contribution.\n\n**Decision Recommendation:** Major Revision",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors present an vision-language model for  anatomy-aware medical report generation, adressing the task of false negative reduction.  A large (private) dataset is curated and good performances on public datasets are presented.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n* The study addresses an interesting multi-modal learning problem from the medical domain\n* The presented false negative reduction is a meaningful integration to vision-language model report generation\n* The results are promising indicating the relevance of this addition\n\n### Weaknesses\n\n* The methodology is not quite clear\n* There is no codebase, preproduction may be a challenge.\n\n### Questions\n\n* Image Encoder Setup: The authors mention using a ViT base model with MAE pretraining on ImageNet-1K and a patch size of 16 x 16 x 32 (referenced in A.2). However, it‚Äôs unclear how they process 3D volumes with 3D patch embeddings through a 2D ViT model. Although they provide details on the training procedure, the overall model architecture remains vague. Given the lack of source code, a clearer description of the model's structure is crucial. Can the authors improve?\n* Comparison with 2D CLIP Methods: The comparisons made with 2D CLIP methods in Table 1 are not fully explained. Specifically, it's unclear how they adapted these methods to a 3D context and how the training was conducted. Please clarify.\n* Evaluation with CT-CLIP: In Table 3, the comparison with CT-CLIP models is ambiguous. Did the authors fine-tune a pre-trained model (based on their in-house data) on CT-RATE, or was there a different approach taken?\n* t-SNE Consistency: To enhance the consistency of the embedding spaces shown in Fig. 7, the authors could consider setting a random seed before calculating the t-SNE embeddings.\n* HU Value Clipping: The authors chose to clip HU values between -300 and 400, which raises a question: Does this range cover all abnormalities? For instance, lung-related abnormalities are typically closer to -1000 (representing air), while bone-related abnormalities can reach values close to 1000. Since they utilized both non-contrast and contrast-enhanced CT scans (arterial, venous, and delayed phases), did they apply this same HU preprocessing across all scans? Additionally, did the authors use this range for the CT-RATE training ablations (which is different than the original CT-RATE paper)? Please clarify.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nnone",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a vision‚Äìlanguage model designed for anatomy‚Äëaware medical report generation, with a particular focus on reducing false negatives. The authors curate a large private dataset and report strong performance on public benchmarks. The topic is relevant to multimodal learning in the medical domain, and the results suggest meaningful contributions. However, certain methodological and presentation aspects are insufficiently detailed, affecting clarity and reproducibility.\n\n**Major Comments**  \n1. **Methodological Clarity** ‚Äì The model design and workflow require clearer explanation. Specifically, the architectural integration of 3D image volumes within a ViT base pretrained on ImageNet‚Äë1K is unclear. The paper notes patch embeddings of 16‚ÄØ√ó‚ÄØ16‚ÄØ√ó‚ÄØ32 but does not explain how these 3D patches are processed through a 2D ViT model. Without source code, a more transparent description of the model‚Äôs structure is essential for reproducibility.  \n2. **Comparative Evaluation** ‚Äì The comparisons with 2D CLIP methods (Table‚ÄØ1) and CT‚ÄëCLIP models (Table‚ÄØ3) lack sufficient methodological context. It remains ambiguous how 2D CLIP methods were adapted for 3D data and whether CT‚ÄëCLIP results involved fine‚Äëtuning on in‚Äëhouse data or other procedures. These details are necessary to interpret the results fairly.  \n3. **Data Preprocessing** ‚Äì The HU clipping range (‚Äë300‚ÄØto‚ÄØ400) may not encompass all relevant abnormalities, especially given the inclusion of both lung (‚âà‚ÄØ‚Äë1000‚ÄØHU) and bone (‚âà‚ÄØ1000‚ÄØHU) regions across different scan phases. The authors should clarify whether this range was consistently applied for all scans and how it aligns with prior CT‚ÄëRATE work.  \n\n**Minor Comments**  \n- To improve reproducibility, setting a random seed before computing t‚ÄëSNE embeddings (Fig.‚ÄØ7) would enhance visual consistency.  \n- Quality of presentation could be improved; the exposition of methods and architectural choices is sometimes hard to follow.  \n- The absence of code limits ease of verification and reuse.\n\n**Summary Paragraph**  \nOverall, the paper addresses an important problem in multimodal medical imaging and proposes a plausible strategy for false negative reduction in report generation. The study is valuable in concept and yields promising results, but methodological ambiguity and missing implementation details reduce transparency. Providing clearer model descriptions, explicit experimental procedures, and code release would substantially strengthen the work.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work discusses the fine-grained alignment issue for vision-language clip-like pretraining for CT and reports. To improve the granularity of alignment the authors propose to pre-segment key anatomical structures in CT using a public segmentation model (Totalsegmenter), and to pre-segment descriptions corresponding to individual anatomical structures in the free-text report using an open-access LLM (Qwen). This allows fine-grained contrastive learning at the anatomical structure level. Considerations are made to avoid pushing representations of the same organ/condition away. Improved performances are shown for zero-shot abnormality detection compared with several existing approaches.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe idea of increasing the granularity of contrastive learning by data pre-segmentation using public tools is neat, simple, and straightforward. \n\nThe paper is written with sufficient clarity, where the insights behind each design and the implementations details are well-presented. \n\nImproved performance on zero-shot abnormality detection is shown compared with some existing works.\n\n### Weaknesses\n\nFor a comprehensive assessment of report generation some essential scores are missing, such as BLEU 1-3, ROUGE-L, and METEOR. Also, comparisons with vision encoders from peer vision-language pre-training for CT works may be needed. This is my major concern as the authors have claimed improved performance on report generation task.\n\nDespite improved granularity, attributing sentences of reports `mentioning` a structure to the image feature of that structure may sometimes be misleading especially when manual verification/correction is not accessible. E.g., in practice pancreatitis is often associated with the entire abdomen instead of the pancreas alone and neighboring structures. The authors are encouraged to comment on this.\n\n### Questions\n\nA high-level question: The capability of open-source image segmentation tools (totalsegmenter and/or SAM family) and open-source LLM advance fiercely -- does it imply that the utility of (un-/weakly-supervised) vision-language pretraining for CT will gradually phase out? One may directly construct semantic labels from text reports using strong enough LLM tools and turn the problem back to large scale supervised learning. I am curious to hear the authors‚Äô insight.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nA separate ICLR ethics reviewer might not be needed. However, as the proposed study involves curating and analyzing human CT images and reports, the authors need to make sure that essential ethical approval is obtained, and pertinent privacy laws/regulations are complied with.",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses fine-grained alignment in vision-language pretraining for CT images and corresponding reports. The authors propose segmenting CT scans into anatomical structures using a public segmentation tool (TotalSegmenter) and segmenting textual descriptions using an open-access large language model (Qwen). The approach enables contrastive learning at the anatomical structure level while minimizing conflicts in representation learning of identical organs or conditions. The paper is clearly written, presenting both conceptual motivation and implementation details effectively. Experiments demonstrate improved zero-shot abnormality detection performance when compared with existing methods.\n\n**Major Comments**  \n1. **Evaluation Metrics for Report Generation**: The manuscript claims improved performance on report generation, yet omits common quantitative measures such as BLEU 1‚Äì3, ROUGE-L, and METEOR. A complete evaluation including these scores is required to substantiate this claim.  \n2. **Comparative Analysis**: The study would benefit from direct comparison with vision encoders from peer vision-language pretraining approaches for CT. This comparison is important to contextualize the proposed method‚Äôs advantages.  \n3. **Attribution Accuracy**: Although the method aims for fine-grained alignment, assigning report sentences mentioning an anatomical structure directly to that structure‚Äôs image features may be unreliable. Certain clinical findings (e.g., pancreatitis) relate to broader regions rather than single organs. Clarification of how such ambiguities are handled‚Äîor a discussion of potential misalignment‚Äîwould strengthen the work.  \n4. **Long-Term Utility of the Approach**: Given the rapid progress of open-source segmentation and large language models, the necessity of unsupervised or weakly supervised pretraining for CT data may diminish. The authors are encouraged to address this strategic question and provide perspectives on future relevance.\n\n**Minor Comments**  \n- Ensure ethical approval and compliance with data protection regulations are clearly stated, given the involvement of human CT data.  \n- Minor editorial refinement could further improve readability and flow.\n\n**Summary Paragraph**  \nOverall, the paper introduces a simple yet effective approach to enhancing alignment granularity in CT-based vision-language pretraining. The technical presentation and design rationale are clear, and empirical improvements are demonstrated. However, the quantitative evaluation for report generation and comparisons with related pretraining models are incomplete, and the attribution scheme warrants deeper clarification regarding its clinical validity. Addressing these concerns would considerably strengthen the manuscript.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to‚ÄØTMI** ‚Äì  \nThe paper primarily contributes a *methodological innovation* in medical imaging AI: a new **fine‚Äëgrained vision‚Äìlanguage pre‚Äëtraining strategy (fVLM)** for CT image understanding. It tackles anatomical‚Äëlevel image‚Äìreport alignment and advances multimodal learning beyond global CLIP‚Äëstyle pairing. This fits well within‚ÄØ*TMI*‚Äôs scope on imaging methodology (image reconstruction, representation learning, quantitative analysis, and AI interpretability).\n\n2. **Novelty & Contribution Level** ‚Äì  \nThe work extends foundations of medical vision‚Äìlanguage modeling with (i) explicit anatomy‚Äëlevel alignment rather than implicit attention, and (ii) a dual false‚Äënegative reduction and co‚Äëteaching scheme. Given the novelty of anatomical parsing and disease‚Äëaware contrastive objectives for 3‚ÄëD CT data, the paper exceeds incremental improvement. However, its conceptual originality lies mainly in adapting and integrating known components (segmentation‚Äëbased locality, CLIP‚Äëstyle loss, co‚Äëteaching) rather than entirely new theory. It demonstrates strong empirical advancement.\n\n3. **Technical & Experimental Rigor** ‚Äì  \nThe study uses a very large in‚Äëhouse dataset (69‚ÄØk‚ÄØpatients,‚ÄØ272‚ÄØk‚ÄØCTs) with report parsing via LLMs and external validation on CT‚ÄëRATE and‚ÄØRad‚ÄëChestCT. Ablation, scaling, and visualization analyses are sound. Details of preprocessing, annotation, and statistical significance testing are only partly shown in appendices; clarity on IRB approval and specific error analysis could be improved.  \n\n4. **Clarity and Presentation** ‚Äì  \nThe manuscript is clearly written, with logical structure, extensive comparisons, and readable figures/tables. Minor language polishing would help (grammar, figure referencing, shorthand consistency).  \n\n5. **Ethical and Reproducibility Compliance** ‚Äì  \nDataset creation claims anonymization but does not explicitly mention institutional ethics approval or data‚Äësharing policy for‚ÄØMedVL‚ÄëCT69K. Code availability is stated via GitHub. The use of LLMs (Qwen‚ÄØ2.5) for report parsing should confirm that no protected health information was exposed externally.\n\n---\n\n**Phase‚ÄØ2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \nThis paper introduces **fVLM**, a fine‚Äëgrained vision‚Äìlanguage pre‚Äëtraining framework for 3‚ÄëD CT image interpretation. It segments CT volumes into 36 grouped anatomies, decomposes clinical reports into corresponding anatomical sentences using LLM‚Äëdriven parsing, and performs per‚Äëanatomy contrastive learning. To mitigate false negatives in contrastive objectives, the authors design a *dual false‚Äënegative correction* that accounts for normal‚Äìnormal and disease‚Äëconsistent samples, implemented via a co‚Äëteaching scheme between two models. Evaluations on a newly built large‚Äëscale dataset (MedVL‚ÄëCT69K) and public benchmarks show improved zero‚Äëshot and report‚Äëgeneration performance over CLIP and prior medical VLMs.\n\n2. **Strengths**  \n- Ambitious and timely topic: large‚Äëscale, multimodal CT foundation modeling.  \n- Methodologically more granular than existing VLMs, improving interpretability.  \n- Comprehensive experiments, including ablations, scaling analyses, and multi‚Äëdataset validation.  \n- Demonstrates credible quantitative gains (‚âà‚ÄØ8‚Äì13‚ÄØAUC‚ÄØpoints) and qualitative improvements in attention localization.  \n- Public code release enhances reproducibility.\n\n3. **Weaknesses**  \n- Novelty is somewhat incremental (integration of established ideas) rather than theoretical breakthrough.  \n- Heavy reliance on an internal dataset; lack of detailed description of patient cohort, scanner diversity, and ethical oversight.  \n- The dual false‚Äënegative and co‚Äëteaching strategy needs stronger mathematical or empirical justification (hyperparameter sensitivity, convergence).  \n- Report decomposition via LLMs may introduce unquantified errors; no inter‚Äëannotator or parsing accuracy analysis reported.  \n- Limited discussion of computational cost and inference efficiency for clinical deployment.\n\n4. **Major Comments**  \n1. **Dataset Ethics and Generalizability:**‚ÄØProvide explicit approval or de‚Äëidentification description for‚ÄØMedVL‚ÄëCT69K and clarify whether the data can be publicly released or partially shared.  \n2. **Quantitative Robustness:**‚ÄØAdd confidence intervals or statistical tests for improvements (AUC‚ÄØgains) across tasks and anatomies.  \n3. **LLM‚ÄëBased Report Parsing:**‚ÄØQuantify parsing accuracy against a radiologist‚Äëverified subset; discuss robustness to differing report templates.  \n4. **False‚ÄëNegative Reduction:**‚ÄØOffer an intuition or analytic analysis of how the label‚Äëcorrection weighting (Œ±‚ÄØ=‚ÄØ0.5) affects training stability; consider providing ablation over‚ÄØŒ±.  \n5. **Computational Efficiency:**‚ÄØReport GPU hours and inference speed, comparing to‚ÄØCT‚ÄëCLIP or‚ÄØMerlin, as this informs clinical scalability.  \n6. **Visualization/Interpretability:**‚ÄØInclude qualitative examples illustrating where fVLM improves localization relative to‚ÄØCLIP in 3‚ÄëD context.\n\n5. **Minor Comments**  \n- Check typographical inconsistencies (e.g., ‚ÄúÔ¨Åndings/impression,‚Äù ‚ÄúSpec/Sens‚Äù).  \n- Ensure figure legends are self‚Äëcontained.  \n- Reference numbering for certain works (e.g., duplicate Lin‚ÄØ2023) should be verified.  \n- Clarify abbreviations in tables (GREEN, CoT).  \n- Provide full specification of training hyperparameters (batch size, learning rate).  \n\n6. **Overall Recommendation** ‚Äì **Major‚ÄØRevision**\n\n*Justification:*‚ÄØThe study demonstrates clear methodological and experimental promise highly relevant to‚ÄØ*TMI*, but key clarifications on ethical compliance, dataset transparency, and deeper analysis of the proposed modules are necessary before publication.\n\n7. **Confidence Level** ‚Äì‚ÄØ**4‚ÄØ/‚ÄØ5** (High confidence in technical assessment; limited access to supplementary ethics details)\n\n---\n\n**Guidance to the Authors:**  \nTo elevate the paper to‚ÄØ*TMI* standards, emphasize methodological insight‚Äîformalize the false‚Äënegative correction, validate report parsing quantitatively, and provide transparency on dataset governance. Strengthening theoretical motivation and reproducibility documentation would make fVLM a strong contribution within the medical imaging methodology community.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *fVLM*, a fine‚Äëgrained vision‚Äìlanguage pre‚Äëtraining framework designed for 3‚ÄëD CT image interpretation. It segments CT volumes into anatomical regions and aligns them with corresponding report sentences parsed using large language models. Through anatomy‚Äëlevel contrastive learning and a dual false‚Äënegative correction implemented via co‚Äëteaching, the method aims to enhance image‚Äìtext alignment and interpretability beyond conventional global vision‚Äìlanguage models. The paper is clearly written, well organized, and methodologically ambitious, making a substantive contribution to medical imaging AI through large‚Äëscale multimodal representation learning.\n\n**Major Comments**  \n1. **Dataset Ethics and Transparency:**‚ÄØExplicitly describe the anonymization process, institutional review or ethics approval, and potential for public or partial release of the MedVL‚ÄëCT69K dataset. The absence of these details limits reproducibility and generalizability.  \n2. **Quantitative Robustness:**‚ÄØInclude statistical significance testing or confidence intervals for reported performance gains, particularly across anatomical categories and datasets.  \n3. **LLM‚ÄëBased Report Parsing:**‚ÄØAssess parsing reliability by benchmarking against radiologist‚Äëverified samples, and discuss how differences in report templates may affect alignment accuracy.  \n4. **False‚ÄëNegative Reduction Mechanism:**‚ÄØProvide analytic or empirical justification for the label‚Äëcorrection weighting strategy and examine sensitivity to the hyperparameter Œ±. Clarification of convergence behavior would strengthen confidence in the method‚Äôs stability.  \n5. **Computational Efficiency:**‚ÄØQuantify GPU hours, training duration, and inference speed relative to comparable models to evaluate feasibility for clinical deployment.  \n6. **Visualization and Interpretability:**‚ÄØAdd qualitative examples showing where the proposed framework achieves superior localization or alignment over baseline models.\n\n**Minor Comments**  \n- Correct typographical inconsistencies (e.g., ‚ÄúÔ¨Åndings/impression,‚Äù ‚ÄúSpec/Sens‚Äù).  \n- Ensure figure legends are self‚Äëcontained and all abbreviations (e.g., GREEN, CoT) are defined.  \n- Verify reference numbering and duplicates (e.g., Lin‚ÄØ2023).  \n- Provide complete training parameter details (batch size, learning rate).  \n- Minor language refinement and consistent figure referencing are recommended.  \n\n**Summary Paragraph**  \nThe study introduces a technically sound and well‚Äëevaluated multimodal learning framework that outperforms prior approaches on both internal and public datasets. It demonstrates strong experimental design and interpretability advantages, yet falls short in transparency of dataset governance, ethical compliance, and quantitative validation of new components. Strengthening these areas and clarifying methodological analysis would substantially improve the manuscript‚Äôs impact and reliability.\n\n**Decision Recommendation:**‚ÄØ**Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes fVLM, a fine-grained vision-language model for CT image understanding that addresses misalignment issues in existing CLIP-based approaches. The method performs anatomy-level alignment between CT scan regions and corresponding radiology report descriptions, rather than global image-report contrastive learning. The approach uses TotalSegmentator to parse anatomical structures (Section 3.1, Page 4) and employs LLMs to decompose reports into anatomy-specific descriptions (Figure 2, Page 4). To handle false negatives arising from abundant normal samples and similar pathologies, the authors introduce a dual false negative reduction strategy with co-teaching (Section 3.3, Pages 6-7). The method is evaluated on MedVL-CT69K, a curated dataset of 272,124 CT scans from 69,086 patients. Results show fVLM achieves 81.3% average AUC across 54 disease diagnosis tasks, outperforming CLIP by 12.9% and supervised methods by 8.0% (Table 1, Page 8). Additional evaluations on CT-RATE and Rad-ChestCT benchmarks demonstrate 7.4% and 4.8% absolute AUC improvements respectively (Table 2, Page 9).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - The similarity calculation in Equation 1 (Page 6) uses inconsistent notation where Nj represents \"number of structurally complete samples\" but the summation indices suggest batch-level operations, creating confusion about the actual computation scope\n  - The loss function in Equation 2 lacks clear definition of what constitutes \"structurally complete samples\" and how this affects gradient computation across anatomies with different sample counts\n  - The co-teaching formulation in Equation 3 (Page 7) sets Œ±=0.5 empirically without theoretical justification or sensitivity analysis, undermining the mathematical rigor of the approach\n\n‚Ä¢ **Insufficient experimental validation and evaluation gaps**\n  - The dataset split details show only 1,151 validation patients versus 64,476 training patients (Page 8), representing an unusually small validation set that may not provide reliable model selection and hyperparameter tuning\n  - The comparison with radiologists is relegated to \"Appendix A.3\" (Page 9) without presenting quantitative results in the main paper, limiting assessment of clinical relevance\n  - Missing ablation studies on key architectural choices such as the grouping of 104 anatomical regions into 36 categories (Section 3.1, Page 4), which directly impacts the core fine-grained alignment capability\n\n‚Ä¢ **Technical approach limitations and methodological concerns**\n  - The reliance on TotalSegmentator for anatomy parsing (Figure 2, Page 4) introduces potential error propagation from segmentation failures, but no analysis is provided on how segmentation quality affects downstream performance\n  - The report decomposition using LLMs with string-matching fallback (Section 3.1, Page 5) lacks quantitative evaluation of decomposition accuracy, which is critical since misaligned text-image pairs would undermine the entire training objective\n  - The false negative identification strategy assumes anatomies not mentioned in impression sections are normal (Page 7), but this heuristic may not hold across different reporting styles or institutions, potentially introducing systematic biases\n\n‚Ä¢ **Experimental design and reproducibility concerns**\n  - The co-teaching strategy employs \"different model initialization, data iteration sequences and augmentations\" (Page 8) for model diversity, but these implementation details are not specified, hindering reproducibility\n  - The MedVL-CT69K dataset curation process lacks details about data quality control, institutional diversity, or potential selection biases that could affect generalizability\n  - The zero-shot evaluation setup does not clearly specify how disease categories were defined or annotated, particularly for the 54 diseases across 15 anatomies mentioned in the abstract but not fully detailed until appendix references\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation clarity**\n  - Clearly define Nj in Equation 1 and specify whether it represents per-anatomy sample counts or batch-level indices, and provide explicit algorithmic descriptions for handling variable anatomy presence across samples\n  - Add theoretical analysis or empirical sensitivity study for the Œ±=0.5 choice in Equation 3, demonstrating robustness to this hyperparameter selection\n  - Include formal definitions of \"structurally complete samples\" and provide computational complexity analysis for the anatomy-specific contrastive learning framework\n\n‚Ä¢ **Strengthen experimental validation with comprehensive baselines**\n  - Increase validation set size or provide justification for the current split, and include cross-validation results to demonstrate model stability across different data partitions\n  - Present quantitative radiologist comparison results in the main paper with statistical significance testing and inter-rater agreement analysis\n  - Add comprehensive ablation studies on anatomical grouping strategies, comparing performance across different granularity levels from 104 regions to various grouping schemes\n\n‚Ä¢ **Address technical limitations with robust evaluation**\n  - Quantitatively evaluate TotalSegmentator segmentation quality on the dataset and analyze correlation between segmentation accuracy and final model performance\n  - Develop and report metrics for report decomposition quality, potentially using expert annotations on a subset to validate LLM-based anatomy extraction accuracy\n  - Test the impression-section heuristic across different institutions or report templates, and provide alternative strategies for normal case identification\n\n‚Ä¢ **Improve experimental design and ensure reproducibility**\n  - Provide complete implementation details for the co-teaching diversity strategies, including specific augmentation types, initialization schemes, and iteration order variations\n  - Include detailed dataset curation methodology with quality control measures, institutional sources, and potential bias analysis\n  - Clearly specify disease annotation protocols and provide inter-annotator agreement statistics for the 54 disease categories, with detailed breakdown by anatomy type",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *fVLM*, a fine-grained vision-language model for CT image understanding designed to overcome the misalignment issues of existing CLIP-based methods. The model aligns anatomical regions of CT scans with corresponding report descriptions through anatomy-level parsing using TotalSegmentator and large language model‚Äìbased report decomposition. To address false negatives from abundant normal or similar pathological samples, a dual false-negative reduction strategy with co-teaching is introduced. Experiments on MedVL-CT69K and two additional benchmarks show consistent AUC improvements over CLIP and supervised baselines. The work is clearly motivated and technically ambitious, but several aspects of the mathematical formulation, evaluation design, and reproducibility require clarification and strengthening.  \n\n**Major Comments**  \n1. **Mathematical formulation and definition clarity**:  \n   - Notation inconsistencies in Equation‚ÄØ1 regarding \\(N_j\\) create ambiguity about computation scope (batch-level vs. anatomy-level).  \n   - The definition of ‚Äústructurally complete samples‚Äù in Equation‚ÄØ2 is unclear, leaving its impact on gradients uncertain.  \n   - The empirical setting of \\(\\alpha = 0.5\\) in Equation‚ÄØ3 lacks justification or sensitivity analysis, limiting theoretical rigor.  \n\n2. **Experimental validation and evaluation gaps**:  \n   - The validation subset (1,151 patients vs. 64,476 for training) is disproportionately small.  \n   - Radiologist comparisons are confined to an appendix and lack quantitative reporting.  \n   - No ablation on anatomical grouping (104‚ÄØ‚Üí‚ÄØ36‚ÄØcategories) is provided, though this directly affects the proposed fine-grained alignment.  \n\n3. **Technical and methodological limitations**:  \n   - Dependence on TotalSegmentator introduces possible segmentation-error propagation without analysis of its effect.  \n   - Report decomposition via LLMs and string matching is unevaluated quantitatively, leaving uncertainty about alignment reliability.  \n   - The assumption that unmentioned anatomies are normal may be dataset- or institution-dependent and could introduce systematic bias.  \n\n4. **Reproducibility and dataset transparency**:  \n   - The co-teaching diversity strategy (initialization, data order, augmentations) lacks implementation details.  \n   - Dataset curation and quality control procedures are not fully described, raising questions about sample representativeness.  \n   - The definition and annotation of the 54 disease categories are insufficiently detailed for zero-shot evaluation reproducibility.  \n\n**Minor Comments**  \n- Clarify notation around ‚Äústructurally complete samples.‚Äù  \n- Specify figure and table references consistently.  \n- Define all acronyms at first use (e.g., CT-RATE).  \n- Verify that Equations‚ÄØ1‚Äì3 use consistent symbols and indexing.  \n\n**Summary Paragraph**  \nThe paper introduces an innovative anatomy-level vision‚Äìlanguage alignment approach showing strong quantitative advances over prior CLIP-based models. However, ambiguities in mathematical formulation, incomplete validation details, and limited transparency of dataset and implementation weaken the methodological soundness and reproducibility. Addressing clarity of equations, thorough ablations, dataset documentation, and evaluation of preprocessing components would substantially strengthen the contribution and enable fair assessment of its claimed improvements.  \n\n**Decision Recommendation**: **Major Revision**. The method is promising but requires significant clarification and additional experimental support before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces a fine-grained vision-language model (fVLM) designed to enhance CT image interpretation by explicitly aligning anatomical regions of CT images with corresponding descriptions in radiology reports. The proposed method aims to address the limitations of existing global contrastive learning approaches by performing fine-grained alignment at the anatomy level, which is crucial for capturing clinically relevant details. The authors claim that their model outperforms existing methods on both zero-shot classification tasks and downstream report generation tasks, achieving significant improvements in AUC and other metrics. The manuscript is well-written and provides a comprehensive evaluation across a large dataset and multiple anatomies.\n\n## Major Comments\n1. Novelty and Positioning: While the proposed fine-grained alignment approach is innovative, the manuscript could benefit from a more thorough discussion of how it compares to existing methods that also attempt to align images and reports at finer levels. The authors should clarify how their method differs from recent works that also focus on anatomy-level alignment and provide a more detailed comparison.\n\n2. Evaluation Design: The evaluation is primarily conducted on a large in-house dataset (MedVL-CT69K), which is commendable for its size and diversity. However, the authors should consider extending the evaluation to more diverse datasets or real-world clinical scenarios to better validate the model's generalizability. Additionally, the inclusion of more qualitative analyses, such as case studies or reader studies involving radiologists, would strengthen the manuscript.\n\n3. Comparisons: The manuscript compares the proposed method against several baselines, including CLIP and other vision-language models. However, the choice of baselines seems somewhat arbitrary. Including a broader range of state-of-the-art methods, especially those specifically tailored for medical imaging, would provide a more comprehensive assessment of the model's performance.\n\n4. Reproducibility: The authors state that the code will be made available, which is good practice. However, the manuscript lacks sufficient detail regarding the training protocols, preprocessing steps, and hyperparameters. Providing a more detailed description of these aspects is crucial for ensuring reproducibility.\n\n## Minor Comments\n1. Figures: Some figures, such as Figure 3, are cluttered and could benefit from simplification. Showing fewer representative slices with zoomed-in regions would improve readability.\n   \n2. Notation Consistency: There are inconsistencies in the notation used throughout the manuscript, particularly in the description of the forward operator. Clarifying these notations would enhance clarity.\n\n3. Acronyms: Several acronyms, such as \"R=4,\" are used without definition. Defining these acronyms would aid in understanding.\n\n4. Typographical Issues: Minor typographical errors, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7), should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge in enhancing the interpretation of CT images using vision-language models. The proposed fine-grained alignment approach is technically innovative and demonstrates promising results in zero-shot classification and report generation tasks. However, the evaluation could be strengthened by including a broader range of datasets and more detailed qualitative analyses. The reproducibility of the method is somewhat compromised by the lack of detailed methodological descriptions. Overall, while the manuscript has substantial merit, it requires additional validation and clarification to fully meet the standards expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis to include a broader range of state-of-the-art methods, strengthen validation across diverse datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a fine-grained vision‚Äìlanguage model (fVLM) developed to improve CT image interpretation through explicit alignment between anatomical regions and their textual descriptions in radiology reports. By performing anatomy-level alignment, the method aims to overcome limitations of global contrastive learning approaches and better capture clinically meaningful details. The authors report that their model surpasses existing approaches on zero-shot classification and report generation tasks, achieving notable improvements in AUC and related metrics. The paper is clearly written and supported by comprehensive experiments on a large, anatomically diverse dataset.\n\n**Major Comments**  \n1. **Novelty and Positioning:** The fine-grained alignment concept is promising, yet the discussion of related work should be deepened. The manuscript needs clearer differentiation from other recent studies pursuing fine-grained or anatomy-level alignment, with explicit comparison and positioning relative to them.  \n2. **Evaluation Design:** The evaluation on the in-house MedVL-CT69K dataset is valuable but somewhat limited. Broader testing on external or real-world clinical datasets would strengthen claims of generalizability. Incorporating qualitative studies, such as exemplar cases or limited reader studies, could further demonstrate practical impact.  \n3. **Baselines and Comparisons:** While comparisons include CLIP and several other models, the current baseline set appears narrow. Integrating additional state-of-the-art methods designed for medical image‚Äìtext tasks would provide a more robust performance context.  \n4. **Reproducibility:** The manuscript mentions code release, which is commendable, but methodological transparency is incomplete. More detailed descriptions of training protocols, preprocessing steps, and hyperparameters are needed to ensure reproducibility and facilitate independent replication.\n\n**Minor Comments**  \n1. **Figures:** Figures‚Äîparticularly Figure 3‚Äîappear visually busy. Simplifying layouts or showing fewer but clearer examples would aid comprehension.  \n2. **Notation Consistency:** Notations for the forward operator and related equations vary in form; harmonization would improve clarity.  \n3. **Acronyms:** Some abbreviations (e.g., ‚ÄúR=4‚Äù) are undefined and should be introduced at first use.  \n4. **Typographical Errors:** Correct minor typos such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes.‚Äù\n\n**Summary Paragraph**  \nThis study makes a meaningful contribution toward anatomy-aware vision‚Äìlanguage modeling for CT interpretation and presents encouraging quantitative results. However, deeper comparative positioning, broader validation across datasets, and enhanced methodological transparency are necessary for full reproducibility and impact. Addressing these concerns would substantially improve the manuscript‚Äôs rigor and clarity.  \n\n**Decision Recommendation**  \n**Major Revision.** The work demonstrates technical merit and clinical relevance but requires expanded comparative analyses, additional validation, and fuller methodological disclosure before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## LARGE-SCALE AND FINE-GRAINED VISION-LANGUAGE PRE-TRAINING FOR ENHANCED CT IMAGE UNDERSTANDING\n\n### Summary\n\nThis paper presents fVLM, a fine-grained vision‚Äìlanguage pretraining framework for CT that explicitly aligns anatomy-level visual embeddings with anatomy-specific textual descriptions parsed from radiology reports. The approach uses automatic whole-body segmentation (TotalSegmentator) to derive anatomy tokens, LLM-assisted report decomposition to create anatomy-wise text, and a contrastive objective computed per anatomy with a dual false-negative reduction strategy (normal-sample correction via impression-based heuristics and co-teaching‚Äìbased soft targets). On a large in-house dataset (MedVL-CT69K, 69k patients) and two public benchmarks (CT-RATE and Rad-ChestCT), the method shows substantial zero-shot gains over CLIP-style and prior CT-VLP baselines; it also improves downstream report generation.\n\n### Strengths\n\n- Technical novelty and innovationFine-grained anatomy-level contrastive alignment that leverages volumetric masks to extract organ-specific tokens and aligns them to anatomy-decomposed report text.A dual false-negative reduction scheme addressing both abundant normal anatomy samples (impression-based normal correction) and semantically similar abnormal samples (co-teaching with cross-model soft labels).Pragmatic grouping of 104 structures into 36 anatomies to balance granularity with data sufficiency and alignment feasibility in 3D CT.\n- Experimental rigor and validationLarge-scale pretraining on a sizable private dataset (69k patients, multi-phase CT) and evaluation on two external benchmarks (CT-RATE, Rad-ChestCT) with consistent improvements.Ablation studies isolating the contributions of fine-grained alignment, normal false-negative correction, and co-teaching; scaling-law analysis indicating favorable data efficiency over CLIP.\n- Clarity of presentationClear articulation of the motivation for fine-grained alignment in 3D CT versus 2D CXR; informative figures showing anatomy parsing and alignment matrices.Method description includes operational details (mask-to-token mapping; anatomy token queries and self-attention update; per-anatomy losses).\n- Significance of contributionsDemonstrates that anatomy-aware alignment materially improves zero-shot diagnostic performance across diverse organs and diseases, addressing an important gap in clinical CT VLPs.Offers a general recipe (mask pooling + report decomposition + per-anatomy contrast) that could be adapted to other 3D modalities or institutions.\n\n- Fine-grained anatomy-level contrastive alignment that leverages volumetric masks to extract organ-specific tokens and aligns them to anatomy-decomposed report text.\n- A dual false-negative reduction scheme addressing both abundant normal anatomy samples (impression-based normal correction) and semantically similar abnormal samples (co-teaching with cross-model soft labels).\n- Pragmatic grouping of 104 structures into 36 anatomies to balance granularity with data sufficiency and alignment feasibility in 3D CT.\n\n- Large-scale pretraining on a sizable private dataset (69k patients, multi-phase CT) and evaluation on two external benchmarks (CT-RATE, Rad-ChestCT) with consistent improvements.\n- Ablation studies isolating the contributions of fine-grained alignment, normal false-negative correction, and co-teaching; scaling-law analysis indicating favorable data efficiency over CLIP.\n\n- Clear articulation of the motivation for fine-grained alignment in 3D CT versus 2D CXR; informative figures showing anatomy parsing and alignment matrices.\n- Method description includes operational details (mask-to-token mapping; anatomy token queries and self-attention update; per-anatomy losses).\n\n- Demonstrates that anatomy-aware alignment materially improves zero-shot diagnostic performance across diverse organs and diseases, addressing an important gap in clinical CT VLPs.\n- Offers a general recipe (mask pooling + report decomposition + per-anatomy contrast) that could be adapted to other 3D modalities or institutions.\n\n### Weaknesses\n\n- Technical limitations or concernsHeavy reliance on automatically generated anatomy masks (TotalSegmentator) and LLM-based report decomposition introduces unquantified noise; the robustness of fVLM to segmentation and LLM errors is not analyzed.The impression-based rule that ‚Äúnot mentioned implies normal‚Äù risks systematic false labeling; not all abnormal findings are summarized in impressions due to style variability.Co-teaching doubles compute and may propagate shared biases; safeguards are qualitative (different inits/augmentations) without quantitative analysis of divergence/consistency.\n- Experimental gaps or methodological issuesNo sensitivity analysis to segmentation errors, report decomposition accuracy, or anatomy grouping choices (104‚Üí36). Runtime/resource overhead from per-volume segmentation at inference is not reported.Several reported metrics appear inconsistent or implausible (e.g., AUC values of 1.00 for multiple tasks; Table 1 shows F1 far exceeding precision), raising concerns about metric definitions or tabulation.Zero-shot prompting templates for disease detection are not detailed; reproducibility is hindered by the placeholder code link and private data.\n- Clarity or presentation issuesSome tables contain garbled entries (e.g., ‚Äú-◊ë‚Äù in Table 2 Spec/Sens) and possible column misalignment; definitions of metrics (macro vs micro, class balancing, thresholding) need clarification.Reporting of disease-wise sample sizes and confidence intervals is limited in the main text; claims of very high AUC on some categories could be due to small-n effects.\n- Missing related work or comparisonsThe paper does not sufficiently discuss recent VLP strategies that also handle false negatives or many-to-many semantics (e.g., SigLIP-style objectives, semantic-positive mining as in FaNe, robust contrast as in BIUD).Recent 3D CT foundation models that incorporate geometry-aware designs or multi-objective training (e.g., SPECTRE, COLIPRI, task-unified frameworks like Uniferum, multi-scale organ‚Äìphrase alignment like MedVista3D) are not positioned relative to this work.\n\n- Heavy reliance on automatically generated anatomy masks (TotalSegmentator) and LLM-based report decomposition introduces unquantified noise; the robustness of fVLM to segmentation and LLM errors is not analyzed.\n- The impression-based rule that ‚Äúnot mentioned implies normal‚Äù risks systematic false labeling; not all abnormal findings are summarized in impressions due to style variability.\n- Co-teaching doubles compute and may propagate shared biases; safeguards are qualitative (different inits/augmentations) without quantitative analysis of divergence/consistency.\n\n- No sensitivity analysis to segmentation errors, report decomposition accuracy, or anatomy grouping choices (104‚Üí36). Runtime/resource overhead from per-volume segmentation at inference is not reported.\n- Several reported metrics appear inconsistent or implausible (e.g., AUC values of 1.00 for multiple tasks; Table 1 shows F1 far exceeding precision), raising concerns about metric definitions or tabulation.\n- Zero-shot prompting templates for disease detection are not detailed; reproducibility is hindered by the placeholder code link and private data.\n\n- Some tables contain garbled entries (e.g., ‚Äú-◊ë‚Äù in Table 2 Spec/Sens) and possible column misalignment; definitions of metrics (macro vs micro, class balancing, thresholding) need clarification.\n- Reporting of disease-wise sample sizes and confidence intervals is limited in the main text; claims of very high AUC on some categories could be due to small-n effects.\n\n- The paper does not sufficiently discuss recent VLP strategies that also handle false negatives or many-to-many semantics (e.g., SigLIP-style objectives, semantic-positive mining as in FaNe, robust contrast as in BIUD).\n- Recent 3D CT foundation models that incorporate geometry-aware designs or multi-objective training (e.g., SPECTRE, COLIPRI, task-unified frameworks like Uniferum, multi-scale organ‚Äìphrase alignment like MedVista3D) are not positioned relative to this work.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe anatomy-token construction is reasonable: mask-guided token selection with an anatomy query updated by self-attention is a clean way to derive localized representations. However, the approach‚Äôs dependence on accurate masks makes it brittle to segmentor errors and scan heterogeneity (contrast phases, partial FOV). An ablation with perturbed masks or reduced-quality masks would strengthen technical credibility.The dual false-negative reduction is conceptually aligned with semantic-aware contrastive literature. The impression-based normal correction is simple and scalable but could bias towards template normals and potentially collapse semantics of normal anatomies. The co-teaching soft-label update is a plausible mitigation for confirmation bias; still, an explicit comparison to SigLIP-style multi-positive losses or semantic-positive mining (e.g., text-text similarity, entity-level matching) would reinforce the chosen design.Equation and loss formulations are mostly clear, though the mixture of one-hot labels with soft similarities and subsequent normalization warrants a more formal treatment: how does this behave under extreme class imbalance or in batches dominated by normals? Stability analysis would be helpful.\n- Experimental evaluation assessmentThe scale and diversity of MedVL-CT69K are compelling; nonetheless, given it is private, clear documentation of source institutions, de-identification, IRB, and cohort/scan protocol distributions would contextualize generalizability.The improvements on external CT-RATE and Rad-ChestCT are meaningful. Please include 95% CIs, per-task support counts, and formal significance testing. Very high AUCs (approaching 1.0) suggest either small positives or label artifacts; report per-class prevalence and confidence intervals for such cases.The report-generation evaluation is a useful transfer test; however, diagnostic extraction relies on a custom classifier. Provide its validation (AUROC, F1, calibration) and ensure it was not trained on the same reports used for generation to avoid circularity. Adding human evaluation or radiologist QA on a subset would strengthen claims.Ablations are promising but could be more comprehensive: (1) with vs without masks (or using coarse bounding boxes) to quantify reliance on segmentation; (2) with different anatomy groupings; (3) without impression-based correction but with alternative soft-negative treatments (semantic positives from text similarity, RoCo/SigLIP multi-positive labels).\n- Comparison with related work (using the summaries provided)FaNe addresses false negatives via semantic-aware positive mining and local text-conditioned pooling; your co-teaching and normal correction play a similar role. A discussion contrasting your batch-wise co-teaching with FaNe‚Äôs adaptive semantic normalization would clarify design trade-offs (e.g., reliance on text encoders vs model-predicted similarities).BIUD introduces RoCo (multi-positive contrast) and entity-focused masking to reduce false negatives and emphasize clinical tokens. Comparing your co-teaching and impression-based rule to BIUD‚Äôs robust contrast design would be informative‚Äîespecially under cases of subtle abnormal semantics.Anatomy-VLM demonstrates region-level alignment in 2D CXRs via learnable anatomy queries and structured phrase alignment. Your mask-based token extraction is a natural 3D extension; highlighting differences (e.g., explicit masks vs learned object queries; 3D volumetrics; impression-driven normal correction) would situate novelty.Recent 3D CT models (COLIPRI, SPECTRE, MedVista3D, Uniferum) adopt multi-objective training, geometry-aware architectures, or multi-scale alignments. While some are contemporaneous, acknowledging these directions and clarifying how fVLM complements (or could be combined with) such methods would strengthen positioning.\n- Discussion of broader impact and significanceThe anatomy-aware alignment advances interpretability and could improve radiologist trust through better localization. However, requiring segmentation at inference increases computational cost and introduces a failure mode not present in global VLPs; deployment guidance (precomputing masks, QC checkpoints, handling partial FOV) would be valuable.The use of LLMs for report decomposition raises governance questions (hallucination, bias). Quantifying LLM extraction accuracy (with radiologist adjudication on a stratified subset) and offering a fallback when confidence is low would improve safety.Data privacy and ethics should be elaborated (IRB approvals, consent waivers, de-identification). Given strong performance claims, clear statements on intended use, limits, and out-of-distribution risks are important.\n\n- The anatomy-token construction is reasonable: mask-guided token selection with an anatomy query updated by self-attention is a clean way to derive localized representations. However, the approach‚Äôs dependence on accurate masks makes it brittle to segmentor errors and scan heterogeneity (contrast phases, partial FOV). An ablation with perturbed masks or reduced-quality masks would strengthen technical credibility.\n- The dual false-negative reduction is conceptually aligned with semantic-aware contrastive literature. The impression-based normal correction is simple and scalable but could bias towards template normals and potentially collapse semantics of normal anatomies. The co-teaching soft-label update is a plausible mitigation for confirmation bias; still, an explicit comparison to SigLIP-style multi-positive losses or semantic-positive mining (e.g., text-text similarity, entity-level matching) would reinforce the chosen design.\n- Equation and loss formulations are mostly clear, though the mixture of one-hot labels with soft similarities and subsequent normalization warrants a more formal treatment: how does this behave under extreme class imbalance or in batches dominated by normals? Stability analysis would be helpful.\n\n- The scale and diversity of MedVL-CT69K are compelling; nonetheless, given it is private, clear documentation of source institutions, de-identification, IRB, and cohort/scan protocol distributions would contextualize generalizability.\n- The improvements on external CT-RATE and Rad-ChestCT are meaningful. Please include 95% CIs, per-task support counts, and formal significance testing. Very high AUCs (approaching 1.0) suggest either small positives or label artifacts; report per-class prevalence and confidence intervals for such cases.\n- The report-generation evaluation is a useful transfer test; however, diagnostic extraction relies on a custom classifier. Provide its validation (AUROC, F1, calibration) and ensure it was not trained on the same reports used for generation to avoid circularity. Adding human evaluation or radiologist QA on a subset would strengthen claims.\n- Ablations are promising but could be more comprehensive: (1) with vs without masks (or using coarse bounding boxes) to quantify reliance on segmentation; (2) with different anatomy groupings; (3) without impression-based correction but with alternative soft-negative treatments (semantic positives from text similarity, RoCo/SigLIP multi-positive labels).\n\n- FaNe addresses false negatives via semantic-aware positive mining and local text-conditioned pooling; your co-teaching and normal correction play a similar role. A discussion contrasting your batch-wise co-teaching with FaNe‚Äôs adaptive semantic normalization would clarify design trade-offs (e.g., reliance on text encoders vs model-predicted similarities).\n- BIUD introduces RoCo (multi-positive contrast) and entity-focused masking to reduce false negatives and emphasize clinical tokens. Comparing your co-teaching and impression-based rule to BIUD‚Äôs robust contrast design would be informative‚Äîespecially under cases of subtle abnormal semantics.\n- Anatomy-VLM demonstrates region-level alignment in 2D CXRs via learnable anatomy queries and structured phrase alignment. Your mask-based token extraction is a natural 3D extension; highlighting differences (e.g., explicit masks vs learned object queries; 3D volumetrics; impression-driven normal correction) would situate novelty.\n- Recent 3D CT models (COLIPRI, SPECTRE, MedVista3D, Uniferum) adopt multi-objective training, geometry-aware architectures, or multi-scale alignments. While some are contemporaneous, acknowledging these directions and clarifying how fVLM complements (or could be combined with) such methods would strengthen positioning.\n\n- The anatomy-aware alignment advances interpretability and could improve radiologist trust through better localization. However, requiring segmentation at inference increases computational cost and introduces a failure mode not present in global VLPs; deployment guidance (precomputing masks, QC checkpoints, handling partial FOV) would be valuable.\n- The use of LLMs for report decomposition raises governance questions (hallucination, bias). Quantifying LLM extraction accuracy (with radiologist adjudication on a stratified subset) and offering a fallback when confidence is low would improve safety.\n- Data privacy and ethics should be elaborated (IRB approvals, consent waivers, de-identification). Given strong performance claims, clear statements on intended use, limits, and out-of-distribution risks are important.\n\n### Questions for Authors\n\n- How accurate are the segmentation masks and report decomposition outputs in your pipeline? Please provide quantitative assessments (e.g., mask Dice vs a labeled subset; precision/recall for anatomy assignment and text extraction vs radiologist ground truth) and an analysis of how errors in either component affect downstream performance.\n- How is inference performed for zero-shot diagnosis‚Äîdoes it require running TotalSegmentator per study? What is the end-to-end latency and GPU memory footprint? Can masks be precomputed or approximated (e.g., coarse ROI pooling), and what is the performance impact?\n- The heuristic ‚Äúnot mentioned in impression implies normal‚Äù is strong. What fraction of anatomies per report fall into this category, and what is the estimated false-normal rate when audited by radiologists? Have you tried alternatives like semantic-positive mining (text‚Äìtext similarity) or SigLIP-style multi-positive soft targets?\n- The metrics in Table 1 appear inconsistent (e.g., F1 greatly exceeding precision; some AUCs equal 1.00). Could you clarify definitions (macro vs micro vs weighted), thresholding, and fix any tabulation errors? Please add CIs and per-class support to interpret very high AUCs.\n- How sensitive are results to the grouping from 104 regions to 36 anatomies? Did you try different grouping granularities, and how do they trade off batch balance, alignment quality, and final AUC?\n- What zero-shot prompts were used for disease detection across the 54 tasks? Please provide exact templates, handling of negation/uncertainty, and any prompt ensemble or calibration techniques.\n- Can you report comparisons to multi-positive/soft-negative objectives (e.g., SigLIP or RoCo) and semantic-positive mining (as in FaNe or MedCLIP) within your pipeline, to isolate the benefit of co-teaching specifically?\n- For report generation, how was the diagnostic extractor validated, and is there any risk of overlap with training data for the generative model? Could you share results using human evaluation or GREEN with radiologist annotations on a subset?\n- Is code and a smaller reproducible subset planned for release? The current link is a placeholder; concrete timelines and artifacts would help the community build upon this work.\n\n### Overall Assessment\n\nThis paper tackles an important limitation of global CT VLPs by introducing anatomy-level alignment and explicit false-negative mitigation, yielding strong zero-shot results in- and out-of-distribution. The approach is well-motivated and technically sound in concept, and the empirical improvements are promising. However, several aspects need tightening before publication at a top-tier venue: rigorous validation of the segmentation and LLM decomposition steps, clearer and statistically robust reporting (including resolving metric inconsistencies and providing CIs), and stronger positioning against recent methods that also address false negatives and local alignment. Additional analyses on robustness (mask/decomposition noise), inference cost, and sensitivity to anatomy grouping would further strengthen the contribution. With these clarifications and expanded comparisons, the work has solid potential to become a reference for fine-grained CT vision‚Äìlanguage pretraining.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 3,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **fVLM**, a fine-grained vision‚Äìlanguage pre-training framework for CT imaging that aligns anatomy-level visual embeddings with corresponding textual descriptions derived from radiology reports. The model integrates automated segmentation (TotalSegmentator) to generate anatomy tokens and employs large-language-model‚Äìbased report decomposition for anatomy-specific text. A dual false-negative reduction strategy is proposed, combining heuristic-based normal correction and co-teaching with soft targets. Evaluations on a large in-house dataset (MedVL-CT69K) and two public benchmarks (CT-RATE, Rad-ChestCT) demonstrate consistent zero-shot performance gains and improved report generation compared to prior CT-VLP baselines. The paper is clearly articulated, technically motivated, and includes detailed methodological and ablation analyses.  \n\n**Major Comments**  \n1. **Dependence on noisy preprocessing:** Heavy reliance on automatically generated anatomy masks and LLM-based report decomposition introduces potential noise; robustness to segmentation or parsing errors is not analyzed.  \n2. **Labeling heuristic risk:** The rule that ‚Äúimpression not mentioning an anatomy implies normal‚Äù may propagate false labels due to variable report styles.  \n3. **Computational and methodological concerns:** Co-teaching doubles computation and may reinforce shared biases without quantitative divergence analysis. Inference cost and segmentation runtime are unreported.  \n4. **Metric validity and reproducibility:** Some evaluation metrics appear inconsistent or implausible (e.g., AUC = 1.00, F1 exceeding precision); metric definitions, zero-shot prompt templates, and code release details are incomplete, reducing reproducibility.  \n5. **Experimental validation gaps:** No sensitivity studies for segmentation errors, report decomposition accuracy, or anatomy grouping (104‚Üí36). Reporting lacks confidence intervals, sample size disclosure, and significance testing.  \n6. **Comparative positioning:** Limited discussion of related work addressing false negatives or region-level alignments (e.g., SigLIP, FaNe, BIUD, COLIPRI, SPECTRE, MedVista3D, Uniferum). Explicit comparisons would strengthen novelty claims.  \n7. **Ethics and deployment:** Data access, IRB procedures, and deployment considerations (e.g., segmentation overhead, handling of partial fields of view) should be detailed.  \n\n**Minor Comments**  \n- Several tables show garbled or misaligned content (e.g., Table 2) and unclear metric definitions (macro vs. micro, thresholds).  \n- Disease-wise sample sizes and confidence intervals are not provided.  \n- Report-generation evaluation requires validation of the diagnostic extractor to rule out data leakage.  \n- Figures and descriptions are clear, but explanations of anatomy grouping decisions could be expanded.  \n\n**Summary Paragraph**  \nOverall, the paper presents a well-motivated and novel approach for anatomy-aware CT vision‚Äìlanguage modeling, contributing meaningful gains in zero-shot and report-generation tasks. Strengths lie in its fine-grained alignment design, dual false-negative mitigation, and comprehensive experimentation. However, before acceptance, the work requires stronger validation of preprocessing robustness, clarification of metrics and statistical reporting, and deeper contextualization among recent related models. Enhancement of reproducibility (public artifacts, detailed prompts, and reliability testing) would further solidify its impact.  \n\n**Decision Recommendation:** **Major Revision** ‚Äì promising approach and strong results, but methodological validation, metric consistency, and comparative positioning require substantial clarification before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: LARGE-SCALE AND FINE-GRAINED VISION-LANGUAGE PRE-TRAINING FOR ENHANCED CT IMAGE UNDERSTANDING\n\n## 1. Opening Paragraph\n\nThis paper addresses a critical limitation in current vision-language models (VLMs) for medical imaging: the coarse global alignment between entire CT images and radiology reports, which fails to capture the localized associations between specific anatomical regions and corresponding diagnostic descriptions. The authors propose fVLM, a fine-grained vision-language model that explicitly matches anatomical regions in CT scans with their corresponding report descriptions through anatomy-level decomposition and contrastive pre-training. To overcome the false negative challenges inherent in fine-grained alignment, they introduce a dual false negative reduction module that identifies and corrects misaligned pairs from both normal and abnormal samples. The authors curate MedVL-CT69K, the largest CT dataset to date comprising 272,124 scans from 69,086 patients, and demonstrate that fVLM achieves 81.3% average AUC on 54 disease diagnosis tasks, outperforming CLIP by 12.9% and supervised methods by 8.0%. The model also shows significant improvements (7.4% and 4.8% absolute AUC gains) on the CT-RATE and Rad-ChestCT benchmarks, respectively, while maintaining strong performance in downstream report generation tasks.\n\n## 2. Major and Minor Comments\n\n**Major Strengths:**\n- The fine-grained anatomy-level alignment approach directly addresses a fundamental limitation of current medical VLMs that use global image-report contrastive learning, which is well-motivated by the clinical reality that radiology reports document findings at the anatomy level.\n- The dual false negative reduction module is a thoughtful solution to the significant challenge of handling semantically similar samples in fine-grained alignment, with the co-teaching strategy effectively mitigating error accumulation.\n- The creation of MedVL-CT69K, the largest CT dataset with paired images and reports to date, represents a valuable resource for the medical AI community and enables comprehensive evaluation across 54 disease tasks.\n\n**Major Concerns:**\n- The paper relies heavily on LLMs (Qwen 2.5) for report decomposition but does not provide a quantitative analysis of the error rate in this critical preprocessing step. Without understanding the accuracy of anatomy-specific description extraction, it's difficult to assess how much the model performance depends on this potentially noisy component.\n- The requirement for detailed anatomical segmentation masks via Totalsegmentator adds significant computational overhead and may limit clinical applicability, as this segmentation step is not part of standard radiology workflows. The paper should better justify this design choice and discuss potential alternatives.\n- While the paper claims to be \"annotation-free,\" it still depends on human-generated radiology reports, which represent a form of expensive annotation. The paper should more clearly acknowledge this limitation and discuss the practical implications.\n\n**Minor Comments:**\n- The clinical relevance of the 54 specific disease tasks could be better explained - which conditions are most critical in practice, and how does fVLM's performance vary across clinically significant versus less critical conditions?\n- The reader study with radiologists is mentioned but lacks sufficient detail in the main text; expanding this would strengthen the clinical validation.\n- The visualization in Figure 1(e) is compelling but would benefit from highlighting which anatomies show the most significant improvement over CLIP to better illustrate the value of fine-grained alignment.\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a fundamental limitation in medical VLMs that has practical implications for clinical deployment. By enabling more precise alignment between image regions and diagnostic descriptions, fVLM has the potential to improve both diagnostic accuracy and model interpretability in radiology - addressing a critical need in medical AI. The demonstrated performance improvements on clinically relevant tasks, including outperforming supervised methods in zero-shot settings, underscore the potential clinical impact.\n\n**Innovation:** The paper presents a clear conceptual advancement over existing approaches. The anatomy-level fine-grained contrastive learning framework is novel and well-motivated, representing a meaningful departure from global alignment strategies. The dual false negative reduction module with co-teaching is an innovative solution to a previously unaddressed challenge in fine-grained medical VLMs. The approach advances the field beyond simply scaling existing methods to creating more clinically relevant representations.\n\n**Evaluation:** The evaluation is generally thorough, with comprehensive results across multiple datasets (MedVL-CT69K, CT-RATE, Rad-ChestCT) and tasks (abnormality detection, report generation). The ablation studies provide insight into the contribution of each component. However, the evaluation would benefit from more detailed error analysis (e.g., which anatomies/diseases benefit most from fine-grained alignment) and deeper exploration of failure cases. The report generation results, while positive, don't fully leverage the anatomy-specific capabilities of the model.\n\n**Reproducibility:** The paper provides reasonable detail about the method and shares code, which is positive. However, several key aspects need clarification for full reproducibility: specific hyperparameters for the false negative correction, complete description of the anatomy grouping strategy (beyond referencing Appendix Table 6), and computational resources required for training. The dependency on LLM-based report decomposition requires more detailed implementation specifications to ensure consistent results across different environments.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nThe paper presents a significant advancement in medical vision-language modeling with strong technical merit and promising results. However, the concerns regarding the LLM-based report decomposition process, the clinical applicability of the segmentation requirement, and the need for more thorough error analysis and clinical relevance discussion require substantial revisions. The authors should provide quantitative analysis of the report decomposition accuracy, more detailed clinical validation, and clearer discussion of practical implementation considerations before this work can be recommended for publication in IEEE Transactions on Medical Imaging. These revisions would strengthen the paper's contribution and better establish the clinical relevance and robustness of the proposed approach.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes **fVLM**, a fine-grained vision-language pre-training framework aimed at improving CT image understanding by linking anatomical regions in scans to corresponding textual descriptions in radiology reports. This approach addresses the limitation of existing vision-language models that focus on coarse, image-level alignments and fail to capture localized diagnostic associations. The model introduces a **dual false negative reduction module** to handle semantically similar samples and leverages the newly curated **MedVL-CT69K dataset**, the largest paired CT-image‚Äìreport dataset to date. Reported results show substantial performance gains over baseline models across multiple benchmarks and tasks. The paper is clearly written and technically detailed, offering an important contribution to medical image‚Äìlanguage modeling.\n\n---\n\n**Major Comments**  \n1. **LLM-based Report Decomposition:** The method depends on Qwen 2.5 for anatomy-level report parsing but lacks quantitative analysis of decomposition accuracy. The paper should report the error rate or reliability of this critical preprocessing step, as the model‚Äôs performance may hinge on potentially noisy outputs.  \n2. **Segmentation Requirement:** The reliance on Totalsegmentator for detailed anatomical masks introduces heavy computational demands and may limit clinical applicability. The authors should better justify this design choice and discuss feasible alternatives, especially given that segmentation is not standard in routine radiology workflows.  \n3. **Annotation-Free Claim:** The claim of being ‚Äúannotation-free‚Äù is overstated, since the model still depends on radiologist-generated reports. The authors should acknowledge this dependency and clarify its practical and cost implications.  \n\n---\n\n**Minor Comments**  \n- Clarify the **clinical relevance** of the 54 disease tasks and indicate which are most critical in practice.  \n- Provide more detail on the **radiologist reader study** to reinforce the clinical evaluation.  \n- Enhance **Figure‚ÄØ1(e)** by highlighting anatomies where fVLM surpasses CLIP most clearly.  \n\n---\n\n**Summary Paragraph**  \nThe work substantially advances medical vision‚Äìlanguage modeling through anatomy-level representation learning and a novel dual-module strategy for error reduction. It provides compelling empirical results across large and diverse datasets. However, the lack of quantitative validation for LLM-based decomposition, the added segmentation overhead, and limited clinical validation weaken claims of practicality and reproducibility. Addressing these points would significantly strengthen the paper‚Äôs contributions and its relevance for real-world clinical integration.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The study is technically innovative and potentially impactful but requires additional quantitative evaluation, justification of preprocessing dependencies, and expanded clinical discussion before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the problem of coarse‚Äëgrained image‚Äëtext alignment in CT‚Äêbased artificial intelligence by exploiting radiology reports as supervisory signals. To this end the authors assemble a large collection‚Äî272‚ÄØ124 CT examinations from 69‚ÄØ086 patients with accompanying reports‚Äînamed MedVL‚ÄëCT69K, and define 54 diagnostic tasks spanning 15 anatomical regions. Their proposed fine‚Äëgrained vision‚Äëlanguage model (fVLM) aligns segmented anatomical structures with the corresponding sentences in the report, performs contrastive pre‚Äëtraining separately for each anatomy, and incorporates a dual false‚Äënegative reduction (FNR) component that distinguishes normal from abnormal pairs via a co‚Äëteaching scheme. In a series of zero‚Äëshot experiments on the curated dataset the model attains an average AUC of 81.3‚ÄØ% across the diagnostic tasks. The authors present this as a scalable, annotation‚Äëfree framework that furnishes anatomy‚Äëlevel image‚Äëtext correspondence for CT data.  \n\n---  \n\n## General feedback  \n\n*Significance.*  The paper addresses an acknowledged limitation of many medical vision‚Äëlanguage models, namely the lack of fine‚Äëgrained anatomical grounding, and therefore proposes a direction that could improve interpretability and downstream utility.  \n\n*Innovation.*  The explicit anatomy‚Äëlevel matching and the dual FNR/co‚Äëteaching mechanism constitute a novel extension of existing global‚Äëcontrastive and implicit local‚Äëalignment approaches (see Sec.‚ÄØ2.2).  \n\n*Evaluation.*  The authors report extensive zero‚Äëshot results on an internal dataset and two public benchmarks, and also include a report‚Äëgeneration experiment. However, the presentation omits statistical significance testing, per‚Äëdisease breakdowns, and comparisons with strong supervised baselines, which limits the interpretability of the reported gains.  \n\n*Reproducibility.*  While code is made available, many essential details‚Äîincluding the anatomy‚Äëgrouping strategy, the LLM prompting pipeline, random‚Äëcrop handling, and the hyper‚Äëparameters‚Äîare relegated to the supplementary material. Moreover, the MedVL‚ÄëCT69K dataset is proprietary, hindering independent replication.  \n\n---  \n\n## Specific comments/critiques  \n\n1. **Dataset claim.**  The statement that MedVL‚ÄëCT69K is ‚Äúthe largest CT dataset to date‚Äù (Sec.‚ÄØ4.1) is unsupported. A quantitative comparison with existing public resources such as LIDC‚ÄëIDRI or TCIA, together with a citation or a summary table, is required to substantiate this claim.  \n\n2. **Report decomposition.**  Section‚ÄØ3.1 relies on a proprietary large‚Äëlanguage‚Äëmodel prompt pipeline to split reports into anatomy‚Äëspecific sentences. The manuscript offers no assessment of how sensitive the downstream alignment is to variations in prompts or to updates of the underlying LLM. An ablation or robustness analysis would be essential.  \n\n3. **False‚Äënegative reduction assumption.**  The FNR component assumes that the absence of a given anatomy in the *impression* section indicates a normal finding (Fig.‚ÄØ4). In practice many radiology reports simply omit normal statements, which could lead to systematic mislabeling of abnormal cases as normal. The paper lacks an error analysis quantifying the impact of this assumption.  \n\n4. **Training hyper‚Äëparameters.**  Critical settings such as learning rate, batch size, number of epochs, and the contrastive temperature œÑ are buried in Appendix‚ÄØA.2. For transparency these should be summarized in the main text, preferably in a concise table.  \n\n5. **Table‚ÄØ1 presentation.**  The current formatting (e.g., ‚Äú~~68.4~~‚Äù, ‚Äú~~**81.3**~~‚Äù) obscures the actual numbers and precludes evaluation of variance or statistical significance. A clean table with clear values and confidence intervals is needed.  \n\n6. **Statistical reporting of AUCs.**  Zero‚Äëshot AUC improvements are presented without accompanying 95‚ÄØ% confidence intervals or hypothesis tests. Providing such statistics would allow readers to judge whether the observed differences are meaningful.  \n\n7. **Report‚Äëgeneration metrics.**  Table‚ÄØ3 mixes an undefined ‚ÄúGREEN‚Äù score with standard natural‚Äëlanguage‚Äëgeneration metrics. The manuscript should briefly define GREEN, explain why it is relevant, and compare it to established metrics.  \n\n8. **Scaling‚Äëlaw analysis.**  Figure‚ÄØ5 depicts performance versus dataset size, yet the description does not explain how the data subsets were constructed nor whether training hyper‚Äëparameters were kept constant across scales. Without this information the scaling experiment cannot be reproduced.  \n\n9. **Ethical considerations.**  The manuscript does not discuss patient privacy, consent, or Institutional Review Board approval for the MedVL‚ÄëCT69K dataset. Given the clinical nature of the data, a thorough ethical statement is mandatory.  \n\n10. **Reader study.**  Section‚ÄØ4.2 mentions a reader study comparing fVLM output to board‚Äëcertified radiologists, but provides no quantitative results, study design details, or statistical analysis. This omission weakens any claim of clinical relevance.  \n\n---  \n\n## A suggested decision  \n\n**Reject**  \n\n*Rationale.*  The work raises serious ethical concerns related to patient privacy and consent that are not addressed, and the methodological limitations outlined above prevent the results from being considered reliable or reproducible. Until these issues are resolved, the manuscript cannot be accepted for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 3,
          "specificity": 4,
          "correctness": 3,
          "constructiveness": 4,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates coarse‚Äëgrained image‚Äìtext alignment in CT‚Äëbased artificial intelligence, aiming to establish anatomy‚Äëlevel correspondence between radiology images and report sentences. The authors construct a large‚Äëscale dataset (MedVL‚ÄëCT69K) comprising 272‚ÄØ124 CT studies from 69‚ÄØ086 patients and define 54 diagnostic tasks across 15 anatomical regions. A fine‚Äëgrained vision‚Äëlanguage model (fVLM) is proposed that links segmented anatomies to textual descriptions using contrastive pre‚Äëtraining for each region and a dual false‚Äënegative reduction (FNR) mechanism based on co‚Äëteaching to separate normal from abnormal examples. The manuscript claims scalability and annotation‚Äëfree supervision and demonstrates zero‚Äëshot performance averaging 81.3‚ÄØ% AUC. The work is generally clear and addresses a pertinent limitation of existing medical vision‚Äëlanguage models, though key methodological and ethical details are insufficiently developed.  \n\n**Major Comments**  \n1. **Significance and Novelty:** The paper advances fine‚Äëgrained anatomical grounding through explicit region‚Äëlevel alignment and a dual FNR/co‚Äëteaching scheme. However, comparative evaluation with robust supervised baselines is lacking, making it difficult to assess real performance gains.  \n2. **Dataset Claims:** The assertion that MedVL‚ÄëCT69K is the largest CT dataset is unsubstantiated. Quantitative comparisons with prior public datasets and supporting references or a summary table are required.  \n3. **LLM‚ÄëBased Report Decomposition:** The proprietary prompting procedure used to generate anatomy‚Äëspecific sentences is insufficiently validated; sensitivity analysis or ablations are needed to assess robustness to prompt or model variations.  \n4. **FNR Assumption:** Inferring normality from the omission of anatomical mentions may cause systematic mislabeling. An error analysis quantifying the resulting bias is necessary.  \n5. **Evaluation Design:** Missing statistical significance tests, per‚Äëdisease breakdowns, and 95‚ÄØ% confidence intervals for reported AUCs weaken interpretability.  \n6. **Reproducibility:** Key hyper‚Äëparameters (learning rate, batch size, epochs, temperature) are only in the appendix. Including them in the main text would improve transparency.  \n7. **Reporting and Metrics:** The formatting of results tables obscures real values; GREEN metric lacks definition and justification; scaling‚Äëlaw construction details are absent. These should be clarified to enable replication.  \n8. **Ethics and Clinical Validation:** No discussion of patient privacy, consent, or institutional review approval is provided. The described reader study lacks quantitative results and design detail, limiting credibility of clinical claims.  \n\n**Minor Comments**  \n- Ensure clean numerical tables with defined confidence intervals.  \n- State whether training settings were held constant in scaling experiments.  \n- Provide concise definitions for all metrics and acronyms.  \n\n**Summary Paragraph**  \nThis study proposes an anatomically grounded vision‚Äëlanguage model for CT interpretation, supported by a large internal dataset and an alignment‚Äëoriented training approach. The work‚Äôs conceptual direction is promising, but the absence of statistical rigor, critical implementation details, and transparency about ethical oversight undermines the claimed contributions. Without clearer methodological validation and explicit ethical review, reproducibility and reliability remain questionable.  \n\n**Decision Recommendation**  \n**Reject.** The manuscript contains unresolved ethical concerns and lacks sufficient experimental transparency and statistical substantiation to support its conclusions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Le Lu",
      "Lin Yang",
      "Ling Zhang",
      "Qi Zhang",
      "Ruizhe Guo",
      "Sinuo Wang",
      "Tingbo Liang",
      "Weiwei Cao",
      "Xianghua Ye",
      "Zhongyi Shui",
      "Jianpeng Zhang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_2d4fcaeb7d709760b3ac4de319e56760ca9ed11b.pdf",
    "remote_url": "https://openreview.net/pdf/2d4fcaeb7d709760b3ac4de319e56760ca9ed11b.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Vision-language model;  zero-shot enhancement; Structural Representation"
    ],
    "abstract": "Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as prompts for the target disease name during the inference phase.  \nHowever, these methods typically treat prompts as equivalent context to the target name, making it difficult to assign specific disease knowledge based on visual information, leading to a coarse alignment between images and target descriptions. In this paper, we propose StructuralGLIP, which introduces an auxiliary branch to encode prompts into a latent knowledge bank layer-by-layer, enabling more context-aware and fine-grained alignment. Specifically, in each layer, we select highly similar features from both the image representation and the knowledge bank, forming structural representations that capture nuanced relationships between image patches and target descriptions. These features are then fused across modalities to further enhance detection performance.\nExtensive experiments demonstrate that StructuralGLIP achieves a +4.1\\% AP improvement over prior state-of-the-art methods across seven zero-shot medical detection benchmarks, and consistently improves fine-tuned models by +3.2\\% AP on endoscopy image datasets.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a zero-shot object detection model, StructrualGLIP, for medical image-based object detection tasks. The method uses a knowledge bank that stores encoded text prompts, which are later used to select and match relevant image features to form fine-grained structural representations to allow better alignment of image features with the prompt information, achieving accurate, context-aware detection.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. the paper demonstrates a comprehensive evaluation of four medical image modalities.\n2. the paper's representation and structure are mostly clear and easy to follow (although with some language and word choice issues, which will be discussed below).\n\n### Weaknesses\n\n1. The claimed novelty appears to be on the latent knowledge bank and its function to store encoded prompt tokens and later be used at each encoder layer as a vision token selector and vice versa. This is coined as a mutual selection process. The selected information is then merged into the original image and text encoder back at each layer. The entire process is different to a standard contextual prompt method (Fig. 1(a)) but it feels like a quite incremental difference which I don't see much novelty.\n2. The word choice of \"medical detection\" bugs me, in medical science detection refers to \"detecting diseases\" whereas here this is object detection in medical images so this may cause confusion to certain readers.\n3. Line 234 looks unfinished.\n4. Eq. 6&7, perhaps the Top-P/Q^{max} function can be simplified by using argmax/argsort function?\n5. L201/534: \"... fine-grained alignment between target descriptions and medical images\", L:256 \"forming fine-grained structural representations\", can the authors clarify what \"fine-grained\" refers to in those places?\n6. L294: \"...like BLIP Li et al. (2022a)\", is the VQA model BLIP or not? Have you considered other VQA models and would the performance of other VQA models fluctuate your detection results?\n7. L177: I don't entirely agree that zero-shot detection has a real-world clinical need as the clinicians I've encountered would not trust zero-shot settings, in the medical domain, accurate detection/segmentation/diagnosis is the the most important thing.\n\n### Questions\n\nPlease address Weaknesses #1,5,6.\n\n\n------------------\n04/12/2024:\nI thank the authors' detailed responses. Since the reviewer's post deadline has passed so I am writing my final comments here. \n\nMy conclusion is that I will retain my original score. I believe the paper should improve clarity as many new arguments and technical details surfaced during the discussion period which should have been included in the main manuscript. The novelty and settings should be explored in more depth.\n\nMy comments are:\n\n1. Novelty. \n\nI agree with the authors' argument the image and language encoders do not need to be fine-tuned in your zero-shot detection setting. This I already mentioned in my last comment: \"I can understand if the auxiliary branch's language encoder is untrained\". My question was whether the modules in Eq. 8 to 10 need training or not? Especially the MHA module in Eq. 8, GLIP has the X-MHA module, did the author reuse the X-MHA weights for the MHA without retraining? The authors mentioned in L263 \"we employ a multi-head attention (MHA) mechanism\" which reads like it is a new module by the authors' own design. Given an MHA module (as far as I know) should have learnable weights but the authors claim the zero-shot detection setting is training-free, then I find the logic here is contradicted. Furthermore, the authors quoted L210 to demonstrate the training-free paradigm, but that is immediately after Eq.4 and the sentence only describes the encoders. This is an imprecise response to my question so I'm not making a judgment here, but this does make me think the paper should improve clarity. \n\nThe above was not the main point of my original novelty concern, but a spin-off. My original point was that if the authors wanted to demonstrate the \"preventing/addressing domain shifting\" argument for the novelty, you should consider measuring the actual feature distribution difference between GLIP and StructureGLIP.  The authors revealed in the last comments that the domain referred to the features before RPN, and that is what I think you should demonstrate. To recap the authors' comments, the authors claimed the other methods would concatenate prompt so the feature to RPN becomes CLS token of [target_name, prompt] which causes the domain shift, whereas StructualGLIP did the prompts integration in early layers so StructuralGLIP only has CLS of [target_name] to the RPN, and that addresses the domain shift. Conceptually, maybe it is true but please provide empirical evidence. The final performance is a surrogate measure, which does not directly support your domain shift argument. Finally, StructrualGLIP also fuses additional information in the process, so the CLS token should exhibit some domain shift as you are trying to do domain adaptation, these two arguments also have contradictions to each other. \n\nRegarding the \"addressing noise prompt\" argument, I'd imagine the top-P/Q has a certain capability of limiting noise prompts as you only choose the top tokens, so even when you add more prompts, they won't be selected if the network was already trained to have higher attention for the target classes tokens, but that on the other hand, make it trivial argument rather than a novelty.\n\n7. Zero-shot enhancement. After reading the authors' most recent comments, I strongly feel the setting should not be called zero-shot. When the target dataset's labelled data are already used to fine-tune your GLIP base model, then this is weakly supervised. In L175-176, the authors state \"We propose a zero-shot enhancement setting. This involves fine-tuning the model on medical datasets first, and then using prompts to further improve performance on unseen medical images\", initially I considered this as the model was trained on relevant datasets such as same/similar imaging modalities but different tasks or different datasets. However, if the target dataset itself was used for finetuning for training, then the \"further improve performance on unseen medical images\" does not hold.\n\nOther comments:\nThe experiments I requested were for the understanding of how much performance gain was attributable to the design/novelty of StructrualGLIP, i.e., the introduction of the auxiliary branch. The zero-shot enhancement setting (i.e., fine-tuned GLIP) has boosted the performance by a larger margin than the StructualGLIP can bring to GLIP. Also, the choice of the VQA can sometimes affect the AP/AP50 by a noticeable margin (i.e., that 75 AP) but it appears not stable. Then, at least for me, the viability of zero-shot medical detection unfortunately remains questionable.\n\nOnce again I appreciate the authors' effort to address my comments but I'm sorry I will retain my original score based on my thoughts above. The authors are encouraged to delve into the details of the novelty and concepts as well as improve clarity of the manuscript.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a zero-shot object detection model tailored for medical imaging applications. The approach employs a latent knowledge bank that encodes textual prompts and enables layer-wise selection of relevant visual and textual tokens to form fine-grained structural representations. The aim is to better align image features with textual context, thereby improving context-aware object detection across multiple medical modalities. The paper offers clear organization and extensive experimentation, although certain terminological and clarity issues remain.\n\n---\n\n**Major Comments**  \n1. **Novelty and Conceptual Clarity**: The proposed mutual selection process via a latent knowledge bank differs only incrementally from existing contextual prompt methods. The conceptual novelty is limited and requires stronger justification through empirical evidence or new insights.  \n2. **Zero-Shot Definition and Setting**: The description of the ‚Äúzero-shot enhancement‚Äù setting is inconsistent. Since the GLIP base model is fine-tuned on target datasets, the approach would more accurately be described as weakly supervised rather than zero-shot.  \n3. **Training-Free Logic**: The use of multi-head attention (MHA) modules (Eq.‚ÄØ8‚Äì10) raises logical contradictions with the claimed training-free paradigm, as such modules typically include learnable parameters. Clarification on whether pretrained weights are reused is needed.  \n4. **Domain Shift Argument**: The authors claim that StructuralGLIP mitigates domain shift by fusing prompts early in the pipeline, yet no empirical evidence is provided to quantify feature distribution changes. Demonstrating this would solidify the claimed contribution.  \n5. **Evaluation and Experimental Interpretation**: Additional experiments isolating the contribution of the proposed auxiliary branch would clarify how much improvement stems from StructuralGLIP itself versus the fine-tuning process. The variable influence of the VQA model on metrics (AP,‚ÄØAP50) also warrants clarification.\n\n---\n\n**Minor Comments**  \n- The term *‚Äúmedical detection‚Äù* should be revised to avoid confusion with *disease detection*.  \n- Line‚ÄØ234 appears incomplete.  \n- Equations‚ÄØ6‚Äì7 might be simplified using standard `argmax` or `argsort` operations.  \n- The meaning of ‚Äúfine-grained‚Äù in several cited lines (201,‚ÄØ256,‚ÄØ534) should be clarified.  \n- Clarify whether BLIP is indeed the VQA model used and discuss the effect of alternative VQA choices.  \n- Minor language and word-choice issues remain and should be polished for fluency.\n\n---\n\n**Summary Paragraph**  \nThe paper is well structured and demonstrates thorough evaluation across four imaging modalities. However, the conceptual originality of the mutual selection mechanism and the coherence of the ‚Äúzero-shot‚Äù formulation are questioned. The logical consistency of the training-free claim and the lack of empirical evidence on domain adaptation weaken the methodological justification. While the topic is relevant and the experiments comprehensive, important clarifications and deeper analyses are necessary to strengthen the contribution.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The manuscript requires substantial clarification of novelty claims, the zero-shot setting, and experimental validation before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduce a new zero-shot method for medical Vision-Language Models(VLMs) for detecting unknown targets. To address the prompt umatched with the variations in the medcial images, the authors propose a StructualGLIP desgin with a main and an auxiliary branch encoders for text input and introduce a mutual selcetion mechanism. \nThe author explain that the auxiliary branch would work as a knowledge bank where the main branch can extract latent prompt tokens, while the tokens in the knowledge banked are filtered by the mutual selection process.\nOverall, the motivation, method, and performance of this work is good enough, but I still need some explaination for some detail, please refer to the weakness and question section. I will consider adjust my rating based on the authors response.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. This work is aiming for a vital issue in medical image understanding field, which is the generalization capability of foundation models with limit data access. The motivation of improving the existing work is clear and strong, which is the lack of object level prompt and fail to capture the various feature of images during prompt desgin.\n2. This work present a novel but efficient method, called StructualGLIP, to increase the model's zero-shot/few-shot performance on various datasets. The desgin of StructualGLIP introduce the knowledge bank and mutual selection process to help prompt design process. This method address several shortcomings of current method and is novela and effective.\n3. This method largely increased the zero-shot performance on different medical image datasets across different modalities.\n\n### Weaknesses\n\n1. Line 234, the sentence seems not finished. \n2. One of the major problem of the proposed method is not trainable as the Top-K selection operation is non-differentiable, while previous work is differentiable and thus finetuning would result in better performance. I would suggest include Reparameterization Trick for Gumbel-Softmax to improve your method. Though this work is good enough as a stand-alone method for zero-shot detection. But I see a potential to achieve better performance.\n\n### Questions\n\n1. How to evaluate the quality of the generated prompts by VQA method. As some previous work pointed out, the VLM without medical domain adaptation perform poorly on some medical datasets, especially for radiology datasets.\n2. For Prompt as Knowledge Bank ablation study seciton, I would like to see an experiment on the whether adding noisy knowledge (for example, knowledge for another target) would sharply downgrade the StructureGLIP performance. This experiment would the the robustness of the mutual-selection process.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nNA",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *StructuralGLIP*, a new zero-shot approach for medical vision‚Äìlanguage models (VLMs) aimed at detecting unknown targets. The method seeks to address mismatches between prompts and variations in medical images by employing a dual‚Äëbranch architecture comprising a main and an auxiliary text encoder, combined with a mutual selection mechanism. The auxiliary branch acts as a knowledge bank from which the main branch extracts latent prompt tokens, filtered through the mutual selection process. The paper presents clear motivation and reports good performance across multiple datasets. While the work appears sound and potentially impactful for medical image understanding, certain methodological and presentation details require further clarification.  \n\n**Major Comments**  \n1. The proposed method‚Äôs Top‚ÄëK selection operation is non‚Äëdifferentiable, preventing model finetuning, whereas related prior approaches allow such optimization. Incorporating a differentiable approximation (e.g., reparameterization or Gumbel‚ÄëSoftmax) could address this limitation and possibly improve results.  \n2. The mechanism for evaluating prompt quality, particularly when using a VQA method, is unclear. Given evidence that general VLMs perform poorly on some medical datasets without domain adaptation, further detail on evaluation methodology would strengthen the contribution.  \n3. The robustness of the mutual selection process is not fully assessed. An experiment introducing noise into the knowledge bank (e.g., knowledge related to other targets) would better demonstrate the model‚Äôs stability.  \n4. One sentence around line 234 appears incomplete; this should be corrected for clarity.  \n\n**Minor Comments**  \n- Some minor language and typographical issues are present (‚Äúintroduce‚Äù ‚Üí ‚Äúintroduces,‚Äù ‚Äúdesgin‚Äù ‚Üí ‚Äúdesign,‚Äù etc.).  \n- Ensure consistent spelling of terms such as ‚Äúmedical‚Äù and ‚Äústructural.‚Äù  \n- Clarify figure and table references to improve readability.  \n\n**Summary Paragraph**  \nOverall, the paper tackles an important problem‚Äîenhancing generalization in medical image understanding with limited data‚Äîthrough a novel structural prompt design integrating a knowledge bank and mutual selection mechanism. The method shows substantial zero‚Äëshot improvements across modalities and is conceptually well motivated. However, concerns about non‚Äëdifferentiability, unclear evaluation of prompt quality, and incomplete or unclear text require attention. Addressing these points would significantly strengthen the submission.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The study presents a promising and innovative method but needs methodological clarification and minor text corrections before it is ready for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors introduce StructuralGLIP, a novel approach to zero-shot medical detection using vision-language models (VLMs). This method leverages structured representations within a dual-branch architecture that enables nuanced alignment between images and textual prompts, significantly enhancing the model's adaptability to new medical scenarios without needing annotated data. StructuralGLIP uses category-level prompts, maintained in a latent knowledge bank, and a mutual selection mechanism for precise cross-modal fusion, thus improving accuracy across diverse medical imaging datasets.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n(1)\tThe paper introduces an effective structural representation by encoding prompts into a knowledge bank and utilizing a dual-branch structure. This approach enables adaptive and context-aware alignment, which is particularly advantageous for complex medical detection tasks.\n(2)\tStructuralGLIP outperforms traditional zero-shot models by effectively handling both instance-level and category-level prompts, achieving significant improvements across various benchmarks in endoscopy, microscopy, radiology, and more.\n(3)\tBy allowing for zero-shot enhancement, the model can be fine-tuned and then further improved with category-level prompts, a feature well-suited for dynamic medical settings where data annotation is scarce.\n\n### Weaknesses\n\n(1)\tThe proposed dual-branch structure with a knowledge bank requires complex engineering and computational resources, potentially limiting its accessibility for practitioners in less resource-rich environments.\n(2)\tThe paper may not adequately address the potential data imbalance present in the datasets used for evaluation. Some diseases or conditions may have significantly fewer examples, which could impact the model's performance and generalizability.\n(3)\tThe model's inner workings, particularly regarding how it selects and utilizes prompts, may be difficult for practitioners to interpret, limiting trust in its decisions and making it harder to diagnose potential failures.\n(4)\tDespite improvements in alignment, there may still be instances of misalignment between visual features and prompts, especially in cases of atypical presentations, which could lead to missed detections.\n\n### Questions\n\n(1)\tTo what extent can the findings be generalized to other medical imaging modalities or less common diseases? Are there plans to evaluate the model on broader datasets?\n(2)\tBesides Average Precision, what other metrics were considered for evaluating model performance? Are there plans to incorporate user feedback or clinical outcomes in future evaluations?\n(3)\tThis paper focuses on zero-shot medical detection, whereas GLIP was initially developed for natural images. Can the proposed method also be applied effectively to natural image datasets?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a novel vision-language framework designed for zero-shot medical detection. The method introduces structured representations within a dual-branch architecture to enable refined alignment between image features and textual prompts. By maintaining category-level prompts in a knowledge bank and implementing a mutual selection mechanism, the approach aims to enhance adaptability and cross-modal understanding without requiring annotated data. Overall, the study demonstrates meaningful improvements across diverse medical imaging domains, though certain methodological and interpretability challenges remain.\n\n**Major Comments**  \n1. **Model Complexity and Accessibility** ‚Äì The dual-branch structure combined with the knowledge bank introduces substantial engineering and computational demands, which could limit practical adoption, particularly in resource-constrained medical environments.  \n2. **Data Imbalance Considerations** ‚Äì The evaluation may not fully account for data imbalance across conditions and datasets. Uneven representation of disease categories could affect both model reliability and generalizability.  \n3. **Interpretability of Prompt Mechanisms** ‚Äì The internal prompt selection and utilization processes are not clearly explained. Limited interpretability may reduce practitioner trust and make error analysis more difficult.  \n4. **Residual Misalignment Issues** ‚Äì Despite improved alignment, potential misalignment between textual prompts and visual features might persist, particularly in atypical or complex clinical cases, potentially leading to detection failures.  \n5. **Scope of Evaluation** ‚Äì It would be useful to address generalizability across additional imaging modalities or rare diseases. Clarification on evaluation metrics beyond Average Precision, and consideration of clinically relevant outcomes or user feedback, would further strengthen the study.  \n6. **Applicability Beyond Medical Images** ‚Äì Since GLIP originated for natural images, an explicit discussion of how StructuralGLIP might transfer or adapt to natural image domains would broaden the paper‚Äôs contribution.\n\n**Minor Comments**  \n- Clarify details of the datasets used, especially class distributions.  \n- Include more discussion on computational overhead and runtime comparisons.  \n- Improve explanation of prompt selection in figures or schematics for better clarity.\n\n**Summary Paragraph**  \nThe paper makes a noteworthy contribution through its structured approach to prompt-based, zero-shot medical detection and demonstrates strong empirical results. Its strengths lie in the methodological innovation, adaptability, and cross-domain potential. However, concerns about complexity, data balance, interpretability, and the breadth of evaluation limit full confidence in the current form. Addressing these issues would substantially enhance the impact and clarity of the work.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1‚ÄØ‚Äì Analytical and‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI**  \n‚úÖ‚ÄØThe work proposes **StructuralGLIP**, a novel vision‚Äìlanguage framework for *zero‚Äëshot medical detection*.  Its main innovation‚Äîa dual‚Äëbranch ‚Äúknowledge‚Äëbank‚Äù architecture with layer‚Äëwise mutual selection between image and prompt features‚Äîconstitutes a **methodological advancement** in multimodal learning and object detection for medical imaging.  The manuscript fits well within the IEEE‚ÄØTMI scope (image reconstruction, learning‚Äëbased detection, quantitative analysis).\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel**  \nThe idea of embedding prompts into a hierarchical latent memory and dynamically selecting sub‚Äëtokens for cross‚Äëmodal fusion goes beyond prior GLIP or prompt‚Äëpool methods.  Extending ‚Äúzero‚Äëshot‚Äù to a *zero‚Äëshot‚Äëenhancement* setting for fine‚Äëtuned detectors is also new.  Novelty is therefore moderate‚Äëto‚Äëhigh for TMI standards.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \nThe theoretical description is detailed and mathematically consistent.  Experiments cover eight heterogeneous imaging datasets with multiple baselines, including ablations on hyperparameters P/Q, fusion‚Äëlayer choice, prompt noise, and LLM type.  However, quantitative significance tests and runtime/complexity analyses are missing, and reproducibility details (code, pretrained weights, IRB where needed) should be clarified.\n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation**  \nWriting quality is good overall, though the manuscript is lengthy and occasionally overloaded with notation.  Figures are informative but dense; captions could be more self‚Äëexplanatory.  Minor grammatical polishing and consistent notation would improve readability.\n\n**5.‚ÄØEthical‚ÄØand‚ÄØReproducibility‚ÄØCompliance**  \nDatasets are all public; human/animal subject issues are minimal.  The authors state that code will be released, but this should be verified before publication.\n\n---\n\n**Phase‚ÄØ2‚ÄØ‚Äì IEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n**1.‚ÄØSummary**  \nThe paper introduces **StructuralGLIP**, a dual‚Äëbranch vision‚Äìlanguage model for zero‚Äëshot medical object detection.  A main branch processes image and target names, while an auxiliary branch encodes textual prompts into a *latent knowledge bank*.  A mutual‚Äëselection mechanism retrieves the most relevant prompt and image tokens at each layer to form structural representations that enhance cross‚Äëmodal alignment.  The method supports both instance‚Äë and category‚Äëlevel prompts and extends to a ‚Äúzero‚Äëshot enhancement‚Äù mode that boosts performance of fine‚Äëtuned detectors without extra labels.  Experiments on endoscopic, microscopic, photographic, and radiologic datasets show consistent improvements over GLIP, MIU‚ÄëVL, AutoPrompter, and MPT baselines.\n\n**2.‚ÄØStrengths**\n- Clear methodological innovation in structural prompt encoding and layer‚Äëwise mutual selection.  \n- Comprehensive evaluation across multiple imaging modalities.  \n- Demonstrated ability to exploit category‚Äëlevel prompts efficiently‚Äîreducing reliance on image‚Äëspecific prompt generation.  \n- Zero‚Äëshot‚Äëenhancement scenario is practically relevant for continuous model improvement in clinical settings.  \n- Solid ablation and analysis sections demonstrating mechanism behavior.\n\n**3.‚ÄØWeaknesses**\n- Limited theoretical justification of why mutual selection converges or how Top‚ÄëP/Q are chosen; lacks complexity and timing analysis.  \n- Statistical significance or confidence intervals of the reported AP gains are not provided.  \n- Section organization is somewhat verbose; methodology and experiments could be condensed.  \n- No external clinical validation or reader study to demonstrate impact on real clinical workflows.  \n- Reproducibility statement should include dataset licensing confirm‚Äë pliance and actual code availability date.\n\n**4.‚ÄØMajor‚ÄØComments**\n1. **Quantitative significance**‚ÄØ‚Äì add statistical analysis (e.g., paired t‚Äëtest or bootstrap CIs) of AP improvements to substantiate claimed gains.  \n2. **Computational cost**‚ÄØ‚Äì report model parameters, GPU hours, and inference latency compared to GLIP; discuss scalability for clinical use.  \n3. **Clarify mutual‚Äëselection implementation**‚ÄØ‚Äì specify how Top‚ÄëP/Q are determined and whether gradients flow through selection.  \n4. **Reproducibility**‚ÄØ‚Äì include hyperparameter tables, seeds, and ensure public release of prompt lists and code upon acceptance.  \n5. **Generalizability/clinical translation**‚ÄØ‚Äì discuss how category‚Äëlevel prompts could be standardized or curated in clinical vocabularies (e.g.,‚ÄØSNOMED concepts) to facilitate deployment.\n\n**5.‚ÄØMinor‚ÄØComments**\n- Streamline mathematical notation; unify subscript conventions \\(i\\),‚ÄØ\\(l\\),‚ÄØ\\(v\\).  \n- Improve Figure‚ÄØ1 and‚ÄØ3 captions; label components clearly.  \n- Minor grammar issues (‚Äúfine‚Äëgrained a‚ÄØlignment‚Äù ‚Üí ‚Äúalignment‚Äù, ‚ÄúFinetuned‚Äù ‚Üí ‚ÄúFine‚Äëtuned‚Äù).  \n- Verify reference duplication (Li‚ÄØet‚ÄØal.,‚ÄØ2022‚ÄØb/c appear identical).  \n- Provide units or resolution info for datasets in Appendix‚ÄØC.  \n\n**6.‚ÄØOverall‚ÄØRecommendation**  \n**Major‚ÄØRevision.**  \nThe manuscript offers strong methodological novelty and promising results but needs additional quantitative analysis, presentation tightening, and reproducibility assurances to meet TMI publication standards.\n\n**7.‚ÄØConfidence‚ÄØLevel:**‚ÄØ4‚ÄØ/‚ÄØ5  \n(High confidence in evaluation of novelty and technical quality; moderate confidence regarding implementation‚Äëlevel details due to limited reproducibility information.)\n\n---\n\n**Summary Note to Editors:**  \nStructuralGLIP is scientifically interesting and aligns with the journal‚Äôs emphasis on fundamental methodological advances in imaging AI.  With more rigorous quantitative validation and improved clarity, the paper could become a solid contribution to zero‚Äëshot and prompt‚Äëdriven medical image detection research.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a vision‚Äìlanguage framework for zero‚Äëshot medical object detection. The approach integrates a dual‚Äëbranch ‚Äúknowledge‚Äëbank‚Äù architecture that performs layer‚Äëwise mutual selection between image and prompt features to enhance cross‚Äëmodal alignment. This structural prompt mechanism enables both instance‚Äë and category‚Äëlevel prompting and introduces a ‚Äúzero‚Äëshot enhancement‚Äù mode that improves performance of fine‚Äëtuned detectors without additional labels. The work is clearly written and methodologically detailed, offering a substantial contribution to multimodal learning in medical imaging, though the manuscript would benefit from tighter organization and improved reproducibility documentation.\n\n---\n\n**Major Comments**  \n1. **Statistical validation:** Include quantitative significance testing (e.g., paired tests or confidence intervals) to substantiate reported average precision improvements.  \n2. **Computational efficiency:** Provide parameter counts, training time, and inference latency compared to GLIP or similar baselines, and discuss the implications for clinical scalability.  \n3. **Mutual‚Äëselection mechanism:** Clarify the implementation details‚Äîspecifically, how Top‚ÄëP and Top‚ÄëQ are determined, whether gradients propagate through the selection step, and any convergence considerations.  \n4. **Reproducibility:** Supply comprehensive hyperparameter tables, random seeds, and state the public release plan for code, pretrained weights, and prompt lists.  \n5. **Clinical generalizability:** Expand discussion on how category‚Äëlevel prompts could align with standardized terminologies (e.g., SNOMED concepts) for practical use in clinical systems.  \n6. **Presentation and structure:** Condense redundant explanation in the methods and experimental sections to improve readability while retaining technical clarity.  \n\n---\n\n**Minor Comments**  \n- Harmonize mathematical notation and subscript conventions.  \n- Enhance clarity and labeling in Figures‚ÄØ1‚ÄØand‚ÄØ3; make captions self‚Äëcontained.  \n- Correct minor grammar and capitalization inconsistencies (‚Äúfine‚Äëgrained a‚ÄØlignment‚Äù ‚Üí ‚Äúalignment‚Äù).  \n- Check and remove duplicated references (e.g., Li‚ÄØet‚ÄØal.,‚ÄØ2022‚ÄØb/c).  \n- Provide dataset resolutions and units in Appendix‚ÄØC.  \n- Confirm dataset licensing and IRB compliance statements.\n\n---\n\n**Summary Paragraph**  \nOverall, the paper demonstrates strong methodological novelty through structural prompt encoding and mutual‚Äëselection fusion, supported by extensive evaluation across diverse imaging modalities. The zero‚Äëshot‚Äëenhancement setting is particularly relevant for continuous model improvement in clinical contexts. However, the absence of statistical validation, computational cost analysis, and complete reproducibility information limits the current rigor. The manuscript‚Äôs readability and structural flow could also be streamlined. With these revisions, the work has potential to make a notable contribution to zero‚Äëshot and prompt‚Äëdriven medical image detection research.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \n(Substantial methodological strengths but requiring additional quantitative analysis, reproducibility assurance, and presentation improvements before acceptance.)",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper proposes StructuralGLIP, a zero-shot medical detection framework that addresses coarse image-description alignment in existing grounded vision-language models. The method introduces a dual-branch architecture where prompts are encoded into a latent knowledge bank via an auxiliary branch, while the main branch processes target names and images. A mutual selection mechanism dynamically retrieves relevant visual and linguistic tokens at each layer to form structural representations. The approach supports both instance-level and category-level prompts, with category-level prompts remaining fixed across instances of the same class. Experiments across eight medical datasets (endoscopy, microscopy, photography, radiology) demonstrate superior performance in both zero-shot detection and zero-shot enhancement settings, achieving average improvements of +4.2% AP in zero-shot detection and enabling fine-tuned models to surpass fully supervised baselines.\n\n## Weaknesses\n\n**‚Ä¢ Mathematical Formulation Clarity and Consistency Issues**\n  - The mutual selection mechanism equations (6-7) use unclear notation where similarity calculations are embedded within Top-P/Top-Q operations without explicit definition of the similarity function (Section 3.2, Page 5). This obscures the computational process and reproducibility.\n  - The residual connections in Equation (9) lack mathematical justification for why additive combination is optimal compared to other fusion strategies, affecting theoretical soundness.\n  - Inconsistent notation usage where Ki_v and Ki_l represent both selected tokens and their associated values throughout Section 3.2, creating potential confusion in understanding the selection process.\n\n**‚Ä¢ Insufficient Experimental Validation and Baseline Comparisons**\n  - Limited comparison with recent vision-language medical detection methods beyond GLIP-based approaches, with no comparison to domain-specific medical detection frameworks (Tables 1, 4). This restricts assessment of the method's competitive positioning.\n  - Ablation studies focus primarily on hyperparameters P and Q (Table 5) but lack comprehensive analysis of architectural choices such as the dual-branch design versus alternative fusion strategies, limiting understanding of design necessity.\n  - The zero-shot enhancement setting evaluation lacks comparison with other prompt-based enhancement methods on fine-tuned models, making it difficult to assess the uniqueness of the approach (Section 4.3).\n\n**‚Ä¢ Methodological Limitations and Technical Concerns**\n  - The category-level prompt generation relies heavily on GPT-4 quality without systematic evaluation of prompt robustness across different medical domains (Section 3.3, Figure 3b). This creates potential generalization concerns for rare diseases or specialized imaging modalities.\n  - The mutual selection mechanism's computational overhead during inference is not quantified, particularly concerning the Top-P and Top-Q operations across multiple layers (Equations 6-7), which could impact clinical deployment feasibility.\n  - Limited analysis of failure cases or method limitations, with most qualitative results showing successful detections (Figures A, B, E) rather than systematic error analysis, reducing understanding of method boundaries.\n\n## Suggestions for Improvement\n\n**‚Ä¢ Enhance Mathematical Rigor and Notation Consistency**\n  - Explicitly define the similarity function used in Equations (6-7) with mathematical formulation (e.g., cosine similarity, dot product) and provide theoretical justification for the choice. Include algorithmic pseudocode for the Top-P/Top-Q selection process to improve reproducibility.\n  - Provide theoretical analysis or empirical validation for the additive residual connection design in Equation (9), comparing against alternative fusion strategies (concatenation, learned weighting) to justify the architectural choice.\n  - Establish consistent notation throughout Section 3.2, clearly distinguishing between token indices, selected tokens, and their representations, potentially using subscripts or different symbols to avoid confusion.\n\n**‚Ä¢ Expand Experimental Evaluation and Analysis**\n  - Include comparisons with recent domain-specific medical detection methods and non-GLIP-based vision-language approaches to establish comprehensive competitive analysis. Add comparison with other prompt-based enhancement methods on fine-tuned models.\n  - Conduct systematic ablation studies on architectural components including dual-branch versus single-branch designs, alternative fusion mechanisms, and different knowledge bank organizations to validate design necessity.\n  - Provide detailed failure case analysis, computational cost analysis including inference time and memory overhead, and systematic evaluation of prompt robustness across different medical imaging modalities and rare diseases.\n\n**‚Ä¢ Address Methodological Limitations and Practical Concerns**\n  - Develop systematic evaluation framework for prompt quality across different medical domains, including analysis of GPT-4 limitations for rare diseases and alternative prompt generation strategies for specialized medical contexts.\n  - Quantify computational overhead of the mutual selection mechanism and provide efficiency analysis comparing inference costs with baseline methods, including optimization strategies for clinical deployment.\n  - Include comprehensive error analysis with failure case studies, method limitation discussion, and systematic evaluation of robustness to noisy or incomplete prompts, providing clearer understanding of method applicability boundaries.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *StructuralGLIP*, a zero-shot medical detection framework designed to address coarse alignment issues between images and textual descriptions in existing grounded vision-language models. The proposed dual-branch architecture includes an auxiliary branch that encodes prompts into a latent knowledge bank and a main branch that processes target names and images. A mutual selection mechanism integrates visual and linguistic tokens dynamically across layers, enabling instance- and category-level prompt handling. Experiments on eight medical datasets demonstrate improved zero-shot detection and enhancement performance. While the paper presents a promising concept with demonstrated gains, aspects of the mathematical formulation, experimental validation, and methodological rigor warrant clarification and expansion.  \n\n**Major Comments**  \n1. **Clarity and Mathematical Consistency**  \n   - Equations (6‚Äì7) lack explicit definition of the similarity function embedded within the Top‚ÄëP/Top‚ÄëQ operations, reducing transparency and reproducibility.  \n   - The additive residual connection in Equation (9) is insufficiently justified; alternative fusion strategies are neither explored nor discussed.  \n   - Notation inconsistencies in Section 3.2 blur distinctions between selected tokens and their associated values, complicating interpretation of the selection process.  \n\n2. **Experimental Validation and Baseline Comparisons**  \n   - Comparisons are primarily limited to GLIP-based methods, omitting domain-specific medical detection baselines.  \n   - Ablation studies focus only on P and Q hyperparameters without examining architectural choices such as the dual- versus single-branch structure.  \n   - The zero-shot enhancement evaluation omits comparison to other prompt-based fine-tuning or enhancement methods, hindering assessment of novelty.  \n\n3. **Methodological and Practical Limitations**  \n   - Reliance on GPT-4 for category-level prompt generation lacks systematic evaluation across medical domains, raising concerns about prompt robustness and generalization.  \n   - Computational overhead of the mutual selection mechanism is not reported, limiting understanding of deployment feasibility.  \n   - The analysis does not include failure cases or systematic error discussion, reducing insight into model limitations.  \n\n**Minor Comments**  \n- Clarify mathematical notation and ensure consistent use of subscripts and variable symbols.  \n- Include pseudocode or detailed algorithmic steps for the selection mechanism to facilitate reproducibility.  \n- Provide precise figure and table references when discussing qualitative or quantitative results.  \n\n**Summary Paragraph**  \nOverall, the work presents a potentially significant contribution to zero-shot medical detection through structural prompt alignment and a novel dual-branch design. However, insufficient clarity in mathematical formulation, limited baseline coverage, and incomplete methodological analysis currently restrict the paper‚Äôs rigor and interpretability. Addressing these issues‚Äîparticularly through expanded theoretical justification, comprehensive comparisons, and clarity enhancements‚Äîwould substantially strengthen the contribution and reproducibility of the study.  \n\n**Decision Recommendation**  \n**Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces StructuralGLIP, a novel framework for zero-shot medical detection that leverages a dual-branch architecture with a latent knowledge bank to enhance the alignment between target descriptions and medical images. The proposed method encodes prompts into a hierarchical knowledge bank and introduces a mutual selection mechanism to dynamically select the most relevant prompt features for each input image. The authors demonstrate that StructuralGLIP outperforms existing methods across various medical imaging datasets, including endoscopy, microscopy, photography, and radiology. The manuscript is well-written and provides a thorough analysis of the proposed method's performance and robustness.\n\n## Major Comments\n1. Novelty and Positioning: The novelty of StructuralGLIP lies in its dual-branch architecture and the mutual selection mechanism for dynamic prompt selection. However, the manuscript should more explicitly differentiate StructuralGLIP from related works that also integrate category-level prompts and vision-language models. A clearer discussion on how StructuralGLIP's approach to prompt selection and alignment stands out from previous methods would strengthen the novelty argument.\n\n2. Evaluation Design: The experiments are conducted across multiple medical imaging datasets, which is commendable. However, the evaluation could benefit from a broader range of clinical scenarios and imaging conditions. Including prospective studies or real-world clinical data would provide a more comprehensive assessment of StructuralGLIP's practical utility. Additionally, the authors should consider conducting an ablation study to isolate the contributions of each component of the dual-branch architecture.\n\n3. Comparisons: The manuscript compares StructuralGLIP with several state-of-the-art methods, but the inclusion of more recent and competitive baselines would be beneficial. Specifically, including methods that employ advanced prompt engineering techniques or those that have been shown to perform well on similar tasks would provide a more rigorous benchmark. This would help establish the relative improvement brought by StructuralGLIP.\n\n4. Reproducibility: While the authors state that the code will be made available, the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameter settings. Providing a clear and complete description of these aspects is essential for reproducibility. Additionally, the authors should ensure that the code release includes all necessary components for others to reproduce the reported results.\n\n## Minor Comments\n1. Figures: Some figures (e.g., Figure 3) are cluttered and could be improved by showing fewer representative examples with zoomed-in regions to enhance readability.\n   \n2. Notation and Terminology: The notation and terminology used in Section 2.1 should be explained more thoroughly, especially regarding the forward operator and other symbols. Consistent notation throughout the manuscript is also recommended.\n   \n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition, which can be confusing for readers who are not familiar with the domain.\n   \n4. Typographical Issues: Minor typographical errors, such as \"k-spacce\" and \"undersampling maskes,\" should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: improving medical detection without requiring extensive annotated data. The proposed StructuralGLIP framework introduces innovative techniques for fine-grained alignment between image features and target descriptions, particularly through its dual-branch architecture and mutual selection mechanism. The work demonstrates strong empirical results across various medical imaging datasets, indicating its potential clinical value. However, the evaluation could be strengthened by incorporating a wider range of clinical scenarios and more competitive baselines. The reproducibility of the approach is somewhat compromised by the lack of detailed methodological descriptions. Overall, the manuscript offers promising contributions to the field of zero-shot medical detection, but it requires some additional work to fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor Revision. The authors should expand their comparative analysis, strengthen validation across a broader range of clinical scenarios, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *StructuralGLIP*, a framework for zero-shot medical detection that integrates a dual-branch architecture with a latent knowledge bank to enhance alignment between medical images and textual descriptions. The approach encodes prompts hierarchically and employs a mutual selection mechanism to dynamically choose the most relevant prompt features for each image. The study demonstrates that StructuralGLIP achieves superior performance across multiple medical imaging modalities, including endoscopy, microscopy, photography, and radiology. Overall, the manuscript is clearly written, and the proposed method is analyzed comprehensively in terms of performance and robustness.  \n\n**Major Comments**  \n1. **Novelty and Positioning:** The originality of StructuralGLIP lies in its dual-branch design and mutual selection mechanism. However, the paper should more clearly articulate how this framework differs from existing vision-language and prompt-based models. A more explicit discussion on the distinctiveness of StructuralGLIP‚Äôs prompt selection and alignment strategy would strengthen its novelty claim.  \n2. **Evaluation Design:** The evaluation spans multiple datasets, which is commendable, but the scope could be expanded to include more diverse clinical settings or real-world testing scenarios. Prospective or clinical studies would better demonstrate practical applicability. An additional ablation study is recommended to clarify the contributions of each architectural component.  \n3. **Comparative Analysis:** While comparisons to several state-of-the-art methods are included, newer and more competitive baselines‚Äîparticularly those featuring recent prompt-engineering or few-shot learning advances‚Äîshould be incorporated to establish the degree of improvement more convincingly.  \n4. **Reproducibility:** Although the authors indicate that code will be released, the methodological details are insufficient. Key aspects such as training procedures, preprocessing steps, and hyperparameters should be described thoroughly. Ensuring complete and functional code availability is critical for reproducibility.  \n\n**Minor Comments**  \n1. Some figures (notably Figure‚ÄØ3) appear crowded; simplifying visuals or providing zoomed-in regions could improve clarity.  \n2. Section‚ÄØ2.1 should define its notation more clearly, including operators and symbols, and ensure consistent usage throughout.  \n3. Certain acronyms (e.g., ‚ÄúR=4‚Äù) are undefined and should be clarified.  \n4. Minor typographical errors (e.g., ‚Äúk-spacce,‚Äù ‚Äúmaskes‚Äù) should be corrected.  \n\n**Summary Paragraph**  \nThis work tackles an important challenge in zero-shot medical detection, proposing a framework that enhances image-text alignment through innovative architectural and selection mechanisms. The results indicate strong performance and potential clinical relevance. However, the study‚Äôs novelty argument, breadth of evaluation, comparative rigor, and methodological transparency need improvement to meet publication standards. Clarifying these aspects would substantially strengthen the paper‚Äôs impact and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The authors should refine the novelty discussion, expand comparative and clinical evaluations, and provide complete methodological details to support reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## PROMPT AS KNOWLEDGE BANK: BOOST VISION-LANGUAGE MODEL VIA STRUCTURAL REPRESENTATION FOR ZERO-SHOT MEDICAL DETECTION\n\n### Summary\n\nThe paper proposes StructuralGLIP, a zero-shot medical detection framework that augments GLIP with a dual-branch architecture: a main branch for target names and images, and an auxiliary branch that encodes prompts into a latent ‚Äúknowledge bank.‚Äù A mutual selection mechanism retrieves Top-P visual tokens and Top-Q linguistic tokens per layer to build structural representations, which are fused into the main branch via cross-modal attention. The method aims to address coarse image‚Äìtext alignment and distribution shift from na√Øvely concatenated contextual prompts, and shows improvements across zero-shot detection and ‚Äúzero-shot enhancement‚Äù (post-finetuning) settings using both instance- and category-level prompts.\n\n### Strengths\n\n- Technical novelty and innovationThe dual-branch design that separates target-name encoding from prompt encoding and fuses via layer-wise mutual selection is a thoughtful architectural modification to GLIP for mitigating distribution shift.The per-layer Top-P/Top-Q selection to form structural representations is a principled way to retrieve relevant prompt knowledge, aligning with retrieval-augmented paradigms but tailored to detection.Supports category-level prompts effectively, addressing a meaningful practical need (fixed, reusable prompts per disease category).\n- Experimental rigor and validationBroad evaluation across eight datasets spanning four imaging conditions (endoscopy, microscopy, photography, radiology), including ablations on selection hyperparameters (P, Q), fusion layer choices, and prompt source (VQA vs. VQA+LLM).Baseline comparisons include GLIP, MIU-VL, AutoPrompter, and MPT variants; analyses demonstrate favorable performance and highlight the distribution shift issue in zero-shot enhancement where other methods degrade.Robustness analysis with noisy prompts (BCCD) is a useful stress test indicating the selection mechanism‚Äôs filtering behavior.\n- Clarity of presentationThe high-level motivation is clearly stated: contextual prompts can misalign and induce distribution shift; decoupling prompts into a knowledge bank with instance-wise selection can help.Figures effectively contrast contextual vs. structural prompting and illustrate the dual-branch pipeline and prompt generation.\n- Significance of contributionsZero-shot enhancement is a realistic, clinically aligned evaluation setting that is often overlooked; demonstrating gains over fine-tuned GLIP and parity/advantage vs. fully supervised detectors underscores practical value.Showing that category-level prompts can rival or surpass instance-level prompts‚Äîin a method designed to leverage them‚Äîcould materially improve deployment efficiency in clinical workflows.\n\n- The dual-branch design that separates target-name encoding from prompt encoding and fuses via layer-wise mutual selection is a thoughtful architectural modification to GLIP for mitigating distribution shift.\n- The per-layer Top-P/Top-Q selection to form structural representations is a principled way to retrieve relevant prompt knowledge, aligning with retrieval-augmented paradigms but tailored to detection.\n- Supports category-level prompts effectively, addressing a meaningful practical need (fixed, reusable prompts per disease category).\n\n- Broad evaluation across eight datasets spanning four imaging conditions (endoscopy, microscopy, photography, radiology), including ablations on selection hyperparameters (P, Q), fusion layer choices, and prompt source (VQA vs. VQA+LLM).\n- Baseline comparisons include GLIP, MIU-VL, AutoPrompter, and MPT variants; analyses demonstrate favorable performance and highlight the distribution shift issue in zero-shot enhancement where other methods degrade.\n- Robustness analysis with noisy prompts (BCCD) is a useful stress test indicating the selection mechanism‚Äôs filtering behavior.\n\n- The high-level motivation is clearly stated: contextual prompts can misalign and induce distribution shift; decoupling prompts into a knowledge bank with instance-wise selection can help.\n- Figures effectively contrast contextual vs. structural prompting and illustrate the dual-branch pipeline and prompt generation.\n\n- Zero-shot enhancement is a realistic, clinically aligned evaluation setting that is often overlooked; demonstrating gains over fine-tuned GLIP and parity/advantage vs. fully supervised detectors underscores practical value.\n- Showing that category-level prompts can rival or surpass instance-level prompts‚Äîin a method designed to leverage them‚Äîcould materially improve deployment efficiency in clinical workflows.\n\n### Weaknesses\n\n- Technical limitations or concernsThe mathematical formulation of the mutual selection is underspecified and occasionally inconsistent (e.g., o_j^i B_{q_j}^i, b_j^i K_v^i): shapes, normalization, and the exact similarity operators are unclear; selection over sets (K_v^i) vs. single tokens needs formalization.Computational and memory costs of layer-wise Top-P/Top-Q retrieval with long category prompts are not analyzed; per-layer retrieval may be expensive, and scalability is not quantified.It is not entirely clear how much the main-branch target representation is perturbed post-fusion in the fine-tuned setting, given residual connections still inject prompt-derived signals; a more rigorous argument or measurement of distribution shift mitigation is desirable.\n- Experimental gaps or methodological issuesSome tables contain formatting artifacts and missing/misaligned numbers, impeding precise verification; radiology results are partially omitted due to low baselines, which weakens generality claims across modalities.Limited comparisons to broader OVD literature beyond GLIP-based variants (e.g., retrieval-augmented OVD like RALF; caption/prompt-enrichment like PCL) reduce external validity.No statistical significance analysis, seed variance, or confidence intervals; robustness beyond single runs is unclear.The ‚Äúfairness‚Äù of prompt generation: for category-level prompts, the use of GPT-4 augmentation is strong but baselines may not equally exploit LLM-augmented prompt banks; this could inflate gains unless standardized.\n- Clarity or presentation issuesNotation and equations need tightening; some symbol reuse and subscript conventions are confusing (L1 vs L2 encoders ‚Äúwith shared parameters,‚Äù K_v^i vs ùí¶_v^i also appears).Important training details are deferred to appendices; the main text would benefit from a concise but explicit description of backbones, number of layers N, and the exact integration points in the GLIP stack.\n- Missing related work or comparisonsRelated retrieval-augmented or prompt-pool methods in continual learning are cited, but the contrast to instance‚Äëwise soft synthesis/modulation (e.g., Prompt Customization), retrieval-augmented detection (e.g., RALF), and caption-enriched OVD (e.g., PCL) could be sharpened to better isolate novelty.Surveys on medical CLIP/VLP are relevant; deeper discussion could elucidate how StructuralGLIP complements domain-adapted VLPs vs. plug-in enhancements.\n\n- The mathematical formulation of the mutual selection is underspecified and occasionally inconsistent (e.g., o_j^i B_{q_j}^i, b_j^i K_v^i): shapes, normalization, and the exact similarity operators are unclear; selection over sets (K_v^i) vs. single tokens needs formalization.\n- Computational and memory costs of layer-wise Top-P/Top-Q retrieval with long category prompts are not analyzed; per-layer retrieval may be expensive, and scalability is not quantified.\n- It is not entirely clear how much the main-branch target representation is perturbed post-fusion in the fine-tuned setting, given residual connections still inject prompt-derived signals; a more rigorous argument or measurement of distribution shift mitigation is desirable.\n\n- Some tables contain formatting artifacts and missing/misaligned numbers, impeding precise verification; radiology results are partially omitted due to low baselines, which weakens generality claims across modalities.\n- Limited comparisons to broader OVD literature beyond GLIP-based variants (e.g., retrieval-augmented OVD like RALF; caption/prompt-enrichment like PCL) reduce external validity.\n- No statistical significance analysis, seed variance, or confidence intervals; robustness beyond single runs is unclear.\n- The ‚Äúfairness‚Äù of prompt generation: for category-level prompts, the use of GPT-4 augmentation is strong but baselines may not equally exploit LLM-augmented prompt banks; this could inflate gains unless standardized.\n\n- Notation and equations need tightening; some symbol reuse and subscript conventions are confusing (L1 vs L2 encoders ‚Äúwith shared parameters,‚Äù K_v^i vs ùí¶_v^i also appears).\n- Important training details are deferred to appendices; the main text would benefit from a concise but explicit description of backbones, number of layers N, and the exact integration points in the GLIP stack.\n\n- Related retrieval-augmented or prompt-pool methods in continual learning are cited, but the contrast to instance‚Äëwise soft synthesis/modulation (e.g., Prompt Customization), retrieval-augmented detection (e.g., RALF), and caption-enriched OVD (e.g., PCL) could be sharpened to better isolate novelty.\n- Surveys on medical CLIP/VLP are relevant; deeper discussion could elucidate how StructuralGLIP complements domain-adapted VLPs vs. plug-in enhancements.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe architectural intuition is solid: decouple prompts from target names, build a hierarchical store, and inject relevant prompt knowledge with residual attention at deeper layers to preserve target distribution learned during finetuning.However, the selection mechanism‚Äôs math requires clearer exposition:Define the similarity function explicitly (cosine vs. dot product, temperature scaling).Clarify how ‚Äúo_j^i B_{q_j}^i‚Äù is computed; currently it suggests an index coupling across different token spaces (N_v vs N_l), which is ill-defined unless you specify pooling or cross-similarity matrices S = O^i (B^i)^T.If K_v^i is a set/matrix, explain how b_j^i K_v^i produces a scalar score (e.g., max/mean similarity across selected visual tokens), and how normalization or thresholding is applied before Top-P/Top-Q.Provide complexity analysis for constructing S and selecting Top-P/Q at each layer, especially with long category prompts; report actual P, Q, N_l, N_v used and the measured runtime/memory overhead.The residual fusion design is a reasonable way to limit distribution shift, yet it still updates T^i with T_{v2t}^{topP}; add a quantitative analysis (e.g., cosine distance between T^N with and without prompts across datasets) to substantiate the claim of reduced shift.\n- Experimental evaluation assessmentDataset breadth is commendable; the endoscopy-heavy suite matches the method‚Äôs focus. The extension to microscopy and ISIC broadens scope; TBX-11k inclusion in enhancement only is acceptable, but explicitly quantifying failure modes and why would strengthen the story.Ablations on P and Q, and fusion layers (single-layer vs. cumulative from layer k to N) are informative and support the hierarchical integration hypothesis.The noisy-prompt experiment on BCCD is a valuable diagnostic; consider extending to other datasets and to category-level noise to verify generality.Reporting could be crisper: avoid omitting TBX-11k in zero-shot tables even if absolute numbers are low; relative gains still matter. Add standard deviations over multiple runs, or at least mention seed control.\n- Comparison with related work (using the summaries provided)Compared to knowledge-pool prompt methods in continual learning (L2P, DualPrompt, CODA-P, Prompt Customization), StructuralGLIP‚Äôs per-layer mutual selection and application to detection with image‚Äìtext fusion is a legitimate and practical extension. Prompt Customization‚Äôs soft synthesis/modulation pipeline offers a complementary alternative to hard Top-P/Top-Q selection; a soft weighting ablation could be valuable.Retrieval-augmented detection (e.g., RALF) shows the efficacy of external stores and test-time retrieval in OVD. StructuralGLIP similarly leverages a prompt knowledge bank but more tightly integrates it into cross-attention; a discussion of differences (negative mining vs. token retrieval, training vs. test-time use) and a head-to-head comparison on a subset (if feasible) would help situate the contribution in the OVD landscape.PCL demonstrates instance-level caption enrichment to improve generalization; your category-level prompt bank plus selection sits between instance-specific captions and fixed class names. Articulating this position and why your selection helps bridge the two (especially in the medical setting) would strengthen the narrative.Medical VLP/CLIP surveys highlight persistent domain gaps and data scarcity; your zero-shot enhancement scenario aligns with these observations and provides a practical path to continuous improvement without new labels.\n- Discussion of broader impact and significanceThe category-level prompt capability reduces reliance on per-image VQA at inference, a clear efficiency and operational benefit in clinical pipelines.The reliance on LLMs (e.g., GPT-4) for category prompt expansion raises issues of provenance, potential hallucination, and domain gaps for rare conditions. Consider adding clinician-in-the-loop validation or automatic filtering (e.g., CLIP-Score thresholds) to ensure safety.The framework generalizes beyond medicine in principle (e.g., microscopy/photography experiments), but claims should be tempered until evaluated on non-medical OVD benchmarks.\n\n- The architectural intuition is solid: decouple prompts from target names, build a hierarchical store, and inject relevant prompt knowledge with residual attention at deeper layers to preserve target distribution learned during finetuning.\n- However, the selection mechanism‚Äôs math requires clearer exposition:Define the similarity function explicitly (cosine vs. dot product, temperature scaling).Clarify how ‚Äúo_j^i B_{q_j}^i‚Äù is computed; currently it suggests an index coupling across different token spaces (N_v vs N_l), which is ill-defined unless you specify pooling or cross-similarity matrices S = O^i (B^i)^T.If K_v^i is a set/matrix, explain how b_j^i K_v^i produces a scalar score (e.g., max/mean similarity across selected visual tokens), and how normalization or thresholding is applied before Top-P/Top-Q.Provide complexity analysis for constructing S and selecting Top-P/Q at each layer, especially with long category prompts; report actual P, Q, N_l, N_v used and the measured runtime/memory overhead.\n- The residual fusion design is a reasonable way to limit distribution shift, yet it still updates T^i with T_{v2t}^{topP}; add a quantitative analysis (e.g., cosine distance between T^N with and without prompts across datasets) to substantiate the claim of reduced shift.\n\n- Define the similarity function explicitly (cosine vs. dot product, temperature scaling).\n- Clarify how ‚Äúo_j^i B_{q_j}^i‚Äù is computed; currently it suggests an index coupling across different token spaces (N_v vs N_l), which is ill-defined unless you specify pooling or cross-similarity matrices S = O^i (B^i)^T.\n- If K_v^i is a set/matrix, explain how b_j^i K_v^i produces a scalar score (e.g., max/mean similarity across selected visual tokens), and how normalization or thresholding is applied before Top-P/Top-Q.\n- Provide complexity analysis for constructing S and selecting Top-P/Q at each layer, especially with long category prompts; report actual P, Q, N_l, N_v used and the measured runtime/memory overhead.\n\n- Dataset breadth is commendable; the endoscopy-heavy suite matches the method‚Äôs focus. The extension to microscopy and ISIC broadens scope; TBX-11k inclusion in enhancement only is acceptable, but explicitly quantifying failure modes and why would strengthen the story.\n- Ablations on P and Q, and fusion layers (single-layer vs. cumulative from layer k to N) are informative and support the hierarchical integration hypothesis.\n- The noisy-prompt experiment on BCCD is a valuable diagnostic; consider extending to other datasets and to category-level noise to verify generality.\n- Reporting could be crisper: avoid omitting TBX-11k in zero-shot tables even if absolute numbers are low; relative gains still matter. Add standard deviations over multiple runs, or at least mention seed control.\n\n- Compared to knowledge-pool prompt methods in continual learning (L2P, DualPrompt, CODA-P, Prompt Customization), StructuralGLIP‚Äôs per-layer mutual selection and application to detection with image‚Äìtext fusion is a legitimate and practical extension. Prompt Customization‚Äôs soft synthesis/modulation pipeline offers a complementary alternative to hard Top-P/Top-Q selection; a soft weighting ablation could be valuable.\n- Retrieval-augmented detection (e.g., RALF) shows the efficacy of external stores and test-time retrieval in OVD. StructuralGLIP similarly leverages a prompt knowledge bank but more tightly integrates it into cross-attention; a discussion of differences (negative mining vs. token retrieval, training vs. test-time use) and a head-to-head comparison on a subset (if feasible) would help situate the contribution in the OVD landscape.\n- PCL demonstrates instance-level caption enrichment to improve generalization; your category-level prompt bank plus selection sits between instance-specific captions and fixed class names. Articulating this position and why your selection helps bridge the two (especially in the medical setting) would strengthen the narrative.\n- Medical VLP/CLIP surveys highlight persistent domain gaps and data scarcity; your zero-shot enhancement scenario aligns with these observations and provides a practical path to continuous improvement without new labels.\n\n- The category-level prompt capability reduces reliance on per-image VQA at inference, a clear efficiency and operational benefit in clinical pipelines.\n- The reliance on LLMs (e.g., GPT-4) for category prompt expansion raises issues of provenance, potential hallucination, and domain gaps for rare conditions. Consider adding clinician-in-the-loop validation or automatic filtering (e.g., CLIP-Score thresholds) to ensure safety.\n- The framework generalizes beyond medicine in principle (e.g., microscopy/photography experiments), but claims should be tempered until evaluated on non-medical OVD benchmarks.\n\n### Questions for Authors\n\n- Can you formalize the mutual selection computations precisely? Please provide explicit equations for the similarity matrices, normalization, and how set‚Äìvector similarities (e.g., b_j^i with K_v^i) are reduced to scalar scores for Top-P/Top-Q.\n- What are the computational and memory overheads of layer-wise selection and fusion with long category prompts? Please report per-image latency and peak memory vs. GLIP, and how they scale with P, Q, N_l, and the number of fused layers.\n- In the zero-shot enhancement setting, how do you quantify distribution shift mitigation? Can you report cosine similarity (or KL divergence) of target representations T^N with/without prompts and relate that to performance changes vs. baselines that degrade?\n- How are prompts standardized across baselines for fair comparison, especially for category-level prompts enhanced by GPT-4? Did MIU-VL/AutoPrompter/MPT use exactly the same prompt strings and lengths?\n- Do you pre-encode category-level prompts once per dataset/class and reuse them across images? If so, how are they aligned to layer-wise retrieval (e.g., cached B^i per layer), and what is the practical speedup vs. instance-level prompts?\n- Could a soft, learned weighting over prompt tokens (akin to Prompt Customization) further improve over hard Top-P/Top-Q selection? Have you tried continuous selection/fusion as an ablation?\n- For TBX-11k zero-shot detection (non-enhancement), can you include results and qualitative analysis even if absolute performance is low? It would help understand modality-specific limitations.\n- What backbone and N (number of fusion layers) are used? Are encoders fully frozen in zero-shot detection, and which modules are trained (if any)? Please summarize core hyperparameters in the main text.\n- Have you conducted clinician validation of category-level prompts generated by GPT-4? If not, could you integrate automated filtering (e.g., CLIP-Score thresholds or ontology constraints) to reduce hallucinations?\n- How sensitive is performance to the VQA/LLM choices beyond the reported CLIP-Score correlations? Can you release the exact prompt templates and attribute vocabularies to ensure reproducibility?\n\n### Overall Assessment\n\nThis paper addresses an important and practical problem‚Äîimproving zero-shot medical detection, including post-finetuning ‚Äúzero-shot enhancement,‚Äù without additional labeled images. The proposed StructuralGLIP is a well-motivated extension of GLIP that decouples prompts into a knowledge bank and uses layer-wise mutual selection to retrieve and fuse relevant prompt tokens. Empirically, the method demonstrates consistent gains over strong GLIP-based baselines and, notably, makes effective use of category-level prompts‚Äîan operationally appealing setting. The work is technically sound at a high level and the evaluations are broad, with useful ablations and robustness checks. However, the current presentation has several gaps that limit verifiability and reproducibility: the mutual selection math is under-specified, computational overhead is not quantified, some tables are difficult to parse, and comparisons to broader OVD retrieval/caption-based methods could be strengthened. With clearer formalization, stronger methodological transparency, and a tighter placement among retrieval-augmented OVD and prompt-pool literature, this work has solid potential for a top-tier venue. I recommend acceptance after addressing the outlined technical clarifications, fairness controls, and reporting improvements.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *StructuralGLIP*, a framework for zero-shot medical object detection that augments GLIP through a dual-branch architecture‚Äîone for target-name and image encoding, and another that encodes prompts into a latent ‚Äúknowledge bank.‚Äù A mutual selection mechanism retrieves the most relevant visual and textual tokens per layer to construct structural representations, which are fused back into the main branch via cross-modal attention. This design aims to mitigate coarse image‚Äìtext alignment and distribution shift caused by contextual prompt concatenation. The paper reports improvements in both zero-shot detection and ‚Äúzero-shot enhancement‚Äù (fine-tuned) settings across multiple medical imaging modalities.\n\n**Major Comments**  \n1. **Technical specification:** The mathematical formulation of the mutual selection mechanism is underspecified. Equation notation, dimensional consistency (e.g., \\(o_j^i B_{q_j}^i\\), \\(b_j^i K_v^i\\)), normalization, and the definition of similarity functions (cosine vs. dot product, temperature scaling) should be clarified. The distinction between selection over token sets versus individual tokens requires formal definition.  \n2. **Efficiency and scalability:** Computational and memory costs of per-layer Top‚ÄëP/Top‚ÄëQ retrieval with long category prompts are unreported. A quantitative analysis of latency, memory overhead, and scaling with respect to P, Q, and model depth is necessary.  \n3. **Distribution shift mitigation:** It remains uncertain how much the fused branch alters target representations during fine-tuning. Empirical measurement of representation drift (e.g., cosine distance) would substantiate claims of reduced shift.  \n4. **Experimental completeness:** Several tables contain formatting issues or missing data, limiting verification. Radiology results are partly omitted, and comparison breadth is confined mainly to GLIP-derived baselines. Inclusion of retrieval-based and caption-enriched OVD methods (e.g., RALF, PCL) would strengthen evidence.  \n5. **Reproducibility and fairness:** Statistical significance, seed variance, and consistent prompt generation across baselines are missing. The use of GPT‚Äë4 for category-level prompts is advantageous but could produce unfair comparisons if baselines do not use similar augmentation.  \n6. **Presentation and related work:** Some notations are confusing and key training details (backbones, number of fusion layers) are deferred to appendices. Related work discussion could better distinguish StructuralGLIP from retrieval-augmented and continual learning prompt‚Äëpool methods.  \n\n**Minor Comments**  \n- Improve consistency of symbols (e.g., \\(K_v^i\\) vs. \\(ùí¶_v^i\\), L1/L2 encoders).  \n- Clarify whether category-level prompts are pre-encoded and reused across images to improve inference speed.  \n- Add missing results (e.g., TBX‚Äë11k) even if absolute numbers are low.  \n- Provide variation metrics over multiple runs.  \n- Discuss clinician validation or automatic filtering of GPT‚Äë4‚Äìgenerated prompts to ensure reliability.  \n\n**Summary Paragraph**  \nOverall, this work presents a technically interesting and well-motivated extension of GLIP addressing zero-shot medical detection. It achieves notable gains, particularly in category-level prompting, and demonstrates broad empirical coverage. The main limitations lie in incomplete mathematical formalization, lack of explicit efficiency evaluation, partial reporting, and limited external comparison. With clearer exposition, consistent experimental reporting, and stronger situating among related retrieval and prompt-based methods, the paper could make a substantial contribution to zero-shot detection in medical imaging.  \n\n**Decision Recommendation:** *Accept after minor revision.*",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"PROMPT AS KNOWLEDGE BANK: BOOST VISION-LANGUAGE MODEL VIA STRUCTURAL REPRESENTATION FOR ZERO-SHOT MEDICAL DETECTION\"\n\n## 1. Opening Paragraph\n\nThis manuscript addresses the critical challenge of zero-shot medical detection, which enables clinical models to detect new pathologies without requiring additional annotated medical images. The authors identify a key limitation in current approaches: contextual prompt methods that treat prompts as simple equivalents to disease names, resulting in coarse image-description alignment and distribution shift problems. To overcome this, they propose StructuralGLIP, a novel framework that encodes prompts into a latent knowledge bank using a dual-branch architecture with mutual selection mechanism. This approach dynamically matches vision features with relevant prompt features at each layer, enabling fine-grained structural representations that capture nuanced relationships between medical images and target descriptions. The authors demonstrate that StructuralGLIP significantly outperforms existing methods across eight diverse medical datasets spanning endoscopy, microscopy, photography, and radiology imaging modalities. Crucially, their method effectively leverages category-level prompts (which remain fixed across all instances of the same category) to achieve an average 30.4 AP across datasets, substantially outperforming both traditional zero-shot methods and even surpassing supervised models like RetinaNet in the zero-shot enhancement setting.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The paper presents a well-motivated and significant contribution to zero-shot medical detection by addressing a clear limitation in current contextual prompt methods.\n- The dual-branch architecture with mutual selection mechanism is a novel and conceptually sound approach that effectively resolves the distribution shift problem when incorporating prompts.\n- The demonstration that category-level prompts (which require no per-image generation) can outperform instance-specific prompts is particularly valuable for clinical deployment.\n- The comprehensive evaluation across eight diverse medical datasets with different imaging modalities provides strong evidence of the method's robustness and generalizability.\n- The zero-shot enhancement experiments are clinically relevant, showing how the approach can improve already fine-tuned models without requiring additional annotations.\n\n**Limitations:**\n- The paper lacks sufficient detail about the computational overhead of the dual-branch architecture compared to single-branch methods. While the authors mention efficiency benefits of category-level prompts, they don't quantify the actual inference time or resource requirements.\n- The analysis of failure cases is limited; understanding when and why the method underperforms would strengthen clinical applicability claims.\n- The paper doesn't sufficiently address potential clinical risks of deploying zero-shot detection systems, particularly regarding false positives/negatives for rare conditions that might not be well-represented in the prompt knowledge bank.\n\n### Minor Comments\n\n**Strengths:**\n- The visualizations (Figures 1-10) are well-designed and effectively illustrate the proposed method and its advantages.\n- The ablation studies on layer fusion and prompt selection parameters provide valuable insights for potential adopters.\n- The detailed prompt banks in Appendix G offer transparency and reproducibility.\n- The inclusion of natural image experiments (Figure F) helps validate the generalizability of the approach beyond medical imaging.\n\n**Limitations:**\n- The description of the mutual selection mechanism (Section 3.2) could be clarified with more concrete mathematical notation.\n- Some figures (e.g., Figure 9) would benefit from more detailed captions explaining the visualization elements.\n- The paper would benefit from a brief discussion of how to handle extremely rare conditions where LLM-generated prompts might be inaccurate due to limited training data.\n- The CLIP-Score analysis could be expanded to include more diverse medical scenarios beyond the two datasets shown.\n\n## 3. Summary Evaluation\n\n**Significance (High):** The paper addresses a clinically important problem: enabling medical detection systems to incorporate new pathology knowledge without requiring additional annotated data. This has direct relevance to real-world clinical settings where new disease variants emerge and annotated data is scarce. The demonstrated performance (outperforming supervised methods in some cases) represents a meaningful advance for medical AI.\n\n**Innovation (High):** The dual-branch architecture with mutual selection mechanism represents a genuine conceptual advance over existing contextual prompt methods. The key innovation‚Äîtreating prompts as a knowledge bank rather than simple context‚Äîprovides a new paradigm for vision-language alignment in medical imaging. The effective use of category-level prompts is particularly innovative, as previous methods struggled with this approach.\n\n**Evaluation (High):** The evaluation is comprehensive and rigorous, covering eight diverse medical datasets across four imaging modalities. The inclusion of both zero-shot detection and zero-shot enhancement settings provides a realistic assessment of clinical applicability. The ablation studies and analysis of prompt selection mechanisms add depth to the evaluation. However, the paper could strengthen its evaluation with more failure case analysis.\n\n**Reproducibility (High):** The method is described with sufficient detail for replication, including architecture specifications, hyperparameters, and training procedures. The authors promise to release code, which is crucial for reproducibility. The detailed prompt banks in Appendix G further enhance reproducibility. The only limitation is the lack of computational resource specifications.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThis is a strong paper that makes a significant contribution to zero-shot medical detection with a well-motivated, innovative approach and comprehensive evaluation. The results are compelling and demonstrate clear advantages over existing methods. However, before acceptance, I recommend addressing the following:\n\n1. Provide quantitative analysis of computational overhead (inference time, memory requirements) compared to baseline methods\n2. Expand the discussion of failure cases and clinical risks, particularly for rare conditions\n3. Clarify the mutual selection mechanism with additional mathematical notation where appropriate\n4. Include a brief discussion of how the approach might handle extremely rare conditions with limited LLM knowledge\n\nThese revisions would strengthen an already high-quality paper and ensure its clinical relevance is fully articulated. The authors have demonstrated strong technical work that aligns well with TMI's focus on impactful medical imaging research.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates zero-shot medical detection, aiming to enable clinical models to identify previously unseen pathologies without additional annotated data. It highlights a limitation in existing contextual prompt methods, which typically equate prompts with disease names and thus lead to coarse visual-text alignment and distribution shifts. To address this, the authors propose **StructuralGLIP**, a dual-branch framework that encodes prompts into a latent knowledge bank via a mutual selection mechanism, dynamically matching vision and prompt features across layers. The model achieves fine-grained structural representations and demonstrates consistent performance gains across eight diverse medical datasets, outperforming existing zero-shot and even certain supervised detection methods. The paper is clearly written, well-motivated, and supported by comprehensive experimental evidence.  \n\n**Major Comments**  \n1. **Significance and novelty:** The approach presents a meaningful conceptual advance for zero-shot medical detection by reconceptualizing prompts as structured knowledge rather than fixed text labels. The dual-branch design with mutual selection is novel and effectively mitigates prompt-related distribution shifts.  \n2. **Evaluation strength:** Extensive experiments on eight datasets across multiple imaging modalities convincingly show the method‚Äôs robustness and generalizability. The results on zero-shot enhancement are particularly relevant for clinical deployment.  \n3. **Computational analysis:** The manuscript does not quantify computational overhead or inference efficiency compared to single-branch baselines; providing these details would contextualize the claimed advantages.  \n4. **Failure analysis and clinical risks:** Limited exploration of failure cases and potential clinical risks‚Äîespecially regarding rare or unseen conditions‚Äîreduces confidence in real-world applicability.  \n5. **Methodological clarity:** The mutual selection mechanism (Section 3.2) would benefit from clearer mathematical description to improve transparency and reproducibility.  \n\n**Minor Comments**  \n- Figures are well-designed, but some (e.g., Figure 9) require more detailed captions.  \n- Include a short discussion on handling rare pathologies where large language model‚Äìderived prompts may be unreliable.  \n- The CLIP-Score analysis could be extended to more medical scenarios.  \n- The appendix with prompt banks and ablation studies is valuable for reproducibility and should remain emphasized.  \n\n**Summary Paragraph**  \nOverall, this paper offers a strong, innovative contribution to zero-shot medical detection through the StructuralGLIP framework. Its strengths lie in conceptual originality, methodological rigor, and broad empirical validation. Areas requiring improvement include quantitative efficiency analysis, additional exploration of failure modes, and clearer mathematical detail in the selection mechanism. Addressing these points would enhance the work‚Äôs technical completeness and clinical relevance.  \n\n**Decision Recommendation**  \n**Minor Revision** ‚Äì A high-quality and impactful paper requiring only clarifications and expanded discussion before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles zero‚Äëshot medical object detection, aiming to boost detection performance without the need for additional annotated medical images. Existing zero‚Äëshot methods typically rely on simple prompts that consist solely of disease names, which limits the granularity of alignment between visual cues and textual descriptions. To address this limitation, the authors propose **StructuralGLIP**, a dual‚Äëbranch architecture in which an auxiliary branch encodes detailed prompts into a hierarchical knowledge bank, while the primary branch processes the target name together with the image. At every encoder layer a mutual‚Äëselection mechanism extracts the most relevant visual tokens and prompt tokens, producing structural representations that are subsequently fused via cross‚Äëmodal attention. Experiments are reported on eight benchmarks covering endoscopy, microscopy, photography, and radiology, evaluating both pure zero‚Äëshot scenarios and zero‚Äëshot enhancement after fine‚Äëtuning. The results show consistent improvements over previously reported methods across these datasets. The central contribution is presented as a framework that treats prompts as a latent knowledge bank, enabling layer‚Äëwise, instance‚Äëspecific selection and finer vision‚Äëlanguage alignment for medical detection.\n\n---\n\n## General feedback  \n\n- **Significance:** While reducing annotation effort for rare diseases is an attractive goal, the reported performance gains (approximately‚ÄØ+4‚ÄØ%‚ÄØAP) are modest and the manuscript does not demonstrate any downstream clinical relevance.  \n- **Innovation:** The hierarchical knowledge bank and per‚Äëlayer mutual‚Äëselection are described as novel compared with prior prompt‚Äëconcatenation approaches (see Fig.‚ÄØ1). However, the top‚Äëk similarity selection at the heart of the method resembles established retrieval‚Äëbased techniques, and the paper offers no theoretical justification for its design.  \n- **Evaluation:**  \n  1. The absence of confidence intervals or standard deviations makes it impossible to gauge whether the observed improvements are statistically significant.  \n  2. Baseline comparisons are restricted to GLIP‚Äëderived methods (MIU‚ÄëVL, AutoPrompter, MPT); recent open‚Äëvocabulary detectors such as DetCLIP, OWL‚ÄëDET, and PromptDet are not included, limiting the scope of the assessment.  \n  3. The radiology dataset TBX‚Äë11k is omitted from key tables because the GLIP baseline performs poorly, raising doubts about the method‚Äôs generality.  \n  4. In the ‚Äúzero‚Äëshot enhancement‚Äù experiments (Table‚ÄØ4) the authors compare only against RetinaNet and Faster‚ÄØRCNN, neglecting more recent fully‚Äësupervised detectors (e.g., DyHead, Swin‚ÄëTransformer‚Äëbased models).  \n- **Reproducibility:**  \n  * The provided code link is a placeholder (‚ÄúXXX‚Äù) and the repository is unavailable.  \n  * Critical implementation details‚Äîincluding learning rates, optimizer settings for the auxiliary branch, values of the hyper‚Äëparameters P and Q, token dimension D, number of layers N, and GPU memory consumption‚Äîare relegated to an unavailable appendix.  \n  * Only a single hyper‚Äëparameter setting (P‚ÄØ=‚ÄØ10, Q‚ÄØ=‚ÄØ10) is reported (Table‚ÄØ4.4); systematic sweeps over these parameters are missing.\n\n---\n\n## Specific comments/critiques  \n\n- The mutual‚Äëselection mechanism (Equations‚ÄØ6‚Äë7) is presented without an intuitive explanation, pseudo‚Äëcode, or clarification of the similarity metric (cosine versus L2). It is also unclear whether the top‚Äëk selection is performed per image or globally. *(Section‚ÄØ3.2, Fig.‚ÄØ1)*  \n- No analysis of the runtime or memory overhead introduced by maintaining a knowledge bank across all transformer layers is provided; a computational‚Äëcomplexity discussion is absent.  \n- The omission of recent open‚Äëvocabulary detection baselines (DetCLIP, OWL‚ÄëDET, PromptDet) makes it difficult to discern whether the reported gains arise from the architectural changes or simply from richer textual inputs. *(Tables‚ÄØ1‚Äë4)*  \n- Performance tables list single AP values without error bars or statistical testing; the claimed +4.2‚ÄØ%‚ÄØAP improvement may be within experimental variance.  \n- Descriptions of the training/validation/test splits for the eight benchmarks are terse; it is not evident whether cross‚Äëvalidation or a single split was employed, which hampers reproducibility. *(Section‚ÄØ4.1)*  \n- Prompt generation details are insufficient: category‚Äëlevel prompts are said to be generated with GPT‚Äë4 and instance‚Äëlevel prompts with BLIP, yet the actual templates, token counts, and any verification of medical correctness are not disclosed.  \n- The ablation study is limited: Figure‚ÄØ4 investigates fusion at different layers, and Table‚ÄØ4.4 varies P/Q only on the CVC‚Äë300 dataset. Broader ablations across all datasets are needed to confirm the robustness of the design choices.  \n- No user study, radiologist evaluation, or analysis of diagnostic impact is presented, especially for the challenging TBX‚Äë11k radiology dataset.  \n- The manuscript contains numerous typographical and formatting errors (e.g., ‚ÄúBOOST VISION## LANGUAGE MODEL‚Äù, broken symbols) and an incomplete caption for Figure‚ÄØ1, which detracts from readability.  \n- The promised GitHub repository remains a placeholder and is currently inaccessible, precluding verification of the implementation.\n\n---\n\n## A suggested decision  \n\n**Reject**  \n\nThe manuscript falls short of the novelty and methodological rigor required for publication. The incremental performance gains are not substantiated with statistical evidence, the evaluation lacks comprehensive baselines, and critical reproducibility details are missing. Consequently, the work does not meet the standards for acceptance in its current form.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses zero‚Äëshot medical object detection, aiming to improve accuracy without requiring additional annotated images. It introduces **StructuralGLIP**, a dual‚Äëbranch architecture in which an auxiliary branch encodes detailed prompts into a hierarchical knowledge bank, while the primary branch processes target names with image features. A mutual‚Äëselection mechanism at each encoder layer extracts relevant visual and textual tokens, enabling finer cross‚Äëmodal alignment. Experiments across eight benchmarks (endoscopy, microscopy, photography, radiology) show consistent accuracy gains over prior zero‚Äëshot detection methods. While the objective is relevant and the presentation generally clear, the magnitude of improvements is modest and several methodological and reproducibility issues limit the contribution‚Äôs impact.  \n\n---\n\n**Major Comments**  \n1. **Significance and Novelty** ‚Äì Although reducing annotation costs for rare diseases is worthwhile, the reported performance gains (~4‚ÄØ%‚ÄØAP) are small and no analysis of potential clinical benefit is provided. The claimed innovations‚Äîthe hierarchical knowledge bank and mutual‚Äëselection process‚Äîappear similar to existing retrieval‚Äëbased methods, and the paper lacks theoretical justification for the design choices.  \n2. **Evaluation Design** ‚Äì Statistical rigor is insufficient: no confidence intervals or standard deviations are reported, leaving significance of improvements unclear. Baseline comparisons are limited to GLIP‚Äëderived models, excluding more recent open‚Äëvocabulary detectors (DetCLIP, OWL‚ÄëDET, PromptDet). The radiology dataset TBX‚Äë11k is omitted from central result tables, and the ‚Äúzero‚Äëshot enhancement‚Äù experiments use only dated detectors (RetinaNet, Faster‚ÄØRCNN), weakening claims of generality.  \n3. **Reproducibility** ‚Äì The code repository link is nonfunctional, and numerous implementation specifics (hyper‚Äëparameters, network dimensions, optimizer settings, GPU usage) are missing. Systematic parameter sweeps are absent, and the manuscript references an unavailable appendix containing such details.  \n\n---\n\n**Minor Comments**  \n- The mutual‚Äëselection equations (6‚Äì7) lack intuitive explanation, pseudo‚Äëcode, and clarification on similarity metrics or selection granularity.  \n- No analysis is provided of computational or memory overhead from maintaining the knowledge bank.  \n- Training/testing splits for datasets are insufficiently described, and the use of cross‚Äëvalidation is unclear.  \n- Prompt construction with GPT‚Äë4 and BLIP is inadequately documented; exact templates and medical validity checks are missing.  \n- Ablation studies are narrowly focused (one dataset, limited parameters).  \n- Typographical and formatting errors persist, including incomplete figure captions, incorrect symbols, and placeholder repository links.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper presents an interesting adaptation of vision‚Äëlanguage alignment to zero‚Äëshot medical detection, but its methodological novelty and empirical validation are limited. The restricted baseline coverage, lack of statistical analysis, incomplete documentation, and missing code undermine reproducibility and confidence in the findings. Despite promising motivation, the experimental evidence and technical justification do not yet substantiate the claimed contributions.  \n\n---\n\n**Decision Recommendation**  \n**Reject.** The work requires major revisions to address evaluation completeness, reproducibility, and clarity before it could be reconsidered.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Baochang Zhang",
      "Chunyu Xie",
      "Haoyu Huang",
      "Linlin Yang",
      "Tongfei Chen",
      "Xianbin Cao",
      "Yuguang Yang",
      "Dawei Leng"
    ],
    "url": "pdfs/iclr.cc-2025-conference_42ae230c934ff880d86ebd78533b2cdcd90fd1b8.pdf",
    "remote_url": "https://openreview.net/pdf/42ae230c934ff880d86ebd78533b2cdcd90fd1b8.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
    "status": "completed",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Multimodal Learning",
      "Medical Imaging"
    ],
    "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical imaging problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, medical imaging domains introduce two key challenges: dynamic modality fusion and modality-task dependence. The quality and amount of task-related information from different modalities could vary significantly across patient samples, due to biological and demographic factors. Traditional fusion methods apply fixed combination strategies that fail to capture this dynamic relationship, potentially underutilizing modalities that carry stronger diagnostic signals for specific patients. Additionally, different clinical tasks may require dynamic feature selection and combination from various modalities, a phenomenon we term ‚Äúmodality-task dependence.‚Äù To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions. We evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate our method's superiority over state-of-the-art methods under different metrics of classification and segmentation tasks like Accuracy, AUROC, AUPRC, and DICE.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a multi-modal, multi-task, mixture of experts for various medical diagnoses to address the challenges of sample-dynamic modality fusion (and modality-task dependence (selecting the right modalities for a task). Concretely, this is done by using a combination of modality-specific experts and experts shared between modalities and tasks. M4OE shows promising initial results in terms of both absolute performance and enforcing modality utilization.\n\nBased on the weaknesses and questions outlined my score indicates a rejection for now, but I generally like the motivation of the paper, especially the aspect on modality utilization. I am willing to increase my score if my concerns are addressed and questions clarified.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The M4OE is highly effective at enforcing modality utilization - this is a meaningful contribution that many multimodal models suffer from, although I do have some questions about this.\n- The overall performance of the model is outperforming the baseline, even if the results are missing crucial information to validate the statistical significance of the results.\n- Strong visuals that are additive to the understanding of the paper.\n- Good conceptual motivation of the paper, although I believe that the motivation would further benefit from some concrete examples of sample dynamism and clinical examples of tasks that are modality-dependent.\n\n### Weaknesses\n\n- Abstract: the one-liner for sample-dynamic modality fusion is unclear as the specific and shared information always varies per sample unless they are identical. To my knowledge, sample-dynamic spans a much wider field of problems like missingness, robustness to noise, which the manuscript does not consider.\n- Abstract: ‚ÄúResults demonstrate superiority over state-of-the-art methods‚Äù is extremely vague. Along which metric?\n- You claim an expansive space by saying that the method is ‚Äúmulti-modal multi-task‚Äù, but your experiments only look at multi-view settings of a single modality (images). I would encourage you to narrow the scope/claim of the paper as the paper does not consider heterogeneous modalities (images, text, tabular, etc.).\n- Experimental setup: I would encourage you to provide more detail in this section to aid reproducibility. For example, it is unclear whether cross-validation is used. No confidence intervals or standard deviation of results are reported to judge the statistical significance of the results. Additionally, no code was provided in the supplementary materials that would help with the clarification of the experimental setup.\n- Literature: missing out on the largest corpus of literature (intermediate fusion), which many latent variable models for multimodal fusion fall under, many of which are using a mix of modality-specific and shared spaces.\n\n### Questions\n\n- How do you determine which expert sees which task? The connection between Figure 1 and the method section is not very clear.\n- The manuscript talks a lot about sample adaptivity, but how does your experimental setup show that the model handles sample adaptivity effectively? Which aspects of sample adaptivity?\n- Figure 3c suggests that the modality utilization is forced towards the same mean in your method. What about cases where modality dominance/competition is good? For example, if I have one very noisy modality, wouldn‚Äôt it be desirable to have the modality that contains all the signal to get all the model‚Äôs attention? Isn‚Äôt this graph showing that we enforce equal utilisation of all modalities regardless of the signal? Additionally, does this finding not contradict your claim in Figure 1, which is that only some experts are used (as opposed to all experts with a more balanced contribution).\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **M4OE**, a multi-modal, multi-task mixture-of-experts model designed for medical diagnosis. Its stated aim is to address challenges in **sample-dynamic modality fusion** and **modality-task dependence**, ensuring appropriate modality selection per task. The approach combines modality-specific and shared experts across tasks, reporting improved overall performance and enhanced modality utilization. The manuscript is generally clear and visually well presented, though several conceptual and methodological details need clarification before the contribution can be fully assessed.\n\n**Major Comments**  \n1. **Scope and Claims**: The paper claims broad generality as a ‚Äúmulti-modal, multi-task‚Äù framework, yet experiments focus on multi-view settings within a single modality (images). This discrepancy suggests the scope should be narrowed or claims revised.  \n2. **Definition of Sample-Dynamic Fusion**: The abstract‚Äôs description of ‚Äúsample-dynamic modality fusion‚Äù is unclear. The concept as stated does not adequately account for the full range of issues‚Äîsuch as missingness or robustness to noise‚Äîtypically associated with sample-dynamic problems.  \n3. **Evaluation and Statistical Reporting**: Experimental details are insufficient for reproducibility. It is unclear whether cross-validation was used, and no confidence intervals or standard deviations are reported, hindering any assessment of statistical significance. The supplementary materials lack code or documentation that might clarify the setup.  \n4. **Literature Context**: The manuscript omits important related work on intermediate fusion models, particularly those using mixtures of modality-specific and shared latent spaces.  \n5. **Empirical Support for Sample Adaptivity**: While sample adaptivity is emphasized conceptually, the experiments do not clearly demonstrate how the model achieves or tests this property.  \n6. **Interpretation of Modality Utilization (Figure 3c)**: The results appear to enforce uniform utilization across modalities, raising concerns about scenarios where uneven weighting is beneficial (e.g., noisy modalities). This could contradict the earlier claim that only a subset of experts is selectively used.  \n7. **Methodological Clarity**: The process for assigning experts to tasks is not well explained, particularly the linkage between Figure‚ÄØ1 and the method description.\n\n**Minor Comments**  \n- The abstract overstates results with vague claims such as ‚Äúsuperiority over state-of-the-art methods‚Äù without defining specific metrics.  \n- The terminology and notation around modality-task dependence could benefit from additional clarification.  \n- The paper‚Äôs motivation is strong, but inclusion of concrete examples‚Äîe.g., particular diagnostic tasks or modalities‚Äîwould improve accessibility.  \n- Figures are overall clear and helpful.\n\n**Summary Paragraph**  \nThis manuscript presents a promising idea for enforcing modality utilization in multimodal learning. Strengths include solid motivation, appealing visualizations, and indications of performance gains. However, claims of generality, lack of methodological detail, missing statistical analysis, and incomplete engagement with relevant literature limit the manuscript‚Äôs current soundness and reproducibility. These aspects need significant clarification and revision for the work to reach its potential contribution level.\n\n**Decision Recommendation**: **Major Revision / Reject (resubmit after substantial revision)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this paper, the author present a new framework for training multi-modal networks, called the Multi-modal Multi-task Mixture of Experts. The framework consists of two components:\n- MSoE: Modality specific mixture of experts --> for each modality, they learn a function g that applies:\n column-wise softmax (D) on X times a learnable matrix, multiplied by X, followed by row-wise softmax (C) on the output and a linear combination to compute the prediction.\n- MToE: Modality shared modality task mixture of experts --> connects tasks to input modalities by learning a task embedding shared across experts.\n\nThey also propose a mutual information loss and evaluate the approach on four publicly available medical imaging datasets for breast cancer and OCT.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\nThe framework presented is original and interesting. It outperforms existing baseline models. The authors run experiments on multiple datasets and conduct an ablation study.\n\n### Weaknesses\n\n- The paper presentation requires improvement. For example, there is unnecessary use of ; and there is incorrect use of opening quotations \". The authors also repeatedly introduce the abbreviations - this should be done once.\n- I found it difficult to parse through Figure 2 (Can you relate it with the textual explanation of the functions?)\n- The authors only compare to a few baselines, can you incorporate more? There is a lot of literature on multimodal learning now.\n- Are the performance improvements significant? Can you conduct significance testing and provide confidence intervals?\n- The experiments are conducted on medical imaging datasets. How does this apply to other non-imaging modalities where modality competition may be more pronounced. For example, this could be applicable to MIMIC CXR (chest X-rays) and MIMIC EHR where downstream tasks are more dependent on the EHR modality.\n- The main results section in the text should also discuss the quantitative results.\n- What was your hyperparameter tuning strategy? It is unclear if these baselines have been best optimized.\n- Can you also compute AUROC and AUPRC for the classification tasks? Accuracy is not sufficient.\n\n### Questions\n\n- Can the authors discuss the scalability of the framework? What is the computational complexity?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a new framework for training multi-modal networks, termed the *Multi-modal Multi-task Mixture of Experts (M3oE)*. The method combines two components: a modality-specific mixture of experts (MSoE) that learns per-modality expert functions using column- and row-wise softmax operations, and a modality-task mixture of experts (MToE) that links tasks and input modalities via shared task embeddings. A mutual information loss is further proposed. The approach is evaluated on four public medical imaging datasets related to breast cancer and optical coherence tomography (OCT). Overall, the framework is novel and shows promising performance improvements over baselines, though the presentation and experimental depth require refinement.\n\n**Major Comments**  \n1. **Presentation and Clarity:** The manuscript‚Äôs presentation needs improvement. There is inconsistent punctuation (e.g., incorrect use of semicolons and quotation marks) and repeated introduction of abbreviations. Figure 2 is difficult to follow; clearer correspondence between the figure and the textual description of the functions is needed.  \n2. **Comparative Evaluation:** The comparison set is limited to a few baselines, despite extensive recent work in multimodal learning. Including additional relevant methods would strengthen claims of superiority.  \n3. **Statistical Significance:** The reported performance differences should be supported by statistical testing and confidence intervals to establish their significance.  \n4. **Generality Beyond Imaging:** The study focuses solely on medical imaging data. Discussion of applicability to non-imaging modalities (e.g., multimodal datasets combining imaging and EHR such as MIMIC-CXR and MIMIC-EHR) would broaden the paper‚Äôs relevance.  \n5. **Experimental Reporting:** The main results section should more thoroughly discuss quantitative outcomes. Hyperparameter tuning strategies for baselines are not clearly described and could affect fairness.  \n6. **Evaluation Metrics:** For the classification tasks, reporting AUROC and AUPRC in addition to accuracy would provide a more robust assessment.  \n7. **Scalability:** Clarification is needed regarding computational complexity and scalability of the proposed model.\n\n**Minor Comments**  \n- Streamline the use of abbreviations by defining each only once.  \n- Ensure consistent and correct punctuation.  \n- Improve figure captions for readability and cross-reference with the description in text.\n\n**Summary Paragraph**  \nThis paper presents an innovative and potentially impactful framework for multimodal, multitask learning with strong empirical performance. However, the limited comparative analysis, insufficient discussion of statistical robustness and generalizability, and issues with clarity reduce the overall strength of the work. Addressing these concerns would significantly improve both credibility and readability.\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper mainly addresses two challenges in clinical tasks: patient-level and task-level dynamic fusion. For the patient-level fusion, a modality-specific MoE is employed. For the task-level fusion, a modality-task MoE with conditional MI regularization between experts and modalities given tasks is adopted. The experiments using EMBED, RSNA, VinDR, and GAMMA datasets outperform existing methods in both single-task and multi-task settings.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\n- The paper is well-structured and easy to follow. \n- The motivation is clearly stated and convincing. \n- The experiments show promising results over many baselines both in stand-alone and add-on manners.  \n- The paper adopted PID to make a fair comparison of synergy information.\n\n### Weaknesses\n\n- As far as I understand, there have been works leveraging the shared and specific information across modalities and should be included in discussions, see [1-3]. \n- Is there an ablation study for a reduced number of experts? How sensitive is this method when the number of experts decreases compared to other MoE methods? What is the procedure for choosing the number of experts? \n- Please discuss the computational cost compared to the baselines. \n\n\n[1]Wang, Hu, et al. \"Multi-modal learning with missing modality via shared-specific feature modelling.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2]Yao, Wenfang, et al. \"DrFuse: Learning Disentangled Representation for Clinical Multi-Modal Fusion with Missing Modality and Modal Inconsistency.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.\n\n[3]Chen, Cheng, et al. \"Robust multimodal brain tumor segmentation via feature disentanglement and gated fusion.\" Medical Image Computing and Computer Assisted Intervention‚ÄìMICCAI 2019: 22nd International Conference, Shenzhen, China, October 13‚Äì17, 2019, Proceedings, Part III 22. Springer International Publishing, 2019.\n\n### Questions\n\n- Are there any ablation studies on datasets other than EMBED?\n- Are there any theoretical explanations on why the method mitigates gradient conflict?\n- Is there any clinical interpretation of the results in Figure 5? For example, the difference in modality contribution across different tasks.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 3,
          "specificity": 2,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a multimodal fusion framework addressing two levels of challenges in clinical tasks: patient-level and task-level dynamic fusion. At the patient level, a modality-specific mixture of experts (MoE) model is applied, while at the task level, a modality‚Äìtask MoE with conditional mutual information regularization is introduced. The method is evaluated on the EMBED, RSNA, VinDR, and GAMMA datasets, showing superior performance to existing approaches in both single-task and multi-task scenarios. Overall, the paper is clearly structured, well-motivated, and presents solid empirical results.\n\n**Major Comments**  \n1. **Related Work Coverage** ‚Äì The discussion of prior multimodal work should be expanded to include studies that explore shared and modality-specific information across inputs, such as Wang et al. (2023), Yao et al. (2024), and Chen et al. (2019).  \n2. **Ablation Studies on Expert Numbers** ‚Äì Further clarification is needed on the ablation design: is there analysis on how model performance changes with fewer experts? How is the number of experts selected and how does this sensitivity compare with other MoE methods?  \n3. **Computational Efficiency** ‚Äì The paper should include a discussion or quantitative comparison of computational cost relative to baselines, as this is important for assessing practical applicability.  \n4. **Generalization and Theoretical Insight** ‚Äì It would be valuable to present results from ablations on datasets beyond EMBED, and to provide some theoretical explanation regarding how the proposed approach mitigates gradient conflict.  \n5. **Clinical Interpretation** ‚Äì Clarify the clinical meaning of results shown in Figure‚ÄØ5, particularly the observed differences in modality contributions across tasks.\n\n**Minor Comments**  \n- The paper‚Äôs presentation is generally strong, but minor editorial checks for consistency in acronyms and figure captions would further improve clarity.  \n- Ensure all references are correctly formatted and cited in-text.\n\n**Summary Paragraph**  \nThis study presents a well-executed multimodal fusion approach with clear motivation and strong empirical validation. The manuscript‚Äôs main strengths lie in its organization, clarity, and promising results across clinical benchmarks. However, more discussion on related work, computational considerations, and ablation design would enhance the paper‚Äôs completeness and reproducibility.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is well-structured and technically solid but requires additional analysis and discussion to adequately support its claims and situate it within the existing literature.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äî Analytical and Scope Audit**\n\n1. **Scope &‚ÄØRelevance to‚ÄØTMI**‚ÄØ‚Äì‚ÄØ‚úÖ  \n   The manuscript presents a new *methodological* framework for multi‚Äëmodal, multi‚Äëtask learning explicitly tailored to medical image analysis.  The focus on dynamic modality fusion, probabilistic modeling of modality‚Äìtask dependence, and interpretability aligns with IEEE‚ÄØTMI‚Äôs methodological emphasis (image representation, learning theory, and validation on medical data).\n\n2. **Novelty &‚ÄØContribution Level**‚ÄØ‚Äì‚ÄØHigh‚ÄìModerate‚ÄØ‚úÖ  \n   Introducing a *Modality‚ÄëTask Mixture‚Äëof‚ÄëExperts‚ÄØ(MToE)* with a conditional mutual‚Äëinformation objective constitutes a distinct methodological layer beyond standard soft‚ÄëMoE or late/early fusion.  While building upon known MoE and information‚Äëdisentanglement principles, the specific probabilistic design and dynamic modality‚Äìtask coupling appear novel in the context of medical‚Äëimaging multitask learning.\n\n3. **Technical &‚ÄØExperimental‚ÄØRigor**‚ÄØ‚Äì‚ÄØGenerally strong  \n   Mathematical formulation is clear and internally consistent. Implementation details (expert counts, optimizer, hyper‚Äëparameters) are sufficient for replication, though training‚Äëtime computation cost, convergence behavior, and statistical significance of results should be more fully characterized. Dataset use spans four independent cohorts‚Äîgood breadth‚Äîbut each dataset‚Äôs train/validation splits and label‚Äëbalancing protocol are not deeply discussed.\n\n4. **Clarity &‚ÄØPresentation**‚ÄØ‚Äì‚ÄØGood  \n   The manuscript reads smoothly, figures are conceptually clear, and ablation and analysis sections are detailed. Minor grammatical issues and typographic spacing need correction (e.g., inconsistent sub‚Äëscripts and Greek symbols).\n\n5. **Ethical‚ÄØ/‚ÄØReproducibility‚ÄØCompliance**‚ÄØ‚Äì‚ÄØAdequate with minor gaps  \n   All datasets are public and anonymized; IRB concerns minimal. Code availability is stated but placeholder link ‚ÄúXXX‚Äù must be replaced by an accessible repository. Authors should confirm license terms and any preprocessing steps adhering to dataset policies.\n\n\n---\n\n**Phase‚ÄØ2 ‚Äî IEEE‚ÄØTMI Review Report**\n\n### 1.‚ÄØSummary\nThe paper introduces **M4oE**, a *Multi‚Äëmodal‚ÄØMulti‚Äëtask‚ÄØMixture of‚ÄØExperts* framework for medical‚Äëimaging tasks such as mammographic risk/density estimation and retinal glaucoma grading‚ÄØ+‚ÄØsegmentation.  \nM4oE integrates two key modules:  \n- **Modality‚ÄëSpecific‚ÄØMoE (MSoE)**‚ÄØto retain modality‚Äëunique signals and reduce ‚Äúmodality competition.‚Äù  \n- **Modality‚ÄëTask‚ÄØMoE‚ÄØ(MToE)**‚ÄØto learn dynamic, task‚Äëdependent fusion by maximizing conditional mutual information between modalities and experts given tasks.  \nExtensive experiments on EMBED,‚ÄØRSNA,‚ÄØVinDr,‚ÄØand‚ÄØGAMMA datasets demonstrate consistent improvement over multi‚Äëmodal and multi‚Äëtask baselines, with additional interpretability analyses at both sample and population levels.\n\n### 2.‚ÄØStrengths\n- **Methodological innovation:** Novel probabilistic coupling of modality, expert, and task variables within an MoE backbone.  \n- **Broad empirical scope:** Multiple imaging domains and heterogeneous datasets, showing generalizability.  \n- **Interpretability:** Quantitative analysis of modality contributions and synergy provides insight beyond performance tables.  \n- **Solid ablations:** Clear demonstration of each component‚Äôs effect (MSoE,‚ÄØMToE, mutual‚Äëinformation loss).  \n- **Potential clinical relevance:** Tailored to realistic diagnostic protocols requiring multiple modalities and tasks.\n\n### 3.‚ÄØWeaknesses\n- **Incremental overlap** with recent MoE‚Äëbased fusion work (e.g., Soft‚ÄëMoE,‚ÄØFlex‚ÄëMoE) not fully differentiated theoretically.  \n- **Computational cost / scalability:** Lacking analysis of training efficiency, parameter count, and inference latency.  \n- **Statistical reporting:** Mean‚ÄØ¬±‚ÄØSDs appear only in appendix; significance testing or confidence intervals for performance improvements absent.  \n- **Dataset partitioning transparency:** Limited details on cross‚Äësite splits, patient‚Äëlevel separation, and how missing modalities or labels are handled.  \n- **Code accessibility pending:** the promised repository is not provided.\n\n### 4.‚ÄØMajor‚ÄØComments\n1. **Clarify methodological novelty vs existing MoE extensions** ‚Äì Elaborate how the conditional‚ÄëMI term and task‚Äëslot architecture achieve behavior unattainable by generic MoE fusions or adapter‚Äëbased parameter gating.  \n2. **Quantify computational complexity** ‚Äì Report parameter counts, FLOPs, runtime, and memory relative to baselines to ensure practicality in large‚Äëscale or clinical deployment.  \n3. **Provide statistical validation** ‚Äì Include paired statistical comparisons (e.g., Wilcoxon signed‚Äërank or t‚Äëtests) across folds to substantiate ‚Äúsignificant improvement.‚Äù  \n4. **Cross‚Äëdataset generalization** ‚Äì Consider hold‚Äëone‚Äëdataset‚Äëout evaluation to demonstrate robustness to institution shift, which is central to TMI readership.  \n5. **Reproducibility documentation** ‚Äì Supply complete code link and detailed preprocessing pipelines; specify random‚Äëseed runs and hardware to facilitate replication.\n\n### 5.‚ÄØMinor‚ÄØComments\n- Correct minor typographic inconsistencies (spacing around mathematical symbols, use of italics).  \n- Ensure figure font sizes readable at print resolution; merge Figures‚ÄØ3‚Äì5 into supplementary if exceeding page limits.  \n- In Section‚ÄØ3.3 equations, clearly define normalization constants and marginalization steps.  \n- Replace placeholder citation ‚ÄúCarr‚ÄØet‚ÄØal.‚Äù with full bibliographic detail.  \n- Clarify variable naming (e.g., whether‚ÄØM or‚ÄØm denotes number of modalities) to prevent ambiguity.\n\n### 6.‚ÄØOverall Recommendation  \n**Major Revision‚ÄØ‚òëÔ∏è**  \nThe work is promising and methodologically interesting, meeting TMI‚Äôs scope, but requires stronger validation and clearer differentiation from existing MoE variants before acceptance. The revisions should focus on (i) sharpening the theoretical contribution, (ii) adding quantitative significance and efficiency analyses, and (iii) improving reproducibility documentation.\n\n### 7.‚ÄØConfidence‚ÄØLevel‚ÄØ=‚ÄØ4‚ÄØ/‚ÄØ5  \n(High confidence in technical interpretation; moderate confidence pending access to code and full quantitative appendix.)\n\n---\n\n**Summary for Editors:**  \nThis manuscript presents a technically sound and potentially influential framework elevating multi‚Äëmodal, multi‚Äëtask modeling in medical imaging.‚ÄØIt sits comfortably within the methodological focus of‚ÄØTMI but would benefit from expanded validation and clearer exposition of novelty relative to contemporary mixture‚Äëof‚Äëexperts approaches.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a new *Multi‚Äëmodal Multi‚Äëtask Mixture‚Äëof‚ÄëExperts (M4oE)* framework designed for medical image analysis tasks such as mammographic risk assessment and retinal glaucoma grading with segmentation. The approach integrates two probabilistic components: a *Modality‚ÄëSpecific MoE* to preserve modality‚Äëunique features and a *Modality‚ÄëTask MoE* that dynamically fuses information across modalities via a conditional mutual‚Äëinformation objective. Experiments conducted on multiple public datasets demonstrate performance gains over standard fusion and multitask baselines, with further interpretability analysis offered at both individual and population levels. The manuscript is generally clear, mathematically consistent, and aligned with the methodological aims of the field.  \n\n---\n\n**Major Comments**  \n1. **Clarification of novelty:** The authors should more explicitly delineate how the proposed conditional‚ÄëMI‚Äëbased architecture and task‚Äëslot formulation differ from or improve upon existing MoE fusion methods (e.g., Soft‚ÄëMoE, Flex‚ÄëMoE).  \n2. **Computational complexity and scalability:** Provide a quantitative comparison of parameter count, FLOPs, runtime, and memory requirements relative to baseline models to assess feasibility for large‚Äëscale or clinical use.  \n3. **Statistical validation:** Include statistical significance testing (e.g., Wilcoxon or t‚Äëtests) to support claims of consistent improvement and provide confidence intervals for reported results.  \n4. **Dataset partitioning and generalization:** Expand details on data splits, patient‚Äëlevel separation, and handling of missing modalities or labels. Consider cross‚Äëdataset evaluation to test robustness to institutional variation.  \n5. **Reproducibility documentation:** Replace the placeholder code link with a public repository and document preprocessing pipelines, random seeds, and hardware configuration to ensure replicability.  \n\n---\n\n**Minor Comments**  \n- Correct typographic inconsistencies, such as spacing around mathematical symbols and inconsistent subscripts or Greek symbols.  \n- Ensure figures maintain legibility and consider reorganizing Figures‚ÄØ3‚Äì5 if they exceed space limits.  \n- Clearly define normalization constants and marginalization steps in Section‚ÄØ3.3 equations.  \n- Replace placeholder or incomplete references with full citations.  \n- Clarify all variable notation, especially the use of \\(M\\) versus \\(m\\) for the number of modalities.  \n\n---\n\n**Summary Paragraph**  \nOverall, the work presents a solid and methodologically motivated framework with promising experimental results and interpretability features. Strengths include a novel probabilistic coupling of modality, expert, and task spaces; evaluation on multiple datasets; and insightful analysis of modality contributions. However, to reach publication readiness, the authors should strengthen their evidence of novelty, provide comprehensive efficiency and statistical analyses, and improve reproducibility resources. The manuscript‚Äôs contribution appears significant but requires further validation and transparency before it can be considered fully convincing.  \n\n---\n\n**Decision Recommendation:** **Major Revision** ‚Äì The study is of high potential interest but needs expanded quantitative evidence, improved clarity regarding methodological distinctiveness, and complete reproducibility documentation prior to acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses multi-modal multi-task learning in medical imaging by proposing M4oE (Multi-modal Multi-task Mixture of Experts). The authors identify two key challenges: dynamic modality fusion where the quality and relevance of different imaging modalities varies across patients, and modality-task dependence where different clinical tasks require dynamic feature selection from various modalities. M4oE consists of Modality-Specific MoE (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module that dynamically decomposes modality-specific and shared information. The framework uses conditional mutual information loss to encourage experts to learn modality-task dependencies. Experiments on four public datasets (EMBED, RSNA, VinDr-Mammo, GAMMA) for breast cancer screening and retinal disease diagnosis demonstrate superior performance over state-of-the-art methods across classification and segmentation tasks, measured by Accuracy, AUROC, AUPRC, and DICE scores.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - The conditional probability definitions in Section 3.3 lack clear denominators and appear inconsistent (e.g., P(Ej|Tk) formula uses summation over modalities in numerator but tasks and experts in denominator)\n  - The conditional mutual information I(M; E|T) formulation in Section 3.3 is presented without sufficient mathematical rigor regarding how probabilities are estimated from the soft routing weights\n  - The relationship between the routing matrix XŒ¶ and probability distributions needs clearer mathematical justification, particularly how soft weights translate to valid probability measures\n\n‚Ä¢ **Limited experimental rigor and evaluation gaps**\n  - Table 1 shows results only for 3 out of 7 tasks on EMBED dataset, with complete results relegated to appendix, limiting assessment of multi-task performance claims\n  - The gradient conflict analysis in Figure 6 uses only a reduced experiment (16 experts, 7 tasks) rather than the full model configuration, weakening the validity of this key claim\n  - Cross-validation methodology is mentioned as 5-fold but detailed experimental setup, statistical significance testing, and error analysis are insufficient across all experiments\n\n‚Ä¢ **Insufficient baseline comparisons and ablation studies**\n  - The paper lacks comparison with recent MoE-based medical imaging methods, with some comparisons mentioned as \"included in Appendix\" (Page 7) but not thoroughly analyzed\n  - Ablation studies in Table 2 are limited to only 3 tasks from EMBED dataset, insufficient to validate component effectiveness across diverse medical imaging scenarios\n  - The modality competition analysis in Figure 3(a) compares only against basic fusion methods rather than more sophisticated multi-modal approaches from recent literature\n\n‚Ä¢ **Questionable interpretability claims and analysis depth**\n  - The sample-level interpretability claims in Figure 5 lack validation against ground truth or clinical expert assessment, making it unclear whether the interpretations are medically meaningful\n  - The synergy analysis using PID theory in Figure 4 requires external domain knowledge not sufficiently explained, and the clinical relevance of the synergy patterns is not validated\n  - Population-level modality contribution analysis lacks statistical validation and comparison with known clinical understanding of modality importance for different tasks\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation consistency**\n  - Provide rigorous derivations for all conditional probability formulations in Section 3.3, clearly defining how soft routing weights are converted to valid probability distributions\n  - Add formal mathematical proofs or detailed explanations for the conditional mutual information objective, including convergence properties and theoretical justifications\n  - Standardize notation throughout the paper and ensure all probability expressions have consistent and correct mathematical forms with proper normalization\n\n‚Ä¢ **Strengthen experimental validation and statistical analysis**\n  - Report complete results for all 7 tasks on EMBED dataset in the main paper with proper statistical significance testing and confidence intervals\n  - Conduct gradient conflict analysis using the full model configuration (128 experts) rather than reduced settings to validate this key technical claim\n  - Provide detailed cross-validation procedures, statistical significance tests, and comprehensive error analysis for all experimental results across all datasets\n\n‚Ä¢ **Expand baseline comparisons and comprehensive ablation studies**\n  - Include thorough comparison with recent MoE-based medical imaging methods in the main paper rather than relegating to appendix\n  - Conduct ablation studies across all four datasets and all tasks to demonstrate consistent component effectiveness across diverse medical imaging scenarios\n  - Compare against more sophisticated multi-modal fusion approaches from recent literature in the modality competition analysis\n\n‚Ä¢ **Validate interpretability claims with clinical ground truth**\n  - Collaborate with clinical experts to validate whether sample-level and population-level interpretability outputs align with known medical understanding\n  - Provide statistical validation of modality contribution patterns against established clinical knowledge and compare with expert radiologist assessments\n  - Include more detailed explanation of PID theory and clinical validation of synergy patterns, demonstrating that the learned modality-task dependencies reflect genuine medical insights",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 3,
          "specificity": 2,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 2,
          "source": "ai",
          "comment": "The comments simply summarize and describe this manuscript, without offering constructive suggestions regarding any potential scientific issues."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **M4oE (Multi-modal Multi-task Mixture of Experts)**, a framework designed for multi-modal, multi-task learning in medical imaging. The method seeks to address two key challenges‚Äîdynamic modality fusion and modality‚Äìtask dependence‚Äîby combining modality-specific and modality-shared modules (MSoE and MToE). A conditional mutual information loss is applied to model dependencies between tasks and modalities. Experiments spanning four public datasets (EMBED, RSNA, VinDr-Mammo, and GAMMA) demonstrate performance gains over state-of-the-art methods for both classification and segmentation. The paper is conceptually ambitious and addresses a relevant problem, but several issues in mathematical formulation, evaluation rigor, and validation limit its clarity and credibility.  \n\n---\n\n**Major Comments**  \n1. **Mathematical Clarity and Consistency**  \n   - Conditional probability definitions in Section‚ÄØ3.3 lack proper normalization and consistent denominators.  \n   - The conditional mutual information \\(I(M;E|T)\\) term is not explained rigorously, particularly regarding estimation from soft routing weights.  \n   - The mapping from the routing matrix \\(X_\\Phi\\) to valid probability distributions requires clearer theoretical justification.  \n\n2. **Experimental Rigor and Evaluation Gaps**  \n   - Only partial results for the EMBED dataset (three of seven tasks) appear in the main text, limiting assessment of multi-task performance.  \n   - The gradient conflict analysis relies on a reduced configuration (16‚ÄØexperts,‚ÄØ7‚ÄØtasks) rather than the full model, weakening this technical claim.  \n   - Cross-validation setup, significance tests, and error analyses are insufficiently detailed.  \n\n3. **Baseline Comparisons and Ablations**  \n   - Comparisons with recent MoE-based medical imaging approaches are missing or relegated to the appendix.  \n   - Ablation studies cover few tasks and datasets, reducing generalizability of component analysis.  \n   - The modality competition study uses only basic fusion baselines rather than stronger contemporary benchmarks.  \n\n4. **Interpretability and Analytical Depth**  \n   - Sample-level interpretability (Figure‚ÄØ5) is not validated against clinical ground truth.  \n   - The synergy analysis employing PID theory is under-explained and lacks demonstration of clinical relevance.  \n   - Population-level modality contributions are not statistically validated or compared to known modality importance in practice.  \n\n---\n\n**Minor Comments**  \n- Standardize notation across Sections‚ÄØ3.2‚Äì3.3 and ensure consistent probability formatting.  \n- Clarify figure captions (especially Figures‚ÄØ3‚Äì6) to enhance reader comprehension.  \n- Expand explanations for specialized terms (e.g., ‚Äúsynergy metrics‚Äù within PID theory).  \n\n---\n\n**Summary Paragraph**  \nThe work presents a novel and potentially impactful framework for multi-modal, multi-task medical image analysis. Strengths include the clear identification of modality-task interdependencies and an integrated design combining MoE components for adaptability. However, methodological rigor requires reinforcement‚Äîparticularly in formal probability definitions, complete and statistically supported experimental results, and validation of interpretability claims with clinical evidence. Addressing these issues would substantially strengthen the manuscript‚Äôs technical and empirical credibility.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The approach is promising but requires major clarification of mathematical foundations, comprehensive experiments, and validated interpretability analyses before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces M4oE, a novel Multi-modal Multi-task Mixture of Experts framework designed to address the challenges of dynamic modality fusion and modality-task dependence in medical imaging. The authors propose M4oE to dynamically decompose and learn distinct and shared information from different modalities, thereby achieving dynamic fusion and enhancing the interpretability of modality contributions. The method is evaluated on four public multi-modal medical benchmark datasets for breast cancer screening and retinal disease diagnosis, demonstrating superior performance over state-of-the-art methods across various metrics.\n\n## Major Comments\n1. Novelty and Positioning: While the proposed M4oE framework introduces innovative elements such as dynamic modality fusion and task-specific feature selection, the manuscript could benefit from a more detailed comparison with existing multi-modal multi-task learning methods. The authors should clarify how M4oE stands apart from recent advancements in the field, particularly those that also employ MoE architectures.\n\n2. Evaluation Design: The experimental setup is comprehensive, covering multiple datasets and tasks. However, the inclusion of additional real-world clinical scenarios and a wider variety of medical imaging modalities would further strengthen the validation of M4oE's generalizability. Additionally, the authors should consider providing more detailed insights into the robustness of M4oE under varying data quality and availability conditions.\n\n3. Comparisons: The comparison against state-of-the-art methods is thorough, but the manuscript should include a discussion on the limitations of the chosen baselines. For instance, the authors could elaborate on why certain methods were excluded and how the inclusion of these methods might impact the comparative analysis.\n\n4. Reproducibility: The manuscript mentions that the code will be made available, but it is essential to ensure that the methodology is sufficiently detailed to enable independent replication of the results. Providing explicit instructions on data preprocessing, hyperparameter tuning, and training procedures would greatly enhance the reproducibility of M4oE.\n\n## Minor Comments\n1. Clarity of Figures: Some figures (e.g., Figure 3) are cluttered and could be improved by focusing on fewer representative examples with zoomed-in regions.\n2. Notation Consistency: There are inconsistencies in the notation used for the forward operator and other mathematical symbols, which could be clarified.\n3. Acronym Definitions: Several acronyms (e.g., \"R=4\") are used without definitions, which should be addressed for clarity.\n4. Typographical Errors: Minor typographical errors (\"k-spacce\", \"undersampling maskes\") should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge in medical imaging by proposing M4oE, a framework that dynamically integrates information from multiple modalities and tasks. The innovation lies in the dynamic fusion of modalities and the explicit modeling of modality-task dependence, which could lead to improved interpretability and performance in medical diagnosis. The evaluation demonstrates the method's superiority over existing approaches across various datasets and tasks, indicating its potential clinical impact. However, the evaluation could be strengthened by including a broader range of clinical scenarios and modalities. The reproducibility of M4oE is promising, although additional methodological details are necessary to ensure complete replicability. Overall, M4oE shows promise but requires further refinement in terms of comparative analysis and generalizability to fully meet the standards of TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include a broader range of methods, strengthen the validation across different clinical scenarios, and provide more detailed methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "ai",
          "comment": "The major comments primarily focus on textual analysis, lacking analysis of tables and figures."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *M4oE*, a Multi-modal Multi-task Mixture of Experts framework developed to address challenges in dynamic modality fusion and modality‚Äìtask dependence within medical imaging. The approach decomposes and learns both shared and distinct modality information, enabling adaptive fusion and enhancing interpretability of modality contributions. The method is evaluated on four public benchmark datasets for breast cancer screening and retinal disease diagnosis, showing superior performance relative to state-of-the-art approaches across multiple metrics. Overall, the paper is well structured and the technical presentation is clear.\n\n**Major Comments**  \n1. **Novelty and Positioning** ‚Äì The proposed M4oE framework introduces dynamic fusion and task-specific feature selection in a novel manner, but the manuscript would benefit from a clearer comparison to existing multi-modal multi-task learning approaches. The distinction from related Mixture of Experts (MoE)-based architectures should be explicitly articulated to strengthen the paper‚Äôs contribution and positioning within current literature.  \n2. **Evaluation Design** ‚Äì While the experiments comprehensively cover several datasets and tasks, additional results on real-world clinical scenarios and a wider range of imaging modalities would substantially reinforce claims of generalizability. Discussion of robustness to varying data quality or incomplete modalities would also be valuable.  \n3. **Comparative Analysis** ‚Äì Although comparisons with state-of-the-art baselines are largely adequate, the manuscript lacks discussion regarding the rationale for chosen baselines and the exclusion of others. Clarifying these choices would help contextualize the reported improvements.  \n4. **Reproducibility** ‚Äì The intent to release code is appreciated, yet methodological transparency could be improved. Expanded details on data preprocessing, hyperparameter tuning, and training configurations are necessary to allow independent replication of the reported results.\n\n**Minor Comments**  \n- Figures such as Figure 3 are visually cluttered and would benefit from focusing on fewer, more informative examples with magnified regions.  \n- Mathematical notation (e.g., forward operator symbols) appears inconsistent and should be standardized.  \n- Undefined acronyms such as ‚ÄúR=4‚Äù should be clearly introduced.  \n- Minor typographical errors (e.g., ‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù) require correction.\n\n**Summary Paragraph**  \nThis study proposes an innovative framework tackling multi-modal and multi-task integration in medical imaging, offering potentially improved interpretability and diagnostic performance. Experimental outcomes demonstrate clear advantages over existing methods. Nonetheless, the paper would be strengthened through broader experimental validation, more explicit comparisons with related methods, and deeper methodological transparency to ensure reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The work is promising but requires expanded comparative analysis, more diverse validation scenarios, and additional methodological detail before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## DYNAMIC MODELING OF PATIENTS, MODALITIES AND TASKS VIA MULTI-MODAL MULTI-TASK MIXTURE OF EXPERTS\n\n### Summary\n\nThe paper proposes M‚Å¥oE, a multi-modal multi-task Mixture-of-Experts framework that learns both sample-dynamic modality-specific information and task-dependent modality-shared information. The architecture combines modality-specific Soft-MoE modules (MSoE) with a modality-shared modality‚Äìtask MoE (MToE) guided by a conditional mutual information (CMI) regularizer, and introduces a joint probability view to interpret modality‚Äìexpert‚Äìtask dependencies. Experiments on four public datasets across mammography and ophthalmology show consistent gains over strong baselines for classification and segmentation, with analyses on modality competition, modality‚Äìtask synergy, and gradient conflicts.\n\n### Strengths\n\n- Technical novelty and innovationThe decomposition into modality-specific and modality-shared MoE components is well-motivated and addresses the ‚Äúmodality competition‚Äù issue in a principled way.The MToE introduces task slots and task embeddings, providing a practical mechanism to model modality‚Äìtask dependence and to specialize experts across tasks.The conditional mutual information regularizer to encourage task-conditional dependence between modalities and experts is an interesting design that aims to improve identifiability and interpretability.The joint probabilistic perspective over modalities, experts, and tasks offers a path toward sample-level and dataset-level interpretability.\n- Experimental rigor and validationExtensive evaluation across four public datasets (EMBED, RSNA, VinDr, GAMMA) with heterogeneous modalities and tasks (classification, segmentation) demonstrates generality.Ablation studies show the contribution of both MSoE and MToE and the effect of the CMI term and number of experts.Analyses include synergy (PID-based) to study modality‚Äìtask dependence and gradient conflict evaluation, going beyond aggregate metrics.\n- Clarity of presentationThe high-level motivation and clinical relevance are clearly articulated, with concrete examples (e.g., FFDM vs 2DS, CFP vs OCT) and intuitive hypotheses.The architectural overview and the stepwise data flow (embedders ‚Üí MToE ‚Üí MSoE ‚Üí fusion ‚Üí task heads) are conceptually clear and supported by figure descriptions.\n- Significance of contributionsThe problem of dynamic, task-dependent multimodal fusion is important in medical imaging, where modality quality and utility vary across patients and tasks.Demonstrating performance gains and interpretability across multiple institutions and populations suggests meaningful potential impact for clinical AI systems.\n\n- The decomposition into modality-specific and modality-shared MoE components is well-motivated and addresses the ‚Äúmodality competition‚Äù issue in a principled way.\n- The MToE introduces task slots and task embeddings, providing a practical mechanism to model modality‚Äìtask dependence and to specialize experts across tasks.\n- The conditional mutual information regularizer to encourage task-conditional dependence between modalities and experts is an interesting design that aims to improve identifiability and interpretability.\n- The joint probabilistic perspective over modalities, experts, and tasks offers a path toward sample-level and dataset-level interpretability.\n\n- Extensive evaluation across four public datasets (EMBED, RSNA, VinDr, GAMMA) with heterogeneous modalities and tasks (classification, segmentation) demonstrates generality.\n- Ablation studies show the contribution of both MSoE and MToE and the effect of the CMI term and number of experts.\n- Analyses include synergy (PID-based) to study modality‚Äìtask dependence and gradient conflict evaluation, going beyond aggregate metrics.\n\n- The high-level motivation and clinical relevance are clearly articulated, with concrete examples (e.g., FFDM vs 2DS, CFP vs OCT) and intuitive hypotheses.\n- The architectural overview and the stepwise data flow (embedders ‚Üí MToE ‚Üí MSoE ‚Üí fusion ‚Üí task heads) are conceptually clear and supported by figure descriptions.\n\n- The problem of dynamic, task-dependent multimodal fusion is important in medical imaging, where modality quality and utility vary across patients and tasks.\n- Demonstrating performance gains and interpretability across multiple institutions and populations suggests meaningful potential impact for clinical AI systems.\n\n### Weaknesses\n\n- Technical limitations or concernsProbability modeling for interpretability appears to use unnormalized routing logits (XŒ¶) instead of normalized dispatch/combine probabilities, which is not a valid probabilistic measure and could bias the estimated P(E|T), P(M|T), and P(M,E|T).The CMI objective relies on the above probabilities; without proper normalization and calibration, its theoretical grounding and interpretability are weakened.The MoE training typically benefits from load-balance/importance regularizers to prevent expert collapse; these are not described, raising concerns about stability and expert utilization skew.Computational complexity may scale as O(l m n p) for routing across tokens, modalities, experts, and tasks; the paper does not quantify training/inference cost or memory versus baselines.Handling missing modalities and asynchrony‚Äîcommon in real clinical workflows‚Äîis not addressed.\n- Experimental gaps or methodological issuesStatistical significance, confidence intervals, and calibration (e.g., AUROC/AUPRC with bootstrapped CIs, ECE) are missing; many gains are modest and would benefit from significance testing.Fairness of comparisons is unclear: expert counts and model capacities are larger than many baselines; hyperparameter tuning budgets for baselines are not reported; inference cost comparisons are absent.The segmentation pipeline details are thin (decoder architecture, loss, how tokens map to masks), limiting reproducibility and interpretability of segmentation results.No robustness studies (distribution shift, missing modalities, noise) or subgroup analyses (e.g., by breast density, age, scanner/vendor) are reported.Cross-dataset generalization (train on one site, test on another) is not evaluated; all results appear to be within-dataset cross-validation.\n- Clarity or presentation issuesNotation and shape definitions in the MToE and probability modeling sections are inconsistent (e.g., rows vs columns softmax; sums over indices; M2oE vs M‚Å¥oE naming; occasional typos like ‚ÄúMSvE/MSoE‚Äù).The dispatch/combine matrices and their shapes and normalization axes are not consistently specified, hindering precise understanding.The probabilistic modeling section mixes token- and modality-level quantities without a clean marginalization derivation.\n- Missing related work or comparisonsTask-conditioned MoE for multi-task vision (e.g., M3ViT) directly tackles gradient conflicts and should be cited and compared or discussed.Recent MoE fusion works such as FuseMoE (Laplace gating, improved expert balance, missing-modality robustness), and differentiable routing alternatives like SMEAR should be discussed to better situate the approach.Multi-modal mammography transformers for risk and detection (e.g., large-scale temporal cross-modal models) are highly relevant for contextualizing the mammography results and clinical significance.Prompt-expert approaches (MoPE) and data-free MoE merging (WEMoE) illustrate other ways to achieve adaptive specialization; relating to them would enrich the framing.\n\n- Probability modeling for interpretability appears to use unnormalized routing logits (XŒ¶) instead of normalized dispatch/combine probabilities, which is not a valid probabilistic measure and could bias the estimated P(E|T), P(M|T), and P(M,E|T).\n- The CMI objective relies on the above probabilities; without proper normalization and calibration, its theoretical grounding and interpretability are weakened.\n- The MoE training typically benefits from load-balance/importance regularizers to prevent expert collapse; these are not described, raising concerns about stability and expert utilization skew.\n- Computational complexity may scale as O(l m n p) for routing across tokens, modalities, experts, and tasks; the paper does not quantify training/inference cost or memory versus baselines.\n- Handling missing modalities and asynchrony‚Äîcommon in real clinical workflows‚Äîis not addressed.\n\n- Statistical significance, confidence intervals, and calibration (e.g., AUROC/AUPRC with bootstrapped CIs, ECE) are missing; many gains are modest and would benefit from significance testing.\n- Fairness of comparisons is unclear: expert counts and model capacities are larger than many baselines; hyperparameter tuning budgets for baselines are not reported; inference cost comparisons are absent.\n- The segmentation pipeline details are thin (decoder architecture, loss, how tokens map to masks), limiting reproducibility and interpretability of segmentation results.\n- No robustness studies (distribution shift, missing modalities, noise) or subgroup analyses (e.g., by breast density, age, scanner/vendor) are reported.\n- Cross-dataset generalization (train on one site, test on another) is not evaluated; all results appear to be within-dataset cross-validation.\n\n- Notation and shape definitions in the MToE and probability modeling sections are inconsistent (e.g., rows vs columns softmax; sums over indices; M2oE vs M‚Å¥oE naming; occasional typos like ‚ÄúMSvE/MSoE‚Äù).\n- The dispatch/combine matrices and their shapes and normalization axes are not consistently specified, hindering precise understanding.\n- The probabilistic modeling section mixes token- and modality-level quantities without a clean marginalization derivation.\n\n- Task-conditioned MoE for multi-task vision (e.g., M3ViT) directly tackles gradient conflicts and should be cited and compared or discussed.\n- Recent MoE fusion works such as FuseMoE (Laplace gating, improved expert balance, missing-modality robustness), and differentiable routing alternatives like SMEAR should be discussed to better situate the approach.\n- Multi-modal mammography transformers for risk and detection (e.g., large-scale temporal cross-modal models) are highly relevant for contextualizing the mammography results and clinical significance.\n- Prompt-expert approaches (MoPE) and data-free MoE merging (WEMoE) illustrate other ways to achieve adaptive specialization; relating to them would enrich the framing.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe core MoE design is reasonable; however, the probability modeling of P(E|T), P(M|T), P(M,E|T) should use normalized routing probabilities (e.g., expected dispatch/combination weights D and C) aggregated over tokens and samples. Using raw logits (XŒ¶) breaks probabilistic interpretation. Recommend computing empirical conditional distributions from normalized D and C, with appropriate marginalization over tokens and samples, and possibly temperature scaling or calibration.The conditional mutual information objective I(M;E|T) is sound conceptually, but its estimation should be based on properly normalized empirical distributions with smoothing to avoid zero-probability issues. Clarify whether estimates are per-batch or running averages, and whether gradients are stopped through the probability estimates or not (to avoid degenerate feedback loops).Add a standard MoE load-balancing/importance loss to avoid expert collapse and improve specialization stability, or clarify if the CMI term implicitly achieves this. Report expert utilization entropy and coefficient of variation.Complexity: Give FLOPs/throughput and memory for representative settings (tokens l, modalities m, experts n, tasks p) and compare to late/early fusion and to MoE baselines (e.g., soft MoE without MToE). Consider top-k/sparse routing variants or per-modality routers to reduce O(n p) scaling.Segmentation head: Provide architectural details (decoder design, upsampling, loss functions), how token features are mapped to spatial masks, and how multi-scale information is handled.\n- Experimental evaluation assessmentReport AUROC/AUPRC with bootstrapped 95% CIs for classification tasks (especially imbalanced risk), and DICE with CIs for segmentation. Add calibration metrics (ECE, Brier) and threshold selection protocol.Perform statistical tests for pairwise comparisons where absolute gains are small (e.g., +0.5‚Äì1.5%), and report effect sizes.Baseline fairness: Match model capacity where possible (parameter counts, depth/width) or include capacity-controlled ablations. Detail tuning budgets and early stopping criteria for all baselines.Robustness: Evaluate missing-modality robustness (e.g., modality dropout during training and test-time ablations), noise corruptions, and cross-site generalization (train on one dataset, test on another), especially given the clinical heterogeneity.Subgroup analyses: For mammography, stratify by BI-RADS density, age groups, and vendor; test whether modality contributions correlate with known clinical patterns and whether performance holds across subgroups.Ablations: Include ablation of task embeddings, number of task slots per expert, routing temperature, and effect of replacing soft routing with top-k routing. Quantify the impact of the CMI loss on synergy patterns and expert specialization.\n- Comparison with related work (using the summaries provided)FuseMoE (2402.03226) shows Laplace distance-based gating improves expert balance and convergence; discussing or testing Laplace gating in MToE could further mitigate modality competition and improve routing stability.M3ViT (2210.14793) demonstrates that task-dependent sparse MoE reduces gradient conflicts; positioning your gradient conflict analysis against M3ViT would strengthen claims and suggest architectural synergies.SMEAR (2306.03745) provides fully differentiable expert merging; while different in goal, contrasting routing stability and compute trade-offs would contextualize your design choices.Large multimodal mammography transformers (e.g., MMT/NYU) demonstrate strong cross-modal fusion for detection and long-term risk; referencing them clarifies the clinical landscape and how your approach relates in scope (e.g., simultaneous multi-task learning versus longitudinal modeling).Prompt-expert fusion (MoPE, 2403.10568) and data-free merging (WEMoE, 2402.00433) highlight alternative specialization mechanisms; brief discussion can help frame your expert specialization and interpretability contributions.\n- Discussion of broader impact and significanceThe proposed interpretability (sample- and population-level modality contributions) is clinically valuable but should be presented cautiously: routing weights are a proxy for contribution, not causal evidence. Consider complementary attribution analyses (e.g., intervention by ablating modalities) to validate interpretability claims.Clinical deployment would require calibration, stability under acquisition differences, and bias assessment across demographics; the diverse datasets are a good starting point, but targeted fairness audits are recommended.Computational cost and energy footprint of large expert pools can be a concern in clinical settings; reporting throughput and memory will help practitioners assess deployability.\n\n- The core MoE design is reasonable; however, the probability modeling of P(E|T), P(M|T), P(M,E|T) should use normalized routing probabilities (e.g., expected dispatch/combination weights D and C) aggregated over tokens and samples. Using raw logits (XŒ¶) breaks probabilistic interpretation. Recommend computing empirical conditional distributions from normalized D and C, with appropriate marginalization over tokens and samples, and possibly temperature scaling or calibration.\n- The conditional mutual information objective I(M;E|T) is sound conceptually, but its estimation should be based on properly normalized empirical distributions with smoothing to avoid zero-probability issues. Clarify whether estimates are per-batch or running averages, and whether gradients are stopped through the probability estimates or not (to avoid degenerate feedback loops).\n- Add a standard MoE load-balancing/importance loss to avoid expert collapse and improve specialization stability, or clarify if the CMI term implicitly achieves this. Report expert utilization entropy and coefficient of variation.\n- Complexity: Give FLOPs/throughput and memory for representative settings (tokens l, modalities m, experts n, tasks p) and compare to late/early fusion and to MoE baselines (e.g., soft MoE without MToE). Consider top-k/sparse routing variants or per-modality routers to reduce O(n p) scaling.\n- Segmentation head: Provide architectural details (decoder design, upsampling, loss functions), how token features are mapped to spatial masks, and how multi-scale information is handled.\n\n- Report AUROC/AUPRC with bootstrapped 95% CIs for classification tasks (especially imbalanced risk), and DICE with CIs for segmentation. Add calibration metrics (ECE, Brier) and threshold selection protocol.\n- Perform statistical tests for pairwise comparisons where absolute gains are small (e.g., +0.5‚Äì1.5%), and report effect sizes.\n- Baseline fairness: Match model capacity where possible (parameter counts, depth/width) or include capacity-controlled ablations. Detail tuning budgets and early stopping criteria for all baselines.\n- Robustness: Evaluate missing-modality robustness (e.g., modality dropout during training and test-time ablations), noise corruptions, and cross-site generalization (train on one dataset, test on another), especially given the clinical heterogeneity.\n- Subgroup analyses: For mammography, stratify by BI-RADS density, age groups, and vendor; test whether modality contributions correlate with known clinical patterns and whether performance holds across subgroups.\n- Ablations: Include ablation of task embeddings, number of task slots per expert, routing temperature, and effect of replacing soft routing with top-k routing. Quantify the impact of the CMI loss on synergy patterns and expert specialization.\n\n- FuseMoE (2402.03226) shows Laplace distance-based gating improves expert balance and convergence; discussing or testing Laplace gating in MToE could further mitigate modality competition and improve routing stability.\n- M3ViT (2210.14793) demonstrates that task-dependent sparse MoE reduces gradient conflicts; positioning your gradient conflict analysis against M3ViT would strengthen claims and suggest architectural synergies.\n- SMEAR (2306.03745) provides fully differentiable expert merging; while different in goal, contrasting routing stability and compute trade-offs would contextualize your design choices.\n- Large multimodal mammography transformers (e.g., MMT/NYU) demonstrate strong cross-modal fusion for detection and long-term risk; referencing them clarifies the clinical landscape and how your approach relates in scope (e.g., simultaneous multi-task learning versus longitudinal modeling).\n- Prompt-expert fusion (MoPE, 2403.10568) and data-free merging (WEMoE, 2402.00433) highlight alternative specialization mechanisms; brief discussion can help frame your expert specialization and interpretability contributions.\n\n- The proposed interpretability (sample- and population-level modality contributions) is clinically valuable but should be presented cautiously: routing weights are a proxy for contribution, not causal evidence. Consider complementary attribution analyses (e.g., intervention by ablating modalities) to validate interpretability claims.\n- Clinical deployment would require calibration, stability under acquisition differences, and bias assessment across demographics; the diverse datasets are a good starting point, but targeted fairness audits are recommended.\n- Computational cost and energy footprint of large expert pools can be a concern in clinical settings; reporting throughput and memory will help practitioners assess deployability.\n\n### Questions for Authors\n\n- In the probability modeling section, why are raw routing scores (XŒ¶) used to compute P(E|T), P(M|T), and P(M,E|T) instead of normalized dispatch/combine probabilities? Could you provide corrected formulations and results using normalized weights?\n- Do you use any load-balancing/importance regularization to prevent expert collapse (as in standard MoE)? If not, what empirical evidence shows the CMI term is sufficient to guarantee balanced expert utilization?\n- How are the empirical probabilities for I(M;E|T) estimated in practice (per batch vs moving average)? Are gradients backpropagated through the probability estimates? How do you prevent degenerate solutions?\n- What is the computational complexity and memory footprint of MToE as a function of tokens l, modalities m, experts n, and tasks p? Please provide throughput/FLOPs/VRAM for your main configurations and comparisons to baselines.\n- How is the segmentation head implemented? Please detail the decoder architecture, loss functions, and how token features are mapped to spatial resolutions for mask prediction.\n- How do you handle missing modalities at training/inference time? Have you evaluated robustness to missing or corrupted modalities (e.g., modality dropout during training)?\n- What loss weighting strategy across tasks is used (uniform, uncertainty weighting, dynamic reweighting)? Did you experiment with gradient-balancing techniques, and how do results compare?\n- Could you report calibration metrics (ECE/Brier) and confidence intervals for key metrics, particularly on imbalanced risk tasks?\n- Did you evaluate cross-dataset generalization (e.g., train on EMBED, test on RSNA/VinDr) and subgroup performance (e.g., dense vs non-dense breasts)? If so, how do modality contributions and performance vary?\n- Can you compare to or discuss task-conditioned MoE baselines like M3ViT and Laplace-gated FuseMoE, and clarify how your MToE differs or could incorporate such routing to improve stability?\n- In the gradient conflict analysis, how sensitive are the results to the choice of layer and the cosine similarity thresholds? Do you observe consistent reductions across multiple layers?\n- For interpretability, have you validated that the modality contribution estimates correlate with performance changes under modality ablations on a per-sample basis?\n\n### Overall Assessment\n\nThis paper tackles an important and underexplored aspect of multimodal medical AI: dynamic, task-dependent fusion that varies across patients and tasks. The M‚Å¥oE architecture is thoughtful and practically motivated, combining modality-specific and task-conditioned shared experts with an information-theoretic regularizer and offering interpretability hooks. Empirical results are strong across multiple public datasets and tasks, and the accompanying analyses (modality competition, synergy, gradient conflicts) deepen understanding. The main reservations are technical and methodological: the current probability modeling and CMI estimation rely on unnormalized routing scores, which undermines the probabilistic interpretation and could affect both the interpretability claims and the regularization‚Äôs efficacy. In addition, computational complexity, baseline fairness, calibration, robustness to missing modalities, and segmentation details need clearer treatment. With these issues addressed‚Äîparticularly correcting the probabilistic formulations, adding load-balancing, reporting compute and statistical significance, and situating the work more fully among recent MoE-based MTL/fusion approaches‚Äîthis work would be a strong candidate for a top-tier venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 5,
          "stance": 3,
          "source": "ai",
          "comment": "The major comments primarily focus on textual analysis, lacking analysis of tables and figures."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **M‚Å¥oE**, a multi-modal, multi-task Mixture-of-Experts (MoE) framework designed to capture both modality-specific and task-dependent shared representations. The approach integrates modality-specific Soft-MoE modules with a modality‚Äìtask MoE, guided by a conditional mutual information (CMI) regularizer and interpreted through a joint probabilistic formulation linking modalities, experts, and tasks. Evaluations on four public datasets in mammography and ophthalmology demonstrate consistent improvements over baselines for both classification and segmentation tasks. The work addresses an important problem in dynamic, task-dependent multimodal fusion with clear clinical motivation, but certain methodological, experimental, and clarity issues limit its current reliability and interpretability.  \n\n---\n\n**Major Comments**  \n1. **Probabilistic Modeling** ‚Äì The interpretability analysis employs unnormalized routing logits rather than normalized probabilities, compromising the validity of estimated conditional distributions (e.g., \\(P(E|T)\\), \\(P(M|T)\\)). This weakens both the theoretical grounding of the CMI objective and its interpretability.  \n2. **Expert Regularization and Stability** ‚Äì The absence of standard load-balancing or importance regularizers raises concern about potential expert collapse and uneven utilization. Clarification on whether the CMI term fulfills this role is needed.  \n3. **Computational Complexity** ‚Äì The framework‚Äôs computational and memory cost likely scales with tokens, modalities, experts, and tasks, but empirical measurements and comparisons to baselines are not provided.  \n4. **Experimental Rigor and Reproducibility** ‚Äì Statistical significance, calibration metrics, and confidence intervals are missing. Model capacity fairness, tuning budgets, and inference cost comparisons to baselines are unclear. Segmentation setup lacks essential details about decoder design and loss formulation.  \n5. **Robustness and Generalization** ‚Äì No evaluation is presented for missing modalities, noise robustness, or cross-dataset generalization, limiting clinical applicability.  \n6. **Clarity and Notation** ‚Äì Inconsistencies in shape definitions, normalization axes, and naming (e.g., M2oE vs M‚Å¥oE) reduce readability.  \n7. **Related Work Contextualization** ‚Äì Key comparisons to recent task-conditioned and fusion MoE architectures (e.g., M3ViT, FuseMoE, SMEAR, MoPE, WEMoE) are missing, as are references to multimodal mammography transformers that frame clinical significance.  \n\n---\n\n**Minor Comments**  \n- Define dispatch/combine matrices explicitly and specify normalization axes.  \n- Clarify whether empirical probabilities for I(M;E|T) are estimated per batch or via running averages.  \n- Quantify expert utilization variance or entropy to substantiate balanced training.  \n- Report throughput, memory use, and compute requirements relative to plain MoE and fusion baselines.  \n- Add subgroup and calibration analyses, and perform ablations on task embeddings, expert counts, and routing temperature. Typographical issues (e.g., ‚ÄúMSvE/MSoE‚Äù) should also be corrected.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper presents a technically ambitious and clinically relevant framework for adaptive multimodal, multitask learning. Its core idea‚Äîa decomposition into modality-specific and modality-shared experts with task conditioning and mutual information regularization‚Äîis novel and well-motivated. Strengths include architectural design, interpretability potential, and broad empirical coverage. However, crucial gaps remain in probabilistic correctness, experimental validation, statistical robustness, computational reporting, and contextual comparison. Addressing these limitations would substantially enhance the validity, reproducibility, and interpretability of the results and strengthen the work‚Äôs impact.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The approach is promising and well-motivated but requires revision to correct probabilistic modeling, add normalization-based analysis, include expert regularization, provide computational comparisons and statistical significance tests, and discuss recent related methods.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for IEEE Transactions on Medical Imaging\n\n## 1. Opening Paragraph\n\nThis manuscript addresses a critical challenge in medical imaging: how to effectively model the dynamic relationships between multiple imaging modalities and multiple clinical tasks. The authors identify two key limitations in existing multi-modal multi-task learning approaches: 1) the inability to capture patient-specific dynamic modality fusion (where the relative importance of different modalities varies across patient subgroups), and 2) the lack of modeling modality-task dependence (where different clinical tasks require different combinations of information from available modalities). To address these issues, the authors propose M‚Å¥oE (Multi-modal Multi-task Mixture of Experts), a novel framework that incorporates both Modality-Specific MoE (MSoE) modules and a Modality-Shared Modality-Task MoE (MToE) module. The MToE component specifically models the joint probability distribution of modalities, experts, and tasks through conditional mutual information loss, enabling dynamic fusion that adapts to both patient characteristics and target tasks. The method demonstrates superior performance across four public medical imaging datasets for breast cancer screening and retinal disease diagnosis, outperforming state-of-the-art baselines in multiple metrics including accuracy, AUROC, AUPRC, and DICE scores. Additionally, the framework provides valuable sample-level and population-level interpretability of modality contributions to clinical predictions.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The paper identifies two clinically important but overlooked problems in medical multi-modal multi-task learning: dynamic modality fusion and modality-task dependence. These issues are well-motivated with clinical examples from breast imaging and ophthalmology.\n- The M‚Å¥oE framework is novel and well-designed, with the MToE module providing a mathematically sound approach to modeling the joint probability of modalities, experts, and tasks.\n- The extensive evaluation across four diverse medical imaging benchmarks (with different modalities, tasks, and patient populations) provides strong evidence of the method's generalizability and superiority.\n- The interpretability analysis is particularly valuable, showing how the model dynamically allocates modality importance at both population and individual patient levels, which has clear clinical relevance.\n\n**Limitations:**\n- The computational complexity and resource requirements are not thoroughly discussed. With 128 experts in the MToE module and 32 experts for each MSoE module, the model is likely much more resource-intensive than baselines, which could limit clinical applicability. The paper mentions hardware (4 NVIDIA A100s and 4 L40s) but lacks specific inference time or memory footprint comparisons.\n- While the paper demonstrates the method's superiority on standard metrics, it doesn't provide analysis of whether the performance gains translate to clinically meaningful improvements (e.g., how many additional cancers might be detected with the proposed method compared to baselines).\n- The conditional mutual information loss formulation (Section 3.3) lacks theoretical justification for why this specific formulation addresses the identified problems. The relationship between the mutual information term and the dynamic fusion properties needs clearer explanation.\n\n### Minor Comments\n- The visualization in Figure 5 (modality contribution) would benefit from clearer labeling of which modalities correspond to which colors in the patient examples.\n- Some sections contain minor grammatical issues that should be addressed (e.g., \"M‚Å¥oE has inherent sample and population level interpretability\" should be \"M‚Å¥oE provides inherent sample and population level interpretability\").\n- The paper would benefit from additional discussion about potential failure modes or scenarios where the approach might not work well (e.g., with highly imbalanced modalities or extremely limited data).\n- The ablation study on Œ± (Figure 7b) shows that performance degrades when Œ± > 0.1, but the paper doesn't explain why this happens or how to choose optimal Œ± values for different applications.\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a critical gap in medical multi-modal learning where existing approaches fail to capture dynamic relationships between modalities and tasks. The clinical motivation is strong, with examples showing how modality importance varies by patient characteristics (e.g., breast density affecting which mammography views are most informative). The ability to dynamically adapt fusion strategies could significantly improve diagnostic accuracy in real-world medical settings where patient populations are heterogeneous. The clinical interpretability component adds further value by providing insights into modality contributions that could inform clinical practice. This work has high significance for medical AI research and potential clinical impact.\n\n**Innovation:** The paper introduces several innovative elements. The MToE module with its joint probability modeling of modalities, experts, and tasks is a novel contribution to medical AI. The conditional mutual information loss specifically designed to capture modality-task dependence represents a creative solution to the identified problem. The framework's ability to provide both sample-level and population-level interpretability of modality contributions is also innovative. The approach goes beyond standard multi-modal fusion by explicitly modeling the dynamic aspects of modality utility across patients and tasks.\n\n**Evaluation:** The evaluation is comprehensive across four diverse medical imaging datasets covering different anatomical regions, modalities, and clinical tasks. The comparison includes multiple baseline categories: medical AI single-task methods, medical AI multi-task methods, MoE-related baselines, and natural domain multi-modal multi-task methods. The authors include thorough ablation studies and multiple forms of analysis including modality utilization, synergy analysis, gradient conflict mitigation, and interpretability. The paper would be strengthened by additional clinical impact analysis (e.g., how performance improvements translate to clinical outcomes) and more detailed computational efficiency metrics.\n\n**Reproducibility:** The paper provides sufficient detail on the method architecture, training procedures, and hyperparameters (128 experts for MToE, 32 for each MSoE, Adam optimizer with lr=1e-4, Œ±=0.05). The datasets used are all public, and the evaluation metrics are standard for the field. The authors mention that additional details are in the appendix, which would be helpful for reproducibility. However, the lack of specific computational resource requirements and inference time metrics reduces reproducibility for resource-constrained settings. The code is not mentioned as being publicly available, which would further enhance reproducibility.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThe paper presents a significant contribution to medical multi-modal learning with strong technical innovation and thorough evaluation across multiple medical imaging benchmarks. The framework addresses clinically important challenges in dynamic modality fusion and modality-task dependence that have been overlooked in previous work. The results are compelling, and the interpretability analysis adds substantial value.\n\nHowever, before acceptance, the authors should address the following key points in revision:\n1. Provide computational efficiency analysis including inference time and memory requirements compared to baselines\n2. Add discussion of clinical impact (how performance gains translate to potential clinical benefits)\n3. Clarify the theoretical justification for the conditional mutual information loss formulation\n4. Address the minor issues regarding visualization clarity and grammatical corrections\n\nThese revisions would strengthen the paper significantly while not requiring new experiments. The work is promising and addresses important challenges in medical AI, making it suitable for TMI after these revisions.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 2,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates a central problem in medical imaging‚Äîhow to model dynamic relationships across multiple imaging modalities and multiple clinical tasks. The authors identify two main deficiencies in prior work: lack of patient-specific dynamic modality fusion and inadequate modeling of modality‚Äìtask dependencies. They propose **M‚Å¥oE (Multi-modal Multi-task Mixture of Experts)**, a framework integrating Modality-Specific MoE (MSoE) and a Modality-Shared Modality-Task MoE (MToE) module. The MToE jointly models modalities, experts, and tasks through conditional mutual information loss, enabling adaptive multimodal fusion. Evaluations on four public datasets (breast cancer screening and retinal disease diagnosis) demonstrate consistent performance gains over state-of-the-art methods in multiple metrics, and the interpretability analysis highlights how modality relevance varies by patient and task. Overall, the paper is clearly written, methodologically solid, and clinically motivated.  \n\n---\n\n**Major Comments**  \n\n1. **Strengths**  \n   - Identifies two clinically important gaps‚Äîdynamic modality fusion and modality‚Äìtask dependence‚Äîand motivates them with realistic clinical examples.  \n   - Proposes a technically sound, mathematically defined framework; the MToE module offers a novel probabilistic modeling of modalities, experts, and tasks.  \n   - Demonstrates strong generalization across four heterogeneous medical imaging datasets, covering diverse modalities and clinical targets.  \n   - The interpretability component is valuable, offering both population- and subject-level insights into modality importance.  \n\n2. **Limitations and Concerns**  \n   - Computational cost is not adequately analyzed. With large numbers of experts (128 in MToE, 32 per MSoE), resource demands may exceed those of baselines; inference time and memory comparisons are missing.  \n   - The link between improved quantitative metrics and potential clinical benefit is unexplored. Quantifying the clinical impact (e.g., incremental detection improvement) would be informative.  \n   - The conditional mutual information loss requires stronger theoretical justification; how exactly it enforces dynamic fusion remains unclear.  \n\n---\n\n**Minor Comments**  \n- Figure‚ÄØ5‚Äôs visualization would benefit from clearer modality‚Äìcolor labeling.  \n- Minor grammatical corrections are needed (e.g., ‚Äúprovides inherent sample and population-level interpretability‚Äù).  \n- Discussion of potential failure conditions (e.g., highly imbalanced modalities or limited data) would enhance completeness.  \n- The reason for sensitivity to parameter Œ±‚ÄØ>‚ÄØ0.1 (Figure‚ÄØ7b) and guidance on its tuning should be elaborated.  \n\n---\n\n**Summary Paragraph**  \nThe study makes a substantial and well-motivated contribution to multi-modal, multi-task medical imaging. Its innovations‚Äîparticularly the joint expert framework and interpretability analysis‚Äîaddress clear gaps in prior research. The evaluation is broad and rigorous, but the paper would benefit from expanded discussion on computational efficiency, theoretical justification of the loss function, and clinical significance of improvements. Reproducibility is generally supported through detailed methodological descriptions and use of public datasets, although reporting of inference cost and code availability could strengthen it further.  \n\n---\n\n**Decision Recommendation:‚ÄØMinor Revision**  \nThe work is technically strong and clinically relevant, requiring only targeted clarifications and additional analyses (computational efficiency, theoretical rationale, and minor language and visualization refinements) prior to acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the problem of dynamically fusing heterogeneous medical‚Äëimage modalities while explicitly modelling task‚Äëdependent modality relations. Existing pipelines often treat modality fusion as a static operation, which can provoke competition between inputs. The authors evaluate their proposal on four public multimodal collections: EMBED, RSNA, and VinDr‚ÄëMammo (mammography) and the GAMMA ophthalmology set. These datasets span breast‚Äëdensity estimation, cancer‚Äërisk prediction, BI‚ÄëRADS categorisation, glaucoma detection, and retinal segmentation. Their M[4]oE architecture couples Modality‚ÄëSpecific Mixture‚Äëof‚ÄëExperts (MSoE) modules that retain modality‚Äëspecific representations with a Modality‚ÄëTask Mixture‚Äëof‚ÄëExperts (MToE) module that learns joint modality‚Äëtask distributions via a conditional mutual‚Äëinformation regulariser. Across all tasks the method reports higher accuracy, AUROC, AUPRC and Dice scores than a suite of baselines. The primary contribution is a multimodal multi‚Äëtask MoE framework that purportedly adapts fusion per sample, captures dynamic modality‚Äëtask dependencies, and yields interpretable modality contributions at both the individual‚Äësample and cohort levels.  \n\n---\n\n## General feedback  \n\n*Significance.* Addressing dynamic modality fusion and modality‚Äëtask dependence is clinically relevant, particularly for workflows that combine full‚Äëfield digital mammography with 2‚ÄëD synthetic views, or fundus photography with OCT.  \n\n*Innovation.* The three‚Äëpart design‚Äî(i) modality‚Äëspecific soft MoE, (ii) a task‚Äëconditioned shared MoE, and (iii) a conditional mutual‚Äëinformation loss that promotes diverse expert‚Äëmodality‚Äëtask assignments‚Äîappears novel within the medical‚Äëimaging literature. The joint probability formulation over modalities, experts, and tasks (Sec.‚ÄØ3.3) differentiates the approach from recent natural‚Äëimage MoE variants such as AdaMV‚ÄëMoE and Fuse‚ÄëMoE.  \n\n*Evaluation.* Results are presented on four heterogeneous datasets covering five tasks, with improvements over strong baselines (MIRAI, AsymMIRAI, EyeMost, etc.) on standard metrics (Table‚ÄØ1). Ablation studies (Table‚ÄØ2) and investigations of expert count and regularisation strength (Fig.‚ÄØ7) support the authors‚Äô design choices. However, the manuscript lacks statistical significance testing, confidence intervals, and any discussion of class imbalance or dataset size effects.  \n\n*Reproducibility.* The code repository is referenced only as a placeholder (‚Äú[in XXX]‚Äù). Training details are confined to optimiser settings and the number of experts; the exact implementation of the conditional MI loss, hyper‚Äëparameter optimisation, data‚Äësplit protocol, and MoE load‚Äëbalancing mechanisms are not described, which impedes replication.  \n\n---\n\n## Specific comments/critiques  \n\n1. **Clarity of the probabilistic formulation.** Section‚ÄØ3.3 contains typographical mistakes and ambiguous symbols (e.g., ‚Äúœï‚ÄØ‚àà‚ÄØR_[m]√ó[n]√ó[p]‚Äù without explicit dimensionality). Precise definitions of the dispatch/combine tensors and a step‚Äëby‚Äëstep derivation of the MI term are required for reproducibility. *(Fig.‚ÄØ2, Sec.‚ÄØ3.3)*  \n\n2. **Baseline comparability.** It is unspecified whether the baselines were retrained using the same backbone, parameter budget, data‚Äëaugmentation pipeline, and optimiser as M[4]oE. Differences in these settings could account for part of the reported gains. Full disclosure of the baseline training procedures is necessary. *(Table‚ÄØ1)*  \n\n3. **Absence of statistical validation.** Table‚ÄØ1 reports only mean values (e.g., 84.0‚ÄØ% for MIRAI vs. 87.6‚ÄØ% for M[4]oE). No confidence intervals, p‚Äëvalues, or bootstrapped estimates accompany these numbers, making it impossible to judge whether the improvements are statistically reliable. *(Table‚ÄØ1)*  \n\n4. **Depth of ablation analysis.** Table‚ÄØ2 examines the presence or absence of MSoE, MToE, and the MI regulariser, but does not test alternative routing strategies (hard versus soft), varying expert‚Äëslot counts per task, or different sparsity levels. Such experiments would clarify the contribution of each component. *(Table‚ÄØ2, Fig.‚ÄØ7)*  \n\n5. **Interpretability assessment.** The modality‚Äëutilisation visualisations (Figs.‚ÄØ3‚Äë5) are purely qualitative. A quantitative validation‚Äîsuch as correlation with clinician‚Äëprovided modality importance scores or established interpretability metrics‚Äîwould substantiate the claim of sample‚Äëlevel interpretability. *(Fig.‚ÄØ5)*  \n\n6. **Robustness to missing modalities.** Although the introduction stresses patient‚Äëspecific modality quality, none of the experiments simulate absent or corrupted modalities. An experiment where one modality is deliberately dropped or perturbed would demonstrate the method‚Äôs resilience in realistic clinical scenarios. *(Dataset description)*  \n\n7. **External clinical validation.** All reported experiments rely on publicly available datasets. Evaluation on an independent clinical cohort would provide stronger evidence of generalisation and real‚Äëworld applicability. *(Dataset section)*  \n\n8. **MoE training stability.** The MToE employs 128 experts; literature on large MoE models frequently observes expert collapse or load‚Äëimbalance. The manuscript does not discuss any balancing loss, utilisation statistics, or measures taken to ensure stable training. *(Fig.‚ÄØ7)*  \n\n9. **Code availability and documentation.** The repository link is a placeholder, preventing reviewers from inspecting the implementation. Providing a functional codebase, complete training scripts, and a hyper‚Äëparameter log is essential for reproducibility. *(Abstract, Acknowledgements)*  \n\n---\n\n## A suggested decision  \n\n**Major Revision**  \n\nThe work introduces an interesting approach to dynamic multimodal fusion, yet the current manuscript lacks essential baseline transparency, statistical validation, and thorough ablation. Substantial revisions are required before the paper can be considered for acceptance.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 3,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai",
          "comment": "The major comments primarily focus on textual analysis, lacking analysis of tables and figures."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of dynamically fusing heterogeneous medical‚Äëimaging modalities while accounting for task‚Äëspecific relationships. The proposed M[4]oE framework integrates Modality‚ÄëSpecific Mixture‚Äëof‚ÄëExperts (MSoE) modules that learn modality‚Äëspecific features with a Modality‚ÄëTask Mixture‚Äëof‚ÄëExperts (MToE) module guided by a conditional mutual‚Äëinformation regularizer. The approach is assessed across four public multimodal datasets covering five distinct clinical tasks, consistently outperforming strong baselines. The paper presents a coherent motivation and clear experimental scope but omits several methodological details that affect transparency and reproducibility.  \n\n**Major Comments**  \n1. **Probabilistic formulation clarity:** Section‚ÄØ3.3 contains typographical errors and ambiguous notation (e.g., undefined tensor dimensions). A clearer mathematical derivation of the conditional MI term and dispatch/combine operations is needed.  \n2. **Baseline comparability:** It is unclear whether baseline models were retrained using identical backbones, data‚Äëaugmentations, and optimizers. Inconsistent settings could influence the reported gains; explicit disclosure of baseline training conditions is necessary.  \n3. **Lack of statistical validation:** Reported performance metrics include only means without confidence intervals or statistical testing, limiting assessment of significance.  \n4. **Limited ablation analysis:** The current study toggles only the presence of key components but does not examine alternative routing strategies, expert‚Äëslot configurations, or sparsity levels.  \n5. **Interpretability evaluation:** Qualitative modality‚Äëutilization visualizations lack quantitative corroboration against independent or clinical measures of modality relevance.  \n6. **Robustness to missing data:** No experiments address scenarios with absent or degraded modalities, despite motivation involving variable modality quality.  \n7. **External clinical validation:** All evaluations rely on public datasets; testing on an independent clinical cohort would better demonstrate generalizability.  \n8. **Training stability:** The manuscript provides no discussion of expert utilization statistics or balancing techniques to prevent expert collapse during training.  \n9. **Reproducibility and code availability:** The referenced repository is placeholder‚Äëonly. Full code, configuration files, and implementation details of the conditional MI loss and MoE mechanisms are required for replication.  \n\n**Minor Comments**  \n- Typographical and symbol inconsistencies in Section‚ÄØ3.3 should be corrected.  \n- Table and figure captions could better describe experimental setups and parameter settings.  \n- Clarify dimensionality symbols and index notations for tensors in mathematical sections.  \n\n**Summary Paragraph**  \nThis work presents a novel multimodal, multi‚Äëtask mixture‚Äëof‚Äëexperts architecture with promising empirical results and a conceptually interesting formulation addressing dynamic fusion. Major strengths lie in its ambition to model modality‚Äëtask dependencies and strong baseline comparisons. However, the study falls short on reproducibility, statistical rigor, and external validation. Significant clarifications on mathematical definitions, baseline training consistency, and code release are required to substantiate its claims.  \n\n**Decision Recommendation**  \n**Major Revision.** The approach is technically appealing but demands additional methodological transparency, statistical validation, and experimental breadth before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Liyue Shen",
      "Luning Wang",
      "Zhengxu Tang",
      "Zitao Shuai",
      "Chenwei Wu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_59d8664cbd1f44ea7da5f36cba0de2bec3d71759.pdf",
    "remote_url": "https://openreview.net/pdf/59d8664cbd1f44ea7da5f36cba0de2bec3d71759.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Synthesizing Realistic fMRI: A Physiological Dynamics-Driven Hierarchical Diffusion Model for Efficient fMRI Acquisition",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "learning on time series and dynamical systems"
    ],
    "keywords": [
      "Time Series",
      "Diffusion"
    ],
    "abstract": "Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications. While generative models such as diffusion models can synthesize fMRI signals to alleviate these issues, they often underperform due to neglecting the brain's complex structural and dynamic properties.\nTo address these limitations, we propose the Physiological Dynamics-Driven Hierarchical Diffusion Model, a novel framework integrating two key brain physiological properties into the diffusion process: brain hierarchical regional interactions and multifractal dynamics. \nTo model complex interactions among brain regions, we construct hypergraphs based on the prior knowledge of brain functional parcellation reflected by resting-state functional connectivity (rsFC). This enables the aggregation of fMRI signals across multiple scales and generates hierarchical signals. \nAdditionally, by incorporating the prediction of two key dynamics properties of fMRI‚Äîthe multifractal spectrum and generalized Hurst exponent‚Äîour framework effectively guides the diffusion process, ensuring the preservation of the scale-invariant characteristics inherent in real fMRI data.\nOur framework employs progressive diffusion generation, with signals representing broader brain region information conditioning those that capture localized details, and unifies multiple inputs during denoising for balanced integration.\nExperiments demonstrate that our model generates physiologically realistic fMRI signals, potentially reducing acquisition time and enhancing data quality, benefiting clinical diagnostics and machine learning in neuroscience.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nIn this paper, the authors proposed a novel framework named the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) for fMRI analytics. The PDH-Diffusion framework integrates two essential brain physiological properties, hierarchical regional interactions and multifractal dynamics, into the diffusion process. The primary goal is to improve diffusion models‚Äô capability to generate realistic fMRI time series signals by accurately capturing these physiological characteristics.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nOverall, the major strength of this work lies in its novelty. The authors have developed an innovative framework that captures complex interdependencies and multifractal dynamics within synthetic fMRI signals.\n\nSpecifically, their contribution includes integrating three key components into the diffusion process: a hypergraph-based signal generator, a dynamics properties guiding module, and a cross-brain region progressive diffusion model. This integration enhances the realism of the generated signals. The authors provide a robust theoretical foundation for their methods and perform extensive quantitative analysis, demonstrating the framework‚Äôs accuracy and effectiveness in time series forecasting. The paper is well-organized and includes relevant background information. Results from the proposed method outperform multiple peer models in time-series forecasting and diffusion models, as evidenced by superior MAE, MAPE, and RMSE scores, highlighting the model‚Äôs effectiveness.\n\n### Weaknesses\n\nThe reviewers have multiple concerns about the framework and potential impact in this work. \n\n1). The confusion about physiological fMRI. Usually, fMRI are categorized into resting-state and task-based fMRI. The resting-state fMRI is commonly scanned without specific stimulus, whereas task-based fMRI is acquired based on external stimulus, such as 7 tasks in HCP. Is physiological fMRI is either resting-state or task-based signal? The authors do not clarify the concept even in Introdcution section.\n\n2). Limited motivation and impact. In Abstract, the authors mentioned \"Functional magnetic resonance imaging (fMRI) is essential for mapping brain activity but faces challenges like lengthy acquisition time and sensitivity to patient movement, limiting its clinical and machine learning applications.\" It seems that the authros' work can advance the fMRI for clincial application, but the authors do not generate some neurological or psychiatric fMRI to validate. From reviewers' perspective, using the innovative PDH-Diffusion model, it can assist physician to provide lengthy fMRI signal which will denfinitely reduce the inconvenience of patients. Only generating healthy fMRI can impair the motivation and impact of this work.\n\n3). Lacking of qualitative comparison. The authors have provided an extensive quantitative validation of PDH-Diffusion model with other peer methods using MAE, MAPE, and RMSE. Unfortunately, the autors do not provide any qualitative results, such as Functional Connectivity Map or Brain Connectivity Maps, of PDH-Diffusion. That is, although averaging metrics such as MAE, RMSE, MAPE across 10 runs may demonstrate robustness, these metrics  cannot fully capture the quality or realism of the synthesized fMRI signals. Notably, the qualitative results is also vital in clinics, since these results showcase which brain regions are severly impaired by neurological disorders.  Importantly, there is no visual representation given of the generated fMRI signal, which would be valuable for assessing their plausibility. \n\n4). Several technical issues. The variance schedule (parameters $\\alpha_n$ and $\\beta_n$) in the diffusion process may not be fully optimized for different regions or scales, potentially leading to inappropriate noise levels in certain hierarchical levels. This could result in over-smoothing or overfitting at certain levels. Additionally, conditioning on historical data could lead to overfitting if the model becomes too dependent on past values, especially if the training data does not represent the full spectrum of brain dynamics. Without explicit mitigation measures, such as adaptive variance schedules or regularization techniques, these issues may limit the model‚Äôs ability to generalize to new or varied patterns, impacting its robustness and effectiveness.\n\n5). Multiple typographical mistakes. In Section 4.3 (ABLATION ANALYSIS) there are many typographical errors. The term ‚Äúshare radio‚Äù is used instead of ‚Äòshare ratio‚Äô in ‚ÄúThe influence of share radio‚Äù this section. Similar typographical errors appear in the caption and text of Figure 3(b), as well as in Section 3.4. These typographical mistakes impair the redability of this paper.\n\n### Questions\n\nThe reviewers have raised several questions regarding the weaknesses of this work:\n\n1). Qualitative Comparisons: The authors are strongly encouraged to provide qualitative comparisons between the synthesized and real fMRI signals to allow for a qualitative assessment of the model‚Äôs performance.\n\n2). Optimal Range of Brain Regions: The results suggest an ‚Äúoptimal‚Äù range of brain regions that enhances performance. How is this range determined, and is it manually set? Reviewers are concerned about the reliance on manual design for determining this optimal range.\n\n3). Validation of Multifractal Properties: How do the authors validate that the generated signals preserve multifractal properties?\n\n4). Risk of Overfitting: Do the variance schedule parameters ($\\alpha_n$ and $\\beta_n$) and the historical data used in training the PDH-Diffusion model lead to overfitting?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the *Physiological Dynamics‚ÄëDriven Hierarchical Diffusion Model (PDH‚ÄëDiffusion)*, a framework designed for fMRI analysis that integrates hierarchical regional interactions and multifractal dynamics into the diffusion process. Its stated goal is to enhance the realism of generated fMRI time‚Äëseries signals by faithfully capturing key physiological characteristics. The work is conceptually novel, clearly organized, and supported by extensive quantitative evaluation. However, several conceptual, methodological, and presentation issues limit its overall impact and clarity.  \n\n**Major Comments**  \n1. **Definition and Context of Physiological fMRI** ‚Äì The term ‚Äúphysiological fMRI‚Äù is not clearly defined. It remains unclear whether it corresponds to resting‚Äëstate or task‚Äëbased fMRI, and this ambiguity persists even in the Introduction. Clarification is needed to situate the work appropriately within established fMRI categories.  \n2. **Motivation and Practical Impact** ‚Äì Although the Abstract highlights clinical relevance (e.g., reducing acquisition time and motion sensitivity), the experiments use only synthetic or healthy fMRI data. Without validation on neurological or psychiatric datasets, the clinical applicability of PDH‚ÄëDiffusion is uncertain, reducing its motivation and broader impact.  \n3. **Lack of Qualitative Comparison** ‚Äì The study provides quantitative metrics (MAE, MAPE, RMSE) but omits qualitative results such as functional or brain connectivity maps. These visualizations are important for assessing the physiological plausibility of generated signals and their clinical interpretability.  \n4. **Technical Concerns** ‚Äì The diffusion variance schedule parameters (Œ±‚Çô, Œ≤‚Çô) may not be optimized across hierarchical levels, potentially causing over‚Äësmoothing or overfitting. Strong dependence on historical data could also lead to overfitting if training samples are not sufficiently diverse. The paper lacks discussion of adaptive variance or regularization strategies to mitigate these issues.  \n5. **Clarity and Terminology Issues** ‚Äì The paper contains multiple typographical errors, notably ‚Äúshare radio‚Äù instead of ‚Äúshare ratio‚Äù in Section‚ÄØ4.3 and Figure‚ÄØ3(b), which detract from readability.  \n\n**Minor Comments**  \n- Provide qualitative visualizations comparing synthesized and real fMRI signals.  \n- Explain how the ‚Äúoptimal range‚Äù of brain regions is determined and whether it is manually set.  \n- Clarify how multifractal properties of generated signals are validated.  \n- Describe safeguards against overfitting related to variance scheduling.  \n\n**Summary Paragraph**  \nOverall, the manuscript offers an original and theoretically grounded framework that effectively models hierarchical and multifractal properties in fMRI signal generation. The quantitative performance is strong, yet conceptual clarity and empirical breadth are limited. The lack of qualitative analysis, unclear terminology, and potential overfitting concerns restrict confidence in the model‚Äôs generalizability and practical impact. Addressing these issues would substantially improve the paper‚Äôs robustness and interpretability.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nIn this study, the authors proposed a novel algorithm for synthesizing realistic functional MRI (fMRI) via physiological dynamics-driven hierarchical diffusion model. Then, the authors validated the feasibility of synthesized fMRI data by comparing it to other generative mechanisms. The scientific merit of this work mainly comes from the conceptual advances in their proposed algorithm. It is indeed challenging to synthesize realistic fMRI data while preserving unique aspects of the brain system. The authors combined three different modules, each serving different roles, to synthesize brain dynamics with preserved network-like structure and fractal components. Given results of the extensive validation experiment on the large fMRI cohort, the validity of the proposed algorithm is clear; yet, the scientific significance of this work is somewhat limited as there was no experiment demonstrating the practical usefulness of the proposed algorithm.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe major originality of this study comes from its conceptual advances embedded in the proposed algorithm. It is indeed challenging to synthesize realistic fMRI data while preserving unique aspects of the brain system. The quality and clarity of the models in the main text are reasonably strong as well.\n\n### Weaknesses\n\nThe major concern comes from the unclear significance of this work. As the authors argued in the Introduction, acquisition of fMRI is expensive. Thus, synthesizing fMRI signal can be tempting. Although expensive, real fMRI data reflects unique information of individuals. This study, however, was not able to demonstrate the synthesized fMRI still convey unique information of subjects. Slight improvement in forecasting future timepoints of fMRI signal does not suggest the significance or practical usefulness of the model. This works needs additional analysis or applications highlighting the unique advantage of the synthesized fMRI data from the proposed model.\n\n### Questions\n\n1. In table 1, does ‚ÄúT_Pred=32 or 64‚Äù mean predicting 32 or 64 time points of fMRI data?\n2. If my speculation in the former question is right, it is counter-intuitive that forecasting more time points (96 vs. 32) did not lead to an increase in errors. Please clarify this. \n3. According to Fig 1, it looks like the resolution of synthesized data is bounded to the level of ROI. It is more desirable to synthesize fMRI dynamics at the level of vertex. Is the proposed method able to be applied at the vertex level as well?     \n4. In line 451, please double check the definition of MAE.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a novel algorithm designed to synthesize realistic functional MRI (fMRI) data using a physiological dynamics‚Äìdriven hierarchical diffusion model. The approach integrates three complementary modules to generate brain dynamics that maintain network-like structures and fractal properties. The authors validate their method through extensive experiments on a large fMRI cohort and compare it with existing generative mechanisms. While the algorithm appears technically sound and its feasibility well demonstrated, the overall scientific significance is seen as limited due to the lack of experiments illustrating its practical utility.\n\n**Major Comments**  \n1. **Scientific Significance**: The main limitation lies in the unclear practical impact of the proposed model. Although the algorithm successfully synthesizes realistic fMRI signals, the study does not show that the generated data preserve subject-specific information. Without demonstrating that synthetic fMRI can capture unique individual features or enable new applications, its usefulness remains uncertain.  \n2. **Evaluation Scope**: The improvement reported in predicting future time points of fMRI signals, while noticeable, is insufficient to establish the significance of the model. Additional analyses or real-world applications are needed to substantiate the advantages of the synthesized data.  \n3. **Clarifications Needed in Tables and Figures**:  \n   - In Table‚ÄØ1, the term ‚ÄúT_Pred‚ÄØ=‚ÄØ32 or‚ÄØ64‚Äù should be explained more clearly‚Äîdoes it correspond to forecasting that number of time points?  \n   - If so, the observation that predicting more time points (e.g., 96 vs.‚ÄØ32) does not increase errors appears counterintuitive; further clarification is warranted.  \n   - Figure‚ÄØ1 suggests that synthesized data are limited to the ROI level. Please discuss whether the method could be extended to vertex-level synthesis.  \n   - In line‚ÄØ451, the definition of MAE should be verified for correctness.\n\n**Minor Comments**  \n- Presentation and clarity are generally acceptable, though small improvements in definitions and figure explanations would assist readability.  \n- Acronyms and parameter definitions should be clearly stated at first use.\n\n**Summary Paragraph**  \nThis work presents a conceptually innovative approach to generating realistic fMRI data through a hierarchical diffusion model, supported by technically solid validation. The primary strength is methodological originality and coherent model design. However, its current contribution is limited by the absence of evidence linking synthesized data to meaningful biological or subject-level insights. Clarifications to tables, figures, and metrics would strengthen the presentation.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The manuscript demonstrates methodological promise but requires additional analyses and clarification to establish its significance and practical relevance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper proposes a novel approach for generating realistic fMRI data using diffusion models, specifically designed to account for regional interactions and spectral features of the brain. The model captures regional connectivity in a hierarchical structure, where fine-scale signals are conditioned on larger-scale signals. To learn spectral features, it incorporates loss functions that capture fractal characteristics. Results demonstrate improved performance over existing time-series forecasting and diffusion-based models. Additionally, ablation studies validate the effectiveness of each model component.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The proposed method, which captures connectivity and spectral features, is a novel approach. \n2. The method is rigorously validated using multiple benchmarks and ablation studies.\n\n### Weaknesses\n\n1. The paper has limited reproducibility due to missing details about data preparation and experimental setup. Additional information is needed on the dataset used, including whether it involved resting-state or task-based fMRI, whether subjects were healthy or under specific conditions, and the rationale for selecting regions of interest (ROI), which were reduced from 268 to 82. Clarification on data split (e.g., train/test division, sample counts) is also required. If the codebase will not be provided, the paper should include a detailed description of the network architecture (such as layer specifications and activation functions) and the training setup for benchmark methods in an appendix.\n2. The practical implications, particularly the clinical applications of the proposed method, are somewhat unclear and could benefit from further exploration and discussion.\n\n### Questions\n\n1. What is the sampling frequency for the fMRI?\n2. It would be beneficial to analyze the reconstructed signal to determine if the observed patterns align with expectations. Calculating the spectrum and fractal characteristics would provide an important validation of the model‚Äôs effectiveness. Additionally, some neurological features can also be checked. For instance, if the data is from resting-state fMRI, does it reveal the default mode network?  \n3. Line 465 typo 69->96\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a novel approach for generating realistic fMRI data using diffusion models that explicitly account for regional interactions and spectral characteristics of brain activity. The proposed hierarchical structure enables fine-scale signals to be conditioned on broader regional dynamics, while customized loss functions capture fractal features in the spectral domain. Experimental evaluations indicate that the method outperforms existing time-series forecasting and diffusion-based baselines, with ablation analyses verifying the contribution of individual components. Overall, the paper is clearly written and presents a technically sound contribution.  \n\n**Major Comments**  \n1. **Reproducibility and Experimental Details:** The study lacks sufficient detail to ensure reproducibility. More comprehensive information is needed on data preparation, including whether the dataset involves resting-state or task-based fMRI, the nature of the participant cohort (healthy or clinical), and the rationale for reducing regions of interest from 268 to 82. Details on the data split (training/testing, sample counts) should also be provided. If the code is not released, a detailed appendix describing the network architecture (layers, activation functions) and training procedures for both the proposed and benchmark models would be necessary.  \n2. **Practical and Clinical Relevance:** The manuscript would benefit from a clearer articulation of the real-world or clinical implications of the proposed model. The potential applications in neuroscientific or medical contexts remain underdeveloped and should be discussed more explicitly.  \n\n**Minor Comments**  \n- Clarify the fMRI sampling frequency used in experiments.  \n- It would strengthen validation to analyze reconstructed signals for alignment with expected neural patterns, such as comparing spectral or fractal characteristics and checking for recognizable features (e.g., presence of the default mode network in resting-state data).  \n- Correct typographical error on line 465 (‚Äú69‚Äù should be ‚Äú96‚Äù).  \n\n**Summary Paragraph**  \nThe paper introduces an innovative and well-validated method for simulating fMRI data, demonstrating both conceptual novelty and quantitative improvement over existing baselines. The primary limitations concern incomplete experimental transparency and limited discussion of potential applications. Addressing these would materially strengthen the manuscript‚Äôs impact and reproducibility.  \n\n**Decision Recommendation**  \n**Minor Revision** ‚Äì The work is strong and novel but requires additional methodological detail and clarification of implications before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**PHASE 1 ‚Äì Analytical and Scope Audit**\n\n**1. Scope & Relevance to IEEE TMI**  \n‚úÖ The paper clearly targets methodological innovation in medical imaging, introducing a new generative modeling framework for *fMRI signal synthesis and forecasting*. It intertwines imaging physics‚Äìinspired priors (functional connectivity and fractal dynamics) with modern diffusion-based generative modeling. This lies squarely within TMI‚Äôs interest in imaging reconstruction, signal modeling, and quantitative imaging theory.  \n\n**2. Novelty & Contribution Level**  \nThe ‚ÄúPhysiological Dynamics‚ÄìDriven Hierarchical Diffusion Model (PDH‚ÄëDiffusion)‚Äù is an ambitious method integrating multiple physiological priors into a generative diffusion model. There is meaningful novelty in:\n- Coupling functional connectivity‚Äìderived *hypergraph structures* with diffusion processes.  \n- Embedding *multifractal dynamics* (generalized Hurst exponent, multifractal spectrum) as explicit conditioning signals.  \n- Using a *progressive multi‚Äëscale diffusion chain* guided by brain‚Äëregion hierarchies.  \n\nThe conceptual integration surpasses standard diffusion‚Äëtime‚Äëseries applications, though the core diffusion machinery is known. Methodological contribution is thus moderate‚Äëto‚Äëstrong; the main innovation lies in the *physiological conditioning design*.  \n\n**3. Technical and Experimental Rigor**  \nThe paper is technically dense and mathematically coherent. Derivations of fractal properties are thorough. Experimental validation uses the HCP dataset; the training pipeline and baselines are well‚Äëchosen. However, shortcomings include:\n- Limited discussion of how ‚Äúphysiologically realistic‚Äù generation is quantitatively validated beyond forecasting metrics (MAE/RMSE/MAPE).  \n- Absence of uncertainty estimates, reproducibility statistics, or clinical interpretability analysis.  \n- Lack of details about subject variability, test‚Äìretest reliability, or whether generated fMRI preserve spatial‚Äìtemporal physiological realism beyond numeric metrics.  \n\n**4. Clarity and Presentation**  \nGenerally well structured, albeit verbose. Figures are helpful but the writing could be tightened for readability. Derivations may be too detailed for TMI‚Äôs page format; clearer schematic flowcharts of the pipeline and loss functions would help.  \n\n**5. Ethical and Reproducibility Compliance**  \nThe dataset (HCP) is public and anonymized; code availability is promised via GitHub. No direct patient study occurs‚Äîminimal ethical concern observed.\n\n\n---\n\n**PHASE 2 ‚Äì IEEE TMI Review Report**\n\n**1. Summary (in reviewer‚Äôs words)**  \nThe authors propose *PDH‚ÄëDiffusion*, a hierarchical diffusion model that synthesizes realistic fMRI time‚Äëseries. The model fuses three components: (i) a hypergraph‚Äëbased hierarchical signal generator modeling multi‚Äëscale functional connectivity among regions; (ii) a dynamics‚Äëguiding module that enforces multifractal temporal behavior via predicted spectra and Hurst exponents; and (iii) a cross‚Äëregion progressive diffusion process where coarse‚Äëscale regional trends guide fine‚Äëscale generation. Evaluated on the Human Connectome Project dataset, PDH‚ÄëDiffusion reportedly surpasses transformer and diffusion baselines in MAE/RMSE/MAPE metrics.\n\n**2. Strengths**\n- Strong methodological concept embedding *neurophysiological priors* into machine learning generation.  \n- Mathematically consistent link between fractal brain dynamics and diffusion conditioning.  \n- Clear ablation study validating the role of each module.  \n- Open‚Äësource intent and usage of a widely recognized dataset.  \n\n**3. Weaknesses**\n- Evaluation metrics focus on generic time‚Äëseries accuracy, not physiological or spatial realism; missing evaluation of dynamic functional connectivity preservation.  \n- Clinical or acquisition‚Äëtime reduction claims are speculative without empirical evidence.  \n- Hyperparameters and model reproducibility details (e.g., initialization, computational cost) are sparse.  \n- Writing density and formula presentation may impede accessibility for imaging readers.  \n\n**4. Major Comments**\n1. **Validation of physiological realism:** Move beyond numerical forecasting errors; include comparisons of fractal spectra, power‚Äëlaw slopes, or rsFC statistics between generated and real signals to support the ‚Äúphysiologically realistic‚Äù claim.  \n2. **Empirical link to acquisition efficiency:** If the model intends to reduce fMRI acquisition time, demonstrate reconstruction/denoising from undersampled or partial scans rather than only forecasting.  \n3. **Ablation breadth:** Provide qualitative visualization (temporal correlations, autocorrelation, PSD comparisons) to reveal how each module preserves dynamic structure.  \n4. **Reproducibility:** Clarify computational cost per subject, diffusion steps, total parameter count, and inference time.  \n5. **Manuscript condensation:** The theoretical exposition on fractal dynamics (Section‚ÄØ2) is excessive; condensing or relocating into supplementary material would improve readability.  \n\n**5. Minor Comments**\n- Figures‚ÄØ1‚Äì2: increase font size; delineate modules with clearer color coding.  \n- Verify consistency in notation (e.g., \\(R\\), \\(r\\), \\(œÉ_r\\)) across sections.  \n- Reference formatting shows unescaped LaTeX; conform to IEEE style.  \n- Typographical errors (‚ÄúA P HYSIOLOGI -CAL‚Äù, ‚Äúb y r esting-state‚Äù) should be corrected.  \n- Future directions could mention potential adaptation to BOLD reconstruction from partially sampled k‚Äëspace.  \n\n**6. Overall Recommendation**  \n**Major Revision.**  \nThe work fits TMI‚Äôs methodological scope and demonstrates interesting novelty by fusing physiological priors with diffusion modeling. However, the current validation and exposition require significant strengthening‚Äîparticularly quantitative physiological evaluation, clarity of contributions, and evidence for claimed acquisition efficiency‚Äîto meet TMI standards.\n\n**7. Confidence Level:** 4/5 (high confidence in assessment; moderate familiarity with diffusion and fMRI generative modeling).",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a generative modeling framework, the *Physiological Dynamics‚ÄìDriven Hierarchical Diffusion Model (PDH‚ÄëDiffusion)*, designed to synthesize and forecast realistic fMRI signals. The method combines neurophysiological priors‚Äîincluding functional connectivity and multifractal dynamics‚Äîwith modern diffusion-based generation. The work is technically solid, mathematically coherent, and methodologically aligned with imaging signal modeling and reconstruction. While the approach is ambitious and conceptually novel in its integration of physiological conditioning, certain validation and presentation aspects require further strengthening to substantiate the claimed physiological realism and efficiency.\n\n---\n\n**Major Comments**  \n1. **Validation of Physiological Realism:** Current evaluation relies on standard forecasting metrics (MAE/RMSE/MAPE). The authors should include comparisons of fractal spectra, power‚Äëlaw slopes, or resting‚Äëstate connectivity statistics between generated and real signals to substantiate physiological plausibility.  \n2. **Empirical Support for Acquisition Efficiency:** The discussion of potential scan‚Äëtime reduction remains speculative. Demonstrations on reconstruction or denoising from undersampled data would better justify this claim.  \n3. **Ablation and Visualization:** Although ablation studies are included, additional qualitative analyses (e.g., autocorrelation patterns, power spectral density comparisons, temporal correlation structure) would clarify how each module preserves dynamic brain‚Äëlike patterns.  \n4. **Reproducibility and Computational Detail:** Provide details on model size, parameter counts, diffusion steps, computational cost, and inference time per subject to improve transparency and reproducibility.  \n5. **Manuscript Readability:** The theoretical exposition on fractal dynamics is lengthy; condensing or moving derivations to supplementary materials could enhance clarity without loss of rigor.\n\n---\n\n**Minor Comments**  \n- Enlarge fonts and clarify color coding in key figures to improve visual readability.  \n- Ensure consistent notation (e.g., \\(R\\), \\(r\\), \\(œÉ_r\\)) throughout.  \n- Correct typographical errors and LaTeX formatting issues in references.  \n- Streamline dense mathematical sections and consider schematic flowcharts summarizing the pipeline.  \n- Mention potential extensions, such as adaptation to reconstruction from partially sampled data, in future work.\n\n---\n\n**Summary Paragraph**  \nThe study offers a creative fusion of neurophysiological modeling and generative diffusion methods, demonstrating moderate‚Äëto‚Äëstrong novelty and solid mathematical grounding. Experiments on the Human Connectome Project dataset show improvements over existing baselines, but the physiological interpretation and empirical validation remain limited to generic time‚Äëseries metrics. The primary strengths lie in the physiological conditioning design, open‚Äësource intent, and use of a public dataset. Weaknesses include incomplete evaluation of biological realism, limited reproducibility information, and verbose exposition. Addressing these issues would considerably strengthen the work‚Äôs impact and clarity.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial revisions are needed to enhance empirical validation of physiological realism, reproducibility transparency, and overall presentation.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper addresses fMRI acquisition challenges including lengthy scan times and motion sensitivity by proposing the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) for generating realistic fMRI signals. The method integrates two key brain physiological properties: hierarchical regional interactions modeled through hypergraphs based on resting-state functional connectivity, and multifractal dynamics captured via generalized Hurst exponents and multifractal spectra. The framework comprises three components: a Hypergraph-based Hierarchical Signals Generator that aggregates fMRI signals across multiple spatial scales, a Dynamics Properties Guiding Module that predicts multifractal characteristics to guide diffusion generation, and a Cross-brain Region Guiding Progressive Diffusion Model that uses broader regional signals to condition localized signal generation. Experiments on the Human Connectome Project dataset demonstrate superior performance compared to time series forecasting methods and existing diffusion models across prediction horizons of 32, 64, and 96 timesteps, with ablation studies confirming the contribution of each proposed component.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Equation 1 defines scale-free property with Œ≤‚â•0 but subsequent derivations assume specific relationships without clear justification (Page 3, Section 2.2)\n  - The transition from Equation 2 to Equation 3 lacks mathematical rigor in establishing the self-similarity property, particularly the equality of finite dimensional distributions notation (Page 3)\n  - Equation 9 combines MSE and KL divergence losses without theoretical justification for this specific combination or guidance on relative weighting (Page 7)\n  - Notation inconsistencies appear throughout, such as using both X(t) and Y(t) for time series without clear distinction of their roles (Pages 3-4)\n\n‚Ä¢ **Insufficient experimental validation and baseline comparisons**\n  - Limited to single dataset (HCP) evaluation without cross-dataset validation to demonstrate generalizability (Page 8, Section 4.1)\n  - Missing comparisons with recent fMRI-specific generation methods, focusing only on general time series forecasting and two diffusion models (Table 1, Page 9)\n  - No statistical significance testing reported for performance differences, making it difficult to assess the reliability of claimed improvements (Table 1)\n  - Evaluation metrics (MAE, RMSE, MAPE) are standard forecasting measures but lack fMRI-specific quality assessments such as functional connectivity preservation or physiological plausibility measures (Page 8)\n\n‚Ä¢ **Limited technical novelty and methodological concerns**\n  - Hypergraph construction relies on standard k-hop neighbor methods without novel contributions to brain connectivity modeling (Page 6, Section 3.2)\n  - The multifractal analysis uses established MFDFA techniques without methodological advances (Page 6, Section 3.3)\n  - Progressive diffusion concept is borrowed from existing multi-granularity approaches with limited adaptation to brain-specific requirements (Page 7, Section 3.4)\n  - The integration of multiple conditioning inputs lacks theoretical foundation for why this specific combination is optimal for fMRI generation (Figure 1, Page 5)\n\n‚Ä¢ **Incomplete evaluation of physiological realism**\n  - No direct validation that generated signals preserve actual brain physiological properties beyond multifractal characteristics (Abstract claims realistic signals but provides no physiological validation)\n  - Missing analysis of whether functional connectivity patterns are maintained in generated signals compared to real fMRI data (Claims to model brain regional interactions but doesn't validate preservation)\n  - Lack of clinical or neuroscientific expert evaluation of generated signal quality and biological plausibility (Abstract mentions clinical applications but no clinical validation)\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical rigor and presentation**\n  - Provide complete mathematical derivations for the transition from scale-free properties to self-similarity, particularly justifying the finite dimensional distribution equality in Equation 3\n  - Include theoretical analysis or empirical justification for combining MSE and KL divergence in the fractal loss function, with guidance on hyperparameter selection\n  - Standardize notation throughout the manuscript, clearly distinguishing between different time series representations and their roles\n  - Add mathematical proof or strong empirical evidence for why the proposed conditioning combination is theoretically sound\n\n‚Ä¢ **Expand experimental validation comprehensively**\n  - Validate the approach on additional fMRI datasets beyond HCP to demonstrate cross-dataset generalizability and robustness\n  - Include comparisons with recent fMRI-specific generation methods and more comprehensive diffusion-based time series models\n  - Conduct statistical significance testing using appropriate multiple comparison corrections to establish reliability of performance claims\n  - Incorporate fMRI-specific evaluation metrics such as functional connectivity preservation, network topology maintenance, and physiological signal characteristics\n\n‚Ä¢ **Enhance technical contributions and methodology**\n  - Develop novel hypergraph construction methods specifically tailored to brain connectivity patterns rather than relying on standard k-hop approaches\n  - Propose methodological advances in multifractal analysis for fMRI applications beyond standard MFDFA implementation\n  - Design brain-specific progressive diffusion mechanisms that leverage unique properties of neural signal hierarchies\n  - Provide theoretical framework explaining why the proposed multi-condition integration is optimal for brain signal generation\n\n‚Ä¢ **Validate physiological realism thoroughly**\n  - Implement comprehensive validation of generated signals' physiological properties including spectral characteristics, connectivity patterns, and temporal dynamics\n  - Conduct quantitative analysis of functional connectivity matrix preservation and network topology maintenance in generated versus real fMRI data\n  - Collaborate with neuroscientists or clinicians to provide expert evaluation of generated signal quality and biological plausibility for claimed clinical applications",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) to address key fMRI acquisition challenges such as long scan durations and motion sensitivity. The model integrates two physiological properties‚Äîhierarchical inter-regional interactions via hypergraphs and multifractal dynamics captured by generalized Hurst exponents‚Äîto generate realistic fMRI signals. The framework includes hypergraph-based hierarchical signal generation, a dynamics-guided diffusion module, and a cross-region progressive conditioning mechanism. While the paper is well-structured and demonstrates improved performance on the Human Connectome Project dataset compared to existing time-series forecasting and diffusion models, several conceptual, mathematical, and validation issues limit its current strength and generalizability.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity** ‚Äì Equations contain inconsistencies and lack rigorous justification, particularly in defining scale-free and self-similarity properties. The combination of MSE and KL losses is insufficiently motivated, and notational ambiguity (e.g., between \\(X(t)\\) and \\(Y(t)\\)) obscures interpretation.  \n2. **Experimental evaluation** ‚Äì Validation relies solely on the HCP dataset, with no cross-dataset testing. Comparisons omit recent fMRI-specific generation methods, and no statistical significance testing is provided. Evaluation metrics focus on forecasting accuracy but exclude fMRI-related measures such as connectivity preservation or physiological plausibility.  \n3. **Technical novelty** ‚Äì Core components adopt standard methods (e.g., k-hop hypergraph construction, MFDFA) without clear innovation. The progressive diffusion design follows prior multi-scale diffusion approaches and lacks theoretical rationale for its adaptation to fMRI.  \n4. **Physiological realism** ‚Äì The claim of generating realistic physiological signals is unsubstantiated. There is no evidence that the generated data preserve functional connectivity or other brain-relevant properties, nor any expert evaluation supporting clinical applicability.  \n\n**Minor Comments**  \n- Improve notation consistency and clearly define all variables.  \n- Provide theoretical or empirical justification for the chosen loss design and conditioning structure.  \n- Clarify derivations linking scale-free measures to self-similarity notation.  \n\n**Summary Paragraph**  \nThis work presents a conceptually interesting framework for physiologically informed fMRI signal generation but currently lacks the mathematical rigor, technical novelty, and validation depth expected for strong empirical claims. The approach could be strengthened by formal derivations, expanded datasets, comprehensive statistical and physiological validation, and clearer theoretical motivation for its design choices.  \n\n**Decision Recommendation**  \n**Major Revision.** The concept is promising, but substantial mathematical clarification, methodological justification, and empirical validation are required before the paper can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript proposes a novel framework, the Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion), for generating realistic fMRI time series signals. The framework integrates two key aspects of brain fMRI signals: hierarchical regional interactions modeled via hypergraphs and multifractal dynamics. By incorporating these physiological properties, the authors aim to generate fMRI signals that are more physiologically realistic, potentially reducing acquisition time and enhancing data quality. The manuscript is well-written and provides a thorough background on the challenges faced by traditional fMRI signal generation methods.\n\n## Major Comments\n1. Novelty and Positioning: While the integration of hierarchical regional interactions and multifractal dynamics into the diffusion model is innovative, the manuscript does not sufficiently differentiate this work from existing literature on hierarchical and multifractal modeling in fMRI. The authors should clarify how their approach differs from other recent works that also consider brain network dynamics and multifractal characteristics.\n\n2. Evaluation Design: The experimental evaluation is primarily conducted on the Human Connectome Project (HCP) dataset, which is standard but limited. The authors should consider validating their model on additional datasets, including those from different populations or conditions, to strengthen the generalizability of their findings. Furthermore, the inclusion of a prospective validation study would be beneficial.\n\n3. Comparisons: The comparison with baseline methods is thorough, but the manuscript should include additional diffusion-based models and recent advancements in time series generation that explicitly consider physiological dynamics. This would help establish the relative improvement of the proposed method over state-of-the-art techniques.\n\n4. Reproducibility: Although the authors mention that the code will be made available, the manuscript lacks detailed descriptions of the preprocessing steps, hyperparameter tuning, and specific implementation choices. Providing a more comprehensive methodology section would enhance the reproducibility of the results.\n\n## Minor Comments\n1. Figures: Figures 3 and 4 could be improved by adding more representative slices with zoomed-in regions to enhance readability.\n   \n2. Notation Consistency: The notation for the forward operator is inconsistent throughout the manuscript. Standardizing the notation would improve clarity.\n   \n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without being defined initially, which can confuse readers unfamiliar with the field.\n   \n4. Typographical Issues: Minor typographical errors, such as \"k-spacce\" (page 6) and \"undersampling maskes\" (page 7), should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge in fMRI signal generation by proposing a novel framework that integrates hierarchical regional interactions and multifractal dynamics into a diffusion model. The innovation is noteworthy, as it aims to generate fMRI signals that are more physiologically realistic. However, the evaluation is limited to a single dataset, and while the method shows promising results, broader validation is necessary to establish its generalizability. The reproducibility of the approach is also uncertain due to incomplete methodological details. Overall, the manuscript has the potential to make a substantial contribution to the field, but it requires additional validation and more detailed documentation to meet the rigorous standards of TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis, strengthen validation across different datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces the *Physiological Dynamics‚ÄëDriven Hierarchical Diffusion Model (PDH‚ÄëDiffusion)*, designed to generate physiologically realistic fMRI time series data. The framework combines hierarchical brain region interactions, represented through hypergraphs, with multifractal signal dynamics to better capture the complex temporal and spatial characteristics of fMRI signals. By integrating these physiological properties, the authors aim to improve realism, potentially enabling shorter acquisition times and enhanced data fidelity. The paper is clearly written and offers a comprehensive overview of existing challenges in fMRI signal generation.\n\n**Major Comments**  \n1. **Novelty and Positioning** ‚Äì The proposed integration of hierarchical and multifractal modeling within a diffusion framework is promising, but the manuscript does not sufficiently distinguish this contribution from prior studies involving similar physiological modeling approaches. The authors should clarify the conceptual and methodological differences between PDH‚ÄëDiffusion and recent related frameworks addressing brain network and multifractal properties.  \n2. **Evaluation Design** ‚Äì Experiments focus almost exclusively on the Human Connectome Project dataset. Including evaluations across additional datasets from varied populations or conditions would better demonstrate robustness and generalizability. A prospective validation study could also strengthen the empirical support.  \n3. **Comparisons with Baselines** ‚Äì Current comparisons are appropriate but could be extended. Incorporating recent diffusion‚Äëbased or physiologically informed time‚Äëseries generation models would allow for a clearer assessment of relative performance and novelty.  \n4. **Reproducibility** ‚Äì Although code release is mentioned, important implementation details‚Äîsuch as preprocessing steps, hyperparameter settings, and specific parameter choices‚Äîare not fully described. A more detailed methodology section would substantially improve reproducibility.\n\n**Minor Comments**  \n1. **Figures** ‚Äì Figures‚ÄØ3 and‚ÄØ4 would benefit from clearer visualization, adding zoomed‚Äëin regions or representative slices.  \n2. **Notation** ‚Äì Ensure consistent notation for the forward operator throughout.  \n3. **Acronyms** ‚Äì Define all acronyms (e.g., ‚ÄúR=4‚Äù) upon first use.  \n4. **Typos** ‚Äì Correct minor typographical errors, including ‚Äúk‚Äëspacce‚Äù and ‚Äúundersampling maskes.‚Äù\n\n**Summary Paragraph**  \nThe study proposes a technically interesting and physiologically grounded framework for fMRI signal generation. Its main strengths lie in integrating hierarchical and multifractal dynamics into a unified model and in the clarity of exposition. However, the limited evaluation scope, incomplete methodological transparency, and lack of broader comparisons constrain the strength of the evidence. Expanding validation and detailing implementation would considerably enhance the contribution.\n\n**Decision Recommendation**  \n**Major Revision.** Additional comparative analyses, multi‚Äëdataset validation, and a more detailed methodological description are required to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## SYNTHESIZING REALISTIC FMRI: A PHYSIOLOGICAL DYNAMICS-DRIVEN HIERARCHICAL DIFFUSION MODEL FOR EFFICIENT FMRI ACQUISITION\n\n### Summary\n\nThis paper proposes PDH-Diffusion, a diffusion-based framework tailored to fMRI time-series generation/forecasting that explicitly injects two neuroscientific priors into the denoising process: multi-scale inter-regional interactions via a hypergraph-based hierarchical signals generator (HHSG) constructed from resting-state functional connectivity, and scale-invariant temporal dynamics via a dynamics properties guiding module (DPGM) that predicts multifractal descriptors (generalized Hurst exponent H(q) and multifractal spectrum f(Œ±)). A progressive, cross-region conditioning scheme guides finer-grained regional synthesis using coarser aggregated signals. On HCP resting-state data, PDH-Diffusion outperforms several time-series baselines and two diffusion baselines on point-wise forecasting metrics, with ablations suggesting each proposed component contributes to performance.\n\n### Strengths\n\n- Technical novelty and innovationIntegrates hypergraph-based multi-scale spatial aggregation with diffusion, aligning with the known hierarchical organization of brain networks.Introduces multifractal dynamics as an explicit conditioning/guidance signal in time-series diffusion, a physiologically motivated design for fMRI.Proposes a progressive cross-region conditioning strategy that cascades coarse-scale dynamics to finer scales within the diffusion process.\n- Experimental rigor and validationConducts ablations removing HHSG, DPGM, and the cross-region progressive module, each showing notable degradation.Studies sensitivity to conditioning weights and to the ‚Äúshare ratio‚Äù in the progressive diffusion schedule, indicating some attention to hyperparameter effects.\n- Clarity of presentationProvides clear high-level modular decomposition (HHSG, DPGM, progressive diffusion), with a reasonably intuitive narrative on why each component is needed.Includes preliminaries that situate fractal/multifractal properties and self-similarity, useful for readers outside the neuroimaging dynamics community.\n- Significance of contributionsAddresses an important pain point for fMRI acquisition and downstream ML: reducing effective scan burden and preserving physiologically meaningful characteristics of BOLD signals.If validated, the approach could impact both clinical and research workflows by enabling higher-quality imputations/forecasts and potentially shorter acquisitions.\n\n- Integrates hypergraph-based multi-scale spatial aggregation with diffusion, aligning with the known hierarchical organization of brain networks.\n- Introduces multifractal dynamics as an explicit conditioning/guidance signal in time-series diffusion, a physiologically motivated design for fMRI.\n- Proposes a progressive cross-region conditioning strategy that cascades coarse-scale dynamics to finer scales within the diffusion process.\n\n- Conducts ablations removing HHSG, DPGM, and the cross-region progressive module, each showing notable degradation.\n- Studies sensitivity to conditioning weights and to the ‚Äúshare ratio‚Äù in the progressive diffusion schedule, indicating some attention to hyperparameter effects.\n\n- Provides clear high-level modular decomposition (HHSG, DPGM, progressive diffusion), with a reasonably intuitive narrative on why each component is needed.\n- Includes preliminaries that situate fractal/multifractal properties and self-similarity, useful for readers outside the neuroimaging dynamics community.\n\n- Addresses an important pain point for fMRI acquisition and downstream ML: reducing effective scan burden and preserving physiologically meaningful characteristics of BOLD signals.\n- If validated, the approach could impact both clinical and research workflows by enabling higher-quality imputations/forecasts and potentially shorter acquisitions.\n\n### Weaknesses\n\n- Technical limitations or concernsPotential information leakage in constructing the rsFC-derived graph/hypergraphs: it is unclear whether FC is computed using all time points (including the forecast window) for each subject, which would contaminate forecasting experiments.The hypergraph construction is deterministic (thresholded rsFC + k-hop neighborhoods) and message passing is a single-step mean aggregation; this may limit expressivity and collapses to smoothing without learning high-order structures or weights.The DPGM is trained with multifractal targets computed on the future window, but there is no explicit evaluation that generated series actually match multifractal properties or PSD slopes.\n- Experimental gaps or methodological issuesClaims about ‚Äúphysiologically realistic‚Äù and ‚Äúreducing acquisition time‚Äù are not directly validated: no downstream analyses (e.g., FC preservation, network metrics, spectral/multifractal fidelity, behavioral/clinical endpoints) or truncated-scan simulations demonstrate practical acquisition savings.Baseline coverage is incomplete for diffusion time-series forecasting: omits strong and recent methods such as TimeDiff, CCDM, WaveStitch, and CSDI; also lacks graph-aware forecasting baselines (e.g., DCRNN, STGCN) and fMRI-specific dynamic models.Metrics focus on point accuracy (MAE, RMSE, MAPE), which are not fully informative for fMRI where temporal/spectral structure and inter-regional correlation preservation are paramount.No uncertainty-aware evaluation (e.g., CRPS) despite using a generative model.\n- Clarity or presentation issuesInconsistent ROI counts (82 vs 86); key construction details are missing (FC threshold, k values per scale, diffusion denoiser architecture, diffusion steps, sampling strategy).Some preliminaries have notation/rendering errors that obscure derivations (e.g., Eq. 3).The Flowformer baseline cited appears to be from optical flow; if a time-series variant was used, this needs clarification.\n- Missing related work or comparisonsLimited engagement with recent diffusion-for-time-series literature (TimeDiff 2023; CCDM 2024; WaveStitch 2025) and hypergraph/fMRI works that learn higher-order structures (e.g., HA-STA 2025) and multiscale graph modeling (MAHGCN).No discussion of frequency band or scale-aware fMRI models (e.g., MBBN), which also incorporate scale-free dynamics.\n\n- Potential information leakage in constructing the rsFC-derived graph/hypergraphs: it is unclear whether FC is computed using all time points (including the forecast window) for each subject, which would contaminate forecasting experiments.\n- The hypergraph construction is deterministic (thresholded rsFC + k-hop neighborhoods) and message passing is a single-step mean aggregation; this may limit expressivity and collapses to smoothing without learning high-order structures or weights.\n- The DPGM is trained with multifractal targets computed on the future window, but there is no explicit evaluation that generated series actually match multifractal properties or PSD slopes.\n\n- Claims about ‚Äúphysiologically realistic‚Äù and ‚Äúreducing acquisition time‚Äù are not directly validated: no downstream analyses (e.g., FC preservation, network metrics, spectral/multifractal fidelity, behavioral/clinical endpoints) or truncated-scan simulations demonstrate practical acquisition savings.\n- Baseline coverage is incomplete for diffusion time-series forecasting: omits strong and recent methods such as TimeDiff, CCDM, WaveStitch, and CSDI; also lacks graph-aware forecasting baselines (e.g., DCRNN, STGCN) and fMRI-specific dynamic models.\n- Metrics focus on point accuracy (MAE, RMSE, MAPE), which are not fully informative for fMRI where temporal/spectral structure and inter-regional correlation preservation are paramount.\n- No uncertainty-aware evaluation (e.g., CRPS) despite using a generative model.\n\n- Inconsistent ROI counts (82 vs 86); key construction details are missing (FC threshold, k values per scale, diffusion denoiser architecture, diffusion steps, sampling strategy).\n- Some preliminaries have notation/rendering errors that obscure derivations (e.g., Eq. 3).\n- The Flowformer baseline cited appears to be from optical flow; if a time-series variant was used, this needs clarification.\n\n- Limited engagement with recent diffusion-for-time-series literature (TimeDiff 2023; CCDM 2024; WaveStitch 2025) and hypergraph/fMRI works that learn higher-order structures (e.g., HA-STA 2025) and multiscale graph modeling (MAHGCN).\n- No discussion of frequency band or scale-aware fMRI models (e.g., MBBN), which also incorporate scale-free dynamics.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe overall idea‚Äîto align diffusion generation with spatial hierarchy and scale-invariant temporal dynamics‚Äîis well motivated. However:Hypergraph design: constructing hyperedges as k-hop neighborhoods from thresholded FC and then applying one-step mean updates risks over-smoothing and may not capture higher-order synchrony beyond pairwise FC neighborhoods. Consider learnable hyperedge masks/weights, attention-based aggregation over hyperedges, and multi-step message passing to better capture high-order dependencies.Dynamics module: predicting H(q) and f(Œ±) from history is sensible for conditioning, but the loss alone does not ensure that the generated trajectories exhibit the same multifractal characteristics. A consistency regularizer comparing generated vs. target multifractal descriptors would close the loop.Progressive diffusion: the share-ratio schedule and cross-region conditioning follow prior multi-granularity diffusion ideas; more formal justification or ablation controlling for parameter count would strengthen the claim that gains come from the proposed physiology-inspired hierarchy rather than extra capacity.\n- Experimental evaluation assessmentForecasting-only evaluation with MAE/RMSE/MAPE is insufficient to support claims about physiological realism. Strongly recommended additions:Dynamics fidelity: compare generated vs. ground-truth PSD slopes (Œ≤), H(q) curves, multifractal spectra f(Œ±), and their deviations per ROI. Report distributional distances (e.g., KL/Wasserstein) and correlation of descriptors across ROIs.Connectivity preservation: compute FC (and dynamic FC) from generated tails vs. true tails; quantify similarity using geometry-aware distances (e.g., Venkatesh et al., 2020), matrix correlation, and effects on graph metrics (modularity, efficiency, small-worldness).Downstream utility: test whether replacing the last X% of a scan with generated data preserves performance on standard tasks (e.g., sex classification, fluid intelligence regression) or clinical classification; compare with simply shortening the scan and not imputing.Uncertainty: report CRPS and calibration of predictive intervals, especially given the clinical framing.Baselines:Include competitive diffusion forecasters: TimeDiff (autoregressive init + future mixup), CCDM (contrastive conditional diffusion), WaveStitch (inference-time conditioning). Also consider CSDI and SSSD with recent refinements.Include graph-aware time-series baselines (DCRNN, STGCN, GTS) and fMRI-specific models where applicable.Robustness:Clarify whether rsFC used to build graphs/hypergraphs is computed only from the conditioning window or from training-set templates; avoid using future windows to prevent leakage. Provide a variant using a group-average FC and another using history-only FC to show robustness.Report standard deviations and statistical significance over multiple runs; current tables present only means.\n- Comparison with related work (using the summaries provided)Time-series diffusion: Compared to TimeDiff (2306.05043), CCDM (2410.02168), and WaveStitch (2503.06231), the proposed conditioning is physiologically grounded (hypergraph hierarchy and multifractal dynamics) rather than purely algorithmic (future mixup, contrastive training, inference-time guidance). However, these methods demonstrate state-of-the-art forecasting performance, efficient sampling, and strong conditioning strategies; omitting them weakens the empirical case. Incorporating some of their conditioning or sampling ideas (e.g., efficient solvers, contrastive regularization) might reduce steps and improve generalization.fMRI hypergraph/hierarchical modeling: HA-STA (2505.12068) learns sparse higher-order structures with an information bottleneck; MAHGCN (2209.11232) uses atlas-based multiscale pooling. In contrast, this paper uses fixed rsFC-derived k-hop hyperedges and mean aggregation. Learning hyperedge masks/weights, sparsity constraints, or attention could produce richer and more interpretable multi-scale structures aligned with HA-STA‚Äôs insights.Scale-aware brain modeling: MBBN (2503.23394) explicitly models frequency bands tied to scale-free dynamics and reports interpretable band-specific patterns. While PDH-Diffusion emphasizes multifractality, it does not analyze band-resolved behavior of generated series. A frequency-resolved evaluation (band-wise PSDs or Œ≤ exponents) would make the physiological claim more concrete.\n- Discussion of broader impact and significancePotential: If the model can accurately extend or repair fMRI sequences while preserving inter-regional structure and multifractal dynamics, it could reduce required acquisition time, mitigate motion-corrupted segments, and improve the utility of datasets for machine learning and clinical decision support.Risks: Over-claiming acquisition-time reduction without direct validation; reliance on rsFC from the same subject may inadvertently encode future information; synthetic data may obscure pathophysiological signals if used indiscriminately. Strong validation and transparent release (code, seeds, preprocessing scripts) are important.\n\n- The overall idea‚Äîto align diffusion generation with spatial hierarchy and scale-invariant temporal dynamics‚Äîis well motivated. However:Hypergraph design: constructing hyperedges as k-hop neighborhoods from thresholded FC and then applying one-step mean updates risks over-smoothing and may not capture higher-order synchrony beyond pairwise FC neighborhoods. Consider learnable hyperedge masks/weights, attention-based aggregation over hyperedges, and multi-step message passing to better capture high-order dependencies.Dynamics module: predicting H(q) and f(Œ±) from history is sensible for conditioning, but the loss alone does not ensure that the generated trajectories exhibit the same multifractal characteristics. A consistency regularizer comparing generated vs. target multifractal descriptors would close the loop.Progressive diffusion: the share-ratio schedule and cross-region conditioning follow prior multi-granularity diffusion ideas; more formal justification or ablation controlling for parameter count would strengthen the claim that gains come from the proposed physiology-inspired hierarchy rather than extra capacity.\n\n- Hypergraph design: constructing hyperedges as k-hop neighborhoods from thresholded FC and then applying one-step mean updates risks over-smoothing and may not capture higher-order synchrony beyond pairwise FC neighborhoods. Consider learnable hyperedge masks/weights, attention-based aggregation over hyperedges, and multi-step message passing to better capture high-order dependencies.\n- Dynamics module: predicting H(q) and f(Œ±) from history is sensible for conditioning, but the loss alone does not ensure that the generated trajectories exhibit the same multifractal characteristics. A consistency regularizer comparing generated vs. target multifractal descriptors would close the loop.\n- Progressive diffusion: the share-ratio schedule and cross-region conditioning follow prior multi-granularity diffusion ideas; more formal justification or ablation controlling for parameter count would strengthen the claim that gains come from the proposed physiology-inspired hierarchy rather than extra capacity.\n\n- Forecasting-only evaluation with MAE/RMSE/MAPE is insufficient to support claims about physiological realism. Strongly recommended additions:Dynamics fidelity: compare generated vs. ground-truth PSD slopes (Œ≤), H(q) curves, multifractal spectra f(Œ±), and their deviations per ROI. Report distributional distances (e.g., KL/Wasserstein) and correlation of descriptors across ROIs.Connectivity preservation: compute FC (and dynamic FC) from generated tails vs. true tails; quantify similarity using geometry-aware distances (e.g., Venkatesh et al., 2020), matrix correlation, and effects on graph metrics (modularity, efficiency, small-worldness).Downstream utility: test whether replacing the last X% of a scan with generated data preserves performance on standard tasks (e.g., sex classification, fluid intelligence regression) or clinical classification; compare with simply shortening the scan and not imputing.Uncertainty: report CRPS and calibration of predictive intervals, especially given the clinical framing.\n- Baselines:Include competitive diffusion forecasters: TimeDiff (autoregressive init + future mixup), CCDM (contrastive conditional diffusion), WaveStitch (inference-time conditioning). Also consider CSDI and SSSD with recent refinements.Include graph-aware time-series baselines (DCRNN, STGCN, GTS) and fMRI-specific models where applicable.\n- Robustness:Clarify whether rsFC used to build graphs/hypergraphs is computed only from the conditioning window or from training-set templates; avoid using future windows to prevent leakage. Provide a variant using a group-average FC and another using history-only FC to show robustness.Report standard deviations and statistical significance over multiple runs; current tables present only means.\n\n- Dynamics fidelity: compare generated vs. ground-truth PSD slopes (Œ≤), H(q) curves, multifractal spectra f(Œ±), and their deviations per ROI. Report distributional distances (e.g., KL/Wasserstein) and correlation of descriptors across ROIs.\n- Connectivity preservation: compute FC (and dynamic FC) from generated tails vs. true tails; quantify similarity using geometry-aware distances (e.g., Venkatesh et al., 2020), matrix correlation, and effects on graph metrics (modularity, efficiency, small-worldness).\n- Downstream utility: test whether replacing the last X% of a scan with generated data preserves performance on standard tasks (e.g., sex classification, fluid intelligence regression) or clinical classification; compare with simply shortening the scan and not imputing.\n- Uncertainty: report CRPS and calibration of predictive intervals, especially given the clinical framing.\n\n- Include competitive diffusion forecasters: TimeDiff (autoregressive init + future mixup), CCDM (contrastive conditional diffusion), WaveStitch (inference-time conditioning). Also consider CSDI and SSSD with recent refinements.\n- Include graph-aware time-series baselines (DCRNN, STGCN, GTS) and fMRI-specific models where applicable.\n\n- Clarify whether rsFC used to build graphs/hypergraphs is computed only from the conditioning window or from training-set templates; avoid using future windows to prevent leakage. Provide a variant using a group-average FC and another using history-only FC to show robustness.\n- Report standard deviations and statistical significance over multiple runs; current tables present only means.\n\n- Time-series diffusion: Compared to TimeDiff (2306.05043), CCDM (2410.02168), and WaveStitch (2503.06231), the proposed conditioning is physiologically grounded (hypergraph hierarchy and multifractal dynamics) rather than purely algorithmic (future mixup, contrastive training, inference-time guidance). However, these methods demonstrate state-of-the-art forecasting performance, efficient sampling, and strong conditioning strategies; omitting them weakens the empirical case. Incorporating some of their conditioning or sampling ideas (e.g., efficient solvers, contrastive regularization) might reduce steps and improve generalization.\n- fMRI hypergraph/hierarchical modeling: HA-STA (2505.12068) learns sparse higher-order structures with an information bottleneck; MAHGCN (2209.11232) uses atlas-based multiscale pooling. In contrast, this paper uses fixed rsFC-derived k-hop hyperedges and mean aggregation. Learning hyperedge masks/weights, sparsity constraints, or attention could produce richer and more interpretable multi-scale structures aligned with HA-STA‚Äôs insights.\n- Scale-aware brain modeling: MBBN (2503.23394) explicitly models frequency bands tied to scale-free dynamics and reports interpretable band-specific patterns. While PDH-Diffusion emphasizes multifractality, it does not analyze band-resolved behavior of generated series. A frequency-resolved evaluation (band-wise PSDs or Œ≤ exponents) would make the physiological claim more concrete.\n\n- Potential: If the model can accurately extend or repair fMRI sequences while preserving inter-regional structure and multifractal dynamics, it could reduce required acquisition time, mitigate motion-corrupted segments, and improve the utility of datasets for machine learning and clinical decision support.\n- Risks: Over-claiming acquisition-time reduction without direct validation; reliance on rsFC from the same subject may inadvertently encode future information; synthetic data may obscure pathophysiological signals if used indiscriminately. Strong validation and transparent release (code, seeds, preprocessing scripts) are important.\n\n### Questions for Authors\n\n- How is the rsFC matrix used to construct graphs/hypergraphs computed in the forecasting experiments? Is it computed from the entire subject time series (which would include future), only from the observed conditioning window, or from a group-average template learned on the training set? Please clarify and, if needed, re-run to eliminate leakage.\n- Can you provide explicit evaluations showing that generated sequences preserve PSD slopes, H(q) curves, and multifractal spectra f(Œ±), per ROI and at the group level? How close are these descriptors between generated and ground truth, and how does DPGM influence this gap?\n- What is the denoiser architecture (layers, hidden sizes, conditioning mechanism such as concatenation vs adaLN, diffusion steps at train/inference, sampler used)? What is the runtime compared to autoregressive forecasters and to efficient diffusion forecasters (e.g., TimeDiff)?\n- Which thresholds and k values are used to build the rsFC graph and the k-hop hypergraphs at each scale? How sensitive are results to these choices, and do you use any hyperedge weighting or attention?\n- Did you control for parameter count in the ablations (e.g., replace removed modules with capacity-matched alternatives) to ensure improvements are due to the proposed mechanisms rather than added capacity or auxiliary losses?\n- Why were strong diffusion baselines such as TimeDiff, CCDM, WaveStitch, and CSDI omitted? Can you include them, along with graph-aware time-series baselines, in a revised comparison?\n- To substantiate acquisition-time reduction, can you evaluate downstream tasks (e.g., sex classification, fluid intelligence regression) when the last X% of the scan is replaced by generated data, and quantify FC/dFC preservation and biomarker stability?\n- MAPE can be misleading for BOLD signals with large means and small relative fluctuations. Can you add correlation, R^2, spectral distance, and FC similarity metrics to better reflect neuroscientific fidelity?\n- There is an inconsistency in ROI counts (82 vs 86) and unclear preprocessing details. Can you clarify parcellation, node exclusions, motion scrubbing, normalization, and windowing strategy used to construct train/test samples?\n- The repository link is a placeholder. Will code, preprocessing scripts, and hyperparameter settings be released to ensure reproducibility?\n\n### Overall Assessment\n\nThe paper tackles an important and timely problem‚Äîphysiologically faithful generation/forecasting of fMRI time series‚Äîby injecting domain priors into diffusion via multi-scale hypergraph aggregation and multifractal dynamics conditioning. Conceptually, this is a promising direction and the modular design is appealing. However, the experimental validation is currently narrower than the claims: results are limited to point-forecasting errors without demonstrating preservation of critical neuroscientific properties (spectral/multifractal signatures, FC/dFC structure) or practical acquisition-time savings. There are also methodological clarity concerns (possible FC leakage, missing architectural/diffusion details) and baseline gaps relative to modern diffusion forecasters and graph-aware models. With rigorous leakage-free experiments, physiology-aware evaluations, stronger baselines, and clearer implementation details, this work could become a compelling contribution at the intersection of diffusion models and neuroimaging. As it stands, it shows promise but needs a more comprehensive and neuroscientifically grounded evaluation to reach a top-tier standard.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **PDH-Diffusion**, a diffusion-based framework for generating and forecasting fMRI time series. It integrates two neuroscientific priors: (1) multi-scale spatial hierarchy modeled through a hypergraph-based hierarchical signal generator (HHSG) derived from resting-state functional connectivity, and (2) scale-invariant temporal dynamics captured by a dynamics property guiding module (DPGM) predicting multifractal descriptors. A progressive cross-region conditioning scheme enables coarse-to-fine signal generation. Experiments on HCP data show improvements over several time-series and diffusion baselines, supported by ablation studies. The work is conceptually strong and presents a clear modular structure, but questions remain about data leakage, evaluation scope, baseline completeness, and clarity of implementation.  \n\n---\n\n**Major Comments**  \n1. **Potential Data Leakage and Graph Construction:** The rsFC-based hypergraph design may induce leakage if connectivity is computed using the full time series, including forecast windows. Clarify whether only conditioning data or group-average templates were used.  \n2. **Limited Expressivity in Hypergraph Modeling:** The deterministic thresholding and single-step mean aggregation may oversmooth signals; learnable or attention-based hyperedges and multi-step message passing would better capture higher-order dependencies.  \n3. **Validation of Dynamics Preservation:** The DPGM is trained on multifractal targets from future windows, yet there is no evidence that generated sequences reproduce those multifractal or spectral properties.  \n4. **Insufficient Evaluation of Physiological Realism:** Claims regarding ‚Äúphysiological fidelity‚Äù and reduced acquisition time lack direct validation. No analyses of FC/dFC preservation, multifractal fidelity, or downstream task performance are provided.  \n5. **Incomplete Baseline Coverage:** Strong diffusion (TimeDiff, CCDM, WaveStitch, CSDI) and graph-based (DCRNN, STGCN, GTS) forecasting baselines are omitted, weakening empirical comparisons.  \n6. **Metrics and Uncertainty:** Current metrics (MAE, RMSE, MAPE) capture point errors but not structural fidelity. Spectral, correlation-based, and uncertainty scores (e.g., CRPS) should be added.  \n7. **Clarity and Reproducibility Issues:** Inconsistent ROI counts (82 vs 86), missing FC thresholds, k values, diffusion step parameters, and unclear architecture details limit reproducibility. Placeholder code links and absent statistical reporting further reduce transparency.  \n8. **Related Work Engagement:** Limited discussion of recent diffusion-for-time-series and multiscale fMRI models. Including comparisons with TimeDiff, CCDM, HA-STA, MAHGCN, and MBBN would better contextualize contributions.  \n\n---\n\n**Minor Comments**  \n- Correct notation errors (e.g., in Equation 3) and clarify rendering.  \n- Specify denoiser architecture, diffusion steps, and runtime relative to autoregressive baselines.  \n- Standardize parcellation description, preprocessing steps, and motion correction strategy.  \n- Clarify if Flowformer refers to a time-series version.  \n- Provide standard deviations and significance testing across runs.  \n\n---\n\n**Summary Paragraph**  \nThe study offers a creative integration of hierarchical spatial modeling and multifractal dynamics into diffusion-based fMRI generation. The conceptual novelty and potential relevance to efficient fMRI acquisition are clear strengths. However, the experimental evidence currently falls short of substantiating claims of physiological realism and acquisition-time reduction. Concerns regarding possible data leakage, limited baseline breadth, and missing methodological details constrain confidence in the reported results. The contribution would be significantly strengthened by leakage-free analyses, richer neuroscience-relevant evaluations, inclusion of competitive baselines, and open methodological documentation.  \n\n---\n\n**Decision Recommendation:** **Major Revision** ‚Äî Promising concept and clear structure, but requires substantial additional validation, comparisons, and methodological clarification before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for IEEE Transactions on Medical Imaging\n\n## 1. Opening Paragraph\n\nThis manuscript addresses the critical challenge of lengthy acquisition time and patient movement sensitivity in fMRI scanning, which limit its clinical utility and machine learning applications. The authors propose a novel Physiological Dynamics-Driven Hierarchical Diffusion Model (PDH-Diffusion) that integrates two key physiological properties of brain activity into the diffusion framework: hierarchical regional interactions and multifractal dynamics. The solution comprises three core components: (1) a Hypergraph-based Hierarchical Signals Generator that models complex brain connectivity across multiple scales, (2) a Dynamics Properties Guiding Module that incorporates multifractal characteristics of fMRI signals into the diffusion process, and (3) a Cross-brain Region Guiding Progressive Diffusion Model that enables progressive generation from broader to more localized brain regions. The experimental results demonstrate significant improvements over state-of-the-art time series forecasting and diffusion-based methods on the HCP dataset, with the proposed approach achieving lower MAE (28.56 vs. 29.36 for best baseline at Tpred=32), RMSE, and MAPE metrics across multiple prediction horizons. The visualizations confirm the model's ability to generate physiologically realistic fMRI signals that preserve complex temporal patterns.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- **Physiologically grounded approach**: The integration of two fundamental brain properties (hierarchical connectivity and multifractal dynamics) into the diffusion framework addresses a critical gap in existing generative models that typically ignore these physiological characteristics.\n- **Innovative hypergraph-based hierarchical modeling**: The progressive construction of hypergraphs at multiple scales (Figure 2) provides a biologically meaningful way to capture brain regional interactions that standard graph approaches cannot represent.\n- **Comprehensive ablation studies**: The paper provides thorough ablation analysis (Table 2) that quantitatively demonstrates the contribution of each component to the overall performance.\n- **Physiological validation**: The weight sensitivity analysis (Figure 4a) shows the importance of both multifractal and cross-brain region conditions, with optimal weights that align with physiological expectations.\n\n### Major Limitations\n- **Lack of physiological validation metrics**: While the paper demonstrates numerical superiority in standard forecasting metrics, it lacks specialized validation metrics that specifically assess physiological realism (e.g., functional connectivity preservation, power spectrum analysis, or fractal dimension consistency).\n- **Insufficient comparison to medical imaging generative models**: The baselines focus primarily on time series forecasting methods without adequate comparison to medical imaging-specific generative models that might be more relevant for fMRI synthesis.\n- **Limited clinical relevance demonstration**: The paper doesn't clearly demonstrate how the generated fMRI signals would translate to improved clinical outcomes or diagnostic capabilities, which is essential for medical imaging applications.\n\n### Minor Strengths\n- **Clear visualizations**: The framework diagrams (Figure 1) and hypergraph construction (Figure 2) effectively communicate the complex methodology.\n- **Theoretical grounding**: The paper provides solid theoretical background on multifractal analysis and hypergraph construction, supporting the technical approach.\n- **Parameter sensitivity analysis**: The thorough exploration of share ratios (Figure 4b) and brain region range settings (Table 3) demonstrates thoughtful model design.\n\n### Minor Limitations\n- **Implementation details**: Some implementation details of the diffusion process (e.g., exact architecture of the denoising network) are not fully specified.\n- **Computational efficiency**: The paper doesn't discuss the computational requirements of the proposed approach compared to baselines, which is important for practical clinical deployment.\n- **Figure 5 interpretation**: The 3D surface plot in Figure 5 would benefit from additional explanation to clarify the relationship between share ratios and brain region ranges.\n\n## 3. Evaluation Against TMI Editorial Criteria\n\n**Significance**: The problem addressed is highly significant for medical imaging, as fMRI acquisition time and motion sensitivity directly impact clinical feasibility and diagnostic quality. The proposed solution has potential to enable shorter scans, reduce motion artifacts, and expand fMRI applications in clinical settings. However, the paper could better articulate the specific clinical pathways through which this technology would impact patient care.\n\n**Innovation**: The work demonstrates strong innovation by integrating two physiological properties (hierarchical connectivity and multifractal dynamics) that have not been previously combined in fMRI generation. The hypergraph-based hierarchical modeling approach represents a novel application of hypergraph theory to fMRI signal generation, and the progressive diffusion framework with cross-brain region guidance is conceptually distinctive. The innovation is well-grounded in neuroscience literature.\n\n**Evaluation**: The experimental evaluation is methodologically sound with appropriate datasets (HCP), comprehensive metrics (MAE, RMSE, MAPE), and thorough ablation studies. However, the evaluation lacks domain-specific validation metrics that would demonstrate physiological realism beyond numerical forecasting accuracy. The comparison to baselines is comprehensive for time series forecasting but insufficient for medical imaging applications. The statistical significance of results across multiple runs is appropriately reported.\n\n**Reproducibility**: The paper provides sufficient methodological detail for replication, including clear descriptions of the three core components, training procedures, and hyperparameter settings. The code availability statement (though with placeholder URL) supports reproducibility. However, some implementation details of the diffusion network architecture could be more specific, and the precise preprocessing steps for the HCP dataset could be elaborated for full reproducibility.\n\n## 4. Decision Recommendation\n\nMinor Revision\n\nThe manuscript presents a novel and methodologically sound approach to fMRI signal generation with strong potential clinical impact. The work is technically rigorous and demonstrates clear superiority over existing methods on standard forecasting metrics. However, to meet TMI's standards for medical imaging research, the authors should address the major limitations related to physiological validation metrics and clinical relevance demonstration. I recommend minor revision with specific attention to:\n1) Adding domain-specific validation metrics that assess the physiological fidelity of generated signals\n2) Including comparisons to medical imaging-specific generative models where applicable\n3) Clarifying the clinical pathway and potential impact on diagnostic capabilities\n\nThe theoretical foundation, experimental design, and core contributions are strong, and with these revisions, the paper would make a valuable contribution to the medical imaging literature.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the significant challenge of prolonged acquisition times and motion sensitivity in fMRI scanning, proposing a Physiological Dynamics‚ÄëDriven Hierarchical Diffusion Model (PDH‚ÄëDiffusion) to enhance both physiological realism and predictive performance. The model integrates hierarchical regional brain interactions and multifractal dynamics into a diffusion framework comprising three components: a hypergraph‚Äëbased hierarchical signal generator, a dynamics‚Äëguided module embedding multifractal characteristics, and a cross‚Äëregion progressive diffusion process. Evaluation on the HCP dataset indicates consistent improvements over state‚Äëof‚Äëthe‚Äëart forecasting and diffusion baselines, along with visually plausible generated signals that preserve complex temporal patterns. Overall, the paper is clearly written, technically rigorous, and well supported by experimental analysis.  \n\n**Major Comments**  \n1. **Physiological grounding** ‚Äì The integration of hierarchical connectivity and multifractal dynamics into a generative framework is a notable strength, addressing a gap in existing fMRI generation approaches.  \n2. **Innovative modeling** ‚Äì The use of multi‚Äëscale hypergraphs to represent hierarchical brain structures provides a biologically meaningful alternative to standard graph methods.  \n3. **Ablation and validation** ‚Äì The ablation studies convincingly quantify the contribution of each module, and weight sensitivity analyses support the physiological interpretation of model components.  \n4. **Need for physiological validation metrics** ‚Äì The evaluation relies mainly on forecasting metrics (MAE, RMSE, MAPE) but lacks physiological‚Äëspecific validation such as functional connectivity or spectral analyses.  \n5. **Baseline coverage** ‚Äì Baselines focus on forecasting models without comparison to medical imaging‚Äìspecific generative methods that could provide greater contextual relevance.  \n6. **Clinical translation** ‚Äì The manuscript would benefit from clearer articulation of how the generated signals could impact diagnostic workflows or clinical decision‚Äëmaking.  \n\n**Minor Comments**  \n- Implementation details of the diffusion network architecture should be described more explicitly.  \n- Computational efficiency relative to baselines is not discussed and may affect clinical applicability.  \n- Figure‚ÄØ5 requires clearer explanation of the relationships depicted.  \n- Minor strengths include well‚Äëdesigned visualizations, solid theoretical foundations, and careful parameter sensitivity studies.  \n\n**Summary Paragraph**  \nThe study presents a technically robust and conceptually innovative framework that advances fMRI signal synthesis by integrating physiologically meaningful brain dynamics. Its main strengths lie in methodological novelty, comprehensive ablations, and clear presentation. However, the paper would benefit from domain‚Äëspecific physiological validation, broader comparisons with imaging‚Äëoriented models, and a stronger link to potential clinical applications. These additions would more convincingly demonstrate the method‚Äôs relevance and translational potential within medical imaging.  \n\n**Decision Recommendation**  \n**Minor Revision.** The work is strong overall but should address the noted issues concerning physiological validation, comparative baselines, and clarification of clinical impact before publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe authors address the well‚Äëknown limitations of functional magnetic resonance imaging (fMRI) ‚Äì long scan times and susceptibility to motion ‚Äì by proposing a generative framework that aims to produce realistic resting‚Äëstate time series. Using the Human Connectome Project (HCP) database, they extract 86 cortical regions from 900 participants, yielding 696 training and 174 test samples. Each sample consists of an 86‚ÄØ√ó‚ÄØ1200 matrix; the model receives a context window of 64 time points and predicts 32, 64, or 96 steps ahead.  \n\nThe proposed Physiological Dynamics‚ÄëDriven Hierarchical Diffusion Model contains three parts: (1) a hypergraph‚Äëbased hierarchical generator that aggregates regional signals according to resting‚Äëstate functional connectivity (rsFC); (2) a dynamics‚Äëproperties module that predicts the multifractal spectrum and generalized Hurst exponent to condition the diffusion process; and (3) a cross‚Äëbrain‚Äëregion progressive diffusion scheme that refines coarse‚Äëscale outputs into fine‚Äëscale detail. On the HCP test set the authors report lower mean absolute error (MAE), root‚Äëmean‚Äësquare error (RMSE), and mean absolute percentage error (MAPE) than several baseline time‚Äëseries forecasters and diffusion models across all prediction horizons. The stated contribution is a diffusion‚Äëbased generator that embeds rsFC and multifractal dynamics to produce physiologically plausible fMRI signals.\n\n---\n\n## General feedback  \n\n* **Significance.** The problem of generating realistic fMRI data is relevant for data augmentation and for reducing acquisition burden.  \n* **Innovation.** Combining a hypergraph representation of rsFC with multifractal‚Äëguided diffusion is uncommon in the fMRI literature.  \n* **Evaluation.** Quantitative forecasting improvements are demonstrated on a large HCP cohort, but the study lacks statistical testing, external validation, and any assessment of whether the generated signals preserve functional connectivity patterns.  \n* **Reproducibility.** The authors promise to release code, yet critical implementation details (e.g., rsFC threshold, k‚Äëhop parameters, diffusion schedule, loss weighting œâ·µ£, share ratios œÉ, optimizer settings) are omitted, and the provided GitHub link is a placeholder.\n\n---\n\n## Specific comments / critiques  \n\n1. **Hypergraph construction.** The manuscript shows Figure‚ÄØ2 to illustrate the hypergraph, but it never specifies the numeric rsFC threshold or the list of k‚Äëhop distances used to define hyperedges. Without these numbers the construction cannot be reproduced.  \n\n2. **Baseline selection.** Only generic time‚Äëseries forecasters and two diffusion models (TimeGrad, SSSD) are used for comparison. Recent fMRI‚Äëspecific generative approaches‚ÄîGANs, VAEs, flow‚Äëbased models‚Äîor state‚Äëof‚Äëthe‚Äëart medical‚Äëimage diffusion frameworks are absent, which makes it difficult to judge whether the proposed method truly advances the field.  \n\n3. **Statistical significance.** Table‚ÄØ1 lists mean MAE, RMSE, and MAPE values but provides no confidence intervals, p‚Äëvalues, or other hypothesis‚Äëtest results. Consequently it is unclear whether the reported improvements are statistically robust.  \n\n4. **Dataset limitation.** All experiments are confined to the HCP dataset. No external cohort (e.g., ADNI, clinical patient data) is used to test generalisation, leaving the transferability of the model unverified.  \n\n5. **Ablation design.** The ablation in Table‚ÄØ2 removes the hypergraph module altogether, replacing it with no hierarchical structure rather than with a plain graph‚Äëbased baseline. This design does not isolate the specific benefit of hypergraph versus graph aggregation.  \n\n6. **Multifractal predictor details.** The architecture of the RNN encoder that predicts the multifractal spectrum, the optimizer type, learning‚Äërate schedule, and the weighting Œ≥ between MSE and KL terms are not reported. These omissions hinder replication and make it difficult to assess the impact of the dynamics‚Äëconditioning component.  \n\n7. **Computational cost.** No information is given on training time, memory requirements, or inference speed. For any potential clinical or real‚Äëtime application, such metrics are essential.  \n\n8. **Downstream utility.** The authors claim that the model could aid diagnosis or reduce acquisition time, yet no experiments are presented to substantiate these claims (e.g., training a classifier on synthetic versus real data, or measuring preservation of rsFC patterns in the generated series).  \n\n9. **Code availability.** The cited repository URL (‚Äúhttps://github.com/XXX‚Äù) does not resolve to a functional location, which impedes reproducibility and verification of the reported results.  \n\n10. **Presentation issues.** Numerous typographical errors (broken symbols, inconsistent notation such as ‚ÄúPhysiological##‚Äù, ‚Äú_‚àó_‚Äù) distract from the content and should be corrected before publication.\n\n---\n\n## A suggested decision  \n\n**Major Revision.** The manuscript presents an interesting idea, but the lack of adequate baselines, statistical validation, external testing, and essential implementation details makes the current evidence insufficient for acceptance. The authors should address the points above before the paper can be reconsidered.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a generative modeling framework to mitigate key limitations of functional MRI acquisition‚Äînamely long scan times and motion sensitivity‚Äîby synthesizing realistic resting‚Äëstate time series. Using data from 900 subjects in the Human Connectome Project, the authors segment 86 cortical regions and employ these as input‚Äìoutput sequences to train the *Physiological Dynamics‚ÄëDriven Hierarchical Diffusion Model*. The model integrates a hypergraph‚Äëbased hierarchical generator, a multifractal property predictor, and a cross‚Äëregion progressive diffusion process. Quantitative results show lower forecasting errors than several baseline time‚Äëseries and diffusion models. The study addresses a meaningful and timely problem and is presented clearly, though important methodological and validation aspects remain underdeveloped.  \n\n---\n\n**Major Comments**  \n1. **Hypergraph construction:** The numeric parameters for resting‚Äëstate functional connectivity thresholding and k‚Äëhop distance selection are omitted, preventing replication.  \n2. **Baseline selection:** Comparisons are limited to general forecasters and two diffusion models. More relevant fMRI‚Äëspecific generative models (e.g., GANs, VAEs, flow‚Äëbased approaches) are not included, weakening the evaluation of novelty and performance.  \n3. **Statistical analysis:** Reported metrics lack confidence intervals or significance testing, leaving it uncertain whether improvements are statistically meaningful.  \n4. **Dataset limitation:** All analyses use a single dataset (HCP). No external validation is performed, so model generalizability to other cohorts is untested.  \n5. **Ablation design:** The ablation removes the entire hypergraph structure rather than comparing against a simpler graph‚Äëbased variant, which limits interpretability of component contributions.  \n6. **Model detail omissions:** Key implementation choices‚Äîsuch as the architecture of the multifractal predictor, optimizer, learning rate, and loss weighting‚Äîare unspecified and hinder reproducibility.  \n7. **Computational efficiency:** No data on training time or resource requirements are reported, although such metrics are important for practical use.  \n8. **Downstream evaluation:** Claims of diagnostic or acquisition benefits are unverified by experiments measuring functional‚Äëconnectivity preservation or downstream performance.  \n9. **Code availability:** The provided code link is a placeholder, impeding result verification.  \n\n---\n\n**Minor Comments**  \n- Several typographical and notation inconsistencies (e.g., broken symbols, mixed identifiers) should be corrected for clarity.  \n\n---\n\n**Summary Paragraph**  \nOverall, the work introduces a potentially innovative synthesis of hypergraph modeling and multifractal dynamics within a diffusion framework for resting‚Äëstate fMRI generation. The manuscript demonstrates quantitative improvements but lacks essential methodological transparency, statistical validation, and external testing. Addressing reproducibility and evaluation gaps would considerably strengthen the contribution and its credibility.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The proposed idea is promising but requires additional methodological detail, stronger baselines, statistical analysis, and external validation before reconsideration.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Wuyang Li",
      "Yixuan Yuan",
      "Yujiang",
      "Yufan Hu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_4e63299434922414da63f4df9505c42a3d79bc3b.pdf",
    "remote_url": "https://openreview.net/pdf/4e63299434922414da63f4df9505c42a3d79bc3b.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Tolga"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "data synthesis",
      "diffusion models",
      "cardiac MRI",
      "lung nodule CT",
      "segmentation"
    ],
    "abstract": "Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR.",
    "decision": "Accept (Spotlight)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a novel 3D lesion inpainting method, LeFusion, which uses diffusion models to address data scarcity in medical imaging. Its primary aim is to generate synthetic lesions in lung CT and cardiac MRI scans for augmenting training data in lesion segmentation tasks. The approach is validated through both visual quality assessments and data augmentation derived segmentation \n\nperformance improvement. Three key contributions can be summarised below: \nLeFusion Model: The authors identify that existing lesion inpainting methods struggle to preserve anatomically accurate backgrounds alongside the inpainted lesion, remarking that modelling the former is both hard and unnecessary. LeFusion is introduced to address this challenge incorporating two distinct features: (a) Training on a lesion focused diffusion loss, which only considers the lesion region. (b) Preserving the background at inference time with RePaint [1] by generating the lesion separately, while integrating forward-diffused background contexts into the reverse diffusion process. This design yields realistic lesions, better preserved backgrounds and improves data augmentation outcomes in both CT and MRI compared to non-lesion-specific models (Cond-Diffusion) both with and without RePaint based sampling. \n\nModality-Specific Variants: Two specialized variants are introduced to address modality-specific challenges. LeFusion-H uses histogram-based conditioning to capture diverse lesion textures in CT, succesfully solving the texture mode collapse observed for the baseline LeFusion. LeFusion-J models multiple tissue subtypes in MRI via multi-channel decomposition, which enables the joint generation of different lesion tissue types typically observed in cardiac lesions. Both variants demonstrate superior data augmentation effectiveness in their respective modalities. \n\nDiffMask for Mask Generation: All variants of LeFusion rely on either existing real masks or handcrafted ones as priors for generating lesions in healthy scans. As a more flexible alternative, DiffMask is a diffusion model that generates synthetic lesion masks from basic spatial constraints, defined as a sphere with user specified location and size. Using the generated masks for data augmentation leads to the largest improvement in segmentation performance relative to the baseline in both CT and MRI.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nLesion generating models are tools with significant potential for mitigating bias in medical vision AI algorithms concerning lesion detection, segmentation and quantification. Advancements in this topic should be highlighted in venues like this. \nThe manuscript is sufficiently well written, all the provided Figures/Tables are insightful and adequately formatted. \nThe choice of a 3D method for this inpainting problem is most adequate for CT and MRI. In these modalities, clinical lesion analysis workflows depend on the visualisation of multiple affected slices and 2D slice-wise inpainting methods would lead to slice-wise discontinuities. \n\nThe proposed method is sufficiently contextualised in the Introduction and Related work sections, where the reseach gap is clearly defined. Beyond that, this gap is empirically demonstrated by experimenting with state-of-the-art approaches (Cond-Diffusion variants and RePaint). \n\nThe proposed methodologies are thoroughly evaluated through comparisons with multiple other approaches, focusing on visual inspection of inpainted lesions (including comparison with real lesions) and their their downstream usability for training segmentation models. The latter evaluation used two different segmentation models, which contributes to the robustness of the findings across different segmentation training strategies. In addition, evaluating the approach on both MRI and CT datasets, ensures that the findings are not only applicable to one imaging domain. \n\nThis paper provides multiple key contributions which not only address the research gap but also deal with modality specific challenges related to lesion texture and shape heterogeneity. The corresponding claims are well supported by the results.\n\n### Weaknesses\n\nWhile S4, the Introduction and Background sections seem to imply that the proposed lesion focused loss is a novel contribution proposed for the first time by the authors. This might not be necessarily true considering that there have been other works that employ similar approaches [2, 3]. While few and perhaps not as thoroughly evaluated, mentioning them could further strengthen the contextualisation of the approach. \n\nThe description of the RePaint method in the experimental section implicitly suggests it consists of Cond-Diffusion using the RePaint [1] inference scheme. If that is the case it should be mentioned explicitly, if not then it should be better described. \nIn the segmentation experiments, it is understood that masks priors for generating lesions in healthy scans (N‚Äô) are either derived from real masks, handcrafted or generated by DiffMask. However, additional information should be provided on how exactly the conditioning histograms in this N‚Äô setting are selected when using LeFusion-H variants. \n\nRegarding DiffMask, the definition and role of boundary mask is not very clear. From Figure 4, it is presumed that it corresponds to the bounding box defining the volume crop centred on the lesion. However, the statement ‚ÄúThe boundary mask removes areas outside the boundary at each diffusion step‚Äù challenges this concept. Further clarity on this point would be appreciated. Furthermore, it is only implicit, that the DiffMask takes the CT/MRI volume crop as an input in addition to the conditioning control sphere. Section 3.3. should be updated to enhance clarity on all these aspects. \n\nAdding supplementary details on how the model training and checkpoint selection was conducted for the RePaint, Cond-Diffusion, Cond-Diffusion (L) would improve transparency. \n \n[Minor]\t \nMore detail on the dataset preprocessing would be beneficial for further reproducibility. A mention to the volume resolution is particularly lacking. \n\nThe choice of the specific crop-size could be further supported on previous work, for instance [4]. In addition, while not critical for acceptance, it would be interesting to study its effect over the results and would maybe answer the question: ‚ÄúHow much local context is it necessary to generate realistic lesion?‚Äù \n\nWhile the purpose of the inpainted lesions is for downstream model training, further validating them using a radiologist would safeguard from potential biases that the generative model might be introducing the lesions. \n\nWhile describing Tables 1 and 2 it would be useful to clarify what is considered as ‚Äúsignificant‚Äù. Since no standard deviations were provided, it is implied that these results were obtained for a single fold, so the concept of significance here is vague. In addition, while S5, the robustness of these findings to the specific data split could still be reinforced by adopting some sort of cross validation strategy. \nThe authors left unclear whether the segmentation model was trained on the volume crops centred on the lesion or on the entire scans. From using the Copy-Paste method in the evaluation, the latter is presumed but it is not explicitly mentioned. \n\nIn the cardiac MRI experiments, the LeFusion baseline of modelling the two lesion tissue types with separate models is mentioned as LeFusion in Table 2 but as LeFusion-S in Figure 5 and in the Appendix. It is suggested that the authors stick to one terminology. \nAs a work mainly focusing on specific diffusion model mechanics for improved lesion inpainting, it makes sense that the evaluation focus on comparing different diffusion based methods. That said, it would still be interesting to see how GAN based approaches like [4, 5] would fair in this comparison. \n \nReferences:  \n[1] Lugmayr, Andreas, et al. \"Repaint: Inpainting using denoising diffusion probabilistic models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. \n[2] Hansen, Colin, et al. \"Inpainting Pathology in Lumbar Spine MRI with Latent Diffusion.\"‚ÄØarXiv preprint arXiv:2406.02477‚ÄØ(2024). \n[3] Rouzrokh, Pouria, et al. \"Multitask brain tumor inpainting with diffusion models: A methodological report.\"‚ÄØarXiv preprint arXiv:2210.12113‚ÄØ(2022). \n[4] Yang, Jie, et al. \"Class-aware adversarial lung nodule synthesis in CT images.\"‚ÄØ2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019). IEEE, 2019. \n[5] Wu, Linshan, et al. \"FreeTumor: Advance Tumor Segmentation via Large-Scale Tumor Synthesis.\"‚ÄØarXiv preprint arXiv:2406.01264‚ÄØ(2024)\n\n### Questions\n\nFor specific questions please refer to the points made in weaknesses.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a 3D lesion inpainting framework leveraging diffusion models to address data scarcity in medical imaging. Its goal is to synthesize realistic lesions in lung CT and cardiac MRI scans for augmenting segmentation training data. The paper describes three main contributions: the LeFusion model with lesion-focused training and background-preserving inference; modality-specific variants (LeFusion-H and LeFusion-J) addressing texture and tissue diversity; and DiffMask, a diffusion-based lesion mask generator guided by spatial constraints. The manuscript is clearly written, well-structured, and supported by meaningful figures and tables demonstrating performance across two modalities.\n\n**Major Comments**  \n1. The paper implies novelty in the proposed lesion-focused loss, but similar approaches exist in prior works. Acknowledging these earlier efforts would enhance contextualization.  \n2. The description of RePaint usage is ambiguous. It should be clarified whether it represents Cond-Diffusion with the RePaint inference scheme or a distinct implementation.  \n3. Further detail is needed on how histogram conditioning is selected for healthy scans when generating lesions with LeFusion-H.  \n4. The DiffMask section lacks clarity regarding the definition and role of the boundary mask, its interaction with volume crops, and the specific inputs to the model. Section 3.3 should be revised to clarify these aspects.  \n5. Additional transparency on training procedures and checkpoint selection for comparative models (RePaint, Cond-Diffusion) is necessary for reproducibility.  \n6. Statistical interpretation of tables is vague. Without standard deviations or multiple folds, ‚Äúsignificant‚Äù results are not well supported; cross-validation could improve robustness.  \n7. The review of GAN-based alternatives could provide broader comparative insight.  \n8. Clarify whether segmentation models were trained using lesion-centered crops or whole scans.  \n\n**Minor Comments**  \n- Add preprocessing details, including volume resolution.  \n- Justify crop-size choice and discuss potential effects on realism.  \n- Clarify terminology inconsistencies (e.g., LeFusion-S vs. LeFusion).  \n- Define what constitutes statistical significance in tables.  \n- Suggest including radiologist validation to check for potential bias in synthetic lesions.  \n\n**Summary Paragraph**  \nThe study is well-motivated and technically competent, addressing a meaningful problem in medical image synthesis with well-designed experiments across modalities. Strengths include clear writing, robust evaluation using two segmentation models, and modality-specific methodological innovations. However, several aspects of methodological description and comparative contextualization require clarification. Addressing these points would enhance transparency, contextual rigor, and reproducibility.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper has solid contributions and promising results but requires clarification and methodological refinements before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors introduce a latent diffusion model-based method for inserting lesions into healthy medical images while also providing an accompanying mask. They utilize a number of additions to their model to address limitations of prior work or na√Øve approaches to this task (both pre-existing and seemingly novel), such as combining forward-diffused backgrounds with reverse-diffused foregrounds, introducing intensity histogram-conditioning to the diffusion model to control lesion texture, as well as techniques for further control of the shape, size etc. of the generated lesion. They evaluate their method for a variety of experimental scenarios on 3D cardiac MRI lesion and CT lung nodule generation, showing that their technique results in noticeable improvements to existing approaches with respect to using their generated data to train downstream task segmentation models.\n\n### Soundness: 4\n\n### Presentation: 4\n\n### Contribution: 3\n\n### Strengths\n\nMajor\n1. The paper is polished, well-written and well-presented. Topics and concepts are organized and presented in a digestible fashion.\n2. Overall, decent technical novelty. This incorporates many techniques which all come together to result in a strongly-performing methods, some pre-existing (such as combined noised backgrounds with denoised foregrounds), and some seemingly novel (such as histogram-based textural control). Also, despite the many components, the approach still seems relatively watertight because these additions are all pretty lightweight/simple (a good thing). No requirement for an additional network or something of that sort.\n3. Overall, results are strong. Clear improvements over baseline methods is basically all cases, using reasonable metrics. They also study a range of training settings, which is good. Clear improvements over Cond-Diffusion, which would be the na√Øve approach that many would think of first trying for this task; the limitations of it as discussed in the introduction are clear from the experiments.\n4. They also have fairly extensive ablation studies for their method, which is important given the number of components that they propose using. There are still a few related questions that I have, but they are minor.\n5. In general, the evaluation is fair and appropriate. The datasets are challenging benchmarks, and I think two is sufficient given the wide range of experiments completed on them. There is also a good number of baseline models, especially considering that this task is relatively niche, so the methodological baselines that they compare to seem strong.\n\nMinor\n1. The motivation for this problem is clear: pathological subjects are indeed rare, especially for screening populations. Your survey of the limitations of existing lesion synthesis approaches also supports the motivation; for example, they result in low quality backgrounds, they lack precise control over generated lesions, etc.\n2. The use of a histogram representation to condition the model on may seem too reductive for some applications, but it seems to work well here (makes sense given the clear correspondence between histogram shape/number of peaks and generated lesion morphology shown in Fig. 3), supported by the clear improvement to your method that including the -H module produced.\n\n### Weaknesses\n\nMajor\n1. Some limitations of impact/scope: This task is clinically important but still fairly niche in medical image analysis, which itself is fairly niche within general machine learning and computer vision. The method (and task itself) also requires that dataset used needs the required annotations, which many medical datasets may not possess, and can be expensive/time-consuming to acquire. Overall, these limit the impact of the work somewhat, in the context of an ML conference at the level of ICLR, compared to a venue a bit more niche like MICCAI.\n\nMinor\n1. The benefits from using multi-channel decomposition (comparing the \"-J\" to no \"-J\" variants of your model in Table 2) are quite small. Can you provide some analysis or discussion of why this is the case, even if just hypothesizing? (However, I am guessing that the computational requirement to adding this component is practically negligible, so there is not really any harm in including it even if it results in only a very small performance improvement.)\n2. You state in the abstract that synthesizing multi-peak and multi-class lesions is a \"major challenge\" I agree with the multi-peak case given how much your histogram-conditioning improved the generation of such lesions, but based on your channel decomposition module's only very small improvements to performance, I'm unsure if generating multi-class lesions could not already be done well by prior methods. Could you clarify this/point to your results that support this, and/or provide quantitative evidence that multi-class synthesis is challenging for prior approaches?\n\nTo summarize, the paper is methodologically solid, with some technical novelty, and demonstrates clear improvements to prior techniques for lesion generation tasks in medical images via well-designed experiments and baselines. However, the main limitation is just that the task is relatively niche within medical image ML, which makes it more niche within general ML, and so may be less impactful at a venue like ICLR as opposed to a medical imaging-focused venue such as MICCAI or MIDL. Still, these limitations do not take away the good things about the paper (of which there are many), so I vote for a marginal accept.\n\n### Questions\n\n1. In the tables (e.g. table 1), what do you mean by the significantly adverse/positive effects denoted by red/blue? Could you please clarify this in the text as well via a small note in the table caption(s)?\n2. My suggestion: move image quality assessment quantitative results in the appendix (Table A2) to the main text if you have room. These are important metrics. You can shorten the related works to make space, that section doesn't need to be quite so extensive (or some of it could be moved to the supplementary).\n    - Also, why didn't you evaluate unpaired perceptual metrics like FID, KID (https://arxiv.org/abs/1801.01401), SWD (https://arxiv.org/abs/1710.10196) etc.? the first two may have limitations for this task given that they use pretrained natural image features, but despite this they are still commonly used metrics for generative medical image models. I would consider adding these for future work, and also explaining why they are not used (particularly for the wider ICLR audience).\n3. For the multiclass lesion case/-J model, did you study how performance/generation quality scales with adding more classes? This point may be a bit moot given how small the changes in performance were measured after adding the channel decomposition module to the base model, but I'm still curious.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a latent diffusion model‚Äìbased framework for inserting lesions into healthy medical images, accompanied by a generated lesion mask. The method introduces several innovations to improve control and realism compared with prior approaches, including a scheme that combines forward-diffused backgrounds with reverse-diffused foregrounds, an intensity histogram‚Äìbased conditioning mechanism to modulate lesion texture, and modules for controlling lesion shape and size. The authors evaluate their approach on 3D cardiac MRI lesions and CT lung nodules, demonstrating consistent improvements in downstream segmentation tasks and over existing diffusion-based baselines. The paper is clearly written, well-organized, and technically detailed.  \n\n**Major Comments**  \n1. **Novelty and Methodological Strengths:** The combination of multiple lightweight but effective components‚Äîparticularly histogram-conditioning for texture control‚Äîis technically sound and shows clear conceptual and empirical advantages over prior methods such as conditional diffusion. Ablation studies are thorough and support the validity of design choices.  \n2. **Evaluation Design:** The experiments span two clinically relevant datasets with a comprehensive set of baselines. The improvements are consistent and well quantified. Evaluation metrics are appropriate and justified.  \n3. **Impact and Scope:** While methodologically solid, the work‚Äôs overall reach is somewhat limited. The specific task‚Äîlesion insertion in medical images‚Äîis narrow within the broader ML domain, and the requirement for annotated datasets may constrain general applicability. This may reduce impact for a general ML conference relative to specialized venues (e.g., MICCAI).  \n4. **Clarifications Requested:**  \n   - Explain the meaning of red/blue markers in result tables.  \n   - Provide discussion or justification for the small improvement from multi-channel decomposition (‚Äú‚ÄìJ‚Äù variant).  \n   - Clarify the claim that multi-class lesion synthesis is a ‚Äúmajor challenge.‚Äù Provide supporting evidence if available.  \n   - Consider adding or at least discussing metrics such as FID, KID, or SWD, or explain their omission.  \n\n**Minor Comments**  \n- The motivation is well stated and supported by prior work.  \n- A concise note should be added to table captions to clarify significance indicators.  \n- Moving quantitative image quality results from the appendix to the main text could enhance readability.  \n- The relationship between histogram shape and generated lesion morphology (Fig.‚ÄØ3) is effectively demonstrated.  \n- No ethical concerns were identified.  \n\n**Summary Paragraph**  \nOverall, this is a technically strong and well-presented paper offering moderate novelty through a practical extension of diffusion models for lesion synthesis. Its evaluations are fair and convincing, with clear performance gains and appropriate baselines. The principal limitation lies in the niche scope of the task, which may limit general ML impact. Nonetheless, the method itself appears robust, interpretable, and reproducible, making a valuable contribution to medical image generation and augmentation research.  \n\n**Decision Recommendation:** **Accept (marginal).**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper focuses on generating lesion-containing images from healthy images to address challenges in downstream segmentation tasks, such as real-world data scarcity and long-tail distribution issues. Previous research on medical image synthesis has primarily concentrated on lesion generation design, often overlooking high-fidelity background preservation. The authors propose a lesion-focused diffusion model, LeFusion, which maintains high-fidelity background by integrating the background from forward diffusion into the reverse diffusion process, thus simplifying the learning process and improving output control. Additionally, two effective strategies are introduced: histogram-based texture control and multi-channel decomposition to address the two main challenges in lesion texture synthesis: 1) multimodal and 2) multiclass lesions. The paper is well-written, with comprehensive experimental comparisons.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The overall paper structure is clear and well-expressed.\n2. A novel diffusion model is redesigned from the perspective of high-fidelity background preservation, with two texture generation control techniques developed to address multimodal and multiclass issues.\n3. The comparative methods are recent benchmarks from the past two years, making the results highly convincing.\n\n### Weaknesses\n\n1.There is a lack of detail on implementation specifics (such as the sampling process) and theoretical support for the method.\n2. Analysis and discussion on the continuity at the fusion boundaries between lesion and background are missing, as well as the impact on downstream tasks.\n\n### Questions\n\n1. The reverse diffusion sampling process is not clearly defined; it appears to rely solely on the transformation in Equation (1), without detailing the sampling process or providing theoretical justification for omitting it.\n2. Although the background from forward diffusion is used as the background in the reverse sampling process, and the loss constraint is applied only to the lesion area, how is continuity and smoothness ensured in the intersecting regions between the lesion and background?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a lesion-focused diffusion model designed to generate lesion-containing images from healthy medical images to mitigate data scarcity and long-tail distribution problems in lesion segmentation tasks. Unlike prior works that primarily emphasize lesion generation, the proposed method preserves high-fidelity background structures by incorporating background information from the forward diffusion stage into the reverse process. It further introduces two strategies‚Äîhistogram-based texture control and multi-channel decomposition‚Äîto effectively handle multimodal and multiclass lesion texture synthesis. The paper is clearly structured and supported by comprehensive experimental comparisons with recent benchmark methods.\n\n**Major Comments**  \n1. **Methodological transparency:** The paper lacks sufficient detail on implementation aspects, particularly concerning the sampling process within the reverse diffusion procedure. The description appears limited to the transformation in Equation (1) without an explicit account of sampling steps or justification for omitting them.  \n2. **Theoretical grounding:** Additional theoretical explanation would strengthen understanding of how the proposed modifications influence diffusion dynamics and lesion synthesis outcomes.  \n3. **Boundary continuity:** The manuscript does not analyze or discuss continuity at the junctions between generated lesions and preserved background regions. Clarity on how smooth transitions are maintained would improve confidence in the realism and downstream usability of the synthesized images.  \n4. **Impact on downstream tasks:** The results could benefit from further discussion on how the proposed background-preserving mechanism affects lesion segmentation or other downstream tasks, especially in challenging boundaries or texture transitions.\n\n**Minor Comments**  \n- Minor clarifications regarding implementation specifics, notation, and sampling parameters would aid reproducibility.  \n- Figures and equations could be slightly expanded with explanatory notes to enhance reader comprehension.\n\n**Summary Paragraph**  \nOverall, the study addresses an important gap in medical image synthesis by prioritizing background fidelity during lesion generation. The approach is novel and evaluated against relevant, recent baselines with convincing results. However, incomplete methodological detail and limited analysis of lesion‚Äìbackground transitions reduce transparency and theoretical clarity. Clarifying these aspects would substantially improve the robustness of the contribution.\n\n**Decision Recommendation**  \n**Minor Revision.** The paper is promising and well-presented but requires additional methodological specification and discussion for completeness.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The paper fits well within *IEEE TMI*‚Äôs methodological scope. It presents a new generative modeling framework (LeFusion) for lesion synthesis in medical imaging, focusing on algorithmic innovation rather than pure application. By modifying diffusion model objectives to isolate lesion synthesis and maintain background fidelity, it contributes to the growing area of generative modeling for medical image augmentation, reconstruction, and fairness ‚Äî all core interests of TMI.\n\n2. **Novelty & Contribution Level**  \nThe paper introduces several methodologically distinct components:\n- Reformulated diffusion process combining forward-diffused background and reverse-diffused lesion inference.\n- A lesion-focused training objective (mask-weighted diffusion loss).\n- Histogram-based texture control for multi-peak lesion synthesis.\n- Multi-channel decomposition for simultaneous multi-class lesion generation.\n- Lesion mask diffusion (DiffMask) for synthesizing controllable lesion masks.\n\nWhile the paper builds conceptually on existing diffusion and inpainting frameworks, its lesion-specific reformulation and multi-level control mechanisms appear novel and well-motivated. The scope is broader than an incremental application; it introduces a coherent set of algorithmic innovations intended to generalize across modalities.\n\n3. **Technical and Experimental Rigor**  \nThe work demonstrates a high level of technical rigor:\n- Mathematical formulations are clearly defined.  \n- Both 3D CT and MRI validation are performed.  \n- Quantitative evaluation includes Dice, NSD, FID/KID/SWD, and PSNR/SSIM, with comparisons to multiple baselines.  \n- Downstream segmentation tasks strengthen the argument for clinical utility.  \nAreas that could be improved:\n  - Reproducibility would benefit from open, verified code links.  \n  - Statistical tests or confidence intervals on segmentation improvements would solidify claims.  \n  - The use of recurrent inference and control parameter selection could be explained theoretically.\n\n4. **Clarity and Presentation**  \nGenerally clear and logically organized, though the paper is dense. The figures (e.g., Fig. 1‚Äì4) are informative but could be simplified for readability. The writing style is occasionally verbose, and consistent terminology (‚ÄúLeFusion-H/J/S‚Äù) could be better introduced. English usage is sound with minor grammatical issues.\n\n5. **Ethical and Reproducibility Compliance**  \n‚úÖ Ethical compliance is satisfactory. Only public datasets (LIDC, Emidec) are used; privacy and IRB concerns are addressed. The reproducibility statement is strong, promising open-source release with detailed documentation.\n\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n**1. Summary**  \nThe manuscript proposes *LeFusion*, a diffusion-based generative framework designed to synthesize realistic lesion-containing image‚Äìmask pairs from normal medical scans. Unlike standard conditional diffusion models that generate entire images, LeFusion focuses the diffusion process on lesion regions while reusing forward-diffused real backgrounds to ensure anatomical consistency. It introduces histogram-based texture control for multi-modal lesion textures, multi-channel decomposition for multiple lesion categories, and a lesion mask diffusion (DiffMask) mechanism enabling control over lesion size and location.  \nThe method is validated on lung CT (LIDC) and cardiac MRI (Emidec), showing improved segmentation performance over conventional synthesis approaches and other diffusion frameworks.\n\n**2. Strengths**\n- Clearly defined methodological novelty beyond prior diffusion/inpainting work.  \n- Comprehensive experimental validation across modalities.  \n- Effective demonstration of downstream benefit with segmentation models (nnUNet, SwinUNETR).  \n- Conceptually elegant solution to background preservation using background-forward diffusion.  \n- Thoughtful design of control mechanisms (texture, mask, class).  \n- Open reproducibility plan and ethical transparency.\n\n**3. Weaknesses**\n- Though novel, the improvements in diffusion formulation remain largely empirical; limited theoretical justification for loss weighting and convergence behavior.  \n- No statistical significance analysis for segmentation improvements.  \n- Comparison limited to diffusion-based baselines; GAN-based or transformer-based generative models might offer complementary benchmarks.  \n- Some sections are repetitive or overly lengthy; a more concise presentation would aid readability.  \n- Qualitative examples mainly show improved realism but lack blinded reader studies.\n\n**4. Major Comments**\n1. **Quantitative robustness** ‚Äì Include confidence intervals or statistical tests (e.g., paired t-tests) on segmentation improvements to validate significance.  \n2. **Ablation justification** ‚Äì While variants (H/J/S) are tested, a structured ablation isolating each design element (histogram control, mask diffusion, multi-channel) would strengthen claims.  \n3. **Theoretical grounding** ‚Äì Provide deeper analysis of why lesion-focused learning improves convergence or fidelity. Discuss trade-offs in model capacity vs. region focus.  \n4. **Comparative scope** ‚Äì Consider discussing performance relative to non-diffusion generative modalities (e.g., GAN or transformer generators) to clarify generalizability.  \n5. **Reproducibility link** ‚Äì Ensure code and pretrained weights are fully accessible with exact experiment scripts before publication.\n\n**5. Minor Comments**\n- Minor typos: e.g., ‚Äúb ackgrounds b y i ntegrating‚Äù (abstract).  \n- Figure 1 labeling could be simplified; arrows and sub-panels could be clearer.  \n- Define abbreviation ‚ÄúNSD‚Äù on first use.  \n- Clarify ‚Äúcontrol sphere‚Äù terminology; consider merging with ‚Äúbounding sphere.‚Äù  \n- Reference formatting: remove redundant ‚ÄúinConference‚Äù artifacts.  \n- Some results tables (Tabs. 1‚Äì2) could use visual grouping or highlight improvements more clearly.\n\n**6. Overall Recommendation:**  \n**Major Revision**  \nThe paper is well-aligned with *IEEE TMI* scope and shows strong potential for publishability. However, to reach TMI‚Äôs methodological and scientific rigor, the authors should better quantify improvement significance, streamline presentation, and add theoretical or statistical validation. With these enhancements, the paper would make a meaningful contribution to generative modeling for medical imaging.\n\n**7. Confidence Level:** 5 (High)\n\n---\n\n**Summary Judgment:**  \nLeFusion offers clear methodological advances in lesion-focused diffusion modeling for medical image synthesis, presenting solid empirical evidence across modalities. With added statistical rigor and improved clarity, it could meet *IEEE Transactions on Medical Imaging*‚Äôs standards for methodological innovation.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a diffusion-based generative framework for synthesizing realistic lesion-containing image‚Äìmask pairs from normal medical scans. The approach reformulates the diffusion process to focus generation on lesion regions while maintaining realistic anatomical backgrounds through forward diffusion of real images. It also introduces mechanisms for histogram-based texture control, multi-channel lesion class generation, and mask-based lesion controllability (DiffMask). The work is methodologically innovative and clearly aligned with the methodological scope of medical image analysis. The manuscript is generally well written and organized, though occasionally dense, with figures that could be simplified for better readability.\n\n---\n\n**Major Comments**  \n1. **Statistical significance and quantitative robustness** ‚Äì The manuscript would benefit from reporting confidence intervals or statistical tests on segmentation improvements to substantiate the claimed performance gains.  \n2. **Ablation study structure** ‚Äì While multiple LeFusion variants (H/J/S) are tested, a clearer ablation isolating the contribution of each design component (histogram texture control, mask diffusion, and multi-channel generation) would strengthen the analysis.  \n3. **Theoretical grounding** ‚Äì The empirical design of loss weighting and lesion-focused diffusion is convincing, but additional theoretical discussion on convergence, model capacity, and trade-offs between lesion emphasis and global fidelity would increase rigor.  \n4. **Comparative scope** ‚Äì Present comparisons or discussion relative to alternative generative modalities, such as GANs or transformer-based generators, to situate the method‚Äôs contributions more broadly.  \n5. **Reproducibility** ‚Äì While the reproducibility statement is strong, the paper should ensure that verified code and pretrained weights are accessible, along with precise experiment scripts, to fully enable replication.\n\n---\n\n**Minor Comments**  \n- Correct minor typographical errors (e.g., spacing issues in the abstract).  \n- Simplify or relabel Figure 1 for visual clarity, especially arrows and sub-panels.  \n- Define ‚ÄúNSD‚Äù and other abbreviations upon first appearance.  \n- Clarify ‚Äúcontrol sphere‚Äù terminology and its relation to ‚Äúbounding sphere.‚Äù  \n- Improve reference formatting by removing redundant artifacts.  \n- Enhance readability of Tables‚ÄØ1‚Äì2 with clearer visual groupings or highlights of key improvements.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper demonstrates substantial methodological novelty, integrating multiple lesion-specific mechanisms within a coherent diffusion framework. Its experimental evaluation across CT and MRI domains is comprehensive and demonstrates potential clinical utility via segmentation performance. Ethical and data-use practices are fully compliant, and the reproducibility plan is reasonable. However, the theoretical justification remains limited, statistical validation is missing, and some presentation aspects could be streamlined. With stronger quantitative validation, clearer ablation, and improved presentation, the work could make a valuable contribution to generative modeling in medical imaging.\n\n---\n\n**Decision Recommendation: Major Revision**  \nThe study is technically strong and well-motivated but requires additional statistical rigor, theoretical discussion, and clarity improvements before it can be fully recommended for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses data scarcity and class imbalance in medical AI by proposing LeFusion, a lesion-focused diffusion model for generating synthetic pathological data. The method combines forward-diffused backgrounds with reverse-diffused foregrounds while training only on lesion regions (Figure 1). Key contributions include histogram-based texture control for multi-peak lesions, multi-channel decomposition for multi-class lesions, and lesion mask diffusion (DiffMask) for controlling size, location, and boundary. The approach is validated on lung nodule CT (LIDC dataset) and cardiac lesion MRI (Emidec dataset), demonstrating improvements in downstream segmentation performance with nnUNet and SwinUNETR models. Results show Dice score improvements of up to 5.18% for lung nodules and enhanced performance for cardiac lesions compared to baseline and competing methods (Tables 1-2).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and unclear notation**\n  - Equation (3) uses element-wise multiplication (‚äô) but the dimensions of ot‚àí1 and ÀÜxt‚àí1 are not clearly established to ensure compatibility, particularly in the multi-channel case where dimensions expand to RD√óH√óW√ón (Page 4-5)\n  - The relationship between Œ≤ÃÑt and Œ≤t in Equation (2) is defined inconsistently - Œ≤ÃÑt is defined as ‚àèts=1(1‚àíŒ≤s) but standard notation typically uses ·æ±t for this cumulative product (Page 4)\n  - Equation (7-8) in Appendix C introduces parameters r, s, p, q without proper definition of their values or how they are determined, making the histogram control mechanism theoretically unclear (Page 16)\n\n‚Ä¢ **Inadequate experimental validation and comparison methodology**\n  - The comparison with Cond-Diffusion (L) relies on \"fine-tuned open-source code\" rather than implementing the method under identical conditions, introducing potential implementation bias (Page 8, Section 4.1)\n  - Dataset splits are small with only 10 test cases for cardiac lesions (Emidec), limiting statistical significance of reported improvements (Page 7, Section 4.1)\n  - No statistical significance testing is provided for the reported performance improvements, despite claims of \"significant\" effects marked in blue/red in tables (Tables 1-2)\n  - The evaluation focuses primarily on segmentation downstream tasks but lacks direct assessment of lesion realism through clinical expert evaluation\n\n‚Ä¢ **Technical approach limitations and unclear design choices**\n  - The lesion-focused loss in Equation (4) applies masking only during training but the paper doesn't adequately address how this affects the model's ability to generate coherent lesion-background transitions during inference (Page 5, Section 3.1)\n  - The histogram-based texture control method lacks theoretical justification for why histogram matching should preserve lesion authenticity, and the clustering into three groups (Figure 3) appears arbitrary without validation (Page 5, Figure 3)\n  - Multi-channel decomposition strategy doesn't explain how channel correlations are enforced beyond simple summation, potentially leading to unrealistic multi-class lesion combinations (Page 6, Equation 6)\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and standardize mathematical notation**\n  - Provide explicit dimensional analysis for all tensor operations, particularly showing how ot‚àí1 ‚àà RD√óH√óW√ón and ÀÜxt‚àí1 ‚àà RD√óH√óW√ón are compatible in Equation (3) for both single and multi-channel cases\n  - Use standard diffusion model notation (·æ±t instead of Œ≤ÃÑt) and provide clear definitions of all variance schedule parameters with their typical ranges\n  - Include a complete mathematical derivation of the histogram control mechanism with explicit parameter definitions and their optimization procedure\n\n‚Ä¢ **Strengthen experimental design and validation**\n  - Implement all comparison methods from scratch using identical training conditions, hyperparameters, and computational resources to ensure fair comparison\n  - Expand the cardiac lesion dataset or provide cross-validation results to increase statistical power, and include confidence intervals for all reported metrics\n  - Conduct statistical significance tests (t-tests or Wilcoxon signed-rank tests) for all performance comparisons and remove subjective \"significant effect\" color coding without statistical backing\n  - Include clinical expert evaluation studies to assess lesion realism and diagnostic relevance of synthetic images\n\n‚Ä¢ **Address technical gaps and provide better justification**\n  - Conduct ablation studies showing how the lesion-focused loss affects boundary quality and provide quantitative metrics for lesion-background transition coherence\n  - Validate the histogram clustering approach through medical domain knowledge, compare with established lesion classification schemes, and show that three clusters capture meaningful clinical distinctions\n  - Develop and evaluate explicit correlation modeling mechanisms for multi-channel decomposition, such as attention mechanisms or learned channel interaction layers, with quantitative assessment of multi-class lesion co-occurrence patterns",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a lesion-focused diffusion framework designed to address data scarcity and class imbalance in medical imaging. The method combines forward-diffused background generation with reverse-diffused lesion foregrounds trained only on lesion regions. Key innovations include histogram-based texture control for multi-peak lesions, multi-channel decomposition for multi-class lesions, and diffusion-based lesion masks (DiffMask) for controllable lesion size, location, and boundary. Experiments on lung nodule CT data (LIDC) and cardiac lesion MRI data (Emidec) demonstrate improved segmentation performance using nnUNet and SwinUNETR backbones. While the idea is novel and potentially valuable for synthetic medical data generation, several critical issues in mathematical clarity, validation methodology, and theoretical justification limit the paper‚Äôs current strength.  \n\n**Major Comments**  \n1. **Mathematical formulation and notation** ‚Äì Several equations lack consistency or precise dimensional definition. Equation (3) applies element-wise multiplication, but tensor dimensions for \\(o_{t-1}\\) and \\(\\hat{x}_{t-1}\\) are not shown to be compatible, particularly in multi-channel cases. The relationship between \\(\\beta_t\\) and \\(\\bar{\\beta_t}\\) is defined inconsistently with standard diffusion notation. In Appendix C, parameters \\(r, s, p, q\\) are introduced without definition, obscuring the histogram control mechanism.  \n2. **Experimental design and comparison fairness** ‚Äì Comparisons with Cond-Diffusion (L) rely on fine-tuned external code rather than a reimplementation under identical conditions, creating possible methodological bias. The Emidec dataset uses only ten test cases, and statistical significance tests are absent, undermining confidence in the reported ‚Äúsignificant‚Äù improvements. Furthermore, experiments emphasize segmentation performance but omit clinical or perceptual evaluation of lesion realism.  \n3. **Technical and conceptual justification** ‚Äì The lesion-focused loss applies masking only during training without confirming coherent lesion-background transitions at inference. The histogram-based texture control and three-cluster grouping lack theoretical or empirical validation. The multi-channel decomposition approach provides no mechanism for enforcing realistic inter-channel correlations.  \n\n**Minor Comments**  \n- Clarify tensor dimensions throughout equations and adopt standard diffusion notation (use \\(\\bar{\\alpha_t}\\) instead of \\(\\bar{\\beta_t}\\)).  \n- Define all parameters in the histogram control mechanism with derivations or references.  \n- Specify dataset splits, performance metrics, and color-coding conventions in tables.  \n\n**Summary Paragraph**  \nOverall, the paper offers an interesting contribution by integrating lesion-level controllability into diffusion-based medical image synthesis. However, its impact is limited by unclear mathematical exposition, insufficient experimental rigor, and unsubstantiated design choices. Providing complete mathematical definitions, ensuring reproducible and statistically valid comparisons, and strengthening the empirical and theoretical justification for each proposed module would substantially improve the work.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper shows promise but requires significant clarification and methodological improvements before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces LeFusion, a lesion-focused diffusion model designed to generate lesion-containing image-segmentation pairs from lesion-free images. The authors aim to address the challenges of generating high-quality lesions with fine control over texture and location, while preserving high-fidelity backgrounds. LeFusion integrates forward-diffused background contexts into the reverse diffusion process and introduces strategies for controlling lesion textures and masks. The authors validate their method on 3D cardiac lesion MRI and lung nodule CT datasets, demonstrating improved performance in downstream segmentation tasks.\n\n## Major Comments\n1. Novelty and Positioning: The manuscript positions LeFusion as a novel approach to lesion synthesis, emphasizing its ability to maintain high-fidelity backgrounds and provide fine control over lesion textures and masks. However, the novelty is somewhat undermined by the existence of similar approaches that also integrate background preservation and texture control mechanisms. The authors need to clarify how LeFusion's contributions are distinct from recent advancements in diffusion models for medical image synthesis.\n\n2. Evaluation Design: The evaluation is primarily conducted on two datasets (LIDC and Emidec), with results showing improvements in segmentation performance. However, the scope of the evaluation could be broader to establish the generalizability of the method. Including additional datasets or anatomical regions would strengthen the claim of the method's broad applicability.\n\n3. Comparisons: While the authors compare LeFusion against several baselines, including hand-crafted methods and other diffusion-based approaches, the comparisons lack depth. Specifically, the inclusion of more recent and competitive baselines would provide a clearer picture of LeFusion's relative performance and innovation. Additionally, the authors should consider discussing limitations and potential drawbacks of their method compared to alternatives.\n\n4. Reproducibility: The reproducibility statement is adequate, with the provision of code and detailed descriptions of data preprocessing and experimental settings. However, the manuscript should explicitly mention the availability of the code and data, and provide clear instructions for accessing and running the code.\n\n## Minor Comments\n1. Clarity of Figures: Some figures, such as Figure 3, could be more readable with fewer representative slices and zoomed-in regions to highlight key differences.\n2. Consistency in Terminology: The notation and terminology used in the paper, particularly in Section 2.1, should be consistent and clearly explained.\n3. Acronym Definitions: Acronyms such as \"R=4\" should be defined when first introduced.\n4. Typographical Issues: Minor typographical errors like \"k-spacce\" and \"undersampling maskes\" should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical imaging by proposing LeFusion, a lesion-focused diffusion model that aims to generate high-quality lesions with fine control over texture and location. The method is innovative in its approach to integrating background preservation and texture control mechanisms. However, the evaluation is somewhat limited in scope, relying on two datasets, which raises questions about the method's generalizability. The comparisons, while thorough, could benefit from the inclusion of more recent and competitive baselines. Reproducibility is ensured with the provision of code and detailed descriptions, although explicit mentions of code and data availability would enhance clarity. Overall, while LeFusion shows promise, the current evidence partially meets the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis, broaden the validation to include additional datasets or anatomical regions, and provide explicit mentions of code and data availability to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LeFusion*, a lesion-focused diffusion model developed to synthesize lesion-containing image‚Äìsegmentation pairs from lesion-free medical images. The approach integrates forward-diffused background contexts into the reverse diffusion process and introduces methods for lesion texture and mask control, aiming to balance fine lesion generation with realistic background preservation. The model is evaluated on 3D cardiac lesion MRI and lung nodule CT datasets, demonstrating improved performance in downstream segmentation tasks. Overall, the manuscript is clearly written and addresses a relevant challenge in medical image synthesis.  \n\n**Major Comments**  \n1. **Novelty and Positioning** ‚Äì Although *LeFusion* is presented as a novel contribution, aspects of background preservation and lesion control resemble methods in recent diffusion-based medical imaging studies. The paper would benefit from a clearer delineation of its unique contributions relative to related work.  \n2. **Evaluation Design** ‚Äì The assessment relies on two datasets (LIDC and Emidec), which limits evidence of generalizability. Supplementary experiments on additional datasets or anatomical regions would strengthen claims of broader applicability.  \n3. **Comparisons** ‚Äì While results include several baselines, the comparative analysis could be deepened by incorporating more recent or stronger diffusion-based methods. Discussion of the limitations and potential weaknesses of *LeFusion* relative to these alternatives would also be valuable.  \n4. **Reproducibility** ‚Äì The reproducibility section provides useful detail on preprocessing and experimental settings. Nonetheless, explicit reference to the availability and accessibility of the implementation and data would further enhance transparency.  \n\n**Minor Comments**  \n- Improve figure clarity (e.g., Figure‚ÄØ3) by reducing the number of slices and adding zoomed-in views for illustrative contrasts.  \n- Ensure consistent notation and terminology, particularly in Section‚ÄØ2.1.  \n- Define acronyms such as ‚ÄúR‚ÄØ=‚ÄØ4‚Äù upon first appearance.  \n- Correct typographical errors (e.g., ‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù).  \n\n**Summary Paragraph**  \n*LeFusion* tackles an important and well-motivated problem in medical image synthesis, demonstrating potential for realistic lesion generation with background fidelity. The proposed ideas are technically sound and accompanied by promising segmentation outcomes. However, the contribution‚Äôs distinctiveness, limited evaluation breadth, and somewhat modest comparative analysis temper the overall strength of the evidence. Clarity and reproducibility could be further improved through explicit reporting of code and data accessibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The manuscript requires expanded comparative evaluation, broader validation across datasets or anatomies, and clearer details on code/data availability before it can be considered for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## LeFusion: Controllable Pathology Synthesis VIA Lesion-Focused Diffusion Models\n\n### Summary\n\nThe paper proposes LeFusion, a lesion-focused diffusion framework for controllable pathology synthesis that preserves background anatomy while generating lesion appearance only within specified masks. The core idea is to fuse forward-diffused real background with reverse-diffused lesion predictions and to train with a mask-weighted loss that concentrates learning on lesion regions. The authors further introduce two control mechanisms‚Äîhistogram-based texture conditioning for multi-peak lesions (e.g., lung nodules) and multi-channel decomposition for multi-class lesions (e.g., MI and PMO in cardiac MRI)‚Äîand a diffusion-based mask generator (DiffMask) that controls lesion size, location, and boundaries. On 3D CT (LIDC) and MRI (EMIDEC), synthesized data from LeFusion improves downstream segmentation over several baselines.\n\n### Strengths\n\n- Technical novelty and innovationThe lesion-focused loss combined with inpainting-style background fusion is a clean and principled way to preserve background while dedicating capacity to lesion appearance; this is a pragmatic, scalable design for medical contexts.Histogram-based texture conditioning is a simple, annotation-free mechanism to control multi-peak intensity distributions (e.g., solid vs. ground-glass nodules) and is particularly suitable when only image‚Äìmask pairs are available.Multi-channel decomposition for joint multi-class lesion generation is a sensible extension that captures inter-class correlations without entangling backgrounds.DiffMask introduces a structured mask diffusion approach with boundary masks and control spheres for size/location control, which is more flexible than morphology-only heuristics.\n- Experimental rigor and validationEvaluation on two 3D datasets (LIDC for CT, EMIDEC for MRI) with two strong downstream baselines (nnU-Net and SwinUNETR) demonstrates consistent segmentation gains.Comparisons include multiple reasonable baselines: hand-crafted textures, conditional diffusion (image and latent), RePaint, and copy-paste; results differentiate contributions clearly and show LeFusion variants outperform prior approaches.Ablations across LeFusion vs. LeFusion-H (histogram control) and LeFusion-J (multi-class joint modeling) are informative and consistent with the stated goals.\n- Clarity of presentationThe central idea (forward-diffused background + reverse-diffused lesion with a lesion-focused objective) is clearly stated and mathematically grounded with concise equations.Problem framing around background preservation, controllability (texture, size, location), and multi-peak/multi-class settings is well-motivated and illustrated.\n- Significance of contributionsAddresses core bottlenecks in clinically relevant synthetic augmentation: maintaining anatomical plausibility, aligning image‚Äìmask pairs, and enabling controllability without extensive additional labels.The proposed approach yields substantial downstream segmentation improvements (notably >4‚Äì5 Dice points in several regimes), which is practically valuable for long-tailed, low-prevalence pathologies.\n\n- The lesion-focused loss combined with inpainting-style background fusion is a clean and principled way to preserve background while dedicating capacity to lesion appearance; this is a pragmatic, scalable design for medical contexts.\n- Histogram-based texture conditioning is a simple, annotation-free mechanism to control multi-peak intensity distributions (e.g., solid vs. ground-glass nodules) and is particularly suitable when only image‚Äìmask pairs are available.\n- Multi-channel decomposition for joint multi-class lesion generation is a sensible extension that captures inter-class correlations without entangling backgrounds.\n- DiffMask introduces a structured mask diffusion approach with boundary masks and control spheres for size/location control, which is more flexible than morphology-only heuristics.\n\n- Evaluation on two 3D datasets (LIDC for CT, EMIDEC for MRI) with two strong downstream baselines (nnU-Net and SwinUNETR) demonstrates consistent segmentation gains.\n- Comparisons include multiple reasonable baselines: hand-crafted textures, conditional diffusion (image and latent), RePaint, and copy-paste; results differentiate contributions clearly and show LeFusion variants outperform prior approaches.\n- Ablations across LeFusion vs. LeFusion-H (histogram control) and LeFusion-J (multi-class joint modeling) are informative and consistent with the stated goals.\n\n- The central idea (forward-diffused background + reverse-diffused lesion with a lesion-focused objective) is clearly stated and mathematically grounded with concise equations.\n- Problem framing around background preservation, controllability (texture, size, location), and multi-peak/multi-class settings is well-motivated and illustrated.\n\n- Addresses core bottlenecks in clinically relevant synthetic augmentation: maintaining anatomical plausibility, aligning image‚Äìmask pairs, and enabling controllability without extensive additional labels.\n- The proposed approach yields substantial downstream segmentation improvements (notably >4‚Äì5 Dice points in several regimes), which is practically valuable for long-tailed, low-prevalence pathologies.\n\n### Weaknesses\n\n- Technical limitations or concernsThe lesion-focused training objective is closely related to mask-weighted losses and diffusion inpainting; while the integration is well-executed, the conceptual novelty is moderate and would benefit from tighter theoretical analysis (e.g., guarantees on boundary consistency and leakage).Histogram conditioning captures only low-level intensity statistics; structure- or texture-level controls (heterogeneity, spiculation, rim enhancement) are not explicitly modeled, which may limit realism in complex phenotypes.DiffMask‚Äôs shape generative process and constraints are only qualitatively evaluated; potential over-regularization from boundary masks and smoothing may bias shape statistics.\n- Experimental gaps or methodological issuesNo statistical significance tests or confidence intervals are reported; EMIDEC test set (n=10 pathological) is small and likely high-variance.Lack of radiologist or expert reader studies; reliance on downstream segmentation and limited unpaired metrics constrains assessment of clinical realism and subtle artifacts.More detailed ablations are needed to isolate the contribution of the lesion-focused objective versus inpainting-style background fusion (e.g., global loss vs. mask-weighted loss using the same sampler).Limited generalization assessment: only two datasets; no out-of-domain tests or robustness to imperfect masks (e.g., using predicted masks).\n- Clarity or presentation issuesSome implementation specifics are missing: network architecture details, diffusion parameterization (epsilon/x0/v-pred), sampling schedule/NFE, compute budget, and training time; these are critical for 3D diffusion reproducibility.The precise pipeline for histogram conditioning (number of bins, normalization, embedding mechanism for cross-attention, selection/definition of histograms at inference) is underspecified.\n- Missing related work or comparisonsThere is limited engagement with recent lesion- or anatomy-focused diffusion frameworks that are closely related in spirit: e.g., Lung-DDPM/Lung-DDPM+ (anatomically aware blending and mask conditioning), LAND (latent 3D CT with mask conditioning), and region-focused fine-tuning methods like MedDiff-FT; as well as clinically guided multi-stage pipelines (e.g., LesionDiffusion) and cardiac lesion-focused work (e.g., CLAIM).A direct comparison to mask-weighted conditional finetuning approaches or latent 3D models operating at higher resolution could help clarify trade-offs in fidelity vs. compute.\n\n- The lesion-focused training objective is closely related to mask-weighted losses and diffusion inpainting; while the integration is well-executed, the conceptual novelty is moderate and would benefit from tighter theoretical analysis (e.g., guarantees on boundary consistency and leakage).\n- Histogram conditioning captures only low-level intensity statistics; structure- or texture-level controls (heterogeneity, spiculation, rim enhancement) are not explicitly modeled, which may limit realism in complex phenotypes.\n- DiffMask‚Äôs shape generative process and constraints are only qualitatively evaluated; potential over-regularization from boundary masks and smoothing may bias shape statistics.\n\n- No statistical significance tests or confidence intervals are reported; EMIDEC test set (n=10 pathological) is small and likely high-variance.\n- Lack of radiologist or expert reader studies; reliance on downstream segmentation and limited unpaired metrics constrains assessment of clinical realism and subtle artifacts.\n- More detailed ablations are needed to isolate the contribution of the lesion-focused objective versus inpainting-style background fusion (e.g., global loss vs. mask-weighted loss using the same sampler).\n- Limited generalization assessment: only two datasets; no out-of-domain tests or robustness to imperfect masks (e.g., using predicted masks).\n\n- Some implementation specifics are missing: network architecture details, diffusion parameterization (epsilon/x0/v-pred), sampling schedule/NFE, compute budget, and training time; these are critical for 3D diffusion reproducibility.\n- The precise pipeline for histogram conditioning (number of bins, normalization, embedding mechanism for cross-attention, selection/definition of histograms at inference) is underspecified.\n\n- There is limited engagement with recent lesion- or anatomy-focused diffusion frameworks that are closely related in spirit: e.g., Lung-DDPM/Lung-DDPM+ (anatomically aware blending and mask conditioning), LAND (latent 3D CT with mask conditioning), and region-focused fine-tuning methods like MedDiff-FT; as well as clinically guided multi-stage pipelines (e.g., LesionDiffusion) and cardiac lesion-focused work (e.g., CLAIM).\n- A direct comparison to mask-weighted conditional finetuning approaches or latent 3D models operating at higher resolution could help clarify trade-offs in fidelity vs. compute.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe inpainting-style fusion x_{t-1} = M_f ‚äô o_{t-1} + M_b ‚äô xÃÇ_{t-1} is sound and enforces background preservation outside M_f at each step; however, boundary transition quality depends on how M_f is defined/smoothed. It would help to clarify whether soft masks or boundary ramps are used to mitigate seams.The lesion-focused loss E[M_f ||Œµ ‚àí pŒ∏(xt, t)||^2] is a reasonable specialization; discussing the effect on gradient magnitudes, potential bias near mask borders, and whether a small weight is used outside M_f to stabilize global context would improve rigor.Multi-channel decomposition is a straightforward extension; detailing how channels interact (shared encoder/decoder with channel-wise outputs, shared attention blocks) and whether cross-channel attention or explicit consistency constraints are used would clarify how inter-class correlations are captured.DiffMask: the boundary mask and control sphere are intuitive; please formalize the boundary mask construction and training loss, specify whether signed distance fields or dilations are used, and describe how multi-channel masks are regularized to avoid overlaps or impossible configurations.\n- Experimental evaluation assessmentThe segmentation gains are substantial and consistent across nnU-Net and SwinUNETR, which strengthens the case for utility. Please add CIs or paired significance tests, especially on EMIDEC where test n=10 is small.Add a quantitative evaluation of histogram control fidelity: e.g., distance between target and realized lesion-region histograms; stratified segmentation performance per nodule phenotype cluster (GGO, part-solid, solid).Provide quantitative mask-shape analysis for DiffMask vs. real vs. hand-crafted (e.g., distributions of compactness, sphericity, elongation, curvature, surface area vs. volume, multi-scale shape descriptors), and report a statistical distance (e.g., MMD) over shape features.Include an ablation isolating (i) lesion-focused loss vs. global loss, and (ii) background fusion vs. standard conditional diffusion under identical UNet and training compute, to precisely attribute the gains.Report compute details: 3D volume sizes, UNet depth/channels/attention, T and sampler (DDIM/DPM-Solver++), number of NFEs at inference, training steps/epochs, GPU types and time; also data preprocessing choices.Consider a small reader study or expert visual Turing test to complement segmentation metrics and to assess clinical plausibility and artifacts (especially for PMO and subtle nodule phenotypes).\n- Comparison with related work (using the summaries provided)Lung-DDPM/Lung-DDPM+ also blend generated lesion-centric content with real context via anatomically aware sampling; discussing similarities/differences (e.g., whole-lung vs patch-level generation, explicit lung masks vs. lesion-focused mask-weighted loss) would situate LeFusion more clearly.LAND shows 3D latent diffusion with anatomical mask conditioning at high resolution on modest hardware; contrasting latent-space vs image-space trade-offs (background fidelity vs VAE compression artifacts, compute vs fidelity) would be informative.MedDiff-FT adopts region-focused fine-tuning with a mask-weighted loss and a lightweight mask generator; comparing controllability, compute efficiency, and 3D vs 2D modality differences would strengthen context.CLAIM (cardiac scar synthesis) uses a lesion-focused objective and clinically constrained mask generation (AHA segments); your DiffMask aims at data-driven shape diversity rather than clinical priors. A discussion of clinically constrained vs. data-driven mask generation (trade-offs in realism, control granularity, and downstream outcomes) would benefit readers.LesionDiffusion employs text-guided control for mask geometry and appearance; contrasting histogram-intensity control with higher-level semantic attribute control would clarify the scope of controllability in LeFusion.\n- Discussion of broader impact and significanceThe framework directly addresses real bottlenecks in medical imaging AI‚Äîdata scarcity, long-tailed distributions, and the need for controllable augmentation that respects anatomy. If reproducibility is ensured (code/models), the community can use it as a practical tool.Risks include misuse for fraudulent image tampering and the potential for subtle, systematic artifacts that could bias downstream models; adding safeguards (watermarking, provenance metadata) and transparent QA would be responsible next steps.Future directions could include richer controllability (e.g., radiologist-attribute conditioning beyond histograms), uncertainty-aware sampling for diverse lesion styles, and multi-centre validation to assess robustness.\n\n- The inpainting-style fusion x_{t-1} = M_f ‚äô o_{t-1} + M_b ‚äô xÃÇ_{t-1} is sound and enforces background preservation outside M_f at each step; however, boundary transition quality depends on how M_f is defined/smoothed. It would help to clarify whether soft masks or boundary ramps are used to mitigate seams.\n- The lesion-focused loss E[M_f ||Œµ ‚àí pŒ∏(xt, t)||^2] is a reasonable specialization; discussing the effect on gradient magnitudes, potential bias near mask borders, and whether a small weight is used outside M_f to stabilize global context would improve rigor.\n- Multi-channel decomposition is a straightforward extension; detailing how channels interact (shared encoder/decoder with channel-wise outputs, shared attention blocks) and whether cross-channel attention or explicit consistency constraints are used would clarify how inter-class correlations are captured.\n- DiffMask: the boundary mask and control sphere are intuitive; please formalize the boundary mask construction and training loss, specify whether signed distance fields or dilations are used, and describe how multi-channel masks are regularized to avoid overlaps or impossible configurations.\n\n- The segmentation gains are substantial and consistent across nnU-Net and SwinUNETR, which strengthens the case for utility. Please add CIs or paired significance tests, especially on EMIDEC where test n=10 is small.\n- Add a quantitative evaluation of histogram control fidelity: e.g., distance between target and realized lesion-region histograms; stratified segmentation performance per nodule phenotype cluster (GGO, part-solid, solid).\n- Provide quantitative mask-shape analysis for DiffMask vs. real vs. hand-crafted (e.g., distributions of compactness, sphericity, elongation, curvature, surface area vs. volume, multi-scale shape descriptors), and report a statistical distance (e.g., MMD) over shape features.\n- Include an ablation isolating (i) lesion-focused loss vs. global loss, and (ii) background fusion vs. standard conditional diffusion under identical UNet and training compute, to precisely attribute the gains.\n- Report compute details: 3D volume sizes, UNet depth/channels/attention, T and sampler (DDIM/DPM-Solver++), number of NFEs at inference, training steps/epochs, GPU types and time; also data preprocessing choices.\n- Consider a small reader study or expert visual Turing test to complement segmentation metrics and to assess clinical plausibility and artifacts (especially for PMO and subtle nodule phenotypes).\n\n- Lung-DDPM/Lung-DDPM+ also blend generated lesion-centric content with real context via anatomically aware sampling; discussing similarities/differences (e.g., whole-lung vs patch-level generation, explicit lung masks vs. lesion-focused mask-weighted loss) would situate LeFusion more clearly.\n- LAND shows 3D latent diffusion with anatomical mask conditioning at high resolution on modest hardware; contrasting latent-space vs image-space trade-offs (background fidelity vs VAE compression artifacts, compute vs fidelity) would be informative.\n- MedDiff-FT adopts region-focused fine-tuning with a mask-weighted loss and a lightweight mask generator; comparing controllability, compute efficiency, and 3D vs 2D modality differences would strengthen context.\n- CLAIM (cardiac scar synthesis) uses a lesion-focused objective and clinically constrained mask generation (AHA segments); your DiffMask aims at data-driven shape diversity rather than clinical priors. A discussion of clinically constrained vs. data-driven mask generation (trade-offs in realism, control granularity, and downstream outcomes) would benefit readers.\n- LesionDiffusion employs text-guided control for mask geometry and appearance; contrasting histogram-intensity control with higher-level semantic attribute control would clarify the scope of controllability in LeFusion.\n\n- The framework directly addresses real bottlenecks in medical imaging AI‚Äîdata scarcity, long-tailed distributions, and the need for controllable augmentation that respects anatomy. If reproducibility is ensured (code/models), the community can use it as a practical tool.\n- Risks include misuse for fraudulent image tampering and the potential for subtle, systematic artifacts that could bias downstream models; adding safeguards (watermarking, provenance metadata) and transparent QA would be responsible next steps.\n- Future directions could include richer controllability (e.g., radiologist-attribute conditioning beyond histograms), uncertainty-aware sampling for diverse lesion styles, and multi-centre validation to assess robustness.\n\n### Questions for Authors\n\n- Could you provide more implementation details: the 3D UNet architecture (depth, channels, attention), diffusion parameterization (epsilon/x0/v prediction), T and sampler used, number of NFEs, training steps, and compute resources/time?\n- How exactly is the histogram conditioning implemented (number of bins, normalization, embedding pipeline for cross-attention, and how histograms are selected/edited at inference)? Can you quantify target-vs-generated histogram alignment within lesion regions?\n- Can you report statistical significance (e.g., paired t-tests or bootstrap CIs) for Dice/NSD improvements, especially on EMIDEC test (n=10 pathological), and provide per-fold variance if cross-validation was considered?\n- Please include an ablation isolating the effects of (a) lesion-focused loss vs global loss, and (b) inpainting-style background fusion vs conditional diffusion, with all else equal.\n- How robust is LeFusion to imperfect masks at training or inference (e.g., using automatically predicted or dilated/eroded masks)? Does boundary quality degrade, and does DiffMask help mitigate misalignment?\n- For DiffMask, can you provide quantitative shape-distribution comparisons (e.g., sphericity, elongation, compactness histograms, MMD over shape features) against real masks and hand-crafted masks?\n- How does LeFusion perform out-of-domain (different scanners, protocols, centers)? Have you tested on external datasets for either CT nodules or cardiac scars/PMO?\n- Did you explore combining histogram control with higher-level semantic control (e.g., textual attributes or radiologist labels) to move beyond intensity statistics for complex textures?\n- For multi-class cardiac lesions, how are interactions between MI and PMO controlled (e.g., mutual exclusivity, adjacency)? Is there any explicit constraint to avoid anatomically implausible overlaps?\n- Can you comment on the trade-off between compute cost and sample quality/diversity (e.g., NFEs vs Dice gains), and whether fast samplers (DPM-Solver++) materially affect background seams or lesion texture fidelity?\n\n### Overall Assessment\n\nThis paper presents a practical and well-motivated approach for controllable medical image synthesis that preserves background anatomy by focusing diffusion learning on lesion regions and fusing forward-diffused background. The integration of histogram-based texture control, multi-channel joint modeling, and a diffusion-based mask generator addresses key pain points in medical augmentation‚Äîbackground fidelity, controllability, and diversity‚Äîwhile delivering meaningful segmentation gains on two 3D datasets. The methodological core is incremental relative to existing inpainting and mask-weighted diffusion ideas, but the adaptation and thorough application to lesion-centric 3D synthesis are valuable and executed carefully. To strengthen the case for a top-tier venue, the paper would benefit from more comprehensive ablations, stronger statistical analysis, and deeper engagement with closely related recent works (particularly anatomy-aware 3D diffusion and region-focused finetuning). Adding quantitative mask-shape evaluations, histogram-alignment metrics, robustness to imperfect masks, and possibly a small reader study would further elevate the contribution. Overall, LeFusion is a solid, impactful piece of applied generative methodology with clear utility for the community; with the suggested clarifications and analyses, it would be competitive for publication.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **LeFusion**, a lesion-focused diffusion framework for controllable pathology synthesis that maintains anatomical background while synthesizing lesions within specified masks. The approach fuses a forward-diffused real background with a reverse-diffused lesion prediction and trains via a mask-weighted loss emphasizing lesion regions. Additional features include histogram-based texture conditioning for multi-peak lesions, multi-channel decomposition for multi-class generation, and a diffusion-based mask generator (DiffMask) for size, location, and boundary control. Evaluations on CT (LIDC) and cardiac MRI (EMIDEC) datasets demonstrate improvements in downstream segmentation performance over multiple baselines. Overall, the paper is clearly presented, technically sound, and addresses a relevant problem in medical data augmentation.  \n\n**Major Comments**  \n1. **Technical novelty and analysis:** While well executed, the integration of lesion-focused training with inpainting-style fusion is conceptually close to existing mask-weighted diffusion or inpainting approaches; more theoretical analysis could strengthen claims of novelty and boundary consistency.  \n2. **Methodological limitations:** Histogram conditioning captures low-level intensity statistics but not higher-level structural textures, limiting realism for complex lesion phenotypes. The DiffMask module is qualitatively assessed, and possible over-regularization in shape generation is not quantified.  \n3. **Experimental design:** Statistical significance tests or confidence intervals are absent, and the EMIDEC test set is small, limiting reliability. No expert reader evaluation is provided, and ablations isolating loss and fusion components are incomplete. Generalization is limited to two datasets without out-of-domain testing.  \n4. **Reproducibility and clarity:** Essential implementation details‚Äînetwork architecture, diffusion parameterization, sampling schedule, compute budget, and training time‚Äîare missing. The histogram conditioning and DiffMask construction pipelines need clearer specification.  \n5. **Related work engagement:** The paper omits discussion or comparison with closely related frameworks (e.g., Lung-DDPM, LAND, MedDiff-FT, CLAIM, LesionDiffusion). Including these would better position the contribution and clarify advantages over recent anatomy-aware diffusion models.  \n\n**Minor Comments**  \n- Clarify mask definitions and smoothing at lesion boundaries.  \n- Add quantitative evaluations of histogram fidelity and mask-shape realism (e.g., sphericity, compactness).  \n- Report compute details (volume size, UNet depth, sampler type, NFEs, GPU).  \n- Provide results of ablations separating lesion-focused loss and background fusion effects.  \n- Consider a small reader or visual Turing study to assess clinical realism.  \n\n**Summary Paragraph**  \nLeFusion presents a pragmatic and technically sound diffusion-based framework for anatomically consistent lesion synthesis with controllable attributes. The approach is innovative in combining lesion-focused weighting and controllable mask generation, yielding meaningful gains in segmentation tasks. Nonetheless, improvements in statistical rigor, architectural transparency, and comparative analysis with recent models would enhance both scientific and practical credibility. Expanding ablations, reporting quantitative evaluations for control mechanisms, and validating robustness to imperfect masks would strengthen the manuscript.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper is promising and methodologically solid but requires additional empirical analysis, clarity in implementation details, and a more thorough comparison with related diffusion-based medical synthesis methods before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: LEFUSION: CONTROLLABLE PATHOLOGY SYNTHESIS VIA LESION-FOCUSED DIFFUSION MODELS\n\n## 1. Summary of the Paper\n\nThe paper addresses the critical problem of data scarcity and long-tail imbalances in medical imaging datasets, which often leads to biased outcomes and algorithmic unfairness in AI-based medical analysis. The authors propose LeFusion, a novel lesion-focused diffusion model that synthesizes pathological images by concentrating diffusion learning objectives exclusively on lesion areas while preserving high-fidelity backgrounds through integration of forward-diffused background contexts. The method introduces three key innovations: histogram-based texture control for multi-peak lesions (e.g., lung nodules with distinct texture types), multi-channel decomposition for multi-class lesions (e.g., different cardiac lesions), and lesion mask diffusion for controlling lesion size, location, and boundary. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion demonstrates substantial improvements in downstream segmentation tasks, with generated data boosting performance of state-of-the-art models (nnUNet and SwinUNETR) by up to 5.18% in Dice score and 4.4% in normalized surface distance compared to baseline. The paper effectively demonstrates that synthetic data generated from normal scans using this approach can significantly enhance the performance of segmentation models for rare pathological conditions.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The lesion-focused approach is conceptually sound and directly addresses a fundamental limitation in current medical image synthesis methods‚Äîpreservation of high-fidelity anatomical backgrounds‚Äîwhich is particularly important in clinical applications where background integrity is crucial.\n- The histogram-based texture control is an elegant solution to the multi-peak lesion problem that requires no additional annotations beyond standard image-mask pairs, making it highly practical for real-world implementation.\n- The multi-channel decomposition strategy for multi-class lesions is well-motivated and effectively captures correlations between different lesion types, as demonstrated in the cardiac lesion experiments.\n- The experimental design is comprehensive, with multiple comparison methods, two different medical imaging modalities, and thorough evaluation metrics including both image quality assessment and downstream segmentation performance.\n- The results are impressive and clinically relevant, showing consistent improvements in segmentation performance across different experimental settings, with gains as high as 5.18% in Dice score.\n\n**Limitations:**\n- The paper does not provide a clear analysis of the computational cost and inference time requirements of LeFusion compared to baseline methods, which is critical for potential clinical implementation where efficiency matters.\n- There is limited discussion about failure cases or limitations of the approach‚Äîspecifically, what types of lesions or imaging scenarios might not be well-synthesized by the method.\n- While the ethics statement mentions potential misuse, there is no deeper discussion of the ethical implications of synthetic medical image generation, particularly regarding how to ensure generated data doesn't introduce new biases or how to verify synthetic data quality before clinical use.\n- The paper demonstrates effectiveness on two datasets but lacks discussion about generalizability to other medical imaging domains (e.g., neurological disorders, abdominal imaging) or other types of lesions not covered in the current evaluation.\n\n### Minor Comments\n- Some figures (particularly Figure 5 and Figure 6) could be improved with clearer labeling and better visual contrast to make the differences between methods more apparent to readers.\n- A more detailed ablation study showing the individual contribution of each component (histogram control, multi-channel decomposition, lesion mask diffusion) would strengthen the paper's claims about the importance of each innovation.\n- Certain technical details about the diffusion model architecture and training procedure could be expanded for better reproducibility (e.g., specific U-Net architecture details, number of parameters).\n- There are a few instances of unclear phrasing (e.g., the description of the recurrent mechanism in Section F) that could be improved for better readability.\n\n## 3. Evaluation Against TMI Editorial Criteria\n\n**Significance:** The work addresses a critical challenge in medical AI‚Äîdata scarcity for rare pathological conditions‚Äîwhich directly impacts the development and deployment of reliable diagnostic tools. The solution has significant potential to improve the performance of medical image analysis models across various clinical applications where pathological data is limited. The demonstrated improvements in segmentation performance for both lung nodules and cardiac lesions highlight the broad applicability of the approach.\n\n**Innovation:** The paper presents several novel contributions that advance the field beyond standard conditional diffusion models. The lesion-focused diffusion approach that preserves background integrity by design is conceptually innovative. The histogram-based texture control for multi-peak lesions and multi-channel decomposition for multi-class lesions address specific challenges in medical image synthesis that existing methods don't adequately handle. These innovations represent a meaningful advancement in controllable medical image synthesis.\n\n**Evaluation:** The evaluation is comprehensive and well-designed, with multiple comparison methods, two different medical imaging modalities, and thorough metrics. The inclusion of both quantitative metrics (Dice, NSD, PSNR, SSIM) and qualitative visual comparisons strengthens the evaluation. The downstream segmentation tasks provide clinically relevant validation of the synthetic data quality. However, a more detailed ablation study of the individual components would further strengthen the evaluation by clearly establishing the contribution of each innovation.\n\n**Reproducibility:** The paper provides a good level of detail about the methodology and experimental setup. The authors commit to releasing code and pre-processed data, which is essential for reproducibility. The implementation details section (Appendix F) provides sufficient information about model architectures, training parameters, and evaluation procedures. However, some additional details about the specific diffusion model architectures and the exact configuration of comparison methods would enhance reproducibility for researchers looking to build upon this work.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nThe paper presents a significant and innovative approach to a critical problem in medical imaging with compelling experimental results. The methodological contributions are valuable, and the evaluation is generally strong. However, the paper requires revisions to address the computational efficiency concerns, expand on the limitations and failure cases, provide a more detailed ablation study, and improve the discussion of broader implications and generalizability. The authors should also address the minor issues regarding figure clarity and technical details. With these revisions, the paper would be a strong contribution to the field and suitable for publication in IEEE Transactions on Medical Imaging. The requested revisions are substantial enough to warrant a major revision decision, as they would significantly improve the paper's completeness and impact.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **LeFusion**, a lesion‚Äëfocused diffusion framework designed to address data scarcity and long‚Äëtail imbalances in medical imaging. The method selectively applies diffusion processes to lesion regions while preserving anatomically consistent backgrounds through forward‚Äëdiffused context integration. Three contributions are highlighted: histogram‚Äëbased texture control for multi‚Äëpeak lesions, multi‚Äëchannel decomposition for multi‚Äëclass lesions, and lesion mask diffusion for parametric control of size and boundaries. Experiments on cardiac lesion MRI and lung nodule CT show improved segmentation performance of nnUNet and SwinUNETR models, demonstrating both methodological novelty and clinical relevance. The paper is clearly written and presents substantial empirical evidence, though several aspects require clarification and elaboration before publication.  \n\n**Major Comments**  \n1. **Computational Efficiency:** The manuscript lacks quantitative analysis of computational cost and inference speed relative to baselines, which is vital for assessing clinical feasibility.  \n2. **Failure Cases and Limitations:** The discussion of potential weaknesses is limited. Clarifying which lesion types or imaging conditions challenge the model would help define the method‚Äôs boundaries.  \n3. **Ethical and Practical Considerations:** Although ethical implications are briefly mentioned, the paper should provide greater depth on preventing bias propagation and ensuring quality control for synthetic medical data.  \n4. **Generalizability:** The study focuses on two datasets; a discussion of expected behavior across other modalities (e.g., neuro or abdominal imaging) would broaden context.  \n5. **Ablation and Component Analysis:** A detailed ablation isolating the effects of histogram control, multi‚Äëchannel decomposition, and lesion mask diffusion would more convincingly support claims of contribution.  \n\n**Minor Comments**  \n- Figures 5 and 6 would benefit from clearer labeling and improved contrast for visual comparison.  \n- Additional architectural and training details (e.g., U‚ÄëNet configuration, parameter counts) would aid reproducibility.  \n- Some phrasing in Section‚ÄØF describing the recurrent mechanism is unclear and should be refined for readability.  \n\n**Summary Paragraph**  \nOverall, the paper makes a significant and innovative contribution by emphasizing lesion‚Äëfocused diffusion for controllable pathology synthesis. The approach effectively enhances segmentation outcomes and advances controllable medical image generation. The work is well‚Äëevaluated and reproducible, but revisions are needed to address efficiency analysis, further clarify limitations and ethical aspects, and strengthen evidence through ablation studies. Resolving these issues will enhance both the technical completeness and the practical impact of the work.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the persistent problems of data scarcity and long‚Äëtail class imbalance in medical imaging by synthesizing lesion‚Äëcontaining image‚Äësegmentation pairs from scans that originally lack lesions. The authors evaluate their approach on two publicly available datasets: 3‚ÄëD lung nodule CT (LIDC) and cardiac lesion MRI (Emidec). In each case, normal and pathological cases are partitioned into separate training and testing sets. The core of the method is a lesion‚Äëfocused diffusion model, named **LeFusion**, which confines the diffusion loss to the lesion region, merges forward‚Äëdiffused background information with reverse‚Äëdiffused foreground content, and adds a histogram‚Äëbased texture controller to handle multi‚Äëpeak lesions together with a multi‚Äëchannel decomposition scheme for multi‚Äëclass lesions. An auxiliary module, **DiffMask**, diffuses lesion masks so that size, location, and boundary can be manipulated in a controllable way. Down‚Äëstream segmentation experiments using nnUNet and SwinUNETR demonstrate modest Dice‚Äëscore improvements on both datasets. The principal contribution is a diffusion‚Äëbased pipeline that creates high‚Äëfidelity synthetic lesions while preserving the realism of the surrounding anatomy.\n\n---\n\n## General feedback  \n\n- **Significance:** The challenges of limited annotated data and skewed class distributions are well‚Äëknown in medical imaging. A technique that can reliably generate diverse, lesion‚Äërich training samples while maintaining realistic background anatomy could be highly valuable for clinical applications.  \n- **Innovation:** The introduction of a lesion‚Äëfocused loss and the reuse of forward‚Äëdiffused background context represent a thoughtful adaptation of diffusion‚Äëinpainting to medical imaging. The histogram‚Äëbased texture control and the multi‚Äëchannel lesion generation are original ideas, even though similar conditioning strategies have recently appeared in the diffusion literature.  \n- **Evaluation:** The authors report gains in Dice and NSD on two datasets and supplement the analysis with perceptual metrics such as FID, KID, and SWD. Nevertheless, the manuscript lacks statistical testing and a comprehensive ablation that isolates each component (lesion‚Äëfocused loss, histogram control, multi‚Äëchannel handling, DiffMask). Moreover, there is no external validation on an additional dataset or cross‚Äëmodality experiments that would strengthen the generality of the approach.  \n- **Reproducibility:** While a code repository is mentioned and hardware/training details (6‚ÄØ√ó‚ÄØA100 GPUs, learning rate 1e‚Äë4, batch size 16, 300 timesteps) are provided, several crucial implementation specifics are missing. Details such as the exact noise schedule, depth of the U‚ÄëNet backbone, hyper‚Äëparameters governing histogram conditioning, random seeds, and the precise protocol used to split the datasets are not disclosed, which hampers exact replication.\n\n---\n\n## Specific comments / critiques  \n\n- **Loss formulation clarity:** Equation‚ÄØ(4) incorporates a mask **M_f**, yet the manuscript does not explain how **M_f** is obtained for normal images (i.e., how synthetic masks are generated) in Section‚ÄØ3.1.  \n- **Histogram conditioning details:** Equation‚ÄØ(5) refers to conditioning on a histogram **h**, but the dimensionality of **h**, the preprocessing steps applied to it, and the way it is fed into the cross‚Äëattention module (see Figures‚ÄØ3 and‚ÄØ6) are omitted.  \n- **Ablation of DiffMask:** Table‚ÄØ1 contrasts ‚ÄúLeFusion‚Äù with ‚ÄúLeFusion‚ÄëH‚Äù but does not isolate the contribution of the DiffMask module. An ablation study that directly compares performance with and without DiffMask would clarify its impact (see Figure‚ÄØ4).  \n- **Statistical analysis:** The reported Dice improvements (e.g., +2.3‚ÄØ% for LeFusion‚ÄëH) are presented without confidence intervals or p‚Äëvalues, leaving it unclear whether the gains are statistically significant (Tables‚ÄØ1 and‚ÄØ2).  \n- **Dataset split reproducibility:** The manuscript specifies the number of training and test cases (e.g., 808/202 for LIDC) but does not disclose the random seed, stratification method, or whether the split is performed on a patient‚Äëwise basis (Section‚ÄØ4.1).  \n- **Baseline coverage:** Recent diffusion‚Äëbased lesion synthesis works, such as Chen‚ÄØet‚ÄØal.‚ÄØ2024 and Lai‚ÄØet‚ÄØal.‚ÄØ2024, receive only a brief mention. Including direct quantitative comparisons on the same data splits would provide a more rigorous evaluation (Related Work).  \n- **Hyper‚Äëparameter justification:** The control‚Äëlevel ratio (75‚ÄØ:‚ÄØ20‚ÄØ:‚ÄØ5) and the ¬±10‚ÄØ% histogram fluctuation described in Section‚ÄØF are introduced without any sensitivity analysis or justification for these choices.  \n- **Inference speed comparison:** Generation time is reported as 30‚Äì50‚ÄØseconds per sample, yet no benchmark against competing methods is presented, making it difficult to assess practical efficiency.  \n- **Mask diversity limitations:** DiffMask relies on a ‚Äúcontrol sphere‚Äù derived from real masks; the manuscript does not quantify how this constraint might limit the diversity of generated mask shapes.\n\n---\n\n## A suggested decision  \n\n**Reject**  \n\nThe current submission lacks essential baseline comparisons and statistical validation, which leaves the main claims insufficiently supported.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses data scarcity and long‚Äëtail class imbalance in medical image segmentation by generating lesion‚Äëcontaining image‚Äìmask pairs from scans originally devoid of lesions. The proposed method, **LeFusion**, is a lesion‚Äëfocused diffusion framework designed to synthesize realistic lesions while preserving anatomical background consistency. The approach is tested on 3‚ÄëD CT lung nodule and cardiac MRI datasets, showing modest segmentation improvements using nnUNet and SwinUNETR. Overall, the work presents a technically interesting adaptation of diffusion modeling to medical lesion synthesis, though aspects of evaluation and reproducibility remain limited.  \n\n---\n\n**Major Comments**  \n1. **Evaluation and Statistical Validation:** Reported segmentation gains are modest and lack confidence intervals or statistical testing, making it uncertain whether improvements are significant.  \n2. **Ablation Completeness:** The contribution of individual modules‚Äîparticularly the lesion‚Äëfocused loss, histogram conditioning, multi‚Äëchannel handling, and DiffMask‚Äîis not adequately isolated. A more thorough ablation is needed to clarify their respective effects.  \n3. **Baseline Comparisons:** Related diffusion‚Äëbased lesion synthesis methods (e.g., Chen‚ÄØet‚ÄØal.‚ÄØ2024; Lai‚ÄØet‚ÄØal.‚ÄØ2024) are only briefly cited, with no quantitative comparison on common data splits. This omission limits the ability to assess relative performance.  \n4. **Reproducibility Issues:** While computational environment details are stated, critical implementation parameters‚Äînoise schedule, backbone depth, histogram‚Äëconditioning hyper‚Äëparameters, random seeds, and exact dataset‚Äësplit protocol‚Äîare omitted, hindering replication.  \n5. **Loss and Conditioning Clarification:** Equation‚ÄØ(4) includes a mask \\(M_f\\) without sufficient explanation of how synthetic masks are generated for normal images. Similarly, Equation‚ÄØ(5) lacks details about the histogram descriptor‚Äôs dimensionality, preprocessing, and integration into the cross‚Äëattention module.  \n6. **DiffMask Role:** The effect of DiffMask is not separated in the results, preventing assessment of its benefit.  \n7. **Efficiency and Diversity:** Inference time (30‚Äì50‚ÄØs/sample) is reported without benchmarking against alternatives, and the reliance on a ‚Äúcontrol sphere‚Äù for mask generation may limit shape diversity.  \n8. **Hyper‚Äëparameter Justification:** The ratios and fluctuation levels used for control and histogram adjustment lack motivation or sensitivity analysis.  \n\n---\n\n**Minor Comments**  \n- Clarify dataset‚Äësplit methodology (patient‚Äëwise, stratified, and random seed used).  \n- Improve discussion of figures illustrating histogram conditioning and DiffMask (Figures‚ÄØ3,‚ÄØ4,‚ÄØ6).  \n- Minor typographical and notation clarifications may improve readability.  \n\n---\n\n**Summary Paragraph**  \nThe work proposes an inventive lesion‚Äëfocused diffusion strategy that could help address data imbalance in medical imaging and shows preliminary improvements in segmentation performance. However, the paper‚Äôs validation is incomplete: ablation evidence, statistical testing, and baseline benchmarking are insufficient to substantiate the method‚Äôs claimed advantages. Reproducibility details are under‚Äëspecified, and empirical analysis of efficiency and mask diversity remains limited.  \n\n---\n\n**Decision Recommendation**  \n**Reject.** The manuscript presents promising ideas but lacks comprehensive experimental validation and essential comparative and statistical analyses to support its conclusions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Hantao Zhang",
      "Jiancheng Yang",
      "Pascal Fua",
      "Shouhong Wan",
      "Wei Peng",
      "Xinyuan Wang",
      "Yuhe Liu"
    ],
    "url": "pdfs/iclr.cc-2025-conference_c4c499d3163801b947e70b673f9eadca4be34844.pdf",
    "remote_url": "https://openreview.net/pdf/c4c499d3163801b947e70b673f9eadca4be34844.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Medical Image Segmentation",
      "Vision-Language Pre-Training",
      "Zero-Shot Segmentation"
    ],
    "abstract": "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia.",
    "decision": "Accept (Poster)",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces Malenia, a framework for zero-shot lesion segmentation in 3D medical images, aimed at addressing the challenge of transferring image-level knowledge to pixel-level segmentation tasks. Building on advancements in medical vision-language pre-training, Malenia integrates a multi-scale mask-attribute alignment framework and a Cross-Modal Knowledge Injection module to link visual and textual features, enabling the model to handle previously unseen lesions. The authors evaluate Malenia's performance through experiments on three datasets (two public and one private) across 12 lesion categories.\n\n### Soundness: 2\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\n1. The topic on zero-shot lesion segmentation is valuable, as clinical settings often involve diverse, emerging anomalies, and data collection is challenging, increasing the need for models that handle unseen diseases in an open-set context.\n2. The authors strengthen their claims with both qualitative and quantitative results, enhancing the credibility of their findings and providing a well-rounded evaluation of the proposed approach.\n3. The paper is well-organized, with a clear and logical structure that makes complex concepts accessible and easy to follow for readers.\n\n### Weaknesses\n\n1. The Zero-Shot Inference section lacks sufficient detail on how the model handles test CT images containing unseen tumor types. For instance, if a test image includes both a kidney tumor and a gallbladder tumor, it is unclear how the model predicts these different tumors within the same image. Additionally, the phrase ‚Äúwe obtain the class information for each predicted lesion mask by referencing a clinical knowledge table‚Äù is ambiguous. Further clarification is needed on what this clinical knowledge table entails and how it is used in the inference process.\n2. The comparisons in Table 1 and Table 2 may not be entirely fair. The authors introduce additional ATTRIBUTE DESCRIPTIONS annotations, which provide extra information not available to baseline methods like TransUNet, nnUNet, and Swin UNETR. These baselines rely solely on image data, while the proposed approach leverages attribute annotations, giving it an advantage in performance comparisons.\n3. The fully-supervised performance of baseline methods reported in Table 2 appears unusually low. For instance, in the official nnUNet paper, liver tumor segmentation achieved a Dice score of 76 on the test set, whereas the presented result here is only 61.33. Similarly, lung tumor segmentation originally reported a Dice score of 74, but this paper reports 54.44. This discrepancy raises concerns about the reproducibility and fairness of the comparisons.\n4. In Table 7, the authors compare their method to existing vision-language pretraining strategies, but it is unclear how these methods were reproduced. Most vision-language pretraining approaches require access to diagnostic reports, which are not included in the public MSD and KiTS23 datasets. It would be helpful to understand how the authors conducted these comparisons without complete diagnostic reports. Additionally, the absence of comparisons with important pretraining models like CT-CLIP weakens the evaluation.\n5. Scalability of the proposed method is questionable due to its reliance on ATTRIBUTE DESCRIPTIONS. For practical clinical applications, this approach would require physicians to provide additional lesion descriptions across eight visual attribute aspects, which may not be feasible in all settings. This dependency could limit the method‚Äôs applicability in diverse clinical environments.\n\n### Questions\n\nplease refer to Weaknesses\n\n### Flag For Ethics Review\n\n- Yes, Privacy, security and safety\n\n### Rating: 5\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nThis paper utilizes private clinical data, which may require an ethics review to ensure compliance with privacy, security, and safety standards.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **Malenia**, a framework for zero-shot lesion segmentation in 3D medical images. The approach addresses the challenge of transferring image-level knowledge to pixel-level segmentation tasks by leveraging medical vision-language pre-training. Malenia combines a multi-scale mask‚Äìattribute alignment framework with a cross-modal knowledge injection module to link visual and textual features, enabling segmentation of previously unseen lesion types. The work is evaluated on three datasets (two public, one private) covering 12 lesion categories. The paper is clearly structured and readable, and the topic is relevant to open-set medical imaging research.\n\n---\n\n**Major Comments**  \n1. **Zero-Shot Inference Clarity** ‚Äì The inference process for cases with multiple unseen tumor types is insufficiently described. It remains unclear how the model distinguishes lesions within a single test image and how the ‚Äúclinical knowledge table‚Äù is defined and used to assign class information to predicted lesions.  \n2. **Fairness of Comparisons** ‚Äì Tables 1 and 2 may present unfair comparisons since Malenia benefits from additional *attribute description* annotations that are not available to baseline methods (TransUNet, nnUNet, Swin UNETR). This input advantage may distort comparative performance.  \n3. **Baseline Reproducibility** ‚Äì Fully supervised baseline results (e.g., nnUNet) are notably lower than reported in original publications, raising reproducibility concerns. Clarification of training protocols and test settings is needed to validate these results.  \n4. **Vision-Language Pretraining Reproduction** ‚Äì Table 7 comparisons with prior vision-language pretraining methods lack methodological clarity. As many of these methods require diagnostic reports unavailable in public datasets, it is unclear how reproduction was achieved. Missing comparisons with relevant models such as CT-CLIP also limit the strength of the evaluation.  \n5. **Scalability and Clinical Feasibility** ‚Äì The requirement for *attribute descriptions* across eight lesion aspects may hinder real-world deployment, as obtaining these annotations from physicians is not always practical.\n\n---\n\n**Minor Comments**  \n- Provide additional explanation or footnote for the ‚Äúclinical knowledge table.‚Äù  \n- Verify numerical consistency across tables and ensure dataset provenance is transparent.  \n- Check figure captions and terminology (e.g., ATTRIBUTE DESCRIPTIONS) for clarity and uniform formatting.\n\n---\n\n**Summary Paragraph**  \nOverall, the paper addresses an important and timely problem in medical image analysis and presents a well-written account of a novel framework. However, significant concerns remain regarding inference explanation, fairness of experimental comparisons, and reproducibility of baseline benchmarks. The dependency on supplementary attribute annotations may also limit scalability in clinical applications. Ethical review is advised due to use of private clinical data, requiring compliance with privacy and safety standards.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The manuscript shows promise and technical depth but requires substantial clarification and improved experimental transparency before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces Malenia, a novel framework specifically designed for 3D zero-shot lesion segmentation in medical images. The authors aim to address the challenges of transferring image-level knowledge to pixel-level tasks‚Äîsuch as lesion segmentation‚Äîby proposing a multi-scale lesion-level mask-attribute alignment approach. Malenia improves the alignment between the visual features of unseen lesions with textual representations, leveraging Cross-Modal Knowledge Injection (CMKI) to enhance both visual and textual embeddings. Experimental results across three datasets demonstrate that Malenia outperforms state-of-the-art (SOTA) methods in zero-shot lesion segmentation.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed Malenia framework is novel, particularly in its use of multi-scale mask-attribute alignment for 3D lesion segmentation. This is a meaningful extension of zero-shot segmentation methods to the medical domain, where unseen lesion categories are common (e.g., \"hepatocellular carcinoma,\" mentioned in Sec. 1).\n2. The CMKI module is a strong contribution because it combines visual and textual embeddings, enriching the segmentation prediction with complementary information from both modalities. This is a novel approach that enhances the zero-shot capability of the model.\n3. The paper introduces a multi-positive contrastive loss to fine-tune the alignment between lesion masks and textual attributes. This approach allows the model to generalize better to unseen lesions by learning from shared visual attributes, which distinguishes it from prior works like SAM (Kirillov et al., 2023).\n\n### Weaknesses\n\n1. The method relies on transforming patient reports into structured descriptions of eight visual attributes, which requires expert radiologist involvement. This process may not be scalable or practical in real-world applications where such annotations are not readily available.\n2. As noted in Appendix D, the model performs less effectively on attributes requiring complex visual semantic understanding, such as \"Surface Characteristics\" and \"Specific Features\". This suggests a potential limitation in capturing intricate visual patterns.\n3. While the model shows strong performance on 12 lesion categories, the diversity of lesions in clinical practice is vast. The ability of Malenia to generalize to a wider range of unseen lesions without additional training remains uncertain.\n\n### Questions\n\n1. In Section 3.1, the authors employ the Multi-Positive InfoNCE (MP-NCE) loss for aligning mask embeddings with multiple attribute embeddings. Could the authors provide more details on how the MP-NCE loss is computed and how it facilitates learning with multiple positive pairs? Specifically, how are the positive and negative pairs defined in the context of multiple attributes?\n   Could the authors explain how this formulation effectively handles multiple positive pairs for each mask token?\n2. The model uses a fixed number of mask tokens ($N=16$) and defines $R=8$ attribute aspects. Have the authors conducted any experiments to assess how varying $N$ or the number of attributes affects the model's performance? An ablation study on these hyperparameters could provide insights into the model's sensitivity and optimal settings.\n3. As noted in Appendix D, the model performs less well on attributes like \"Surface Characteristics\" and \"Specific Features\". What are the potential reasons for this lower performance? Could incorporating additional contextual information or advanced features help the model better capture complex visual semantics?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a framework for 3D zero-shot lesion segmentation in medical imaging. The approach aims to bridge the gap between image-level and pixel-level understanding by aligning lesion masks with textual attributes through a multi-scale mask‚Äìattribute alignment strategy. Central to the method is a Cross-Modal Knowledge Injection (CMKI) module that integrates visual and textual embeddings to enhance segmentation of unseen lesion types. Experiments on three datasets report that Malenia surpasses state-of-the-art zero-shot segmentation baselines. Overall, the paper is clearly written and provides a technical solution with promising results.  \n\n**Major Comments**  \n1. **Practical scalability** ‚Äì The method depends on converting patient reports into structured descriptions of eight visual attributes, a process requiring expert radiologist input. This dependence may limit applicability in large-scale or routine clinical contexts where such annotations are scarce.  \n2. **Attribute-level limitations** ‚Äì According to Appendix D, performance is weaker on attributes representing complex visual semantics (e.g., ‚ÄúSurface Characteristics‚Äù and ‚ÄúSpecific Features‚Äù). This indicates possible difficulty in modeling fine-grained or high-level visual patterns.  \n3. **Generalization scope** ‚Äì While evaluation covers 12 lesion categories, the diversity of lesions in clinical practice is much larger. The capacity of the model to generalize to completely unseen lesion types without retraining remains uncertain.  \n4. **Loss formulation clarity** ‚Äì Clarification is requested regarding the Multi-Positive InfoNCE (MP-NCE) loss (Section‚ÄØ3.1): specifically, how multiple positive pairs are defined and how positive/negative relationships are established for mask tokens.  \n5. **Hyperparameter sensitivity** ‚Äì The model uses a fixed number of mask tokens (N‚ÄØ=‚ÄØ16) and eight attribute aspects (R‚ÄØ=‚ÄØ8). Experiments varying these parameters would strengthen the paper by revealing sensitivity and optimal settings.  \n6. **Improving complex attribute modeling** ‚Äì Given the noted difficulty with certain attributes, the authors could discuss whether incorporating richer contextual cues or advanced visual features could mitigate this limitation.  \n\n**Minor Comments**  \n- None reported beyond those embedded in major comments; the manuscript‚Äôs presentation appears clear and well structured.  \n\n**Summary Paragraph**  \nThe paper offers a novel and well-motivated framework extending zero-shot segmentation techniques to medical imaging through cross-modal feature alignment. Strengths include methodological innovation (especially CMKI and multi-positive contrastive loss) and solid empirical performance. Main concerns involve the scalability of radiologist-produced attribute annotations, incomplete handling of complex visual semantics, and limited exploration of hyperparameter and generalization behavior.  \n\n**Decision Recommendation**  \n**Recommendation: Major Revision** ‚Äî The contribution is innovative and promising, but clarification of methodological details and additional analysis are needed to address practical and performance limitations.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis work presents a zero-shot medical image semantic segmentation framework that is based on maskformer and attribute-based visual-language feature fusion. To achieve fine-grain alignment between visual and textual concepts, the authors propose to decompose radiological reports into attributes describing lesions of interest, assisted by LLMs and human annotators. The obtained attributes allows fine-grain fusion and alignment between lesion features and textual concepts, yielding improved generalization performance upon unseen lesions. The proposed framework is evaluated on tumor segmentation tasks, and demonstrates improved segmentation results compared with existing zero-shot segmentation approaches on unseen lesions.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe idea of decomposing free-text reports into attributes for fine-grain training and inference is intuitive and feasible. It can significantly reduce the noise and ambiguity associated with unstructured free-text reports and yields improved performance on unseen lesions.\n\nThe key components and the detailed implementations are described in good detail. \n\nThe proposed approach demonstrates performance improvements compared with previous zero-shot image segmentation methods.\n\nDetailed ablation studies are presented to verify the effectiveness of key components.\n\n### Weaknesses\n\nDespite stronger segmentation performance, the major framework looks similar to ZePT (Jiang et al., 2024): maskformer backbone + interacting visual and textual features that are obtained from detailed description of the lesions. Therefore, the key take-home message for readers may be a bit unclear: should the readers interpret the key technical contribution as explicitly decomposing textual descriptions/reports into categorized attributes for the ease of learning and inference?\n\nReaders may argue that using GPT-4 for attribute construction from radiological reports is often infeasible in practice due to the privacy and legal concerns. The authors are encouraged to argue if switching to local open-source LLMs would yield a similar level of performance gain. \n\nThe authors may want to decompose some of technical details in Figure 1 into separate figures and move them closer to corresponding paragraphs.\n\n### Questions\n\nConsidering the practicality issue, would replacing GPT-4 with an open-source LLM for attribute construction (probably also without human annotators?) reaching a similar level of performance gain (as mentioned above)?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a zero-shot medical image semantic segmentation framework built upon a MaskFormer backbone and an attribute-based visual‚Äìlanguage feature fusion strategy. To establish fine-grained alignment between visual features and textual concepts, the approach decomposes radiological reports into structured lesion-level attributes with assistance from large language models (LLMs) and human annotators. This method aims to improve generalization to unseen lesion categories. Evaluation on tumor segmentation tasks demonstrates enhanced segmentation performance compared with existing zero-shot methods. Overall, the paper is clearly written, describes its components in detail, and presents extensive ablation studies supporting the design choices.\n\n**Major Comments**  \n1. **Novelty and Contribution** ‚Äì While the framework achieves improved segmentation results, its structure closely resembles ZePT (Jiang et al., 2024), which also uses a MaskFormer backbone and combined visual‚Äìtextual features derived from lesion descriptions. The principal innovation may thus be limited to the explicit decomposition of free-text reports into organized attribute categories. The authors should clarify whether this explicit attribute-based formulation constitutes the core technical novelty and discuss its conceptual advantage over prior designs.  \n2. **Practical Feasibility** ‚Äì The use of GPT‚Äë4 for attribute extraction raises concerns regarding data privacy and legal restrictions in medical contexts. The authors are encouraged to discuss whether comparable results could be obtained using locally deployable open-source LLMs, and to what extent human annotation remains necessary for reliable attribute extraction.  \n3. **Clarity of Presentation** ‚Äì Figure‚ÄØ1 aggregates multiple technical details, which may reduce readability. The authors might consider separating some components into distinct figures and aligning them with the relevant explanatory text.\n\n**Minor Comments**  \n- Ensure that figure placement corresponds to text flow for greater clarity.  \n- Edit minor typographical and formatting inconsistencies as needed.\n\n**Summary Paragraph**  \nThis study applies an attribute-based decomposition of radiological reports to improve zero-shot lesion segmentation by achieving finer alignment between text and image modalities. The method is technically sound, clearly described, and empirically validated through thorough ablations. The main issues concern the clarity of the claimed novelty relative to prior work and the practicality of relying on closed LLMs for attribute generation. Addressing these points would strengthen the paper‚Äôs contribution and applicability.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a new methodological framework ‚Äî *Malenia* ‚Äî for **3D zero‚Äëshot lesion segmentation**, combining cross‚Äëmodal representation learning, multi‚Äëscale alignment, and a novel cross‚Äëmodal fusion (CMKI) mechanism. The contribution squarely targets core topics of *IEEE Transactions on Medical Imaging*: imaging informatics, segmentation methodology, and cross‚Äëmodal learning in radiology.\n\n2. **Novelty & Contribution Level**  \nThe paper claims two central technical novelties:  \n- **Multi‚Äëscale mask‚Äëattribute alignment** bridging global image‚Äìtext pre‚Äëtraining and dense voxel‚Äëlevel prediction.  \n- **Cross‚ÄëModal Knowledge Injection (CMKI)** to jointly update mask and text embeddings.  \nWhile the work builds on existing transformer backbones (Mask2Former) and CLIP‚Äëstyle pre‚Äëtraining concepts, the formulation of *multi‚Äëpositive contrastive alignment using structured radiology attributes* is original and represents a meaningful extension beyond prior ZePT or SAM‚Äëbased pipelines. Novelty is moderate‚Äëto‚Äëhigh for TMI, assuming reproducibility and dataset access are adequate.\n\n3. **Technical and Experimental Rigor**  \nExperiments are comprehensive: three datasets (MSD, KiTS23, and a private cohort, totaling 12 lesion types) and extensive ablations, including attribute construction, CMKI design, hyper‚Äëparameter studies, and comparisons to multiple baselines. The methodology is mathematically consistent. However:  \n- The reproducibility of the *semi‚Äëautomatic attribute extraction* pipeline using GPT‚Äë4 plus radiologist correction could be better documented (e.g., inter‚Äërater agreement, automation error rate).  \n- Public availability of the processed attribute annotations is only promised, not confirmed.  \n- Assertions regarding clinical interpretability and ‚Äúdiagnostic precision‚Äù might be slightly overstated without expert reader studies.\n\n4. **Clarity and Presentation**  \nOverall well organized, technically clear, and grammatically sound. Figures are dense but informative; minor language editing would improve readability (inconsistent capitalization and spacing, e.g., ‚ÄúZ ERO -SHOT,‚Äù ‚ÄúP ancreas‚Äù). The manuscript length is high but proportional to complexity.\n\n5. **Ethical and Reproducibility Compliance**  \nEthics statements for the private dataset are provided; anonymization appears sound. Code release is stated publicly on GitHub but anonymized for review. For TMI, explicit confirmation that IRB approval and data‚Äësharing guidelines will be satisfied is necessary.\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n1. **Summary**  \nThe authors propose *Malenia*, a **vision‚Äëlanguage pre‚Äëtraining framework** enabling zero‚Äëshot 3D lesion segmentation. The approach integrates structured radiology attributes (e.g., shape, density, enhancement status) into a **multi‚Äëscale mask‚Äëattribute alignment** loss and introduces a **Cross‚ÄëModal Knowledge Injection (CMKI)** module that fuses fine‚Äëgrained visual and textual embeddings to improve segmentation of both seen and unseen diseases. Evaluation on multiple public and private CT datasets demonstrates superior Dice and NSD scores compared with SAM‚Äëbased and prior vision‚Äëlanguage segmentation models.\n\n2. **Strengths**\n- Addresses an important and underexplored problem: zero‚Äëshot lesion segmentation in 3D medical imaging.  \n- Methodologically innovative: multi‚Äëpositive contrastive learning using structured attribute tokens, and bi‚Äëdirectional feature fusion.  \n- Extensive quantitative and qualitative validation, including ablations and fine‚Äëtuning analyses.  \n- Strong empirical improvement over state‚Äëof‚Äëthe‚Äëart baselines.  \n- Clear ethical protocol and commitment to releasing attribute annotations.\n\n3. **Weaknesses**\n- The reliance on proprietary LLMs (GPT‚Äë4) for attribute extraction limits reproducibility; minimal quantification of annotation accuracy.  \n- The architecture largely inherits from Mask2Former/nnUNet; theoretical justification for losses and hyperparameters is empirical rather than analytical.  \n- Discussion of clinical utility remains speculative‚Äîno reader study or uncertainty quantification.  \n- Minor clarity issues and formatting inconsistencies.  \n\n4. **Major Comments**\n1. **Reproducibility of Attribute Generation:** Provide quantitative assessment of LLM‚Äëbased attribute extraction (e.g., consistency with radiologist edits, Œ∫‚Äëscores) and describe how these structured attributes might generalize across institutions.  \n2. **Statistical Validation:** Include significance testing for Dice/NSD improvements to substantiate ‚Äústate‚Äëof‚Äëthe‚Äëart‚Äù claims.  \n3. **Ablation Transparency:** Clarify the relative contribution of CMKI vs. multi‚Äëpositive loss on the same training split; consider a formal cross‚Äëvalidation summary instead of selective examples.  \n4. **Clinical Interpretability:** The claim that Malenia ‚Äúidentifies specific disease categories via a knowledge table‚Äù borders on diagnostic inference; specify that this is attribute‚Äëdriven label matching, not verified clinical diagnosis.  \n5. **Open‚ÄëSource and Data Access:** Specify timelines and scope of planned code and attribute releases to meet IEEE data‚Äëavailability expectations.\n\n5. **Minor Comments**\n- Normalize typographic artifacts (e.g., hyphenation, mixed uppercase).  \n- Improve figure readability‚ÄîFigure‚ÄØ1 and‚ÄØ2 contain small fonts.  \n- Provide network parameter counts per module in supplemental for clarity.  \n- Verify that all references cited as 2024 preprints are in acceptable public form.\n\n6. **Overall Recommendation**  \n**‚≠ë Major Revision** ‚Äì The manuscript exhibits strong methodological novelty and promising results, aligning well with TMI's scope. However, additional substantiation of reproducibility, statistical rigor, and clearer separation of methodological versus application claims are required before acceptance.\n\n7. **Confidence Level:** **4‚ÄØ/‚ÄØ5** ‚Äì High confidence in the technical evaluation, moderate uncertainty about dataset and annotation reproducibility.\n\n---\n\n**Summary Judgment:**  \n*Malenia* represents a creative and potentially influential step toward cross‚Äëmodal zero‚Äëshot segmentation in 3D medical imaging. With improved documentation of its annotation pipeline, quantitative statistical analyses, and clarified claims, the work could merit publication in *IEEE Transactions on Medical Imaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Malenia*, a vision‚Äëlanguage pre‚Äëtraining framework for three‚Äëdimensional zero‚Äëshot lesion segmentation. The method integrates structured radiology attributes into a multi‚Äëscale mask‚Äëattribute alignment mechanism and introduces a Cross‚ÄëModal Knowledge Injection (CMKI) module to fuse visual and textual embeddings. Experiments on public and private CT datasets demonstrate improvements over existing segmentation models. Overall, the paper is clearly written, well organized, and technically detailed, though certain claims and reproducibility aspects warrant further clarification.\n\n---\n\n**Major Comments**  \n1. **Reproducibility of Attribute Generation** ‚Äì The semi‚Äëautomatic attribute extraction pipeline using GPT‚Äë4 and radiologist corrections is insufficiently documented. Quantitative evaluation of consistency (e.g., inter‚Äërater agreement or error rate) is needed to assess reproducibility and generalizability across institutions.  \n2. **Statistical Validation** ‚Äì The study reports performance gains but lacks statistical significance testing for Dice and NSD metrics. Including such analyses would strengthen claims of superiority over baselines.  \n3. **Ablation Transparency** ‚Äì The influence of each major component (CMKI vs. multi‚Äëpositive alignment loss) should be clarified. Presenting results on the same training splits or a comprehensive cross‚Äëvalidation summary would improve transparency.  \n4. **Clinical Interpretability** ‚Äì Statements implying diagnostic capability are overstated. The authors should emphasize that the model performs attribute‚Äëbased matching rather than clinical diagnosis.  \n5. **Open‚ÄëSource and Data Access** ‚Äì The paper promises public release of code and processed attribute annotations but does not specify timing or scope. A clearer plan is necessary to ensure reproducibility and compliance with data‚Äësharing expectations.  \n6. **Ethical and Compliance Documentation** ‚Äì Although ethics approval and anonymization are mentioned, explicit confirmation of compliance with institutional review and data‚Äësharing policies should be included.\n\n---\n\n**Minor Comments**  \n- Harmonize capitalization and spacing inconsistencies (e.g., ‚ÄúZERO‚ÄëSHOT,‚Äù ‚ÄúPancreas‚Äù).  \n- Improve figure readability; some fonts are small in Figures‚ÄØ1‚Äì2.  \n- Provide parameter counts per module in supplemental material for clarity.  \n- Ensure that all 2024 preprints cited are publicly accessible and properly referenced.  \n\n---\n\n**Summary Paragraph**  \nThe study tackles an important and underexplored problem‚Äîzero‚Äëshot 3D lesion segmentation‚Äîthrough an innovative cross‚Äëmodal learning approach. Strengths include methodological creativity, comprehensive experiments, and clear presentation. However, reproducibility of attribute extraction, lack of statistical validation, and overextended clinical claims limit the current evidence for robustness and practical impact. Addressing these points would substantially strengthen the manuscript‚Äôs contribution.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is technically sound and potentially impactful but requires additional analyses and clearer reproducibility documentation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents Malenia, a vision-language pre-training framework for 3D zero-shot lesion segmentation in CT scans. The method addresses the challenge of transferring image-level knowledge to pixel-level segmentation tasks by introducing multi-scale mask-attribute alignment and a Cross-Modal Knowledge Injection (CMKI) module. The approach decomposes medical reports into eight structured visual attributes (location, shape, density, etc.) and aligns these with mask representations at multiple scales using a multi-positive contrastive loss. The CMKI module enhances both visual and textual features through cross-attention mechanisms. Experiments across three datasets (MSD, KiTS23, in-house) and 12 lesion categories demonstrate superior performance compared to existing methods, with improvements of 6.40% DSC on MSD, 8.14% on KiTS23, and 9.08% on the in-house dataset over the best competing method.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Equation (2) defines the multi-positive contrastive loss but lacks clear explanation of how the expectation over positive pairs is computed in practice (Section 3.1, Page 5), which affects reproducibility\n  - The notation S(m_j, t_k) = m_j¬∑t_k/œÑ assumes normalized embeddings but this normalization is not explicitly stated (Section 3.1, Page 5), creating ambiguity in implementation\n  - The scaling factor ‚àöC in Equation (5) lacks justification for why this specific normalization is chosen over alternatives (Section 3.2, Page 6), undermining theoretical grounding\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - The private dataset contains only 30 cases per lesion type (Table 5, Page 16), which is insufficient for robust evaluation of zero-shot generalization capabilities\n  - Cross-validation methodology is inconsistent across datasets, with 5-fold cross-validation only reported for seen lesions (Table 2, Page 9) but not for unseen lesions, limiting statistical reliability\n  - The comparison with SAM-based methods may be unfair as they require manual prompts during testing (Table 1, Page 8), while Malenia uses stored embeddings from training, creating different evaluation conditions\n\n‚Ä¢ **Technical approach and novelty concerns**\n  - The multi-scale alignment strategy lacks theoretical justification for why three specific resolution levels (H/32, W/32, D/32) to (H/8, W/8, D/8) are optimal (Section 3.1, Page 4), appearing arbitrary\n  - The eight visual attributes are derived from expert consultation but lack systematic validation or comparison with alternative attribute taxonomies (Section 3.1, Page 4), questioning their generalizability\n  - The CMKI module design shows limited novelty as it primarily combines standard cross-attention and self-attention mechanisms (Equation 3, Page 6) without significant architectural innovation\n\n‚Ä¢ **Data dependency and generalization issues**\n  - The method requires extensive manual annotation by radiologists to create structured attribute descriptions (Section B.2, Page 16), which may not scale to diverse clinical settings\n  - The clinical knowledge table used during inference (Table 11, Page 25) appears manually crafted and disease-specific, potentially limiting generalization to new lesion types not covered in the predefined taxonomy\n  - The semi-automatic pipeline using GPT-4 for attribute extraction introduces dependency on proprietary models (Section 3.1, Page 5), raising concerns about reproducibility and accessibility\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and implementation clarity**\n  - Explicitly state the normalization requirements for embeddings in the similarity computation and provide pseudocode for the multi-positive contrastive loss implementation\n  - Justify the choice of ‚àöC scaling factor by comparing with alternative normalization schemes (e.g., temperature scaling, learnable parameters) in ablation studies\n  - Include detailed derivations or references for the mathematical formulations to ensure reproducibility and theoretical soundness\n\n‚Ä¢ **Strengthen experimental validation and statistical rigor**\n  - Increase the private dataset size to at least 100 cases per lesion type or provide power analysis justifying the current sample size for meaningful statistical conclusions\n  - Apply consistent cross-validation methodology across all experiments, including confidence intervals and statistical significance tests for performance comparisons\n  - Conduct fair comparisons by either providing manual prompts for SAM-based methods or developing prompt-free versions of competing approaches\n\n‚Ä¢ **Provide systematic justification for technical choices**\n  - Conduct ablation studies on different numbers of resolution levels and specific scale choices to demonstrate the optimality of the current multi-scale design\n  - Compare the proposed eight-attribute taxonomy with alternative attribute sets through systematic evaluation or provide theoretical frameworks for attribute selection\n  - Benchmark the CMKI module against other cross-modal fusion strategies (e.g., early fusion, late fusion, attention-based fusion) to demonstrate its superiority\n\n‚Ä¢ **Address scalability and accessibility limitations**\n  - Develop automated or semi-automated methods for attribute extraction that reduce dependency on manual annotation and proprietary models\n  - Create a systematic framework for expanding the clinical knowledge table to new lesion types, potentially using hierarchical or embedding-based approaches\n  - Provide open-source alternatives to GPT-4 for attribute extraction and validate their effectiveness in maintaining segmentation performance",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Malenia*, a vision‚Äìlanguage pre-training framework for 3D zero-shot lesion segmentation in CT scans. The approach aims to bridge the gap between image-level knowledge and pixel-level segmentation by incorporating multi-scale mask‚Äìattribute alignment and a Cross-Modal Knowledge Injection (CMKI) module. It decomposes radiology reports into eight structured visual attributes and aligns these with mask representations through a multi-positive contrastive loss. Experiments conducted on three datasets (MSD, KiTS23, and an in-house dataset) across 12 lesion categories report notable improvements over previous methods. The paper is technically ambitious and well-motivated but would benefit from clearer mathematical formulation, stronger experimental validation, and more justification of design choices to enhance reproducibility and generalizability.  \n\n**Major Comments**  \n1. **Mathematical Formulation and Clarity** ‚Äì Equations lack adequate explanation. The computation of the expectation in the multi-positive contrastive loss (Eq. 2) is unclear, notation for normalized embeddings is omitted, and the ‚àöC scaling factor (Eq. 5) is unjustified. These issues limit theoretical transparency and reproducibility.  \n2. **Experimental Design and Evaluation** ‚Äì The private dataset includes only 30 cases per lesion type, which is insufficient for robust evaluation. Cross-validation is inconsistently applied (performed only for seen lesions), and comparisons with SAM-based methods are not conditionally matched since their inference settings differ.  \n3. **Technical Approach and Novelty** ‚Äì The choice of three resolution levels for alignment appears arbitrary, and the eight attribute categories, although expert-derived, lack systematic validation. The CMKI module relies on standard attention mechanisms and offers limited architectural innovation.  \n4. **Data Dependency and Generalization** ‚Äì The framework depends heavily on manual annotation for structured attributes and a handcrafted clinical knowledge table, which constrains scalability. Moreover, reliance on GPT-4 for attribute extraction raises concerns about accessibility and reproducibility.  \n\n**Minor Comments**  \n- Clarify normalization and notation in similarity computation.  \n- Include pseudocode or derivations for mathematical sections.  \n- Provide consistent reporting of dataset splits and statistical significance.  \n\n**Summary Paragraph**  \nOverall, the paper presents a compelling framework with strong empirical results but faces challenges in formulation clarity, experimental rigor, and scalability. Strengthening theoretical justifications, expanding datasets, validating design choices, and reducing reliance on proprietary tools would enhance the work‚Äôs robustness and reproducibility.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces Malenia, a novel framework for zero-shot lesion segmentation in 3D CT scans that leverages vision-language pre-training and multi-scale mask-attribute alignment. The authors address the challenge of transferring image-level knowledge to pixel-level tasks by aligning fine-grained lesion features with disease-related textual representations. They incorporate a Cross-Modal Knowledge Injection (CMKI) module to enhance both visual and textual features. The method is validated across three datasets and 12 lesion categories, showcasing superior performance compared to existing approaches.\n\n## Major Comments\n1. Novelty and Positioning: While the multi-scale mask-attribute alignment and CMKI module are innovative, the manuscript could benefit from a more thorough discussion of how these contributions differ from and improve upon existing work in vision-language pre-training and zero-shot segmentation. There is a need to clarify the unique contributions of Malenia relative to similar approaches, such as ZePT and other SAM-based methods.\n\n2. Evaluation Design: The experimental validation is extensive, covering both seen and unseen lesions across multiple datasets. However, the manuscript should include a more detailed analysis of how Malenia performs on specific lesion types with varying characteristics (e.g., lesions with ambiguous boundaries, complex shapes). Additionally, the evaluation should consider the impact of different imaging modalities and anatomical regions to assess generalizability.\n\n3. Comparisons: The manuscript compares Malenia with several state-of-the-art methods, but it could benefit from a more comprehensive comparison against recent diffusion-based and transformer-based models specifically designed for medical image segmentation. This would provide a clearer picture of Malenia's relative performance and its place in the evolving landscape of medical image analysis.\n\n4. Reproducibility: The manuscript mentions that code will be released, but it lacks sufficient detail regarding the training protocols, preprocessing steps, and hyperparameters. Providing a more detailed description of these aspects is crucial for ensuring reproducibility. The manuscript should also clarify the extent to which the model's performance is dependent on specific implementation choices or dataset characteristics.\n\n## Minor Comments\n1. Figure Clarity: Figures 3 and 6 are cluttered; presenting fewer representative slices with zoomed-in regions would improve readability.\n   \n2. Notation Consistency: Section 2.1 introduces notation without sufficient explanation; in particular, the forward operator is denoted inconsistently.\n   \n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition.\n   \n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" (p. 6), \"undersampling maskes\" (p. 7) should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: zero-shot lesion segmentation in 3D medical images. The proposed multi-scale mask-attribute alignment and CMKI module are technically innovative and show promise in bridging the gap between vision-language pre-training and pixel-level segmentation tasks. The evaluation is thorough, covering multiple datasets and lesion categories, which supports the claim of improved performance. However, the manuscript could benefit from a more detailed comparison with recent methods and a clearer distinction from existing work. The reproducibility of the approach is somewhat limited due to incomplete methodological details. Overall, while the idea has merit, the current evidence partially meets the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand comparative analysis, strengthen validation across specific lesion types and imaging modalities, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Malenia*, a framework for zero-shot lesion segmentation in 3D CT scans that leverages vision‚Äìlanguage pre-training with multi-scale mask‚Äìattribute alignment. The approach aims to transfer image-level semantic knowledge to pixel-level segmentation by aligning lesion features with textual disease descriptions and incorporating a Cross-Modal Knowledge Injection (CMKI) module to enhance multimodal representations. The model is evaluated on three datasets encompassing twelve lesion categories and reportedly achieves superior performance relative to prior methods. Overall, the paper addresses an important and challenging problem with promising methodological innovations, though several aspects need further clarification and strengthening.\n\n**Major Comments**  \n1. **Novelty and Positioning**: The multi-scale mask‚Äìattribute alignment and CMKI module appear original, yet the manuscript should better articulate how these elements differ from prior work in vision‚Äìlanguage pre-training and zero-shot segmentation. A clearer positioning of *Malenia* relative to existing methods such as ZePT and SAM-based approaches is needed to establish its unique contribution.  \n2. **Evaluation Design**: While the experiments cover both seen and unseen lesion types across several datasets, the analysis would benefit from deeper case-level evaluation. Specifically, performance trends for lesions of varying shape complexity, boundary ambiguity, and anatomical location should be discussed, as should the method‚Äôs generalizability across different imaging modalities.  \n3. **Comparisons**: Current comparisons are informative but not exhaustive. Additional benchmarking against recent diffusion-based and transformer-based segmentation models would help situate *Malenia* in the broader context of contemporary medical image analysis.  \n4. **Reproducibility**: Although the authors state that code will be released, details on training protocols, preprocessing, and hyperparameter choices are insufficient. More explicit documentation is required to ensure experimental reproducibility and to clarify sensitivity to dataset or implementation variations.\n\n**Minor Comments**  \n1. **Figures**: Figures‚ÄØ3 and‚ÄØ6 are visually crowded; using fewer representative slices with enlarged views would improve clarity.  \n2. **Notation**: In Section‚ÄØ2.1, some symbols, including the forward operator, are introduced without explanation and used inconsistently.  \n3. **Acronyms**: Certain abbreviations (e.g., ‚ÄúR=4‚Äù) lack definitions.  \n4. **Typos**: Correct minor errors such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes.‚Äù\n\n**Summary Paragraph**  \nThe study tackles a clinically relevant problem and proposes technically interesting mechanisms to bridge vision‚Äìlanguage modeling with pixel-level segmentation. Experimental results are promising and support the potential utility of the method. However, the work requires clearer differentiation from existing frameworks, broader comparative evaluation, and more transparent methodological documentation to fully substantiate its claims.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The manuscript should expand comparative analyses, refine evaluation across lesion types and modalities, and provide complete implementation details to enhance clarity and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## UNLEASHING THE POTENTIAL OF VISION-LANGUAGE PRE-TRAINING FOR 3D ZERO-SHOT LESION SEGMENTATION VIA MASK-ATTRIBUTE ALIGNMENT\n\n### Summary\n\nThe paper proposes Malenia, a 3D zero-shot lesion segmentation framework that aligns mask tokens with structured textual descriptions of fundamental disease visual attributes (e.g., location, shape, density) through a multi-scale, multi-positive contrastive objective. It further introduces a Cross-Modal Knowledge Injection (CMKI) module that fuses matched mask and text embeddings to produce segmentation masks, and demonstrates improved performance over several recent zero-shot baselines across three datasets and 12 lesion categories.\n\n### Strengths\n\n- Technical novelty and innovationLeveraging mask-level (rather than pixel-level) alignment to bridge image-level pretraining and dense segmentation is well motivated and aligns with emerging mask-text literature; the extension to 3D lesion segmentation is timely.The decomposition of reports into shared visual attributes and the use of multi-positive contrastive learning is a thoughtful way to handle attribute sharing across seen and unseen diseases, encouraging extensible representations.The CMKI module provides a principled mechanism for bidirectional refinement between visual mask tokens and text embeddings, and the two-branch ensembling is simple and effective.\n- Experimental rigor and validationEvaluation spans multiple public datasets (MSD, KiTS23) and a private clinical dataset, covering 12 lesion types with both seen and unseen categories.Ablations tease apart the contributions of multi-scale alignment, attribute structuring, and multi-positive contrastive loss; CMKI components are also carefully dissected (TE vs MT vs deep fusion).\n- Clarity of presentationThe overall pipeline and the four-step inference workflow are clearly explained, with helpful conceptual illustrations of mask-token partitioning, matching, and fusion.Mathematical description of the MP-NCE loss and multi-scale application is adequate for replication in principle.\n- Significance of contributionsZero-shot lesion segmentation in 3D remains a challenging and impactful problem; the proposed approach represents a substantive step towards fine-grained, attribute-aware generalization beyond closed-set training.The notion of storing attribute embeddings for prompt-free inference and mapping predicted attributes to diseases via a clinical knowledge table could have practical utility in real-world workflows.\n\n- Leveraging mask-level (rather than pixel-level) alignment to bridge image-level pretraining and dense segmentation is well motivated and aligns with emerging mask-text literature; the extension to 3D lesion segmentation is timely.\n- The decomposition of reports into shared visual attributes and the use of multi-positive contrastive learning is a thoughtful way to handle attribute sharing across seen and unseen diseases, encouraging extensible representations.\n- The CMKI module provides a principled mechanism for bidirectional refinement between visual mask tokens and text embeddings, and the two-branch ensembling is simple and effective.\n\n- Evaluation spans multiple public datasets (MSD, KiTS23) and a private clinical dataset, covering 12 lesion types with both seen and unseen categories.\n- Ablations tease apart the contributions of multi-scale alignment, attribute structuring, and multi-positive contrastive loss; CMKI components are also carefully dissected (TE vs MT vs deep fusion).\n\n- The overall pipeline and the four-step inference workflow are clearly explained, with helpful conceptual illustrations of mask-token partitioning, matching, and fusion.\n- Mathematical description of the MP-NCE loss and multi-scale application is adequate for replication in principle.\n\n- Zero-shot lesion segmentation in 3D remains a challenging and impactful problem; the proposed approach represents a substantive step towards fine-grained, attribute-aware generalization beyond closed-set training.\n- The notion of storing attribute embeddings for prompt-free inference and mapping predicted attributes to diseases via a clinical knowledge table could have practical utility in real-world workflows.\n\n### Weaknesses\n\n- Technical limitations or concernsPotential leakage/ambiguity about using patient-specific reports: it is unclear whether text derived from test-set reports (or from images containing unseen classes during training) is used directly or indirectly in training or inference, which could compromise the strictness of the zero-shot protocol.The approach depends on curated attribute descriptions (via GPT-4 plus radiologist editing). Generalization to institutions with different reporting styles or incomplete attributes is not assessed; robustness to attribute vocabulary/domain shift is unknown.There is an architectural ambiguity: the method is ‚Äúbuilt upon Mask2Former,‚Äù yet the implementation states nnUNet as the 3D encoder/decoder. The precise integration of the transformer decoder and pixel decoder requires clarification for reproducibility.\n- Experimental gaps or methodological issuesBaseline selection and fairness need more detail. SAM/SAM2 variants are sensitive to prompts and pre-/post-processing; it is unclear if hyperparameters/prompting strategies were tuned to strong settings. A broader set of zero-/open-vocabulary segmentation baselines would strengthen claims.No stratification by lesion size/contrast or robustness analysis to domain shift (scanner, acquisition, contrast phases). The method‚Äôs strengths on small/low-contrast lesions are asserted but not thoroughly dissected.Limited statistical analysis: confidence intervals or significance testing for zero-shot results are absent; only some seen-lesion results include mean¬±std.\n- Clarity or presentation issuesThe role and construction of the Clinical Knowledge Table (Step-IV) is underspecified: how attribute-to-disease mappings are built, whether they include unseen diseases, and how conflicts/uncertainty are handled.The storage and selection mechanism for ‚Äúall visual attribute descriptions‚Äù used at inference is not fully specified (e.g., deduplication, normalization, top-K matching, thresholds).\n- Missing related work or comparisonsMask-text alignment literature (e.g., MTA-CLIP) is closely related conceptually; positioning relative to this body of work would sharpen novelty claims, even if those methods target natural images.More discussion contrasting with ZePT‚Äôs query-knowledge alignment and organ/tumor disentangling would be useful, including potential complementarities or failure modes.\n\n- Potential leakage/ambiguity about using patient-specific reports: it is unclear whether text derived from test-set reports (or from images containing unseen classes during training) is used directly or indirectly in training or inference, which could compromise the strictness of the zero-shot protocol.\n- The approach depends on curated attribute descriptions (via GPT-4 plus radiologist editing). Generalization to institutions with different reporting styles or incomplete attributes is not assessed; robustness to attribute vocabulary/domain shift is unknown.\n- There is an architectural ambiguity: the method is ‚Äúbuilt upon Mask2Former,‚Äù yet the implementation states nnUNet as the 3D encoder/decoder. The precise integration of the transformer decoder and pixel decoder requires clarification for reproducibility.\n\n- Baseline selection and fairness need more detail. SAM/SAM2 variants are sensitive to prompts and pre-/post-processing; it is unclear if hyperparameters/prompting strategies were tuned to strong settings. A broader set of zero-/open-vocabulary segmentation baselines would strengthen claims.\n- No stratification by lesion size/contrast or robustness analysis to domain shift (scanner, acquisition, contrast phases). The method‚Äôs strengths on small/low-contrast lesions are asserted but not thoroughly dissected.\n- Limited statistical analysis: confidence intervals or significance testing for zero-shot results are absent; only some seen-lesion results include mean¬±std.\n\n- The role and construction of the Clinical Knowledge Table (Step-IV) is underspecified: how attribute-to-disease mappings are built, whether they include unseen diseases, and how conflicts/uncertainty are handled.\n- The storage and selection mechanism for ‚Äúall visual attribute descriptions‚Äù used at inference is not fully specified (e.g., deduplication, normalization, top-K matching, thresholds).\n\n- Mask-text alignment literature (e.g., MTA-CLIP) is closely related conceptually; positioning relative to this body of work would sharpen novelty claims, even if those methods target natural images.\n- More discussion contrasting with ZePT‚Äôs query-knowledge alignment and organ/tumor disentangling would be useful, including potential complementarities or failure modes.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe multi-positive contrastive alignment is an appropriate objective given multiple attributes per lesion; using MP-NCE is a principled choice. However, the construction of positives/negatives must ensure no inadvertent positives for background or false negatives due to overlapping attributes; a description of safeguards (e.g., attribute normalization, synonym handling) would improve rigor.Multi-scale alignment is technically sensible for lesions with diverse sizes and morphologies. Detailing how bipartite matching is done per scale (and stability across scales) would help others reproduce results.CMKI‚Äôs bidirectional cross-attention/self-attention is a reasonable design, and the dual-branch matching to pixel features is consistent with masked attention paradigms; an ablation on the weighting (Œ≤1, Œ≤2) and the necessity of both branches across tasks would be informative.\n- Experimental evaluation assessmentThe datasets and splits are appropriate; however, more transparent counts (cases per class per set, training/test volumes per seen/unseen category) should be included in the main text. Appendix references help but key numbers should be surfaced.Zero-shot protocol needs explicit guarantees that no test-set reports or test-specific attribute texts are used to construct stored embeddings. If images with unseen lesions appear during training without segmentation labels, clarify whether their reports/attributes are used in any form.Efficiency results are deferred to the appendix; reporting wall-clock times, memory usage, and throughput for 96√ó96√ó96 patches, along with N=16 mask tokens sensitivity, would be valuable to practitioners.Additional robustness tests would strengthen the paper: performance vs lesion size bins, sensitivity to attribute noise (e.g., removing one or more attributes), and to text encoder choice (Clinical-BERT vs BioBERT/BlueBERT).\n- Comparison with related work (using the summaries provided)SAM2 (Yamagishi et al., 2024) shows strong performance for large, well-bounded organs but struggles on small/ambiguous structures; Malenia‚Äôs attribute-centric design is better aligned to lesion heterogeneity. Explicitly analyzing performance on small/low-contrast lesions could reinforce this contrast.ZePT (Jiang et al., 2024) performs query-disentangling with GPT-4/Clinical-BERT class descriptions; Malenia‚Äôs innovation is aligning to elemental attributes rather than class-level prompts and using multi-positive alignment. A controlled experiment replacing Malenia‚Äôs attributes with disease-name prompts would quantify the gain due to attribute decomposition.Mask-text alignment (e.g., MTA-CLIP) demonstrates that mask-text is preferable to pixel-text alignment in natural images. Positioning Malenia as a 3D medical instantiation that moves from class-level to attribute-level alignment is a compelling narrative; a discussion of differences in training dynamics, matching strategies, and inference usage would contextualize contributions.\n- Discussion of broader impact and significanceThe attribute-level design is clinically meaningful and could support explainability (e.g., reporting which attributes drove a segmentation). Demonstrations of attribute attribution maps or per-attribute confidence would enhance trust.Dependence on curated, institution-specific attribute vocabulary raises generalization and equity concerns; building open, standardized attribute ontologies and evaluating cross-institution generalization would be impactful next steps.The four-step inference, notably Step-IV disease identification via attribute querying, suggests a pathway towards integrated diagnosis; ethical considerations and calibrating uncertainty for clinical use should be discussed.Data governance is briefly addressed; releasing de-identified attribute templates and code for the semi-automatic structuring pipeline (prompts, QA guidelines) would materially benefit the community.\n\n- The multi-positive contrastive alignment is an appropriate objective given multiple attributes per lesion; using MP-NCE is a principled choice. However, the construction of positives/negatives must ensure no inadvertent positives for background or false negatives due to overlapping attributes; a description of safeguards (e.g., attribute normalization, synonym handling) would improve rigor.\n- Multi-scale alignment is technically sensible for lesions with diverse sizes and morphologies. Detailing how bipartite matching is done per scale (and stability across scales) would help others reproduce results.\n- CMKI‚Äôs bidirectional cross-attention/self-attention is a reasonable design, and the dual-branch matching to pixel features is consistent with masked attention paradigms; an ablation on the weighting (Œ≤1, Œ≤2) and the necessity of both branches across tasks would be informative.\n\n- The datasets and splits are appropriate; however, more transparent counts (cases per class per set, training/test volumes per seen/unseen category) should be included in the main text. Appendix references help but key numbers should be surfaced.\n- Zero-shot protocol needs explicit guarantees that no test-set reports or test-specific attribute texts are used to construct stored embeddings. If images with unseen lesions appear during training without segmentation labels, clarify whether their reports/attributes are used in any form.\n- Efficiency results are deferred to the appendix; reporting wall-clock times, memory usage, and throughput for 96√ó96√ó96 patches, along with N=16 mask tokens sensitivity, would be valuable to practitioners.\n- Additional robustness tests would strengthen the paper: performance vs lesion size bins, sensitivity to attribute noise (e.g., removing one or more attributes), and to text encoder choice (Clinical-BERT vs BioBERT/BlueBERT).\n\n- SAM2 (Yamagishi et al., 2024) shows strong performance for large, well-bounded organs but struggles on small/ambiguous structures; Malenia‚Äôs attribute-centric design is better aligned to lesion heterogeneity. Explicitly analyzing performance on small/low-contrast lesions could reinforce this contrast.\n- ZePT (Jiang et al., 2024) performs query-disentangling with GPT-4/Clinical-BERT class descriptions; Malenia‚Äôs innovation is aligning to elemental attributes rather than class-level prompts and using multi-positive alignment. A controlled experiment replacing Malenia‚Äôs attributes with disease-name prompts would quantify the gain due to attribute decomposition.\n- Mask-text alignment (e.g., MTA-CLIP) demonstrates that mask-text is preferable to pixel-text alignment in natural images. Positioning Malenia as a 3D medical instantiation that moves from class-level to attribute-level alignment is a compelling narrative; a discussion of differences in training dynamics, matching strategies, and inference usage would contextualize contributions.\n\n- The attribute-level design is clinically meaningful and could support explainability (e.g., reporting which attributes drove a segmentation). Demonstrations of attribute attribution maps or per-attribute confidence would enhance trust.\n- Dependence on curated, institution-specific attribute vocabulary raises generalization and equity concerns; building open, standardized attribute ontologies and evaluating cross-institution generalization would be impactful next steps.\n- The four-step inference, notably Step-IV disease identification via attribute querying, suggests a pathway towards integrated diagnosis; ethical considerations and calibrating uncertainty for clinical use should be discussed.\n- Data governance is briefly addressed; releasing de-identified attribute templates and code for the semi-automatic structuring pipeline (prompts, QA guidelines) would materially benefit the community.\n\n### Questions for Authors\n\n- Zero-shot protocol: Do any training-time texts (reports/attributes) originate from cases containing unseen lesions (even if unlabeled)? If so, how do you prevent leakage of unseen disease information through text?\n- At inference, you ‚Äústore the text features of all visual attribute descriptions.‚Äù Are these the unique attribute phrases from training only, or do they include patient-specific phrasings? How are synonyms/duplicates handled and how large is this inventory?\n- Clinical Knowledge Table: How is it constructed for unseen diseases? Is it derived from external knowledge, codified by clinicians, or learned? How do you resolve conflicting attribute matches and determine thresholds/top-K for disease mapping?\n- Architectural clarity: You state Malenia is built on Mask2Former but also use nnUNet as the 3D encoder/decoder. Could you provide a precise component diagram clarifying which parts come from Mask2Former (transformer decoder, mask tokens) and which from nnUNet (feature pyramid)?\n- Matching at multiple scales: Do you perform independent bipartite matching at each scale? If matches disagree across scales, how are conflicts handled when computing Lsim?\n- CMKI ablations: How sensitive are results to the ensemble weights Œ≤1/Œ≤2, number of CMKI layers, and the choice of using self-attention in addition to cross-attention?\n- Attribute robustness: What happens if one or more of the eight attributes are missing or noisy? Can the model operate with a subset of attributes, and how does performance degrade?\n- Text encoder choice: Why Clinical-BERT over other medical encoders (e.g., PubMedBERT, Bio+Clinical BERT variants)? Can you share a comparison?\n- Baselines: For SAM/SAM2 and prompt-free adaptations, what prompt generation and hyperparameter settings were used? Were they tuned per dataset? Could you provide more details to ensure fairness?\n- Complexity: What are training/inference runtimes and GPU memory footprints per volume/patch, and how do they scale with N (mask tokens)?\n- Small lesions: Can you report performance stratified by lesion size and contrast? Where does Malenia most help relative to baselines?\n- Reproducibility: Will you release the structured-attribute extraction prompts, annotation guidelines, and the Clinical Knowledge Table to facilitate replication?\n\n### Overall Assessment\n\nMalenia presents a compelling approach to 3D zero-shot lesion segmentation by shifting from class-level or global text alignment to fine-grained, multi-scale mask‚Äìattribute alignment, complemented by a cross-modal fusion (CMKI) module. The method is well-motivated, reasonably designed, and demonstrates consistent gains over several recent baselines across multiple datasets. The paper‚Äôs core ideas‚Äîmulti-positive attribute alignment and bidirectional mask‚Äìtext fusion‚Äîare likely to be influential for open-vocabulary medical segmentation. However, several aspects require clarification and strengthening before publication at a top-tier venue: ensuring a rigorously leak-free zero-shot protocol (especially regarding text usage), expanding and detailing baseline fairness and robustness analyses, resolving architectural ambiguities, and providing more transparency on the attribute inventory and disease mapping. Addressing these points, along with releasing the structuring pipeline resources, would substantially improve the paper‚Äôs rigor, reproducibility, and impact.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **Malenia**, a framework for 3D zero-shot lesion segmentation that aligns visual mask tokens with textual descriptions of disease attributes (e.g., location, shape, density) using a multi-scale, multi-positive contrastive objective. A Cross-Modal Knowledge Injection (CMKI) module fuses mask and text embeddings to generate segmentations. The approach is evaluated across three datasets and twelve lesion categories, demonstrating notable improvements over recent zero-shot baselines. The manuscript is generally clear, with well-organized methodological and experimental descriptions.\n\n---\n\n**Major Comments**  \n1. **Protocol and Data Clarity**: The zero-shot setup risks text leakage from test cases; explicit guarantees that no test-set reports or unseen disease texts inform training are needed. The construction and usage of stored attribute texts and the Clinical Knowledge Table require clearer explanation.  \n2. **Dependence on Curated Attributes**: Attribute descriptions were partly generated via GPT-4 and expert review. Generalization to new institutions or reporting vocabularies is not assessed, leaving uncertainty about robustness under domain shifts.  \n3. **Architectural Ambiguity**: The paper states it builds on Mask2Former but simultaneously employs nnUNet components. A detailed clarification of component integration (transformer decoder vs. convolutional feature extractor) is required for reproducibility.  \n4. **Baseline Fairness and Statistical Rigor**: Comparisons with SAM/SAM2 and other zero-/open-vocabulary segmentation methods lack sufficient detail on prompt engineering and hyperparameter tuning. Robustness to lesion size, contrast, and domain variation is under-evaluated. Statistical significance and confidence intervals are largely absent.  \n5. **Missing Contextualization**: Related work on mask‚Äìtext alignment (e.g., MTA-CLIP) and models like ZePT should be discussed to better articulate novelty and conceptual positioning.  \n6. **Technical and Efficiency Details**: Transparent reporting of dataset splits, wall-clock times, memory usage, and parameter sensitivity (mask token number, CMKI weighting) would aid reproducibility.  \n\n---\n\n**Minor Comments**  \n- Clarify how visual attribute descriptions are deduplicated and normalized for inference.  \n- Specify how conflicts and uncertainty are resolved when mapping attributes to diseases in the Clinical Knowledge Table.  \n- Provide legends and precise labeling for architectural diagrams and formulas.  \n- Consider including per-lesion-size and per-contrast performance analyses.  \n- Typographical and formatting issues are minor and do not impede comprehension.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper presents a technically innovative contribution that advances attribute-level alignment for 3D medical segmentation. Its strengths lie in conceptual novelty, clear motivation, and solid empirical results. Yet, concerns remain regarding protocol integrity, baseline transparency, architectural clarity, and statistical robustness. Addressing these issues and providing details on efficiency and data governance would considerably strengthen the work‚Äôs reproducibility and credibility.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper introduces promising and original ideas but needs clarification of methodology, stronger experimental validation, and clearer documentation to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical challenge of zero-shot 3D lesion segmentation, where the goal is to segment lesions from categories not encountered during training‚Äîa common scenario in clinical practice due to the diversity of pathological conditions and the difficulty of collecting comprehensive annotated datasets. The authors propose Malenia, a novel vision-language pre-training framework that bridges the gap between image-level pre-training and pixel-level segmentation through multi-scale lesion-level mask-attribute alignment. The key innovation lies in structuring medical reports into eight fundamental visual attributes (location, shape, density, etc.) and aligning these with mask tokens at multiple resolution scales using a multi-positive contrastive loss. The authors further introduce a Cross-Modal Knowledge Injection (CMKI) module that enhances both visual and textual representations through mutual information exchange. Extensive experiments across three datasets (MSD, KiTS23, and an in-house dataset) with 12 diverse lesion categories demonstrate that Malenia significantly outperforms state-of-the-art methods, achieving average DSC improvements of 6.40%, 8.14%, and 9.08% over the best competing approaches on the respective datasets. The method also shows strong performance on seen lesions, with average improvements of 1.46% DSC on MSD and 1.35% on KiTS23 compared to fully supervised baselines.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- The paper tackles a clinically significant yet underexplored problem: zero-shot lesion segmentation in 3D medical imaging, which has substantial potential for real-world deployment where new lesion types constantly emerge.\n- The multi-scale mask-attribute alignment strategy is well-motivated by clinical practice and represents a significant advancement over previous vision-language approaches that used single-scale alignment or generic report text.\n- The decomposition of medical reports into eight structured disease attributes provides a fine-grained alignment mechanism that better captures the visual semantics of lesions than previous methods.\n- The Cross-Modal Knowledge Injection module effectively leverages complementary information from both modalities, with ablation studies demonstrating its clear contribution to performance.\n- The comprehensive evaluation across three diverse datasets with 12 lesion categories, including both seen and unseen lesions, provides convincing evidence of the method's effectiveness.\n\n### Major Limitations\n- The method is currently limited to CT scans, and no experiments are presented for other modalities (MRI, X-ray), which limits the demonstrated generalizability of the approach despite the clinical importance of multi-modality applications.\n- The requirement for structured attribute descriptions (rather than raw reports) may present a significant barrier to clinical adoption, as current medical reporting practices do not typically follow this structured format.\n- The paper does not adequately address computational efficiency concerns for clinical deployment, particularly regarding inference time for 3D segmentation, which is critical for clinical utility.\n- Cross-domain generalization (e.g., from abdominal to brain lesions) is not evaluated, which limits the demonstration of the method's true zero-shot capability across diverse anatomical regions.\n\n### Minor Comments\n- The ablation studies are thorough but could better quantify the individual contribution of each component (e.g., through more precise percentage improvements for specific lesion types).\n- Figure 3 (qualitative results) could benefit from higher resolution or clearer labeling to better illustrate the segmentation differences between methods.\n- The comparison with ZePT (the most relevant baseline) would be strengthened by a more detailed technical discussion in the main text rather than relegating it to the appendix.\n- The discussion on limitations (Section I) is good but could be expanded with specific, actionable suggestions for addressing each limitation in future work.\n\n## 3. Evaluation Along TMI Editorial Criteria\n\n**Significance (High):** The paper addresses an important clinical challenge with high practical relevance. The ability to segment unseen lesions without requiring additional training data or expert prompting has substantial potential to improve clinical workflow efficiency and diagnostic capabilities, particularly in resource-constrained settings where comprehensive training data for rare conditions is unavailable.\n\n**Innovation (High):** The work presents several novel contributions that advance the state of the art. The multi-scale mask-attribute alignment strategy, decomposition of reports into structured visual attributes, multi-positive contrastive learning framework, and the CMKI module collectively represent a significant conceptual and technical advancement over previous vision-language pre-training approaches for medical segmentation. The clinical motivation for using attribute-based alignment rather than raw report text is particularly compelling.\n\n**Evaluation (Good):** The evaluation is comprehensive across multiple datasets and lesion types, with appropriate metrics and statistical analysis. The ablation studies effectively demonstrate the contribution of each component. However, the evaluation would be strengthened by more detailed analysis of failure cases, particularly for the challenging \"Specific Features\" attribute where performance is lowest (82-85% DSC), and by more direct comparison with the most relevant baselines in the main text.\n\n**Reproducibility (Good):** The paper provides clear implementation details, including hyperparameters, training procedures, and dataset descriptions. The authors mention they will open-source their code and attribute annotations, which significantly enhances reproducibility. However, the dataset preprocessing details could be more explicit (e.g., exact HU windowing parameters), and the paper would benefit from more specific computational resource requirements for training and inference.\n\n## 4. Decision Recommendation\n\nMinor Revision\n\nThe paper presents a significant contribution to the field of zero-shot medical image segmentation with a well-motivated, innovative approach and strong experimental validation. The method demonstrates clear advantages over state-of-the-art techniques and addresses an important clinical need. However, to strengthen the paper for TMI publication, the authors should: 1) Address the limitation regarding modality generalization by discussing how the approach might extend to other imaging modalities; 2) Provide more concrete details on computational efficiency for clinical deployment; 3) Enhance the main text with more direct technical comparison to the most relevant baseline (ZePT); and 4) Expand the limitations section with specific suggestions for future work. These revisions would address the minor concerns while preserving the paper's substantial contributions and strong experimental foundation.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Malenia*, a vision-language pre-training framework for zero-shot 3D lesion segmentation. The study targets the challenge of segmenting unseen lesion categories‚Äîa clinically relevant problem given the diversity of pathologies and data scarcity. The proposed approach links image- and text-level representations by aligning multi-scale lesion masks with structured report attributes using a multi-positive contrastive loss. The Cross-Modal Knowledge Injection (CMKI) module further strengthens feature exchange across modalities. Extensive experiments on MSD, KiTS23, and an in-house dataset covering 12 lesion categories show notable Dice Similarity Coefficient (DSC) gains over existing methods. Overall, the paper is clearly written, technically sound, and addresses an important gap in clinically generalizable lesion segmentation.  \n\n**Major Comments**  \n1. **Clinical Scope and Generalizability:** The experiments are limited to CT data; there is no evidence for other modalities (e.g., MRI, X-ray). Broader validation is required to demonstrate cross-modality applicability.  \n2. **Structured Input Requirement:** The reliance on structured attribute annotations rather than free-text reports may constrain clinical adoption, as existing reporting workflows are typically unstructured.  \n3. **Computational Efficiency:** The paper does not sufficiently discuss inference time or resource requirements, which are critical for clinical deployment.  \n4. **Cross-Domain Generalization:** No results are presented across distinct anatomical domains, limiting evidence of true zero-shot transferability.  \n5. **Evaluation Details:** While ablation studies are comprehensive, clearer quantification of individual component gains and more discussion of failure cases (e.g., for specific visual attributes) would improve interpretability.  \n\n**Minor Comments**  \n- Improve Figure‚ÄØ3 resolution and labeling for better qualitative comparison.  \n- Move the comparison with ZePT from the appendix to the main text for stronger contextualization.  \n- Expand the limitations section with practical suggestions for future extensions.  \n- Provide more explicit dataset preprocessing information (e.g., HU windowing parameters).  \n\n**Summary Paragraph**  \nThis paper presents a compelling and innovative step toward generalizable lesion segmentation via structured vision-language pre-training. Strengths include clear clinical motivation, methodological novelty, and strong multi-dataset evaluation. Limitations relate primarily to generalizability across modalities and domains, and to incomplete efficiency analysis. Overall, the work is significant and reproducible, with room for moderate refinement to strengthen clinical applicability and comparative clarity.  \n\n**Decision Recommendation**  \n**Minor Revision.** The contribution is substantial and well-supported, but addressing the concerns about modality extension, computational efficiency, and comparative discussion would further solidify the paper‚Äôs readiness for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles an important limitation of current vision‚Äëlanguage pre‚Äëtraining: while such models are highly effective for image‚Äëlevel zero‚Äëshot disease classification, they fall short when it comes to voxel‚Äëlevel segmentation of lesions that have not been seen during training. To bridge this gap, the authors assemble a training set comprising lesions from twelve disease categories, each paired with a radiology report, using data drawn from the Medical Segmentation Decathlon, KiTS23, and a private collection. Evaluation is performed on both familiar and novel lesion types. The proposed system, named **Malenia**, introduces a multi‚Äëscale mask‚Äëattribute alignment that links hierarchical mask embeddings with structured textual descriptions of eight disease attributes. A multi‚Äëpositive contrastive loss is used to encourage fine‚Äëgrained correspondences. In addition, a Cross‚ÄëModal Knowledge Injection (CMKI) module fuses mask and attribute embeddings through cross‚Äëattention, producing the final segmentation. Experiments on three datasets show consistent improvements in Dice and Normalized Surface Distance for unseen lesions, demonstrating the benefit of the mask‚Äëattribute alignment and the cross‚Äëmodal fusion strategy.  \n\n---  \n\n## General feedback  \n\n- **Relevance:** Extending vision‚Äëlanguage pre‚Äëtraining to voxel‚Äëlevel segmentation of previously unseen pathologies addresses a clear need for flexible clinical AI tools (see Introduction).  \n- **Originality:** The combination of multi‚Äëscale mask‚Äëattribute alignment, a multi‚Äëpositive contrastive objective, and the CMKI fusion module represents a novel departure from earlier single‚Äëscale, single‚Äësentence approaches such as ZePT and CT‚ÄëGLIP (Sections‚ÄØ3.1‚Äë3.2, Figure‚ÄØ1).  \n- **Experimental scope:** The authors benchmark Malenia against five recent zero‚Äëshot methods (Table‚ÄØ1) and four supervised baselines (Table‚ÄØ2), reporting Dice gains of 6‚Äì9‚ÄØ% on unseen lesions. Ablation experiments (Tables‚ÄØ3‚Äë4, Figures‚ÄØ4‚Äë5) provide a clear picture of each component‚Äôs contribution. However, the lack of statistical significance testing and the relatively small size of the private in‚Äëhouse dataset (‚âà30 cases per lesion) limit the strength of these claims (Table‚ÄØ1, Table‚ÄØ5).  \n- **Reproducibility:** The promise to release code and attribute annotations, together with a description of basic training settings (optimizer, scheduler, batch size) in Section‚ÄØ4.1, is encouraging. Nonetheless, essential details‚Äîsuch as the total number of training epochs, data‚Äëaugmentation strategies, the values of the loss‚Äëweighting coefficients (Œª‚ÇÅ‚ÄëŒª‚ÇÉ), and the mechanism for storing and retrieving attribute embeddings (Figure‚ÄØ2)‚Äîare omitted. The dependence on GPT‚Äë4 for attribute extraction also raises reproducibility concerns, even though an open‚Äësource LLM alternative is examined (Table‚ÄØ8).  \n\n---  \n\n## Specific comments / critiques  \n\n1. **Statistical validation:** Dice scores in Tables‚ÄØ1 and‚ÄØ2 are presented without any measure of variability (e.g., standard deviations, confidence intervals, or p‚Äëvalues). Consequently, it is difficult to judge whether the reported 6‚Äì9‚ÄØ% improvements are statistically reliable.  \n\n2. **Training protocol details:** The manuscript does not specify the total number of training epochs, the exact learning‚Äërate schedule, the data‚Äëaugmentation pipeline, or the numeric values of the loss‚Äëweighting factors Œª‚ÇÅ, Œª‚ÇÇ, Œª‚ÇÉ (Section‚ÄØ3.3). These omissions hinder reproducibility.  \n\n3. **Transparency of attribute extraction:** The attribute extraction pipeline relies on GPT‚Äë4 prompts followed by radiologist correction (Section‚ÄØ3.1). However, the exact prompts, the amount of manual effort required, and the full workflow are not disclosed. While Table‚ÄØ8 shows comparable performance with an open‚Äësource LLM, the contribution of human refinement‚Äîwhich appears to boost results substantially‚Äîis not quantified in the primary experiments.  \n\n4. **CMKI architecture specifics:** The description of the CMKI module remains high‚Äëlevel (Section‚ÄØ3.2). Details such as the number of attention heads, the dimensionality of the feed‚Äëforward layers, and the balance between cross‚Äë and self‚Äëattention layers are missing. Table‚ÄØ4 documents overall gains but does not isolate the effect of each fusion step.  \n\n5. **Mask‚Äëtoken count justification:** Figure‚ÄØ4 indicates a performance drop when using fewer than 12 tokens, leading the authors to adopt N‚ÄØ=‚ÄØ16 as the default. Nevertheless, the study does not explore dataset‚Äëspecific tuning (e.g., adjusting token count based on lesion class frequency within a volume).  \n\n6. **Limited size of the private dataset:** The in‚Äëhouse dataset comprises only 30 scans per lesion type (Table‚ÄØ5) and provides scant information about acquisition protocols. This raises concerns about potential bias and limits the assessment of generalizability beyond the three public datasets.  \n\n7. **Zero‚Äëshot claim nuances:** Malenia depends on pre‚Äëcomputed ‚Äústored attribute embeddings‚Äù derived from the training set (Section‚ÄØ3.3). It is unclear how the system would handle a truly novel disease that lacks any prior attribute description, which somewhat contradicts a strict zero‚Äëshot scenario.  \n\n8. **Computational cost reporting:** Table‚ÄØ10 lists FLOPs for a 96¬≥ input, but does not include real‚Äëworld inference latency, memory footprint, or runtime on full‚Äësize CT volumes‚Äîinformation that is essential for evaluating clinical feasibility.  \n\n9. **Qualitative analysis without quantitative error metrics:** Figures‚ÄØ3 and‚ÄØ7 present appealing visual examples, yet there is no accompanying quantitative analysis of false‚Äëpositive/false‚Äënegative rates or boundary accuracy.  \n\n10. **Manuscript polish:** The text contains typographical errors (e.g., ‚ÄúSegmenTA## TION‚Äù) and inconsistent spacing that distract from the scientific content and should be corrected.  \n\n---  \n\n## A suggested decision  \n\n**Major Revision** ‚Äì While the work is promising and tackles a worthwhile problem, the manuscript requires additional baseline comparisons, statistical analyses, and detailed methodological information before it can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Malenia*, a vision‚Äìlanguage framework designed to extend zero‚Äëshot image‚Äëlevel classification capabilities to voxel‚Äëlevel lesion segmentation, particularly for unseen disease categories. The method integrates multi‚Äëscale mask‚Äëattribute alignment and a Cross‚ÄëModal Knowledge Injection (CMKI) module to link visual and textual representations through a multi‚Äëpositive contrastive objective. Training data encompass lesions from twelve disease classes associated with radiology reports, drawn from multiple public and private datasets. Experimental results on three benchmark datasets indicate consistent improvements in Dice and Normalized Surface Distance for novel lesion types. Overall, the study addresses an important limitation in current medical vision‚Äìlanguage models, though several aspects require clarification and validation.  \n\n**Major Comments**  \n1. **Statistical reliability:** The reported Dice gains (6‚Äì9%) are presented without measures of variability or significance testing, limiting confidence in the claimed improvements.  \n2. **Incomplete training details:** Crucial hyperparameters‚Äîtotal epochs, learning‚Äërate schedule, data‚Äëaugmentation methods, and loss‚Äëweighting coefficients (Œª‚ÇÅ‚ÄìŒª‚ÇÉ)‚Äîare not reported, hindering reproducibility.  \n3. **Attribute extraction transparency:** The GPT‚Äë4‚Äëbased attribute pipeline lacks details on prompt design, manual correction effort, and the quantitative impact of human refinement, despite mention of an open‚Äësource alternative.  \n4. **CMKI architectural clarity:** The CMKI description omits implementation specifics such as attention‚Äëhead counts and dimensionalities. The ablation studies do not isolate contributions of individual fusion components.  \n5. **Mask‚Äëtoken configuration:** The rationale for selecting 16 tokens is only partly justified; dataset‚Äëspecific variation or sensitivity analysis is absent.  \n6. **Private dataset limitations:** The in‚Äëhouse set is small (‚âà30 scans per lesion type) and lacks acquisition details, constraining generalizability and bias assessment.  \n7. **Zero‚Äëshot scope:** Dependence on pre‚Äëstored attribute embeddings raises questions about performance on truly novel diseases lacking prior descriptors.  \n8. **Computational efficiency:** FLOPs are listed, but real‚Äëworld inference time, memory use, and processing of full‚Äëvolume CT data are not evaluated.  \n9. **Quantitative validation of visuals:** Qualitative figures lack accompanying error analyses such as false‚Äëpositive or boundary metrics.  \n\n**Minor Comments**  \n- Address typographical and spacing errors (e.g., inconsistent capitalization and word breaks).  \n- Ensure clarity and consistency in figure references and table labeling.  \n\n**Summary Paragraph**  \n*Malenia* presents an innovative adaptation of vision‚Äìlanguage pre‚Äëtraining for volumetric lesion segmentation and offers promising results on unseen pathologies. Strengths include the meaningful clinical motivation, clear modular design, and comprehensive ablation studies. However, the absence of statistical testing, limited dataset diversity, and insufficient methodological disclosure constrain the interpretability and reproducibility of the results. Resolving these issues‚Äîparticularly through transparent reporting of training procedures, architectural parameters, and validation methods‚Äîwould considerably strengthen the contribution.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The study is promising and potentially impactful, but substantial clarification, statistical validation, and methodological detail are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Shaoting Zhang",
      "Wenhui Lei",
      "Xiaofan Zhang",
      "Yankai Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_743b761d4a84ae02565015701e9c3f189a2f413f.pdf",
    "remote_url": "https://openreview.net/pdf/743b761d4a84ae02565015701e9c3f189a2f413f.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "MONICA: Benchmarking on Long-tailed Medical Image Classification",
    "status": "completed",
    "evaluators": [
      "Yixuan",
      "Justin"
    ],
    "primary_area": [
      "datasets and benchmarks"
    ],
    "keywords": [
      "Long-tailed Learning",
      "Benchmark",
      "Medical Image Classification"
    ],
    "abstract": "Long-tailed learning is considered to be an extremely challenging problem in data imbalance learning. It aims to train well-generalized models from a large number of images that follow a long-tailed class distribution. In the medical field, many diagnostic imaging exams such as dermoscopy and chest radiography yield a long-tailed distribution of complex clinical findings. Recently, long-tailed learning in medical image analysis has garnered significant attention. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often leads to unfair comparisons and inconclusive results. To help the community improve the evaluation and advance, we build a unified, well-structured codebase called Medical OpeN-source Long-taIled ClassifiCAtion (MONICA), which implements over 30 methods developed in relevant fields and evaluated on 12 long-tailed medical datasets covering 6 medical domains. Our work provides valuable practical guidance and insights for the field, offering detailed analysis and discussion on the effectiveness of individual components within the inbuilt state-of-the-art methodologies. We hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning. The codebase will be publicly available on GitHub.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper presents a framework and codebase for structured benchmarking of long-tailed (LT) learning methods on various medical image classification tasks. The benchmark, MONICA, implements over 30 LT learning methods, with comprehensive experiments assessing performance across 12 LT medical image classification datasets spanning 6 different modalities. The experiments analyze which methods, and categories of methods, provide the most benefit to LT medical image classification across tasks in a controlled environment.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- This work addresses an important problem, namely the variability in dataset/hyperparameters/etc. when evaluating LT learning methods for medical image classification tasks. These variations in setting make head-to-head comparisons difficult, so MONICA serves to provide a ‚Äúfair playing ground‚Äù for these LT learning methods.\n- The framework will become publicly available and should serve as an extensible resource going forward for LT medical image classification research.\n- The organization and presentation quality of the paper is strong, with helpful use of formatting (typesetting, color, etc.) and high-quality figures.\n- Experiments are very thorough, spanning many relevant methods, datasets, and tasks.\n- Discussion is thoughtful, going beyond simply displaying all benchmark results. The authors try to synthesize takeaways, provide caveats/limitations, assess out-of-distribution performance, and more.\n\n### Weaknesses\n\n- Writing can be improved throughout. See specific comments below for examples of awkward wording, inconsistent naming, grammatical errors, etc.\n- It is possible that choosing a fixed set of hyperparameters across methods unintentionally advantages certain methods. Ideally, one could argue that each method should be individually tuned on each task; however, I am aware that this would require a vast amount of resources and time, so I do not consider this a major limitation. More practical solutions to enhance the benchmark would be the following: (i) uncertainty estimates should be provided (e.g., bootstrapped confidence intervals or standard deviations over multiple runs), and (ii) multiple performance metrics should be provided (e.g., AUROC).\n\n### Questions\n\n- Is it possible that the chosen hyperparameters used across all methods happen to be more advantageous for certain methods and suboptimal for others? In one sense, using the same set of hyperparameters across methods appears ‚Äúfair‚Äù; however, it may actually be more fair to individually tune each method on each task. I recognize the difficulty of conducting fair comparisons in such a large-scale experimental setting, where it is costly to, e.g., run multiple trials of all experiments. I am not asking the authors to necessarily perform such experiments, but rather to consider this point and perhaps comment on it as a limitation/consideration.\n- Can the authors provide a summary of practical suggestions for which methods to use in a few sentences near the Conclusion?\n- I might suggest including the **rank** of each method on a given task in all tables. This would also enable you to *quantitatively* assess method performance across tasks (which method has the lowest average/median rank overall?). To make this work logistically (fit all columns in the table), you may need to reduce the precision to one decimal place, e.g.\n- The two paragraphs ‚ÄúLTMIC improves out-of-distribution detection‚Äù and ‚ÄúUsing imbalanced validation dataset for checkpoint selection‚Äù are not properly set up. For the former, what does it mean to use ‚ÄúImageNet as OOD samples, with 1,000 randomly selected images‚Äù? What exactly is the task, how is it formulated, and how are experiments conducted? Further, why do we care about this model behavior? For the former, Figure 4 and its findings are confusing ‚Äì why exactly does this demonstrate ‚Äústable convergence‚Äù? My general advice: **Use the methods section to describe and prepare the reader to understand everything that appears in the results**. When I come to these results sections, I should already have an idea of what experiments you have performed.\n\n**Minor comments/questions:**\n- Avoid editorializing with value judgments: ‚Äúbenchmark is **meticulously** designed‚Äù; ‚Äúwe‚Ä¶ develop a‚Ä¶ **well-structured** codebase‚Äù; ‚Äúour work provides **valuable** practical guidance‚Äù. Simply present your work and let the reader make these judgments!\n- ‚Äúdata imbalance learning‚Äù is not a phrase I have heard. Perhaps ‚Äúimbalanced learning‚Äù?\n- ‚Äúunified, strictly formulated, and comprehensive benchmark‚Äù. Unsure what ‚Äústrictly formulated‚Äù means. Could simply say ‚Äúunified, comprehensive benchmark‚Äù\n- ‚Äúwe build a‚Ä¶ codebase‚Ä¶, which implements over 30 methods‚Ä¶ and evaluated on 12‚Ä¶ datasets‚Äù. It seems that ‚Äúevaluated on‚Äù is the wrong tense; also, what is being evaluated?\n- This does not belong in an abstract: ‚ÄúWe hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning.‚Äù\n- Often unnecessary inclusion of ‚Äúthe‚Äù before concepts: ‚ÄúThe deep learning techniques‚Äù; ‚Äúthe collected image datasets‚Äù; ‚Äúthe long-tailed imbalance‚Äù\n- ‚ÄúThe deep learning techniques have proven effective for most computer vision tasks benefiting from the grown-up dataset scale.‚Äù Remove ‚ÄúThe‚Äù; what does ‚Äúgrown-up dataset scale‚Äù mean? ‚ÄúGrown-up‚Äù is not the right adjective ‚Äì be more concrete.\n- Refrain from claims like ‚Äúalways result‚Äù (line 57) ‚Äì soften to ‚Äúusually‚Äù or similar\n- Confused by this justification: ‚Äúit is vital to recognize these rare diseases in real-world practice, as they are relatively rare for doctors and may also lack diagnostic capacity.‚Äù This reads as ‚Äúit is vital to recognize rare diseases because they are rare‚Äù.\n- Line 65: can change ‚Äúcontributions, i.e.,‚Äù -> ‚Äúcontributions:‚Äù\n- Be consistent with capitalization/presentation of terms: ‚ÄúRe-sampling‚Äù vs ‚Äúre-sampling‚Äù; ‚ÄúModule improvement‚Äù vs. ‚ÄúModule Improvement‚Äù; ‚Äúmnist‚Äù vs. ‚ÄúMNIST‚Äù; ‚Äúmixup‚Äù vs. ‚ÄúMixUp‚Äù; etc.\n- Line 82: ‚Äúwe are still curious to explore‚Äù. Perhaps just ‚Äúwe aim to explore‚Äù?\n- ‚ÄúThe partition schemes are vita important‚Äù. What does ‚Äúvita‚Äù mean?\n- The last two paragraphs of the introduction are probably better off being formatted as bulleted or numbered lists. Also, it is unclear why these numbered lists are formatted differently: **1) xxx.** vs. (1) xxx.\n- ‚Äúof class $k$ where $\\rho$ denoted as imbalance ratio‚Äù. The phrase ‚Äúdenoted as‚Äù is awkward + need a comma after $k$\n- ‚Äúa common assumption in long-tailed learning is when the classes are sorted by cardinality in decreasing order‚Äù I‚Äôm not sure what this means or why this represents an ‚Äúassumption‚Äù. I would just remove this sentence since it does not seem to be used later.\n- Line 144: ‚Äúis a long-tailed version constructed from‚Äù. Need to say it is a version ‚Äúof‚Äù something; alternatively, use a word other than ‚Äúversion‚Äù like ‚Äúdataset‚Äù\n- Inconsistent spacing/use of commas in numbers. ‚Äú10, 015‚Äù -> ‚Äú10,015‚Äù; ‚Äú3200 fundus images‚Äù -> ‚Äú3,200 fundus images‚Äù\n- Inconsistent spacing around commas and colons: ‚Äútraining/off-site testing / on-site testin‚Äù; ‚Äú7 : 1: 2‚Äù; etc.\n- ‚ÄúLiver Tumor Segmentation Benchmark‚Äù -> ‚Äúthe Liver Tumor Segmentation Benchmark‚Äù\n- I realize it is hard to categorize some methods into one bin but GCL loss going in Information Augmentation is interesting, particularly since all other losses fall under re-sampling. It seems to also have module improvement as well.\n- ‚ÄúCausal classifier (Tang et al., 2020) resorted to causal inference for keeping the good and removing the bad momentum causal effects in long-tailed learning.‚Äù The phrase ‚Äúresorted to‚Äù is strange and has a negative connotation; also, what do ‚Äúgood‚Äù and ‚Äúbad‚Äù mean?\n- ‚ÄúAll these designs are for the fairness and the practicality of the comparison on the benchmark.‚Äù Too vague ‚Äì in what specific way do these support fairness?\n- Table 3: Inconsistent ‚ÄúAvg‚Äù vs ‚ÄúAvg.‚Äù vs ‚Äúavg‚Äù\n- Table 4: Consider using a line break occasionally (so one loss function occupies two rows). This would allow you to use a larger font size. Also, be consistent ‚ÄúCrossEntropy‚Äù vs ‚ÄúCE‚Äù?\n- ‚Äúassessing MixUp based solely on performance is not fair‚Äù. Soften to ‚Äúmay not be fair‚Äù\n- ‚Äúled to a significant performance decline, e.g,‚Äù. Refrain from saying ‚Äúsignificant‚Äù without statistical significance test + change ‚Äúe.g,‚Äù -> ‚Äúe.g.,‚Äù\n- ‚ÄúUse two-stage training as a general paradigm‚Äù sounds like a command. Perhaps ‚ÄúUsing‚Äù?\n- Define ‚ÄúSSL‚Äù acronym at first use\n- ‚ÄúModify classifier to reduce prediction bias‚Äù -> ‚ÄúClassifier modification to reduce prediction bias‚Äù\n- ‚ÄúIn Fig. 2, We visualize‚Äù -> ‚ÄúIn Fig. 2, we visualize‚Äù\n- Table 5 indicates the meaning of asterisk, which is never used in the table.\n- ‚Äúmodels with larger parameters‚Äù. The parameters are not ‚Äúlarger‚Äù ‚Äì could say ‚Äúmore parameters‚Äù or ‚Äúa larger parameter count‚Äù perhaps.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "ai",
          "comment": "The major comments primarily focus on textual analysis, lacking analysis of tables and figures."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **MONICA**, a benchmarking framework and accompanying codebase designed to enable fair and structured evaluation of long-tailed (LT) learning methods for medical image classification. It includes implementation of more than 30 LT methods and provides comprehensive experiments across 12 datasets representing six imaging modalities. The study aims to identify which categories of LT methods perform best under controlled conditions. The work is clearly motivated, well-organized, and visually well-presented, though the writing at times lacks polish.  \n\n**Major Comments**  \n1. **Methodological Fairness and Hyperparameters:** Using a single set of hyperparameters for all methods may unintentionally favor some approaches and disadvantage others. While individualized tuning would improve fairness, it may be infeasible at this scale. The authors should at least acknowledge this limitation and discuss potential impacts.  \n2. **Benchmark Completeness:** Including uncertainty estimates (e.g., bootstrapped confidence intervals, multiple-run standard deviations) and additional metrics such as AUROC would strengthen the benchmark and improve interpretability.  \n3. **Experimental Clarity:** Certain results sections are insufficiently motivated. For instance, experiments on out-of-distribution detection using ImageNet as OOD samples and the description of imbalanced validation datasets lack clear task definition and methodological justification. These experiments should be better explained in the Methods section to prepare readers for the results.  \n4. **Practical Guidance:** The conclusion could summarize practical recommendations‚Äîe.g., which LT methods are generally most effective‚Äîbased on the benchmark‚Äôs findings.  \n5. **Presentation of Results:** Including ranks of methods in each table could facilitate cross-task performance comparison (e.g., average or median rank).  \n\n**Minor Comments**  \n- Simplify subjective language (‚Äúmeticulously designed,‚Äù ‚Äúvaluable guidance‚Äù).  \n- Improve grammatical consistency and phrasing throughout (e.g., remove unnecessary ‚Äúthe,‚Äù revise tense, clarify ambiguous expressions).  \n- Ensure consistent capitalization and naming of terms and tables.  \n- Revise unclear sentences (‚Äúvital to recognize rare diseases because they are rare‚Äù), fix typos (‚Äúvita important‚Äù), and correct formatting inconsistencies in numbers, spacing, and notation.  \n- Clarify or soften specific phrases (‚Äúassessing MixUp solely on performance may not be fair‚Äù).  \n- Define all acronyms at first use (e.g., SSL).  \n\n**Summary Paragraph**  \nOverall, **MONICA** addresses an important gap by providing a unified, extensible benchmark for LT medical image classification. The large-scale experimentation and thoughtful discussion are valuable contributions. The primary issues concern writing clarity, presentation, and completeness of evaluation rather than technical soundness. With minor methodological clarifications and improved exposition, this work could serve as a strong community resource.  \n\n**Decision Recommendation:** **Minor Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents a unified benchmark for long-tailed learning in the medical domain by integrating several existing datasets and implementing a complete pipeline from data loading to model training and evaluation. The authors claim that the benchmark supports over 30 methods for comparison and provides an analysis of their performances.\n\n- Update after the discussion phase:\n\nThank you for the detailed responses! I've raised my score. While my concerns are not entirely resolved, I believe with careful revisions, its future version has the potential to be accepted.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThe paper attempts to provide a comprehensive benchmark for long-tailed medical image classification. The idea of integrating multiple existing methods and datasets into a unified platform could potentially be useful for researchers who want to compare various methodologies under a standardized framework.\n\n### Weaknesses\n\n1. Motivation.¬†The paper lacks sufficient justification for evaluating long-tailed problems specifically in medical imaging tasks. While the authors mention some motivations at the beginning, these arguments are not convincing. Is there a fundamental difference between long-tailed problems in medical imaging and those in conventional tasks? Would this difference necessitate different methodologies? Even if the data modalities and evaluation methods are distinct (e.g., balanced vs. imbalanced test sets), would this lead to fundamentally different approaches? The paper analyzes multiple methods based on this premise but fails to provide insightful conclusions, which further deepens my skepticism about the motivation.\n\n2. Dataset Contribution.¬†Although the paper claims to use 12 datasets, 7 of these come from MedMNIST, and several of them are derived from previous work. This reduces the originality of the dataset contribution. Furthermore, the split between multi-class and multi-label datasets is 9/3, respectively. It is worth noting that many existing studies have already utilized MedMNIST for long-tailed learning (https://scholar.google.com/scholar?cites=11226954386823169312&scipsc=1&q=long+tail). Given that 7 out of the 12 datasets in this paper are from MedMNIST, why should users choose MONICA over MedMNIST, which already has extensive use and coverage in the medical imaging field? Additionally, the experimental methods used for multi-class and multi-label datasets are almost entirely different, and the analysis of multi-label results is limited to a single vague statement that multi-label classification is more challenging. This gives the impression that multi-label datasets were included just for the sake of completeness, rather than being a key focus.\n\n3. Code Contribution.¬†The code is not provided in the appendix, nor is there an anonymous GitHub link, which means the authors' claims about the code cannot be verified. By comparison, the NeurIPS D&B track (single-blind review) usually includes dataset or code links, along with information about author affiliations, licenses, and ethics. Although such links may be added after acceptance, this suggests that such work may not be well-suited for ICLR's double-blind review process.\n\n    Additionally, the description of the code structure in Section 3.1 is not particularly informative. The modular design described is basic and lacks novel insights. A more impactful modular design, like in mmdetection, which breaks down components into backbone, neck, and bbox head, would have been more meaningful. As it stands, the description feels unnecessary.\n\n4. Experimental Analysis Lacks of Insights. Comments below:\n\n    - Despite using multiple datasets, the authors only provide a generic / systematic comparison of the methods without analyzing differences across domains. For example, there is no discussion about which methods are better suited for dermatology versus ophthalmology. Almost all discussion is very general, without any specific insights related to medical applications. This diminishes the value of using 12 datasets, as the conclusions drawn are not substantially different from what could be obtained from a single dataset.\n\n    - The analysis in Section 4.2 is poorly organized. There is no clear structure, with the discussion jumping from evaluation metrics (e.g., \"Curse of shot-based group evaluation\") to re-sampling methods, MixUp, two-stage training, and even self-supervised learning in a seemingly random fashion. Many claims are also not supported by data. The overall takeaway from the experimental section is unclear, and I did not gain any insights on how to design better models.\n\n    - In Section 4.1, there is inconsistency in the training strategies used: some methods use a unified training strategy, while others use the one specified in the original paper (e.g., SAM, Line 306), with no explanation for this discrepancy.\n\n    - There are issues with the tables, such as Table 2, where it is unclear what methods like ERM, cRT, and LWS represent, as they are not referenced properly. Additionally, Section 3.2.3 does not fully align with the table.\n\n    - The categorization of methods is confusing. The authors categorize methods into three types‚Äîclass re-sampling, information augmentation, and module improvement‚Äîbut later mention that re-sampling and MixUp are used in many methods, making the classification in Tables 2/3 somewhat meaningless.\n\n    - The discussion on self-supervised learning (Line 398) appears out of place, as it is not introduced earlier.\n\n    - Similarly, the mention of OOD detection (Line 421) is abrupt and lacks context.\n\n    - The section on using an imbalanced validation dataset for checkpoint selection is unclear about its purpose. The conclusion seems to be that GCL exhibits lower fluctuations, but the reasoning and implications are not well explained. Additionally, Figure 4 lacks labels for the x and y axes, making interpretation difficult.\n\n    - Line 475 suddenly states that multi-label classification is more challenging without providing adequate context or analysis.\n\n    - Line 504 claims that \"the most advanced long-tailed learning methods no longer focus on improving a single strategy,\" but this claim is not well-supported by the preceding analysis.\n\n### Questions\n\nSee weakness sections. Some more questions below:\n\nCould you elaborate on why the results of self-supervised learning and OOD detection are relevant in this paper? They seem out of place given the main focus on long-tailed classification.\n\nWhy did the authors not include a domain-specific analysis (e.g., which methods work better for certain medical fields)? It seems like an important missed opportunity.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a unified benchmark for long-tailed learning in medical image classification, integrating several existing datasets and supporting over 30 methods for standardized comparison. It aims to offer a comprehensive evaluation pipeline covering data loading, training, and assessment. The overall presentation is clear, but the work‚Äôs novelty and depth of analysis are limited. Following the authors‚Äô responses, the reviewer raised their overall assessment, acknowledging the effort while noting unresolved concerns that may be addressed through revision.\n\n---\n\n**Major Comments**  \n1. **Motivation and Rationale** ‚Äì The justification for focusing on long-tailed problems specifically within medical imaging is insufficient. The paper does not convincingly explain how long-tailed distributions in this domain differ from other fields or why unique methodologies are needed. Despite extensive experiments, the lack of domain-specific insights weakens the paper‚Äôs motivation.  \n2. **Dataset Contribution and Originality** ‚Äì Of the 12 datasets used, seven originate from MedMNIST, diminishing the originality of the benchmark. Since MedMNIST is already widely used for similar purposes, the added value of the proposed MONICA benchmark remains unclear. The imbalance between multi-class and multi-label datasets and the minimal analysis of multi-label results make their inclusion appear superficial.  \n3. **Code Availability and Transparency** ‚Äì No code or anonymous repository link is provided, preventing verification of claims. The description of the modular code design is basic and offers little novel insight compared with existing frameworks such as mmdetection.  \n4. **Experimental Analysis and Organization** ‚Äì The analysis lacks structure and depth. Comparisons across medical domains are absent, and discussions are mostly generic. Method categorization is inconsistent, tables and section references are unclear, and several claims lack supporting data. The flow between topics (e.g., self-supervision, OOD detection, validation strategy) is disjointed, hindering comprehension and insights.  \n5. **Specific Methodological Issues** ‚Äì Inconsistencies exist in training strategies among methods, unclear table labeling, and abrupt introductions of certain topics. Figures lack proper axis labels, and key conclusions (e.g., that multi-label classification is ‚Äúmore challenging‚Äù) are not substantiated.\n\n---\n\n**Minor Comments**  \n- Ensure consistent referencing of methods (e.g., ERM, cRT, LWS) and alignment between text and tables.  \n- Improve figure labeling and clarify section transitions.  \n- Provide stronger linking of results to claims throughout Sections 4.1‚Äì4.2.  \n- Clarify the purpose and interpretation of the imbalanced validation experiments.  \n\n---\n\n**Summary Paragraph**  \nThe paper‚Äôs goal of creating a unified long-tailed benchmark for medical imaging is potentially valuable, but current shortcomings in motivation, dataset originality, code transparency, and analysis depth limit its impact. The experimental presentation is fragmented, and key conclusions are not fully supported by results. With substantial revisions‚Äîparticularly clarifying the unique contribution, improving methodological rigor, and releasing the promised code‚Äîthe work may reach publishable quality.\n\n---\n\n**Decision Recommendation**  \n**Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors introduce the problem of long-tailed medical image classification and challenges in the field. Then they develop MONICA which is a package to benchmark various methods ranging from different loss functions, augmentations, etc on the benchmark datasets across medical image classification tasks. They provide an overview of datasets and methods and experiment results on the datasets. The authors additionally share learnings and observations.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n- A good overview of the datasets curated for this work\n- important contribution of decoupling the codebase\n- A good overview of the method approaches\n- practically useful to AI researchers in medical imaging\n\n### Weaknesses\n\n- It would help to expand the benchmark datasets and bring in a canonical set for a field such as Camlyon for Pathology, etc. WILDS (medical subset) is a great example of a dataset to bring in to this benchmarking codebase\n- Resnet-50 is used as a backbone but the community has generally moved on to more complex backbones such as ConvNext / Swin or foundation model backbones for different datasets. \n- Generally the community uses pretrained backbones rather than training the backbones from the scratch.\n- The same backbone is used for every task for fairness but generally a sweep over backbones would help since different modalities and tasks require different approaches\n- Top-1 accuracy is an in appropriate metric for model selection in imbalances settings and AUROC, AUPRC, F1 should be used\n- error bars are missing in experiments\n- More thorough error analysis\n- Clearer articulation of novel insights\n- Better connection to clinical relevance\n- More detailed ablation studies\n\n### Questions\n\n- Are their any key trends that you'll observed across the board to narrow down the design space for the future across the general task space? The results are not convincing in any one direction across the board on tasks and methods\n- Do you'll think stronger backbones can help learn better features?\n- Did you'll consider trying complex augmentation techniques such as AugMix or even learned augmentations?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of long-tailed medical image classification by introducing *MONICA*, a benchmarking package designed to evaluate a variety of methods‚Äîsuch as different loss functions and data augmentations‚Äîacross multiple medical imaging datasets. The work provides an overview of curated datasets, methods, and their experimental performance, alongside a summary of key observations. The paper is clearly structured and practical for researchers in medical imaging, though certain methodological and evaluation aspects could be strengthened.\n\n**Major Comments**  \n1. The benchmarking suite could be expanded to include more canonical and widely used medical datasets (e.g., Camelyon for pathology, the medical subset of WILDS), which would enhance generality and field relevance.  \n2. The use of ResNet-50 as the sole backbone is outdated; the community has largely transitioned to more advanced architectures (e.g., ConvNeXt, Swin, or foundation models).  \n3. Pretraining strategies should be considered, as training backbones from scratch is uncommon and may hinder performance comparisons.  \n4. While fairness is maintained by reusing the same backbone across tasks, exploring a range of backbones could demonstrate robustness across modalities and problem types.  \n5. Top-1 accuracy is not an ideal metric for evaluating models under class imbalance; metrics such as AUROC, AUPRC, or F1-score would be more suitable.  \n6. Experimental results lack error bars, and further error analysis would strengthen claims.  \n7. The discussion of novel insights is somewhat limited; clearer articulation would increase the contribution‚Äôs impact.  \n8. The connection to clinical relevance should be elaborated to contextualize the work‚Äôs practical significance.  \n9. Additional ablation studies are recommended to better characterize model behaviors and design trade-offs.\n\n**Minor Comments**  \n- Consider discussing trends observed across tasks to narrow the design space for future work.  \n- The manuscript could benefit from comments on whether stronger backbones or more complex augmentation techniques (e.g., AugMix, learned augmentations) were considered.  \n- No ethical issues are identified.\n\n**Summary Paragraph**  \nOverall, the paper presents a useful and well-organized benchmarking framework with practical value for the community. Its strengths lie in dataset curation, code modularization, and broad coverage of methods. However, limitations in dataset diversity, backbone selection, metric choice, and experimental depth reduce the study‚Äôs impact and clarity of insights. Addressing these issues would substantially improve the work‚Äôs rigor and generalizability.\n\n**Decision Recommendation**  \n**Major Revision.** The paper offers a meaningful contribution but requires expanded datasets, stronger baselines, improved evaluation metrics, and deeper analysis to reach publishable quality.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1. Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript, *MONICA: Benchmarking on Long-Tailed Medical Image Classification*, develops and releases a large-scale benchmark and modular codebase for evaluating long-tailed learning (LTL) algorithms within medical imaging classification tasks. While the work does not introduce a novel algorithm or theoretical contribution in imaging methodology per se, it addresses a significant methodological gap ‚Äî the lack of standardized, reproducible evaluation and fair comparison across long-tailed medical imaging datasets. Given that IEEE TMI occasionally publishes benchmark or methodological synthesis papers that materially advance the reproducibility and methodological understanding of imaging AI systems, the scope can be considered **conditionally relevant**. However, the contribution is more infrastructural and empirical than fundamentally methodological.\n\n**2. Novelty & Contribution Level**  \n‚ö†Ô∏è The core novelty resides in curating and unifying 12 medical datasets, incorporating 30+ long-tailed methods into a coherent codebase, and empirically analyzing their cross-domain generalization. These are useful engineering and benchmarking contributions, but the methodological novelty is low‚Äîno new loss function or model architecture is proposed. The paper‚Äôs unique contribution is thus its comprehensiveness, reproducibility, and analysis rather than scientific innovation. This positions the work **below the typical TMI novelty threshold**, although it has practical value.\n\n**3. Technical and Experimental Rigor**  \nThe paper demonstrates strong experimental organization: unified training pipelines, cross-validation protocols, extensive comparisons, and consistent baselines. Tables 2‚Äì5 are detailed, and implementation transparency is high (code release promised). However:  \n- Statistical significance or uncertainty analysis (e.g., standard deviations, CIs) is missing.  \n- Many experimental decisions (hyperparameters, group thresholds) appear heuristic.  \n- Multi-label evaluation could be more rigorous; e.g., exact mean average precision values and per-class calibration are not included.  \n- No theoretical analysis or in-depth model interpretability assessment.  \n\n**4. Clarity and Presentation**  \nOverall clear and readable. The manuscript is lengthy but well structured; tables and figures are legible. Minor grammar and phrasing inconsistencies appear (‚Äúvita important,‚Äù ‚Äúwe will shortly introduce‚Äù etc.). Figures could use higher resolution or graphical consistency (e.g., Fig. 3‚Äì4 fonts).  \n\n**5. Ethical and Reproducibility Compliance**  \nThe datasets referenced are all public (e.g., ISIC, CheXpert, MedMNIST). There is no human data collection in this work. Ethical compliance and anonymization are hence adequate. Reproducibility is strong, with clear modular design and promised GitHub release.  \n\n---\n\n**Phase 2. IEEE TMI Review Report**\n\n**1. Summary**  \nThis paper presents **MONICA**, a comprehensive open-source benchmark for **long-tailed medical image classification (LTMIC)**. It integrates 12 publicly available datasets across 6 medical domains (dermatology, ophthalmology, radiology, pathology, hematology, histology, gastroenterology) and over 30 long-tailed learning algorithms grouped into resampling, data augmentation, and module-modification methods. The authors evaluate these under uniform training conditions (mainly ResNet‚Äë50 backbone, Adam optimizer) and provide detailed cross-dataset comparisons, highlighting empirical insights into what factors drive performance differences across imbalance levels and dataset modalities.\n\n**2. Strengths**  \n- First unified and reproducible benchmark addressing long-tailed imbalance for medical imaging.  \n- Integrates diverse datasets and 30+ SOTA LTL algorithms, providing immediate utility to the community.  \n- Transparent and modular codebase structure; practical resource for fair comparison.  \n- Methodological findings (e.g., interplay between resampling and classifier normalization, importance of two-stage training) offer empirical guidance.  \n- Strong reproducibility plan and detailed documentation.\n\n**3. Weaknesses**  \n- Limited methodological novelty: mostly repackages and systematizes existing methods.  \n- Statistical rigor is limited ‚Äî no confidence intervals, hypothesis testing, or extensive ablation studies.  \n- Evaluation metrics focus on accuracy/mAP; lacks calibration, uncertainty, or clinical interpretation analysis relevant to medical imaging applications.  \n- Multi-label LTMIC discussion is brief and not deeply explored.  \n- Conclusions are largely descriptive without deeper methodological insight or theoretical generalization.\n\n**4. Major Comments**  \n1. **Methodological Novelty & Positioning** ‚Äì Clarify the positioning of MONICA: Is it proposed as a *benchmark paper* or an *algorithmic contribution*? For TMI readership, emphasize its utility for methodological validation and reproducibility, rather than as a new method. Consider reframing contributions accordingly.  \n2. **Statistical and Uncertainty Analysis** ‚Äì Report variance over multiple training runs and statistical comparisons between methods. Include significance testing or confidence intervals for at least a subset of key results (Table 2‚Äì3).  \n3. **Evaluation Metrics** ‚Äì Expand to include calibration metrics (e.g., ECE, Brier score) to align better with TMI‚Äôs focus on clinically reliable AI.  \n4. **Multi-label Classification Section** ‚Äì Deepen this section: describe evaluation metrics (exact mAP computation), address label co-occurrence quantitatively, and elaborate on class-wise distributions.  \n5. **Clinical Interpretation & Relevance** ‚Äì Discuss how the benchmark insights might influence clinical deployment or development of medically robust algorithms (e.g., rare disease detection, regulatory validation).  \n6. **Benchmark Maintenance** ‚Äì Describe future update strategy (versioning, dataset expansion) for reproducibility and long-term community usage.\n\n**5. Minor Comments**  \n- Minor wording issues: correct typographical errors (‚Äúvita important,‚Äù ‚ÄúRe-search‚Äù ‚Üí ‚ÄúResearch‚Äù).  \n- Standardize the capitalization and naming of methods and datasets.  \n- Some tables (e.g., Table 2‚Äì3) are dense‚Äîconsider highlighting top-performing methods in each category.  \n- Include GitHub link upon release for verification.  \n- Clarify figures (axes labels in Fig. 2‚Äì4 somewhat small).  \n\n**6. Overall Recommendation**  \n‚ö†Ô∏è **Reject (Out of Scope / Low Novelty)**  \nWhile technically solid and valuable as a benchmarking resource, the work predominantly provides an organizational framework and comparative analysis rather than a new methodological advancement in imaging science. IEEE TMI prioritizes methodological innovation or theoretical insight; this paper would be more suited to journals emphasizing open resources and applied benchmarking (e.g., *Medical Image Analysis ‚Äì Datasets and Benchmarks track* or *Scientific Data*).  \nIf the authors can include novel evaluation methodology (e.g., new metric for imbalance calibration, theoretical rationale for algorithm behaviors) or deeper analysis of medical imaging‚Äìspecific LTL phenomena, re-submission to TMI could be reconsidered.\n\n**7. Confidence Level:** 4 / 5  \n\n**Summary Judgment:**  \nValuable benchmarking contribution with strong engineering and reproducibility merit but insufficient methodological innovation for IEEE TMI‚Äôs core scope. Recommend redirection to a benchmark- or data-oriented venue, or substantial reframing to include analytic or theoretical advancement suitable for TMI.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "ai",
          "comment": "The major comments primarily focus on textual analysis, lacking analysis of tables and figures."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript *MONICA: Benchmarking on Long-Tailed Medical Image Classification* presents a large-scale, open-source benchmark and modular codebase for evaluating long-tailed learning (LTL) algorithms in medical image classification. It compiles 12 publicly available datasets from six medical domains and integrates more than 30 existing LTL methods under a unified experimental framework. The paper‚Äôs focus is infrastructural and empirical‚Äîoffering standardization, reproducibility, and cross-domain evaluation‚Äîrather than proposing novel algorithms. Overall, the manuscript is clearly written, well structured, and technically comprehensive, but the originality and conceptual contribution are limited.\n\n**Major Comments**  \n1. **Positioning and Contribution** ‚Äì The work should more clearly position itself as a benchmarking or resource paper rather than an algorithmic contribution. Its main value lies in reproducibility and empirical synthesis.  \n2. **Statistical and Uncertainty Analysis** ‚Äì Current results lack variance estimates, statistical significance testing, or confidence intervals. Including these would enhance the rigor of cross-method comparisons.  \n3. **Evaluation Metrics** ‚Äì The analysis emphasizes accuracy and mean average precision but omits calibration and uncertainty measures. Incorporating metrics such as expected calibration error or Brier score would strengthen clinical relevance.  \n4. **Multi-label Classification** ‚Äì The multi-label evaluation is underdeveloped. Greater detail on dataset composition, label co-occurrence, and per-class performance should be provided to support conclusions.  \n5. **Clinical Relevance** ‚Äì The discussion could better connect empirical findings to practical implications for clinical algorithm development, such as generalization to rare disease detection.  \n6. **Benchmark Sustainability** ‚Äì Specify how the benchmark and codebase will be maintained, updated, and versioned to ensure long-term utility.\n\n**Minor Comments**  \n- Correct minor typographical and grammatical errors (e.g., ‚Äúvita important,‚Äù ‚ÄúRe-search‚Äù).  \n- Standardize naming conventions for methods and datasets.  \n- Improve figure legibility and font consistency (notably Figures 2‚Äì4).  \n- Consider visually highlighting top-performing methods in dense tables.  \n- Include the GitHub repository link to verify reproducibility upon release.\n\n**Summary Paragraph**  \nThe paper provides a valuable and well-executed benchmarking resource addressing the lack of standardized evaluation for long-tailed medical image classification. Strengths include broad dataset coverage, transparent experimental pipelines, and a clear commitment to reproducibility. However, limited methodological novelty, absence of statistical robustness analyses, and insufficient exploration of clinically oriented metrics constrain its scientific reach. The study‚Äôs contribution is primarily infrastructural and empirical rather than theoretical or methodological.\n\n**Decision Recommendation**  \n**Reject (Low Methodological Novelty / Out of Scope)** ‚Äì While the resource is technically solid and useful for the community, it lacks the conceptual innovation and methodological depth expected for acceptance. A reframed submission to a data- or benchmark-focused venue is recommended.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces MONICA (Medical OpeN-source Long-taIled ClassifiCAtion), a comprehensive benchmark for evaluating long-tailed medical image classification methods. The authors implement over 30 state-of-the-art long-tailed learning methodologies across 12 medical datasets spanning 6 medical domains (dermatology, ophthalmology, radiology, pathology, hematology, histology, gastroenterology). The benchmark addresses the lack of unified evaluation frameworks in long-tailed medical image learning by providing a modular codebase with standardized experimental protocols. Key findings include: (1) methods combining re-sampling, information augmentation, and module improvements (like GCL) achieve superior performance, (2) two-stage training paradigms offer significant flexibility, (3) simple re-sampling strategies remain effective baselines, and (4) multi-label classification presents additional complexities compared to multi-class tasks. The work aims to establish fair comparison standards and provide practical guidance for researchers developing long-tailed medical image classification systems.\n\n## Weaknesses\n\n‚Ä¢ **Insufficient mathematical formulation clarity and consistency**\n  - The problem definition in Section 2.1 (lines 108-119) uses inconsistent notation, where œÄ represents both a vector and individual frequencies, creating ambiguity in mathematical interpretation\n  - The imbalance ratio definition œÅ = n1/nk lacks clear specification of which classes n1 and nk represent, particularly given the ordering assumption\n  - Multi-label formulation introduces LCard metric (line 118) without proper mathematical context or connection to the main problem formulation\n\n‚Ä¢ **Limited experimental rigor and statistical validation**\n  - No statistical significance testing is reported across the extensive results in Tables 2, 3, and 5, making it impossible to distinguish meaningful performance differences from noise\n  - Single-run experiments without confidence intervals or multiple trials weaken the reliability of comparative conclusions, particularly given the modest performance differences observed\n  - Hyperparameter selection methodology is not systematically described (Section 4.1, lines 281-288), potentially introducing bias in method comparisons\n\n‚Ä¢ **Inadequate baseline establishment and evaluation protocol**\n  - The choice of ResNet-50 as the unified backbone (line 284) is not justified against other architectures, limiting generalizability of conclusions\n  - Missing ablation studies to isolate the contribution of individual components within complex methods like GCL, making it difficult to understand which aspects drive performance improvements\n  - Validation set construction methodology varies inconsistently across datasets (Table 1), compromising the fairness of cross-dataset comparisons\n\n‚Ä¢ **Incomplete coverage of relevant methodologies and recent advances**\n  - Several important recent long-tailed learning methods are mentioned as implemented but excluded from results presentation without clear justification criteria\n  - Self-supervised learning exploration (lines 407-420) is dismissed too readily based on limited experimentation, despite its potential relevance to medical imaging\n  - Multi-label long-tailed learning receives insufficient treatment given its clinical importance, with methods evaluation limited to basic adaptations\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical precision and notation consistency**\n  - Revise Section 2.1 to use distinct notation for the frequency vector œÄ and individual class frequencies œÄk, providing clear mathematical definitions for all symbols\n  - Explicitly define the class ordering convention and specify that œÅ represents the ratio between most frequent and least frequent classes\n  - Provide rigorous mathematical formulation for multi-label metrics and their relationship to the core long-tailed learning problem\n\n‚Ä¢ **Strengthen experimental methodology with statistical validation**\n  - Conduct multiple independent runs for all experiments and report mean performance with confidence intervals or standard deviations\n  - Apply appropriate statistical significance tests (e.g., paired t-tests) to validate performance differences between methods\n  - Establish systematic hyperparameter selection protocols using validation sets and document the optimization process for reproducibility\n\n‚Ä¢ **Develop comprehensive baseline evaluation framework**\n  - Justify the backbone architecture choice through systematic comparison with alternatives (e.g., Vision Transformers, EfficientNets) on representative datasets\n  - Include detailed ablation studies for complex methods to isolate the contribution of individual components\n  - Standardize validation set construction methodology across all datasets to ensure fair comparison protocols\n\n‚Ä¢ **Expand methodological coverage and analysis depth**\n  - Include results for all implemented methods with clear criteria for inclusion/exclusion in the main results presentation\n  - Conduct more systematic exploration of self-supervised learning with proper hyperparameter tuning and augmentation strategies tailored to medical imaging\n  - Develop specialized evaluation protocols and metrics for multi-label long-tailed learning that account for label co-occurrence patterns and clinical relevance",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 3,
          "specificity": 2,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 2,
          "source": "ai",
          "comment": "The main comments are general and do not offer any professional commentary or critical opinions on the content of the manuscript."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **MONICA (Medical OpeN-source Long-taIled ClassifiCAtion)**, a benchmark designed to evaluate long-tailed medical image classification methods across twelve datasets from six medical domains. The authors implement over thirty state-of-the-art long-tailed learning approaches within a unified and modular framework, addressing the lack of standardized evaluation in this area. Key findings highlight patterns in the effectiveness of method combinations, training strategies, and the additional complexity of multi-label classification tasks. Overall, the manuscript is comprehensive in scope and offers a valuable foundation for reproducible and fair assessment of long-tailed classification methods in medical imaging, though several methodological and presentation issues limit its rigor and interpretability.  \n\n**Major Comments**  \n1. **Mathematical clarity and notation consistency** ‚Äì Section 2.1 contains inconsistent notation, where a single symbol (œÄ) is used for both a vector and its elements, creating ambiguity. The imbalance ratio œÅ = n‚ÇÅ/n‚Çñ is introduced without clear specification of which classes these represent, and the multi-label formulation introduces the LCard metric without formal definition or connection to the main framework.  \n2. **Experimental rigor and statistical validation** ‚Äì The evaluation lacks statistical testing, confidence intervals, or multiple experimental runs. Without such analysis, observed performance differences cannot be confirmed as significant. Additionally, the hyperparameter selection process is insufficiently documented, raising reproducibility concerns.  \n3. **Baseline and evaluation protocol limitations** ‚Äì The exclusive use of ResNet-50 as the unified backbone is not justified, and no ablation studies are provided for complex models such as GCL, limiting interpretability. Variation in validation set construction across datasets further undermines fair comparison.  \n4. **Incomplete methodological coverage** ‚Äì Several implemented methods are omitted from results without stated criteria. The exploration of self-supervised approaches appears cursory, and multi-label learning receives insufficient attention despite its clinical importance.  \n\n**Minor Comments**  \n- Improve the clarity of Section 2.1 through consistent notation and explicit class ordering conventions.  \n- Expand definitions for metrics such as LCard and clarify their mathematical relationships.  \n- Provide systematic documentation of datasets, validation splits, and hyperparameter settings.  \n\n**Summary Paragraph**  \nThe paper‚Äôs aims and dataset breadth are commendable, offering a needed benchmark for long-tailed medical image analysis. However, issues of mathematical precision, inconsistent experimental protocols, and incomplete methodological inclusion reduce its reliability and generalizability. Addressing these areas would significantly strengthen the study‚Äôs reproducibility, interpretability, and utility for future research.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The contribution is substantial but requires significant improvement in mathematical formulation, statistical validation, and methodological completeness before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces MONICA, a comprehensive benchmark for long-tailed medical image classification (LTMIC). It evaluates over 30 methodologies across 12 long-tailed medical datasets spanning 6 medical domains, aiming to provide a unified framework for assessing the performance of various long-tailed learning methods. The authors argue that this benchmark addresses the lack of standardized evaluation frameworks in the field and offers valuable insights into the effectiveness of different methodologies. The manuscript is well-written, and the motivation for creating a standardized benchmark is clear.\n\n## Major Comments\n1. Novelty and Positioning: While the introduction of MONICA is a valuable contribution, the manuscript should provide a clearer distinction between existing benchmarks and MONICA's unique contributions. For instance, the authors could elaborate on how MONICA differs from other long-tailed learning benchmarks, such as those used in natural image recognition. Additionally, a more thorough discussion of the limitations of previous benchmarks and how MONICA addresses these limitations would strengthen the paper's novelty.\n\n2. Evaluation Design: The evaluation is conducted on a diverse set of datasets, which is commendable. However, the reliance on Pareto-distributed training sets for multi-class classification tasks may not fully capture the complexity of real-world scenarios, especially in multi-label classification. The authors should discuss the implications of this choice and possibly include more realistic imbalance scenarios. Furthermore, the inclusion of a broader range of medical imaging modalities and tasks could enhance the benchmark's applicability and robustness.\n\n3. Comparisons: While the manuscript includes a comprehensive set of baseline methods, the authors should ensure that the comparison includes the most recent and relevant long-tailed learning methods. For instance, the inclusion of recent advancements in self-supervised learning and domain-specific techniques would provide a more complete picture of the current state of the art. Additionally, the manuscript should explicitly state whether the implementations of these methods are based on the authors' own implementations or third-party codebases, to clarify reproducibility concerns.\n\n4. Reproducibility: The manuscript states that the codebase will be made publicly available, but the details provided on the implementation and training protocols are insufficient for full reproducibility. The authors should provide a more detailed description of the hyperparameters, data preprocessing steps, and any other necessary configurations to ensure that the results can be independently replicated. Additionally, a clear plan for releasing the codebase and maintaining it would be beneficial.\n\n## Minor Comments\n1. Figures: Figures 2 and 3 are cluttered and could benefit from a more streamlined presentation. Showing fewer representative slices with zoomed-in regions would improve readability.\n\n2. Section 2.1: The introduction of notation in Section 2.1 lacks sufficient explanation. In particular, the forward operator is denoted inconsistently.\n\n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definition.\n\n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7) should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: evaluating the performance of long-tailed learning methods in medical image classification. The introduction of MONICA as a comprehensive benchmark is technically interesting and could represent a meaningful innovation if convincingly distinguished from related benchmarks. However, the evaluation is limited to Pareto-distributed datasets, which may not fully capture real-world complexity. Additionally, the reproducibility of the approach is uncertain due to incomplete methodological details. Overall, while the idea has promise, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand comparative analysis, strengthen validation across datasets and modalities, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents MONICA, a benchmark designed for long-tailed medical image classification (LTMIC). It evaluates more than 30 methods across 12 datasets in six medical domains, aiming to establish a unified framework for assessing long-tailed learning methods in medical imaging. The paper is clearly written and motivated by the need for standardized evaluation in this area, providing an organized and coherent overview of its purpose and contributions.\n\n**Major Comments**  \n1. **Novelty and Positioning** ‚Äì Although the introduction of MONICA is valuable, the paper should more explicitly differentiate this benchmark from existing ones, including those developed for natural image recognition. A deeper discussion of the shortcomings of previous benchmarks and how MONICA addresses them would strengthen the work‚Äôs originality.  \n2. **Evaluation Design** ‚Äì The diversity of datasets is a strength, but the use of Pareto-distributed training sets for multi-class classification may not capture real-world clinical conditions, particularly in multi-label contexts. The authors should discuss the impact of this choice and consider incorporating more realistic imbalance scenarios and additional modalities or tasks to enhance the benchmark‚Äôs representativeness.  \n3. **Comparisons** ‚Äì Although a wide range of baselines is included, the comparisons could benefit from incorporating recent long-tailed learning methods, including self-supervised and domain-specific approaches. The manuscript should also clarify whether the reported results rely on the authors‚Äô own implementations or external codebases to improve transparency and reproducibility.  \n4. **Reproducibility** ‚Äì While public release of the codebase is promised, methodological details are sparse. Additional information about hyperparameters, preprocessing, and training configurations is necessary to ensure reproducibility. A clear maintenance and release plan would also strengthen the manuscript‚Äôs utility.\n\n**Minor Comments**  \n- Figures 2 and 3 are visually cluttered; simplifying and focusing on representative slices would improve clarity.  \n- Section 2.1 introduces notation inconsistently, particularly for the forward operator.  \n- Some acronyms (e.g., ‚ÄúR=4‚Äù) are undefined.  \n- Minor typographical errors should be corrected (‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù).\n\n**Summary Paragraph**  \nOverall, this work tackles an important problem by proposing a benchmark to evaluate long-tailed learning methods for medical image classification. MONICA could offer substantial value if its distinctions from existing benchmarks are clarified and its evaluation broadened. However, current limitations in experimental design and reproducibility reduce confidence in the findings. Strengthening comparative analyses, dataset diversity, and methodological transparency would significantly improve the contribution.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The manuscript requires expanded comparative evaluation, additional methodological detail, and improved clarity to demonstrate novelty and reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## MONICA: Benchmarking on Long-tailed Medical Image Classification\n\n### Summary\n\nThe paper introduces MONICA, a comprehensive benchmark and modular codebase for long-tailed medical image classification (LTMIC), integrating over 30 methods and evaluating them across 12 datasets spanning 6 medical domains. The study standardizes training and evaluation pipelines, reports extensive results with head/medium/tail breakdowns, and offers practical insights into re-sampling, augmentation, and module-improvement strategies (e.g., two-stage training, classifier normalization). It aims to provide a reproducible, unified framework to guide method selection and future research in LTMIC.\n\n### Strengths\n\n- Technical novelty and innovationA unified, modular codebase that decouples augmentation, sampling, model backbones/heads, and losses, easing fair comparisons and method composition.Broad integration of 30+ long-tailed learning techniques across three main paradigms (re-sampling, information augmentation, module improvements), plus preliminary SSL and OOD extensions.Useful methodological synthesis and practical guidance (e.g., two-stage training as a general paradigm; weight-norm calibration; the role of MixUp within two-stage pipelines).\n- Experimental rigor and validationCoverage of 12 datasets from 6 domains, including both multi-class and multi-label tasks, and presentation of results by head/medium/tail groups.Multi-label datasets included (ODIR, RFMiD, CheXpert) with mAP evaluation; head/medium/tail groupings provide nuanced perspectives beyond overall accuracy.Additional OOD analysis with OpenOOD shows that LTMIC training can benefit post-hoc OOD methods.\n- Clarity of presentationClear taxonomy of supported methods and coherent organization of the codebase components.Head/medium/tail breakdown tables provide interpretable trade-offs; weight-norm visualization offers intuitive classifier-bias diagnosis.Articulation of practical takeaways (e.g., resampling is a robust baseline, normalized linear classifier is a safe default).\n- Significance of contributionsAddresses a clear gap: lack of standardized, domain-specific benchmarking for long-tailed medical image classification across multiple modalities.Likely to be valuable infrastructure for the community if code, splits, and settings are fully released, enabling reproducible comparisons and method development.\n\n- A unified, modular codebase that decouples augmentation, sampling, model backbones/heads, and losses, easing fair comparisons and method composition.\n- Broad integration of 30+ long-tailed learning techniques across three main paradigms (re-sampling, information augmentation, module improvements), plus preliminary SSL and OOD extensions.\n- Useful methodological synthesis and practical guidance (e.g., two-stage training as a general paradigm; weight-norm calibration; the role of MixUp within two-stage pipelines).\n\n- Coverage of 12 datasets from 6 domains, including both multi-class and multi-label tasks, and presentation of results by head/medium/tail groups.\n- Multi-label datasets included (ODIR, RFMiD, CheXpert) with mAP evaluation; head/medium/tail groupings provide nuanced perspectives beyond overall accuracy.\n- Additional OOD analysis with OpenOOD shows that LTMIC training can benefit post-hoc OOD methods.\n\n- Clear taxonomy of supported methods and coherent organization of the codebase components.\n- Head/medium/tail breakdown tables provide interpretable trade-offs; weight-norm visualization offers intuitive classifier-bias diagnosis.\n- Articulation of practical takeaways (e.g., resampling is a robust baseline, normalized linear classifier is a safe default).\n\n- Addresses a clear gap: lack of standardized, domain-specific benchmarking for long-tailed medical image classification across multiple modalities.\n- Likely to be valuable infrastructure for the community if code, splits, and settings are fully released, enabling reproducible comparisons and method development.\n\n### Weaknesses\n\n- Technical limitations or concernsUniform hyperparameters across methods may disadvantage some techniques that require specific tuning, risking biased conclusions about relative efficacy.Exclusive reliance on ResNet-50 (and mostly 224√ó224 input) may not reflect current best practices in medical imaging, where higher resolutions, ViTs/ConvNeXts, and strong pretraining matter.For CheXpert, mapping ‚Äúuncertain‚Äù to negative without further calibration or uncertainty-aware training may limit applicability; medical classification commonly reports AUC and calibration, not only mAP.\n- Experimental gaps or methodological issuesLack of multi-seed repeats and variance/error bars; analyses (e.g., convergence stability) are performed ‚Äúwithout repeating trials,‚Äù limiting robustness claims.Several dataset statistics and split descriptions appear inconsistent or contain formatting errors (e.g., decimal points in counts; very small val/test sizes indicated in text vs tables), which can impede reproducibility.Important clinical metrics (AUC, balanced accuracy, macro F1, per-class recall, calibration metrics) are not consistently reported across MC tasks; top-1 accuracy alone is insufficient in medical contexts.Reporting bias risk: the text states that results not outperforming ERM are filtered to avoid redundancy, which is at odds with the role of a benchmark and seems inconsistently applied given included tables with sub-ERM results.\n- Clarity or presentation issuesTypos and grammatical issues are recurrent (e.g., ‚Äúvita important,‚Äù ‚Äúlong-talled,‚Äù ‚Äúimpartially evaluated‚Äù), and some method names/abbreviations are inconsistently formatted (e.g., PriorCE/PriorCELoss).Some method descriptions are overly brief or conflate ideas (e.g., LADE description mixes calibration/post-hoc and training-time details; Table 4 contains unconventional or imprecise formulas).\n- Missing related work or comparisonsLimited inclusion of medical-specific long-tailed methods beyond a few dermatology/radiology works; recent domain-tailored approaches (e.g., ECL, SPMix, curriculum/difficulty-aware sampling such as Flexible Sampling, or gradient-aware adjustments like GALA) deserve explicit positioning and, where feasible, inclusion.No evaluation of class-incremental or federated settings where imbalance is acute and clinically relevant; these are active areas in medical ML and have emerging solutions.\n\n- Uniform hyperparameters across methods may disadvantage some techniques that require specific tuning, risking biased conclusions about relative efficacy.\n- Exclusive reliance on ResNet-50 (and mostly 224√ó224 input) may not reflect current best practices in medical imaging, where higher resolutions, ViTs/ConvNeXts, and strong pretraining matter.\n- For CheXpert, mapping ‚Äúuncertain‚Äù to negative without further calibration or uncertainty-aware training may limit applicability; medical classification commonly reports AUC and calibration, not only mAP.\n\n- Lack of multi-seed repeats and variance/error bars; analyses (e.g., convergence stability) are performed ‚Äúwithout repeating trials,‚Äù limiting robustness claims.\n- Several dataset statistics and split descriptions appear inconsistent or contain formatting errors (e.g., decimal points in counts; very small val/test sizes indicated in text vs tables), which can impede reproducibility.\n- Important clinical metrics (AUC, balanced accuracy, macro F1, per-class recall, calibration metrics) are not consistently reported across MC tasks; top-1 accuracy alone is insufficient in medical contexts.\n- Reporting bias risk: the text states that results not outperforming ERM are filtered to avoid redundancy, which is at odds with the role of a benchmark and seems inconsistently applied given included tables with sub-ERM results.\n\n- Typos and grammatical issues are recurrent (e.g., ‚Äúvita important,‚Äù ‚Äúlong-talled,‚Äù ‚Äúimpartially evaluated‚Äù), and some method names/abbreviations are inconsistently formatted (e.g., PriorCE/PriorCELoss).\n- Some method descriptions are overly brief or conflate ideas (e.g., LADE description mixes calibration/post-hoc and training-time details; Table 4 contains unconventional or imprecise formulas).\n\n- Limited inclusion of medical-specific long-tailed methods beyond a few dermatology/radiology works; recent domain-tailored approaches (e.g., ECL, SPMix, curriculum/difficulty-aware sampling such as Flexible Sampling, or gradient-aware adjustments like GALA) deserve explicit positioning and, where feasible, inclusion.\n- No evaluation of class-incremental or federated settings where imbalance is acute and clinically relevant; these are active areas in medical ML and have emerging solutions.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe codebase design and method integrations are conceptually sound and practically useful. The emphasis on two-stage training and classifier normalization reflects current insights in long-tailed learning.However, ‚Äúunified hyperparameters‚Äù is a double-edged sword: it improves fairness of protocol but risks masking method-specific strengths. At minimum, a limited per-method tuning budget or reporting of ‚Äúreference‚Äù hyperparameters from original papers would strengthen conclusions.Calibration and decision-thresholding are central in medical AI; focusing on accuracy/mAP without calibration measures (ECE, Brier score) and AUC limits clinical relevance and soundness of the evaluation.\n- Experimental evaluation assessmentBreadth is a major strength (12 datasets, 6 domains). The head/medium/tail breakdowns are helpful for diagnosing trade-offs.Variance is not reported; single-seed evaluations particularly in imbalanced low-shot classes can be noisy and may change rankings. Provide mean¬±std over 3‚Äì5 seeds for key results.Dataset processing requires more precision: explicitly publish exact train/val/test splits and sampling scripts. Clarify inconsistencies (e.g., ISIC: ‚Äú50 and 100 images‚Äù vs Table 2 showing 400/800 val/test; counts like ‚Äú29.276‚Äù likely mean 29,276). These details are critical for a benchmark.For CheXpert and other ML datasets, consider reporting per-label AUC/mAP, macro-averages, label cardinality, and label co-occurrence statistics; discuss how these interact with resampling strategies (e.g., risk of spurious correlations).Efficiency/cost: a benchmark should also report training time, GPU-hours, and memory for major methods to inform practical adoption.\n- Comparison with related work (using the summaries provided)Balanced Softmax (Menon et al. 2007.10740/2007.07314) and logit adjustment are included; consider a clearer mapping of your ‚ÄúPriorCE/WeightedSoftmax‚Äù to these works and cite their statistical grounding.Flexible Sampling (2204.03161) is directly relevant (also ISIC-2019-LT) and proposes difficulty-aware curricula; discuss how curriculum or competence-based rebalancing fits within MONICA and whether such strategies are included or could be added.Medical-specific long-tail methods like ECL (2307.04136) and SPMix (2406.10801) explicitly enhance tail-class representation with contrastive or saliency-guided mixing; including them (or explaining exclusion) would sharpen the benchmark‚Äôs medical relevance.Gradient-aware methods such as GALA (2403.09036) provide principled logit/gradient balancing that often outperform GCL; incorporating or at least positioning them would modernize the coverage.Emerging medical CIL/FL under imbalance (2407.13768; 2206.13803) are important application contexts; while out-of-scope for now, positioning MONICA as extensible toward CIL/FL tasks can broaden impact.\n- Discussion of broader impact and significanceMONICA can standardize LTMIC evaluation and accelerate method development while avoiding confounded comparisons‚Äîa clear community benefit.To align with clinical practice, consider adding medically oriented metrics (AUC, sensitivity/specificity at clinically relevant operating points, calibration), and documenting dataset provenance, labeling quality, and potential biases.Release policies, licensing, and patient-privacy compliance should be clearly documented for all datasets and code to build trust and facilitate adoption.\n\n- The codebase design and method integrations are conceptually sound and practically useful. The emphasis on two-stage training and classifier normalization reflects current insights in long-tailed learning.\n- However, ‚Äúunified hyperparameters‚Äù is a double-edged sword: it improves fairness of protocol but risks masking method-specific strengths. At minimum, a limited per-method tuning budget or reporting of ‚Äúreference‚Äù hyperparameters from original papers would strengthen conclusions.\n- Calibration and decision-thresholding are central in medical AI; focusing on accuracy/mAP without calibration measures (ECE, Brier score) and AUC limits clinical relevance and soundness of the evaluation.\n\n- Breadth is a major strength (12 datasets, 6 domains). The head/medium/tail breakdowns are helpful for diagnosing trade-offs.\n- Variance is not reported; single-seed evaluations particularly in imbalanced low-shot classes can be noisy and may change rankings. Provide mean¬±std over 3‚Äì5 seeds for key results.\n- Dataset processing requires more precision: explicitly publish exact train/val/test splits and sampling scripts. Clarify inconsistencies (e.g., ISIC: ‚Äú50 and 100 images‚Äù vs Table 2 showing 400/800 val/test; counts like ‚Äú29.276‚Äù likely mean 29,276). These details are critical for a benchmark.\n- For CheXpert and other ML datasets, consider reporting per-label AUC/mAP, macro-averages, label cardinality, and label co-occurrence statistics; discuss how these interact with resampling strategies (e.g., risk of spurious correlations).\n- Efficiency/cost: a benchmark should also report training time, GPU-hours, and memory for major methods to inform practical adoption.\n\n- Balanced Softmax (Menon et al. 2007.10740/2007.07314) and logit adjustment are included; consider a clearer mapping of your ‚ÄúPriorCE/WeightedSoftmax‚Äù to these works and cite their statistical grounding.\n- Flexible Sampling (2204.03161) is directly relevant (also ISIC-2019-LT) and proposes difficulty-aware curricula; discuss how curriculum or competence-based rebalancing fits within MONICA and whether such strategies are included or could be added.\n- Medical-specific long-tail methods like ECL (2307.04136) and SPMix (2406.10801) explicitly enhance tail-class representation with contrastive or saliency-guided mixing; including them (or explaining exclusion) would sharpen the benchmark‚Äôs medical relevance.\n- Gradient-aware methods such as GALA (2403.09036) provide principled logit/gradient balancing that often outperform GCL; incorporating or at least positioning them would modernize the coverage.\n- Emerging medical CIL/FL under imbalance (2407.13768; 2206.13803) are important application contexts; while out-of-scope for now, positioning MONICA as extensible toward CIL/FL tasks can broaden impact.\n\n- MONICA can standardize LTMIC evaluation and accelerate method development while avoiding confounded comparisons‚Äîa clear community benefit.\n- To align with clinical practice, consider adding medically oriented metrics (AUC, sensitivity/specificity at clinically relevant operating points, calibration), and documenting dataset provenance, labeling quality, and potential biases.\n- Release policies, licensing, and patient-privacy compliance should be clearly documented for all datasets and code to build trust and facilitate adoption.\n\n### Questions for Authors\n\n- Will you release exact train/val/test splits and sampling scripts for each dataset (including Pareto parameters and random seeds), and a permanent Git tag/commit for the code to guarantee reproducibility?\n- Are models initialized with ImageNet (or ImageNet-21k) pretrained weights? If so, which variants, and are results sensitive to pretraining? If not, why, given common medical practice?\n- How were hyperparameters chosen for each method? Did you allow any method-specific tuning (e.g., loss margins, temperatures) or strictly fix global defaults? Could you provide a small per-method tuning budget to reduce bias?\n- Can you report multi-seed results (e.g., 3‚Äì5 runs) with mean¬±std for the key tables to quantify variability, especially in tail classes?\n- For multi-class medical datasets, can you add AUC, macro F1, balanced accuracy, and calibration metrics (ECE/Brier) to align with clinical evaluation standards?\n- The text mentions filtering out methods that do not beat ERM ‚Äúto avoid redundancy,‚Äù yet the tables include sub-ERM methods. What is the final policy on reporting? For a benchmark, would you include all results (including negative ones) in an appendix?\n- Several dataset statistics appear inconsistent (e.g., punctuation in counts, ISIC val/test sizes). Can you clarify and correct these numbers, and provide a consolidated data card per dataset?\n- For CheXpert, why map uncertain labels to negative rather than using uncertainty-aware loss or label smoothing? How sensitive are results to this decision?\n- Could you include medical-domain LTL methods (e.g., ECL, SPMix) or justify their exclusion? If resource-limited, could you at least position these works and discuss expected behavior on your benchmark?\n- Can you provide training-time/resource comparisons and a discussion of method scalability (e.g., SAM, BBN, multi-expert methods) to inform practical deployment?\n\n### Overall Assessment\n\nThis paper presents a timely and potentially impactful benchmark and codebase for long-tailed medical image classification. Its breadth of datasets, integration of a large suite of methods, and pragmatic insights (two-stage training, classifier normalization, resampling as a strong baseline) are valuable for the community. However, to reach top-tier benchmark standards, the work should strengthen experimental rigor and clinical relevance: report multi-seed variability, add clinically standard metrics (AUC, calibration), correct and document dataset splits/statistics, clarify hyperparameter policies, and minimize reporting bias by including all results. Broader positioning against domain-specific LTL methods and modern logit/gradient adjustment techniques would further enhance completeness. With these improvements, MONICA could serve as a robust reference point for LTMIC research and deployment.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes MONICA, a large-scale benchmark and modular codebase for long-tailed medical image classification (LTMIC). It integrates more than 30 long-tailed learning methods across 12 datasets and 6 medical domains, providing a unified framework for fair comparison, reproducibility, and method composition. The work standardizes training and evaluation pipelines, reports extensive head/medium/tail breakdowns, and extracts practical lessons on re-sampling, data augmentation, and model improvement strategies. Overall, the paper is clearly presented, well-organized, and addresses an important gap in standardized evaluation for long-tailed medical classification.\n\n---\n\n**Major Comments**  \n1. **Experimental Design and Reproducibility:** The use of uniform hyperparameters may bias results against methods requiring specific tuning. Multi-seed evaluations and error bars are missing, reducing robustness. Dataset statistics and splits show inconsistencies or formatting errors, which hinder reproducibility.  \n2. **Modeling Choices:** All experiments use ResNet-50 at 224√ó224 resolution, which diverges from current medical imaging standards involving higher resolutions, ViTs/ConvNeXTs, and stronger pretraining. Mapping ‚Äúuncertain‚Äù to negative in CheXpert without uncertainty handling limits medical validity.  \n3. **Evaluation Metrics:** The benchmark focuses on accuracy and mAP, omitting clinically key metrics such as AUC, calibration (ECE, Brier), macro F1, or per-class recall, which affects clinical interpretability.  \n4. **Comparative Scope:** The inclusion of long-tailed methods is broad but omits several medical-domain-tailored or recent gradient-aware strategies (e.g., ECL, SPMix, Flexible Sampling, GALA). Their absence or lack of discussion weakens contextualization.  \n5. **Presentation and Consistency:** Text contains typographical and formatting issues; some method descriptions (e.g., LADE) conflate training and calibration concepts. Clarifications are needed regarding naming conventions and certain formulas in tables.  \n6. **Benchmark Policy:** Excluding sub-ERM results contradicts the stated goal of unbiased benchmarking. Reporting all outcomes, including non-improving methods, would enhance integrity.  \n\n---\n\n**Minor Comments**  \n- Add training-time, resource, and scalability comparisons for major algorithms.  \n- Clarify pretraining details (e.g., ImageNet vs. ImageNet-21k) and sensitivity to initialization.  \n- Provide complete scripts, seeds, and dataset cards for transparency.  \n- Correct minor typos (e.g., ‚Äúvita important,‚Äù ‚Äúlong-talled‚Äù) and ensure consistent abbreviation usage (e.g., PriorCE vs. PriorCELoss).  \n\n---\n\n**Summary Paragraph**  \nThe work is an ambitious and valuable attempt to unify and standardize evaluation for long-tailed medical image classification. Its strengths lie in breadth of coverage, code modularity, and clear methodological taxonomy, offering a high community utility. Nonetheless, improved reproducibility, inclusion of clinically meaningful metrics, correction of dataset irregularities, explicit hyperparameter policies, and expanded methodological scope are necessary to elevate reliability and clinical relevance. With these refinements, MONICA could become an enduring benchmark resource for LTMIC research and practice.  \n\n---\n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: MONICA: BENCHMARKING ON LONG-TAILED MEDICAL IMAGE CLASSIFICATION\n\n## 1. Opening Paragraph\n\nThe manuscript addresses the critical challenge of long-tailed learning in medical image classification, where datasets exhibit highly imbalanced class distributions with rare conditions having limited training samples‚Äîa common scenario in clinical practice where rare but serious conditions must be accurately identified despite their scarcity in training data. The authors propose MONICA (Medical OpeN-source Long-tailed ClassifCation), a comprehensive benchmarking framework that implements over 30 long-tailed learning methodologies across 12 medical datasets spanning 6 clinical domains (Dermatology, Ophthalmology, Radiology, Pathology, Hematology, Histology, and Gastroenterology). Their work provides a unified codebase with modular architecture supporting various resampling, information augmentation, and module improvement techniques, enabling standardized evaluation of these methods under consistent experimental settings. The results demonstrate that certain techniques like GCL and DisAlign effectively calibrate classifier weight norms and improve performance across head, medium, and tail classes, while also enhancing out-of-distribution detection capabilities. The authors provide valuable insights on methodological trade-offs and practical considerations for implementing long-tailed learning in medical contexts.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The comprehensive nature of this benchmark covering 6 medical domains and 12 datasets with varying imbalance ratios represents a significant contribution to the medical imaging community, addressing a critical need for standardized evaluation.\n- The well-structured, modular codebase implementing over 30 methods provides immediate practical utility for researchers working on long-tailed medical image classification problems.\n- The detailed analysis of weight norm calibration across head/medium/tail classes (Figure 2) offers valuable insights into how different methods mitigate class imbalance.\n- The demonstration of improved OOD detection capabilities through long-tailed methods (Figure 3) reveals an important secondary benefit that enhances model reliability in clinical settings.\n\n**Limitations:**\n- The paper lacks explicit connection between technical performance improvements and clinical impact. Without demonstrating how these methods affect diagnostic accuracy or patient outcomes, the medical relevance remains theoretical.\n- The evaluation primarily focuses on standard machine learning metrics (accuracy, AUROC) without incorporating clinically meaningful metrics that would better establish the value for medical practitioners.\n- The benchmarking is conducted on publicly available datasets that may not fully represent the complexity and variability of real-world clinical data, raising questions about clinical generalizability.\n\n### Minor Comments\n\n**Strengths:**\n- The clear categorization of methods into resampling, information augmentation, and module improvement provides an intuitive framework for understanding the landscape of long-tailed learning techniques.\n- The detailed documentation of dataset statistics, partition schemes, and implementation details enhances transparency and facilitates adoption by other researchers.\n- The analysis of training stability (Figure 4) provides practical guidance for model checkpoint selection in real-world applications.\n\n**Limitations:**\n- The figures would benefit from more detailed captions explaining the clinical significance of the results, particularly for non-expert readers in medical fields.\n- The paper could better articulate how MONICA compares with existing benchmarks like MedMNIST or other domain-specific evaluation frameworks.\n- Some sections contain excessive detail about implementation nuances that might be better suited for supplementary materials rather than the main text.\n\n## 3. Summary of Evaluation\n\n**Significance:** The work addresses an important problem in medical imaging where rare conditions with limited training samples must be accurately identified. The lack of standardized benchmarks has previously hindered progress in this area, making this contribution highly significant for the community. However, the paper could strengthen its significance by more explicitly connecting technical improvements to potential clinical impact.\n\n**Innovation:** While MONICA itself is primarily a benchmarking framework rather than a novel method, the systematic evaluation across diverse medical domains and the insights derived from this comprehensive comparison represent valuable innovation. The identification of specific techniques that improve both classification performance and OOD detection offers new perspectives on long-tailed learning in medical contexts.\n\n**Evaluation:** The evaluation is extensive across multiple datasets, methods, and metrics, demonstrating thoroughness. However, the evaluation would be strengthened by including more clinically relevant metrics and by conducting statistical significance testing across the results to confirm the reliability of the observed performance differences.\n\n**Reproducibility:** The work excels in reproducibility with a well-structured codebase, detailed documentation of datasets and partition schemes, and clear methodology descriptions. The modular design facilitates adaptation to new datasets and methods, and the planned GitHub release will further enhance reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision (Reject/Resubmit)**\n\nWhile MONICA represents a valuable contribution to the medical imaging community with its comprehensive benchmarking framework and well-structured codebase, the paper requires significant revisions before it can be accepted for publication in IEEE Transactions on Medical Imaging. The most critical issues are the insufficient connection between technical results and clinical relevance, and the lack of clinically meaningful metrics. The authors should substantially revise the paper to:\n\n1. Explicitly articulate the clinical impact of the technical improvements demonstrated\n2. Incorporate metrics that reflect diagnostic utility or patient outcomes where possible\n3. Strengthen the discussion of how MONICA advances medical practice rather than just methodology\n4. Provide more detailed analysis of the most promising techniques for specific medical domains\n\nThese revisions would better align the work with TMI's focus on medical imaging research that demonstrates clinical relevance and potential for improving healthcare outcomes. The authors have provided an excellent foundation with MONICA, but the manuscript needs to better emphasize the medical significance of their work to meet TMI's standards.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript, *MONICA: Benchmarking on Long-Tailed Medical Image Classification*, addresses the central challenge of class imbalance in medical image datasets, particularly the accurate identification of rare yet clinically critical conditions. The authors introduce MONICA (Medical OpeN-source Long-tailed ClassifCation), a comprehensive benchmarking framework that implements over 30 long-tailed learning methods across 12 datasets in six medical domains, offering a unified and modular codebase for standardized evaluation. The study presents comparative results demonstrating how certain methods, such as GCL and DisAlign, effectively calibrate weight norms and improve balanced accuracy, including enhanced out-of-distribution detection. The paper is technically sound and clearly presented, offering a valuable resource for researchers in medical image analysis, though its connection to clinical application could be made stronger.  \n\n**Major Comments**  \n1. **Scope and Contribution:** The benchmark‚Äôs breadth‚Äîspanning 6 domains and 12 datasets‚Äîrepresents a notable advancement, providing a standardized foundation for long-tailed learning in medical imaging.  \n2. **Technical Depth:** The modular codebase with detailed documentation and standardized evaluations significantly improves reproducibility and usability.  \n3. **Insightful Analyses:** Analyses of weight norm calibration (Figure 2) and OOD detection (Figure 3) yield useful observations relevant to model calibration and robustness.  \n4. **Clinical Relevance:** The study lacks a clear link between technical improvements and clinical utility; the implications for diagnostic accuracy or patient outcomes are not demonstrated.  \n5. **Evaluation Metrics:** The reliance on standard metrics (accuracy, AUROC) overlooks more clinically meaningful measures that could better contextualize the impact for practitioners.  \n6. **Dataset Limitations:** The use of public datasets may not capture the full spectrum of real-world clinical variability, limiting the generalizability of conclusions.  \n\n**Minor Comments**  \n- The categorization of long-tailed learning methods is clear and pedagogical.  \n- Figures would benefit from captions highlighting the clinical relevance for medical readers.  \n- Comparisons with existing benchmarks (e.g., MedMNIST) should be clarified.  \n- Some implementation details could be moved to supplementary materials for conciseness.  \n\n**Summary Paragraph**  \nMONICA provides a significant, comprehensive benchmark for evaluating long-tailed learning in medical imaging, combining methodological rigor with transparent implementation. The manuscript‚Äôs strengths lie in its breadth, reproducibility, and depth of analysis. Its principal shortcomings are the limited articulation of clinical impact and the absence of medically interpretable performance metrics. Addressing these issues would make the work more aligned with the applied goals of medical imaging research, ensuring its relevance beyond methodological comparison.  \n\n**Decision Recommendation: Major Revision**  \nThe study constitutes a strong foundational contribution but requires major revisions before acceptance. The authors should enhance the discussion of clinical relevance, include clinically oriented metrics, and better connect technical findings to potential healthcare implications. These revisions would elevate the manuscript from a methodological resource to a contribution demonstrably advancing medical imaging practice.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe authors tackle the problem of long‚Äëtailed learning in medical image classification, where many clinical imaging collections are heavily imbalanced. To support transparent and reproducible research, they introduce MONICA (Medical Open‚Äësource Long‚Äëtailed Classification), an open‚Äësource benchmark that assembles more than thirty algorithms from related fields and evaluates them on twelve long‚Äëtailed medical datasets covering six clinical areas (e.g., dermoscopy, chest radiography). MONICA supplies a unified codebase, standardized training and testing pipelines, and detailed analyses of individual methodological components. The paper reports empirical results for every incorporated method on the chosen datasets, providing a broad reference for performance comparison. The main contribution is the delivery of a publicly available, well‚Äëorganized platform that consolidates state‚Äëof‚Äëthe‚Äëart long‚Äëtailed learning approaches for medical imaging.\n\n---\n\n## General feedback  \n\n* **Significance:** Imbalanced class distributions are common in clinical imaging; a shared benchmark has the potential to speed up method development and to promote fair comparisons across the community.  \n* **Innovation:** The novelty lies primarily in the engineering effort of gathering a large suite of methods and datasets rather than in proposing new algorithms. Although incremental, this contribution addresses a clear need.  \n* **Evaluation:** The manuscript presents extensive performance results (Tables‚ÄØ1‚Äë5, Figures‚ÄØ1‚Äë4) but omits several essential details, such as the split ratios used for each dataset, the exact evaluation metrics, and any statistical significance testing.  \n* **Reproducibility:** A public GitHub repository is announced, yet the paper does not include the repository URL, version information, environment specifications, or random‚Äëseed/hardware settings that are necessary for exact replication.\n\n---\n\n## Specific comments/critiques  \n\n1. **Dataset splits:** The train/validation/test ratios for the twelve datasets are not disclosed (see Fig.‚ÄØ1), which hampers reproducibility.  \n2. **Hyper‚Äëparameters:** Settings such as learning rate, batch size, optimizer, and number of epochs for the 30+ methods are missing, making it impossible to reproduce the experiments.  \n3. **Performance tables:** Tables‚ÄØ1‚Äë5 lack captions, clear column headings, and confidence intervals; without statistical testing it is unclear whether observed differences are meaningful.  \n4. **Epoch‚Äëselection analysis:** Figure‚ÄØ4 displays a discrepancy between the epoch chosen on validation and the best test epoch, yet the criterion used for validation‚Äëbased selection (e.g., early‚Äëstopping threshold) is not described.  \n5. **OOD detection experiments:** Figure‚ÄØ3 reports out‚Äëof‚Äëdistribution detection results without specifying which OOD datasets were used, the metrics applied, or how these experiments were integrated with the long‚Äëtailed methods.  \n6. **Weight‚Äënorm trends:** Figure‚ÄØ2 shows weight‚Äënorm evolution only for the ISIC‚Äë2019‚ÄëLT dataset; it remains unclear whether similar patterns appear in the other domains.  \n7. **Method taxonomy:** The paper does not present a rationale for the inclusion or exclusion of the 30+ algorithms, nor does it discuss possible bias arising from heterogeneous dataset sources.  \n8. **Code release details:** The statement about releasing code omits a URL, licensing terms, and a DOI, leaving the reproducibility claim unsupported.\n\n---\n\n## A suggested decision  \n\n**Major Revision** ‚Äì The work has clear merit, but considerable improvements to methodological transparency, analysis, and reproducibility are needed before the manuscript can be accepted.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of long‚Äëtailed learning in medical image classification, where class imbalance is prevalent across clinical datasets. It introduces MONICA (Medical Open‚Äësource Long‚Äëtailed Classification), an open‚Äësource benchmark encompassing over thirty algorithms tested on twelve medical imaging datasets across six clinical areas. The framework offers standardized pipelines, unified code, and comprehensive comparative analyses. Overall, the work provides a valuable resource for benchmarking and community transparency. The presentation is generally clear, but key methodological details required for reproducibility are insufficiently documented.  \n\n**Major Comments**  \n1. **Dataset Splits:** The paper does not report the train/validation/test ratios for the twelve datasets, limiting reproducibility.  \n2. **Hyper‚Äëparameter Details:** Hyper‚Äëparameters for the more than thirty included methods (e.g., learning rate, batch size, optimizer, number of epochs) are not provided, impeding faithful reimplementation.  \n3. **Performance Reporting:** Tables‚ÄØ1‚Äë5 lack descriptive captions, precise column labels, and confidence intervals. The absence of statistical significance testing makes it uncertain whether the observed differences are meaningful.  \n4. **Epoch Selection:** Figure‚ÄØ4 highlights a mismatch between validation and test epochs, but the criterion for selecting epochs (e.g., early‚Äëstopping policy) is not clarified.  \n5. **Out‚Äëof‚ÄëDistribution (OOD) Detection:** Figure‚ÄØ3 presents OOD detection results without identifying the datasets, applied metrics, or integration procedure with long‚Äëtailed methods.  \n6. **Weight‚ÄëNorm Analysis:** Figure‚ÄØ2 discusses weight‚Äënorm behavior only for the ISIC‚Äë2019‚ÄëLT dataset. It would be useful to know whether similar phenomena occur in the other domains.  \n7. **Algorithm Taxonomy:** The rationale for the inclusion and exclusion criteria among the 30+ algorithms is not discussed, raising questions about potential selection bias from heterogeneous dataset sources.  \n8. **Code Availability:** Although public release is mentioned, the absence of a repository URL, license information, and DOI undermines the reproducibility claim.  \n\n**Minor Comments**  \n- Clarify table and figure annotations for readability.  \n- Explicitly report evaluation metrics and dataset split ratios.  \n- Provide environment specifications, software versions, and random‚Äëseed settings.  \n\n**Summary Paragraph**  \nThe submission offers a well‚Äëstructured benchmark that addresses a significant gap in reproducible long‚Äëtailed learning research for medical imaging. Its main strength lies in assembling and standardizing a wide range of existing algorithms under a unified framework. However, essential methodological details‚Äîincluding dataset splits, training configurations, and code information‚Äîare missing or incomplete, limiting transparency and validation. The study‚Äôs contribution is thus primarily infrastructural rather than algorithmic, and it would benefit from major revisions aimed at improving clarity, documentation, and reproducibility.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Huimin Lu",
      "Lie Ju",
      "Peibo Duan",
      "Siyuan Yan",
      "Xiaodan Xing",
      "Yang Nan",
      "Yukun Zhou",
      "Zongyuan Ge"
    ],
    "url": "pdfs/iclr.cc-2025-conference_42d1ec064be79233cea3b21fed2023b04f20381e.pdf",
    "remote_url": "https://openreview.net/pdf/42d1ec064be79233cea3b21fed2023b04f20381e.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Precision Medicine; Patient-Specific Segmentation; Out-of-Distribution Patient Adaptation"
    ],
    "abstract": "Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}ersonalized}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the selected part-aware prompt, we propose a distribution-similarity-based retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^{\\text{2}}\\text{SAM}$ improves the performance by $\\texttt{+} 8.0$% and $\\texttt{+} 2.0$% mean Dice score within two patient-specific segmentation tasks, and exhibits impressive generality across different domains, $\\textit{e.g.}$, $\\texttt{+} 6.4$% mIoU on the PerSeg benchmark. Code will be released upon acceptance.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe article addresses challenges in personalized treatment within modern precision medicine, particularly in the context of medical image segmentation. The key issues it aims to solve include:\n\n1.\tPatient Variability: There is considerable variability among different patients, which complicates the segmentation of tumors and critical organs in medical images.\n2.\tLimited Annotated Data: Many existing segmentation algorithms rely on large amounts of annotated training data. However, personalized treatment often encounters a shortage of such data for individual patients, making it difficult to train models effectively.\n\nTo overcome these obstacles, the authors propose a new approach formulated as an in-context segmentation problem, leveraging the promptable segmentation mechanism of the Segment Anything Model (SAM). Their method, named P2SAM (Part-aware Personalized Segment Anything Model), allows for seamless adaptation to new, out-of-distribution patients using only one-shot patient-specific data.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The manuscript is clearly expressed and presents the research in a logical and structured manner, although some sentences are lengthy and could benefit from simplification to enhance readability.\n2. The paper presents substantial qualitative results and includes experiments across multiple datasets, demonstrating a considerable amount of work.\n\n### Weaknesses\n\n1. Representation: The overall logic of the paper appears problematic to me. The title and introduction emphasize \"Precision Medicine,\" yet the writing primarily focuses on highlighting the general applicability of the proposed method. Numerous examples and results are presented from the natural image domain, while the content related to \"Precision Medicine\" is notably limited, which may cause confusion for readers.\n\n2. The motivation for the \"Part-aware Prompt Mechanism\" is unclear: what is the reasoning behind this approach, and how does it address challenges in medical tasks? The architecture diagram is also based on natural image applications, leaving it unclear how the proposed method tackles issues specific to precision medicine. Additionally, there is no discussion on how this approach handles different modalities in medical imaging, which the paper should address.\n\n3. Method: Several aspects require further clarification and enhancement. \n\n     a. Firstly, the decision to use only a single negative point per cluster raises concerns regarding its sufficiency. A more robust approach would involve utilizing multiple negative points to enhance model generalization.\n\n     b. Secondly, while the method is designed for patient-specific segmentation, it raises concerns about its application to multi-segmentation tasks. A discussion on how the part-aware prompt mechanism could adapt to scenarios involving multiple segmentations would improve the methodology's applicability.\n\n     c. Additionally, the manuscript relies on 2D segmentation, which requires numerous points for effective performance. This issue remains unaddressed, and the choice not to utilize native 3D models is not sufficiently justified. Given the advantages of 3D models in capturing spatial relationships and providing more comprehensive context in medical imaging, exploring their potential application in this study would strengthen the methodology.\n\n4. Experiments: Several areas require clarification and enhancement.\n\n     a. Firstly, it is unclear whether the number of clusters set for K-means is consistent across different datasets. A detailed explanation of how the clustering parameters are determined for each dataset would improve the robustness of the results and ensure comparability.\n\n     b. Secondly, the comparison methods lack the inclusion of the latest medical-related benchmarks. Integrating more recent studies or state-of-the-art approaches would provide a more comprehensive evaluation of the proposed method's performance. This would help contextualize the results within the current landscape of medical imaging segmentation research.\n\n5. The paper lacks comparisons with state-of-the-art (SOTA) methods. Numerous improvements to SAM tailored for medical imaging have been proposed, yet many of them are omitted in this work‚Äôs comparative experiments. Furthermore, several segmentation baselines widely used in the medical domain are not referenced, which casts doubt on the demonstrated effectiveness of the proposed method.\n\n### Questions\n\n1. Is the proposed method truly aimed at addressing \"Precision Medicine,\" as suggested in the title and introduction, or is it intended as a more general approach applicable to both natural image and medical imaging domains? These are fundamentally different focuses, and clarity on this point is essential.\n\n2. What is the motivation behind the proposed method, and how does it address some of the fundamental challenges in the medical imaging field, such as unclear lesion boundaries and significant variations across different imaging modalities? Clarity on how these issues are tackled is crucial.\n\n3. Since the method claims to address patient-specific segmentation, it must consider the significant variability in characteristics across different diseases, such as liver cancer, pancreatic cancer, and rectal cancer. How does the proposed method ensure effectiveness when dealing with patients suffering from various diseases with distinct features?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript aims to advance personalized treatment within precision medicine by addressing key challenges in medical image segmentation, namely patient variability and limited annotated data. It introduces P2SAM (Part-aware Personalized Segment Anything Model), which reformulates segmentation as an in-context problem and leverages the promptable mechanism of the Segment Anything Model (SAM) for one-shot patient-specific adaptation. The paper is clearly written and logically structured, with extensive qualitative results across multiple datasets; however, some sentences are lengthy and the connection to precision medicine requires clarification.\n\n---\n\n**Major Comments**  \n1. **Scope and Conceptual Consistency** ‚Äì The paper‚Äôs framing around precision medicine is inconsistent with its content, which largely centers on general image segmentation. Much of the motivation, examples, and experimental evidence are drawn from natural images rather than medical imaging, creating confusion regarding the primary application domain.  \n2. **Motivation and Relevance of the Part-aware Prompt Mechanism** ‚Äì The justification for introducing this mechanism is insufficient. Its specific benefits for medical image tasks are unclear, and neither the architecture nor the discussion addresses modality differences or domain-specific challenges in precision medicine.  \n3. **Methodological Clarifications Needed** ‚Äì  \n   a. The rationale for using only one negative point per cluster appears weak; incorporating multiple negatives could improve robustness.  \n   b. The extension of the method to multi-segmentation scenarios is not explained.  \n   c. The use of 2D segmentation requires numerous prompts, yet the paper does not justify why native 3D models‚Äîmore appropriate for volumetric data‚Äîwere not adopted.  \n4. **Experimental Design** ‚Äì  \n   a. The selection of K-means clustering parameters across datasets is unclear and should be detailed for reproducibility.  \n   b. Comparative experiments omit recent medical-specific SAM variants and commonly used medical imaging baselines, limiting the significance of the reported results.  \n5. **Positioning and Evaluation** ‚Äì The lack of state-of-the-art medical segmentation comparisons weakens claims of contribution and effectiveness within the intended domain.\n\n---\n\n**Minor Comments**  \n- Some sentences are unnecessarily long and could be simplified for clarity.  \n- The architecture diagram should better reflect the medical imaging context.  \n- The manuscript would benefit from a clearer distinction between general and precision medicine applications.  \n\n---\n\n**Summary Paragraph**  \nOverall, the manuscript presents a technically ambitious approach to patient-specific segmentation and demonstrates considerable implementation effort. Nonetheless, the conceptual positioning toward precision medicine is not convincingly supported, and several methodological and experimental aspects require stronger justification. Addressing the alignment between claims, motivation, and empirical evidence‚Äîalong with improving clarity and comparative evaluation‚Äîwould substantially enhance the paper‚Äôs contribution.\n\n---\n\n**Decision Recommendation**: **Reject** (major conceptual and methodological issues prevent a clear demonstration of contribution and relevance at this stage).",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors target patient-specific segmentation by proposing a new segmentation pipeline: (1) obtaining multiple point-prompts from the part-aware prompt mechanism, and (2) feeding these point-prompts to a prompt-based segmentation network, such as SAM. The authors also propose a similarity-based refinement to control the number of prompts found during step 1. Main experiments were conducted on the NSCLC and CVC-ClinicDB datasets where the proposed method achieves the state-of-the-art performances. Further experiments were conducted to show the method's performance against tracking algorithms and on the one-shot segmentation task.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The paper is well-written and easy to follow.\n2. The experiments are thorough, covering various important and related topics. \n3. The proposed refinement strategy on the number of prompts is conceptually easy and seems effective.\n\n### Weaknesses\n\n1. Unclearness in writing: (a) The motivation for Table 3 is unclear. It overlaps with Table 1 and 2 and brings little additional information. (b) For the baseline methods: direct-transfer and fine-tune, looks like no prompts are provided during the evaluation stage. Can the authors verify this is true?\n2. The contribution on the Part-aware Prompt Mechanism is unclear. Although the authors have demonstrated the effectiveness of the overall method, it is unclear if the improvements are from the part-based sampling strategy or merely because of having multiple prompts.\n3. (Minor) The application range of the method is limited to patients with multi-exams.\n\n### Questions\n\n1. To demonstrate the effectiveness of the Part-aware Prompt Mechanism, could the author compared to a modified version of PerSAM where instead of selecting \"two points with the highest and lowest confidence values\", the authors may select the top K and bot K' points where K and K' matches P^2SAM. In this way, the authors can demonstrate that points found by the Part-aware Prompt Mechanism are more effective. \n2. Since authors have shown better one-shot segmentation performance, I am curious to see if the authors can show their method's performance beyond patients with multi-exams. For example, for some medical segmentation datasets without patients with multi-exams, can the author randomly select one image-mask pair as the prior and test the model's performance on other pairs.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a patient-specific medical image segmentation pipeline consisting of two main components: a part-aware prompt mechanism that generates multiple point-prompts and a subsequent prompt-based segmentation network (e.g., SAM). A similarity-based refinement strategy is additionally proposed to control the number of prompts. Experiments are performed on the NSCLC and CVC-ClinicDB datasets, demonstrating state-of-the-art results, and further evaluations explore the method‚Äôs performance in tracking and one-shot segmentation tasks. Overall, the paper is clearly written and generally easy to follow, with a broad set of experiments supporting the proposed approach.\n\n**Major Comments**  \n1. **Clarity of Motivation and Baselines**: The motivation for including Table 3 is unclear because it appears to overlap with Tables 1 and 2 and adds limited new insights. Additionally, for the baseline methods (direct-transfer and fine-tune), it seems no prompts were provided at evaluation time. This should be clarified or verified.  \n2. **Contribution Attribution**: The specific contribution of the Part-aware Prompt Mechanism remains ambiguous. While the overall performance gains are evident, it is not clear whether improvements stem from the part-based sampling strategy itself or merely from using multiple prompts. More detailed ablation or comparison would strengthen the claim.  \n3. **Generality of Application**: The proposed method appears applicable only to patients with multiple exams, limiting its broader utility. Extending or discussing performance in settings without multi-exam data would enhance its impact.\n\n**Minor Comments**  \n- The paper is overall well-written, but some explanations could be more precise, particularly regarding motivation for certain tables.  \n- Clarify terminology in describing baseline and ablation settings.  \n- No ethical concerns are identified.\n\n**Summary Paragraph**  \nThe submission presents a clear and well-executed study with strong experimental results and a conceptually simple yet effective refinement mechanism. However, the paper would benefit from improved clarity regarding specific methodological contributions and the scope of applicability. Addressing these issues would enhance both transparency and general significance.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis study introduces a segmentation model called P2SAM which enables efficient, personalized medical image segmentation based on one-shot patient-specific data, improving segmentation performance across different patients and domains.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 4\n\n### Strengths\n\n1. The work introduces a \"part-aware prompt mechanism\" that leverages one-shot, patient-specific data for segmentation without additional fine-tuning. This feature is particularly suited to precision medicine applications where data annotation is minimal.\n\n2. Unlike standard SAM applications, P2SAM integrates a distribution-similarity-based retrieval approach that optimizes the selection of prompts, enhancing segmentation performance and reducing ambiguities.\n\n3. P2SAM is demonstrated to be effective across a variety of medical and natural image datasets, making it versatile for both patient-specific and general segmentation tasks.\n\n4. Quantitative Improvements: The model shows notable improvement over existing methods (PerSAM and Matcher), with significant gains in Dice scores and mIoU in various datasets, proving the robustness of the approach in handling out-of-distribution medical images.\n\n5. The paper presents extensive comparisons across multiple datasets and includes qualitative results that illustrate the model‚Äôs adaptability and performance in challenging segmentation tasks.\n\n### Weaknesses\n\n1. Complexity and Computational Cost: The proposed method, while innovative, involves complex modules (e.g., retrieval approach for optimal prompt selection) that may increase computational demands, potentially impacting usability in real-time clinical applications.\n\n2. Limited Generalization to Other Modalities: Although versatile within specific settings, the method's performance in more diverse medical imaging modalities (e.g., MRI or ultrasound) remains unexplored.\n\n3. Ambiguity in Module Contributions: The study could benefit from additional ablation studies to further clarify each component's role, such as the part-aware prompt mechanism versus the distribution-similarity-based retrieval.\n\n4. Dependence on SAM Backbone: While P2SAM leverages SAM effectively, it is still heavily reliant on SAM‚Äôs architecture. Any significant updates to SAM or its successors may require substantial adaptation of P2SAM.\n\n### Questions\n\n1. Scalability to Other Imaging Tasks: Could P2SAM be effectively adapted to other domains, such as pathological imaging, or expanded to handle 3D volumetric data with higher efficiency?\n\n2. Integration with SAM 2: The authors briefly mention SAM 2. How would the proposed retrieval mechanism and prompt system adapt to SAM 2's expanded capabilities in video and sequential data processing?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **P2SAM**, a segmentation framework designed for efficient and personalized medical image segmentation using one-shot, patient-specific data. The model incorporates mechanisms for part-aware prompting and distribution-similarity-based retrieval to improve segmentation accuracy across patients and domains. The paper is clearly structured and demonstrates solid experimental results, with an emphasis on applications to precision medicine. The overall quality of presentation and contribution is satisfactory, though certain methodological and generalization aspects could be clarified or expanded.\n\n**Major Comments**  \n1. **Model Complexity and Practicality** ‚Äì While the proposed architecture is conceptually strong, the inclusion of complex retrieval and prompting modules may lead to greater computational overhead, potentially limiting feasibility in time-sensitive clinical workflows.  \n2. **Generalization to Other Modalities** ‚Äì The experiments focus on specific datasets and modalities, leaving uncertain how well the approach transfers to other medical imaging types such as MRI or ultrasound. Broader empirical validation would strengthen claims of versatility.  \n3. **Component Contribution Analysis** ‚Äì The manuscript would benefit from additional ablation experiments to disentangle the effects of key components (e.g., part-aware prompt mechanism vs. retrieval-based selection) and clarify their respective contributions to performance.  \n4. **Dependence on SAM Backbone** ‚Äì P2SAM is closely tied to the SAM architecture, which may limit adaptability if newer backbone models, such as SAM 2, change core mechanisms. Discussion of long-term maintainability and transferability would be valuable.\n\n**Minor Comments**  \n- The manuscript could further discuss scalability to alternative imaging domains or 3D volumetric data to highlight future applicability.  \n- Clarification on how the method integrates or adapts to extensions of SAM (e.g., SAM 2 for video or sequential data) would enhance completeness.  \n- Presentation, figures, and notations are generally clear, with no major clarity issues noted.\n\n**Summary Paragraph**  \nOverall, the paper introduces a technically sound and promising method that advances personalized segmentation by integrating patient-specific prompting mechanisms. Its empirical performance across multiple datasets indicates robustness and adaptability. However, aspects of computational cost, dependence on SAM, and limited exploration of broader modalities temper its generalization claims. Strengths lie in originality and experimental depth, while weaknesses concern practical scalability and component-level transparency.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "---\n\n## **Phase‚ÄØ1‚ÄØ: Analytical‚ÄØand‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI‚ÄØ‚Äì‚ÄØ‚úÖ**  \nThe manuscript proposes *P¬≤SAM ‚Äì Part‚Äëaware Personalized Segment Anything Model*, a segmentation strategy intended for **patient‚Äëspecific medical imaging**. The work introduces a new prompt‚Äëbased mechanism and a distribution‚Äësimilarity retrieval procedure compatible with general promptable models (SAM,‚ÄØSAM‚ÄØ2). It squarely targets methodological innovation in image segmentation, not an application‚Äëonly study. Thus, the paper fits TMI‚Äôs methodological scope.\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel‚ÄØ‚Äì‚ÄØMedium‚ÄëHigh**  \nInnovation lies in formalizing *patient‚Äëspecific segmentation as an in‚Äëcontext segmentation problem* and augmenting SAM with multi‚Äëpoint, *part‚Äëaware* prompts plus a *distribution‚Äësimilarity retrieval* criterion to select the number of parts automatically. These ideas extend prior PerSAM and Matcher works by introducing (i) a feature‚Äëdriven prompt generation process and (ii) a principled adaptation heuristic. While this builds on existing SAM foundations, it provides recognizable novelty and conceptual contribution at the method level ‚Äî likely within TMI‚Äôs novelty threshold.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor‚ÄØ‚Äì‚ÄØModerate‚ÄØto‚ÄØGood**  \nExperiments cover multiple medical domains (CT lung, endoscopy) and compare P¬≤SAM against strong baselines on both *in‚Äëdistribution* and *out‚Äëof‚Äëdistribution* datasets, with ablation and cross‚Äëdomain extension to natural images. Implementation details, hyperparameters, and retrieval settings are extensive (Appendices‚ÄØB‚ÄìC).  \nHowever:  \n- Reproducibility of one‚Äëshot ‚Äúprior‚Äù data setup could benefit from explicit configuration scripts or public code.  \n- Uncertainty analysis and statistical significance testing are absent.  \n- Although IRB aspects are likely met via public datasets, explicit ethical statements should be added.\n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation‚ÄØ‚Äì‚ÄØGood‚ÄØbut‚ÄØdense**  \nThe manuscript is technically detailed and readable for an imaging‚Äëmethodology audience. Nonetheless, figure readability (visual scale/labels), equation typesetting, and excess references lead to occasional clutter. Editorial tightening would aid flow.  \n\n**5.‚ÄØEthical‚ÄØand‚ÄØReproducibility‚ÄØCompliance‚ÄØ‚Äì‚ÄØPartially‚ÄØaddressed**  \nAuthors use public datasets (TCIA,‚ÄØKvasir,‚ÄØCVC), promising code release on acceptance. Explicit mention of IRB exemption and patient anonymization should appear in the Methods.\n\n---\n\n## **Phase‚ÄØ2‚ÄØ:‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n### **1.‚ÄØSummary**\nThe paper presents *P¬≤SAM*, a **training‚Äëfree, patient‚Äëspecific segmentation framework** leveraging the Segment‚ÄëAnything model‚Äôs promptable structure. Instead of fine‚Äëtuning, P¬≤SAM clusters reference‚Äëimage features into part‚Äëlevel embeddings that guide multi‚Äëpoint positive/negative prompts for a new patient image. A *distribution‚Äësimilarity retrieval* (using Wasserstein distance) then determines the optimal number of parts. Demonstrations on CT lung tumor and endoscopy polyp segmentation show performance gains of‚ÄØ‚âà‚ÄØ8‚ÄØ%‚ÄØDice over comparable methods, with cross‚Äëdomain validation on natural‚Äëimage few‚Äëshot benchmarks.\n\n### **2.‚ÄØStrengths**\n- Clear methodological innovation in prompt selection and adaptive part‚Äëcount retrieval.  \n- Sound integration with foundation models‚ÄØ(SAM/SAM‚ÄØ2) without retraining.  \n- Comprehensive experiments, including ablations and extension to multiple domains.  \n- Addresses a clinically relevant scenario ‚Äî *patient‚Äëspecific (one‚Äëshot) segmentation* ‚Äî under the precision‚Äëmedicine paradigm.  \n\n### **3.‚ÄØWeaknesses**\n- Improvements over PerSAM after domain adaptation are modest; novelty may appear incremental relative to other prompt‚Äëselection strategies.  \n- Statistical validation of performance differences is missing.  \n- Ethical and data‚Äëuse compliance needs explicit statement.  \n- Paper length and figure density affect readability; discussion could better isolate clinical significance.  \n- Some domain‚Äëadaptation steps are tangential and could be streamlined.  \n\n### **4.‚ÄØMajor‚ÄØComments**\n1. **Clarify novelty relative to Matcher and PerSAM.** Provide algorithmic and conceptual comparisons (e.g., computational cost, robustness) in a dedicated subsection.  \n2. **Statistical significance and uncertainty.** Report variance or confidence intervals for Dice/mIoU; possibly perform Wilcoxon or paired t‚Äëtests on patient‚Äëwise folds.  \n3. **Reproducibility.** Supply pseudocode for the retrieval process, specify seed control, and release a minimal code snapshot.  \n4. **Ethics statement.** Explicitly confirm datasets are public/anonymized and no new patient data or identifiable information were used.  \n5. **Discussion depth.** Expand on clinical feasibility‚Äîe.g., execution time per case and how one‚Äëshot prior images would be collected in clinical pipelines.  \n\n### **5.‚ÄØMinor‚ÄØComments**\n- Improve figure legends and resize multi‚Äëpanel figures for readability.  \n- Fix minor typographic errors (spacing, subscripts).  \n- Cite recent TMI works on adaptive or federated segmentation to situate within field.  \n- Reorder Appendix‚ÄØsections: AMI summary first, less critical visualizations last.  \n- Include a short limitations paragraph in the main text, not only the Appendix.  \n\n### **6.‚ÄØOverall‚ÄØRecommendation**\n**Major‚ÄØRevision** ‚Äî The paper presents a valuable methodological contribution suitable for TMI, yet requires clearer articulation of novelty, stronger statistical evidence, and tighter presentation before acceptance. With these revisions, it could reach publishable quality.\n\n### **7.‚ÄØConfidence‚ÄØLevel‚ÄØ**\n**4‚ÄØ/‚ÄØ5‚ÄØ‚Äì‚ÄØHigh confidence.** I am familiar with SAM‚Äëbased segmentation and medical imaging adaptation. The assessment reflects domain understanding and TMI standards.\n\n---\n\n**Summary‚ÄØVerdict:**  \nP¬≤SAM extends SAM to patient‚Äëspecific one‚Äëshot segmentation with a novel part‚Äëaware prompting framework. The study is methodologically relevant, technically sound, and potentially impactful for personalized imaging, but requires additional statistical rigor, explicit ethical compliance, and improved clarity prior to publication in *IEEE‚ÄØTransactions‚ÄØon‚ÄØMedical‚ÄØImaging*.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *P¬≤SAM ‚Äì Part‚Äëaware Personalized Segment Anything Model*, a training‚Äëfree segmentation framework designed for patient‚Äëspecific medical imaging tasks. By leveraging the promptable structure of large foundation models such as SAM and SAM‚ÄØ2, P¬≤SAM clusters reference‚Äëimage features into part‚Äëlevel embeddings that generate adaptive multi‚Äëpoint prompts for new patient images. A distribution‚Äësimilarity retrieval mechanism determines the optimal number of parts. The study is methodologically oriented, offering an innovative approach to one‚Äëshot or patient‚Äëspecific segmentation, with demonstrations across CT lung and endoscopy datasets. The presentation is technically detailed and generally clear, though occasionally dense.  \n\n**Major Comments**  \n1. **Clarification of novelty and positioning.** The authors should more explicitly differentiate P¬≤SAM from PerSAM and Matcher, describing algorithmic distinctions, computational efficiency, and robustness advantages. The current description risks appearing incremental.  \n2. **Statistical and uncertainty analysis.** Performance improvements are promising but require quantitative validation. Dice and mIoU results should include variance or confidence intervals, with appropriate statistical testing (e.g., paired or nonparametric tests).  \n3. **Reproducibility.** While appendices include parameters, full reproducibility would benefit from pseudocode for the retrieval process, clear seed specification, and provision of minimal configuration scripts or public code.  \n4. **Ethical compliance.** The manuscript should explicitly confirm that all datasets (TCIA,‚ÄØKvasir,‚ÄØCVC) are publicly available, anonymized, and IRB‚Äëexempt, with no new patient data collected.  \n5. **Clinical feasibility discussion.** Expand on the deployment perspective‚Äîexpected runtime, requirements for obtaining one‚Äëshot prior images, and integration into clinical workflows.  \n\n**Minor Comments**  \n- Improve readability of multi‚Äëpanel figures and enlarge labels.  \n- Correct minor typographical and formatting inconsistencies.  \n- Cite recent work on adaptive or federated segmentation for context.  \n- Reorder appendices to present key algorithmic summaries earlier.  \n- Add a short limitations paragraph within the main text.  \n\n**Summary Paragraph**  \nOverall, the study presents a meaningful advance in adapting foundation models for patient‚Äëspecific segmentation through a novel part‚Äëaware prompting and retrieval strategy. Its strengths lie in methodological clarity, cross‚Äëdomain evaluation, and clinical relevance. However, clearer articulation of novelty, inclusion of statistical validation, explicit ethical statements, and improved presentation are needed to meet publication standards.  \n\n**Decision Recommendation**  \n**Major Revision.** The work is promising and within the journal‚Äôs methodological scope, but requires additional rigor and clarification before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes P2SAM (Part-aware Personalized Segment Anything Model), a data-efficient segmentation method for patient-specific applications. The method addresses the challenge of adapting segmentation models to new patients using only one-shot prior data without model fine-tuning (Section 1). P2SAM introduces a part-aware prompt mechanism that clusters foreground features from reference images into multiple parts using k-means++ clustering (Equation 1, Section 3.2), then selects multiple-point prompts based on cosine similarity between part-level features and target image features (Equation 2). A distribution-similarity-based retrieval approach using Wasserstein distance determines the optimal number of parts for each case (Section 3.2, Figure 5). The method can be integrated into various promptable segmentation models like SAM and SAM 2. Experiments on two patient-specific segmentation tasks show improvements of +8.0% and +2.0% mean Dice score, with additional validation on natural image benchmarks achieving 95.7% mIoU on PerSeg (Tables 1-2, Table 4).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation lacks precision and clarity**\n  - Equation 1 uses the symbol \"‚ó¶\" for mask selection without formal definition, making the operation ambiguous for readers unfamiliar with this notation\n  - The clustering procedure in Section 3.2 mentions k-means++ but doesn't specify how the number of clusters n relates to the retrieval range, creating inconsistency between methodology and implementation\n  - The Wasserstein distance computation for distribution similarity (Section 3.2, Table 7) lacks mathematical formulation, making it difficult to reproduce the approach\n\n‚Ä¢ **Experimental design contains methodological inconsistencies**\n  - Table 1 shows the \"Meta huge\" model achieves only 28.52% Dice score while fine-tuned versions reach 60%+, but no analysis explains this dramatic performance gap or validates the fairness of comparison\n  - The retrieval ranges in Tables 9-10 vary significantly across datasets and models without principled justification, suggesting ad-hoc hyperparameter tuning rather than systematic methodology\n  - Comparison with MedSAM in Table 3 uses human-provided box prompts during inference while P2SAM uses only first-visit ground truth, creating an unfair evaluation setup that undermines the claimed advantages\n\n‚Ä¢ **Limited technical novelty and insufficient baseline coverage**\n  - The core contribution of using multiple prompts instead of single prompts is incremental, as acknowledged in Section 2 where SAM's original paper states \"ambiguity is much rarer with multiple prompts\"\n  - Missing comparisons with recent medical segmentation methods beyond MedSAM, particularly other SAM-based medical approaches mentioned in Section F but not evaluated\n  - The k-means++ clustering for part-level features (Section 3.2) is a standard technique without novel adaptation for medical imaging characteristics, limiting the technical contribution\n\n‚Ä¢ **Evaluation methodology shows significant limitations**\n  - The patient-specific segmentation tasks only include 13 patients with multiple visits from 4D-Lung dataset (Section 4.1), which is insufficient for robust statistical validation of the claimed improvements\n  - Qualitative results in Figures 6-9 show cherry-picked examples without systematic analysis of failure cases or statistical significance testing of the quantitative improvements\n  - Cross-domain generalization claims are undermined by the large performance variations across datasets (Tables 1-2) without adequate analysis of when and why the method succeeds or fails\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and reproducibility**\n  - Provide formal mathematical definition for the mask selection operator \"‚ó¶\" in Equation 1 and specify the exact clustering algorithm parameters\n  - Add complete mathematical formulation for the Wasserstein distance computation used in the retrieval approach, including implementation details\n  - Establish principled criteria for determining retrieval ranges instead of the current ad-hoc approach shown in Tables 9-10\n\n‚Ä¢ **Strengthen experimental validation and fairness**\n  - Conduct systematic analysis explaining the performance gap between Meta and fine-tuned models, including statistical significance tests for all reported improvements\n  - Implement standardized comparison protocols ensuring all methods use equivalent information during inference, particularly for the MedSAM comparison\n  - Establish unified hyperparameter selection methodology across all datasets and provide sensitivity analysis for key parameters like the number of parts n\n\n‚Ä¢ **Expand technical contributions and baseline coverage**\n  - Develop novel clustering or feature selection techniques specifically adapted for medical imaging characteristics rather than relying solely on standard k-means++\n  - Include comprehensive comparisons with recent SAM-based medical segmentation methods mentioned in the related work but omitted from experiments\n  - Provide deeper analysis of the relationship between prompt selection strategies and segmentation performance to establish clearer technical insights beyond the incremental multiple-prompt approach\n\n‚Ä¢ **Improve evaluation comprehensiveness and statistical rigor**\n  - Expand the patient cohort for validation beyond the current 13 patients, and include power analysis to determine adequate sample sizes for statistical validation\n  - Implement systematic failure case analysis and provide statistical significance testing for all quantitative comparisons rather than relying primarily on qualitative examples\n  - Develop comprehensive analysis framework explaining performance variations across different medical imaging modalities and domains to better understand method limitations and applicability",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **P2SAM (Part-aware Personalized Segment Anything Model)**, a data-efficient segmentation method designed for patient-specific applications. The approach aims to adapt segmentation models to new patients using only one-shot prior data, without model fine-tuning. It introduces a **part-aware prompt mechanism** that clusters foreground features from reference images into multiple parts using k-means++ and selects representative prompts based on cosine similarity. A **distribution-similarity-based retrieval method** using the Wasserstein distance determines the optimal number of parts per case. The method is adaptable to different promptable segmentation frameworks, such as SAM and SAM2, and experiments show quantitative improvements on medical and natural image benchmarks.  \n\n**Major Comments**  \n1. **Mathematical formulation issues:** Key notations lack clarity. The mask selection operator ‚Äú‚ó¶‚Äù in Equation 1 is undefined, the relation between cluster number *n* and retrieval range in Section 3.2 is unspecified, and the Wasserstein-based distribution similarity lacks a formal mathematical definition, limiting reproducibility.  \n2. **Methodological inconsistencies in experiments:** Large unexplained performance gaps appear between the ‚ÄúMeta huge‚Äù baseline and fine-tuned variants. Retrieval ranges differ widely without justification, suggesting ad-hoc tuning. The comparison with MedSAM is potentially unfair, as different prompting strategies are used.  \n3. **Limited novelty and incomplete baseline coverage:** The main innovation‚Äîmultiple part-aware prompts‚Äîis incremental relative to prior SAM designs. Comparisons exclude several relevant SAM-based medical segmentation methods, and the clustering technique (k-means++) is standard, with no adaptation to domain specifics.  \n4. **Insufficient evaluation validity:** The dataset includes only 13 patients, providing weak statistical reliability. Qualitative figures appear selectively chosen, lacking failure-case analysis or significance testing. Reported cross-domain generalization is inconsistent across datasets without accompanying explanation.  \n\n**Minor Comments**  \n- Figures and tables should explicitly define all notations and parameters.  \n- Ensure consistent reporting of retrieval ranges and clustering parameters.  \n- Clarify the distinction between sections describing methodology and experimental design for better readability.  \n\n**Summary Paragraph**  \nOverall, the study proposes a clear and potentially practical framework for patient-specific segmentation but suffers from **unclear formulation, limited methodological rigor, and modest novelty**. The evaluation lacks statistical solidity and fair comparison protocols, constraining the credibility of its performance claims. Strengthening mathematical definitions, standardizing evaluation procedures, and expanding comparative baselines would notably improve the manuscript‚Äôs reproducibility and impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper introduces an interesting concept but requires substantial methodological clarification, expanded evaluation, and improved experimental fairness before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces P2SAM, a novel method for patient-specific segmentation that leverages one-shot patient-specific data without requiring additional model fine-tuning. The core innovation lies in a part-aware prompt mechanism that selects multiple-point prompts based on part-level features of the one-shot data, combined with a distribution-similarity-based retrieval approach to determine the optimal number of part-level features. The authors demonstrate improvements in mean Dice scores across different patient-specific segmentation tasks and achieve a new state-of-the-art result on the personalized segmentation benchmark PerSeg. The manuscript is well-written, and the motivation for addressing the challenges of limited annotated data and high variability across patients is clearly articulated.\n\n## Major Comments\n1. Novelty and Positioning: While the part-aware prompt mechanism and distribution-similarity-based retrieval approach are innovative, the manuscript does not sufficiently distinguish itself from existing work on few-shot and in-context learning methods. Specifically, the authors should clarify how their approach differs from recent studies such as Matcher (Liu et al., 2023) and PerSAM (Zhang et al., 2023), which also address few-shot segmentation problems. The manuscript should provide a more detailed discussion of the strengths and weaknesses relative to these methods.\n\n2. Evaluation Design: The experiments are primarily conducted on two medical datasets and one natural image benchmark. While the datasets are relevant, the evaluation could be strengthened by including additional datasets that vary in terms of anatomy and imaging modalities. Furthermore, the authors should consider evaluating P2SAM on a wider range of segmentation tasks to better assess its generalizability and robustness.\n\n3. Comparisons: The baseline comparisons are comprehensive, but the manuscript could benefit from including more recent and relevant methods, such as the latest advancements in few-shot and personalized segmentation techniques. Additionally, the manuscript should provide a clear rationale for why certain methods were excluded from the comparison.\n\n4. Reproducibility: The authors state that the code will be released upon acceptance, but the manuscript lacks sufficient detail regarding the training protocols, preprocessing steps, and model hyperparameters. Providing more explicit information on these aspects is crucial for ensuring reproducibility.\n\n## Minor Comments\n1. Figures: Figures 6, 7, 8, 9, 10, and 11 are quite cluttered and could be improved by showing fewer representative slices with zoomed-in regions.\n   \n2. Notation Consistency: The notation for the forward operator is inconsistent throughout Section 2.1 and should be clarified.\n   \n3. Acronyms: Several acronyms, such as \"R=4,\" are used without definition, which can be confusing for readers.\n   \n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7) should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical image segmentation: personalizing segmentation models to handle the variability across different patients with limited annotated data. The proposed P2SAM method, which incorporates a part-aware prompt mechanism and a distribution-similarity-based retrieval approach, is technically sound and shows promise in improving segmentation performance. However, the evaluation is somewhat limited in scope, focusing on specific datasets and tasks, which weakens the claims of generalizability. The reproducibility of the approach is also a concern due to the lack of detailed methodological descriptions. Overall, while the idea has merit, the current evidence does not fully meet the rigorous standards expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis, strengthen validation across a broader range of datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **P2SAM**, a method for patient-specific segmentation that utilizes one-shot data without additional fine-tuning. The key innovation is a part-aware prompt mechanism that selects multiple-point prompts through part-level feature analysis, paired with a distribution-similarity-based retrieval strategy to determine the appropriate number of part-level features. The method reports improved mean Dice scores across several patient-specific segmentation tasks and achieves leading results on the PerSeg benchmark. The paper is clearly written, and the motivation‚Äîaddressing limited annotated data and inter-patient variability‚Äîis well articulated.\n\n**Major Comments**  \n1. **Novelty and Positioning** ‚Äì Although the part-aware prompt mechanism and distribution-similarity retrieval are promising ideas, the manuscript does not clearly delineate its distinction from existing few-shot or in-context segmentation methods. The authors should discuss in greater depth how P2SAM contrasts with recent approaches such as Matcher (Liu et al., 2023) and PerSAM (Zhang et al., 2023), highlighting relative strengths and weaknesses.  \n2. **Evaluation Design** ‚Äì The study evaluates performance primarily on two medical datasets and one natural image benchmark. Broader validation across datasets that vary by anatomy and imaging modality would strengthen the evidence for generalizability and robustness.  \n3. **Comparative Analysis** ‚Äì Baseline selection is generally solid, but inclusion of additional recent personalized or few-shot segmentation methods would offer a more complete picture. The authors should also clarify the rationale behind excluded baselines.  \n4. **Reproducibility** ‚Äì Despite the intention to release code upon acceptance, the paper omits key details on training protocols, preprocessing, and hyperparameter settings. Providing these specifics is necessary to ensure reproducibility and future benchmarking.\n\n**Minor Comments**  \n- **Figures:** Figures 6‚Äì11 appear crowded; simplifying them by showing fewer examples with zoomed-in details would improve readability.  \n- **Notation:** Inconsistent notation for the forward operator in Section‚ÄØ2.1 needs clarification.  \n- **Acronyms:** Define all acronyms (e.g., ‚ÄúR=4‚Äù) upon first use.  \n- **Typos:** Correct minor errors such as ‚Äúk-spacce‚Äù (p.‚ÄØ6) and ‚Äúundersampling maskes‚Äù (p.‚ÄØ7).\n\n**Summary Paragraph**  \nOverall, the manuscript tackles an important issue in medical image segmentation‚Äîadapting models to patient-specific differences from limited data. The technical approach is well-motivated and yields encouraging quantitative results. However, the limited evaluation scope, incomplete methodological detail, and insufficient comparison with closely related methods reduce the overall impact and reproducibility. Substantial revision is required to meet publication standards.  \n\n**Decision Recommendation**  \n**Major Revision.** The authors should broaden comparisons, diversify dataset evaluation, and enhance methodological transparency to substantiate the current claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## PART-AWARE PERSONALIZED SEGMENT ANYTHING MODEL FOR PATIENT-SPECIFIC SEGMENTATION\n\n### Summary\n\nThe paper proposes P2SAM, a training-free, data-efficient method for patient-specific segmentation that adapts promptable segmentation models (SAM and SAM 2) to new patients using a single reference image-mask pair. The key idea is a part-aware prompt mechanism: cluster foreground features from the reference to obtain part-level prototypes, compute cosine similarity maps on the target image, and place multiple positive (and, for medical images, multiple negative) point prompts for the decoder. To mitigate prompt outliers and tune the number of parts, the authors introduce a retrieval strategy that selects the part count minimizing a distribution distance (Wasserstein) between the reference and predicted target foreground feature sets. The method achieves consistent gains across two medical tasks and shows generality on natural-image one-shot benchmarks.\n\n### Strengths\n\n- Technical novelty and innovationThe part-aware prompt mechanism is simple, training-free, and model-agnostic, improving upon single-point prompting (PerSAM) and patch-level prompting (Matcher).The distribution-similarity-based retrieval for selecting the number of parts addresses prompt outliers and adapts to case variability, with comparative analyses across multiple distance measures.Integration with both SAM and SAM 2, and applicability across medical and natural-image domains, underscore practical versatility.\n- Experimental rigor and validationEvaluations on two clinically motivated patient-specific tasks (NSCLC on 4D-Lung; polyps on CVC-ClinicDB) with different SAM backbones (Meta, LoRA, Full-Fine-Tune) provide a thorough look at deployment regimes.Competent ablations on part count, retrieval, similarity metrics, and model size, including comparison against PerSAM and Matcher, support the design choices.Extension to standard one-shot benchmarks (COCO-20i, FSS-1000, LVIS-92i) and PerSeg with consistent gains strengthens generality claims.\n- Clarity of presentationThe methodological pipeline is clearly structured (feature extraction, clustering, similarity maps, prompt selection, retrieval), with intuitive figures demonstrating ambiguity reduction and prompt placement.The paper separates core contributions (training-free prompting) from optional domain adaptation to SAM, aiding readers‚Äô understanding of where gains originate.\n- Significance of contributionsAddresses a real and important pain point in precision medicine: robust, single-reference adaptation to new patients without fine-tuning.Provides a generally useful prompting principle for promptable segmentation models that can benefit both clinical and non-clinical applications.\n\n- The part-aware prompt mechanism is simple, training-free, and model-agnostic, improving upon single-point prompting (PerSAM) and patch-level prompting (Matcher).\n- The distribution-similarity-based retrieval for selecting the number of parts addresses prompt outliers and adapts to case variability, with comparative analyses across multiple distance measures.\n- Integration with both SAM and SAM 2, and applicability across medical and natural-image domains, underscore practical versatility.\n\n- Evaluations on two clinically motivated patient-specific tasks (NSCLC on 4D-Lung; polyps on CVC-ClinicDB) with different SAM backbones (Meta, LoRA, Full-Fine-Tune) provide a thorough look at deployment regimes.\n- Competent ablations on part count, retrieval, similarity metrics, and model size, including comparison against PerSAM and Matcher, support the design choices.\n- Extension to standard one-shot benchmarks (COCO-20i, FSS-1000, LVIS-92i) and PerSeg with consistent gains strengthens generality claims.\n\n- The methodological pipeline is clearly structured (feature extraction, clustering, similarity maps, prompt selection, retrieval), with intuitive figures demonstrating ambiguity reduction and prompt placement.\n- The paper separates core contributions (training-free prompting) from optional domain adaptation to SAM, aiding readers‚Äô understanding of where gains originate.\n\n- Addresses a real and important pain point in precision medicine: robust, single-reference adaptation to new patients without fine-tuning.\n- Provides a generally useful prompting principle for promptable segmentation models that can benefit both clinical and non-clinical applications.\n\n### Weaknesses\n\n- Technical limitations or concernsThe use of Wasserstein distance ‚Äúfollowing WGAN‚Äù for high-dimensional feature distributions is under-specified; exact computation vs. approximation (e.g., exact OT, sliced Wasserstein, critic-based approximation) and computational complexity are unclear.The method operates on 2D slices for 3D CT; implications for volumetric consistency and 3D deployment (runtime and memory when retrieving across slices) are not discussed.Prompt selection takes the single argmax per part from noisy similarity maps; no smoothing, local non-maximum suppression, or spatial constraints are described to reduce spurious maxima.\n- Experimental gaps or methodological issuesFairness of baselines: re-implementations of PANet and Matcher ‚Äúsharing the same SAM encoder‚Äù may not reflect their intended training/backbone regimes and could disadvantage them; details of re-training or adaptation are limited.Comparisons omit some recent SAM-based medical one-shot or prompt-engineering methods (e.g., Med-PerSAM, CycleSAM, SAMIC) that specifically target medical or few-shot prompting.No statistical significance, variance, or per-patient variability analyses are reported; the small number of patients and visits in 4D-Lung calls for uncertainty quantification.\n- Clarity or presentation issuesOccasional formatting artifacts (e.g., P\"SAM vs P¬≤SAM) and minor equation rendering issues impede precision in some places.The ‚ÄúHungarian‚Äù and ‚ÄúJensen‚ÄìShannon‚Äù variants in retrieval are described briefly; algorithmic details (e.g., clustering settings, matching cost, PC selection) are insufficient for reproduction.\n- Missing related work or comparisonsLimited engagement with concurrent prompt selection/engineering pipelines that learn or regularize prompting, and with bounding-box-driven SAM 2 polyp pipelines (detection + SAM 2) which are strong in videos.A deeper connection to part-aware prototype literature in FSS beyond citing ECCV‚Äô20 would strengthen context.\n\n- The use of Wasserstein distance ‚Äúfollowing WGAN‚Äù for high-dimensional feature distributions is under-specified; exact computation vs. approximation (e.g., exact OT, sliced Wasserstein, critic-based approximation) and computational complexity are unclear.\n- The method operates on 2D slices for 3D CT; implications for volumetric consistency and 3D deployment (runtime and memory when retrieving across slices) are not discussed.\n- Prompt selection takes the single argmax per part from noisy similarity maps; no smoothing, local non-maximum suppression, or spatial constraints are described to reduce spurious maxima.\n\n- Fairness of baselines: re-implementations of PANet and Matcher ‚Äúsharing the same SAM encoder‚Äù may not reflect their intended training/backbone regimes and could disadvantage them; details of re-training or adaptation are limited.\n- Comparisons omit some recent SAM-based medical one-shot or prompt-engineering methods (e.g., Med-PerSAM, CycleSAM, SAMIC) that specifically target medical or few-shot prompting.\n- No statistical significance, variance, or per-patient variability analyses are reported; the small number of patients and visits in 4D-Lung calls for uncertainty quantification.\n\n- Occasional formatting artifacts (e.g., P\"SAM vs P¬≤SAM) and minor equation rendering issues impede precision in some places.\n- The ‚ÄúHungarian‚Äù and ‚ÄúJensen‚ÄìShannon‚Äù variants in retrieval are described briefly; algorithmic details (e.g., clustering settings, matching cost, PC selection) are insufficient for reproduction.\n\n- Limited engagement with concurrent prompt selection/engineering pipelines that learn or regularize prompting, and with bounding-box-driven SAM 2 polyp pipelines (detection + SAM 2) which are strong in videos.\n- A deeper connection to part-aware prototype literature in FSS beyond citing ECCV‚Äô20 would strengthen context.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe core idea‚Äîusing reference-part features to place multiple positive prompts on the target‚Äîaligns with SAM‚Äôs observation that ambiguity decreases with more prompts, and is technically sound and simple to implement.The retrieval mechanism is sensible, but the choice and computation of Wasserstein distance over high-dimensional encoder embeddings require explicit specification: whether it is exact OT (and with what solver), sliced/approximate OT, or a critic-based approximation; how many samples are used; and runtime scaling in n and image size.For medical images, using multiple negative prompts drawn from background features is reasonable given high background similarity across visits; however, the selection‚Äôs robustness to large anatomical or acquisition changes should be analyzed.The reliance on a single reference mask assumes clean annotations; sensitivity to mislabeled or coarse reference masks merits analysis.\n- Experimental evaluation assessmentPatient-specific protocols are appropriate and clinically relevant: first visit/frame as reference is a realistic assumption. The improvements over PerSAM and Matcher are consistent, and ablations are helpful.The choice to fine-tune SAM on in-distribution sets and then test on out-of-distribution sets is appropriate; however, per-slice vs per-volume evaluation for 4D-Lung should be clarified, as should aggregation (per-visit/per-patient averaging).Runtime is not reported. Since retrieval requires evaluating multiple part counts and computing distances, end-to-end latency (encoder forward(s), decoder forward(s), clustering, distance computations) should be quantified.Statistical confidence (e.g., standard deviation across patients/visits/videos) and sensitivity to n‚Äôs search range would strengthen the claims.\n- Comparison with related work (using the summaries provided)Compared with PerSAM (single prompt) and Matcher (patch-level prompting), P2SAM‚Äôs part-aware prompting offers a principled middle ground that reduces ambiguity without a heavy external feature backbone; results substantiate this.CycleSAM (surgery) demonstrates multi-point prompt sampling with boundary negatives alongside feature adaptation and consistency constraints; while task/domain differ, discussing overlaps (e.g., multi-point selection, boundary negatives) and differences (no training in P2SAM, retrieval by distribution similarity) would improve context.Med-PerSAM (warping-based prompt generation) and SAMIC (learned spatial prompt network) present alternative ways to obtain reliable prompts in medical/few-shot settings; a discussion of relative pros/cons (training-free vs lightweight learned prompt networks/flow modules; robustness to large deformations; temporal modeling for video) would frame P2SAM‚Äôs niche more clearly.For polyp video, YOLO-SAM 2 highlights a detector+SAM 2 regime achieving strong results with only box annotations; acknowledging this complementary line and clarifying where P2SAM excels (e.g., when only one labeled mask at first frame is available and detectors are unavailable) would be valuable.\n- Discussion of broader impact and significanceThe approach is attractive for clinical adoption due to zero training and minimal annotation burden; it can be inserted into existing SAM-based workflows to reduce ambiguity and improve robustness.Risks include over-reliance on SAM encoders trained on natural images; domain adaptation helps but should be carefully validated across institutions. Misplaced prompts could propagate errors; human-in-the-loop safeguards might be helpful.Extension to 3D segmentation (full volumes) and to multi-lesion cases is a natural next step with potentially strong clinical impact; current limitations are acknowledged.\n\n- The core idea‚Äîusing reference-part features to place multiple positive prompts on the target‚Äîaligns with SAM‚Äôs observation that ambiguity decreases with more prompts, and is technically sound and simple to implement.\n- The retrieval mechanism is sensible, but the choice and computation of Wasserstein distance over high-dimensional encoder embeddings require explicit specification: whether it is exact OT (and with what solver), sliced/approximate OT, or a critic-based approximation; how many samples are used; and runtime scaling in n and image size.\n- For medical images, using multiple negative prompts drawn from background features is reasonable given high background similarity across visits; however, the selection‚Äôs robustness to large anatomical or acquisition changes should be analyzed.\n- The reliance on a single reference mask assumes clean annotations; sensitivity to mislabeled or coarse reference masks merits analysis.\n\n- Patient-specific protocols are appropriate and clinically relevant: first visit/frame as reference is a realistic assumption. The improvements over PerSAM and Matcher are consistent, and ablations are helpful.\n- The choice to fine-tune SAM on in-distribution sets and then test on out-of-distribution sets is appropriate; however, per-slice vs per-volume evaluation for 4D-Lung should be clarified, as should aggregation (per-visit/per-patient averaging).\n- Runtime is not reported. Since retrieval requires evaluating multiple part counts and computing distances, end-to-end latency (encoder forward(s), decoder forward(s), clustering, distance computations) should be quantified.\n- Statistical confidence (e.g., standard deviation across patients/visits/videos) and sensitivity to n‚Äôs search range would strengthen the claims.\n\n- Compared with PerSAM (single prompt) and Matcher (patch-level prompting), P2SAM‚Äôs part-aware prompting offers a principled middle ground that reduces ambiguity without a heavy external feature backbone; results substantiate this.\n- CycleSAM (surgery) demonstrates multi-point prompt sampling with boundary negatives alongside feature adaptation and consistency constraints; while task/domain differ, discussing overlaps (e.g., multi-point selection, boundary negatives) and differences (no training in P2SAM, retrieval by distribution similarity) would improve context.\n- Med-PerSAM (warping-based prompt generation) and SAMIC (learned spatial prompt network) present alternative ways to obtain reliable prompts in medical/few-shot settings; a discussion of relative pros/cons (training-free vs lightweight learned prompt networks/flow modules; robustness to large deformations; temporal modeling for video) would frame P2SAM‚Äôs niche more clearly.\n- For polyp video, YOLO-SAM 2 highlights a detector+SAM 2 regime achieving strong results with only box annotations; acknowledging this complementary line and clarifying where P2SAM excels (e.g., when only one labeled mask at first frame is available and detectors are unavailable) would be valuable.\n\n- The approach is attractive for clinical adoption due to zero training and minimal annotation burden; it can be inserted into existing SAM-based workflows to reduce ambiguity and improve robustness.\n- Risks include over-reliance on SAM encoders trained on natural images; domain adaptation helps but should be carefully validated across institutions. Misplaced prompts could propagate errors; human-in-the-loop safeguards might be helpful.\n- Extension to 3D segmentation (full volumes) and to multi-lesion cases is a natural next step with potentially strong clinical impact; current limitations are acknowledged.\n\n### Questions for Authors\n\n- How exactly is the Wasserstein distance computed between high-dimensional foreground feature sets? Do you use exact OT with a ground metric (e.g., Euclidean), sliced Wasserstein, or a critic-based approximation? Please report solver details, sample sizes, runtime, and sensitivity to feature dimensionality.\n- What is the runtime of the entire method per target image under typical N and n ranges (encoder + decoder passes, clustering, similarity maps, and retrieval)? How does it scale with image resolution and the number of candidate part counts?\n- For 4D-Lung, are metrics computed per-slice or per-volume and then averaged per-visit/patient? Please clarify aggregation protocol and provide per-patient variance/standard deviation.\n- How robust is P2SAM to noisy or partially incorrect reference masks? Have you tested robustness to synthetic perturbations on the reference mask or to inter-observer variability?\n- For negative prompt selection in medical data, do you also cluster background features into parts, or do you sample multiple argmin points from similarity maps? A precise description would help reproducibility.\n- Could you provide additional comparisons to recent medical SAM-based prompting approaches (e.g., Med-PerSAM, CycleSAM, SAMIC) and discuss trade-offs (training-free vs lightweight adaptation, robustness to large deformations, videos)?\n- How is the range of candidate part counts N chosen in practice, and what is the empirical distribution of the selected n across datasets? Is there a strong prior that could reduce the retrieval search space?\n- Do you apply any spatial constraints (e.g., local non-maximum suppression, morphological filtering) when selecting the highest-similarity points per part to avoid spurious peaks? If not, did you observe failure modes, and could simple smoothing further improve stability?\n- For 3D CT, have you considered 3D feature clustering and volumetric prompt selection (e.g., multiple points per slice with temporal/3D consistency)? If attempted, what were the practical barriers?\n- For fairness, how were PANet and Matcher adapted to use SAM encoders, and were they retrained appropriately? Could you provide code/configurations and report their variances?\n\n### Overall Assessment\n\nThis paper presents a practical and generally useful prompting approach that improves training-free adaptation of SAM-like models to patient-specific medical segmentation with only one reference mask. The part-aware prompting is a clear, simple idea that reduces SAM‚Äôs ambiguity, and the retrieval mechanism adds robustness across cases. The experimental results are convincing across two clinical tasks and standard few-shot benchmarks, with thorough ablations. The primary areas to strengthen before a top-tier venue are methodological clarity around the retrieval distance (implementation and efficiency), additional comparisons to recent SAM-based medical prompting methods, and reporting of runtime and statistical variability. Addressing these points would significantly bolster the paper‚Äôs rigor and reproducibility while preserving its appealing simplicity and broad applicability.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **P2SAM**, a training-free, data-efficient framework for patient-specific segmentation that adapts large promptable segmentation models (SAM and SAM 2) using only one annotated reference image. The method employs a *part-aware prompting mechanism* that clusters reference features into part-level prototypes, generates similarity maps on target images, and selects multiple positive and negative point prompts. A retrieval step determines the optimal number of parts by minimizing the Wasserstein distance between reference and predicted foreground features. The approach shows consistent performance gains on medical (4D-Lung, CVC-ClinicDB) and natural-image one-shot segmentation benchmarks. Overall, the paper is clearly structured and presents a simple yet effective idea with demonstrated cross-domain generality.  \n\n**Major Comments**  \n1. **Technical Specification and Reproducibility:** The computation of the Wasserstein distance in high-dimensional feature space is under-specified‚Äîdetails on solver type, approximation, sampling, and computational complexity are needed.  \n2. **3D Deployment and Volumetric Consistency:** The method operates on 2D slices for CT data, but implications for volumetric consistency, runtime, and memory use are not discussed.  \n3. **Prompt Selection Robustness:** Prompt placement based on argmax selection from noisy similarity maps lacks spatial smoothing or non-maximum suppression, risking unstable results.  \n4. **Experimental Fairness and Coverage:** Baselines may be disadvantaged by reimplementation using shared SAM encoders; adaptation details are limited. Comparisons omit several concurrent SAM-based medical prompting methods (e.g., Med‚ÄëPerSAM, CycleSAM, SAMIC).  \n5. **Statistical and Runtime Reporting:** No variance or uncertainty metrics are reported, and runtime scaling of the retrieval process is absent.  \n6. **Clarity and Algorithmic Details:** Some equations and symbols are inconsistently formatted (e.g., ‚ÄúP\"SAM‚Äù). The retrieval step‚Äôs variants (Hungarian, Jensen‚ÄìShannon) and clustering settings need more reproducible description.  \n7. **Related Work Context:** Engagement with recent prompting pipelines and part-aware prototype literature is limited; situating P2SAM relative to these trends would strengthen the narrative.  \n\n**Minor Comments**  \n- Specify how negative prompts are sampled in medical images (clustering vs. argmin selection).  \n- Clarify evaluation aggregation in 4D-Lung (per-slice vs. per-volume).  \n- Explain selection range and typical distribution of candidate part counts.  \n- Minor typographical and rendering issues should be corrected.  \n- Provide details on the adaptation of PANet and Matcher to SAM encoders and share configurations if possible.  \n\n**Summary Paragraph**  \nP2SAM is an innovative and pragmatic contribution toward training-free patient-specific adaptation. Its strengths lie in its conceptual simplicity, broad applicability, and thorough ablation studies across medical and natural-image domains. The main weaknesses concern incomplete methodological specification (particularly for the retrieval metric and runtime), missing comparisons to recent related works, and lack of uncertainty analysis. Addressing these would improve reproducibility and situate the method more firmly within the growing ecosystem of SAM-based medical segmentation approaches.  \n\n**Decision Recommendation:** **Major Revision**  \nThe paper presents a promising idea with solid empirical results but requires clearer methodological description, additional comparisons, and reporting of runtime and statistical analysis before it can be fully recommended.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThe manuscript addresses a critical challenge in precision medicine: patient-specific segmentation that must contend with significant inter-patient variability while operating with extremely limited annotated data per patient. The authors propose P¬≤SAM (Part-aware Personalized Segment Anything Model), a novel data-efficient segmentation framework that leverages the Segment Anything Model (SAM) architecture without requiring any model fine-tuning. The core innovation lies in a part-aware prompt mechanism that strategically selects multiple-point prompts based on part-level features extracted from one-shot patient-specific prior data, combined with a distribution-similarity-based retrieval approach to determine the optimal number of part-level features for each case. The method demonstrates significant improvements over existing approaches, achieving +8.0% and +2.0% mean Dice score improvements in two patient-specific segmentation tasks (NSCLC in radiation therapy and polyp segmentation in endoscopy videos), and establishing a new state-of-the-art result of 95.7% mIoU on the PerSeg benchmark. The approach exhibits impressive generality across medical and natural image domains while requiring only minimal patient-specific input.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n1. **Lack of clear differentiation from Matcher**: While the paper positions P¬≤SAM as superior to Matcher (Liu et al., 2023), the fundamental distinction between part-level features versus patch-level features is not sufficiently explained. The manuscript should provide a more detailed analysis of why part-level clustering produces more meaningful prompts than patch-level approaches, particularly with visual examples showing the qualitative differences between prompt selections.\n\n2. **Insufficient clinical validation**: Although the method demonstrates strong quantitative results, the paper lacks input from clinical experts regarding the practical utility and clinical relevance of the improvements. A brief discussion with radiologists or clinicians about whether the observed Dice score improvements translate to meaningful clinical impact would strengthen the paper's medical significance.\n\n3. **Incomplete explanation of distribution similarity**: The paper states that \"tumors and normal organs manifest in distinct distributions within medical imaging technologies\" but doesn't sufficiently explain how the Wasserstein distance measurement specifically relates to medical imaging characteristics. More domain-specific justification for using this metric in the medical context would strengthen the methodological foundation.\n\n4. **Ambiguous performance claims**: The abstract states that P¬≤SAM \"improves the performance by +8.0% and +2.0% mean Dice score\" without specifying the baseline. The paper should clarify whether these improvements are relative to the direct-transfer baseline or other specific methods.\n\n### Minor Comments\n\n1. **Figure improvements**: Several figures (particularly Figure 3) could benefit from more explicit annotations to better illustrate the relationship between part-level features and selected prompts. Additional visual examples showing the clustering process in medical images would enhance understanding.\n\n2. **Computational complexity**: The paper would benefit from a brief discussion of the computational overhead introduced by the part-aware mechanism compared to single-prompt approaches, especially regarding clinical applicability.\n\n3. **Failure case analysis**: While Appendix E discusses limitations with multiple similar objects, a more comprehensive analysis of failure cases with specific medical examples would strengthen the paper's practical utility assessment.\n\n4. **Reproducibility details**: Although Appendix C provides some implementation details, adding specific information about hyperparameter sensitivity and recommended settings for different medical applications would improve reproducibility.\n\n5. **Terminology clarification**: The paper uses \"in-domain\" and \"out-of-domain\" terminology which could be more precisely defined in the medical imaging context to avoid confusion with standard machine learning usage.\n\n## 3. Evaluation along TMI Editorial Criteria\n\n**Significance**: The paper addresses a clinically important problem in precision medicine where personalized segmentation is critical but limited by data scarcity. The ability to adapt to new patients with only one-shot prior data without fine-tuning has substantial potential for clinical implementation. However, the paper could better articulate the clinical impact of the observed performance improvements, potentially through expert consultation.\n\n**Innovation**: The part-aware prompt mechanism represents a meaningful advancement beyond existing prompt-based segmentation approaches. The integration of part-level feature clustering with distribution-similarity-based retrieval for optimal prompt selection is novel and well-motivated. The approach effectively addresses the ambiguity problem in promptable segmentation models, particularly for medical applications where single-point prompts often fail.\n\n**Evaluation**: The experimental evaluation is generally strong, covering multiple medical datasets (4D-Lung, CVC-ClinicDB) and natural image benchmarks (PerSeg, COCO-20). The comparison against multiple baselines (PerSAM, Matcher, direct-transfer) is appropriate. However, the evaluation would be strengthened by more comprehensive failure case analysis and clinical validation. The ablation studies provide good insights into design choices.\n\n**Reproducibility**: The paper provides detailed methodological descriptions and references to implementation details in the appendix. The authors promise code release upon acceptance. However, some specifics about the retrieval range determination and optimal parameter settings for different medical applications would enhance reproducibility. The experimental setup is well-described, though some hyperparameter choices could be better justified.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nThe paper presents a promising and well-structured approach to patient-specific segmentation with significant methodological contributions. However, several major issues need addressing before publication. The paper would benefit from a clearer explanation of how the part-aware mechanism fundamentally differs from existing approaches, particularly Matcher, and why this difference matters in the medical context. More detailed clinical validation and discussion of the practical impact of the improvements would strengthen the medical significance. The ambiguous performance claims need clarification, and the methodological foundation for the distribution similarity approach requires better medical domain-specific justification.\n\nWith these revisions, the paper has strong potential for publication in TMI as it addresses an important clinical challenge with an innovative, data-efficient solution that demonstrates impressive results across multiple domains. The authors have demonstrated a thorough understanding of the field and provided comprehensive experimental validation, but the clinical relevance and methodological justification need strengthening to meet TMI's high standards.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a novel approach to patient-specific medical image segmentation, addressing the central challenge of inter-patient variability under conditions of limited labeled data. The proposed method, P¬≤SAM (Part-aware Personalized Segment Anything Model), extends the Segment Anything Model to the medical imaging domain through a part-aware multi-point prompt mechanism combined with a distribution-similarity-based retrieval strategy. This framework allows data-efficient segmentation without model fine-tuning and achieves notable improvements on tasks such as non‚Äìsmall-cell lung cancer and polyp segmentation, as well as state-of-the-art performance on the PerSeg benchmark. The paper is thorough in experimentation and well-structured, though several aspects require clarification and further validation.\n\n---\n\n**Major Comments**\n\n1. **Conceptual distinction from prior work:** The differentiation between P¬≤SAM and Matcher (Liu et al., 2023) is not sufficiently clear. The authors should explicitly detail how part-level feature clustering offers distinct advantages over patch-level approaches, ideally supported by visual comparisons of the resulting prompt selections.  \n2. **Clinical validation:** While quantitative improvements are strong, the study lacks clinical expert input to contextualize the results. Discussion with clinicians regarding whether the reported Dice score gains translate into clinically significant improvements would enhance the manuscript‚Äôs medical relevance.  \n3. **Justification of distribution similarity:** The rationale for using the Wasserstein distance to model differences between tumors and normal tissue distributions needs stronger domain-specific justification. Clear explanation of why this metric is suited to medical images would improve methodological grounding.  \n4. **Performance interpretation:** The reported performance gains (+8.0% and +2.0% Dice score) are presented without identifying the reference baseline. Clarifying which specific methods these improvements are measured against is essential for accurate interpretation.\n\n---\n\n**Minor Comments**\n\n1. Figures, particularly Figure 3, should include clearer annotations linking part-level features to selected prompts; additional visual examples of clustering in medical images would improve illustration.  \n2. A short discussion of computational overhead introduced by the part-aware mechanism would help assess clinical feasibility.  \n3. Expand the analysis of failure cases with concrete medical examples beyond those briefly mentioned in the appendix.  \n4. Add more details on hyperparameter sensitivity and parameter settings for different applications to improve reproducibility.  \n5. Define ‚Äúin-domain‚Äù and ‚Äúout-of-domain‚Äù more precisely to avoid ambiguity in the medical imaging context.\n\n---\n\n**Summary Paragraph**  \nOverall, the paper tackles an important problem in precision medical image analysis with an innovative and data-efficient approach. Its main strengths lie in methodological novelty, broad applicability, and robust experimental comparisons. However, the manuscript would be strengthened by a clearer conceptual distinction from related work, explicit clinical validation, detailed justification of methodological choices, and clarification of performance claims. The reproducibility and interpretability would also benefit from additional technical and visual documentation.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe work is promising and potentially impactful, but substantial revisions are required to clarify methodological differences, substantiate clinical significance, and reinforce the medical rationale behind the design choices.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles patient‚Äëspecific image segmentation in the face of large inter‚Äëpatient variability and a scarcity of annotated images. The authors cast the problem as an in‚Äëcontext segmentation task that relies on a single image‚Äëmask pair from a new patient as a prior. Their pipeline extracts part‚Äëlevel features from this prior image, clusters them, and then selects a set of positive and negative point prompts based on similarity to the target image. A distribution‚Äësimilarity retrieval step is used to decide how many parts should be generated for each case. The selected prompts are supplied to a promptable segmentation backbone (SAM or SAM‚ÄØ2) without any explicit fine‚Äëtuning. Experiments on several medical datasets (lung tumour and colon polyp) as well as standard one‚Äëshot benchmarks report higher Dice and IoU scores than the baselines. The central claim is that a part‚Äëaware prompting scheme together with an optimal‚Äëpart retrieval strategy yields data‚Äëefficient, patient‚Äëadapted segmentation.\n\n---\n\n## General feedback  \n\n- **Relevance:** The goal of achieving accurate patient‚Äëspecific segmentation with minimal annotation is clearly important for precision medicine and could, in principle, reduce the burden on clinicians.  \n- **Novelty:** Introducing part‚Äëaware prompts (i.e., clustering foreground features into multiple prototypes) and using a Wasserstein‚Äëbased criterion to choose the number of parts represents a fresh angle on prompt engineering for SAM‚Äëtype models. Nonetheless, the manuscript does not convincingly explain why clustering at the part level should outperform simpler patch‚Äëlevel prompting schemes.  \n- **Experimental validation:** The authors evaluate four medical datasets (two in‚Äëdistribution, two out‚Äëof‚Äëdistribution) and four natural‚Äëimage one‚Äëshot benchmarks, reporting mean Dice or IoU improvements. Important details are missing, however: no statistical significance tests or confidence intervals are provided, and the exact patient‚Äëwise train/validation/test splits are not described. Consequently the robustness of the reported gains cannot be verified. Runtime and memory implications of the added clustering and retrieval stages are also absent.  \n- **Reproducibility:** While the authors promise to release code after acceptance, several implementation specifics are omitted from the paper: how the number of clusters (**k**) is chosen, the initialization scheme for k‚Äëmeans++, parameters governing the Wasserstein computation, and the hyper‚Äëparameters of the retrieval algorithm. Moreover, the claim of ‚Äúno fine‚Äëtuning‚Äù appears at odds with the fine‚Äëtuning experiments described in Section‚ÄØ4.1, which raises a consistency issue.\n\n---\n\n## Specific comments / critiques  \n\n1. **Inconsistent fine‚Äëtuning claim** ‚Äì The abstract asserts that the method works ‚Äúwithout any model fine‚Äëtuning,‚Äù yet Section‚ÄØ4.1 explicitly states that SAM was fine‚Äëtuned on the NSCLC‚ÄëRadiomics and Kvasir‚ÄëSEG datasets before testing on out‚Äëof‚Äëdistribution data. The authors need to clarify whether the proposed pipeline truly operates in a zero‚Äëshot regime after this adaptation. *(Section‚ÄØ4.1, Table‚ÄØ1/2)*  \n\n2. **Clustering methodology under‚Äëdescribed** ‚Äì The text mentions the use of k‚Äëmeans++, but it does not disclose how the number of parts *n* is determined, what initialization strategy is employed, the stopping criteria, or how empty clusters are handled. These choices can materially affect performance and are essential for reproducibility. *(Fig.‚ÄØ3, Section‚ÄØ3.2)*  \n\n3. **Details of the retrieval step missing** ‚Äì The Wasserstein distance between foreground feature distributions is introduced (Fig.‚ÄØ5), yet the manuscript omits critical information such as the dimensionality reduction applied, the number of samples drawn from each distribution, and the exact formulation of the distance metric.  \n\n4. **Baseline comparability unclear** ‚Äì Some baselines (e.g., Matcher, PerSAM) are evaluated using a fine‚Äëtuned SAM backbone, whereas others (direct‚Äëtransfer) rely on the pretrained model. The paper does not state explicitly whether all baselines share identical backbone weights, which could bias the comparison. *(Table‚ÄØ1, Table‚ÄØ2)*  \n\n5. **Lack of statistical reporting** ‚Äì All quantitative tables (Tables‚ÄØ1,‚ÄØ2,‚ÄØ4) present only mean Dice or mIoU values. No standard deviations, confidence intervals, or hypothesis‚Äëtesting results are offered, making it impossible to assess whether the observed improvements are statistically significant.  \n\n6. **Dataset split information absent** ‚Äì The number of patients allocated to training, validation, and testing for the out‚Äëof‚Äëdistribution datasets (4D‚ÄëLung, CVC‚ÄëClinicDB) is not disclosed, nor is the patient‚Äëwise split strategy. Without this information, potential data leakage cannot be ruled out. *(Section‚ÄØ4.1)*  \n\n7. **Computational cost not quantified** ‚Äì The additional clustering and retrieval components may introduce non‚Äëtrivial overhead, especially for 3‚ÄëD CT volumes. Yet the manuscript provides no runtime or memory benchmarks to substantiate the claim of efficiency.  \n\n8. **Ablation study limited** ‚Äì Table‚ÄØ6 explores only the effect of varying the number of parts *n*. The contribution of the retrieval module (with vs. without) and the role of negative‚Äëpoint prompts are not examined, leaving the importance of these components ambiguous.  \n\n9. **Qualitative results biased** ‚Äì Figures‚ÄØ6‚Äë9 display only successful segmentations. Including failure cases would help readers understand the method‚Äôs limitations and failure modes.  \n\n10. **Overstated generalisation claim** ‚Äì The authors state that the approach can be integrated into ‚Äúany promptable segmentation model,‚Äù but the experimental evidence is confined to SAM and SAM‚ÄØ2. No validation on alternative architectures (e.g., CLIP‚ÄëSeg, SegFormer) is presented.  \n\n---\n\n## A suggested decision  \n\n**Reject**  \n\nThe manuscript introduces an interesting idea but falls short on several crucial fronts: incomplete methodological description, insufficient statistical analysis, unclear experimental protocol, and a lack of evidence supporting its broader claims. Until these issues are addressed, the work does not meet the standards required for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a method for patient‚Äëspecific image segmentation under conditions of high inter‚Äëpatient variability and limited annotated data. The approach frames segmentation as an in‚Äëcontext learning task using a single image‚Äëmask pair as a prior. It introduces a part‚Äëaware prompting mechanism that clusters features from the prior image, selects representative positive and negative prompts, and employs a retrieval step based on distribution similarity to determine an optimal number of parts. These prompts are then used within a promptable backbone (SAM or SAM‚ÄØ2) without explicit fine‚Äëtuning. Experiments across medical and one‚Äëshot benchmarks suggest improved Dice and IoU scores over competing methods. While the objective is highly relevant to precision medicine, the manuscript exhibits several methodological and reporting shortcomings that limit the credibility of its claims.  \n\n**Major Comments**  \n1. **Fine‚Äëtuning inconsistency:** The abstract asserts zero‚Äëshot operation, yet Section‚ÄØ4.1 indicates fine‚Äëtuning of SAM on specific datasets prior to evaluation. This inconsistency must be clarified.  \n2. **Underspecified clustering procedure:** Details on choosing the number of parts, initialization, termination criteria, and handling of empty clusters are missing, hindering reproducibility.  \n3. **Retrieval step description insufficient:** Key implementation aspects‚Äîfeature dimensionality, sample size, and metric formulation for the Wasserstein computation‚Äîare not provided.  \n4. **Baseline comparability unclear:** Some baselines use fine‚Äëtuned backbones while others employ pretrained weights. The paper should specify whether identical backbone versions were used.  \n5. **Lack of statistical evidence:** Quantitative results report only mean Dice/IoU values without deviations or statistical tests, preventing assessment of significance.  \n6. **Missing dataset split information:** Patient‚Äëwise train/validation/test splits are not described, raising concerns about potential data leakage.  \n7. **Unreported computational overhead:** Claims of efficiency are unsupported by runtime or memory analyses.  \n8. **Limited ablation exploration:** Only the number of parts is varied; the contributions of retrieval modules and negative prompts are not isolated.  \n9. **Selective qualitative display:** Only successful examples are shown; inclusion of failures would provide a more balanced view.  \n10. **Overgeneralized applicability claim:** Despite claiming model‚Äëagnostic relevance, experiments are restricted to SAM and SAM‚ÄØ2.  \n\n**Minor Comments**  \n- Clarify notation and parameter definitions in Sections‚ÄØ3.2 and‚ÄØ4.1.  \n- Include precise references in figure captions (e.g., Fig.‚ÄØ3,‚ÄØ5).  \n- Improve consistency of terminology between parts and clusters.  \n\n**Summary Paragraph**  \nThe paper contributes a potentially useful framework for patient‚Äëadapted prompting but suffers from incomplete methodological transparency, inadequate statistical treatment, and inconsistent descriptions of experimental settings. While the idea of integrating part‚Äëaware prompting with distribution‚Äëbased retrieval is conceptually appealing, the empirical validation and reproducibility information are insufficient to support the strength of the conclusions. Addressing these issues would be necessary before the work could be considered reliable.  \n\n**Decision Recommendation**  \n**Reject.** The submission presents an interesting concept but lacks the methodological clarity, experimental rigor, and statistical validation needed for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chenhui Zhao",
      "Liyue Shen"
    ],
    "url": "pdfs/iclr.cc-2025-conference_10d7d4d4e6a581994367bb8103dcf41b5a52cd66.pdf",
    "remote_url": "https://openreview.net/pdf/10d7d4d4e6a581994367bb8103dcf41b5a52cd66.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Single Domain Generalization for Rare Event Detection in Medical Imaging",
    "status": "completed",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Deep Learning",
      "Knowledge",
      "Rare Event Detection",
      "Out-of-distribution detection"
    ],
    "abstract": "Single Domain Generalization (SDG) addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset. In the medical diagnosis and disease detection domain, where data is often limited and events of interest are rare, deep learning (DL) models frequently exhibit suboptimal performance, leading to poor generalization across datasets. In multi-center studies, disparate data sources, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare event characteristics. This paper addresses this challenge by first leveraging a pre-trained large vision model to rank classes based on their similarity to the rare event class, allowing focused handling of the most similar class, and then integrates domain-invariant knowledge on rare event with DL to accurately classify the rare event class. By carefully incorporating expert knowledge with data-driven DL, our technique effectively regularizes the model, enhancing robustness and performance even with limited data availability. We present a case study on seizure onset zone detection using fMRI data, demonstrating that our approach significantly outperforms state-of-the-art vision transformers, large vision models, and knowledge-based systems, achieving an average F1 score of 90.2% while maintaining an overall F1 score of 85.0% across multi-center datasets.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe authors introduce an algorithm for generalizing to unknown domains using only training data from a single known domain, \"RareSaGE\", which proposes to integrate domain-invariant expert knowledge with neural networks for the problem of rare class classification. Their method achieves improved performance for this task compared to several neural network- and expert knowledge-based techniques.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper focuses on a lesser known, but still seemingly important problem which deserves attention. They test on a realistic dataset comprised of medical data sampled from real centers, demonstrating real life applicability.\n2. The method is seemingly fairly technically novel. The rarity quantification doesn't seem novel (they cite Li et al), but overall, the combination of expert knowledge (an old field) with modern neural network models is interesting and unusual.\n3. Improvements over the compared baselines are significant for all metrics (except maybe precision) (Table 1)\n4. The intuition behind the method is reasonable, and the formalism discussed does help with understanding this. The reasoning behind \"rationale for definition 3.1\" makes sense, i.e., explaining the challenges in working with rare classes with respect to class entropy.\n5. I think the four characterizations of a rare event make sense (not sure if these are novel or not?), although discrimination and significance may be a bit redundant.\n\n### Weaknesses\n\n**Major:**\n1. **Generally limited experiments (don't even begin until page 9):** I would suggest moving many of the less-important dataset, algorithm and method details into the supplementary, and filling the missing space with more experiments, especially those considering ablation studies, failure cases for your method, computational analyses, and others. The biggest issues are as follows.\n    1. For a method defined in such generality, the experiments are quite limited: just a case study based on one fMRI datasets. This is a big negative, because it is unclear if the method could generalize to other problems, or if it was bespoke to (or developed specifically for) this case study. This hurts the paper's suitability for ICLR, as the larger impact on machine learning is not clear to me, which would require testing on other datasets/problem settings/etc.\n    2. Moreover, while the improvements over the compared baselines are significant for all metrics (except maybe precision) the comparison/baseline models may not be sufficient:\n        - I'm not an expert in domain generalization, but are the comparison models in Table 1 really proper baselines (maybe another reviewer can chime in)? Simple training or finetuning on one domain and testing on another domain (as well as the knowledge-based systems) doesn't seem like a strong, appropriate baseline. Are there better unknown domain generalization techniques which should have been tested?\n        - Alternatively, what about comparing to one-class-classification methods, i.e. OOD/anomaly-detection methods? This is also a large body of work which seems suitable. At the very least, why weren't these discussed in the related work?\n        - why wasn't AUC presented if sensitivity and precision were also presented? This would be a better general performance metric than accuracy for accounting for class imbalance.\n\t\n2. **Lack of ablation/hyperparameter sensitivity studies:**\n    1. this method has quite a few moving parts, which each could be brittle. The more of these moving parts that there are, the more ways that the method could fail when extended to new datasets/problems. It is important to consider how changing or removing one component would affect the performance, in order to judge how reliant to algorithm is on that component. However, the paper critically lacks any ablation/sensitivity studies for such hyperparameters/settings. I will discuss potential ablation studies which could be done, but I challenge the authors to think of more. Some examples:\n        1. On pg. 4, a rare class is defined by a 2-sigma distance from the mean class entropy. Why/how was 2 sigma chosen? This is seemingly an important parameter, yet you do no ablation/sensitivity studies on the effect of different values for this (number of sigmas used to define a rare class)\n        2. Algorithm 1 is quite complex, additionally with certain steps not quantitatively/explicitly defined.\n        3. The use of PCA on clip features may have limitations, as its linearity may be too much of a restriction. Why wasn't some form of nonlinear component/degrees-of-freedom analysis tested as well?\n        3. The form of the expert knowledge in the tested scenario (Eq. 6) is quite specific. Are there viable alternatives to this formulation that could have been tested on this dataset? Also, in general, what is the feasibility for converting expert knowledge into this format?\n\t\t\t\n**Minor:**\n1. More clear, technical method details are needed in the abstract and introduction to be clear what these contributions are. For example:\n    1. In the abstract: \"This paper addresses ‚Ä¶ even with limited data availability.\" This really lacks in explicit details on how the method actually works. how is this ranking done, specifically? is it via a novel algorithm? also, what does \"focused handling\" mean? and how is domain-invariant knowledge \"integrated\"? These questions could be gleaned from the main text of the paper, but a better level of detail could still be provided in the abstract with less vague wording.\n    2. Similarly in the introduction, more technical details are needed. Your repeatedly describe the \"integration\" of domain invariant expert knowledge with deep learning, but I'm unclear from reading this what this actually explicitly describes.\n2. Misleading claims/vague wording:\n    1. In the introduction, you say \"This concept is particularly crucial in the field of artificial intelligence (AI) for medicine, where the aim is to accurately diagnose new patient cases across centers\". This is misleading. AI for medicine extends far beyond diagnosis, including tasks such as segmentation, registration, harmonization, etc‚Ä¶ and that's just for images, not including AI for medicine beyond images.\n    2. \"In medical imaging, there are various factors ‚Ä¶  acute disorders\" also in the introduction; what are the \"classes\" being described here? I'm not sure if this is really intra-class variability (at least since you don't explain what the classes are here), its really just various factors that can contribute to domain shift problems in medical datasets.\n    3. \"In the medical diagnosis and disease detection domain, where data is often limited ‚Ä¶ variability in rare event characteristics. \" in the abstract is phrased confusingly. in the first sentence, is performance poor on the data because of limited data, rare cases of interest, or both? Maybe say that the second sentence describes a problem which exacerbates the problem described in the first sentence.\n3. Other missing technical details:\n    1. how is feature overlap/resemblance (described in the first paragraph of Sec. 3.2 actually computed? cosine similarity of the feature vectors?\n    2. Algorithm 1 shouldn't really be labeled as an \"algorithm\" in my opinion, because many of the steps are not defined explicitly, and left up to interpretation. More details are needed.\n    3. They provided anonymous code link in Sec. 4.1 is broken. It would have been helpful to see how such a novel and relatively complex algorithm was implemented in practice.\n4. Writing/clarity issues: the paper suffers from issues in writing quality, including clarity and typos, discussed next. For example:\n    1. The paper can be a bit challenging to read, not because of technical details, but because redundant information is often provided (see for example, \"solution overview\" and \"solution details\" could certainly be shortened and combined in 3.2)\n    2. Many sentences are terse and relatively telegraphic. The paper is still understandable, but could have better flow and general writing.\n    3. There is a too-long run-on sentence in \"To improve SDG for rare events, leveraging expert knowledge such as clinical opinion on ‚Ä¶, has immense potential\" in the introduction.\n5. Typos/formatting issues:\n    1. In the abstract: \"Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset\"\n        1. also, this is phrased confusingly. maybe say extensively studied in classification of balanced datasets?  But is having an imbalanced dataset not just kind of a triviality?\n    2. also in abstract \"integrates domain-invariant knowledge on rare event\" another typo.\n    3. Also in the intro, page 2 \"with DL using pre-trained large vision model (LVM) \"\n    4. There seems to be an issue in how in-text citations are formatted: no spacing between them, not parenthetical, etc. Makes it a bit harder to read the paper.\n    5. Why are the equations written with such tiny text? They require zooming in just to read. I'm not sure if this formatting modification is allowed for ICLR.\n\n### Questions\n\n1. Is \"single domain generalization\" a common term when describing your problem scenario? it sounds like it describes generalizing TO a single domain, not training on a single one and being able to generalize to unknown ones.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RareSaGE*, an algorithm designed to enable generalization to unseen domains when training data are available only from a single known domain. The method integrates domain‚Äëinvariant expert knowledge with neural networks for rare‚Äëclass classification tasks. The paper addresses an underexplored yet important problem, demonstrating potential real‚Äëworld applicability using a realistic medical dataset. The exposition conveys the basic idea, though the presentation could be more concise and technically explicit.  \n\n**Major Comments**  \n1. **Limited Experimental Validation:** The experimental section is narrow in scope, focusing only on a single fMRI‚Äëbased medical dataset. This restricts assessment of generalizability beyond the tested setting and weakens claims about broader machine‚Äëlearning impact. Additional datasets, case studies, or problem types are needed.  \n2. **Inadequate Baselines and Missing Comparisons:** The chosen baselines in Table‚ÄØ1 may not reflect the state of the art in domain generalization or one‚Äëclass/out‚Äëof‚Äëdistribution detection. The authors should justify these selections and consider or at least discuss stronger alternatives. The absence of AUC, a standard metric for class‚Äëimbalanced scenarios, further limits interpretability.  \n3. **Lack of Ablation and Sensitivity Studies:** The method involves multiple components and hyperparameters, yet no ablation or parameter sensitivity analyses are provided. Key design choices‚Äîsuch as the two‚Äësigma threshold for defining rarity, use of PCA for feature reduction, and the specific formulation of expert knowledge‚Äîrequire empirical support or discussion of alternatives.  \n4. **Algorithmic Clarity:** Several methodological steps (e.g., feature overlap computation, Algorithm‚ÄØ1 definitions) are insufficiently specified. This hinders reproducibility and understanding of how expert knowledge is operationalized.  \n\n**Minor Comments**  \n- Abstract and introduction need clearer, more technical summaries of the proposed method and its novelty.  \n- Some claims are overstated or vague, e.g., the scope of ‚ÄúAI for medicine‚Äù and class variability descriptions.  \n- The anonymous code link is broken, preventing verification of implementation details.  \n- Writing can be streamlined by removing redundancy between ‚Äúsolution overview‚Äù and ‚Äúsolution details.‚Äù  \n- Formatting issues: small equation font, citation spacing, and occasional typos and run‚Äëon sentences.  \n- Clarify whether ‚Äúsingle domain generalization‚Äù is standard terminology for this setting.  \n\n**Summary Paragraph**  \nThe paper tackles a relevant and challenging problem‚Äîsingle‚Äëdomain generalization for rare class recognition‚Äîand proposes an interesting integration of expert knowledge with deep learning. Nevertheless, the work‚Äôs impact is limited by minimal experimental evidence, incomplete comparisons, and missing ablation analyses that make it difficult to assess robustness and generalizability. Clarity, writing quality, and formatting require improvement. With stronger validation and clearer methodological exposition, the contribution could become more convincing.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents RareSaGe, a novel approach for SGD in detecting rare events in medical imaging. First, this method handles the most similar classes by using a pre-trained large vision model (LVM) to rank classes based on their similarity to the rare event class, which aims to effectively classify the rare event. Then, it further integrates expert knowledge with deep learning to enhance robustness and address challenges related to limited data. Experiments conducted on multi-center datasets for seizure onset zone (SOZ) detection in fMRI data demonstrate that RareSaGe achieves high generalization performance, significantly outperforming other models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The manuscript is written in clear English and is relatively easy to follow.\n2. The experimental results demonstrate the effectiveness of the proposed framework.\n3. The motivation for developing the method makes sense\n\n### Weaknesses\n\n1. The title is focused on MEDICAL IMAGING, but the data is too singular, validated only on fMRI data and limited to two categories (two centers).\n2. The introduction to the dataset in Section 4.1 and the Supplementary Material is relatively limited.\n2. There are missing many important ablation studies, such as validating the effectiveness of the two types of expert knowledge on the method.\n\n### Questions\n\n1. The title is focused on MEDICAL IMAGING, but the data is too singular, validated only on fMRI data and limited to two categories (two centers).\n2. In Section 4.1, the link to the dataset is unavailable; it was not previously a publicly available dataset. The introduction to the dataset in Section 4.1 and the Supplementary Material is relatively limited. \n3. Additionally, there are no visual results of the dataset presented in the manuscript, leading to insufficient feasibility of the results.\n4. In Section 3.4.2 EXPERT KNOWLEDGE ON RS-fMRI, it is mentioned that \"These locations can be extracted employing established image processing algorithms.\" The visual results obtained from these algorithms for specific locations should be presented.\n5. There are many important ablation studies missing, such as validating the effectiveness of the two types of expert knowledge on the method.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 3,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **RareSaGe**, a novel stochastic gradient descent (SGD)‚Äìbased framework for detecting rare events in medical imaging. The approach leverages a pre-trained large vision model (LVM) to rank classes according to their similarity to the target rare event class and combines this with expert knowledge to improve robustness under limited data conditions. Experiments on multi-center datasets for seizure onset zone (SOZ) detection using fMRI demonstrate strong generalization and performance. Overall, the paper is well written and the motivation is clear, though certain methodological and experimental aspects require further validation and detail.  \n\n**Major Comments**  \n1. **Dataset Limitations:** Although the title emphasizes medical imaging in general, the study is validated only on fMRI data from two centers, which limits its generalizability. Broader evaluation across different modalities or datasets would strengthen the claims.  \n2. **Dataset Description:** The introduction to the dataset in Section 4.1 and the supplementary materials is insufficient. The dataset link appears unavailable, and more information is needed regarding its contents and accessibility.  \n3. **Missing Visualizations:** The manuscript does not present any visual examples or results of the dataset or the rare event detections, which reduces confidence in the reported feasibility and interpretability of the results.  \n4. **Expert Knowledge Integration:** Section 3.4.2 mentions that relevant locations ‚Äúcan be extracted using established image processing algorithms,‚Äù but the visual outcomes of such algorithms are not shown. Including these would clarify how expert knowledge contributes to the model.  \n5. **Ablation Studies:** Key ablation experiments are missing, particularly those validating the distinct contributions of the two forms of expert knowledge on overall performance.  \n\n**Minor Comments**  \n- The paper would benefit from a more detailed dataset introduction and improved presentation of the supplementary information.  \n- Acronym definitions and section cross-references should be checked for clarity and consistency.  \n\n**Summary Paragraph**  \nThe study proposes an interesting and promising framework that effectively combines deep learning and expert knowledge for rare event detection in medical imaging. The manuscript is clearly written and motivated, with convincing empirical improvements. However, its scope is narrow due to dataset constraints, and the lack of detailed dataset description, visual results, and ablation analyses weakens its evidential foundation. Addressing these issues would substantially enhance the methodological soundness and reproducibility.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper discusses Single Domain Generalization (SDG) in the context of rare event classification, particularly in medical diagnosis, where limited data and imbalanced datasets pose significant challenges. The authors propose a method that utilizes a pre-trained large vision model to rank classes by their similarity to rare events, enabling focused classification. By integrating domain-invariant expert knowledge with data-driven deep learning, the approach improves model robustness and performance, even with scarce data. A case study on detecting seizure onset zones using fMRI data shows that this method achieves an average F1 score of 90.2%.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis paper addresses an interesting and important problem: how to leverage large vision models (LVMs) to tackle rare diseases, which involve imbalanced, infrequent cases and domain shifts. While there are many SDG methods available, the authors claim that none have been specifically applied to rare cases in medical data. It appears that the authors are addressing a new problem.\n\n### Weaknesses\n\nProblematic Settings\n\n1. While there are currently no methods addressing SDG in the context of limited data and rare diseases, this setting appears to combine elements of SDG with few-shot imbalanced classification. If a SDG method is sufficiently robust, it could potentially be integrated with existing imbalanced learning or rare disease fine-tuning techniques to effectively address this issue. However, the authors lack experiments to validate this point, which raises questions about the relevance of this particular setting in the field. The authors have the opportunity to tackle a significant challenge: how to integrate large vision models (LVMs) for rare disease classification, taking into account that rare diseases can vary in size, may be in-domain or out-of-domain, and may be either imbalanced or balanced, thereby providing a more comprehensive framework.\n\nLimited Discussions and Comparisons of Results\n\n2. The authors have not compared their approach with existing SDG and imbalanced learning methods, which diminishes the convincing nature of their experiments and calls into question their claim of being state-of-the-art (SOTA).\n\n### Questions\n\nPlease see my comments above\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates Single Domain Generalization (SDG) for rare event classification, with particular emphasis on medical diagnosis tasks that suffer from limited and imbalanced datasets. The authors propose a method leveraging a large pre-trained vision model to rank classes by similarity to rare events, thereby enhancing focus and performance in classification. By integrating domain-invariant expert knowledge with data-driven deep learning, the approach aims to improve robustness under data scarcity. The method is evaluated on an fMRI-based seizure onset zone detection task, achieving a reported average F1 score of 90.2%. Overall, the paper addresses a timely and important problem but leaves several methodological and comparative aspects insufficiently explored.\n\n**Major Comments**  \n1. **Problem Setting and Validation:**  \n   Although the idea of applying SDG to rare diseases and limited-data scenarios is novel, the proposed setting appears to overlap conceptually with few-shot and imbalanced classification problems. The authors do not present experiments or comparisons that clarify whether SDG alone suffices for these challenges, or how it could be effectively integrated with existing rare disease modeling or imbalanced learning techniques. This omission weakens the methodological justification of the proposed approach.  \n2. **Lack of Comparative Discussion:**  \n   The manuscript does not provide adequate comparisons with existing SDG or imbalanced learning baselines. Without quantitative or qualitative evaluation against established methods, the claim of achieving state-of-the-art performance is not convincing. Broader experimental comparisons are necessary to substantiate the contributions and demonstrate relevance within the current research landscape.\n\n**Minor Comments**  \n- The manuscript would benefit from an expanded discussion situating the proposed method within the broader SDG and imbalanced learning literature.  \n- Clarification of terminology (e.g., how rare events are defined, and to what extent they correspond to domain or class imbalance) would improve readability.\n\n**Summary Paragraph**  \nThis paper tackles an important and underexplored topic: adapting SDG techniques to rare disease classification using large vision models. The conceptual direction is promising, but key issues remain regarding the definition of the problem setting, empirical validation, and comparative evaluation. Clarifying these aspects would substantially strengthen the paper‚Äôs contribution and credibility.\n\n**Decision Recommendation**  \n**Major Revision.** The topic and approach are promising, but the paper requires additional methodological justification, comparative experiments, and expanded discussion to support its claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI:**  \n‚úÖ The manuscript proposes *RareSaGe*, a methodological framework for *Single-Domain Generalization (SDG)* targeting *rare event detection in medical images*. Its focus on integrating discriminative expert knowledge with data-driven deep learning (DL) for improved generalization is methodological rather than purely application-oriented. The case study ‚Äî seizure onset zone (SOZ) detection from fMRI ‚Äî is used to validate the general method. This aligns with *IEEE TMI‚Äôs methodological scope* (imaging algorithms, integration of machine learning with imaging).  \n\n**2. Novelty & Contribution Level:**  \nThe paper identifies a gap in prior SDG literature: lack of techniques handling *rare class problems* (imbalanced and limited data). It defines a new theoretical framing (entropy-based rarity criterion) and integrates symbolic expert knowledge with DL to handle overlap classes. Methodological novelty lies in:  \n- an entropy-driven definition of ‚Äúrare class,‚Äù  \n- algorithmic framework (RareSaGe) combining DL and expert knowledge via a cascading classifier design,  \n- demonstration of domain invariance and single-domain generalization without target data.  \nThis is *conceptually distinct* but somewhat incremental in its integration aspects (DL + knowledge-based models). The contribution would reach TMI‚Äôs novelty threshold if claims on generalizability and cross-center validation are well substantiated.  \n\n**3. Technical and Experimental Rigor:**  \n- Mathematical formalization (entropy-based rarity, CRLB rationale) is solid, though some derivations could benefit from clearer connection to performance metrics.  \n- Experimental design includes cross-center (A‚ÜîB) and aggregated validations, with appropriate metrics (accuracy, precision, sensitivity, F1).  \n- Reported F1 > 90% across domains, demonstrating robustness ‚Äî yet statistical significance or uncertainty quantification beyond SDs is limited.  \n- Datasets are realistic (fMRI from pediatric/adult centers), IRB-approved, anonymized; however, generalizability beyond SOZ detection is not empirically shown.  \n- Reproducibility: code link provided, though anonymized (acceptable for review).  \n\n**4. Clarity and Presentation:**  \nGenerally clear and well organized. However:  \n- Long sentences and heavy use of inline references hinder readability.  \n- Figures and tables are appropriate but schematic (Fig.‚ÄØ1 could better indicate data flow and model interaction).  \n- Some grammatical errors and formatting inconsistencies (e.g., equation alignment).  \n\n**5. Ethical and Reproducibility Compliance:**  \n‚úî Explicit IRB approval and anonymization are stated.  \n‚úî Code and data (anonymized link) provided.  \nOverall compliance is adequate.  \n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThe manuscript presents *RareSaGe*, a novel algorithm for detecting rare events in medical imaging under single-domain settings. It introduces an entropy-based quantification of rarity and integrates expert knowledge‚Äìderived symbolic features through a quadratic optimization classifier with deep learning classifiers that handle overlapping classes. Using resting-state fMRI data for seizure onset zone localization across two centers, the method demonstrates strong cross-center and aggregated performance, reportedly outperforming standard CNNs, Vision Transformers, and knowledge-only systems.\n\n**2. Strengths**  \n- Clear methodological focus on the *SDG + rare-event* intersection.  \n- Innovative fusion of domain knowledge with DL in a structured, theoretically supported framework.  \n- Empirical validation across institutions, with substantial improvement in F1 scores.  \n- Conceptual interpretability through propositional knowledge features, enhancing trust and explainability in clinical AI.\n\n**3. Weaknesses**  \n- Limited generalization evidence beyond a single clinical application (SOZ detection).  \n- Some theoretical constructs (entropy definition vs Fisher information) are insufficiently validated empirically.  \n- Absence of ablation study quantifying the contribution of each module (e.g., knowledge integration vs DL alone).  \n- Presentation occasionally dense; method flow and parameter choices (e.g., choice of tc =‚ÄØ0.9) need clearer justification.  \n- Comparisons to related hybrid or physics-informed learning techniques could be expanded.  \n\n**4. Major Comments**  \n1. **Methodological clarity:** Please provide a detailed pseudocode or diagram clarifying how DL and knowledge modules interact at inference (currently only briefly described).  \n2. **Ablation analysis:** Quantify how much each component (entropy selection, overlap separation, knowledge QO machine) contributes to the final performance.  \n3. **Theoretical justification:** Validate the entropy‚Äìinformation relationship empirically (show correlation between class entropy and misclassification rate).  \n4. **Generalizability:** Discuss applicability beyond the fMRI task‚Äîwould RareSaGe generalize to other modalities (CT, histopathology)?  \n5. **Statistical robustness:** Add confidence intervals or statistical tests comparing baseline vs proposed method.  \n\n**5. Minor Comments**  \n- Improve figure captions and ensure all acronyms (e.g., QO, RSN) are defined where first used.  \n- Check reference formatting for consistency (e.g., MICCAI volumes, doi spacing).  \n- Correct some grammar/typographical errors.  \n- Clarify whether the hybrid method was tuned using validation from the same domain only (to ensure proper SDG evaluation).  \n\n**6. Overall Recommendation:**  \n**Major Revision** ‚Äì The work demonstrates substantial methodological potential and solid experimental basis, but requires additional clarification, broader generalization evidence, and minor writing improvement to meet TMI publication standards.\n\n**7. Confidence Level:** 4/5 (high confidence in assessment; moderate confidence in reproducibility of quantitative claims).\n\n---\n\n**Summary Statement:**  \n*RareSaGe* presents a promising framework linking expert knowledge with data-driven learning to address single-domain generalization for rare-event imaging. Technically sound and novel enough for *IEEE‚ÄØTMI*, contingent upon stronger experimental substantiation and clearer presentation.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RareSaGe*, a methodological framework for single-domain generalization (SDG) targeting rare event detection in medical images. The approach integrates discriminative expert knowledge with data-driven deep learning to improve generalization without access to target domain data. Validation is provided through seizure onset zone (SOZ) detection from fMRI across two centers. The paper is conceptually sound, well organized, and aligns with the methodological focus of imaging and machine learning research, though certain sections could benefit from improved clarity and empirical substantiation.\n\n---\n\n**Major Comments**  \n1. **Methodological clarity:** A more explicit explanation‚Äîsuch as pseudocode or a detailed diagram‚Äîshould describe the interaction between the deep learning and symbolic knowledge modules at inference time.  \n2. **Component evaluation:** Include an ablation or sensitivity study quantifying the specific contributions of each component (entropy-driven rarity definition, overlap separation, and knowledge-based classifier) to the final performance.  \n3. **Theoretical validation:** The proposed entropy‚Äìinformation linkage is conceptually interesting but not empirically supported; correlations between entropy measures and misclassification or generalization performance should be demonstrated.  \n4. **Generalizability:** The study remains limited to fMRI-based SOZ detection. Discussion or preliminary evidence for applicability to other imaging modalities would improve the manuscript‚Äôs impact.  \n5. **Statistical robustness:** Report confidence intervals or perform statistical testing to evaluate whether observed cross-center performance gains are significant.  \n6. **Presentation and readability:** Simplify long sentences, improve logical flow in the methods section, and better illustrate data flow in figures (especially Fig.‚ÄØ1).  \n7. **Comparative context:** Expand discussion to include related hybrid or physics-informed learning methods to situate this approach clearly within existing literature.\n\n---\n\n**Minor Comments**  \n- Define all acronyms at first use and ensure reference formatting consistency.  \n- Correct grammatical and typographical errors and align equation formatting.  \n- Clarify validation protocol to confirm that only same-domain data are used for tuning in the SDG setting.  \n- Enhance figure captions for interpretability and ensure diagram readability.  \n\n---\n\n**Summary Paragraph**  \n*RareSaGe* offers a theoretically motivated and practically relevant framework that merges symbolic expert knowledge with deep learning to address rare event detection under single-domain conditions. The paper demonstrates strong cross-center performance and methodological soundness, but further analysis is needed to substantiate generalizability, reinforce theoretical claims, and clarify methodological details. Overall, it represents a promising contribution that requires targeted revisions to achieve the necessary rigor and clarity.  \n\n---\n\n**Decision Recommendation:** **Major Revision** ‚Äì The study is conceptually strong and potentially impactful but requires additional experiments, clearer exposition, and enhanced validation of theoretical and empirical claims.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses Single Domain Generalization (SDG) for rare event detection in medical imaging, focusing on seizure onset zone (SOZ) identification from fMRI data. The authors propose RareSaGe, which integrates domain-invariant expert knowledge with deep learning to handle challenges of limited data, class imbalance, and domain shifts. The method first uses CLIP to identify the \"overlap class\" most similar to the rare event class, then employs deep learning for overlap/non-overlap classification and knowledge-based quadratic optimization for rare/non-rare distinction. Expert knowledge is encoded as atomic propositions about anatomical brain regions and activation patterns. Evaluation on two-center datasets (pediatric and adult patients) demonstrates F1 scores of 90.2% for across-trial validation and 94.7% for aggregate-trial validation, outperforming state-of-the-art vision transformers and large vision models.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - Definition 3.1's entropy-based rarity criterion uses 2œÉ threshold but lacks theoretical justification (Section 3.1, Equations 1-3)\n  - The connection between Cramer-Rao Lower Bound theory and class entropy in Equation 4 is stated without rigorous derivation (Page 5)\n  - Algorithm 1 references \"significant change in validation accuracy\" in line 2 but this termination criterion is never formally defined or used (Page 6)\n\n‚Ä¢ **Experimental design limitations and evaluation gaps**\n  - Limited dataset size with only two centers (52 and 31 patients) raises questions about generalizability claims (Section 4.1)\n  - No comparison with domain adaptation or multi-source domain generalization methods that the paper criticizes (Table 1)\n  - Cross-validation methodology unclear for aggregate trials - whether patient-level or image-level splits were used affects validity (Section 4.2)\n\n‚Ä¢ **Methodological concerns regarding knowledge integration**\n  - Expert knowledge formula Œ∫SOZ in Equation 6 appears ad-hoc without validation of individual atomic propositions (Section 3.4.2)\n  - CLIP similarity threshold of 0.78 vs 0.74 for overlap class selection lacks sensitivity analysis (Section 3.4.3)\n  - Confidence threshold tc set to 0.9 without systematic optimization or ablation study (Section 3.4.3)\n\n‚Ä¢ **Insufficient technical details and reproducibility issues**\n  - Missing hyperparameters and training details for baseline methods make comparison validity questionable (Table 1)\n  - SMOTE application for SOZ class balancing mentioned but parameters not specified (Section 3.4.2)\n  - No statistical significance testing reported despite claims of superior performance (Tables 1-2)\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical foundations and provide rigorous derivations**\n  - Provide theoretical justification for the 2œÉ threshold in Definition 3.1 through empirical analysis or literature support\n  - Include complete derivation linking Barron's entropy-Fischer information relationship to the CRLB argument\n  - Formalize the termination criterion in Algorithm 1 or remove the undefined validation accuracy condition\n\n‚Ä¢ **Expand experimental evaluation and address dataset limitations**\n  - Include additional medical centers or synthetic domain shifts to strengthen generalizability claims beyond two-center validation\n  - Add quantitative comparisons with domain adaptation and multi-source methods using the same datasets\n  - Clarify cross-validation splits and ensure patient-level separation to avoid data leakage, particularly for aggregate trials\n\n‚Ä¢ **Validate knowledge integration approach systematically**\n  - Conduct ablation studies on individual atomic propositions in Equation 6 to demonstrate their necessity and sufficiency\n  - Perform sensitivity analysis on CLIP similarity thresholds and provide principled selection criteria\n  - Optimize confidence threshold tc systematically through grid search or cross-validation rather than fixed selection\n\n‚Ä¢ **Enhance technical documentation and statistical rigor**\n  - Provide complete hyperparameter specifications and training protocols for all baseline methods to ensure fair comparison\n  - Specify SMOTE parameters and justify synthetic sample generation strategy for the rare SOZ class\n  - Include statistical significance tests (e.g., McNemar's test, confidence intervals) to validate performance improvements over baselines",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RareSaGe*, a method for Single Domain Generalization (SDG) in rare event detection from fMRI data, specifically targeting seizure onset zone (SOZ) identification. The approach integrates domain-invariant expert knowledge with deep learning to address limited data, class imbalance, and domain shift challenges. It employs CLIP-based identification of an ‚Äúoverlap class,‚Äù followed by classification and knowledge-based optimization distinguishing rare from non-rare events. Expert knowledge is incorporated through atomic propositions describing brain regions and activation patterns. Experiments on pediatric and adult datasets are reported to achieve high F1 scores and surpass state-of-the-art models. The overall presentation is intelligible, but several aspects require stronger theoretical grounding, clearer methodology, and expanded evaluation for full credibility.\n\n---\n\n**Major Comments**  \n1. **Mathematical formulation and theoretical justification** ‚Äì The entropy-based rarity criterion in Definition 3.1 lacks justification for the 2œÉ threshold. The link between Cramer‚ÄìRao Lower Bound and class entropy is asserted without derivation, and Algorithm 1‚Äôs termination condition referencing ‚Äúvalidation accuracy changes‚Äù is neither defined nor applied.  \n2. **Experimental design and generalizability** ‚Äì Evaluation relies on data from only two centers, limiting evidence for generalization. Comparisons omit domain adaptation and multi-source generalization methods that are directly criticized in the paper. The description of cross-validation procedures for aggregate trials is unclear, particularly regarding whether splits are patient- or image-level.  \n3. **Knowledge integration methodology** ‚Äì The expert knowledge formula (Œ∫SOZ) appears ad hoc, with no validation of its atomic propositions. CLIP similarity and confidence thresholds are chosen without sensitivity analysis or systematic optimization.  \n4. **Reproducibility and reporting gaps** ‚Äì Baseline model hyperparameters, training details, and SMOTE configuration are missing. No statistical significance testing accompanies performance claims, reducing confidence in the reported superiority.\n\n---\n\n**Minor Comments**  \n- Clarify parameter symbols and equations to avoid ambiguity.  \n- Standardize referencing of definitions, sections, and figures.  \n- Improve formatting of Tables 1‚Äì2 for readability.\n\n---\n\n**Summary Paragraph**  \nThis work proposes an ambitious integration of expert-driven and deep learning approaches for rare event SDG in medical imaging. While the conceptual contribution is interesting and potentially impactful, the manuscript lacks theoretical justification in key derivations, provides limited experimental breadth, and leaves aspects of the knowledge encoding and evaluation insufficiently validated. Enhanced transparency in implementation and expanded comparison baselines are needed to strengthen the study‚Äôs credibility and reproducibility.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper presents a promising idea with strong potential but requires substantial clarification, additional experiments, and theoretical reinforcement before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces a novel framework called RareSaGe for single domain generalization (SDG) in rare event detection in medical imaging. The method integrates domain-invariant expert knowledge with deep learning (DL) to improve the detection of rare events, such as seizure onset zones (SOZ) in fMRI data, by addressing challenges like limited and imbalanced data, overlapping features, and high intra-class variability. The authors claim that their approach outperforms state-of-the-art DL techniques and knowledge-based systems, achieving an average F1 score of 90.2% across multi-center datasets.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: While the integration of expert knowledge with DL for rare event detection is a promising direction, the manuscript needs to more clearly articulate how RareSaGe differentiates itself from existing methods that combine expert knowledge and DL. The authors should provide a more thorough discussion of related work and highlight unique contributions.\n\n2. Evaluation Design: The experiments are conducted on two datasets from independent centers, but the validation strategy could be more comprehensive. The manuscript should include additional validation scenarios, such as more diverse anatomical regions or other types of medical imaging data, to further validate the generalizability of the proposed method.\n\n3. Comparisons: The comparisons against state-of-the-art DL techniques and knowledge-based systems are commendable, but the manuscript should include more recent and relevant baselines, such as federated learning approaches or domain adaptation techniques specifically designed for rare event detection. This would provide a more robust benchmark for assessing the effectiveness of RareSaGe.\n\n4. Reproducibility: Although the authors mention that the code and data are available, the manuscript lacks detailed descriptions of hyperparameters, preprocessing steps, and model architectures. Providing these details is crucial for ensuring reproducibility and allowing others to build upon the presented work.\n\n###\n\n## Minor Comments\n1. Clarity of Figures: Figures 1 and 2 are somewhat cluttered. Simplifying these figures and adding more explanatory text would enhance their clarity and utility.\n\n2. Notation Consistency: There are inconsistencies in the notation used throughout the manuscript, particularly in the definition of class-wise entropy. Ensuring consistency in notation would improve readability.\n\n3. Acronym Definitions: Some acronyms, such as \"IC,\" are used without clear definitions. Defining these terms would help readers unfamiliar with the terminology.\n\n4. Typographical Issues: Minor typographical errors, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7), should be corrected for a polished presentation.\n\n###\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: detecting rare events in medical imaging with limited and imbalanced data. The proposed RareSaGe framework, which integrates expert knowledge with DL, is technically innovative and demonstrates promising results in the context of SOZ detection in fMRI data. However, the evaluation is somewhat limited and could benefit from broader validation scenarios. The reproducibility of the approach is also partially compromised by incomplete methodological details. Overall, while the idea is promising, the current evidence does not fully meet the rigorous standards expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis, strengthen validation across more diverse datasets and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a framework called *RareSaGe* for single-domain generalization in rare event detection within medical imaging. The approach combines domain-invariant expert knowledge with deep learning to address challenges from limited and imbalanced datasets, overlapping feature distributions, and high intra-class variability. The authors evaluate the method on multi-center fMRI datasets targeting seizure onset zone (SOZ) detection, reporting an average F1 score of 90.2% and claiming improvements over current state-of-the-art deep learning and knowledge-based approaches.  \n\n**Major Comments**  \n1. **Novelty and Positioning:** The study‚Äôs integration of expert knowledge and deep learning is promising; however, the manuscript should more clearly delineate how RareSaGe differs from prior work that also combines knowledge-based reasoning and DL. A more detailed related work discussion highlighting distinctive aspects would clarify the novelty.  \n2. **Evaluation Design:** Validation on two independent datasets is a good start, but the evaluation remains limited. Expanding experiments to include additional anatomical regions or imaging modalities would strengthen claims about generalizability.  \n3. **Comparative Analysis:** While comparisons with well-established baselines are useful, the study should incorporate more recent methods, including federated learning or domain adaptation approaches relevant to rare event detection, to provide a more robust benchmark.  \n4. **Reproducibility:** Although code and data availability are mentioned, the paper lacks essential implementation details such as hyperparameter settings, preprocessing procedures, and architectural specifications. These omissions hinder replication and should be addressed for transparency and reproducibility.  \n\n**Minor Comments**  \n1. **Figures:** Figures 1 and 2 appear crowded; simplifying layouts and providing additional explanations would improve clarity.  \n2. **Notation:** Some inconsistencies appear in the notation, particularly for class-wise entropy; harmonizing notation would enhance readability.  \n3. **Acronyms:** Certain acronyms (e.g., ‚ÄúIC‚Äù) are undefined and should be explicitly introduced.  \n4. **Typos:** Minor typographical errors, such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes,‚Äù should be corrected.  \n\n**Summary Paragraph**  \nOverall, the manuscript addresses an important problem‚Äîrobust detection of rare medical events under constrained data conditions‚Äîand introduces an appealing expert-informed DL framework. Despite strong potential and promising reported results, the current work would benefit from clearer novelty positioning, broader and more diverse validation, and fuller methodological transparency. These improvements are needed to substantiate claims and support reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The authors should enhance comparative analysis, broaden validation experiments, and provide complete methodological details to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## SINGLE DOMAIN GENERALIZATION FOR RARE EVENT DETECTION IN MEDICAL IMAGING\n\n### Summary\n\nThe paper proposes RareSaGe, a single-domain generalization (SDG) approach for rare event detection in medical imaging that combines data-driven deep learning with domain-invariant expert knowledge. The method first uses CLIP-derived, class-agnostic embeddings to identify an ‚Äúoverlap‚Äù class most visually similar to the rare class and delegates that discrimination to a DL module, while separating the rare class from remaining non-overlap classes using an expert-knowledge SVM built on interpretable propositions. A case study on seizure onset zone (SOZ) detection from rs-fMRI independent components across two centers demonstrates substantial gains over fine-tuned ViTs, CLIP, and knowledge-only baselines, achieving F1 scores above 85% in across-center tests and 94.7% in aggregated multi-center validation.\n\n### Strengths\n\n- Technical novelty and innovationThe orchestration strategy that explicitly identifies the most confusable ‚Äúoverlap‚Äù class via class-agnostic CLIP embeddings and routes it to a DL classifier while handling the rare-vs-non-rare decision via a knowledge-based SVM is a thoughtful decomposition aligned with the data imbalance and feature-overlap realities of clinical rare-event settings.The class-wise entropy criterion for selecting among candidate models is a principled, classifier-agnostic heuristic and ties to a broader theoretical motivation about information scarcity in rare classes.The approach leverages interpretable, anatomically grounded propositions for SOZ (cluster count, gray/white/vascular overlap, spectral sparsity) that clinicians can audit and reason about.\n- Experimental rigor and validationAcross-center evaluation (train on Center A, test on Center B and vice versa) is the appropriate SDG protocol and more convincing than random splits.Aggregate 5-fold 3-repeat cross-center validation provides additional evidence that the approach learns domain-invariant cues when more data are available.The age-based cross-distribution test (children‚Üíadults) is a useful additional stress test for distribution shift.\n- Clarity of presentationThe overall pipeline, including the role of CLIP/PCA for overlap discovery, DL for noise vs non-noise, expert propositions, SMOTE, and the fusion rule, is explained at a high level with a clear narrative.The expert knowledge is made explicit in a concise logical form, aiding interpretability and reproducibility.\n- Significance of contributionsSDG for rare events in medical imaging is both under-explored and clinically important; the proposed method addresses high-stakes deployment challenges (scarcity, imbalance, domain shift, interpretability).Demonstrated performance improvements over strong DL baselines suggest practical utility for multi-center clinical workflows.\n\n- The orchestration strategy that explicitly identifies the most confusable ‚Äúoverlap‚Äù class via class-agnostic CLIP embeddings and routes it to a DL classifier while handling the rare-vs-non-rare decision via a knowledge-based SVM is a thoughtful decomposition aligned with the data imbalance and feature-overlap realities of clinical rare-event settings.\n- The class-wise entropy criterion for selecting among candidate models is a principled, classifier-agnostic heuristic and ties to a broader theoretical motivation about information scarcity in rare classes.\n- The approach leverages interpretable, anatomically grounded propositions for SOZ (cluster count, gray/white/vascular overlap, spectral sparsity) that clinicians can audit and reason about.\n\n- Across-center evaluation (train on Center A, test on Center B and vice versa) is the appropriate SDG protocol and more convincing than random splits.\n- Aggregate 5-fold 3-repeat cross-center validation provides additional evidence that the approach learns domain-invariant cues when more data are available.\n- The age-based cross-distribution test (children‚Üíadults) is a useful additional stress test for distribution shift.\n\n- The overall pipeline, including the role of CLIP/PCA for overlap discovery, DL for noise vs non-noise, expert propositions, SMOTE, and the fusion rule, is explained at a high level with a clear narrative.\n- The expert knowledge is made explicit in a concise logical form, aiding interpretability and reproducibility.\n\n- SDG for rare events in medical imaging is both under-explored and clinically important; the proposed method addresses high-stakes deployment challenges (scarcity, imbalance, domain shift, interpretability).\n- Demonstrated performance improvements over strong DL baselines suggest practical utility for multi-center clinical workflows.\n\n### Weaknesses\n\n- Technical limitations or concernsThe theoretical link between the proposed class-wise entropy (KNN-density based) and Fisher information/CRLB is suggestive but not rigorous for the specific entropy definition used; claims risk being over-stated without tighter assumptions or empirical validation on synthetic data.The overlap-class selection uses CLIP embeddings and PCA with cosine similarity, but sensitivity to design choices (K, number of PCs, distance metric, CLIP variant) is not analyzed; the reported similarity margins (e.g., 0.78 vs 0.74) appear small.The ‚Äúmachine orchestration‚Äù procedure selects models by minimum entropy computed on the training domain, which may bias selection; a validation protocol for model selection is not fully specified.\n- Experimental gaps or methodological issuesMissing ablations on key components: CLIP-based overlap identification vs alternatives, the effect of the 0.9 confidence threshold, SMOTE on expert features, and the contribution of each proposition to generalization (across centers).Important hyperparameters are under-specified: K in the KNN-entropy, exact distance metric, number of PCA components, SVM kernel/C, CNN architecture/training details, and how class weights, early stopping, and threshold tc are chosen.Comparison to closely related hybrid pipelines (e.g., prior SOZ expert+DL methods from the same group) under the same across-center protocol is missing; this makes it harder to isolate the contribution of the new ‚Äúoverlap-first orchestration‚Äù beyond earlier two-stage DL+knowledge systems.\n- Clarity or presentation issuesInconsistencies in notation and logic: overlap class is described as ‚Äúmost similar to rare class‚Äù but later defined as c‚ÇÄ = argmin(Œ∏·µ¢) (minimum entropy), which does not directly operationalize similarity; this needs correction.Some metric reporting is unclear (F1 vs ‚ÄúAverage F1‚Äù by class), and the abstract‚Äôs numbers do not perfectly align with Table 1 in phrasing.Algorithm 1‚Äôs ‚Äúwhile‚Äù loop conditions (‚ÄúŒ® not empty and significant change in validation accuracy‚Äù) are not tied to concrete stopping criteria in the experiments.\n- Missing related work or comparisonsThe SDG literature in medical imaging primarily focuses on segmentation; nonetheless, several single-source DG methods (e.g., causality-inspired augmentations, low-rank latent regularization) could be adapted to classification and should be discussed as conceptual baselines.Recent hybrid neural-symbolic integrations (e.g., SAIF-DL, NeurASP, DeepProbLog) and related rare-event orchestration frameworks (e.g., STORM) deserve a more explicit contrast with the present approach‚Äôs training-time vs. post-hoc fusion design, especially around rule uncertainty and domain invariance.\n\n- The theoretical link between the proposed class-wise entropy (KNN-density based) and Fisher information/CRLB is suggestive but not rigorous for the specific entropy definition used; claims risk being over-stated without tighter assumptions or empirical validation on synthetic data.\n- The overlap-class selection uses CLIP embeddings and PCA with cosine similarity, but sensitivity to design choices (K, number of PCs, distance metric, CLIP variant) is not analyzed; the reported similarity margins (e.g., 0.78 vs 0.74) appear small.\n- The ‚Äúmachine orchestration‚Äù procedure selects models by minimum entropy computed on the training domain, which may bias selection; a validation protocol for model selection is not fully specified.\n\n- Missing ablations on key components: CLIP-based overlap identification vs alternatives, the effect of the 0.9 confidence threshold, SMOTE on expert features, and the contribution of each proposition to generalization (across centers).\n- Important hyperparameters are under-specified: K in the KNN-entropy, exact distance metric, number of PCA components, SVM kernel/C, CNN architecture/training details, and how class weights, early stopping, and threshold tc are chosen.\n- Comparison to closely related hybrid pipelines (e.g., prior SOZ expert+DL methods from the same group) under the same across-center protocol is missing; this makes it harder to isolate the contribution of the new ‚Äúoverlap-first orchestration‚Äù beyond earlier two-stage DL+knowledge systems.\n\n- Inconsistencies in notation and logic: overlap class is described as ‚Äúmost similar to rare class‚Äù but later defined as c‚ÇÄ = argmin(Œ∏·µ¢) (minimum entropy), which does not directly operationalize similarity; this needs correction.\n- Some metric reporting is unclear (F1 vs ‚ÄúAverage F1‚Äù by class), and the abstract‚Äôs numbers do not perfectly align with Table 1 in phrasing.\n- Algorithm 1‚Äôs ‚Äúwhile‚Äù loop conditions (‚ÄúŒ® not empty and significant change in validation accuracy‚Äù) are not tied to concrete stopping criteria in the experiments.\n\n- The SDG literature in medical imaging primarily focuses on segmentation; nonetheless, several single-source DG methods (e.g., causality-inspired augmentations, low-rank latent regularization) could be adapted to classification and should be discussed as conceptual baselines.\n- Recent hybrid neural-symbolic integrations (e.g., SAIF-DL, NeurASP, DeepProbLog) and related rare-event orchestration frameworks (e.g., STORM) deserve a more explicit contrast with the present approach‚Äôs training-time vs. post-hoc fusion design, especially around rule uncertainty and domain invariance.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe core decomposition‚ÄîDL for the overlap class and SVM on expert features for rare vs non-rare‚Äîis well-motivated by class imbalance and inter-class similarity. The fusion rule that allows a high-confidence SOZ label to override a DL noise label should reduce false negatives on the rare class, which is desirable clinically.The KNN-entropy is a plausible heuristic for model selection and rarity quantification, but reproducible specification requires: K, metric, feature normalization, PCA dimensionality, and tie-breaking rules. A sensitivity study is necessary to confirm robustness.The theoretical justification connecting entropy to Fisher information/CRLB is conceptually appealing but not guaranteed for the proposed class-wise entropy functional; consider attenuating the claim or adding a controlled synthetic study demonstrating the monotonic relation under your settings.The CLIP-based overlap discovery is an intuitive way to remain class-agnostic while capturing visual similarity. However, medical images (rs-fMRI IC maps) are far from CLIP‚Äôs pretraining distribution. Include evidence that CLIP embeddings yield stable inter-class similarity rankings across centers and are superior to alternatives (e.g., self-supervised features learned on source data, VGG- or ResNet-based features, or handcrafted features).\n- Experimental evaluation assessmentThe across-center results are compelling and well-aligned with SDG goals. The additional children‚Üíadults test strengthens the case for generalization under demographic shift.Please add ablations:With and without CLIP-based overlap step; vary overlap candidate to show the benefit of the overlap-first decomposition.Threshold analysis for the 0.9 confidence rule; plot SOZ sensitivity/precision vs threshold to help users set operating points.SMOTE on expert features vs class weighting or other imbalance remedies; assess leakage risks and ensure SMOTE is applied only within training folds.Per-proposition contribution across centers (e.g., drop one proposition at a time; report cross-center stability).Entropy-based model selection vs validation-accuracy selection; show that entropy is not just correlating with in-domain accuracy.Report full training details for reproducibility: CNN architecture, optimizer, epochs, augmentation, early stopping criterion, SVM kernel and C/Œ≥, PCA components, K in KNN, distance metrics, and CLIP variant and preprocessing.Provide class-wise confusion matrices for across-center tests, particularly SOZ false negatives vs false positives, and error analysis of overlap errors (SOZ misrouted as noise).\n- Comparison with related work (using the summaries provided)Compared to causality-inspired single-source DG augmentations (e.g., GIN+IPA), your approach addresses a different failure mode: extreme class imbalance and intentional use of domain-invariant clinical rules, rather than inducing invariance via augmentation. A short discussion clarifying this complementarity and whether your overlap-first orchestration could benefit from such augmentations would help.Latent low-rank approaches (e.g., LDDG) argue for compact class-dependent latents to improve cross-domain generalization. While primarily used in segmentation/classification on natural dermoscopy images, discuss whether adding such a structural constraint to your DL overlap classifier would further help in low-data clinical settings.Hybrid neural-symbolic training (e.g., SAIF-DL) injects rules into the loss; your design maintains separation and fuses at the decision stage. A short analysis of trade-offs (training-time constraints vs. post-hoc interpretability, handling of uncertain/contradictory rules) would contextualize your design choice.STORM-style entropy-based orchestration of modalities for rare events is conceptually close; articulating the differences (overlap-class discovery via class-agnostic embeddings, two-stage routing, and SOZ-specific propositions) will clarify your novelty.\n- Discussion of broader impact and significanceThe approach directly addresses a critical barrier to clinical deployment: robust rare-event detection across centers without accessing target-domain data. The interpretability of proposition-based features and the ability to audit model decisions are strong positives for clinical adoption.Potential risks: reliance on fixed expert rules may under-detect atypical SOZ manifestations or novel phenotypes; consider adding uncertainty measures or soft rule weighting to mitigate brittleness.The design is generalizable to other rare clinical findings (e.g., specific lesion subtypes) provided domain propositions can be articulated. Including a brief discussion on adapting the pipeline to another modality/task would strengthen generality claims.\n\n- The core decomposition‚ÄîDL for the overlap class and SVM on expert features for rare vs non-rare‚Äîis well-motivated by class imbalance and inter-class similarity. The fusion rule that allows a high-confidence SOZ label to override a DL noise label should reduce false negatives on the rare class, which is desirable clinically.\n- The KNN-entropy is a plausible heuristic for model selection and rarity quantification, but reproducible specification requires: K, metric, feature normalization, PCA dimensionality, and tie-breaking rules. A sensitivity study is necessary to confirm robustness.\n- The theoretical justification connecting entropy to Fisher information/CRLB is conceptually appealing but not guaranteed for the proposed class-wise entropy functional; consider attenuating the claim or adding a controlled synthetic study demonstrating the monotonic relation under your settings.\n- The CLIP-based overlap discovery is an intuitive way to remain class-agnostic while capturing visual similarity. However, medical images (rs-fMRI IC maps) are far from CLIP‚Äôs pretraining distribution. Include evidence that CLIP embeddings yield stable inter-class similarity rankings across centers and are superior to alternatives (e.g., self-supervised features learned on source data, VGG- or ResNet-based features, or handcrafted features).\n\n- The across-center results are compelling and well-aligned with SDG goals. The additional children‚Üíadults test strengthens the case for generalization under demographic shift.\n- Please add ablations:With and without CLIP-based overlap step; vary overlap candidate to show the benefit of the overlap-first decomposition.Threshold analysis for the 0.9 confidence rule; plot SOZ sensitivity/precision vs threshold to help users set operating points.SMOTE on expert features vs class weighting or other imbalance remedies; assess leakage risks and ensure SMOTE is applied only within training folds.Per-proposition contribution across centers (e.g., drop one proposition at a time; report cross-center stability).Entropy-based model selection vs validation-accuracy selection; show that entropy is not just correlating with in-domain accuracy.\n- Report full training details for reproducibility: CNN architecture, optimizer, epochs, augmentation, early stopping criterion, SVM kernel and C/Œ≥, PCA components, K in KNN, distance metrics, and CLIP variant and preprocessing.\n- Provide class-wise confusion matrices for across-center tests, particularly SOZ false negatives vs false positives, and error analysis of overlap errors (SOZ misrouted as noise).\n\n- With and without CLIP-based overlap step; vary overlap candidate to show the benefit of the overlap-first decomposition.\n- Threshold analysis for the 0.9 confidence rule; plot SOZ sensitivity/precision vs threshold to help users set operating points.\n- SMOTE on expert features vs class weighting or other imbalance remedies; assess leakage risks and ensure SMOTE is applied only within training folds.\n- Per-proposition contribution across centers (e.g., drop one proposition at a time; report cross-center stability).\n- Entropy-based model selection vs validation-accuracy selection; show that entropy is not just correlating with in-domain accuracy.\n\n- Compared to causality-inspired single-source DG augmentations (e.g., GIN+IPA), your approach addresses a different failure mode: extreme class imbalance and intentional use of domain-invariant clinical rules, rather than inducing invariance via augmentation. A short discussion clarifying this complementarity and whether your overlap-first orchestration could benefit from such augmentations would help.\n- Latent low-rank approaches (e.g., LDDG) argue for compact class-dependent latents to improve cross-domain generalization. While primarily used in segmentation/classification on natural dermoscopy images, discuss whether adding such a structural constraint to your DL overlap classifier would further help in low-data clinical settings.\n- Hybrid neural-symbolic training (e.g., SAIF-DL) injects rules into the loss; your design maintains separation and fuses at the decision stage. A short analysis of trade-offs (training-time constraints vs. post-hoc interpretability, handling of uncertain/contradictory rules) would contextualize your design choice.\n- STORM-style entropy-based orchestration of modalities for rare events is conceptually close; articulating the differences (overlap-class discovery via class-agnostic embeddings, two-stage routing, and SOZ-specific propositions) will clarify your novelty.\n\n- The approach directly addresses a critical barrier to clinical deployment: robust rare-event detection across centers without accessing target-domain data. The interpretability of proposition-based features and the ability to audit model decisions are strong positives for clinical adoption.\n- Potential risks: reliance on fixed expert rules may under-detect atypical SOZ manifestations or novel phenotypes; consider adding uncertainty measures or soft rule weighting to mitigate brittleness.\n- The design is generalizable to other rare clinical findings (e.g., specific lesion subtypes) provided domain propositions can be articulated. Including a brief discussion on adapting the pipeline to another modality/task would strengthen generality claims.\n\n### Questions for Authors\n\n- How sensitive is the overlap-class selection to the embedding choice (CLIP vs other backbones), PCA dimensionality, and similarity metric? Can you provide an ablation or stability analysis across these choices?\n- What are the exact KNN-entropy hyperparameters (K, distance metric, feature scaling), and how were they selected? Do results significantly change with different K or metrics?\n- For the SVM-based expert classifier, which kernel and hyperparameters were used, and how were they tuned? Was SMOTE applied only on the training set within each fold to avoid leakage?\n- Can you clarify the inconsistency between selecting the overlap class by ‚Äúmost similar to rare class‚Äù vs ‚Äúargmin entropy‚Äù? Which criterion was used in practice?\n- How was the 0.9 confidence threshold chosen for overriding DL noise labels? Can you provide ROC/PR curves showing the trade-off between SOZ sensitivity and false positives as the threshold varies?\n- Could you compare against your prior hybrid SOZ pipelines under the same across-center protocol to isolate the incremental gain from the current overlap-first orchestration?\n- Did you analyze the cross-center stability of individual expert propositions (e.g., distributions of pg, pw, pv across centers)? Which propositions contribute most to SDG performance?\n- How does the method perform under stronger scanner/protocol shifts (e.g., additional centers) or different preprocessing pipelines for ICA? Any insights on robustness to ICA variability?\n- What is the computational cost (training and inference time) for the full pipeline compared to DL-only baselines? Is the orchestration step (entropy calculations) a bottleneck?\n- Could causality-inspired or frequency-domain augmentations (single-source DG methods) improve the overlap DL classifier? If attempted, what were the outcomes?\n\n### Overall Assessment\n\nThis paper tackles an important and under-explored problem: single-domain generalization for rare event detection in clinical imaging. The proposed decomposition‚Äîfinding an overlap class via class-agnostic embeddings, routing it to a DL classifier, and using interpretable expert propositions with an SVM for the rare-vs-non-rare decision‚Äîdirectly addresses the intertwined challenges of scarcity, class overlap, and domain shift. The across-center experiments and additional demographic-shift test are convincing and show large improvements over several strong DL baselines, and the aggregate validation suggests the method scales positively with data. However, several aspects require tightening before top-tier publication: clearer and more rigorous specification of the entropy machinery and overlap selection, ablations on core design choices (CLIP, KNN parameters, SMOTE, thresholding, contribution of propositions), and a direct comparison to closely related hybrid pipelines to isolate the novelty and gains of the proposed orchestration. The theoretical motivation around entropy and Fisher information is intriguing but currently too informal to carry weight; an empirical validation or a toned-down claim would improve credibility. With these additions, the paper would make a solid and practically impactful contribution to robust, interpretable rare-event detection under SDG constraints.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **RareSaGe**, a single-domain generalization (SDG) framework for rare event detection in medical imaging. The approach integrates data-driven deep learning with domain-invariant expert knowledge, aiming to improve robustness across clinical centers without target-domain access. It identifies an ‚Äúoverlap‚Äù class using class-agnostic CLIP embeddings, delegates its discrimination to a DL module, and separates rare from non-overlap classes via an SVM trained on interpretable, clinically grounded propositions. Validation on seizure onset zone (SOZ) detection from rs-fMRI components shows high performance across centers and age distributions. The work is clearly presented, technically detailed, and addresses a clinically important and under-studied SDG scenario.  \n\n**Major Comments**  \n1. **Theoretical and methodological grounding:** The link between the KNN-based class-wise entropy and Fisher information is suggestive but not formally justified. Stronger theoretical argumentation or a synthetic validation would improve credibility.  \n2. **Overlap and orchestration design:** The CLIP+PCA overlap-class selection requires sensitivity analysis (choice of K, PCs, similarity metric, CLIP variant). The selection of models by minimum entropy on training data could bias results; a clearer validation-based selection procedure is needed.  \n3. **Ablation and reproducibility gaps:** The paper lacks ablations isolating contributions of CLIP-based overlap detection, the 0.9 confidence threshold, SMOTE usage, and individual expert propositions. Precise hyperparameters (e.g., KNN settings, SVM kernel/C, CNN details) should be reported for reproducibility.  \n4. **Comparative evaluation:** Comparison to prior hybrid SOZ pipelines and related SDG or neural-symbolic systems (e.g., SAIF-DL, STORM, DeepProbLog) is missing, limiting assessment of incremental novelty.  \n5. **Presentation and consistency:** Some notation is inconsistent (similarity vs. entropy definition of overlap class), and metric reporting (F1 vs. average F1) is unclear. Algorithmic stopping criteria also need explicit specification.  \n\n**Minor Comments**  \n- Provide stability experiments for CLIP embeddings across centers.  \n- Include class-wise confusion matrices and threshold analyses.  \n- Discuss computational cost of entropy-based orchestration.  \n- Briefly clarify how propositions might adapt to other modalities or phenotypes.  \n- Improve alignment between abstract and table metrics.  \n\n**Summary Paragraph**  \nThis work presents a conceptually strong and interpretable SDG framework addressing rare-event detection in medical imaging. Its decomposition strategy and across-center evaluation offer clear practical relevance and technical novelty. Nonetheless, its theoretical justification, methodological robustness, and clarity require refinement. Additional ablations, rigorous specification of entropy parameters, validation of overlap selection, and contextualized comparisons would significantly strengthen the contribution.  \n\n**Decision Recommendation:** **Major Revision** ‚Äî The study is promising and well-motivated but needs further theoretical clarification, comprehensive ablations, and clearer methodological reporting before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"SINGLE DOMAIN GENERALIZATION FOR RARE EVENT DETECTION IN MEDICAL IMAGING\"\n\n## 1. Opening Paragraph\n\nThis manuscript addresses the critical challenge of single domain generalization (SDG) for rare event detection in medical imaging, a problem that has been largely unexplored despite its significant clinical relevance. The authors identify that while SDG has been extensively studied for general image classification, existing approaches fail to handle the unique challenges of rare event detection in imbalanced medical datasets, where limited data availability and class overlap create substantial hurdles. The proposed RareSaGe framework introduces a novel approach that first leverages a pre-trained CLIP model to identify class similarities and isolate the most overlapping class with the rare event, then integrates domain-invariant expert knowledge with deep learning to effectively distinguish rare events across domains. The method is validated through a comprehensive case study on seizure onset zone (SOZ) detection in fMRI data from two independent medical centers, demonstrating significant performance improvements over state-of-the-art techniques with an average F1 score of 90.2% across cross-center validation while maintaining overall F1 scores above 85%. The work effectively demonstrates how carefully orchestrated integration of data-driven and knowledge-based approaches can overcome the limitations of existing methods for rare event detection in resource-constrained medical settings.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n1. **Lack of comprehensive comparison with existing SDG methods:** While the paper thoroughly compares with various DL architectures and knowledge-based systems, it lacks a detailed comparison with recent SDG-specific techniques (e.g., those mentioned in Section 2 like EPVT, CLIP the Gap). Given that the core contribution is a novel SDG approach for rare events, a more systematic comparison with SDG baselines would strengthen the paper's claims about novelty and effectiveness. The authors should include and discuss results from applying these methods to their rare event detection task.\n\n2. **Insufficient details on knowledge representation:** Section 3.4.2 describes the expert knowledge as logical propositions, but the implementation details of how these propositions are encoded and integrated into the QO machine are not sufficiently clear. The paper would benefit from a more detailed explanation of how the knowledge features are quantified, normalized, and processed before being fed into the SVM classifier. The relationship between the atomic propositions and the final feature vector needs clarification.\n\n3. **Limited discussion of failure cases and limitations:** The paper focuses primarily on successful outcomes without adequately addressing scenarios where the approach might fail or underperform. A more balanced analysis that includes qualitative examples of false negatives/positives, discussion of cases where the knowledge-DL integration breaks down, and limitations of the current implementation would provide readers with a more realistic understanding of the method's boundaries.\n\n### Minor Comments\n\n1. **Unclear confidence threshold selection:** Section 3.4.3 mentions using a confidence threshold of 0.9 but doesn't explain how this value was determined or whether it was optimized. The authors should clarify the threshold selection process and potentially include an ablation study showing how performance varies with different threshold values.\n\n2. **Inconsistent terminology:** The paper sometimes refers to the knowledge-based component as \"EKE\" (Expert Knowledge Extractor) and other times as \"QO\" (Quadratic Optimization) machine. Standardizing the terminology would improve readability and conceptual clarity.\n\n3. **Limited justification for SVM selection:** While Section 3.4.2 mentions SVM as the QO machine, there's insufficient justification for why SVM was chosen over other classifiers for the knowledge-based component. A brief discussion of alternative classifiers considered and why SVM was ultimately selected would strengthen the methodological explanation.\n\n4. **Ambiguity in Figure 1:** The \"Label Predictor\" section of Figure 1 shows a mathematical expression (Œ£exp.Wk) that isn't clearly explained in the main text. The authors should either provide a clearer explanation of this component or simplify the figure to match the textual description.\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a highly significant clinical challenge‚Äîrare event detection with limited data in medical imaging‚Äîa problem with substantial real-world implications for diagnostics and treatment planning. The focus on single domain generalization is particularly relevant given the practical constraints of multi-center data sharing in healthcare. The demonstrated application to seizure onset zone detection has direct clinical utility, making this work both theoretically sound and practically valuable.\n\n**Innovation:** The paper presents a novel integration of expert knowledge with deep learning specifically designed for rare event SDG, which has not been adequately addressed in prior literature. The entropy-based class ranking approach using CLIP features to identify overlap classes represents a creative solution to the data overlap challenge. However, the innovation would be stronger with more explicit differentiation from existing knowledge-DL integration approaches, particularly those mentioned in Section 2.\n\n**Evaluation:** The experimental design is generally robust, with two complementary validation approaches (across trial and aggregate trial) across two independent medical centers. The comprehensive comparison with multiple baselines provides strong evidence of the method's superiority. However, the evaluation would be strengthened by including more SDG-specific baselines and providing deeper error analysis to understand where and why the method succeeds or fails.\n\n**Reproducibility:** The paper demonstrates strong commitment to reproducibility with an anonymous code and data repository provided. The methodology is sufficiently detailed to allow replication, though some implementation specifics (particularly regarding the knowledge feature extraction) could be more precise. The inclusion of detailed hyperparameter settings in the supplementary material further enhances reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nThe manuscript presents a valuable contribution to the field of medical imaging with a well-motivated approach to a significant clinical challenge. The integration of expert knowledge with deep learning for rare event detection in single domain settings is both timely and potentially impactful. However, the paper requires substantial revisions to adequately address the limitations in the comparative analysis, methodological description, and critical evaluation of failure cases. The requested revisions, particularly the more comprehensive comparison with SDG methods and clarification of knowledge representation, are essential to establish the novelty and practical utility of the proposed approach. With these revisions, the paper would be a strong candidate for publication in TMI. I recommend a major revision with the expectation that the authors will address all major concerns and provide sufficient details to enable proper evaluation of the method's contributions relative to the state of the art.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the problem of **single domain generalization (SDG)** for **rare event detection in medical imaging**, a topic with considerable clinical relevance but limited prior exploration. The authors propose **RareSaGe**, a framework that integrates pre-trained CLIP features to identify overlapping classes with rare events and combines expert knowledge with deep learning to improve detection across domains. The approach is evaluated on **seizure onset zone (SOZ) detection** in fMRI data from two medical centers, achieving strong cross-center performance with average F1 scores above 85% and a reported 90.2% mean F1. The work convincingly shows that integrating data-driven and knowledge-based methods can address challenges in detecting rare events in resource-constrained healthcare contexts.  \n\n**Major Comments**  \n1. **Comparison with SDG methods:** While the authors benchmark against multiple deep learning and knowledge-based models, the study lacks direct experimental comparison with recent SDG-specific methods (e.g., EPVT, CLIP the Gap). Including these would more convincingly establish the method‚Äôs novelty and performance advantage.  \n2. **Detail of knowledge representation:** The description of how expert knowledge is formulated and used within the Quadratic Optimization (QO) machine is insufficient. Clarify how logical propositions are encoded, normalized, and connected to the final SVM-based representation to enhance methodological transparency.  \n3. **Discussion of limitations:** The paper primarily emphasizes positive outcomes but omits detailed analysis of failure cases (e.g., false positives/negatives, breakdowns in knowledge‚ÄìDL integration). Including these would provide a more balanced and informative evaluation of the approach‚Äôs boundaries.  \n\n**Minor Comments**  \n1. Clarify how the **confidence threshold (0.9)** was determined and whether it was optimized; consider adding an ablation study on threshold sensitivity.  \n2. Standardize terminology between **EKE (Expert Knowledge Extractor)** and **QO machine** to maintain consistency.  \n3. Provide justification for choosing **SVM** as the classifier in the knowledge-based component, including potential alternatives.  \n4. In **Figure 1**, the label predictor expression (Œ£exp.Wk) is unclear; ensure it is either explained or simplified for alignment with the text.  \n\n**Summary Paragraph**  \nThis work tackles an important clinical and methodological challenge by proposing an original framework for rare event SDG. It contributes a creative combination of expert knowledge integration and deep learning, validated on a clinically relevant task. The evaluation is thorough and reproducible, supported by released code and detailed experimental settings. However, the paper‚Äôs impact would be strengthened through broader comparative experiments with SDG baselines, clearer explication of the knowledge representation pipeline, and explicit discussion of known weaknesses or errors.  \n\n**Decision Recommendation:** **Major Revision**  \nThe study is promising and potentially impactful but requires significant revision to clarify methodological details, broaden benchmarking, and deepen evaluation analyses before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe manuscript tackles single‚Äëdomain generalisation (SDG) for the detection of rare events in medical imaging‚Äîa setting in which only a handful of highly imbalanced samples are available from a single clinical centre. The authors focus on resting‚Äëstate fMRI recordings from two independent sites, comprising three classes: noise, normal resting‚Äëstate networks, and seizure‚Äëonset‚Äëzone (SOZ). The SOZ class accounts for fewer than‚ÄØ5‚ÄØ% of the total samples.  \n\nTheir proposed RareSaGe pipeline first employs a large, pretrained vision model to extract class‚Äëagnostic representations and to locate the ‚Äúoverlap class‚Äù, i.e., the class whose embedding is most similar to that of the rare class. A deep‚Äëlearning classifier is then tasked with separating overlap from non‚Äëoverlap instances, while a knowledge‚Äëdriven quadratic‚Äëoptimisation module‚Äîbuilt from expert‚Äëderived anatomical and signal propositions‚Äîdiscriminates the rare class from the remaining non‚Äëoverlap samples.  \n\nAcross both cross‚Äëcentre and pooled‚Äëvalidation experiments, the combined system attains high F1 scores (‚âà‚ÄØ90‚ÄØ% for the SOZ class and >‚ÄØ85‚ÄØ% overall). The primary contribution is the seamless integration of domain‚Äëinvariant expert knowledge with deep learning, using class‚Äëwise entropy to guide the selection of the most appropriate classifier for rare‚Äëevent detection when data are scarce.\n\n---\n\n**## General feedback**\n\n*Significance* ‚Äì The work addresses an under‚Äëexplored aspect of SDG: recognising rare pathological events when only a single, highly imbalanced source is available. The SOZ detection scenario is clinically relevant and illustrates the problem well.\n\n*Innovation* ‚Äì The authors introduce a three‚Äëpart strategy: (i) CLIP‚Äëbased ranking of class similarity, (ii) entropy‚Äëguided choice between a deep‚Äëlearning model and a knowledge‚Äëbased optimiser, and (iii) explicit encoding of expert knowledge as logical propositions. While each element builds upon existing methods, their combination for rare‚Äëevent SDG is novel.\n\n*Evaluation* ‚Äì The experimental section includes thorough cross‚Äëcentre comparisons against pretrained CNN/Vit models, CLIP fine‚Äëtuning, and a purely knowledge‚Äëbased baseline. Reported improvements in F1 are impressive, yet the study would benefit from statistical significance testing, ablation of individual components, and validation on a third, completely unseen centre.\n\n*Reproducibility* ‚Äì The authors state that code and data are publicly released and that hyper‚Äëparameters are listed in ¬ß‚ÄØ4.2. Nevertheless, essential details‚Äîsuch as the precise 2‚ÄëD CNN architecture, training schedule, SMOTE parameters, the confidence‚Äëthreshold‚ÄØ*t_c*, and random seeds‚Äîare missing from the main manuscript, which hampers full replication.\n\n---\n\n**## Specific comments / critiques**\n\n1. **Rare‚Äëclass definition (¬ß‚ÄØ3.1, Eqns‚ÄØ1‚Äë3).**  \n   The criterion relies on class‚Äëwise entropy, but the manuscript does not disclose the value of *K* (the number of nearest neighbours), the actual entropy values (Œ∏) computed for each class, nor the œÉŒ∏ threshold employed to declare a class as rare. Providing these numbers would clarify the selection process.\n\n2. **Overlap‚Äëclass identification (¬ß‚ÄØ3.2).**  \n   Cosine similarity of CLIP embeddings (0.78‚ÄØvs‚ÄØ0.74) is used to pick a single most‚Äësimilar class. The rationale for selecting only one class, rather than applying a similarity threshold or considering multiple candidates, is not discussed. Exploring a more robust selection strategy could strengthen the method.\n\n3. **Stopping condition in Algorithm‚ÄØ1.**  \n   The phrase ‚Äúwhile‚ÄØŒ®‚ÄØis not empty and significant change in validation accuracy‚Äù is ambiguous. It would be helpful to define what constitutes a ‚Äúsignificant change‚Äù (e.g., a specific Œîaccuracy) and to report the number of iterations actually performed.\n\n4. **Knowledge‚Äëbased SVM (¬ß‚ÄØ3.4.2).**  \n   The SVM is trained on SMOTE‚Äëaugmented SOZ features, yet the manuscript does not present the SVM‚Äôs standalone performance nor the effect of the synthetic data. Including these results would illuminate the contribution of the knowledge module.\n\n5. **Presentation of Table‚ÄØ1.**  \n   The table mixes Train‚ÄØA/Test‚ÄØB and Train‚ÄØB/Test‚ÄØA results while reporting a single ‚ÄúAverage‚ÄØF1‚Äëscore‚Äù. It is unclear whether this average is computed per centre or across pooled data, which makes it difficult to assess cross‚Äëcentre variability.\n\n6. **Variability reporting.**  \n   Standard deviations accompany the aggregated‚Äëtrial results in Table‚ÄØ2, but are absent from the across‚Äëtrial results in Table‚ÄØ1. Adding dispersion measures for Table‚ÄØ1 would allow readers to gauge the stability of the reported metrics.\n\n7. **Age‚Äëshift experiment.**  \n   The manuscript mentions training on pediatric data and testing on adult data, yet no quantitative figures, confidence intervals, or statistical analyses are provided. Supplying these details would make the observation more compelling.\n\n8. **Aggregated‚Äëtrial validation (¬ß‚ÄØ4.2).**  \n   Combining data from both centres deviates from the strict SDG premise of training on a single source. A brief discussion of how this hybrid setting influences the interpretation of the observed improvements would be valuable.\n\n9. **Baseline comparisons.**  \n   The study does not benchmark against recent SDG approaches designed for imbalanced medical data, such as the contrastive SDG method of Hu‚ÄØet‚ÄØal. (2023) or entropy‚Äëbased sampling strategies (Li‚ÄØet‚ÄØal.,‚ÄØ2019). Including such baselines would situate the contribution more clearly within the current literature.\n\n10. **Minor issues.**  \n    A number of typographical and formatting problems (broken citations, misplaced symbols) detract from readability. A careful proof‚Äëread would improve the overall presentation.\n\n---\n\n**## Suggested decision**\n\n**Reject** ‚Äì While the topic is important and the proposed integration is promising, the manuscript currently falls short of the standards required for publication. Addressing the concerns above‚Äîparticularly those related to methodological transparency, statistical validation, and comprehensive baselines‚Äîwill be essential before the work can be considered for acceptance.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates single‚Äëdomain generalization for detecting rare events in medical imaging, using resting‚Äëstate fMRI data from two centres with three classes: noise, normal resting‚Äëstate networks, and seizure‚Äëonset‚Äëzone (SOZ). The proposed RareSaGe pipeline integrates a pretrained vision model for class‚Äëagnostic representation learning, a deep‚Äëlearning classifier for overlap detection, and a knowledge‚Äëbased quadratic‚Äëoptimization module guided by expert anatomical rules. Reported F1 scores exceed‚ÄØ85‚ÄØ% overall and about‚ÄØ90‚ÄØ% for the SOZ class. The paper aims to combine data‚Äëdriven and expert‚Äëdriven reasoning when only limited, imbalanced data are available. The manuscript is generally clear and the problem clinically relevant.  \n\n**Major Comments**  \n1. **Significance and Novelty** ‚Äì The topic of rare‚Äëevent recognition under single‚Äëdomain data scarcity is meaningful. The combination of CLIP‚Äëbased embedding analysis, entropy‚Äëguided classifier selection, and explicit expert rules is novel in this context.  \n2. **Evaluation Design** ‚Äì Although the experimental comparison is thorough, additional analyses such as statistical significance tests, ablations of individual modules, and validation on a third independent site are needed to confirm robustness.  \n3. **Reproducibility** ‚Äì The manuscript lacks full implementation specifics. Details of the CNN architecture, training schedule, SMOTE settings, the threshold‚ÄØ*t_c*, and random seeds should be included to enable replication.  \n4. **Parameter Definition (¬ß‚ÄØ3.1).** Key variables (number of neighbours‚ÄØ*K*, entropy‚ÄØŒ∏‚ÄØvalues, and œÉŒ∏‚ÄØthreshold) used for identifying rare classes are not specified.  \n5. **Overlap‚ÄëClass Selection (¬ß‚ÄØ3.2).** Justification for selecting a single most‚Äësimilar class rather than multiple candidates or a threshold‚Äëbased rule is missing.  \n6. **Algorithm Stopping Criterion.** The meaning of ‚Äúsignificant change in validation accuracy‚Äù is unclear and should be quantitatively defined, along with the number of iterations.  \n7. **Knowledge‚ÄëBased SVM (¬ß‚ÄØ3.4.2).** The contribution of SMOTE‚Äëaugmented training and standalone SVM performance should be documented.  \n8. **Result Presentation.** Clarify averaging in Table‚ÄØ1 (per‚Äëcentre vs pooled) and report standard deviations for all results to assess variability.  \n9. **Generalization Experiments.** The pediatric‚Äëto‚Äëadult analysis lacks quantitative details, and the aggregated‚Äëtrial validation partially departs from the SDG assumption; both aspects warrant discussion.  \n10. **Baseline Coverage.** Comparisons with more recent SDG or imbalance‚Äëhandling approaches would better contextualize the contribution.  \n\n**Minor Comments**  \n‚Äì Correct typographical and formatting inconsistencies (broken citations, misplaced symbols).  \n‚Äì Ensure all notation and parameter definitions are self‚Äëcontained in the manuscript.  \n\n**Summary Paragraph**  \nOverall, the study proposes an interesting integration of pretrained visual features and expert knowledge to address rare‚Äëevent detection when domain diversity is limited. The concept is relevant and implementation creative, but the current manuscript requires clearer methodological specification, statistical and comparative validation, and more thorough reporting of experimental variability. Without these, the evidence for strong generalization remains incomplete despite encouraging reported performance.  \n\n**Decision Recommendation**  \n**Reject** ‚Äì The contribution is promising but lacks sufficient methodological transparency and empirical validation to meet publication standards at this stage.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Ayan Banerjee",
      "BIN XU",
      "Payal Kamboj",
      "Sandeep Gupta"
    ],
    "url": "pdfs/iclr.cc-2025-conference_6b804922fe464f4b49ffe111b1d8d47e1d6592be.pdf",
    "remote_url": "https://openreview.net/pdf/6b804922fe464f4b49ffe111b1d8d47e1d6592be.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Training Physics-Driven Deep Learning Reconstruction without Raw Data Access for Equitable Fast MRI",
    "status": "not_started",
    "evaluators": [
      "Justin",
      "Tolga"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "Computational Imaging",
      "Fast MRI",
      "Unsupervised Learning",
      "Compressed Sensing",
      "Deep Learning",
      "Equity"
    ],
    "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Even though PD-DL offers higher acceleration rates compared to existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. One impediment for their deployment is the difficulties with generalization to pathologies or population groups that are not well-represented in training sets. This has been noted in several studies, and fine-tuning on target populations to improve reconstruction has been suggested. However, current training approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and underserved areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose CUPID for high-quality PD-DL training, using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates the goodness of the output with a compressibility-based approach, while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show that CUPID achieves similar quality compared to well-established PD-DL training strategies that require raw k-space data access, while outperforming conventional compressed sensing (CS) and state-of-the-art generative methods. We also demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden. As an approach that radically deviates from existing strategies, CUPID presents an opportunity to provide equitable access to fast MRI for underserved populations in an attempt to reduce the inequalities associated with this expensive imaging modality.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe article proposes a deep-learning reconstruction method without having access to multi-coil $k$-space data. For the full-column-rank forward operator $E_{\\Omega}$ and the under-sampled $k$-space $\\textbf{y}$, the authors assume access to $E_{\\Omega}^+ \\textbf{y}$ where $+$ denotes the Moore-Penrose inverse. Compared to having access to fully sampled $k$-space as in fully supervised reconstruction, or undersampled $k$-space as in self-supervised reconstruction, this assumption is less restrictive. The proposed method was shown to perform on par with PD-DL methods requiring $k$-space data at relatively low acceleration rates ($4\\times$).\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n1. The data access assumption is less restrictive compared to fully- and self-supervised reconstruction methods, and the article focuses on a real-life problem, as raw k-space access is a known limitation in this field.\n2. The method is physics-based; thus, it is expected to be less prone to hallucinations compared to methods that do not utilize the forward operator.\n3. The method was tested in various settings, focusing on both retrospective and prospective undersampling.\n\n### Weaknesses\n\n1. The quantitative results table is missing the standard error of the mean, which is crucial for a fair comparison.\n2. It is incorrect to say that equispaced undersampling patterns were not explored in score-based models. In fact, the authors have already cited at least one article (\"Robust Compressed Sensing MRI with Deep Generative Priors\") that explored equispaced undersampling, discussing it not only in the appendix but also in the main text (e.g., Fig. 2, 9, 10). \n3. The generative method baseline is weak and outdated, given how rapidly state-of-the-art methods are advancing.  Several approaches published in the last 12 months can achieve better results. Including this particular method is fine, but labeling it as state-of-the-art is misleading.\n\n### Questions\n\n1. Does the clinical PI reconstruction readily allow saving the complex-valued image without any research agreements, or does it only allow the magnitude image to be saved? I am asking because it is rare to see a DICOM dataset containing anything other than magnitude images.\n2. Regardless of whether ScoreMRI is state-of-the-art, its performance appears unexpectedly low. Is it possible that it was significantly under-trained? In our experience, these models require much longer training times compared to unrolled networks, which might explain why it underperforms even at $4\\times$.\n3. The choice of perturbations seems somewhat arbitrary. Could you elaborate on this choice? Learning the perturbations simultaneously with reinforcement learning could also be a potential direction for future work.\n4. In Figure $2$, loss function, the left hand side was written as $\\mathcal{L}(\\mathbf{x}^{(k)}, \\mathbf{x_{PI}})$ but the $\\mathcal{L}_{\\text{comp}}$ on the right hand side has $\\mathbf{x}^{(m)}$ rather than $\\mathbf{x}^{(k)}$. Is this intentional or a typo? \n5. Line $151-152$, while unrolled methods achieved top positions in reconstruction challenges three years ago, it is unknown if they remain the highest performers. To the best of my knowledge, no recent, fair comparison between state-of-the-art diffusion and unrolled methods has been published, including a reader study to compare their clinical effectiveness.\n\n**Minor comments** \n\n* The fastMRI knee matrix size is $320\\times 320$, not $320\\times 368$. Please see Table 1 in DOI: 10.1148/ryai.2020190007\n* Line $42-43$, demand have shown -> demand has shown\n* Line $269$, shapes that has different intensity values -> shapes that have different intensity values\n* Line $46$, Fig.$1$ -> Fig. $1$, the space was omitted in most, if not all, references to figures, tables, and sections.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a deep learning‚Äìbased MRI reconstruction approach that avoids reliance on multi-coil \\(k\\)-space data. Assuming access to \\(E_{\\Omega}^+ \\mathbf{y}\\) (the Moore‚ÄìPenrose pseudoinverse of the under-sampled data), the method aims to relax data-access constraints typical of fully or self-supervised reconstruction frameworks. The paper demonstrates reconstruction quality comparable to physics-driven deep-learning (PD-DL) methods that require full \\(k\\)-space data, particularly at moderate acceleration rates (\\(4\\times\\)). The study addresses a practical limitation in MRI research data availability, and overall presentation and methodological clarity are satisfactory.  \n\n**Major Comments**  \n1. **Statistical reporting:** Quantitative results lack standard errors of the mean, which limits reliable comparison between methods.  \n2. **Prior work accuracy:** The claim that equispaced undersampling has not been studied in score-based models is incorrect. The cited paper *‚ÄúRobust Compressed Sensing MRI with Deep Generative Priors‚Äù* included such patterns in both its main text and appendix.  \n3. **Baseline selection:** The generative model baseline used is outdated relative to current progress in the field. While its inclusion is reasonable, labeling it as state-of-the-art is misleading given more recent competitive methods.  \n4. **Experimental details and implementation:** Clarification is needed regarding: (a) whether the clinical PI reconstruction allows saving complex-valued outputs, (b) potential undertraining of the ScoreMRI baseline given its low performance, (c) the rationale for the specific perturbation choices and possible extension to reinforcement learning, (d) a possible inconsistency in Figure‚ÄØ2‚Äôs loss function notation (\\(\\mathbf{x}^{(k)}\\) vs. \\(\\mathbf{x}^{(m)}\\)), and (e) the current relevance of unrolled methods versus diffusion models given the absence of recent head-to-head comparisons.  \n\n**Minor Comments**  \n- Correct the fastMRI knee matrix size to \\(320 \\times 320\\).  \n- Fix grammatical and typographical issues: ‚Äúdemand have shown‚Äù ‚Üí ‚Äúdemand has shown‚Äù; ‚Äúshapes that has‚Äù ‚Üí ‚Äúshapes that have.‚Äù  \n- Insert missing spaces in figure/table/section citations (e.g., ‚ÄúFig.‚ÄØ1‚Äù formatting).  \n\n**Summary Paragraph**  \nThe paper tackles a relevant and impactful problem‚Äîdeveloping a physics-informed reconstruction method that reduces data-access requirements while maintaining competitive performance. Strengths include a realistic problem formulation, physics-based design expected to reduce hallucinations, and a broad experimental scope. Weaknesses primarily involve incomplete quantitative reporting, an outdated baseline choice, and several presentation inconsistencies. Addressing these would substantially strengthen clarity and credibility.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper present an novel MRI reconstruction method that only requires dicom data (even undersampled data), with the motivation that rural regions won't have the raw data access. The key idea is through using random perturbations.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\n1. I like the idea of using random image domain perturbations, sounds like a good way to capture pior distribution.\n2. I appreciate the authors for the experiments on the prospective undersampled MRI reconstruction, which is a very important experiment for all MRI reconstruction tasks.\n\n### Weaknesses\n\nOverall, there are a few weaknesses that concerns me:\n1. I didn't really buy the idea of this rural region raw-data access. why we need raw-data? its for the training purpose! we already have a great number of raw k-space data (from fastMRI, mridata.org and others), as long as we have a good model trained, the rural regions can just use it without any training. In the end of the day, all MRI scanner will sample raw k-space data, so the whole motivation doesn't sound solid to me. \nMeanwhile, from what i understand from your method, the input is CG-SENSE of undersampled MRI, is it realistic? the input quality is not really diagnoistic.\n\n2. The presentation is not very clear and the paper is not well-writtened, all the method section for your method is condensed to one *single* page with a figure (figure 2), i have to go back and forth to understand whats R, whats DF, and how you do the perturbations, thats supposed to be the main part of the paper.\n\n3. The score-MRI results look kinda weird to me, try to look at other generative models. It shouldn't be that bad.\n\n4. Figure 3, the result doesn't look good to me, over smoothing textures.\n\n### Questions\n\n1. how do your method compared to single-coil DL-based reconstruction.\n2. how sensitive is your method to different input? (different vendors tend to have different reconstruction pipelines.)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a novel MRI reconstruction method that operates directly on DICOM data, including undersampled cases, motivated by scenarios in which raw k-space data are unavailable, such as in rural clinical environments. The method is based on introducing random image-domain perturbations to model data variability. While the concept is interesting and experimentally supported, the manuscript‚Äôs overall clarity and presentation are limited, which hinders full understanding of the approach and its contributions.\n\n**Major Comments**  \n1. **Motivation and Practicality** ‚Äì The core motivation regarding lack of raw-data access in rural areas is not convincing. Raw k-space data are primarily needed for training, and existing large public datasets (e.g., fastMRI, mridata.org) already enable model development. Once a model is pre-trained, it can be deployed without requiring local raw data. Moreover, all MRI scanners inherently collect raw k-space data, making the stated premise less compelling. Additionally, the assumed input (CG-SENSE reconstructions of undersampled data) may not correspond to realistic diagnostic-quality inputs.  \n2. **Method Presentation and Clarity** ‚Äì The methods section is overly condensed (approximately one page plus Figure 2), making it difficult to follow key components such as the definitions of R, DF, and the perturbation procedure. The limited explanation forces the reader to repeatedly review earlier sections to piece together the approach.  \n3. **Quantitative and Qualitative Results** ‚Äì The results labeled as ‚Äúscore-MRI‚Äù appear unexpectedly poor compared to those from other generative models. Similarly, Figure 3 demonstrates excessive smoothing and texture loss, which raises doubts about reconstruction fidelity.  \n4. **Comparative Evaluation** ‚Äì It remains unclear how the proposed method performs relative to single-coil deep learning‚Äìbased reconstructions. Further comparison and sensitivity analysis across different input types and scanner vendors would strengthen the evaluation.\n\n**Minor Comments**  \n- The overall writing and organization could be improved for readability and consistency.  \n- Clarify all abbreviations and notation within the figures and text.  \n- Ensure figures are of diagnostic quality and sufficient resolution.\n\n**Summary Paragraph**  \nThe proposed method demonstrates an interesting idea using random perturbations for MRI reconstruction from DICOM data, and the inclusion of prospective undersampling experiments is appreciated. However, the unclear motivation, limited methodological exposition, and suboptimal experimental results substantially weaken the manuscript. Addressing clarity issues and expanding experimental comparisons would be critical for improving its impact and credibility.\n\n**Decision Recommendation**  \n**Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis manuscript proposed an unsupervised approach to improve MR DICOM image quality by introducing the loss term $l_{comp}+\\lambda l_{pif}$, where $l_{comp}$ is to suppress noise and $l_{pif}$ is to keep fidelity to the input.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThis manuscript is well-written, and the figures are well-drafted.\n\n### Weaknesses\n\n1. The motivation is weak. Starting reconstruction from DICOM output rather than raw k-space is unusual for MRI practitioners, as it does not allow us to construct the encoding operator, $\\mathrm{E}$.\n\n2. The title is misleading: claiming a \"physics-driven\" approach while dismissing the need for raw k-space data is contradictory. In MRI, k-space data represents the underlying physical imaging process.\n\n3. The reported results for the baseline method, scoreMRI, are questionable, with performance significantly below that in the original publication.\n\n4. The novelty and contribution of this work are limited. The author attempted to improve the image quality with the proposed loss term $l_{comp}+\\lambda l_{pif}$, where $l_{comp}$ is to suppress noise and $l_{pif}$ is to keep fidelity to the input. In my eyes, this is a simple combination of existing approaches for MRI image quality enhancement.\n\n### Questions\n\n1. Refer to Weakness 1, how the coil sensitivity and sampling pattern are handled in the $\\mathrm{E}$?\n\n2. Refer to Weakness 2, how to construct the $\\mathrm{E}$ for scoreMRI?\n\n3. The proposed method resembles SSDU, yet its performance is reportedly better to CUPID. Could you clarify the reason for this?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes an unsupervised approach to enhance MR DICOM image quality through introducing a composite loss term, \\( l_{comp} + \\lambda l_{pif} \\), where \\( l_{comp} \\) suppresses noise and \\( l_{pif} \\) maintains fidelity to the input. The paper is clearly written and supported by well-prepared figures. However, the overall scientific motivation and methodological positioning within MRI reconstruction standards appear limited, and the claimed contributions are questioned in terms of novelty and physical justification.  \n\n**Major Comments**  \n1. **Motivation and Methodological Foundation** ‚Äì The methodological choice to start reconstruction from DICOM output rather than from raw k-space data is poorly justified. In standard MRI practice, this approach excludes the use of the encoding operator \\( \\mathrm{E} \\), which is central to modeling the physical imaging process.  \n2. **Title and Claim of Being ‚ÄúPhysics-Driven‚Äù** ‚Äì The title and framing are potentially misleading. Describing the approach as ‚Äúphysics-driven‚Äù conflicts with the dismissal of raw k-space data, which fundamentally represents the physical acquisition process.  \n3. **Baseline Comparison Validity** ‚Äì The reported baseline results for scoreMRI are inconsistent with those published in the original source, making the comparisons and resulting conclusions difficult to trust.  \n4. **Novelty and Contribution** ‚Äì The methodological innovation appears limited. The proposed loss \\( l_{comp} + \\lambda l_{pif} \\) represents a straightforward combination of existing noise-suppression and fidelity-maintenance terms commonly applied in MRI image enhancement tasks.  \n\n**Minor Comments**  \n- Clarify how coil sensitivities and sampling patterns are managed within the operator \\( \\mathrm{E} \\).  \n- Specify how \\( \\mathrm{E} \\) is constructed for the scoreMRI baseline.  \n- The proposed method resembles SSDU yet performs reportedly better than CUPID; an explanation for this discrepancy would strengthen the paper.  \n\n**Summary Paragraph**  \nThe manuscript is well-presented and technically clear but raises substantial doubts concerning motivation, physical grounding, and originality. Comparative validity and conceptual consistency require significant elaboration and correction before the contribution can be appropriately assessed within MRI reconstruction research.  \n\n**Decision Recommendation**: Major Revision",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äì Analytical‚ÄØ&‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI**  \n‚úÖ‚ÄØThe manuscript proposes a new learning framework for physics‚Äëdriven deep‚Äëlearning MRI reconstruction that eliminates the need for raw‚ÄØk‚Äëspace data.‚ÄØThis is a clear methodological advance in image‚Äêreconstruction theory and thus lies squarely within the technical scope of‚ÄØIEEE‚ÄØTMI.\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel**  \nThe work introduces *CUPID‚ÄØ(Compressibility‚Äëinspired‚ÄØUnsupervised‚ÄØLearning‚ÄØvia‚ÄØParallel‚ÄØImaging‚ÄØFidelity)*‚Äîan unsupervised, zero‚Äëshot training loss for PD‚ÄëDL networks requiring only routine DICOM images.  Unlike prior self‚Äësupervised or generative schemes, it removes dependency on raw‚ÄØdata by combining a re‚Äëweighted‚ÄØ‚Ñì‚ÇÅ compressibility term with a novel ‚Äúparallel‚Äëimaging‚Äëfidelity‚Äù constraint based on synthetic perturbations.  This represents a substantive conceptual innovation.‚ÄØNovelty and potential impact are high.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \nTheory and equations are consistent, the relationship to existing PD‚ÄëDL formulations is well grounded, and the method is evaluated on both retrospective (fastMRI‚ÄØdatasets) and prospective‚ÄØ7‚ÄØT‚ÄØacquisitions.‚ÄØComparisons against multiple baselines, ablation on Œª‚ÄØweight and perturbation number are provided.‚ÄØHowever, two aspects merit clarification:  \n- Details of perturbation generation and statistical reproducibility (randomness, parameter sensitivity).  \n- Statistical significance of quantitative metrics and error bars across subjects.\n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation**  \nOverall readability and organization are strong.‚ÄØFigures are informative, though crowded; certain visual panels (e.g., Figs.‚ÄØ4‚Äì5) could benefit from consistent scale bars and annotation fonts.‚ÄØMinor language polishing would further improve flow.\n\n**5.‚ÄØEthical‚ÄØ&‚ÄØReproducibility‚ÄØCompliance**  \nThe text indicates datasets from fastMRI‚ÄØ(public) and an institutional 7‚ÄØT‚ÄØstudy with subject anonymization; code release is planned.‚ÄØThese satisfy typical TMI reproducibility standards, but explicit mention of IRB‚ÄØapproval for the in‚Äëhouse study would be advisable.\n\n---\n\n**Phase‚ÄØ2 ‚Äì‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n###‚ÄØ1.‚ÄØSummary  \nThe paper introduces‚ÄØ*CUPID*, a physics‚Äëdriven unsupervised training strategy for deep‚Äëlearning MRI reconstruction that operates without access to raw‚ÄØk‚Äëspace.‚ÄØThe approach combines a compressibility‚Äëbased regularization with a parallel‚Äëimaging‚Äëfidelity loss implemented through designed perturbations, enabling network training directly from scanner‚Äëexported DICOMs.‚ÄØExperiments on knee and brain datasets show performance comparable to supervised or self‚Äësupervised PD‚ÄëDL methods that require raw‚ÄØdata, and superior to compressed‚Äësensing and generative‚Äëprior baselines.‚ÄØZero‚Äëshot adaptation on prospectively undersampled‚ÄØ7‚ÄØT‚ÄØdata further demonstrates practicality for deployment in resource‚Äëlimited settings.\n\n###‚ÄØ2.‚ÄØStrengths\n- **High novelty:** first demonstration of PD‚ÄëDL training using only image‚Äëdomain data.  \n- **Strong methodological foundation:** well‚Äëformulated loss combining compressibility and physics consistency.  \n- **Comprehensive evaluation:** multi‚Äëdataset, retrospective‚ÄØ+‚ÄØprospective tests, and thoughtful ablation studies.  \n- **Potential clinical impact:** removes data‚Äësharing and vendor‚Äëaccess barriers, promoting equitable dissemination of advanced MRI‚ÄØreconstructions.\n\n###‚ÄØ3.‚ÄØWeaknesses\n- Perturbation design, randomness control, and computational overhead not fully quantified.  \n- Limited statistical analysis of performance variation across subjects or scanners.  \n- Minor clarity issues in figures and notation.  \n- No explicit statement of IRB‚ÄØor consent procedures for prospective experiments.\n\n###‚ÄØ4.‚ÄØMajor‚ÄØComments\n1.‚ÄØ**Clarify perturbation strategy** ‚Äì‚ÄØProvide algorithmic detail or pseudocode defining amplitude, support, and sampling of the perturbation‚ÄØp‚ÄØsets; discuss sensitivity of results to these design choices and potential domain dependence.  \n2.‚ÄØ**Quantify uncertainty / statistics** ‚Äì‚ÄØInclude standard deviation or confidence intervals for PSNR/SSIM across test subjects and report statistical significance versus baselines.  \n3.‚ÄØ**Resource and runtime analysis** ‚Äì‚ÄØContrast training time and compute requirements of CUPID versus SSDU or supervised PD‚ÄëDL to substantiate claims of minimal fine‚Äëtuning burden.  \n4.‚ÄØ**Ethics and data use** ‚Äì‚ÄØState whether prospective data collection was IRB‚Äëapproved and whether subjects provided informed consent.‚ÄØAlso specify anonymization and data‚Äësharing plans.  \n5.‚ÄØ**Clarify limitations** ‚Äì‚ÄØExpand on conditions where only magnitude DICOMs are available and outline realistic strategies for extending CUPID to such cases.\n\n###‚ÄØ5.‚ÄØMinor‚ÄØComments\n- Line‚ÄØ53: ‚ÄúPHYSICS‚ÄØ‚Äì‚ÄØDRIVEN‚Äù ‚Äì‚ÄØremove space and hyphen for consistency.  \n- Verify consistent notation between‚ÄØx_PI‚ÄØand‚ÄØxPI.  \n- Figures‚ÄØ3‚Äì5: add scale bars, identical windowing, and mark zoom locations.  \n- References: harmonize citation style (author‚ÄØet‚ÄØal.,‚ÄØyear) across text.  \n- Typographical edits for hyphenation and spacing throughout.\n\n###‚ÄØ6.‚ÄØOverall‚ÄØRecommendation  \n**Major‚ÄØRevision.**  \nThe paper presents a clearly novel and methodologically significant contribution appropriate for‚ÄØTMI.‚ÄØBefore acceptance, additional methodological clarity, statistical analysis, and minor presentation/ethical details are needed to meet the journal‚Äôs standard of rigor and reproducibility.\n\n###‚ÄØ7.‚ÄØConfidence‚ÄØLevel  \n**5‚ÄØ/‚ÄØ5**‚ÄØ‚Äì‚ÄØHigh confidence in assessment based on familiarity with PD‚ÄëDL‚ÄØMRI reconstruction literature.\n\n---\n\n**Summary‚ÄØJudgment:**  \n*CUPID* offers a genuinely new direction for unsupervised PD‚ÄëDL reconstruction without raw‚ÄØdata access.‚ÄØWith additional methodological transparency and expanded quantitative validation, the work has strong potential for publication in‚ÄØIEEE‚ÄØTransactions‚ÄØon‚ÄØMedical‚ÄØImaging.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *CUPID* (Compressibility‚Äëinspired‚ÄØUnsupervised‚ÄØLearning‚ÄØvia‚ÄØParallel‚ÄØImaging‚ÄØFidelity), a novel physics‚Äëdriven deep‚Äëlearning framework for MRI reconstruction that eliminates reliance on raw‚ÄØk‚Äëspace data. The method introduces an unsupervised, zero‚Äëshot loss formulation combining a compressibility term with a parallel‚Äëimaging‚Äëfidelity constraint generated through synthetic perturbations, allowing network training directly from DICOM images. The manuscript is clearly written, well organized, and methodologically rigorous, with both retrospective and prospective validations.\n\n---\n\n**Major Comments**  \n1. **Clarification of perturbation mechanism:** Provide algorithmic details or pseudocode describing generation of the perturbation sets, including their amplitude, support, sampling, and how randomness is controlled. Explain the sensitivity of the results to these choices and their possible dependence on data domain.  \n2. **Statistical rigor and uncertainty:** Report standard deviations, confidence intervals, or other measures of variability for quantitative metrics (e.g., PSNR/SSIM). Indicate whether performance differences versus baselines are statistically significant.  \n3. **Computational resources:** Include a comparison of training time and computational cost between CUPID and existing supervised or self‚Äësupervised PD‚ÄëDL approaches to substantiate claims of reduced fine‚Äëtuning overhead.  \n4. **Ethical and data‚Äëuse transparency:** Explicitly state whether the prospective acquisitions were approved by an institutional review board and whether participants provided informed consent. Clarify anonymization and data‚Äësharing procedures.  \n5. **Limitations and practical scope:** Discuss how the approach would handle cases where only magnitude DICOM data are available, and outline possible extensions to address such limitations.\n\n---\n\n**Minor Comments**  \n- Ensure consistent typography (e.g., remove extraneous hyphenation in ‚ÄúPHYSICS‚ÄëDRIVEN‚Äù).  \n- Verify uniform notation between *x_PI* and *xPI*.  \n- Improve figure readability: add consistent scale bars, identical windowing, and clearly marked zoom regions.  \n- Harmonize reference formatting throughout.  \n- Perform minor language and spacing corrections for smoother readability.\n\n---\n\n**Summary Paragraph**  \nOverall, the work demonstrates strong novelty as the first to enable physics‚Äëdriven unsupervised MRI reconstruction from image‚Äëdomain data alone. The theoretical formulation and experimental validation are sound and cover multiple datasets and practical scenarios. Nonetheless, further clarification on perturbation design, statistical analysis, computational cost, and ethical documentation is needed to reach the expected level of methodological transparency and reproducibility. With these revisions, the study has the potential to make a significant impact in data‚Äëefficient deep‚Äëlearning reconstruction.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The contribution is noteworthy and potentially influential, but additional methodological detail, statistical evaluation, and minor presentation and ethical clarifications are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis work proposes CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), a novel training method for physics-driven deep learning (PD-DL) MRI reconstruction that operates without requiring raw k-space data access. The method addresses the limitation that current PD-DL approaches require raw MRI measurements, which are typically unavailable outside specialized research centers, particularly in rural and underserved areas. CUPID combines two loss terms: a compressibility-based term using reweighted ‚Ñì1 norm on wavelet coefficients (Equation 8) and a parallel imaging fidelity term that ensures consistency through carefully designed perturbations (Equation 9). The approach can operate in both database training and zero-shot subject-specific modes using only routine clinical DICOM images. Experimental validation on fastMRI knee and brain datasets with retrospective R=4 undersampling and prospective R=9 brain acquisition demonstrates performance comparable to supervised and self-supervised methods while outperforming compressed sensing and generative approaches.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - The parallel imaging fidelity loss in Equation 9 lacks theoretical justification for why the expectation over perturbations should approximate true parallel imaging consistency, as no convergence analysis is provided\n  - The notation switches inconsistently between f(yŒ©, EŒ©; Œ∏) in Section 2.2 and f(xPI, EŒ©) in Section 3 without clear explanation (Page 5, line 226-227)\n  - The reweighted ‚Ñì1 formulation in Equation 8 uses x(m) from \"mth reweighting step\" but the iterative reweighting procedure is not mathematically defined or described\n\n‚Ä¢ **Limited experimental validation scope**\n  - Only uniform/equidistant undersampling patterns are evaluated (Section 4.1, line 294-295), which represents a narrow subset of clinical acceleration strategies and limits generalizability assessment\n  - The prospective validation uses only a single high-field 7T acquisition at R=9 (Section 4.1, lines 309-311), providing insufficient evidence for clinical translation across different field strengths and acceleration rates\n  - Comparison with generative methods is limited to ScoreMRI only (Table 1), missing evaluation against other state-of-the-art diffusion-based reconstruction approaches mentioned in Section 2.3\n\n‚Ä¢ **Perturbation design lacks principled foundation**\n  - The choice of perturbations as \"randomly rotated and positioned letters, numbers, card suits\" (Page 5, lines 251-252) appears arbitrary without theoretical or empirical justification for why these specific shapes are optimal\n  - The fold-over constraint design is described only conceptually (Page 5, lines 249-250) without mathematical formulation or validation that perturbations truly satisfy parallel imaging resolvability\n  - The ablation study on perturbation number (Appendix B.1, Figure 7) shows only K‚àà{1,3,6,10} without exploring the convergence behavior or computational trade-offs systematically\n\n‚Ä¢ **Insufficient analysis of clinical deployment constraints**\n  - The discussion of filtering operations in Section 4.6 (lines 491-500) acknowledges limitations with vendor processing but provides no quantitative assessment of robustness to these common clinical modifications\n  - The magnitude-only DICOM limitation (Section 4.6, lines 507-518) is mentioned as future work but represents a significant practical barrier that undermines the claimed accessibility for underserved areas\n  - No computational cost analysis is provided for the training procedure, which is crucial for assessing feasibility in resource-limited settings as claimed in the motivation\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen mathematical rigor and notation consistency**\n  - Provide theoretical analysis demonstrating that the expectation over perturbations in Equation 9 converges to a meaningful parallel imaging consistency measure, including convergence rates and required number of samples\n  - Standardize notation throughout the manuscript, clearly defining when and why the function signature changes from f(yŒ©, EŒ©; Œ∏) to f(xPI, EŒ©) with explicit parameter handling\n  - Formalize the iterative reweighting procedure for Equation 8 with algorithmic description, convergence criteria, and relationship to standard reweighted ‚Ñì1 minimization\n\n‚Ä¢ **Expand experimental validation comprehensively**\n  - Include evaluation on multiple undersampling patterns (random, variable density, radial) across different acceleration rates (R=2,3,6,8) to demonstrate broader clinical applicability beyond uniform patterns\n  - Conduct prospective validation across multiple field strengths (1.5T, 3T, 7T) and anatomical regions with varying acceleration rates to establish generalization capabilities for clinical deployment\n  - Compare against additional state-of-the-art generative reconstruction methods including recent diffusion models and score-based approaches beyond ScoreMRI\n\n‚Ä¢ **Develop principled perturbation methodology**\n  - Establish theoretical criteria for perturbation design based on parallel imaging theory, providing mathematical conditions that ensure resolvability and optimal reconstruction performance\n  - Formalize the fold-over constraint mathematically with explicit conditions relating perturbation spatial frequency content to acceleration factor and coil sensitivity patterns\n  - Conduct systematic ablation studies exploring perturbation characteristics (shape complexity, intensity distribution, spatial frequency content) with convergence analysis for different K values\n\n‚Ä¢ **Address practical deployment considerations quantitatively**\n  - Evaluate robustness to common vendor filtering operations through controlled experiments with simulated and real clinical processing pipelines, providing quantitative performance degradation analysis\n  - Implement and validate solutions for magnitude-only DICOM reconstruction including joint phase and coil sensitivity estimation within the CUPID framework\n  - Provide detailed computational cost analysis comparing training time, memory requirements, and hardware specifications needed for CUPID versus existing methods in resource-constrained environments",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity)**, a training framework for physics-driven deep learning (PD-DL) in MRI reconstruction that eliminates the need for raw k-space data. The approach integrates two complementary loss components: a compressibility-based term using a reweighted ‚Ñì1 norm on wavelet coefficients and a parallel imaging fidelity term implemented through structured perturbations. CUPID operates in both large-scale training and zero-shot reconstruction modes using standard DICOM images and is experimentally evaluated on knee and brain datasets. Results demonstrate reconstruction quality comparable to supervised and self-supervised models and improvements over compressed sensing and generative baselines. The paper is ambitious and addresses an important accessibility challenge, but it exhibits issues related to mathematical formulation, experimental generalizability, and practical feasibility.  \n\n**Major Comments**  \n1. **Mathematical rigor and notation consistency** ‚Äì The theoretical justification of the parallel imaging fidelity loss (Equation 9) is lacking, with no convergence or expectation analysis. Notation changes between sections are inconsistent, and the iterative reweighting step used in Equation 8 is not formally defined.  \n2. **Limited experimental validation** ‚Äì The evaluation includes only uniform undersampling, restricting assessment of real-world generalizability. Prospective testing relies on a single 7T case at R=9, and comparisons with generative methods are limited to ScoreMRI, excluding newer diffusion-based methods.  \n3. **Unjustified perturbation design** ‚Äì The choice of perturbation objects (letters, numbers, card suits) lacks theoretical foundation, and the described fold-over constraint is not expressed mathematically. The perturbation ablation explores few values and omits analysis of convergence or computational implications.  \n4. **Clinical deployment and feasibility** ‚Äì The work acknowledges vendor filter processing and magnitude-only DICOM constraints but provides no quantitative robustness evaluation. The absence of computational cost analysis limits assessment of practicality in low-resource settings.  \n\n**Minor Comments**  \n- Clarify notation transitions (e.g., \\(f(y_\\Omega, E_\\Omega; Œ∏)\\) vs. \\(f(x_{PI}, E_\\Omega)\\)) in the methods section.  \n- Include precise algorithmic descriptions for reweighting steps and perturbation definitions.  \n- Ensure consistency in equation numbering and terminology throughout.  \n\n**Summary Paragraph**  \nOverall, the method presents a promising unsupervised framework for MRI reconstruction without raw data access, addressing a significant practical limitation. However, critical weaknesses in theoretical justification, experimental breadth, and implementation transparency hinder confidence in the results and claims of clinical applicability. Substantial clarifications, expanded validation, and stronger methodological grounding are needed for the work to reach publication standard.  \n\n**Decision Recommendation**  \n**Major Revision.**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID), a novel approach to train Physics-driven Deep Learning (PD-DL) models for MRI reconstruction using only routinely available clinical images exported from MRI scanners. Unlike existing PD-DL methods that require raw k-space data, CUPID leverages compressibility and parallel imaging fidelity to achieve high-quality reconstructions. The authors demonstrate that CUPID performs comparably to supervised PD-DL and self-supervised learning methods that rely on raw k-space data, while also outperforming conventional compressed sensing and generative methods. The manuscript is well-written and clearly articulates the motivations and technical innovations of CUPID.\n\n## Major Comments\n1. Novelty and Positioning: The introduction of CUPID as a method to train PD-DL models without raw k-space data is innovative and addresses a significant limitation of current PD-DL approaches. However, the manuscript should more explicitly position CUPID in relation to existing unsupervised and self-supervised learning methods that also aim to reduce the reliance on raw data. Additionally, the authors should clarify how CUPID's compressibility and parallel imaging fidelity losses differ from and improve upon previous methods.\n\n2. Evaluation Design: The evaluation of CUPID is conducted on both retrospective and prospective undersampled datasets, which is commendable. However, the retrospective study primarily uses equidistant undersampling patterns, which may not fully represent the variability seen in clinical practice. Including more diverse undersampling patterns (e.g., Poisson disk sampling) would strengthen the evaluation. Moreover, the prospective study should ideally include more subjects and a wider range of anatomies to further validate the method's generalizability.\n\n3. Comparisons: The manuscript compares CUPID with several established methods, including supervised PD-DL, self-supervised learning (SSDU), and generative models (ScoreMRI). However, the comparisons could be more extensive. For example, including other recent self-supervised methods that do not require raw data access (e.g., methods based on generative adversarial networks) would provide a more comprehensive assessment of CUPID's performance. Additionally, the authors should discuss why certain methods were excluded from the comparisons.\n\n4. Reproducibility: The authors state that the code will be released upon acceptance, which is positive. However, the manuscript lacks sufficient detail regarding the training protocols, preprocessing steps, and model hyperparameters. Providing a more detailed description of these aspects is crucial for reproducibility. Furthermore, the authors should include a section outlining the data preprocessing steps and any normalization techniques applied to the DICOM images before feeding them into the CUPID framework.\n\n## Minor Comments\n1. Figure Clarity: Figures 3 and 4 are somewhat cluttered. Reducing the number of representative slices and focusing on specific regions of interest (ROIs) would enhance readability and highlight the key differences between methods more effectively.\n\n2. Notation Consistency: The notation used for the forward operator (EŒ©) is inconsistent at times. Clarifying this in the text and ensuring consistency would aid readers' understanding.\n\n3. Acronym Definitions: Several acronyms (e.g., \"R=4\", \"CG-SENSE\") are used without definitions. Providing a glossary of acronyms or defining them upon first use would improve clarity.\n\n4. Typographical Errors: Minor typographical errors exist in the text (e.g., \"k-spacce\", \"undersampling maskes\"). Careful proofreading would eliminate these issues.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: training PD-DL models for MRI reconstruction without access to raw k-space data. The proposed CUPID method is technically innovative and shows promise in providing high-quality reconstructions using only clinical DICOM images. However, the evaluation could benefit from more diverse undersampling patterns and a broader range of anatomies. The reproducibility of the approach is also uncertain due to incomplete methodological details. Overall, while the idea has merit, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include a wider range of self-supervised methods, strengthen validation across diverse undersampling patterns and anatomies, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID)*, a novel Physics-driven Deep Learning (PD-DL) framework for MRI reconstruction that eliminates the need for raw k-space data. CUPID leverages compressibility and parallel imaging fidelity to train models solely from routinely available clinical DICOM images. The paper demonstrates that CUPID achieves reconstruction quality comparable to supervised PD-DL and self-supervised methods requiring raw data, while surpassing traditional compressed sensing and generative approaches. Overall, the manuscript is well organized and clearly explains the motivation and technical framework.  \n\n**Major Comments**  \n1. **Novelty and Positioning** ‚Äì CUPID‚Äôs approach of training PD-DL models without k-space data is innovative and addresses an important limitation in current PD-DL methods. However, the paper should position CUPID more explicitly relative to existing unsupervised and self-supervised methods with similar goals. Clarify the distinctions and improvements offered by CUPID‚Äôs compressibility and parallel imaging fidelity losses over prior formulations.  \n2. **Evaluation Design** ‚Äì The evaluation is carried out on both retrospective and prospective datasets, which is commendable. Yet, the retrospective experiments mainly use equidistant undersampling, limiting clinical representativeness. Incorporating other patterns such as Poisson disk sampling and expanding the prospective validation to more subjects and anatomies would enhance the robustness of results.  \n3. **Comparisons** ‚Äì The current comparisons include supervised PD-DL, self-supervised SSDU, and ScoreMRI. Additional comparisons with other self-supervised or generative methods that do not require raw data would provide a more comprehensive performance context. The rationale for method selection and exclusions should be discussed.  \n4. **Reproducibility** ‚Äì Although public code release is planned, critical implementation details such as training protocols, preprocessing steps, and hyperparameters are insufficiently described. These should be expanded, including explicit discussion of DICOM preprocessing and normalization procedures, to ensure reproducibility.  \n\n**Minor Comments**  \n1. Figures 3‚Äì4 are visually crowded; simplifying them to focus on key ROIs would improve interpretability.  \n2. Ensure consistent notation for the forward operator (*EŒ©*).  \n3. Define technical acronyms (e.g., R=4, CG-SENSE) upon first appearance or in a glossary.  \n4. Correct minor typographical errors (e.g., ‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù).  \n\n**Summary Paragraph**  \nCUPID addresses the important problem of PD-DL MRI reconstruction without raw data, offering a technically creative solution with strong potential impact. Nonetheless, limitations in the evaluation diversity, scope of comparisons, and methodological transparency reduce the current evidential strength. With a more comprehensive experimental design and clearer reporting, the work could make a significant contribution.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Broaden comparative analysis, strengthen evaluation diversity, and provide complete methodological details to support reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## TRAINING PHYSICS-DRIVEN DEEP LEARNING RECONSTRUCTION WITHOUT RAW DATA ACCESS FOR EQUITABLE FAST MRI\n\n### Summary\n\nThe paper proposes CUPID, a framework for training physics-driven deep learning (PD-DL) MRI reconstruction models using only routine clinical images (e.g., DICOM) without access to raw k-space. CUPID combines a compressibility-inspired, reweighted ‚Ñì1 loss with a novel ‚Äúparallel imaging fidelity‚Äù term enforced via carefully designed perturbations that are resolvable by parallel imaging, enabling both database training and zero-shot/subject-specific fine-tuning. Experiments on fastMRI knee/brain (retrospective R=4, equidistant) and a prospective 7T brain GRE acquisition (R=9, kx-only) show CUPID approaches supervised/self-supervised PD-DL performance and outperforms CS, CG-SENSE, and a diffusion baseline, while requiring no raw measurements during training.\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a training paradigm for PD-DL reconstruction that operates purely from clinical reconstructed images, removing the long-standing requirement for raw k-space access in training.Uses a clever fidelity term based on perturbations designed to be resolvable by parallel imaging, stabilizing the compressibility-driven objective and preventing trivial solutions without raw-data fidelity.The reweighted-‚Ñì1 compressibility term is well-motivated (closer to ‚Ñì0) and appropriately combined with a physics-informed fidelity, bridging compressibility-inspired unsupervised learning and PD-DL.\n- Experimental rigor and validationEvaluations span both database training and zero-shot/subject-specific settings, including a prospective accelerated 7T setup that closely mirrors the intended low-resource workflow.Ablations study the sensitivity to the trade-off Œª and the number of perturbations K, offering insight into stability and practical hyperparameter ranges.Comparisons include supervised and self-supervised PD-DL baselines (with raw access) as well as CS, CG-SENSE, and a diffusion-model method that are raw-free at training.\n- Clarity of presentationThe core idea, loss formulation, and training pipeline are clearly depicted, with a helpful schematic explaining the role of perturbations and reweighting.The motivation regarding equitable access and the practical impediments of raw data is well articulated, with a cohesive narrative linking problem, method, and evaluation.\n- Significance of contributionsAddresses a critical translational barrier: the inability of non-academic sites to train/fine-tune PD-DL due to raw-data restrictions, potentially broadening access to high-quality accelerated MRI.Offers a pathway for site-specific adaptation (zero-shot fine-tuning) that could mitigate distribution shift issues and improve generalization to local populations.\n\n- Introduces a training paradigm for PD-DL reconstruction that operates purely from clinical reconstructed images, removing the long-standing requirement for raw k-space access in training.\n- Uses a clever fidelity term based on perturbations designed to be resolvable by parallel imaging, stabilizing the compressibility-driven objective and preventing trivial solutions without raw-data fidelity.\n- The reweighted-‚Ñì1 compressibility term is well-motivated (closer to ‚Ñì0) and appropriately combined with a physics-informed fidelity, bridging compressibility-inspired unsupervised learning and PD-DL.\n\n- Evaluations span both database training and zero-shot/subject-specific settings, including a prospective accelerated 7T setup that closely mirrors the intended low-resource workflow.\n- Ablations study the sensitivity to the trade-off Œª and the number of perturbations K, offering insight into stability and practical hyperparameter ranges.\n- Comparisons include supervised and self-supervised PD-DL baselines (with raw access) as well as CS, CG-SENSE, and a diffusion-model method that are raw-free at training.\n\n- The core idea, loss formulation, and training pipeline are clearly depicted, with a helpful schematic explaining the role of perturbations and reweighting.\n- The motivation regarding equitable access and the practical impediments of raw data is well articulated, with a cohesive narrative linking problem, method, and evaluation.\n\n- Addresses a critical translational barrier: the inability of non-academic sites to train/fine-tune PD-DL due to raw-data restrictions, potentially broadening access to high-quality accelerated MRI.\n- Offers a pathway for site-specific adaptation (zero-shot fine-tuning) that could mitigate distribution shift issues and improve generalization to local populations.\n\n### Weaknesses\n\n- Technical limitations or concernsThe method still requires accurate knowledge of the encoding operator EŒ© (including coil sensitivities) at train/test time. In real DICOM-only scenarios, reliable access to coil maps is nontrivial and vendor-dependent; this practical bottleneck is only partially addressed.The fidelity term relies on hand-designed image-domain perturbations whose amplitude, frequency content, and spatial placement could affect convergence, stability, and potential bias; design guidance is limited.The approach assumes xPI ‚âà (EŒ©^H EŒ©)^{-1} EŒ©^H yŒ©; vendor pipelines often apply filters, partial Fourier, coil compression, or proprietary post-processing that may violate this equivalence.\n- Experimental gaps or methodological issuesRetrospective experiments use fastMRI-derived EŒ© and simulate xPI via CG-SENSE; this does not fully represent the DICOM-only constraint (no demonstration of coil map estimation from only DICOM exports or separate vendor calibration DICOMs in the retrospective setting).Prospective results are qualitative only; no reader study or quantitative surrogate (e.g., repeated acquisitions, central k-space ‚Äúreference,‚Äù or task-based evaluation) to substantiate claims at R=9.Limited sampling patterns (mostly 1D equidistant at R=4 and a single prospective kx-only R=9); generalization to random sampling, 2D sampling, non-Cartesian patterns, and higher R values is not assessed.Baseline tuning for diffusion/score-based methods appears limited (e.g., only ScoreMRI with hard DC), potentially underestimating performance relative to more recent MRI-tailored or LDM-based samplers.\n- Clarity or presentation issuesDetails for deriving EŒ© from DICOM-only pipelines and reliably estimating coil sensitivities are not fully specified; reference [Krueger et al., 2023] does not address coil sensitivity estimation.The reweighting scheme for Lcomp lacks concrete hyperparameters (frequency of updating x^(m), choice of Œµ, the specific wavelet transform used in CUPID).The perturbation design is described qualitatively (‚Äúletters, numbers, suits‚Äù) without specifying amplitude ranges, energy relative to anatomy, or whether intensity normalization is applied.\n- Missing related work or comparisonsRecent unsupervised/posterior-sampling methods and latent diffusion approaches for MRI reconstruction (that may also operate without large raw-data training) are underrepresented in comparisons; e.g., more advanced diffusion samplers or latent adaptation methods.Closely related self-supervised work that incorporates perturbation consistency in the PI setting (though requiring raw data) could be discussed in more depth to clarify conceptual differences and the novelty of moving to DICOM-only training.\n\n- The method still requires accurate knowledge of the encoding operator EŒ© (including coil sensitivities) at train/test time. In real DICOM-only scenarios, reliable access to coil maps is nontrivial and vendor-dependent; this practical bottleneck is only partially addressed.\n- The fidelity term relies on hand-designed image-domain perturbations whose amplitude, frequency content, and spatial placement could affect convergence, stability, and potential bias; design guidance is limited.\n- The approach assumes xPI ‚âà (EŒ©^H EŒ©)^{-1} EŒ©^H yŒ©; vendor pipelines often apply filters, partial Fourier, coil compression, or proprietary post-processing that may violate this equivalence.\n\n- Retrospective experiments use fastMRI-derived EŒ© and simulate xPI via CG-SENSE; this does not fully represent the DICOM-only constraint (no demonstration of coil map estimation from only DICOM exports or separate vendor calibration DICOMs in the retrospective setting).\n- Prospective results are qualitative only; no reader study or quantitative surrogate (e.g., repeated acquisitions, central k-space ‚Äúreference,‚Äù or task-based evaluation) to substantiate claims at R=9.\n- Limited sampling patterns (mostly 1D equidistant at R=4 and a single prospective kx-only R=9); generalization to random sampling, 2D sampling, non-Cartesian patterns, and higher R values is not assessed.\n- Baseline tuning for diffusion/score-based methods appears limited (e.g., only ScoreMRI with hard DC), potentially underestimating performance relative to more recent MRI-tailored or LDM-based samplers.\n\n- Details for deriving EŒ© from DICOM-only pipelines and reliably estimating coil sensitivities are not fully specified; reference [Krueger et al., 2023] does not address coil sensitivity estimation.\n- The reweighting scheme for Lcomp lacks concrete hyperparameters (frequency of updating x^(m), choice of Œµ, the specific wavelet transform used in CUPID).\n- The perturbation design is described qualitatively (‚Äúletters, numbers, suits‚Äù) without specifying amplitude ranges, energy relative to anatomy, or whether intensity normalization is applied.\n\n- Recent unsupervised/posterior-sampling methods and latent diffusion approaches for MRI reconstruction (that may also operate without large raw-data training) are underrepresented in comparisons; e.g., more advanced diffusion samplers or latent adaptation methods.\n- Closely related self-supervised work that incorporates perturbation consistency in the PI setting (though requiring raw data) could be discussed in more depth to clarify conceptual differences and the novelty of moving to DICOM-only training.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe CUPID objective L = Lcomp + Œª Lpif is sensible: reweighted-‚Ñì1 encourages compressibility while the PI-fidelity term targets non-triviality by requiring the network to reproduce resolvable perturbations. The motivation that f(xPI + p) ‚àí p ‚âà f(xPI) when p is PI-resolvable aligns with how the DF block uses EŒ© and the RHS EŒ©^H yŒ©, which can be approximated by EŒ©^H EŒ© xPI under the PI assumption.The assumption that EŒ©^H yŒ© = EŒ©^H EŒ© xPI critically depends on xPI being a true PI solution. In real scanner pipelines, deviations (e.g., filtering, partial Fourier, calibration mismatch) can break this equality. The paper briefly acknowledges filtering, but the stability of CUPID when the equality is only approximate should be investigated (e.g., controlled perturbations to emulate vendor post-processing).The perturbation strategy is original in this training context and leverages PI resolvability. However, the theoretical guarantee that trivial solutions are precluded is qualitative; a brief analysis of failure modes (e.g., p amplitude too small, excessive noise, or coil map errors) would strengthen soundness.\n- Experimental evaluation assessmentRetrospective R=4 equidistant experiments are a good stress test for coherent aliasing; results show CUPID near supervised/SSDU and better than CS/CG-SENSE. That said, R=4 is relatively moderate, and PD-FS is more challenging‚Äîan expanded study at R‚â•6 and a broader set of sequences/organs would better characterize limits.Zero-shot experiments are relevant to the target use case and show that CUPID can approach ZS-SSDU without raw data. This is a compelling result.The prospective 7T case demonstrates practical feasibility at high R with nontrivial artifacts/noise in the vendor DICOM. However, the evaluation is qualitative only. Including a radiologist reader study, a noise/artifact scoring, or use of a partial-reference metric (e.g., low-frequency consistency) would substantiate claims.Ablations on Œª and K are valuable. Further ablations on perturbation amplitude, type, and spatial distribution would guide practitioners and assess robustness to design choices.\n- Comparison with related work (using the summaries provided)Self-supervised SSDU (Yaman et al., 2019/2020) and its variants require raw k-space. CUPID is complementary by removing that requirement and learning from xPI; this is a distinctive contribution.Generative/diffusion methods (e.g., ScoreMRI; more recent LDM-based MRPD-like approaches) can use only images for prior learning but typically require raw measurements for data consistency at inference and are computationally heavy. CUPID avoids large-scale generative training and is scan-efficient, but still depends on EŒ© at test time and leverages an unrolled PD-DL that is faster at inference.Recent unsupervised flow-matching or diffusion posterior methods (e.g., GTF2M, DiffINR) mostly rely on raw measurements at training or sampling; CUPID targets a different training constraint (no raw), so the positioning is appropriate. Including a more diverse set of non-raw training baselines (e.g., deep image prior, scan-specific INRs without raw) would better situate the method.Perturbation-consistency ideas have emerged in self-supervised PD-DL with raw access; CUPID‚Äôs move to DICOM-only training via a PI-compatible perturbation scheme is a clear novelty relative to those.\n- Discussion of broader impact and significanceThe paper addresses a real barrier to clinical dissemination: lack of raw access precluding site-specific PD-DL training. If adopted, CUPID could enable local fine-tuning to mitigate distribution shifts and thereby improve equity in MRI access and quality.Practical hurdles remain: the need for EŒ© and coil maps; GPU availability for fine-tuning; and the variability of vendor pipelines. The discussion acknowledges some of these aspects and suggests possible mitigations (transfer learning, offsite compute). A deeper practical roadmap would enhance impact (e.g., recommended acquisition of calibration DICOMs, QA steps to verify EŒ© consistency).\n\n- The CUPID objective L = Lcomp + Œª Lpif is sensible: reweighted-‚Ñì1 encourages compressibility while the PI-fidelity term targets non-triviality by requiring the network to reproduce resolvable perturbations. The motivation that f(xPI + p) ‚àí p ‚âà f(xPI) when p is PI-resolvable aligns with how the DF block uses EŒ© and the RHS EŒ©^H yŒ©, which can be approximated by EŒ©^H EŒ© xPI under the PI assumption.\n- The assumption that EŒ©^H yŒ© = EŒ©^H EŒ© xPI critically depends on xPI being a true PI solution. In real scanner pipelines, deviations (e.g., filtering, partial Fourier, calibration mismatch) can break this equality. The paper briefly acknowledges filtering, but the stability of CUPID when the equality is only approximate should be investigated (e.g., controlled perturbations to emulate vendor post-processing).\n- The perturbation strategy is original in this training context and leverages PI resolvability. However, the theoretical guarantee that trivial solutions are precluded is qualitative; a brief analysis of failure modes (e.g., p amplitude too small, excessive noise, or coil map errors) would strengthen soundness.\n\n- Retrospective R=4 equidistant experiments are a good stress test for coherent aliasing; results show CUPID near supervised/SSDU and better than CS/CG-SENSE. That said, R=4 is relatively moderate, and PD-FS is more challenging‚Äîan expanded study at R‚â•6 and a broader set of sequences/organs would better characterize limits.\n- Zero-shot experiments are relevant to the target use case and show that CUPID can approach ZS-SSDU without raw data. This is a compelling result.\n- The prospective 7T case demonstrates practical feasibility at high R with nontrivial artifacts/noise in the vendor DICOM. However, the evaluation is qualitative only. Including a radiologist reader study, a noise/artifact scoring, or use of a partial-reference metric (e.g., low-frequency consistency) would substantiate claims.\n- Ablations on Œª and K are valuable. Further ablations on perturbation amplitude, type, and spatial distribution would guide practitioners and assess robustness to design choices.\n\n- Self-supervised SSDU (Yaman et al., 2019/2020) and its variants require raw k-space. CUPID is complementary by removing that requirement and learning from xPI; this is a distinctive contribution.\n- Generative/diffusion methods (e.g., ScoreMRI; more recent LDM-based MRPD-like approaches) can use only images for prior learning but typically require raw measurements for data consistency at inference and are computationally heavy. CUPID avoids large-scale generative training and is scan-efficient, but still depends on EŒ© at test time and leverages an unrolled PD-DL that is faster at inference.\n- Recent unsupervised flow-matching or diffusion posterior methods (e.g., GTF2M, DiffINR) mostly rely on raw measurements at training or sampling; CUPID targets a different training constraint (no raw), so the positioning is appropriate. Including a more diverse set of non-raw training baselines (e.g., deep image prior, scan-specific INRs without raw) would better situate the method.\n- Perturbation-consistency ideas have emerged in self-supervised PD-DL with raw access; CUPID‚Äôs move to DICOM-only training via a PI-compatible perturbation scheme is a clear novelty relative to those.\n\n- The paper addresses a real barrier to clinical dissemination: lack of raw access precluding site-specific PD-DL training. If adopted, CUPID could enable local fine-tuning to mitigate distribution shifts and thereby improve equity in MRI access and quality.\n- Practical hurdles remain: the need for EŒ© and coil maps; GPU availability for fine-tuning; and the variability of vendor pipelines. The discussion acknowledges some of these aspects and suggests possible mitigations (transfer learning, offsite compute). A deeper practical roadmap would enhance impact (e.g., recommended acquisition of calibration DICOMs, QA steps to verify EŒ© consistency).\n\n### Questions for Authors\n\n- How do you obtain EŒ© (especially coil sensitivities) in the strict DICOM-only setting? Can you detail the exact steps and data required to estimate S maps without raw k-space or ACS lines, and report robustness to map errors? The Krueger et al. reference appears unrelated to coil sensitivity estimation.\n- In retrospective experiments, were EŒ© and coil maps derived from raw fastMRI data? If so, can you replicate training using only DICOM-like inputs plus a separate vendor-style calibration DICOM to better emulate the true constraint?\n- How sensitive is CUPID to vendor post-processing that violates xPI ‚âà (E^H E)^{-1} E^H y (e.g., partial Fourier, zero-padding, denoising, intensity corrections)? Can you provide experiments where you intentionally apply such processing to test robustness?\n- What are the perturbation amplitude and spectral characteristics you recommend? Have you studied whether perturbations that are too strong or too weak affect convergence or induce biases/hallucinations in fine textures?\n- Can you quantify zero-shot fine-tuning time and memory footprint for typical 2D and 3D scans, and compare to ZS-SSDU and diffusion-based sampling in terms of wall-clock and compute?\n- Which wavelet transform and Œµ do you use for Lcomp in CUPID, how often do you update x^(m), and how many reweighting steps are performed? Please provide a sensitivity analysis on these hyperparameters.\n- Could you include an additional baseline with a recent latent diffusion approach adapted to MRI magnitudes (e.g., LDM + MRAdapter-style adaptation) or a deep image prior baseline to strengthen raw-free comparisons?\n- For the prospective 7T experiment, can you provide any quantitative surrogate (e.g., low-frequency error, noise index, or radiologist assessment) to support the qualitative gains at R=9?\n- Have you tested CUPID on random 1D, 2D Cartesian, or non-Cartesian sampling to assess generality of the perturbation strategy and training stability?\n- Do you foresee principled ways to automatically select perturbations (amplitude, locations) beyond the proposed reinforcement-learning idea‚Äîe.g., via sensitivity maps and g-factor predictions?\n\n### Overall Assessment\n\nThis paper tackles an important and practical barrier to deploying PD-DL MRI reconstruction in typical clinical environments: the need for raw k-space access during training. CUPID‚Äôs formulation‚Äîmelding compressibility-driven unsupervised learning with a parallel imaging-consistent perturbation fidelity‚Äîis original and well-motivated, and the experimental evidence suggests that, in both database and zero-shot modes, the method attains performance comparable to strong PD-DL baselines while operating without raw data. The work is timely and potentially impactful for equitable MRI access. The primary limitations are practical and experimental: reliable access to EŒ© in DICOM-only settings is not yet demonstrated end-to-end; robustness to vendor post-processing is only qualitatively discussed; and prospective validation lacks quantitative or reader-based assessment. Strengthening these aspects (clearer coil-map pipeline from DICOM-only, additional robustness studies, stronger baselines, and quantitative prospective evaluation) would considerably enhance the paper‚Äôs maturity. Overall, I view the core idea as significant and the results promising; with the suggested clarifications and added experiments, this work would be a strong candidate for a top-tier venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *CUPID*, a physics-driven deep learning (PD-DL) framework for MRI reconstruction that operates without access to raw k-space data, training instead on routine clinical images (e.g., DICOM). The method introduces a reweighted ‚Ñì1 compressibility loss combined with a ‚Äúparallel imaging fidelity‚Äù term based on resolvable perturbations, enabling both database-level training and zero-shot fine-tuning. Experiments on fastMRI knee/brain datasets and a prospective 7T brain study indicate that CUPID approaches supervised and self-supervised PD-DL performance while surpassing compressed sensing and classical reconstructions. The paper is clearly written and technically motivated, addressing an important barrier to equitable clinical deployment of PD-DL methods.  \n\n---\n\n**Major Comments**  \n1. **Dependence on accurate encoding models:** CUPID requires knowledge of the encoding operator and coil sensitivities at both training and inference. In true DICOM-only environments, obtaining reliable coil maps is vendor-specific and often infeasible. This limitation is acknowledged but not fully resolved.  \n2. **Design of perturbations:** The fidelity term relies on handcrafted image-domain perturbations; their magnitude and spatial frequency characteristics may influence convergence and bias. Design guidelines are limited.  \n3. **Assumption of vendor equivalence:** The method presumes \\(x_{PI} ‚âà (E^H E)^{-1} E^H y\\), though vendor pipelines often apply proprietary filters or partial Fourier operations that violate this assumption. Robustness to such discrepancies should be tested.  \n4. **Experimental scope:** Retrospective studies use simulated data with known EŒ© and coil maps, which does not fully emulate raw-free conditions. The 7T prospective result is qualitative only, lacking reader or quantitative evaluation. Sampling diversity (limited to 1D equidistant and one kx-only case) restricts generality.  \n5. **Comparative analysis:** Baseline tuning for diffusion and score-based models appears limited, and related unsupervised methods relying on posterior sampling or perturbation consistency are incompletely discussed. Including modern latent diffusion or deep image prior baselines would strengthen positioning.  \n6. **Implementation specifics:** Important hyperparameters‚Äîsuch as the reweighting update frequency, Œµ choice, and wavelet transform‚Äîare not detailed, and the perturbation procedure lacks quantitative specifications.  \n\n---\n\n**Minor Comments**  \n- Clarify the pipeline for constructing EŒ© and coil maps in the DICOM-only scenario; the cited reference does not describe this process.  \n- Provide details on hyperparameter sensitivity (Œª, K, perturbation amplitude).  \n- Specify how prospective data were normalized and how perturbations were scaled relative to anatomy.  \n- Expand citations to unsupervised or latent diffusion MRI methods operating without raw data.  \n- Improve figure captions with units or perturbation examples for clarity.  \n\n---\n\n**Summary Paragraph**  \nOverall, CUPID introduces a technically original and well-presented approach to PD-DL MRI reconstruction when raw data are unavailable. It effectively merges compressibility-based learning with a novel parallel-imaging-consistent fidelity. The paper‚Äôs strengths lie in its conceptual innovation, lucid exposition, and dual-mode validation (database and zero-shot). Main weaknesses concern practical feasibility‚Äîespecially the unverified DICOM-only workflow, limited robustness tests to vendor post-processing, and qualitative prospective assessments. Addressing these issues through additional ablation and quantitative evaluation would substantially strengthen the paper‚Äôs credibility and clinical relevance.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The work is promising and innovative, but requires clearer demonstration of raw-free feasibility, additional quantitative validation, and expanded comparisons before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThis manuscript addresses the critical problem of limited access to advanced physics-driven deep learning (PD-DL) MRI reconstruction techniques in rural and underserved areas, where commercial MRI scanners typically only provide reconstructed DICOM images without raw k-space data access. The authors propose CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), a novel method that trains PD-DL reconstruction networks using only routine clinical DICOM images exported from scanners. CUPID employs a dual-component loss function: a compressibility term that evaluates sparsity in the wavelet domain, and a parallel imaging fidelity term that uses carefully designed perturbations to ensure reconstruction consistency. The authors demonstrate that CUPID achieves reconstruction quality comparable to supervised PD-DL and SSDU methods (which require raw k-space data), while outperforming conventional compressed sensing and generative approaches across multiple MRI datasets (knee, brain) with both retrospective (R=4) and prospective (R=9) undersampling. The method is also shown to be effective in zero-shot/subject-specific training scenarios, highlighting its potential to democratize access to high-quality fast MRI reconstruction in resource-limited settings.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- The problem addressed has high clinical relevance as it tackles a significant equity issue in healthcare: the inability of rural and underserved communities to benefit from advanced MRI reconstruction techniques due to lack of raw data access.\n- CUPID represents the first successful method to train physics-driven deep learning MRI reconstruction without requiring raw k-space data, which is a substantial innovation in the field.\n- The comprehensive evaluation across multiple datasets (knee and brain MRI), with both retrospective and prospective undersampling patterns, provides strong validation of the method's effectiveness.\n- The theoretical justification for the dual-loss function is well-developed, with clear explanations of how the parallel imaging fidelity term prevents the network from converging to trivial solutions.\n\n### Major Limitations\n- The paper lacks detailed information about computational requirements for training CUPID, which is critical for assessing its feasibility in the resource-limited settings it aims to serve. Specific details about GPU requirements, training time, and memory usage would strengthen the practical contribution.\n- The clinical implementation pathway is not sufficiently discussed. While the method is innovative, more detail is needed on how hospitals in rural/underserved areas would practically implement this solution, including integration with existing workflows.\n- The prospective study section is limited to a single example with R=9 acceleration, making it difficult to assess the method's performance across a broader range of practical acceleration rates that might be encountered in clinical settings.\n\n### Minor Strengths\n- The visualizations (particularly Figure 2) clearly illustrate the CUPID framework and its relationship to parallel imaging reconstruction.\n- The ablation studies on the Œª parameter and number of perturbations provide valuable insights into the robustness of the approach to parameter choices.\n- The demonstration of compatibility with different parallel imaging reconstruction methods (GRAPPA vs. SENSE) strengthens the method's practical applicability.\n\n### Minor Limitations\n- Some figures (e.g., Figure 3, Figure 4) are quite dense and would benefit from more targeted highlighting of key differences between methods.\n- The paper mentions magnitude-only DICOMs as a potential limitation but doesn't provide sufficient discussion about how to handle this common scenario in practice.\n- The prospective study could be strengthened by including more quantitative metrics alongside the qualitative results.\n\n## 3. Summary Evaluation\n\n**Significance (High):** The work addresses a critical real-world problem with substantial clinical impact - the inequitable access to advanced MRI reconstruction techniques between specialized academic centers and rural/underserved areas. By enabling PD-DL training without raw data access, CUPID has the potential to democratize high-quality fast MRI, which could significantly improve healthcare equity.\n\n**Innovation (High):** The paper presents a truly innovative approach to MRI reconstruction training, being the first to successfully train physics-driven deep learning networks using only clinical DICOM images. The dual-component loss function combining compressibility evaluation with parallel imaging fidelity represents a novel conceptual framework that departs significantly from existing methods.\n\n**Evaluation (Good):** The evaluation is comprehensive across multiple datasets and acquisition scenarios, with appropriate comparisons to state-of-the-art methods. The inclusion of both retrospective and prospective undersampling studies strengthens the validation. However, more detailed computational requirements analysis and additional clinical implementation considerations would further strengthen the evaluation.\n\n**Reproducibility (Good):** The paper provides detailed implementation information in the appendices, including network architecture, training parameters, and perturbation strategies. The authors state that code will be released upon acceptance, which supports reproducibility. More specific details about hardware requirements would further enhance reproducibility for potential implementers.\n\n## 4. Decision Recommendation\n\nMinor revision\n\nThe manuscript presents a highly significant and innovative contribution to medical imaging with strong technical evaluation. The work addresses a critical equity issue in healthcare and offers a practical solution with real-world potential. The minor limitations identified (primarily around computational requirements and clinical implementation details) can be addressed with revisions that would strengthen the paper's practical impact without requiring major additional experiments. I recommend minor revision to clarify the computational requirements for training in resource-limited settings and provide more detailed implementation guidance for clinical translation.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces CUPID (Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity), a novel physics-driven deep learning (PD-DL) approach for MRI reconstruction designed to function when only reconstructed DICOM images are available‚Äîa common constraint in rural or underserved areas lacking raw k-space data access. CUPID trains PD-DL networks using DICOM images by combining a compressibility-based loss term with a parallel imaging fidelity term that enforces reconstruction consistency. Experiments on knee and brain MRI datasets with both retrospective (R=4) and prospective (R=9) undersampling demonstrate that CUPID achieves performance comparable to supervised PD-DL and self-supervised (SSDU) methods while outperforming conventional compressed sensing and generative models. The manuscript is generally clear, well-organized, and presents an appealing solution with strong translational potential.  \n\n**Major Comments**  \n1. **Computational Requirements:** The manuscript lacks sufficient details on CUPID‚Äôs computational demands, including GPU specifications, training time, and memory usage‚Äîinformation essential for assessing applicability in the resource-limited environments it seeks to benefit.  \n2. **Clinical Implementation:** The discussion on practical deployment within hospital workflows is limited. Clearer guidance on how this method could be integrated into routine clinical pipelines in rural or underserved settings would enhance the practical impact.  \n3. **Prospective Study Scope:** The prospective results are based on a single R=9 example, which restricts understanding of CUPID‚Äôs performance across other acceleration rates relevant for clinical practice.  \n\n**Minor Comments**  \n- Figures 3 and 4 are visually dense and would benefit from more focused highlighting of key visual differences among methods.  \n- The limitation related to magnitude-only DICOMs is acknowledged but not sufficiently discussed. Further elaboration on possible handling strategies would be helpful.  \n- Additional quantitative metrics could complement the qualitative prospective results.  \n- Figure 2 effectively illustrates the CUPID framework, while ablation studies on the Œª parameter and number of perturbations and tests across GRAPPA/SENSE configurations add robustness.  \n\n**Summary Paragraph**  \nThis work addresses a clinically important problem by enabling PD-DL MRI reconstruction training without access to raw k-space data, thereby promoting more equitable access to advanced imaging technology. The study is technically strong, with a well-founded dual-loss design, comprehensive validation across multiple datasets, and clear demonstrations of generality. While the innovation and significance are high, the manuscript would benefit from elaborating on computational feasibility and practical clinical deployment. These refinements would further strengthen its real-world relevance and reproducibility.  \n\n**Decision Recommendation**  \n**Minor Revision.** The manuscript is innovative, significant, and well validated. Clarifying computational requirements and expanding the discussion of clinical implementation would address the remaining concerns without necessitating major new experiments.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe authors tackle a practical obstacle in modern MRI reconstruction: most physics‚Äëdriven deep‚Äëlearning (PD‚ÄëDL) pipelines presuppose access to raw k‚Äëspace data, which is rarely available outside large academic centres. To circumvent this, they introduce CUPID, an unsupervised training framework that operates solely on conventional DICOM images produced by parallel‚Äëimaging reconstructions. CUPID combines a re‚Äëweighted ‚Ñì‚ÇÅ compressibility loss with a fidelity term derived from deliberately crafted perturbations that remain recoverable by any parallel‚Äëimaging algorithm. The method is evaluated on retrospectively and prospectively undersampled knee and brain scans (acceleration factors R‚ÄØ=‚ÄØ4 and R‚ÄØ=‚ÄØ9). Reported results suggest that CUPID achieves reconstruction quality on par with supervised and self‚Äësupervised PD‚ÄëDL approaches that use raw data, and it outperforms traditional compressed‚Äësensing and recent generative baselines. The claimed novelty is the ability to train high‚Äëperformance PD‚ÄëDL models without raw k‚Äëspace, potentially widening access to accelerated MRI.\n\n---\n\n## General feedback  \n\n*Significance.*  The problem of missing raw k‚Äëspace in routine clinical workflows is genuine, and a solution that works with DICOM‚Äëonly data could indeed broaden the applicability of PD‚ÄëDL methods.\n\n*Innovation.*  The paper merges a compressibility‚Äëbased ‚Ñì‚ÇÅ penalty with a perturbation‚Äëdriven fidelity term.  While each component builds on existing ideas (e.g., re‚Äëweighted ‚Ñì‚ÇÅ from AlcÃßalar‚ÄØet‚ÄØal., 2024; self‚Äësupervision in SSDU), the particular combination is new.  However, the manuscript does not convincingly demonstrate that this synthesis yields a substantive methodological advance beyond what could be obtained by simpler alternatives.\n\n*Evaluation.*  The authors compare CUPID against a fairly extensive set of baselines (supervised PD‚ÄëDL, SSDU, EI, ScoreMRI, CS, CG‚ÄëSENSE) on three datasets and present quantitative tables (1‚Äì2) together with several figures (3‚Äì5,‚ÄØ10).  Yet crucial aspects of the experimental design‚Äîstatistical testing, precise train/validation splits, and computational cost‚Äîare missing, which limits confidence in the reported improvements.\n\n*Reproducibility.*  The description of the network architecture, loss weighting (Œª), perturbation count (K‚ÄØ=‚ÄØ6), and training schedule is adequate, but essential procedural details are omitted.  In particular, the way coil‚Äësensitivity maps are obtained from DICOM‚Äëonly data, the handling of magnitude‚Äëonly images, and a full list of hyper‚Äëparameters are not provided.  Moreover, the code is promised only after acceptance, precluding any independent verification at this stage.\n\n---\n\n## Specific comments / critiques  \n\n1. **Encoding operator‚ÄØE.**  The fidelity term depends on coil‚Äësensitivity maps, yet the manuscript merely cites ‚Äúlow‚Äëresolution calibration scans‚Äù (Sec.‚ÄØ4.1) without explaining how those maps are estimated when only DICOM images are available.  This omission makes the method unreproducible for readers who lack raw k‚Äëspace.\n\n2. **Magnitude‚Äëonly DICOMs.**  Many clinical systems export only the magnitude of the reconstructed image.  CUPID‚Äôs loss function, however, assumes complex‚Äëvalued data.  The authors acknowledge this limitation (Sec.‚ÄØ6.2) but do not present any experimental evidence that the approach can be extended to magnitude‚Äëonly inputs, leaving its utility in real‚Äëworld settings unclear.\n\n3. **Statistical significance.**  Tables‚ÄØ1 and‚ÄØ2 report mean‚ÄØ¬±‚ÄØstandard error but do not include any hypothesis tests (e.g., paired t‚Äëtests or non‚Äëparametric equivalents).  Consequently, claims of ‚Äúcomparable‚Äù performance to supervised PD‚ÄëDL or ‚Äúsuperiority‚Äù over CS/EI/ScoreMRI lack quantitative support.\n\n4. **Computational cost.**  The manuscript provides no information on training time per epoch, total runtime, memory consumption, or inference speed (Sec.‚ÄØ4.4,‚ÄØ4.6).  Without these metrics it is impossible to assess whether the proposed zero‚Äëshot fine‚Äëtuning can be performed on hardware typically available in peripheral clinics.\n\n5. **Perturbation design.**  Figure‚ÄØ9 displays the visual appearance of the perturbations (random letters, numbers, card suits), but the text offers only a qualitative description.  Precise algorithmic details‚Äîsize of the perturbations, intensity distribution, how K‚ÄØ=‚ÄØ6 was selected‚Äîare missing.  Moreover, the trade‚Äëoff between perturbation count and reconstruction quality (Fig.‚ÄØ7,‚ÄØ8) is presented qualitatively; a quantitative analysis (e.g., PSNR versus training time) would be more informative.\n\n6. **Generalization to pathology.**  Although the authors motivate the work by the need to serve underserved populations with diverse disease presentations, all experiments are confined to healthy knee and brain scans.  No evaluation on pathological or multi‚Äëcenter datasets is provided, so the claim of broad clinical relevance remains speculative.\n\n7. **Ablation of the fidelity term.**  Figure‚ÄØ6 illustrates visually the effect of setting Œª‚ÄØ‚Üí‚ÄØ‚àû (compressibility loss only) versus Œª‚ÄØ=‚ÄØ0 (fidelity loss only), yet the corresponding quantitative metrics (PSNR, SSIM) are absent.  This makes it difficult to gauge the actual contribution of the fidelity component.\n\n8. **Dataset split and cross‚Äëvalidation.**  The paper states the use of 300 training slices and 380/100 test slices (Sec.‚ÄØ4.1) but does not clarify whether splits are performed at the subject level, whether any cross‚Äëvalidation was employed, or how data leakage was avoided.  Such details are essential for evaluating the robustness of the results.\n\n9. **Code and data availability.**  The authors commit to releasing code after acceptance but provide no current repository link or plan for data sharing.  The lack of immediate access hampers reproducibility and prevents other groups from benchmarking the method.\n\n---\n\n## A suggested decision  \n\n**Major Revision**  \nThe manuscript introduces a conceptually interesting direction, yet the current presentation suffers from insufficient methodological detail, lack of statistical validation, and incomplete evaluation of practical constraints.  Substantial revisions are required before the work can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents CUPID, an unsupervised training framework designed to address a key limitation in physics‚Äëdriven deep‚Äëlearning (PD‚ÄëDL) MRI reconstruction‚Äînamely, the dependence on raw k‚Äëspace data, which is often inaccessible in clinical settings. CUPID operates directly on standard DICOM images reconstructed by parallel‚Äëimaging methods and integrates a re‚Äëweighted ‚Ñì‚ÇÅ compressibility loss with a perturbation‚Äëdriven fidelity term. The method is tested on retrospectively and prospectively undersampled knee and brain scans with acceleration factors of 4 and 9. Results indicate performance comparable to supervised and self‚Äësupervised PD‚ÄëDL models that use raw k‚Äëspace, and superior to compressed‚Äësensing and generative baselines. Overall, the study is clearly written and tackles a meaningful practical problem, though the evidence supporting methodological novelty and performance remains incomplete.  \n\n**Major Comments**  \n1. **Methodological clarity:** The paper does not sufficiently explain how coil‚Äësensitivity maps are derived from DICOM‚Äëonly data, a crucial element for reproducing the fidelity term.  \n2. **Data constraints:** CUPID assumes complex‚Äëvalued DICOMs, yet many clinical exports are magnitude‚Äëonly. Although this limitation is acknowledged, no experiments demonstrate feasibility in such settings.  \n3. **Statistical validation:** Reported mean‚ÄØ¬±‚ÄØstandard‚Äëerror values lack accompanying hypothesis tests, leaving claims of comparable or superior performance unsubstantiated.  \n4. **Computational efficiency:** Training and inference runtimes, memory demands, and hardware requirements are not reported, obscuring the method‚Äôs real‚Äëworld applicability.  \n5. **Perturbation design:** The description of the crafted perturbations (letters, numbers, card suits) is qualitative only. Quantitative specifications and an analysis of trade‚Äëoffs between perturbation count and reconstruction quality are missing.  \n6. **Evaluation scope:** Experiments are limited to healthy data; no studies on pathological or multi‚Äëcenter datasets are included, weakening arguments about clinical relevance.  \n7. **Ablation details:** The contribution of the fidelity term is shown visually but lacks quantitative metrics (e.g., PSNR, SSIM) to substantiate its importance.  \n8. **Dataset splitting:** Subject‚Äëlevel splits, cross‚Äëvalidation procedures, and leakage prevention methods are not described, raising concerns about result reliability.  \n9. **Reproducibility:** Code and data are promised only post‚Äëacceptance, and key hyper‚Äëparameters remain unspecified.  \n\n**Minor Comments**  \n‚Äì Figures could provide quantitative results alongside qualitative illustrations.  \n‚Äì Clarify notation for Œª and K to ensure interpretability.  \n‚Äì Provide consistent references to related methods such as SSDU and EI.  \n\n**Summary Paragraph**  \nThis work addresses a well‚Äëmotivated and practically important limitation of PD‚ÄëDL MRI by proposing an unsupervised DICOM‚Äëbased approach. The combination of compressibility loss and perturbation‚Äëdriven fidelity is conceptually interesting, but the manuscript requires more evidence to confirm its methodological contribution and empirical superiority. Missing statistical analyses, incomplete ablations, and absent details on computational cost, dataset handling, and code availability limit the current reproducibility and impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The paper offers a promising concept but needs substantial clarification, quantitative validation, and reproducibility enhancements before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Mehmet Akcakaya",
      "Merve Gulle",
      "Yasar Utku Alcalar"
    ],
    "url": "pdfs/iclr.cc-2025-conference_058122326c88eb7ecc35a0739e0ce1f6c8ad76ad.pdf",
    "remote_url": "https://openreview.net/pdf/058122326c88eb7ecc35a0739e0ce1f6c8ad76ad.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Lecture Learning",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "Multimodal assistant",
      "surgical",
      "multimodal instruction-following data",
      "dataset"
    ],
    "abstract": "Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images. Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos. One major contributing factor is the absence of datasets in the surgical field. In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far. To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. We will release our code, model, and the instruction-tuning dataset.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal large language model designed as a conversational assistant for surgical applications. To support this, the authors developed Surg-QA, a large-scale dataset containing 102,000 surgical video-instruction pairs, generated through a structured two-stage question-answer pipeline. This pipeline helps extract structured knowledge from surgical lecture videos, enabling the LLaVA-Surg model to understand complex surgical procedures and answer open-ended questions in a zero-shot setting. The model leverages CLIP for visual encoding and is fine-tuned on Surg-QA to specialize in surgical video question-answering, achieving superior performance compared to existing general-domain models.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1.\tThe authors provide a novel dataset, Surg-QA, which is a significant resource for training multimodal surgical models, covering diverse surgical procedures and question-answer pairs.\n2.\tThe two-stage pipeline for question-answer generation mitigates hallucinations in LLM outputs, resulting in higher quality and reliability of generated data.\n3.\tLLaVA-Surg demonstrates notable improvements over general multimodal models in zero-shot surgical video question-answering tasks, showcasing its efficacy in understanding surgical context.\n\n### Weaknesses\n\n1. The paper should compare its model with recent multimodal LLM approaches, specifically ReAct (Yao et al., 2023), which combines reasoning and action for complex tasks.\n[1] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2023, January). ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations (ICLR).\n2. Using CLIP for frame-by-frame encoding lacks temporal modeling and increases processing costs and redundancy, burdening the LLM as frame count grows.\n3. The paper lacks an in-depth error analysis, especially regarding potential hallucinations or misunderstandings in complex surgical scenarios. Although the authors claim to reduce hallucinations, achieving perfect performance seems challenging.\n4. The model‚Äôs adaptability to other medical or clinical fields is unclear, as broader evaluations on datasets like RAD, SLAKE, and PathVQA are missing, which may limit its wider applicability.\n\n### Questions\n\n1. Does splitting video into frames for CLIP‚Äôs visual encoder lead to a loss of spatiotemporal information, and wouldn‚Äôt a video encoder like Video Swin Transformer [2] better capture temporal dynamics?\n[2] Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., & Hu, H. (2022). Video swin transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 3202-3211).\n2. How does LLaVA-Surg perform compared to other state-of-the-art multimodal methods? In addition to general multimodal models, a detailed comparison with models like ReAct would provide a more comprehensive evaluation. Has comparison with other two-stage methods [3] in VQA task been overlooked?\n[3] Gai, X., Zhou, C., Liu, J., Feng, Y., Wu, J., & Liu, Z. (2024). MedThink: Explaining Medical Visual Question Answering via Multimodal Decision-Making Rationale. arXiv preprint arXiv:2404.12372.\n3. Is the two-stage question-answer generation process applicable to other medical fields, and if so, what adjustments would be required? Additionally, validating the method‚Äôs performance on public datasets like RAD, SLAKE, and PathVQA would strengthen its generalizability.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **LLaVA-Surg**, a multimodal large language model designed as a conversational assistant for surgical applications. It is trained using **Surg-QA**, a newly constructed dataset of approximately 102,000 surgical video‚Äìinstruction pairs generated through a structured two-stage question‚Äìanswer pipeline. This pipeline aims to extract procedural knowledge from surgical lecture videos, thereby enhancing the model‚Äôs ability to understand surgical contexts and respond to open-ended questions in a zero-shot manner. The study reports improved performance over existing general-domain multimodal models, demonstrating potential for application in surgical video question answering.  \n\n**Major Comments**  \n1. Comparison with recent multimodal LLMs such as **ReAct (Yao et al., 2023)** is missing. Including such baselines would enable more meaningful evaluation of LLaVA-Surg‚Äôs reasoning and action capabilities.  \n2. The use of **CLIP** for frame-by-frame encoding introduces redundancy and lacks temporal modeling, which may increase processing overhead and limit understanding of dynamic surgical procedures. Employing a dedicated video encoder could address this limitation.  \n3. The manuscript does not provide a detailed **error or hallucination analysis**, especially in complex surgical scenarios. Although the proposed two-stage generation pipeline is intended to mitigate hallucinations, its limitations remain insufficiently explored.  \n4. The **generalizability** of LLaVA-Surg to other clinical or medical domains is unclear, as no evaluations are conducted on public medical datasets such as **RAD, SLAKE, or PathVQA**. Broader validation would strengthen the paper‚Äôs claims regarding applicability and robustness.  \n\n**Minor Comments**  \n- Clarify whether splitting videos into static frames results in a loss of spatiotemporal information; consideration of **video-based encoders (e.g., Video Swin Transformer)** could improve modeling of temporal dynamics.  \n- Provide explicit results comparing to other two-stage VQA methods (e.g., **MedThink, 2024**) for a more comprehensive performance analysis.  \n- Discuss the adaptability of the two-stage QA generation pipeline to other domains and outline required modifications.  \n\n**Summary Paragraph**  \nOverall, the paper presents a technically sound and well-motivated contribution, notably the creation of the large-scale Surg-QA dataset and the development of a specialized multimodal model for surgical understanding. Its main strengths include the dataset‚Äôs quality, the structured QA-generation strategy, and improvements over general-domain baselines. However, gaps remain in comparative analysis, temporal modeling, error examination, and demonstration of cross-domain generalizability. Addressing these issues would significantly enhance the work‚Äôs impact.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The study offers valuable contributions but requires additional comparative experiments, analysis of model limitations, and evidence of broader applicability before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a novel surgical multimodal dataset, which consists of over 102,000 video-instruction pairs generated through a two-stage pipeline, aimed at enhancing the understanding and conversational capabilities of surgical videos.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n1. With over 102,000 video-instruction pairs, this dataset is the largest in the surgical field.\n2. Structured data annotation pipeline using LLMs minimizes the risk of generating inaccurate or nonsensical content, improving dataset reliability.\n3. Releasing the dataset, model, and code publicly fosters further research and development in the surgical AI domain.\n4. The dataset can be a valuable resource for training and education, helping surgical trainees learn through interactive Q&A about real procedures.\n\n### Weaknesses\n\n1. The paper does not address how the data's quality is maintained as the videos are obtained from the web. The clinicians have reviewed the output of their MLLM model, but the paper does not confirm whether clinicians or domain experts have reviewed the raw data to ensure accuracy and reliability.\n2. Concerns regarding the release, privacy, and permission risks associated with using sensitive surgical videos are not adequately discussed.\n3. The paper lacks comprehensive validation across essential surgical downstream tasks and other surgical QA datasets, which are crucial for demonstrating clinical usability. There is also a need for more rigorous benchmarking against a broader range of state-of-the-art video MLLM architectures to establish the dataset's utility and the model's performance more robustly.\n4. The comparison of the proposed methods with SOTA methods is limited and does not include the latest works. The manuscript also lacks evaluations with models trained on other surgical datasets, limiting the assessment of the proposed model's generalizability across different surgical scenarios.\n5. The paper may need to evaluate the visual quality of the surgical videos.\n\n### Questions\n\n1. How can the quality of the data be ensured? The data collected may already contain a lot of noise and has been reprocessed by an LLM. Is there any person or clinician reviewing these raw data?\n2. Can the data be released? Are there privacy and permission risks associated with the collected data?\n3. The authors need to conduct more zero-shot evaluations on downstream tasks relevant to the surgical field, such as phase recognition, action/instrument classification, and other surgical domain VQA data to demonstrate the clinical usability of their method.\n4. The authors need to compare with more state-of-the-art methods. The comparison methods in Table 3 were all first released in 2023.\n5. The authors may verify their dataset on more benchmarks of SOTA Video MLLM architectures.\n6. Also, the authors need more zero-shot comparisons with the same VLM trained on other surgical datasets, to showcase the generalizability of their proposed dataset.\n7. The authors may evaluate the visual quality of the surgical videos themselves, as they are obtained from the website.\n\n### Flag For Ethics Review\n\n- Yes, Legal compliance (e.g., GDPR, copyright, terms of use)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPotential copyright problem for online data.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a large-scale multimodal dataset for surgical video understanding, comprising over 102,000 video‚Äìinstruction pairs generated via a two-stage pipeline. The goal is to advance multimodal large language models (MLLMs) for surgical video comprehension and conversational reasoning. The paper is generally clear and the proposed resource is potentially valuable for surgical AI research and education. However, while the contribution is significant in scale, the work raises concerns regarding data quality assurance, privacy, and insufficient benchmarking.\n\n**Major Comments**  \n1. **Data Quality Assurance** ‚Äì The manuscript does not sufficiently describe procedures for maintaining the accuracy and reliability of the raw videos collected from the web. Although clinicians review the model outputs, it is unclear whether domain experts have verified the raw data itself.  \n2. **Ethical and Privacy Risks** ‚Äì Potential privacy, permission, and legal issues related to publicly releasing sensitive surgical videos are not thoroughly addressed. Further clarification on compliance with regulations and consent processes is needed.  \n3. **Evaluation and Benchmarking** ‚Äì The validation of the proposed dataset and model across downstream surgical tasks (e.g., phase recognition, instrument classification) is limited. More comprehensive zero-shot and cross-dataset evaluations are necessary to demonstrate generalizability and clinical utility.  \n4. **Comparisons with Prior Work** ‚Äì The experimental comparisons include mainly earlier 2023 methods, overlooking more recent state-of-the-art video MLLM architectures. Broader benchmarking would strengthen the claims of effectiveness.  \n5. **Visual Quality of Videos** ‚Äì The paper does not assess or report the visual quality of the collected surgical videos, which may affect dataset usability and model performance.\n\n**Minor Comments**  \n- The authors should clarify whether clinicians‚Äô reviews extend to the original data or only to model outputs.  \n- Tables presenting comparisons could include baselines from other surgical datasets for context.  \n- Discussion of limitations and ethical compliance should be expanded.  \n\n**Summary Paragraph**  \nOverall, this work proposes a valuable large-scale dataset that could support educational and research applications in surgical AI. The dataset‚Äôs scale and public release are notable strengths. However, weaknesses include insufficient validation across tasks, limited benchmarking with contemporary methods, unclear data quality control, and potential ethical and privacy issues. Addressing these issues would considerably strengthen the paper‚Äôs impact and credibility.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal conversational assistant based on surgical videos. Additionally, they introduce a new dataset with 102,000 question-answer pairs for training multimodal LLMs. The authors provide details of their data generation procedure, which is carefully designed to avoid hallucinations. The paper provides detailed comparisons with existing general-purpose and surgical-purpose datasets. Lastly, the authors provide a human and LLM evaluation of the dataset, showing consistent scores.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\n- **Clarity**: The paper is well-written and easy to follow. \n- **Contributions**: This work makes a significant contribution to the development of surgical chat assistants. The dataset contains a wider range of surgical QAs compared to previous works. The proposed model and dataset may be valuable resources for researchers in this area.\n\n### Weaknesses\n\n- **Dataset Availability**: The surgical videos are available on WebSurg and are not a contribution of the authors. Therefore, the data availability may be subject to license changes from the content owners and WebSurg.\n- **Hallucinations and Data Quality**: As the authors mentioned, there may be hallucinations in the dataset, since it is automatically generated. The authors provide chatGPT and human evaluations, but that is not enough to infer the data quality.\n- **Model Availability**: It is not possible to reproduce the results since the model is not available yet, but enough details are provided to support the paper.\n\n### Questions\n\nThe paper is very well written and addresses its objectives. It also supports its claims and provides adequate experiments. Therefore, I am leaning toward accepting this paper, but I have some minor concerns regarding the legality of using WebSurg's surgical videos. I also have some questions:\n1. The authors mention that the model is limited by hallucinations, which is a serious concern for a surgical chatbot. Could you please provide more details, and types of hallucinations, and give some examples?\n2. Would it be possible to evaluate LLaVA-Surg on the SSG-VQA dataset? I am interested in knowing more about the breadth of your dataset and if it contains enough information for cross-dataset generalization.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nI am wondering about the WebSurg's policies on using their videos to train deep learning models, but I could not find any information about this in their terms of use.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **LLaVA-Surg**, a multimodal conversational assistant tailored for surgical applications and trained on surgical videos. Alongside the model, the authors introduce a new dataset containing approximately 102,000 question‚Äìanswer pairs designed to improve multimodal large language models in surgery-related domains. The data generation process is thoroughly explained, emphasizing strategies to minimize hallucinations. The paper is clearly written and systematically compares its dataset with existing general-purpose and surgical datasets. Both human and language model evaluations are included, showing consistent performance outcomes.\n\n---\n\n**Major Comments**  \n1. **Data Source and Availability**: The surgical videos used are sourced from WebSurg and are not the authors‚Äô original contribution. Consequently, data accessibility may depend on WebSurg‚Äôs licensing policies, which could change and affect reproducibility.  \n2. **Data Quality and Hallucinations**: Although the authors attempt to control for hallucinations, the dataset is automatically generated and may still contain inaccuracies. The provided ChatGPT and human evaluations alone may be insufficient to comprehensively assess data reliability.  \n3. **Model Availability and Reproducibility**: The model itself is currently unavailable, limiting the ability of others to reproduce results. However, the paper includes sufficient methodological detail to understand the system design.  \n4. **Ethical and Legal Considerations**: The manuscript raises concerns about the use of WebSurg data for training deep learning models, given unclear licensing permissions. Clarification on how usage aligns with WebSurg‚Äôs terms is advisable.  \n5. **Evaluation Scope**: The reviewer suggests additional validation, such as testing LLaVA-Surg on the SSG-VQA dataset, to demonstrate generalizability across datasets. Further explanation or examples of hallucination types in model outputs would also strengthen the work.\n\n---\n\n**Minor Comments**  \n- The presentation is clear and logically structured.  \n- Figures and descriptions effectively illustrate the workflow and comparisons.  \n- Minor linguistic refinements could further improve readability.\n\n---\n\n**Summary Paragraph**  \nOverall, the paper offers a meaningful contribution to surgical AI by providing a large multimodal dataset and an associated conversational model. It is well written, and experimental results are consistent with the stated objectives. The main weaknesses concern data licensing, potential hallucination issues, and lack of model accessibility. Addressing these points would significantly enhance reproducibility and robustness.  \n\n---\n\n**Decision Recommendation**: **Minor Revision** (acceptance recommended after addressing data licensing clarification, hallucination analysis, and model availability).",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to‚ÄØTMI**  \n   The paper introduces **LLaVA‚ÄëSurg**, a multimodal vision‚Äìlanguage model for surgical video understanding and conversation, together with **Surg‚ÄëQA**, a 102‚ÄØk‚Äëpair surgical video question‚Äìanswer dataset generated by a structured two‚Äëstage pipeline.  Although the target use case is the surgical domain, the contribution is largely methodological: building and evaluating a multimodal AI system that couples LLMs with video encoders.  That aligns reasonably with *IEEE‚ÄØTMI*‚Äôs scope of imaging methodology, machine learning, and computational analysis rather than purely clinical application. ‚úÖ\n\n2. **Novelty & Contribution Level**  \n   The novelty lies in (i) a two‚Äëstage structured LLM pipeline to generate video‚ÄìQA pairs that aim to reduce hallucination, and (ii) adaptation of visual‚Äìlanguage architectures (CLIP‚ÄØ+‚ÄØLLaMA) to model spatiotemporal surgical video data.  The proposed dataset scale is unprecedented for surgery videos.  However, parts of the model architecture and evaluation pipeline are close to prior‚ÄØart (Video‚ÄëLLaVA, Video‚ÄëChatGPT), so the conceptual advance is moderate though useful.  The main novelty is practical‚Äîdata generation and domain translation‚Äîrather than a new imaging theory.\n\n3. **Technical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \n   ‚Äì‚ÄØImplementation details are explicitly documented: dataset sources (WebSurg,‚ÄØCholecT50), number of procedures, training setup, hardware, and hyperparameters.  \n   ‚Äì‚ÄØThe two‚Äëstage data pipeline is well motivated but only partially validated; quality control of automatically generated QA pairs appears anecdotal.  \n   ‚Äì‚ÄØQuantitative evaluation relies heavily on GPT‚Äëbased automatic scoring, supplemented by a small (n‚ÄØ=‚ÄØ60) clinician validation showing correlation. Statistical significance or inter‚Äërater reliability is not reported.  \n   ‚Äì‚ÄØReproducibility is strong: authors state release of code, weights, and data; therefore experiments could be replicated.\n\n4. **Clarity & Presentation**  \n   Manuscript is clear, with appropriate figures, tables, and consistent terminology.  Some minor grammatical and spacing issues (‚ÄúLLaV‚ÄØA‚ÄëSurg‚Äù artifacts) and typographical inconsistencies need correction, but overall readability is high.\n\n5. **Ethical‚ÄØand‚ÄØReproducibility‚ÄØCompliance**  \n   Data originate from publicly available, de‚Äëidentified surgical videos (WebSurg); no patient PII is included.  Authors commit to open‚Äësourcing under Creative‚ÄØCommons‚ÄØBY‚ÄëNC‚ÄØ4.0.  Ethical and licensing aspects appear properly considered.\n\n---\n\n**Phase‚ÄØ2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n1. **Summary**  \n   The manuscript proposes **LLaVA‚ÄëSurg**, a surgical video conversational assistant built by fine‚Äëtuning a vision‚Äìlanguage model (CLIP‚ÄØ+‚ÄØLLaMA) on **Surg‚ÄëQA**, a new dataset of 102‚ÄØk automatically generated surgical video QA pairs.  The dataset is produced via a two‚Äëstage pipeline that extracts structured observations, reasons, and plans from transcribed surgical lectures using‚ÄØLLM prompting, aiming to reduce hallucination and cost.  The model is evaluated on zero‚Äëshot surgical video QA tasks, outperforming general‚Äëdomain multimodal baselines (Video‚ÄëLLaVA,‚ÄØVideo‚ÄëChatGPT) and correlating well with clinician scoring.\n\n2. **Strengths**  \n   * First large‚Äëscale multimodal conversational dataset for surgery videos.  \n   * Practical innovation in structured Q&A generation mitigating hallucination.  \n   * Demonstrated improvement over multimodal baselines and quantitative clinician validation.  \n   * Clear reproducibility plan with public release of data and code.\n\n3. **Weaknesses**  \n   * Architectural novelty is incremental; the framework closely follows prior general‚Äëdomain models.  \n   * Evaluation heavily depends on GPT‚Äëscoring without strong human‚Äëstudy validation.  \n   * Lack of rigorous uncertainty or error analysis; limited ablation on two‚Äëstage pipeline benefits.  \n   * Clinical or imaging physics insights limited‚Äîtargets more at AI engineering than imaging methodology per‚ÄØse.  \n   * Minor clarity and formatting issues.\n\n4. **Major‚ÄØComments**  \n   1. **Positioning for TMI readers.**‚ÄØClarify how the work advances medical‚Äëimaging methodology beyond dataset creation.  Discuss broader implications for quantitative surgical video analysis (e.g., phase recognition, tool tracking).  \n   2. **Validation robustness.**‚ÄØProvide a larger‚Äëscale human evaluation, or complementary objective metrics (e.g., BLEU/ROUGE, factual accuracy vs‚ÄØreference).  Report inter‚Äërater consistency between clinicians and‚ÄØGPT.  \n   3. **Ablation on pipeline.**‚ÄØQuantify how the two‚Äëstage generation reduces hallucination relative to end‚Äëto‚Äëend prompting across multiple samples, not just anecdotal examples.  \n   4. **Generalizability.**‚ÄØEvaluate transfer to unseen surgical categories or other datasets (e.g.,‚ÄØCholec80‚ÄëVQA) to demonstrate domain robustness.  \n   5. **Ethical/compliance detail.**‚ÄØAdd explicit statement of patient de‚Äëidentification and reuse permissions from WebSurg to conform to‚ÄØTMI reproducibility policies.\n\n5. **Minor‚ÄØComments**  \n   * Fix typographical spacing (‚ÄúLLaV‚ÄØA‚ÄëSurg‚ÄØ‚Üí‚ÄØLLaVA‚ÄëSurg‚Äù), unify variable notation.  \n   * Some figures (e.g., Fig‚ÄØ4) contain small text‚Äîenlarge for print quality.  \n   * Refer to ICLR in‚Äëreview footers should be removed for journal submission.  \n   * Include statistical uncertainties (mean‚ÄØ¬±‚ÄØSD) in Table‚ÄØ3.  \n   * Verify citation formatting consistency (author‚Äìyear vs numeric).  \n\n6. **Overall‚ÄØRecommendation**  \n   **Major‚ÄØRevision.**  \n   The work is promising and methodologically sound but requires strengthened experimental validation and clearer alignment to TMI‚Äôs methodological focus.  With improved quantitative and statistical analyses, the contribution could meet TMI standards.\n\n7. **Confidence‚ÄØLevel:**‚ÄØ4‚ÄØ/‚ÄØ5‚ÄØ‚Äì‚ÄØHigh confidence in technical assessment; moderate confidence in ultimate journal fit.\n\n---\n\n**Summary Judgment:**  \nA solid and timely paper bridging multimodal LLMs and surgical video understanding.  Methodological rigor is acceptable, novelty moderate, and reproducibility excellent.  Recommend *Major‚ÄØRevision* to add comparative and statistical depth and to better articulate methodological innovation for medical‚Äëimaging readership.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **LLaVA‚ÄëSurg**, a multimodal vision‚Äìlanguage model designed for surgical video understanding and conversational assistance, supported by **Surg‚ÄëQA**, a newly developed dataset containing 102‚ÄØk surgical video question‚Äìanswer pairs created via a structured two‚Äëstage LLM‚Äëbased pipeline. The approach integrates CLIP and LLaMA architectures to model spatiotemporal surgical data. The paper‚Äôs objectives are clear‚Äîto advance automated surgical video comprehension and enable domain‚Äëspecific dialogue systems. Overall presentation is coherent, figures and terminology are appropriate, and the writing is generally clear, though minor formatting corrections are needed. The work aligns with imaging and computational methodology rather than purely clinical application.  \n\n---\n\n**Major Comments**  \n1. **Scope and Positioning:** Clarify the methodological contributions beyond dataset creation. Better articulate how this work advances surgical video analysis for imaging or computational research audiences.  \n2. **Novelty:** The architectural design closely follows prior frameworks such as Video‚ÄëLLaVA and Video‚ÄëChatGPT; the primary innovation lies in data generation and domain adaptation. Discuss how these elements advance current multimodal learning approaches.  \n3. **Evaluation Robustness:** The quantitative results rely largely on GPT‚Äëbased automatic scoring supplemented by a small (n‚ÄØ=‚ÄØ60) clinician study. Expand human evaluation or include objective language‚Äëbased metrics (e.g., BLEU/ROUGE) and report inter‚Äërater consistency.  \n4. **Pipeline Validation:** The two‚Äëstage generation method is only anecdotally supported. Provide systematic analysis showing how it reduces hallucination compared to end‚Äëto‚Äëend prompting.  \n5. **Generalizability:** Assess transfer to other surgical categories or datasets (e.g., Cholec80) to demonstrate robustness beyond the training domain.  \n6. **Ethical and Reproducibility Detail:** Explicitly state patient de‚Äëidentification steps, source permissions, and compliance with data reuse policies.  \n\n---\n\n**Minor Comments**  \n- Correct typographical inconsistencies (e.g., ‚ÄúLLaV‚ÄØA‚ÄëSurg‚Äù to ‚ÄúLLaVA‚ÄëSurg‚Äù) and unify notation.  \n- Enlarge small text in selected figures (e.g., Fig‚ÄØ4) for print clarity.  \n- Remove external submission footers and ensure citation style consistency.  \n- Include uncertainty values (mean‚ÄØ¬±‚ÄØSD) in quantitative tables.  \n\n---\n\n**Summary Paragraph**  \nThis study delivers a substantial, well‚Äëdocumented dataset and demonstrates a practical integration of vision‚Äìlanguage modeling for surgical videos. Strengths include clear reproducibility through planned data and code release and effective demonstration of domain‚Äëspecific conversational capabilities. Weaknesses concern limited architectural novelty, heavy reliance on automated evaluation, and insufficient quantitative and statistical validation. The contribution is strongest in engineering implementation and dataset scale, while conceptual advances remain moderate. Strengthening experimental and statistical evidence would make the work more robust and impactful.  \n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe manuscript is technically solid and timely but requires enhanced experimental validation, more rigorous statistical analysis, and clearer articulation of methodological innovation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces LLaVA-Surg, a multimodal large language model designed for surgical video understanding and conversation. The authors create Surg-QA, a dataset of 102,000 surgical video-instruction pairs derived from 44,000 video clips across 2,201 surgical procedures (Page 1, lines 006-009). The core methodological contribution is a novel two-stage question-answer generation pipeline that first extracts structured information (observation, reason, plan, deduction) from surgical lecture transcripts using Llama-3-70B, then generates instruction-tuning data (Figure 2, Section 3). LLaVA-Surg builds upon Video-ChatGPT architecture, combining CLIP visual encoder with Llama language backbone (Page 2, lines 054-056). Evaluation on zero-shot surgical video question-answering shows LLaVA-Surg achieving 2.45/5.0 score compared to 1.32 for Video-LLaVA and 1.04 for Video-ChatGPT (Table 3). The authors validate their GPT-based evaluation framework against clinician assessments (Figure 5).\n\n## Weaknesses\n\n‚Ä¢ **Limited mathematical rigor in model formulation**\n  - Equation 1 provides only high-level notation without specifying video encoding dimensions or feature extraction details\n  - Equation 2 lacks mathematical precision in defining multi-turn conversation structure, particularly the conditional logic\n  - The temporal-fusion operation description (Page 7, lines 324-330) lacks formal mathematical definition of average-pooling operations and concatenation dimensions\n  - No formal loss function or training objective is provided despite claims of end-to-end instruction tuning\n\n‚Ä¢ **Insufficient evaluation methodology and baselines**\n  - GPT-3.5-turbo evaluation on only 4,359 test pairs (Page 7, lines 377-381) may not capture full model capabilities across surgical specialties\n  - Clinician validation limited to 60 samples from only two experts (Page 8, lines 419-427), insufficient for statistical significance\n  - No comparison with recent surgical VQA methods like SurgicalGPT (Seenivasan et al., 2023) which is cited but not benchmarked\n  - Accuracy@1 and accuracy@all metrics (Table 3) lack clear mathematical definition and clinical relevance justification\n\n‚Ä¢ **Dataset quality and bias concerns**\n  - Two-stage generation pipeline claims to reduce hallucination (Page 4, lines 189-195) but provides only anecdotal evidence in Figure 3\n  - Heavy reliance on WebSurg source (2,151 videos, Page 4, line 216) introduces potential institutional and procedural bias\n  - No inter-annotator agreement or quality control metrics for the 102K generated pairs\n  - Surgical knowledge pyramid (Figure 1) categorization lacks validation from surgical education experts\n\n‚Ä¢ **Technical architecture limitations**\n  - Architecture description (Page 6, lines 324-330) lacks novelty, essentially adapting existing Video-ChatGPT without surgical-specific modifications\n  - Training details show only 6 hours on 8 A100 GPUs (Page 7, lines 366-367) which may be insufficient for complex surgical reasoning\n  - No ablation studies on key components like temporal fusion or the structured information extraction approach\n  - Missing details on handling variable-length surgical procedures and multi-stage operations\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical formalization and technical rigor**\n  - Provide detailed mathematical formulation of the temporal-fusion operation including tensor dimensions and operations\n  - Define formal loss functions for instruction tuning and specify optimization objectives clearly\n  - Add rigorous mathematical definition of the multi-turn conversation structure beyond Equation 2\n  - Include formal analysis of the two-stage generation pipeline's effect on reducing hallucination probability\n\n‚Ä¢ **Strengthen evaluation framework and expand baselines**\n  - Conduct evaluation on larger test sets with stratification across surgical specialties represented in Figure 4c\n  - Recruit more clinical experts (minimum 5-10) and evaluate larger sample sizes (500+ cases) for statistical significance\n  - Include comprehensive comparison with SurgicalGPT and other recent surgical VQA methods with standardized metrics\n  - Provide clear mathematical definitions and clinical relevance justification for accuracy@1 and accuracy@all metrics\n\n‚Ä¢ **Address dataset bias and quality validation**\n  - Implement systematic quality control with inter-annotator agreement metrics for generated question-answer pairs\n  - Include datasets from multiple sources beyond WebSurg to reduce institutional bias\n  - Validate the surgical knowledge pyramid framework with surgical education specialists\n  - Provide quantitative analysis of hallucination reduction beyond the single example in Figure 3\n\n‚Ä¢ **Improve technical contributions and experimental design**\n  - Introduce surgical-specific architectural modifications rather than directly adapting Video-ChatGPT\n  - Conduct comprehensive ablation studies on temporal fusion, structured extraction, and multi-stage training\n  - Extend training duration and provide learning curves to demonstrate convergence\n  - Add detailed analysis of performance across different surgical procedure types and complexity levels",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *LLaVA-Surg*, a multimodal large language model tailored for surgical video understanding and conversational reasoning. It introduces *Surg-QA*, a dataset comprising 102,000 video‚Äìinstruction pairs derived from 44,000 video clips across 2,201 procedures. Methodologically, the work proposes a two-stage question‚Äìanswer generation pipeline that extracts structured reasoning from surgical lecture transcripts and uses this to generate instruction-tuning data. The model architecture extends *Video-ChatGPT* by integrating a CLIP visual encoder with a Llama language backbone. Evaluation results suggest improvements in zero-shot surgical QA over prior baselines, and an auxiliary clinician evaluation is reported. While the manuscript addresses an important application area and demonstrates technical implementation, there are substantial concerns regarding methodological rigor, dataset validity, and experimental depth.  \n\n**Major Comments**  \n1. **Mathematical and Technical Rigor**: The presented equations remain conceptual, lacking clear dimensional definitions, mathematical precision for the multi-turn conversation logic, or a formally defined training objective. The temporal-fusion operation is insufficiently specified.  \n2. **Evaluation Methodology and Baselines**: The GPT-based evaluation covers a limited subset (4,359 test pairs), and clinician validation relies on only two experts assessing 60 samples. Comparisons with recent surgical VQA models (e.g., SurgicalGPT) are absent. Additionally, the metrics (accuracy@1 and accuracy@all) are not rigorously defined or justified in clinical terms.  \n3. **Dataset Quality and Possible Bias**: The authors‚Äô claim of hallucination reduction is supported only by anecdotal evidence. The heavy reliance on WebSurg videos introduces institutional bias, and no annotation quality measures or inter-annotator agreement statistics are provided. The ‚Äúsurgical knowledge pyramid‚Äù lacks external validation.  \n4. **Architecture and Experimental Limitations**: The method appears to adapt existing Video-ChatGPT structures with minimal domain-specific innovation. Training appears brief relative to task complexity, and no ablation studies or analyses of variable-length or multi-stage procedures are presented.  \n\n**Minor Comments**  \n- Clarify tensor dimensions and mathematical notation in figures and equations.  \n- Specify metric formulations and ensure consistency in tables and captions.  \n- Provide clearer descriptions of figure references and line numbers.  \n\n**Summary Paragraph**  \nOverall, the study contributes an ambitious attempt to adapt large multimodal models for surgical video understanding and creates a sizable dataset resource. However, the current manuscript lacks mathematical formality, comprehensive evaluation, and convincing validation of dataset quality or clinical relevance. Technical novelty is limited by close dependence on prior architectures, and evaluation methodology is underdeveloped. Addressing these issues would be necessary to demonstrate robustness and credibility for this specialized application domain.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work shows potential significance but requires substantial strengthening in technical rigor, evaluation design, and dataset validation before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces LLaV A-Surg, a novel multimodal vision-language conversational assistant designed to understand and engage in conversations about surgical videos. The authors address the scarcity of specialized datasets for training such models by proposing Surg-QA, a large-scale dataset of surgical video-instruction pairs generated through a two-stage question-answer generation pipeline. The authors claim that LLaV A-Surg significantly outperforms existing models in zero-shot surgical video question-answering tasks, demonstrating robust multimodal conversational skills.\n\n###\n\n## Major Comments\n1. Novelty and Positioning. The creation of Surg-QA and the development of LLaV A-Surg are significant contributions, especially considering the lack of specialized datasets for surgical video understanding. However, the manuscript could benefit from a more thorough discussion of how this work compares to recent advancements in multimodal LLMs for video understanding. While the authors mention several related works, a more detailed comparison with models like Video-LLaV A and Video-ChatGPT would strengthen the novelty argument.\n   \n2. Evaluation Design. The evaluation is primarily conducted on Surg-QA, which is a valuable resource but also poses a limitation since the model is evaluated on the same dataset it was trained on. Including external validation on independent datasets or real-world surgical scenarios would enhance the credibility of the results. Additionally, the authors should consider evaluating the model's performance on different surgical procedures to assess its generalizability.\n\n3. Comparison with Baselines. The comparison with existing models (LLaV A-Med, Video-LLaV A, and Video-ChatGPT) is informative, but the authors should include a broader range of baselines, particularly those that have demonstrated strong performance in multimodal tasks. Including more recent state-of-the-art models would provide a clearer picture of LLaV A-Surg's advancements.\n\n4. Reproducibility. The authors state that they will release the Surg-QA dataset, model, and code, which is commendable. However, the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters used in the experiments. Providing this information is crucial for reproducibility and transparency.\n\n###\n\n## Minor Comments\n1. Figures and Clarity: Figures 1 and 2 are well-designed but could benefit from additional labels and legends to clarify the components and steps involved in the data generation pipeline.\n2. Notation Consistency: The notation used in Section 2.1 is inconsistent, particularly the forward operator, which should be clarified.\n3. Acronyms: Several acronyms (e.g., \"Q&A\", \"GPT\") are used without definition, which can be confusing for readers unfamiliar with the terminology.\n4. Typographical Errors: Minor typographical errors, such as \"k-spacce\" and \"undersampling maskes,\" should be corrected.\n\n###\n\n## Summary Paragraph\n(Focusing on 4 TMI Editorial Criteria)\nThe manuscript tackles a significant challenge in developing a specialized multimodal conversational assistant for surgical videos, addressing a gap in the availability of relevant datasets. The creation of Surg-QA is a noteworthy contribution, providing a large-scale resource for training and evaluation. The proposed two-stage question-answer generation pipeline is innovative and addresses the issue of LLM hallucinations effectively. However, the evaluation primarily relies on the training dataset, which limits the generalizability of the findings. Additionally, while the authors aim to release the dataset, model, and code, more detailed methodological descriptions are required for full reproducibility. Overall, the manuscript has significant potential but requires additional validation and clarity to fully meet the standards of TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis, broaden the validation to include independent datasets, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThis manuscript presents **LLaVA-Surg**, a multimodal vision-language conversational assistant designed for understanding and engaging in surgical video discussions. To address the lack of specialized training data, the authors introduce **Surg-QA**, a large-scale dataset of surgical video‚Äìinstruction pairs generated via a two-stage question‚Äìanswer creation pipeline. The study claims that LLaVA-Surg substantially outperforms previous models in zero-shot surgical video question-answering, demonstrating strong multimodal conversational capability. Overall, the manuscript is clearly written and contributes to an emerging research area at the intersection of surgical data science and multimodal large language models.\n\n---\n\n**Major Comments**  \n1. **Novelty and Positioning**: The introduction of Surg-QA and LLaVA-Surg represents a valuable contribution, addressing the scarcity of domain-specific datasets. However, the manuscript would benefit from a more detailed discussion situating the proposed approach relative to recent multimodal LLMs for video understanding. Explicit comparisons with models such as Video-LLaVA and Video-ChatGPT would help clarify LLaVA-Surg‚Äôs distinct contributions and novelty.  \n\n2. **Evaluation Design**: The evaluation relies primarily on Surg-QA, which is appropriate for demonstrating internal performance but limits assessment of generalization. Validation using external datasets or real-world surgical footage would substantiate the model‚Äôs performance claims. Testing across different surgical procedures could further demonstrate generalizability.  \n\n3. **Comparative Baselines**: While comparisons with LLaVA-Med, Video-LLaVA, and Video-ChatGPT are informative, a broader set of baselines‚Äîespecially more recent or high-performing multimodal models‚Äîshould be considered to contextualize the improvements claimed.  \n\n4. **Reproducibility**: The authors indicate that the dataset, model, and code will be released, which is positive for transparency. Nonetheless, the paper should include more comprehensive details on training protocols, preprocessing steps, and hyperparameters to ensure replicability.\n\n---\n\n**Minor Comments**  \n1. *Figures*: Figures 1 and 2 are effectively designed but would benefit from additional labeling and legends to clarify the pipeline‚Äôs components and steps.  \n2. *Notation*: In Section 2.1, notation inconsistencies‚Äîparticularly regarding the forward operator‚Äîshould be corrected.  \n3. *Acronyms*: Several acronyms (e.g., ‚ÄúQ&A,‚Äù ‚ÄúGPT‚Äù) are introduced without definition and should be clarified for broader readership.  \n4. *Typographical Errors*: Minor typos such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes‚Äù require correction.\n\n---\n\n**Summary Paragraph**  \nThe study makes a meaningful contribution to surgical video understanding through the development of a specialized multimodal assistant and accompanying dataset. The Surg-QA dataset and two-stage question‚Äìanswer generation framework represent substantial advances, particularly in mitigating LLM hallucination. Nonetheless, reliance on an internally generated dataset raises questions about generalizability, and the lack of detailed methodological reporting hinders reproducibility. Strengthening comparative analysis, adding external validation, and expanding methodological transparency would significantly improve the manuscript‚Äôs rigor and impact.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The paper should deepen comparative analysis, validate results on independent datasets, and provide comprehensive implementation details to ensure reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## LLAVA-Surg: Towards Multimodal Surgical Assistant via Structured Lecture Learning\n\n### Summary\n\nThe paper introduces LLaVA-Surg, a multimodal conversational assistant for surgical videos, and Surg-QA, a new dataset of 102K video‚Äìinstruction pairs generated from surgical lecture videos using a two-stage LLM pipeline. The pipeline first extracts structured surgical knowledge (observation, reason, plan, deduction) from ASR transcripts and then generates multi-turn question‚Äìanswer pairs, with an additional 2K concept-alignment pairs from CholecT50. Fine-tuning a Video-ChatGPT-style architecture on this data yields improvements over general-domain baselines on a zero-shot evaluation conducted on Surg-QA, with supplementary clinician ratings showing rough alignment with GPT-based scoring.\n\n### Strengths\n\n- Technical novelty and innovationA clear, practical two-stage extraction‚Üígeneration pipeline that structures lecture transcripts before Q/A synthesis to reduce hallucinations and improve controllability.The knowledge decomposition into observation, reason, plan, and deduction concretely targets higher-level surgical reasoning beyond perception.Efficient training setup (frozen CLIP, lightweight video pooling) yields a workable surgical video assistant with modest compute.\n- Experimental rigor and validationProvides a large-scale, diverse corpus of surgical video clips covering a wide range of procedures and categories.Includes a small clinician validation that compares human ratings to GPT-based scoring to partially justify the automated evaluation approach.Reports multiple quantitative metrics (0‚Äì5 score, Accuracy@1, Accuracy@all) and qualitative examples.\n- Clarity of presentationThe data generation pipeline is presented with step-by-step prompts and few-shot examples, improving reproducibility.Architectural choices and training details are described succinctly and transparently (e.g., frozen visual encoder, learning rate, epochs).\n- Significance of contributionsSurg-QA adds a substantial volume of surgical video Q/A data, especially with reasoning-level content derived from expert lecture narrations.Demonstrates that adapting general video-chat architectures to the surgical domain with structured instruction tuning can meaningfully improve domain-specific performance.\n\nTechnical novelty and innovation\n\n- A clear, practical two-stage extraction‚Üígeneration pipeline that structures lecture transcripts before Q/A synthesis to reduce hallucinations and improve controllability.\n- The knowledge decomposition into observation, reason, plan, and deduction concretely targets higher-level surgical reasoning beyond perception.\n- Efficient training setup (frozen CLIP, lightweight video pooling) yields a workable surgical video assistant with modest compute.\n\nExperimental rigor and validation\n\n- Provides a large-scale, diverse corpus of surgical video clips covering a wide range of procedures and categories.\n- Includes a small clinician validation that compares human ratings to GPT-based scoring to partially justify the automated evaluation approach.\n- Reports multiple quantitative metrics (0‚Äì5 score, Accuracy@1, Accuracy@all) and qualitative examples.\n\nClarity of presentation\n\n- The data generation pipeline is presented with step-by-step prompts and few-shot examples, improving reproducibility.\n- Architectural choices and training details are described succinctly and transparently (e.g., frozen visual encoder, learning rate, epochs).\n\nSignificance of contributions\n\n- Surg-QA adds a substantial volume of surgical video Q/A data, especially with reasoning-level content derived from expert lecture narrations.\n- Demonstrates that adapting general video-chat architectures to the surgical domain with structured instruction tuning can meaningfully improve domain-specific performance.\n\n### Weaknesses\n\n- Technical limitations or concernsVideo modeling is limited to temporal/spatial average pooling with a frozen CLIP encoder; richer temporal modeling (attention/temporal transformers) is absent and may cap performance on fine-grained temporal reasoning.The ‚Äúreduced hallucination‚Äù claim for the two-stage pipeline is shown mostly via anecdotal examples; no systematic ablation quantifies hallucination rates versus end-to-end generation.\n- Experimental gaps or methodological issuesEvaluation is primarily on the authors‚Äô own Surg-QA test split using GPT-3.5 scoring that references the same extracted ‚Äúobservations,‚Äù raising concerns of circularity and bias toward the data generation pipeline.No comparisons to strong, domain-adapted surgical MLLMs (e.g., Surgical-LLaVA, S2CAN, SurgVidLM, SurgVLM, Surgery-R1), nor to broader surgical pretraining frameworks (PeskaVLP, SurgBench) that are directly relevant.The clinician study is small (n=60) and sampling is stratified by GPT score rather than random; correlation is claimed qualitatively without statistics (e.g., correlation coefficients, inter-rater agreement).\n- Clarity or presentation issuesSome equations and tensor shapes are imprecise for ViT/CLIP outputs; minor but could be clarified to avoid confusion.The description of how multi-turn history is encoded and evaluated is brief; it‚Äôs unclear how much the model leverages conversational context versus single-turn prompts.\n- Missing related work or comparisonsImportant contemporaneous surgical MLLMs and datasets are not cited or compared: Surgical-LLaVA (hierarchical instruction tuning from surgical annotations with GPT-3.5 expansion), S2CAN (memory-augmented surgical VQA), SurgVidLM (SVU-31K and StageFocus/MFA), SurgVLM (large-scale, hierarchical multimodal corpus and benchmark), Surgery-R1 (reasoning with RFT and grounding), PeskaVLP (LLM-augmented lecture pretraining), and SurgBench (foundation pretraining and evaluation). Some of these works also leverage lecture narrations/LLM augmentation, challenging ‚Äúfirst‚Äù claims.\n\nTechnical limitations or concerns\n\n- Video modeling is limited to temporal/spatial average pooling with a frozen CLIP encoder; richer temporal modeling (attention/temporal transformers) is absent and may cap performance on fine-grained temporal reasoning.\n- The ‚Äúreduced hallucination‚Äù claim for the two-stage pipeline is shown mostly via anecdotal examples; no systematic ablation quantifies hallucination rates versus end-to-end generation.\n\nExperimental gaps or methodological issues\n\n- Evaluation is primarily on the authors‚Äô own Surg-QA test split using GPT-3.5 scoring that references the same extracted ‚Äúobservations,‚Äù raising concerns of circularity and bias toward the data generation pipeline.\n- No comparisons to strong, domain-adapted surgical MLLMs (e.g., Surgical-LLaVA, S2CAN, SurgVidLM, SurgVLM, Surgery-R1), nor to broader surgical pretraining frameworks (PeskaVLP, SurgBench) that are directly relevant.\n- The clinician study is small (n=60) and sampling is stratified by GPT score rather than random; correlation is claimed qualitatively without statistics (e.g., correlation coefficients, inter-rater agreement).\n\nClarity or presentation issues\n\n- Some equations and tensor shapes are imprecise for ViT/CLIP outputs; minor but could be clarified to avoid confusion.\n- The description of how multi-turn history is encoded and evaluated is brief; it‚Äôs unclear how much the model leverages conversational context versus single-turn prompts.\n\nMissing related work or comparisons\n\n- Important contemporaneous surgical MLLMs and datasets are not cited or compared: Surgical-LLaVA (hierarchical instruction tuning from surgical annotations with GPT-3.5 expansion), S2CAN (memory-augmented surgical VQA), SurgVidLM (SVU-31K and StageFocus/MFA), SurgVLM (large-scale, hierarchical multimodal corpus and benchmark), Surgery-R1 (reasoning with RFT and grounding), PeskaVLP (LLM-augmented lecture pretraining), and SurgBench (foundation pretraining and evaluation). Some of these works also leverage lecture narrations/LLM augmentation, challenging ‚Äúfirst‚Äù claims.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe two-stage pipeline is well-motivated: constrained extraction before generation is a reasonable way to reduce hallucinations and ensure video-answerable content. The prompt designs and few-shot examples are helpful. However, the paper would be strengthened by a quantitative ablation showing reductions in hallucination (e.g., human audit of data quality for staged vs end-to-end pipelines).The model architecture is a faithful adaptation of LLaVA-style video pooling. While simple and efficient, the lack of temporal modeling (beyond mean pooling) may limit handling of transient cues, phase transitions, and cause‚Äìeffect reasoning common in surgery. Prior works such as SurgVidLM‚Äôs multi-frequency attention or memory mechanisms (S2CAN) suggest technical paths to improve temporal reasoning.\n- Experimental evaluation assessmentTraining/eval split by procedures is appropriate, and reporting training times/hyperparameters improves reproducibility. The core limitation is evaluating on Surg-QA with an automated scorer that uses the same extracted observations used in data creation. This risks aligned biases and may not reflect real-world surgical QA difficulty.Clinician evaluation provides a sanity check, but the small, stratified sample and lack of statistical reporting (Pearson/Spearman correlations, confidence intervals, inter-rater reliability) limit conclusions about validity of GPT scoring.Missing: cross-dataset testing on established surgical VQA benchmarks (EndoVis-18-VQA, Cholec80-VQA, SSG-VQA) and comparisons to domain-specialized baselines (S2CAN, Surgical-LLaVA, Surgery-R1, SurgVidLM, SurgVLM). Without these, it is hard to assess external validity, transfer, or true SOTA standing.\n- Comparison with related work (using the summaries provided)Surgical-LLaVA similarly adapts LLaVA-style architectures to surgical video and constructs instruction data via GPT-3.5; it also reports GPT-scored qualitative evaluation and VQA accuracy on public sets. The present paper‚Äôs novelty lies in the structured two-stage mining from lecture transcripts with explicit observation‚Üíreason‚Üíplan‚Üídeduction. This overlap should be discussed, and claims of being ‚Äúfirst‚Äù to enable surgical video conversations should be softened.PeskaVLP uses LLM-based hierarchical augmentation of lecture narrations and procedure-aware training to improve zero-shot transfer and temporal alignment. The idea of leveraging lecture transcripts with LLMs is not entirely new; this paper differentiates itself by transforming extraction into conversational QA pairs and targeting multi-turn dialogues.S2CAN shows memory-based augmentation within an MLLM to improve surgical VQA; Surgery-R1 integrates reasoning and grounding via SFT+RFT. These works highlight methods to reduce hallucination and improve contextual reasoning beyond simple pooling‚Äîrelevant to the current model‚Äôs limitations.SurgVidLM introduces a multi-grained training approach and MFA module, and SVU-31K (31K video‚Äìinstruction pairs) is smaller than Surg-QA, but sets competitive baselines and task designs for temporal/perceptual reasoning. A head-to-head comparison would clarify complementary strengths.SurgVLM (SurgVLM-DB and SurgVLM-Bench) emphasizes hierarchical, broad multimodal surgical corpora and standardized evaluation, again arguing for rigorous external evaluation beyond Surg-QA.SurgBench proposes large-scale pretraining and standardized evaluation across multiple surgical video tasks; leveraging such evaluation would position LLaVA-Surg within a broader ecosystem.\n- Discussion of broader impact and significanceThe dataset and pipeline can substantially catalyze research in surgical video-language understanding, especially for educational and offline analysis contexts. Open-sourcing code and data is impactful.Safety considerations are appropriately acknowledged (hallucinations). Stronger clinical validation, careful disclaimers, and out-of-distribution robustness studies would be needed for any clinical deployment.Potential bias: lecture narrations may include content not visually grounded; despite prompts discouraging non-visual references, empirical auditing is needed to verify ‚Äúvideo-answerable‚Äù adherence and mitigate leakage from text descriptions.\n\nTechnical soundness evaluation\n\n- The two-stage pipeline is well-motivated: constrained extraction before generation is a reasonable way to reduce hallucinations and ensure video-answerable content. The prompt designs and few-shot examples are helpful. However, the paper would be strengthened by a quantitative ablation showing reductions in hallucination (e.g., human audit of data quality for staged vs end-to-end pipelines).\n- The model architecture is a faithful adaptation of LLaVA-style video pooling. While simple and efficient, the lack of temporal modeling (beyond mean pooling) may limit handling of transient cues, phase transitions, and cause‚Äìeffect reasoning common in surgery. Prior works such as SurgVidLM‚Äôs multi-frequency attention or memory mechanisms (S2CAN) suggest technical paths to improve temporal reasoning.\n\nExperimental evaluation assessment\n\n- Training/eval split by procedures is appropriate, and reporting training times/hyperparameters improves reproducibility. The core limitation is evaluating on Surg-QA with an automated scorer that uses the same extracted observations used in data creation. This risks aligned biases and may not reflect real-world surgical QA difficulty.\n- Clinician evaluation provides a sanity check, but the small, stratified sample and lack of statistical reporting (Pearson/Spearman correlations, confidence intervals, inter-rater reliability) limit conclusions about validity of GPT scoring.\n- Missing: cross-dataset testing on established surgical VQA benchmarks (EndoVis-18-VQA, Cholec80-VQA, SSG-VQA) and comparisons to domain-specialized baselines (S2CAN, Surgical-LLaVA, Surgery-R1, SurgVidLM, SurgVLM). Without these, it is hard to assess external validity, transfer, or true SOTA standing.\n\nComparison with related work (using the summaries provided)\n\n- Surgical-LLaVA similarly adapts LLaVA-style architectures to surgical video and constructs instruction data via GPT-3.5; it also reports GPT-scored qualitative evaluation and VQA accuracy on public sets. The present paper‚Äôs novelty lies in the structured two-stage mining from lecture transcripts with explicit observation‚Üíreason‚Üíplan‚Üídeduction. This overlap should be discussed, and claims of being ‚Äúfirst‚Äù to enable surgical video conversations should be softened.\n- PeskaVLP uses LLM-based hierarchical augmentation of lecture narrations and procedure-aware training to improve zero-shot transfer and temporal alignment. The idea of leveraging lecture transcripts with LLMs is not entirely new; this paper differentiates itself by transforming extraction into conversational QA pairs and targeting multi-turn dialogues.\n- S2CAN shows memory-based augmentation within an MLLM to improve surgical VQA; Surgery-R1 integrates reasoning and grounding via SFT+RFT. These works highlight methods to reduce hallucination and improve contextual reasoning beyond simple pooling‚Äîrelevant to the current model‚Äôs limitations.\n- SurgVidLM introduces a multi-grained training approach and MFA module, and SVU-31K (31K video‚Äìinstruction pairs) is smaller than Surg-QA, but sets competitive baselines and task designs for temporal/perceptual reasoning. A head-to-head comparison would clarify complementary strengths.\n- SurgVLM (SurgVLM-DB and SurgVLM-Bench) emphasizes hierarchical, broad multimodal surgical corpora and standardized evaluation, again arguing for rigorous external evaluation beyond Surg-QA.\n- SurgBench proposes large-scale pretraining and standardized evaluation across multiple surgical video tasks; leveraging such evaluation would position LLaVA-Surg within a broader ecosystem.\n\nDiscussion of broader impact and significance\n\n- The dataset and pipeline can substantially catalyze research in surgical video-language understanding, especially for educational and offline analysis contexts. Open-sourcing code and data is impactful.\n- Safety considerations are appropriately acknowledged (hallucinations). Stronger clinical validation, careful disclaimers, and out-of-distribution robustness studies would be needed for any clinical deployment.\n- Potential bias: lecture narrations may include content not visually grounded; despite prompts discouraging non-visual references, empirical auditing is needed to verify ‚Äúvideo-answerable‚Äù adherence and mitigate leakage from text descriptions.\n\n### Questions for Authors\n\n- Can you provide a quantitative ablation comparing data quality between the proposed two-stage pipeline and a direct end-to-end generation pipeline (e.g., human-rated hallucination rate, factuality, and ‚Äúvideo-answerability‚Äù on a stratified sample)?\n- How do you ensure that generated QA pairs are genuinely answerable from video alone rather than leveraging lecture-specific textual knowledge? Did you audit a sample to estimate the proportion of text-only answerable items?\n- Why were comparisons to domain-specific surgical MLLMs (Surgical-LLaVA, S2CAN, SurgVidLM, SurgVLM, Surgery-R1) omitted? Can you include results on public benchmarks (EndoVis-18-VQA, Cholec80-VQA, SSG-VQA) to establish external validity and SOTA claims?\n- Could you report statistical measures for the clinician vs GPT evaluation (e.g., Pearson/Spearman correlation, inter-rater reliability), and describe the selection rationale for the 60 samples to avoid confirmation bias?\n- What is the impact of the small concept-alignment set (‚âà2K from CholecT50)? Can you provide an ablation of training with/without this subset?\n- How sensitive is the model to frame sampling rate and the number of frames? Have you explored lightweight temporal modules (e.g., temporal attention) to complement average pooling?\n- Are there license or usage constraints when distributing URLs or derived annotations from WebSurg content? Please clarify legal and ethical compliance, including any agreements or approvals.\n- Can you release a subset of Surg-QA with independent human-verified ground truth to facilitate fair, model-agnostic evaluation without relying on GPT scorers or the same extraction pipeline?\n\n### Overall Assessment\n\nThis work presents a valuable dataset and a practical pipeline for constructing multimodal conversational training data from surgical lectures, coupled with a straightforward yet effective model adaptation. The structured two-stage extraction‚Üígeneration approach is a thoughtful contribution likely to improve controllability and reduce hallucinations relative to direct prompting. The dataset‚Äôs breadth and emphasis on reasoning (reason/plan/deduction) are particularly useful for advancing surgical video understanding. However, the current evaluation is largely inward-facing: it uses a scorer aligned with the data-generation pipeline and lacks comparisons to recent, relevant surgical MLLMs on established public benchmarks. Claims of being the ‚Äúfirst‚Äù expert-level surgical video conversation model are overstated given contemporaneous works. Strengthening the evaluation with external benchmarks, domain-specific baselines, human audits of data quality, and statistical analysis of clinician correlations would significantly bolster the paper. With these additions and a moderated positioning relative to prior work, the contribution would be strong and of high interest to the multimodal medical AI community.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe paper introduces **LLaVA-Surg**, a multimodal conversational assistant for surgical videos, and **Surg-QA**, a large dataset of 102K video‚Äìinstruction pairs derived from surgical lecture transcripts through a two-stage extraction‚Äìgeneration pipeline. The approach decomposes lecture transcripts into structured surgical knowledge (observation, reason, plan, deduction) before producing multi-turn question‚Äìanswer pairs, supplemented by 2K concept-alignment pairs from CholecT50. Fine-tuning a Video-ChatGPT-style architecture on this dataset yields improved zero-shot results on Surg-QA, with clinician assessments showing rough consistency with automated GPT-based scoring. The manuscript is detailed, logically organized, and technically sound, though its claims and evaluation scope warrant broader validation.  \n\n---\n\n**Major Comments**  \n1. **Evaluation Scope and External Comparisons** ‚Äì The evaluation relies primarily on the Surg-QA test split scored by GPT-3.5 using the same extracted ‚Äúobservations,‚Äù raising concerns of circularity and bias. No experiments are conducted on public benchmarks (EndoVis-18-VQA, Cholec80-VQA, SSG-VQA), nor are comparisons made to strong, domain-adapted surgical MLLMs such as Surgical-LLaVA, S2CAN, SurgVidLM, SurgVLM, or Surgery-R1. This limits external validity and SOTA positioning.  \n2. **Quantitative Validation of Hallucination Reduction** ‚Äì The claim that the two-stage pipeline reduces hallucination is only supported by anecdotal examples; no quantitative ablation or human audit demonstrates this effect.  \n3. **Modeling Limitations** ‚Äì Video modeling depends on temporal/spatial average pooling of frozen CLIP features. The absence of richer temporal modeling (e.g., attention or transformers) constrains fine-grained temporal and causal reasoning.  \n4. **Clinician Evaluation** ‚Äì The clinician validation (n=60) is small and stratified by GPT score, lacking statistical analysis (e.g., correlations, inter-rater reliability). This weakens confidence in the claimed alignment between human and GPT evaluations.  \n5. **Positioning and Related Work** ‚Äì Several contemporary works using LLMs and surgical video data (e.g., Surgical-LLaVA, PeskaVLP, S2CAN, SurgVidLM, Surgery-R1, SurgVLM, SurgBench) are unacknowledged or not compared, challenging the ‚Äúfirst‚Äù claims and obscuring the incremental contribution.  \n\n---\n\n**Minor Comments**  \n- Clarify tensor shapes and notation for ViT/CLIP outputs to prevent confusion.  \n- Provide more detail on how conversational history is encoded and utilized during multi-turn evaluation.  \n- Discuss the role and impact of the 2K concept-alignment subset from CholecT50.  \n- Include statistical metrics (e.g., correlation coefficients) when reporting clinician versus GPT agreement.  \n- Verify ethical and copyright compliance when using lecture video data from WebSurg.  \n\n---\n\n**Summary Paragraph**  \nLLaVA-Surg presents a meaningful contribution by introducing structured extraction‚Äìgeneration for building large-scale surgical QA data and adapting general video‚Äìlanguage architectures to a clinical domain. The work is strong in technical transparency, dataset scale, and methodological clarity. However, its validation remains inward-looking, with limited external benchmarking, small-scale clinician study, and unquantified hallucination analysis. Clarifying its relationship to concurrent MLLMs and incorporating human-verified evaluations would significantly enhance robustness and credibility.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The study offers a promising dataset and pipeline but requires broader comparative evaluation, quantitative validation of its claims, and stronger statistical and methodological support before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: LLAVA-SURG: TOWARDS MULTIMODAL SURGICAL ASSISTANT VIA STRUCTURED LECTURE LEARNING\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical gap in surgical video understanding by introducing Surg-QA, a novel large-scale dataset of 102,000 surgical video-instruction pairs derived from 44,000 video clips across 2,201 surgical procedures. The authors propose a two-stage question-answer generation pipeline that first extracts structured surgical knowledge (encompassing four levels of understanding from basic object identification to expert-level deduction) from surgical lecture videos, then generates high-quality instruction-tuning data while mitigating LLM hallucination. Building upon this dataset, they develop LLaVA-Surg, a vision-language conversational assistant specifically trained for surgical video understanding. The results demonstrate that LLaVA-Surg significantly outperforms existing general-domain multimodal models in zero-shot surgical video question-answering tasks, achieving a score of 2.45 (vs. 1.32 for Video-LLaVA) on a 0-5 scale, with strong alignment between GPT and clinician evaluations confirming the model's surgical domain competence.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The two-stage pipeline for surgical video data generation is a significant methodological contribution that effectively mitigates LLM hallucination while maintaining cost-effectiveness compared to premium LLM services\n- The four-level surgical knowledge pyramid (Object, Action, Reason, Plan & Deduction) provides a valuable conceptual framework for understanding surgical video complexity\n- The comprehensive dataset statistics (Figure 4) demonstrate unprecedented scale and diversity in surgical video instruction data, covering multiple surgical domains and knowledge levels\n- The clinician evaluation showing strong correlation with GPT scoring provides compelling evidence for the evaluation framework's validity\n\n**Limitations:**\n- The paper lacks a clear explanation of how the Surg-QA dataset was partitioned for training and testing, particularly regarding whether procedures were kept in exclusive sets to prevent data leakage\n- The evaluation metrics (0-5 scoring) lack detailed inter-rater reliability analysis between clinicians, which is critical for medical domain validation\n- While the two-stage approach reduces hallucination, the paper doesn't provide quantitative metrics on hallucination reduction compared to end-to-end approaches\n- The model's limitations in handling complex temporal reasoning across long surgical procedures aren't thoroughly analyzed, despite surgical videos often containing multi-step sequences\n\n### Minor Comments\n\n**Strengths:**\n- The visual examples (Figures 5-8) effectively illustrate the model's capabilities and the quality of generated instruction data\n- The detailed prompt engineering (Appendix A) enhances reproducibility of the data generation pipeline\n- The comparison with both general-domain and surgical-specific VQA datasets (Tables 1-2) clearly positions Surg-QA's unique contribution\n- The open-source commitment to release code, model, and dataset aligns well with TMI's reproducibility standards\n\n**Limitations:**\n- The paper would benefit from more detailed error analysis of LLaVA-Surg's failures, particularly regarding specific surgical domains where performance lags\n- Some figures (e.g., Figure 4) contain overlapping text that reduces readability\n- The computational requirements for the two-stage pipeline are mentioned but not fully quantified in terms of total processing time/cost\n- The paper would be strengthened by including comparative training time for the two-stage approach versus end-to-end methods\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a critical gap in surgical AI by providing the first large-scale dataset specifically designed for surgical video understanding and a specialized model that demonstrates superior performance over general-domain alternatives. The clinical relevance is substantial as surgical video understanding can support surgical training, intraoperative assistance, and procedural documentation. The impact is amplified by the open-source release, which will accelerate research in this niche but important medical domain.\n\n**Innovation:** The paper introduces a novel two-stage pipeline that systematically extracts structured surgical knowledge before generating instruction data, representing a significant methodological innovation. The four-level surgical knowledge pyramid provides a new conceptual framework for understanding surgical video complexity. While building upon existing multimodal architectures, the adaptation specifically for surgical video with temporal modeling represents a meaningful advancement beyond general video understanding models.\n\n**Evaluation:** The evaluation is comprehensive, including quantitative metrics (0-5 scoring), clinician validation, and comparative analysis against multiple baselines. The alignment between GPT and clinician scoring (Figure 5) provides confidence in the evaluation methodology. However, the assessment would be strengthened by more detailed error analysis, inter-rater reliability metrics for clinician evaluation, and specific analysis of performance across different surgical domains and knowledge levels.\n\n**Reproducibility:** The paper provides exceptional detail on the data generation pipeline, including complete prompts (Appendix A), which greatly enhances reproducibility. The model architecture, training parameters, and evaluation protocols are thoroughly documented. The commitment to release Surg-QA, LLaVA-Surg, and all code aligns perfectly with TMI's reproducibility standards. The only limitation is the lack of detailed information about dataset partitioning for training/testing.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThe paper presents a significant contribution to surgical AI with methodological innovations, comprehensive evaluation, and strong commitment to open science. The work addresses an important gap in surgical video understanding and demonstrates clear superiority over existing approaches. The minor issues identified (primarily regarding dataset partitioning details and expanded error analysis) can be addressed through revision without requiring additional experiments. With these revisions, this paper would be a valuable addition to the TMI literature.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *LLaVA-Surg*, a multimodal conversational assistant for surgical video understanding, and presents *Surg-QA*, a large-scale dataset consisting of 102,000 surgical video‚Äìinstruction pairs derived from 44,000 clips across 2,201 procedures. The authors design a two-stage question‚Äìanswer generation pipeline that first extracts structured surgical knowledge‚Äîranging from basic object identification to expert-level deductive reasoning‚Äîbefore generating instruction-tuning data while reducing LLM hallucination. Experimental results show that LLaVA-Surg substantially outperforms general-domain multimodal models in zero-shot surgical video QA, with results validated through both GPT and clinician scoring. The paper is clearly written, methodologically detailed, and positioned as a significant step toward domain-specific multimodal assistants in surgical contexts.  \n\n---\n\n**Major Comments**  \n\n1. **Dataset Partitioning:** The paper does not clearly explain how the Surg-QA dataset was divided into training and testing sets, raising concerns about potential data leakage across surgical procedures.  \n2. **Evaluation Metrics:** The use of 0‚Äì5 scoring is reasonable, but a more detailed inter-rater reliability analysis among clinicians would strengthen the medical validity of the results.  \n3. **Hallucination Quantification:** Although the two-stage design is claimed to reduce hallucination, no quantitative metrics or comparative baselines are provided to substantiate this improvement.  \n4. **Temporal Reasoning:** The model‚Äôs capacity for complex temporal reasoning across long surgical sequences is not deeply analyzed, despite its importance in surgical workflows.  \n\n---\n\n**Minor Comments**  \n\n1. The paper‚Äôs visual examples (Figures 5‚Äì8) clearly illustrate model performance, but Figure 4 contains overlapping text that limits readability.  \n2. Appendix A provides excellent detail on prompt engineering, supporting reproducibility.  \n3. Comparative results (Tables 1‚Äì2) effectively situate the contribution relative to both general-domain and surgical VQA datasets.  \n4. The computational resource requirements are mentioned but not fully quantified; total time and cost estimates would be useful.  \n5. Including error analysis for underperforming surgical domains would further clarify limitations.  \n\n---\n\n**Summary Paragraph**  \nThis work makes a substantial contribution by addressing a key gap in surgical AI: comprehensive understanding of surgical videos through structured, multimodal learning. Its significance lies in the scope of the Surg-QA dataset and the tailored LLaVA-Surg model, both of which outperform general models and provide clinically relevant results. Innovation is evident in the two-stage data generation pipeline and the hierarchical surgical knowledge framework, while evaluation is thorough though somewhat limited by missing inter-rater metrics and error breakdowns. Reproducibility is exemplary, with full documentation, code, and data release plans, though dataset partition transparency could be improved.  \n\n---\n\n**Decision Recommendation: Minor Revision**  \nThe study offers a strong, well-supported contribution with notable methodological innovation and clear clinical relevance. Minor clarifications on dataset partitioning, inter-rater reliability, and detailed error analysis should be addressed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe manuscript tackles the chronic shortage of large‚Äëscale multimodal data for surgical video understanding. By assembling a substantial collection of surgery videos paired with textual questions and answers, the authors aim to enable the development of conversational assistants that can reason about operative procedures. The proposed pipeline first extracts salient visual information and then generates answers in a second stage, thereby attempting to curb the hallucination problems that are common in video‚Äëlanguage models.\n\n**## General feedback**\n\n- **Significance:** Addressing the paucity of surgical video‚Äëtext corpora is a clinically relevant endeavour. A dataset of the size described has the potential to accelerate downstream research in intra‚Äëoperative decision support and education.  \n- **Innovation:** The two‚Äëstage extraction‚Äëgeneration framework is a sensible strategy to reduce hallucinations, and it aligns with recent trends in staged prompting. While the overall architecture follows established video‚ÄëLLM designs, the application to a surgical domain is novel.  \n- **Evaluation:** The authors present quantitative results using an automatic GPT‚Äë3.5 scorer (Table‚ÄØ3) and complement these with a modest clinician study (Figure‚ÄØ5). However, the evaluation would benefit from comparisons against existing surgical VQA benchmarks (e.g., EndoVis‚Äë18‚ÄëVQA, Cholec80‚ÄëVQA) and a clearer separation between training and test splits to avoid potential over‚Äëfitting.  \n- **Reproducibility:** Important implementation details‚Äîsuch as video preprocessing steps, full prompt templates, validation split construction, random seeds, licensing of the source videos, and access conditions for Llama‚Äë3‚Äë70B‚Äîare presently missing. The promised open‚Äësource release also lacks a concrete repository link and licensing information, which limits immediate reproducibility.\n\n**## Specific comments / critiques**\n\n1. **Dataset description & bias:** The manuscript reports aggregate numbers (44‚ÄØK videos, 102‚ÄØK QA pairs) but does not provide per‚Äëprocedure statistics, class‚Äëbalance breakdowns, or quality metrics (e.g., human validation rates). Because the source material originates from WebSurg lectures, the diversity of operative settings and camera viewpoints may be limited; a more detailed analysis would help assess potential bias.  \n\n2. **Data split & validation:** While training (1‚ÄØ935 procedures) and test (216 procedures) partitions are mentioned, a validation set is not described. Clarifying whether any hyper‚Äëparameter tuning was performed on the test split (Section‚ÄØ5.1) would strengthen confidence in the reported results.  \n\n3. **Baseline fairness:** Table‚ÄØ3 juxtaposes zero‚Äëshot off‚Äëthe‚Äëshelf Video‚ÄëLLaVA and Video‚ÄëChatGPT, which have not been fine‚Äëtuned on the Surg‚ÄëQA data. Providing fair baselines‚Äîi.e., fine‚Äëtuning these models on the same training set‚Äîwould give a more meaningful performance context.  \n\n4. **Ablation of the two‚Äëstage pipeline:** The paper includes a single qualitative illustration (Figure‚ÄØ3) indicating reduced hallucination, yet quantitative ablations that isolate the extraction stage from a direct end‚Äëto‚Äëend generation are absent. Such experiments would clarify the contribution of each stage.  \n\n5. **Human evaluation scope & rigor:** The clinician study evaluates only 60 video‚Äëquestion pairs (Figure‚ÄØ5) and reports correlation without statistical testing (e.g., Pearson‚Äôs r, confidence intervals). In addition, the ad‚Äëhoc GPT‚Äëbased scoring (0‚Äë5) lacks validation against clinically meaningful correctness criteria (Figure‚ÄØ9). Expanding the sample size and incorporating formal statistical analyses would make the human evaluation more robust.  \n\n6. **Metric justification:** The introduced ‚ÄúScore‚ÄØ0‚Äë5‚Äù and ‚Äúaccuracy@all/1‚Äù metrics are non‚Äëstandard; providing evidence that they correlate with expert judgments or downstream clinical utility would increase their credibility.  \n\n7. **Training & compute details:** Basic training hyper‚Äëparameters (learning rate, epochs, batch size, GPU count) are reported, yet information on total FLOPs, inference latency, and the impact of freezing versus fine‚Äëtuning the visual encoder is missing. Including these details would aid reproducibility and allow assessment of computational efficiency.  \n\n8. **External benchmark testing:** Although the manuscript cites several surgical VQA datasets (Table‚ÄØ2), the proposed model is not evaluated on them. Testing on established benchmarks would demonstrate the model‚Äôs generalizability beyond the newly generated dataset.  \n\n9. **Safety & ethical considerations:** While the authors acknowledge hallucination risks, a systematic mitigation strategy is not presented. Moreover, discussions of privacy, informed consent, and potential misuse of surgical video data are absent. Addressing these ethical aspects would strengthen the manuscript‚Äôs impact.  \n\n10. **Open‚Äësource release clarity:** The lack of a repository URL, explicit license, and verification of the source video licensing (WebSurg terms) hampers reproducibility. Additionally, reliance on the commercial Llama‚Äë3‚Äë70B model may limit exact replication for other researchers. Providing clear release information and, if possible, an open‚Äësource alternative would be valuable.  \n\n**## A suggested decision**\n\n*Reject* ‚Äì While the work addresses an important gap and presents a promising dataset, the current manuscript falls short in terms of novelty, methodological rigor, and comprehensive evaluation. Substantial revisions‚Äîparticularly in benchmarking, ablation studies, reproducibility details, and ethical considerations‚Äîare needed before the contribution can be deemed sufficient for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the scarcity of large‚Äëscale multimodal resources for surgical video understanding. It introduces a dataset of surgery videos paired with question‚Äëanswer annotations and develops a two‚Äëstage framework that first extracts visual features and then generates textual responses to reduce hallucinations. The work is timely and clinically relevant, and the writing is generally clear. However, several aspects of the evaluation, reproducibility, and methodological justification require improvement before the contribution can be fully assessed.\n\n---\n\n**Major Comments**  \n1. **Dataset characterization and bias:** The paper reports aggregated dataset numbers but omits per‚Äëprocedure distributions, class balance, or quality validation metrics. Given the WebSurg source, diversity across surgical settings may be limited; more detailed analyses are needed to assess data bias.  \n2. **Data splits and validation:** Although training and test partitions are given, the absence of a validation set and unclear treatment of hyper‚Äëparameter tuning raise potential concerns about data leakage.  \n3. **Baseline fairness:** Comparison against zero‚Äëshot baseline models that were not fine‚Äëtuned on the proposed dataset limits interpretability. Results from fine‚Äëtuned baselines would provide a fairer reference.  \n4. **Ablation of two‚Äëstage framework:** The contribution of the extraction stage is illustrated only qualitatively. Quantitative ablations are needed to verify that the two‚Äëstage design materially reduces hallucinations.  \n5. **Evaluation design:** The human evaluation comprises 60 samples without formal statistical testing. The automatic GPT‚Äëbased metrics lack validation against expert correctness measures, and justification for non‚Äëstandard metrics (‚ÄúScore‚ÄØ0‚Äì5,‚Äù ‚Äúaccuracy@all/1‚Äù) is insufficient.  \n6. **Reproducibility and implementation:** Crucial details such as preprocessing, prompting templates, split construction, random seeds, and access to model components are missing. Reporting compute cost, inference latency, and fine‚Äëtuning parameters would aid transparency.  \n7. **External benchmarks:** The model is not tested on established surgical VQA datasets, limiting assessment of generalizability.  \n8. **Ethical and safety aspects:** Discussion of hallucination mitigation, privacy protection, and consent procedures is brief. More systematic treatment of these issues is necessary.  \n9. **Open‚Äësource clarity:** The absence of a repository URL, licensing information, and the dependence on a commercial LLM restrict reproducibility.\n\n---\n\n**Minor Comments**  \n- Ensure consistent usage of dataset and metric terminology.  \n- Explicitly define all acronyms at first appearance.  \n- Verify figure and table captions for readability and statistical notation consistency.  \n- Include references to related surgical VQA benchmarks in the main text rather than only in tables.\n\n---\n\n**Summary Paragraph**  \nThe study addresses an important gap by constructing a large surgical video‚Äìtext dataset and proposing a sensible staged framework to mitigate hallucination in video‚Äëlanguage models. Its potential clinical and research significance is clear. Nonetheless, the manuscript presently lacks rigorous experimental evidence, adequate reproducibility information, validated evaluation metrics, and an explicit ethical framework. These deficiencies limit confidence in the reported outcomes and in the dataset‚Äôs practical utility.\n\n---\n\n**Decision Recommendation**  \n**Reject** ‚Äì The contribution is promising but requires major revisions in evaluation design, dataset characterization, reproducibility documentation, and ethical discussion before it can be considered for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Brian R Quaranto",
      "Garrett Skinner",
      "Gene Yang",
      "Jiajie Li",
      "Jinjun Xiong",
      "Peter C W Kim",
      "Steven D Schwaitzberg"
    ],
    "url": "pdfs/iclr.cc-2025-conference_04d73daf100581d96e3a971dd358d0aad68ebdd1.pdf",
    "remote_url": "https://openreview.net/pdf/04d73daf100581d96e3a971dd358d0aad68ebdd1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
    "status": "completed",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Hallucination Benchamrk",
      "Hallucination Evaluation Method",
      "Medical Large Vision Language Model"
    ],
    "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations‚Äîa significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MedihallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MedihallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work have been released.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper introduces a hallucination detection and evaluation benchmark in the medical multimodal domain. Recognizing that large vision-language models often inherit hallucination tendencies from foundational large-language models, the authors propose a framework for more accurate and detailed hallucination assessment in high-stakes medical applications. \nThe paper makes three primary contributions: (i) Med-HallMark, the benchmark dedicated to medical hallucination detection (ii) MediHallDetector, a hallucination detection model tailored for the medical domain; and (iii) MediHall Score, an evaluative metric designed for hallucinations of different severity and types. \nThrough extensive experiments, the paper claims that MediHall Score provides more nuanced insights than traditional metrics and that MediHallDetector enhances hallucination detection performance in medical LVLMs.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper is well-motivated in that it addresses a critical and underexplored area in medical LVLMs: detecting and evaluating hallucinations, which is crucial for ensuring safety in clinical applications. The work frames hallucination detection specifically for the medical domain. The MediHall Score introduces a nuanced metric that prioritizes clinical impact by differentiating hallucinations based on their severity and type. The ablation study of different SFT methods for training the detector also adds robustness to the study.\n\n### Weaknesses\n\nThe paper raises several questions about primarily the soundness of evaluation and the reliability of the proposed metric (detailed in the **Questions** section). For example, the paper limits its IRG evaluation to only one domain-specific model, XrayGPT, which is problematic given that IRG tasks are where hallucination metrics are particularly valuable. Additionally, results for XrayGPT in IRG appear inconsistent with those of Minigpt-4, which was shown to underperform in traditional metrics but achieved the highest MediHall Score among all evaluated models, calling into question the reliability of the MediHall Score. The paper could have strengthened its findings by evaluating other state-of-the-art LVLMs, such as open-source, domain-specific models like BiomedGPT or Med-Flamingo, or proprietary MLLMs like GPT-4 and Gemini Pro, to validate the metric‚Äôs robustness. Finally, the experimental design lacks a thorough investigation into the role of image inputs in tuning MediHallDetector. The study could benefit from ablation studies that assess performance with and without image inputs, especially given that the image encoder and connector taken from LLaVA are frozen during training and could have introduced extra errors due to its lack of visual understanding ability. These design choices suggest that the current benchmark version may be limited in scope and could benefit from further refinement and validation.\n\n### Questions\n\n**Construction of the benchmark:**\n1. Could you elaborate on the motivation behind designing confidence-weakening and counterfactual questions beyond the \"conventional\" questions in established test sets like SLAKE and RAD-VQA? Regarding counterfactual question generation, considering the GPT model is provided with limited information (text-only, single question with its ground-truth answer), how is the quality of GPT-generated counterfactual questions and answers ensured?\n\n2. Robust evaluation of hallucinations in open-ended IRG tasks is inherently challenging, particularly due to synonymous terms (in frameworks like ICD-10, multiple expressions might refer to the same condition). Since the ground truth for open-ended IRG scenarios in this paper derives from medical reports in MIMIC and OpenI test sets, how does the evaluation framework account for potential synonyms in model responses given the limited set of 1800 images and their reports seen by MediHallDetector?\n\n3. In using LLaVA-Med to generate GT responses for Med-VQA, why not utilize the GT already provided in the dataset, given that the questions originate from an established source? For the IRG scenario, are model-generated responses also exclusively from LLaVA-Med? If so, might this narrow distribution affect the generalizability of MediHallDetector, as it is fine-tuned specifically on LLaVA-Med responses?\n\n**Experimental Design:**\n1. How much does the image come into play in tuning the MedihallDetector model? It could be necessary to ablate w./w.o. image input, especially when you freeze the image encoder and connector during training.\n\n**Results**\n1. For medical VQA tasks (Table 1), why introduce a MediHall Score when accuracy already exists as a metric for Med-VQA? Notably, XrayGPT, with an accuracy of only 0.02, has a MediHall Score of 0.36. Could this indicate potential inflation in the MediHall Score?\n\n2. For medical IRG tasks (Table 2), the results for XrayGPT as measured by MediHallDetector seem counter-intuitive compared to Minigpt-4. In the original XrayGPT study [1], Minigpt-4 was a baseline against which improvements were demonstrated using ROUGE (Table 1 in [1]). Here, XrayGPT does outperform Minigpt-4 in all conventional metrics, but in the MediHall Score, with Minigpt-4 obtaining the highest MediHall Score among evaluated models. Could this raise concerns regarding the reliability of the MediHall Score metric?\n\n3. Could you clarify why models like BLIP2, LLaVA-Med, and RadFM do not receive MediHall Scores for medical IRG tasks? Open-ended IRG tasks are particularly significant since accuracy is already used as a metric for VQA. Presently, only XrayGPT is evaluated in IRG tasks using the proposed model and metric; incorporating additional state-of-the-art models would enhance the proposed metric's robustness. For instance, open-source domain-specific models such as BiomedGPT [2] and Med-Flamingo [3], which perform well on established tasks, or widely used proprietary MLLMs like GPT-4, Claude 3.5, and Gemini Pro could be valuable benchmarks.\n\n**Typo**\n1. In Figure 2(a), ‚Äúpairs‚Äù is misspelled.\n2. Could you clarify whether XrayGPT is used in constructing the benchmark? It appears in Figure 1(b) but is not referenced in section 3.4.\n\n[1] https://aclanthology.org/2024.bionlp-1.35.pdf\n[2] https://arxiv.org/abs/2305.17100\n[3] https://arxiv.org/abs/2307.15189\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nn/a",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a benchmark and methodology for hallucination detection and evaluation in medical multimodal large vision‚Äìlanguage models (LVLMs). Recognizing that such models often inherit hallucination tendencies from their foundational large language models, the study addresses a critical issue for clinical safety. The work presents three main components: (i) **Med-HallMark**, a benchmark dataset for medical hallucination detection; (ii) **MediHallDetector**, a detection model tailored to the medical domain; and (iii) **MediHall Score**, a metric that quantifies hallucinations by severity and type. Extensive experiments are conducted, and the authors claim that MediHall Score yields more nuanced insights than traditional metrics, while MediHallDetector improves hallucination detection performance. Overall, the paper is clearly motivated and organized, though some methodological aspects raise concerns regarding evaluation soundness and metric reliability.  \n\n**Major Comments**  \n1. **Evaluation Scope and Reproducibility** ‚Äì The study evaluates MediHallDetector primarily on XrayGPT for image report generation (IRG), omitting testing on other medical LVLMs. This narrow scope limits robustness and generalizability; inclusion of open-source models (e.g., BiomedGPT, Med-Flamingo) or proprietary MLLMs (e.g., GPT-4, Gemini Pro) would strengthen conclusions.  \n2. **Metric Reliability** ‚Äì The MediHall Score results appear inconsistent across models, notably where Minigpt-4 underperforms on traditional metrics yet achieves the highest MediHall Score. Such discrepancies raise doubts about the validity of the proposed metric and its calibration.  \n3. **Experimental Design** ‚Äì The role of image input during MediHallDetector training is insufficiently explored. Since the image encoder and connector are frozen, ablation studies comparing training with and without image input would clarify their effect on performance.  \n4. **Benchmark and Question Generation** ‚Äì The rationale and quality control behind confidence-weakening and counterfactual question construction should be clarified, particularly given limited textual context provided to GPT during generation.  \n5. **Ground-Truth and Evaluation Consistency** ‚Äì It is unclear why generated responses from LLaVA-Med are used rather than existing ground-truth answers in datasets such as Med-VQA. Reliance on model-generated references could reduce evaluation validity and generalizability.  \n\n**Minor Comments**  \n- Address potential synonym variations in IRG responses, as medical terminology often contains equivalent expressions.  \n- In Figure‚ÄØ2(a), the word ‚Äúpairs‚Äù is misspelled.  \n- Clarify whether XrayGPT is used in constructing the benchmark, since it appears in Figure‚ÄØ1(b) but not in Section‚ÄØ3.4.  \n\n**Summary Paragraph**  \nThis work tackles an important and underexplored problem in medical LVLMs and offers domain-specific tools to assess hallucinations. The benchmark, detector, and scoring metric collectively represent a useful direction toward safer clinical AI systems. However, limitations in evaluation breadth, potential unreliability of the MediHall Score, and unanswered methodological questions about data construction and image use temper the impact of the findings. Stronger validation across multiple models and refined analyses would be needed to confirm the framework‚Äôs utility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The concept and motivation are sound, but issues of evaluation scope, metric reliability, and methodological clarity must be addressed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper proposes an LLM/LVLM evaluation benchmark, especially for the hallucination in the medical domain tasks. It also proposes a metric to evaluate the severity of hallucination in LVLMs, and a fine-tuned evaluator model is released to perform the universal evaluation for given set information based on (input image I, the original prompt P, the LVLM answer A and the ground truth GT).\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe paper proposes a novel benchmark for evaluation the LLMs specially in the Medical domain, and provides a comprehensive framework from dataset, metric, and the fine-tuned evaluation model, which is a completed work through the evaluation pipeline.‚ÄØ \n‚óè\tThe paper is writing in a smooth way, easy to follow and simple to understand.‚ÄØ \n‚óè\tThe work is of good significance, should be a meaningful angle to advance the LLM/LVLMs applications in real-world healthcare domain.\n\n### Weaknesses\n\n‚óè\tThe paper did not emphasize the special challenges in Healthcare domain, after the reading, except for the first part of the dataset that involves the `Medical` multi-modality data such as CT, the evaluation process is on the common hallucination challenges from any Vision-language model, it raises the concerns that whether the proposed model stands out from common LVM hallucination evaluation, is other existing work able to solve the same question by simply adapting to the medical data? What is the advantage of the proposed fine-tuned model over other baseline methods?‚ÄØ \n‚óè\tThe hierarchical definition of the MediHALL score seems a bit intuitive, which is a simple evenly scaled value on 5 categories.‚ÄØ \n‚óè\tThe paper lacks discussion on the relevant work that conduct similar hallucination detection in LLMs at Healthcare domain.‚ÄØ \n‚óè\tIt is suggested that the paper could make further clarification on certain questions as in the Question section, if most critical concerns can be addressed, it is plausible to raise the score for the paper‚Äôs quality evaluation.\n\n### Questions\n\n1. The authors explicitly defined 5 types of hallucination levels, among which: Catastrophic Hallucination, Critical Hallucinations, Minor Hallucination are severity levels, but the attribute, prompt-induced ones is indeed the `cause` of the hallucinations. There raise doubts of the rationale of such empirical classifications including attribute, prompt-induced to severity levels. \n \n2. As in line 215, the conventional Q_{conv} questions are generated by the GPT3.5, how is the quality guaranteed? How is the initial conv question obtained?‚ÄØ \n \n3. LLaVA-Med was used to infer answers, what is the size of the whole dataset? Even though the authors claim that the data has been examined, how is the quality guaranteed?‚ÄØ‚ÄØ \n \n4. Also, for the dataset part that is relevant to the IRG scenario, the authors use a sampling method to draw 1,800 images and their corresponding medical reports from existing datasets: MIMIC-test and OpenI datasets. This weakens the contribution towards the data as proposed in line 065.‚ÄØ \n \n5. In line 255, ‚ÄúThis fine-grained metric xxx‚Äù, starts abruptly, since the previous paragraph discusses the existing metrics drawbacks. Then it directly refers to ‚ÄúThis metric‚Äù, which is not transitioning naturally.‚ÄØ \n \n6. The MediHALL score is relatively simple, which is built upon the pre-categorized types of hallucination levels and assigns different values in a hierarchical way.‚ÄØ Based on this, the latter human-annotation and fine-tuning are conducted, are they able to maintain the objectiveness?‚ÄØ \n \n7. In line 318, which directs the training data details to Figure, it explicitly shows the categories, and the types of training data covered for MEDIHALLDETECTOR, but what about the amount of the data? And how is the instruction pair data derived?‚ÄØ \n \n8. While the paper is positioned as the first benchmark for medical LM hallucination evaluation, there is relevant work worth referencing:‚ÄØ \n\n[1] MEDHALU: Hallucinations in Responses to Healthcare Queries by Large Language Models \n[2] MEDIC: TOWARDS A COMPREHENSIVE FRAMEWORK FOR EVALUATING LLMS IN CLINICAL APPLICATIONS \n[3] Faithfulness Hallucination Detection in Healthcare AI\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a benchmark for evaluating hallucinations in large language and vision-language models (LLMs/LVLMs) within the medical domain. It also proposes a new metric, the MediHALL score, to measure hallucination severity and releases a fine-tuned evaluator model for automated assessment using image, prompt, model response, and ground-truth information. The paper is clearly written, well organized, and accessible, offering a complete evaluation pipeline from dataset construction to metric definition and model fine-tuning. Its goal is significant given the importance of reliable model behavior in healthcare contexts.\n\n---\n\n**Major Comments**  \n1. The authors do not sufficiently emphasize challenges specific to the healthcare domain. Apart from the medical multimodality dataset (e.g., CT images), the evaluation focuses on general hallucination problems. This raises concerns about whether the method truly addresses medical-specific issues or could be replicated by adapting existing approaches to medical data.  \n2. The justification for the hierarchical definition of the MediHALL score appears intuitive rather than empirically grounded. The use of evenly scaled five categories could benefit from theoretical or data-driven motivation.  \n3. The work lacks a detailed comparison and discussion of related studies on hallucination detection for medical LLMs, such as MEDHALU, MEDIC, or Faithfulness Hallucination Detection in Healthcare AI.  \n4. Greater clarification is needed on several methodological points:  \n   - Rationale for classifying ‚Äúattribute‚Äù and ‚Äúprompt-induced‚Äù hallucinations as severity levels rather than causes.  \n   - Methods for ensuring the quality of GPT3.5-generated questions and the dataset annotations.  \n   - Dataset scale and sampling justification for MIMIC-test and OpenI sources.  \n   - Clarification of abrupt transitions (e.g., line 255‚Äôs ‚ÄúThis fine-grained metric‚Ä¶‚Äù).  \n   - Explanation of how hierarchical scoring and fine-tuning preserve objectivity, and how instruction pairs were derived.  \n\n---\n\n**Minor Comments**  \n- Improve transitions between sections, particularly where prior discussion does not naturally lead to the next concept.  \n- Specify dataset sizes and distributions more clearly in figures and text references.  \n- Proofread for minor grammatical and stylistic issues.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper presents a timely and valuable attempt to benchmark hallucination evaluation in medical LLMs/LVLMs, with clear writing and a comprehensive structure. However, the lack of domain-specific justification, limited discussion of related work, and empirical weaknesses in the scoring design reduce the current impact. Addressing these methodological and contextual gaps would substantially strengthen the contribution.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper has promise but requires additional clarification, stronger comparative context, and more robust justification of its framework and metric design.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces a new benchmark, Med-HallMark, and proposes evaluation tools specifically tailored to medical contexts. Med-HallMark provides a multi-dimensional framework to identify and assess hallucinations generated by LVLMs. The benchmark includes multi-tasking hallucination support, diverse data for hallucination detection, and a hierarchical categorization of hallucinations based on clinical impact. Additionally, the paper presents MediHall Score, a metric designed to evaluate hallucinations in medical text output, and MediHallDetector, a specialized LVLM aimed at improved hallucination detection through multi-task training.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- The introduction of Med-HallMark and MediHall Score fills a significant gap by addressing hallucination detection in medical LVLMs.\n- The proposed categorization of hallucinations is innovative and highly relevant, allowing for analysis of potential model impacts on medical decision-making.\n- The authors conduct extensive experiments comparing popular LVLM models on Med-HallMark.\n\n### Weaknesses\n\n- Is this very different from the accuracy? After reading the paper, I just feel that this paper just makes a more fine-grained classification of errors, and the classification standard needs to be discussed and re-designed. For example, in the report generation task, the model outputs a nonsense sentence for the chest X-ray, \"This is a chest X-ray of a person\", which is obviously correct, but it is not what we want. How should this be judged?\nSo this makes me wonder whether such a benchmark is necessary.\n- Some recent related work [1,2,3,4,5,6] is missing. \n- The overall data scale is relatively small, and the medical image modalities involved are limited to radiology.\n- How to ensure the accuracy of annotation? \n- Since doctors are hired to do the annotation, have the possible ethical risks been resolved? For example, IRB approval, etc.\n\n\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Yan Q, He X, Wang X E. Med-HVL: Automatic Medical Domain Hallucination Evaluation for Large Vision-Language Models[C]//AAAI 2024 Spring Symposium on Clinical Foundation Models. 2024.\n\n[3] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[4] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n[5] Nan Y, Zhou H, Xing X, et al. Beyond the Hype: A dispassionate look at vision-language models in medical scenario[J]. arXiv preprint arXiv:2408.08704, 2024.\n\n[6] Yan Q, He X, Yue X, et al. Worse than Random? An Embarrassingly Simple Probing Evaluation of Large Multimodal Models in Medical VQA[J]. arXiv preprint arXiv:2405.20421, 2024.\n\n### Questions\n\n- The format of the reference is weird. Please check it.\n- The figure 2 (d) is vague.\n\n### Flag For Ethics Review\n\n- Yes, Responsible research practice (e.g., human subjects, data release)\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nSince doctors are hired to do the annotation, have the possible ethical risks been resolved? For example, IRB approval, etc.",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Med-HallMark*, a benchmark designed to evaluate hallucinations in large vision-language models (LVLMs) within medical contexts. It provides a multi-dimensional framework incorporating diverse data sources, hierarchical hallucination categories reflecting clinical impact, and multi-task hallucination detection. The paper also presents *MediHall Score*, a metric for quantifying hallucinations in generated medical text, and *MediHallDetector*, an LVLM fine-tuned for hallucination detection through multi-task training. Overall, the paper is clearly organized and addresses an important problem in the evaluation of medical LVLMs.\n\n**Major Comments**  \n1. **Conceptual Novelty and Justification:** The benchmark‚Äôs distinction from conventional accuracy metrics is unclear. The proposed categories can appear to be merely finer-grained error classifications rather than conceptually new evaluation dimensions. The criteria underlying these classifications require further discussion and justification.  \n2. **Example Ambiguity:** The treatment of borderline outputs (e.g., ‚ÄúThis is a chest X-ray of a person‚Äù) illustrates uncertainty about what constitutes a hallucination versus an uninformative but correct statement. Clearer definition or annotation guidelines are needed.  \n3. **Necessity of the Benchmark:** Given existing evaluation efforts, the reviewer questions whether the proposed benchmark provides unique value beyond more detailed error labeling.  \n4. **Related Work:** Several recent and relevant studies (e.g., MedVH, Med-HVL, MedThink, CARES, Beyond the Hype, Worse than Random) are not cited or discussed. Their inclusion would strengthen contextual positioning.  \n5. **Dataset Scale and Scope:** The dataset size is limited, and imaging modalities appear restricted to radiology. Broader coverage would improve generalizability.  \n6. **Annotation Accuracy and Ethics:** The reliability of human annotation is not discussed in depth. Furthermore, since medical professionals are employed for annotation, ethical procedures such as Institutional Review Board (IRB) approval should be explicitly addressed.\n\n**Minor Comments**  \n- References are inconsistently formatted and should follow a standard citation style.  \n- Figure 2(d) is unclear and needs improvement in resolution or labeling.\n\n**Summary Paragraph**  \nThe paper makes a potentially valuable contribution by formalizing medical hallucination evaluation for LVLMs and proposing tailored metrics and tools. However, the conceptual grounding and dataset details require reinforcement, and several aspects‚Äîincluding novelty, annotation reliability, comparative scope, and ethical transparency‚Äîneed clarification. Addressing these points would be essential for the work to achieve credibility and impact in this area.\n\n**Decision Recommendation:** Major Revision",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1: Analytical and Scope‚ÄØAudit**\n\n**1.‚ÄØScope &‚ÄØRelevance to‚ÄØTMI** ‚Äì‚ÄØ  \nThe manuscript introduces *Med‚ÄëHallMark*, a benchmark for identifying and quantifying hallucinations in large vision‚Äëlanguage models (LVLMs) applied to medicine, proposes a new metric (*MediHall‚ÄØScore*), and develops a dedicated hallucination‚Äëdetection model (*MediHallDetector*).  The work concerns multimodal machine‚Äëlearning methodology for medical image interpretation rather than a specific clinical application.  It falls within TMI‚Äôs scope of computational‚Äêimaging methodology, particularly assessment and trustworthiness of machine‚Äëlearning systems.‚ÄØ‚úÖ‚ÄØProceed with full review.\n\n**2.‚ÄØNovelty &‚ÄØContribution** ‚Äì‚ÄØ  \nThe authors combine three original components:‚ÄØ(1)‚ÄØa curated benchmark with hierarchical hallucination labels specific to clinical severity,‚ÄØ(2)‚ÄØa fine‚Äëgrained quantitative metric, and‚ÄØ(3)‚ÄØa detection network trained to classify hallucination levels.  While benchmarks and evaluation metrics have appeared in general LVLM literature, adaptation to the medical domain with clinically meaningful categories appears novel.  The contribution is methodological yet primarily evaluative rather than a new imaging‚Äëphysics or reconstruction method.  Novelty is moderate.\n\n**3.‚ÄØTechnical &‚ÄØExperimental‚ÄØRigor** ‚Äì‚ÄØ  \nThe paper describes dataset sources, annotation workflows, and multi‚Äërater validation.  The methodology for question generation, hallucination taxonomy, and score calculation is clearly detailed.  Experiments cover multiple LVLM baselines with quantitative tables.  Statistical uncertainty and inter‚Äëannotator agreement could be more fully reported; claims of superiority of MediHallDetector would benefit from statistical significance testing and clearer definitions of accuracy/recall in Table‚ÄØ5.  Reproducibility appears feasible but relies on released resources.\n\n**4.‚ÄØClarity &‚ÄØPresentation** ‚Äì‚ÄØ  \nOverall well organized and grammatically sound, though the text is lengthy with overlapping explanations between main and supplementary sections.  Figures are informative but crowded.  Some acronyms (e.g., ‚ÄúIRG,‚Äù ‚ÄúVQA‚Äù) should be defined at first mention in the main text.  English usage and formatting are acceptable for IEEE‚ÄØTMI style after copy‚Äëediting.\n\n**5.‚ÄØEthical‚ÄØ&‚ÄØReproducibility‚ÄØCompliance** ‚Äì‚ÄØ  \nAll datasets originate from CC‚Äëlicensed public sources (SLAKE,‚ÄØVQA‚ÄëRAD,‚ÄØMIMIC‚ÄëCXR,‚ÄØOpenI); identifiers were removed.  No new patient data were collected, complying with ethical standards.  The authors state that all resources are released and open source.‚ÄØ‚úÖ‚ÄØCompliant.\n\n---\n\n**Phase‚ÄØ2: IEEE‚ÄØTMI Review‚ÄØReport**\n\n**1.‚ÄØSummary**  \nThe paper addresses the absence of a formal evaluation framework for hallucinations in medical LVLMs.  It introduces the *Med‚ÄëHallMark* benchmark covering medical‚ÄëVQA and imaging‚Äëreport generation tasks with multilayer clinical‚Äëimpact labels; defines the *MediHall‚ÄØScore* metric for granular hallucination scoring; and presents *MediHallDetector*, a supervised fine‚Äëtuned LVLM able to categorize hallucination types.  Baseline results for multiple existing LVLMs and ablation studies demonstrate feasibility and illustrate weaknesses of conventional NLP metrics in reflecting medical factuality.\n\n**2.‚ÄØStrengths**\n- Fills a clear gap: domain‚Äëspecific hallucination detection/benchmarking for medical LVLMs.  \n- Defines a clinically interpretable, hierarchical annotation scheme linking hallucination severity to diagnostic impact.  \n- Provides open resources likely to foster reproducibility and community comparison.  \n- Demonstrates comprehensive experimental coverage across model families.  \n\n**3.‚ÄØWeaknesses**\n- Novelty lies mainly in dataset design and metric engineering; limited algorithmic innovation beyond an SFT variant of existing LVLMs.  \n- Lacks quantitative statistical analysis (confidence intervals or significance testing).  \n- No external expert validation of the hierarchical severity mapping or evidence that MediHall‚ÄØScore correlates with clinical harm.  \n- Presentation is verbose; several results tables could be condensed.  \n\n**4.‚ÄØMajor‚ÄØComments**\n1.‚ÄØ**Validation of Hierarchical Labels:**‚ÄØProvide quantitative inter‚Äëannotator agreement (Œ∫‚ÄØstatistics) to demonstrate reliability of the five‚Äëlevel taxonomy.  \n2.‚ÄØ**Statistical‚ÄØSignificance:**‚ÄØReport variance and perform significance testing for improvements of MediHallDetector over baselines.  \n3.‚ÄØ**Metric‚ÄØJustification:**‚ÄØExplain how numeric weights (0‚Äì1) in the MediHall‚ÄØScore were chosen and whether results are sensitive to these values.  \n4.‚ÄØ**Generalizability:**‚ÄØDiscuss applicability of Med‚ÄëHallMark beyond chest radiography (e.g., CT,‚ÄØMRI) and potential extension to non‚ÄëEnglish datasets.  \n5.‚ÄØ**Comparison with Prior Benchmarks:**‚ÄØClarify differentiation from general hallucination datasets (CHAIR,‚ÄØPOPE) and illustrate unique medical features beyond label granularity.  \n\n**5.‚ÄØMinor‚ÄØComments**\n- Unify notation (e.g., ‚ÄúMediHallDetector‚Äù capitalization).  \n- Fix typographical inconsistencies (missing spaces and superscripts).  \n- Condense redundant dataset descriptions in main text; leave details to appendix.  \n- Ensure figures, especially Fig.‚ÄØ1 and Fig.‚ÄØ2, have legible fonts for publication.  \n- Verify IEEE‚ÄØreference formatting.\n\n**6.‚ÄØOverall‚ÄØRecommendation**‚ÄØ‚Äì‚ÄØ**Major‚ÄØRevision**‚ÄØ  \nThe study is methodologically important and well executed but primarily proposes a benchmark; to meet TMI‚Äôs methodological‚Äërigor expectations the paper requires clearer quantitative validation, succinct presentation, and evidence of the metric‚Äôs clinical relevance.\n\n**7.‚ÄØConfidence‚ÄØLevel**‚ÄØ‚Äì‚ÄØ4‚ÄØ/‚ÄØ5‚ÄØ(High)  \n\n---\n\n**Summary Judgement:**‚ÄØThis submission introduces valuable resources for evaluating hallucinations in medical multimodal models‚Äîa timely and relevant contribution for trustworthy medical imaging AI.  With strengthened analysis and improved clarity, it could merit publication in IEEE‚ÄØTMI after major revision.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Med‚ÄëHallMark*, a comprehensive benchmark for identifying and quantifying hallucinations in large vision‚Äëlanguage models (LVLMs) applied to medicine. It further proposes a quantitative evaluation metric (*MediHall‚ÄØScore*) and a dedicated hallucination‚Äëdetection model (*MediHallDetector*). The work aims to fill the gap in formal frameworks for assessing factual reliability in medical LVLMs. Overall, the paper is clear, well organized, and presents detailed methodological descriptions, though the presentation is somewhat verbose and could be streamlined.\n\n---\n\n**Major Comments**  \n1. **Validation of Label Taxonomy:** The five‚Äëlevel hierarchical labeling of hallucination severity should be supported by quantitative inter‚Äëannotator agreement measures (e.g., Œ∫ statistics) to establish reliability.  \n2. **Statistical Significance of Results:** Improvements claimed for *MediHallDetector* over baselines require variance reporting and significance testing to substantiate performance differences.  \n3. **Metric Design Justification:** Clarify how the numeric weights (0‚Äì1) in the *MediHall‚ÄØScore* were determined and whether outcomes are sensitive to these choices.  \n4. **Scope and Generalizability:** Discuss the potential to extend *Med‚ÄëHallMark* beyond chest radiography to other imaging modalities (CT, MRI) and possibly non‚ÄëEnglish data.  \n5. **Comparison with Prior Work:** Differentiate more explicitly from existing general hallucination benchmarks (e.g., CHAIR, POPE) and emphasize what is uniquely medical beyond hierarchical labeling.  \n6. **Statistical and Annotation Details:** While datasets, annotation workflows, and baselines are well described, the study would benefit from reporting uncertainty estimates and clearer definitions of performance metrics in tables.  \n7. **Clarity and Conciseness:** Several result tables and dataset descriptions could be condensed or moved to supplementary materials to improve readability.\n\n---\n\n**Minor Comments**  \n- Standardize terminology (e.g., capitalization of ‚ÄúMediHallDetector‚Äù).  \n- Correct typographical errors, spacing, and superscript formatting.  \n- Define acronyms such as ‚ÄúIRG‚Äù and ‚ÄúVQA‚Äù at first appearance.  \n- Improve figure readability, especially font sizes in Figures‚ÄØ1‚Äì2.  \n- Check consistency of reference formatting.  \n\n---\n\n**Summary Paragraph**  \nThis work offers a timely and methodologically relevant benchmark addressing hallucination detection for medical LVLMs, providing open resources that enhance reproducibility. Its main strengths lie in establishing a clinically interpretable annotation hierarchy and comprehensive experimental evaluation. However, its novelty is concentrated in benchmark and metric design rather than model innovation. The paper would benefit from stronger statistical analysis, validation of labeling reliability, and a more concise presentation to meet methodological expectations.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nSubstantial improvements in quantitative validation, metric justification, and presentation clarity are needed before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses hallucination detection in Large Vision Language Models (LVLMs) within medical contexts by introducing three main contributions. First, Med-HallMark is presented as the first benchmark for medical multimodal hallucination detection, supporting both Medical Visual Question Answering (Med-VQA) and Imaging Report Generation (IRG) tasks with hierarchical categorization of hallucinations into five severity levels (Section 3). Second, MediHall Score is proposed as a domain-specific evaluation metric that assigns scores based on hallucination severity (Section 4). Third, MediHallDetector is developed as a specialized model for detecting medical hallucinations through supervised fine-tuning (Section 5). The benchmark encompasses 7,341 samples across conventional, confidence-weakening, counterfactual, and image depiction questions (Figure 1). Experimental results demonstrate that existing LVLMs exhibit significant hallucination rates, with the proposed MediHall Score providing more nuanced assessment compared to traditional metrics (Tables 1-2).\n\n## Weaknesses\n\n‚Ä¢ **Insufficient mathematical rigor in metric formulation**\n  - The MediHall Score calculation lacks formal mathematical notation and precision (Section 4, lines 270-275). The averaging formula Hoverall = 1/k Œ£·µ¢‚Çå‚ÇÅ·µè H·µ¢ oversimplifies complex hierarchical relationships between different hallucination types.\n  - Score assignments (Hc = 0.0, Hcr = 0.2, etc.) appear arbitrary without statistical justification or sensitivity analysis supporting these specific values.\n  - No confidence intervals or uncertainty quantification is provided for the proposed metric, limiting its statistical interpretability in clinical contexts.\n\n‚Ä¢ **Limited scope and generalizability of benchmark construction**\n  - The dataset relies heavily on only four source datasets (SLAKE, VQA-RAD, MIMIC, OpenI) as stated in Section 3.4, potentially limiting diversity of medical conditions and imaging modalities.\n  - The hierarchical categorization system is developed with assistance from \"experienced clinicians\" (Section 3.3) but lacks details on inter-annotator agreement, annotation guidelines, or validation across different medical specialties.\n  - GPT-based augmentation for expanding questions (Section 3.4, lines 225-227) may introduce systematic biases that are not adequately controlled or measured.\n\n‚Ä¢ **Inadequate experimental validation and baseline comparisons**\n  - The evaluation of MediHallDetector against GPT-3.5 and GPT-4 is limited to only 300 samples (Section 6.3), which may not provide sufficient statistical power for robust conclusions.\n  - Missing comparisons with other specialized medical hallucination detection methods, as the paper claims to be \"the first\" but does not adequately survey related work in medical AI safety.\n  - Tables 1-2 show inconsistent performance patterns across models without adequate statistical significance testing or error bars to support the claimed superiority of the proposed approach.\n\n‚Ä¢ **Questionable clinical relevance and practical applicability**\n  - The paper lacks validation from actual clinical practitioners beyond the initial annotation phase, limiting evidence of real-world utility (no direct evidence found in the manuscript for clinical validation).\n  - The five-level hierarchical categorization system may be too complex for practical clinical implementation, as evidenced by the low consistency rates of even advanced models like GPT-4 (Figure 2b shows only 76.19% consistency).\n  - No discussion of computational costs, inference time, or scalability considerations for deployment in clinical settings where real-time performance is crucial.\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical formulation and statistical rigor**\n  - Provide formal mathematical definitions with proper notation for all components of the MediHall Score, including explicit handling of edge cases and missing data scenarios.\n  - Conduct sensitivity analysis to justify the specific score assignments and demonstrate robustness to parameter variations through systematic experimentation.\n  - Include confidence intervals and statistical significance testing for all metric comparisons, with appropriate corrections for multiple comparisons.\n\n‚Ä¢ **Expand benchmark diversity and validation methodology**\n  - Include additional medical imaging datasets from different institutions and geographical regions to improve generalizability and reduce potential dataset bias.\n  - Provide detailed inter-annotator agreement statistics (Cohen's kappa, Fleiss' kappa) and establish clear annotation protocols that can be replicated by other research groups.\n  - Replace or supplement GPT-based augmentation with human-generated questions to eliminate potential systematic biases in question formulation.\n\n‚Ä¢ **Strengthen experimental design and comparative analysis**\n  - Expand the evaluation dataset to at least 1,000-2,000 samples with stratification across different medical conditions and hallucination types for more robust statistical analysis.\n  - Include comparisons with domain-adapted versions of existing hallucination detection methods and general-purpose fact-checking systems applied to medical contexts.\n  - Report statistical significance tests, effect sizes, and confidence intervals for all performance comparisons, with proper correction for multiple hypothesis testing.\n\n‚Ä¢ **Validate clinical utility and practical deployment considerations**\n  - Conduct user studies with practicing radiologists and clinicians to evaluate the practical utility and interpretability of the proposed hierarchical categorization system.\n  - Simplify the categorization system based on clinical feedback, potentially reducing to 3-4 levels that align better with clinical decision-making workflows.\n  - Include comprehensive analysis of computational requirements, inference latency, and scalability metrics to demonstrate feasibility for clinical deployment scenarios.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript tackles hallucination detection in large vision‚Äìlanguage models (LVLMs) for medical applications. It introduces three main contributions: (1) **Med-HallMark**, the first benchmark for multimodal medical hallucination detection spanning Medical Visual Question Answering (Med-VQA) and Imaging Report Generation (IRG) tasks, structured with five severity levels of hallucination; (2) **MediHall Score**, a domain-specific evaluation metric quantifying hallucination severity; and (3) **MediHallDetector**, a fine-tuned model for hallucination identification. The study analyzes 7,341 benchmark samples and reports that existing LVLMs exhibit significant hallucination rates, with MediHall Score offering finer granularity than standard metrics. The paper is ambitious and relevant but exhibits methodological and validation deficiencies that limit confidence in its results and clinical relevance.  \n\n**Major Comments**  \n1. **Mathematical and Statistical Rigor**: The MediHall Score formulation lacks formal notation and statistical justification. Averaging across hallucination levels (H_overall = 1/k Œ£·µ¢‚Çå‚ÇÅ·µè H·µ¢) fails to capture hierarchical complexity, while the assigned severity scores appear arbitrary. No uncertainty quantification or confidence intervals are provided, restricting interpretability.  \n2. **Dataset Scope and Benchmark Validity**: The benchmark draws from only four medical datasets, limiting diversity of pathologies and imaging modalities. Details on clinician annotation, including inter-annotator agreement and protocol standardization, are missing. GPT-driven data augmentation may introduce uncontrolled biases.  \n3. **Experimental Design and Baseline Comparisons**: Evaluation is confined to 300 samples, which likely lacks statistical robustness. Comparisons focus mainly on GPT-3.5 and GPT-4, omitting other medical hallucination detection baselines. Reported results lack significance testing or variability measures.  \n4. **Clinical Relevance and Practical Utility**: The study does not involve clinical validation beyond the annotation stage. The five-level hierarchy may be overly granular, and the paper omits discussion of computational efficiency or scalability for real-world clinical use.  \n\n**Minor Comments**  \n- Clarify mathematical notation in Section‚ÄØ4 and align terminology across figures and tables.  \n- Provide explicit references for datasets and describe augmentation methodology with reproducibility details.  \n- Indicate any code or data availability statements to enhance replicability.  \n\n**Summary Paragraph**  \nThe paper introduces an important initiative toward standardized benchmarks and metrics for hallucination detection in medical LVLMs. Nonetheless, its mathematical formulation, experimental scope, and clinical validation are underdeveloped. Broader datasets, more rigorous statistical testing, and practical deployment analyses would increase its impact and credibility. With substantial methodological and validation revisions, the work could make a stronger contribution.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### The manuscript introduces Med-HallMark, a novel benchmark designed to address the critical issue of hallucination detection in medical multimodal contexts. Alongside the benchmark, the authors propose the MediHall Score as a new evaluation metric and develop the MediHallDetector, a specialized model for detecting hallucinations. The abstract succinctly outlines these contributions, emphasizing their significance in enhancing the reliability of medical applications. However, the manuscript could benefit from additional technical depth and comparative analysis to fully articulate the advantages of the proposed methods.\n\n###\n\n## Major Comments\nStrengths:\n- Clear Objectives and Novelty: The abstract effectively communicates the objectives and novelty of the research, highlighting the importance of addressing hallucinations in medical applications.\n- Comprehensive References: The reference section includes a broad range of recent studies, demonstrating the manuscript‚Äôs engagement with contemporary research. The inclusion of reputable sources enhances the credibility of the manuscript.\n\nLimitations:\n- Technical Depth: The abstract lacks detailed technical information about the implementation of the MediHall Score and the hierarchical categorization of hallucinations, which could provide deeper insight into the methodologies and results.\n- Clinical Literature Gap: The reference section predominantly focuses on technical and computational research, lacking integration of clinical literature. This omission may limit the manuscript's applicability and relevance to medical professionals.\n- Formatting Inconsistencies: The reference section has several formatting issues and incomplete citations, which detract from the professionalism of the manuscript.\n\n###\n\n## Minor Comments\n- Abstract Enhancements: The abstract could be improved by providing more concrete examples or brief explanations of the technical aspects of the proposed methods, making them clearer to non-expert readers.\n- Comparative Performance: Including specific comparative results or metrics would help illustrate the effectiveness of the proposed methods relative to existing solutions.\n- Citation Consistency: Addressing inconsistencies in the citation style and ensuring completeness of all references would enhance readability and professionalism.\n\n###\n\n## Summary Paragraph\nEvaluating the manuscript against the TMI editorial criteria:\n- Significance: The work addresses a significant issue in medical imaging and AI applications, with the potential to improve the reliability of multimodal models.\n- Innovation: The introduction of Med-HallMark, MediHall Score, and MediHallDetector represents innovative contributions to the field.\n- Evaluation: While the manuscript demonstrates the use of these innovations, more detailed comparative evaluations against existing benchmarks would strengthen the validation.\n- Reproducibility: The manuscript lacks sufficient detail to ensure reproducibility, particularly in the technical implementation of the proposed methods.\n\n###\n\n## Decision Recommendation\nMajor Revision\n\nThe manuscript presents valuable contributions to the field of medical multimodal hallucination detection but requires substantial revisions to enhance technical depth, incorporate clinical literature, and address formatting inconsistencies. These improvements will ensure the manuscript meets the high standards expected of a top-tier medical imaging journal.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **Med-HallMark**, a new benchmark aimed at detecting hallucinations in medical multimodal contexts. In addition, the authors propose the **MediHall Score** as a novel evaluation metric and develop **MediHallDetector**, a specialized model for hallucination detection. The abstract communicates these contributions clearly and highlights their importance in improving the reliability of medical AI systems. Overall, the aims and contributions are well defined, though the manuscript would benefit from further technical exposition and comparative analysis to better demonstrate the advantages of the proposed methods.  \n\n---\n\n**Major Comments**  \n1. **Clarity and Novelty** ‚Äì The objectives and novelty are well presented, and the work addresses an important challenge in medical AI. The extensive reference list reflects strong engagement with recent and relevant research, reinforcing the manuscript‚Äôs credibility.  \n2. **Technical Depth** ‚Äì The abstract and manuscript could include more detailed descriptions of the MediHall Score implementation and the hierarchical categorization of hallucinations. This additional information would clarify methodological rigor and experimental outcomes.  \n3. **Integration of Clinical Literature** ‚Äì The references emphasize computational and technical sources but overlook key contributions from the clinical domain. Including appropriate clinical studies would help improve the manuscript‚Äôs relevance to medical audiences.  \n4. **Formatting and Citation Quality** ‚Äì The reference section contains formatting inconsistencies and incomplete citations. These issues should be corrected to improve readability and professionalism.  \n\n---\n\n**Minor Comments**  \n- The abstract would be strengthened by adding brief technical explanations or examples to assist readers unfamiliar with the methods.  \n- Comparative performance data or metrics demonstrating the advantage over existing benchmarks would clarify the contribution‚Äôs significance.  \n- Ensure the citation style is consistent and all references are complete.  \n\n---\n\n**Summary Paragraph**  \nThe study is significant in addressing hallucination detection within medical multimodal systems, a topic of clear importance for trustworthy AI in healthcare. Its innovations‚ÄîMed-HallMark, MediHall Score, and MediHallDetector‚Äîrepresent a meaningful step forward. However, to meet the standards of rigorous evaluation and reproducibility, the manuscript requires deeper technical detail, comparative evaluation with existing approaches, and better integration of clinical literature. Attention to formatting and citation detail will further enhance the presentation.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The manuscript makes notable and potentially impactful contributions but needs substantial revisions to enhance technical depth, contextualization within clinical literature, and presentation quality before it can be considered for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## DETECTING AND EVALUATING MEDICAL HALLUCINATIONS IN LARGE VISION LANGUAGE MODELS\n\n### Summary\n\nThe paper introduces Med-HallMark, a benchmark for detecting and evaluating hallucinations in medical LVLMs spanning Med-VQA and imaging report generation (IRG); MediHall Score, a severity-aware metric that scores hallucination impact via a five-level hierarchy; and MediHallDetector, an LVLM-based detector fine-tuned to classify hallucination categories from image, prompt, answer, and ground-truth inputs. The authors annotate responses at utterance- (VQA) and sentence-level (IRG) with a clinician-defined hierarchy (Catastrophic ‚Üí Minor), establish baselines for several popular LVLMs, and report that MediHallDetector is more consistent and faster than GPT-3.5/4 at hallucination categorization.\n\n### Strengths\n\n- Technical novelty and innovationA severity-based, clinician-informed hierarchy for medical hallucinations that goes beyond object/attribute-only definitions common in general-domain LVLMs.A benchmark spanning both fine-grained VQA and long-form IRG with sentence-level annotations, enabling multi-granular analysis.A unified scoring scheme (MediHall Score) that aggregates hallucination categories into interpretable numeric scores for both short and long outputs.A specialized detection model (MediHallDetector) trained with multi-task data and instructions to classify hallucination categories.\n- Experimental rigor and validationBroad baseline coverage including multiple generalist and medical LVLMs across both VQA and IRG tasks using a suite of standard NLG metrics plus MediHall Score.Clinician involvement in the annotation loop for quality assurance and resolution of disagreements.Ablation over different data configurations for detector training, and speed/consistency comparisons against GPT-3.5/4.\n- Clarity of presentationClear articulation of the taxonomy, data construction pipeline, and the division between VQA vs IRG settings.Illustrative examples that show how different metrics (ROUGE/BLEU/BERTScore) can misalign with factuality and how the proposed score responds.\n- Significance of contributionsAddresses a safety-critical gap in medical multimodal evaluation‚Äîhallucination detection beyond shallow text overlap‚Äîby providing a task-rich benchmark and a severity-aligned metric.Practicality for the community if resources are indeed released: a benchmark, metric definition, and a trained detector tailored to medical outputs.\n\n- A severity-based, clinician-informed hierarchy for medical hallucinations that goes beyond object/attribute-only definitions common in general-domain LVLMs.\n- A benchmark spanning both fine-grained VQA and long-form IRG with sentence-level annotations, enabling multi-granular analysis.\n- A unified scoring scheme (MediHall Score) that aggregates hallucination categories into interpretable numeric scores for both short and long outputs.\n- A specialized detection model (MediHallDetector) trained with multi-task data and instructions to classify hallucination categories.\n\n- Broad baseline coverage including multiple generalist and medical LVLMs across both VQA and IRG tasks using a suite of standard NLG metrics plus MediHall Score.\n- Clinician involvement in the annotation loop for quality assurance and resolution of disagreements.\n- Ablation over different data configurations for detector training, and speed/consistency comparisons against GPT-3.5/4.\n\n- Clear articulation of the taxonomy, data construction pipeline, and the division between VQA vs IRG settings.\n- Illustrative examples that show how different metrics (ROUGE/BLEU/BERTScore) can misalign with factuality and how the proposed score responds.\n\n- Addresses a safety-critical gap in medical multimodal evaluation‚Äîhallucination detection beyond shallow text overlap‚Äîby providing a task-rich benchmark and a severity-aligned metric.\n- Practicality for the community if resources are indeed released: a benchmark, metric definition, and a trained detector tailored to medical outputs.\n\n### Weaknesses\n\n- Technical limitations or concernsThe detector formulation explicitly requires ground truth as an input (I, P, A, GT), which limits utility in real-world deployment where GT is unavailable; this positions MediHallDetector more as an evaluator than a detector for live use.MediHall Score relies on a discrete set of handpicked weights (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) without sensitivity analysis or external validation against clinician risk ratings; the mapping from category to clinical risk is plausible but uncalibrated.Dataset construction partially bootstraps from LVLM-generated answers and GPT-based rewriting/counterfactual generation, risking style bias and confirming patterns that detectors may implicitly learn to recognize rather than true medical errors.The method description for MediHallDetector lacks architectural and training detail (loss formulation, label formats, handling of long IRG inputs, class imbalance), and the ablation table includes confusing or unscaled values.\n- Experimental gaps or methodological issuesNo inter-annotator agreement statistics (e.g., Œ∫) are reported despite multiple rounds of clinician review, making label reliability difficult to assess.Possible circularity: parts of the evaluation of models via MediHall Score seem to lean on detector predictions; it is unclear when human labels vs detector-derived labels are used for scoring, which affects the fairness of comparisons.Limited modality scope in IRG (CXR-dominant) and unclear distributional diversity in VQA; no out-of-distribution or cross-modality generalization tests.Absence of significance testing, calibration analysis, or correlation of MediHall Score with blinded clinician risk/severity judgments to validate metric fidelity.Several baselines are reported as ‚Äúnot suitable‚Äù for IRG MediHall Score due to generation format; it should be possible to sentence-segment any textual output and score it consistently.\n- Clarity or presentation issuesSome tables contain clearly erroneous or inconsistent numbers (e.g., ROUGE-1 = 91.17 for a weak baseline; units and decimals are inconsistent in ablations) and figures/tables include artifacts from PDF extraction.The scoring equation for IRG is truncated mid-sentence in the text; training details in the supplement are referenced but not summarized.The rationale for category boundaries and concrete annotation guidelines are summarized but not operationalized with edge-case examples or decision checklists.\n- Missing related work or comparisonsRecent and closely related efforts are not cited or compared: MedHEval (cause-driven medical hallucination benchmark), HEAL-MedVQA (large Med-VQA benchmark with grounding and robustness protocols), Gut-VLM (sentence-level hallucination tags and correction on endoscopy), reference-free detectors like SUQ probes, and tool-augmented detection frameworks like UNIHD. Positioning relative to these is necessary to substantiate claims of being the first domain-specific benchmark/detector.No head-to-head against reference-free detectors (uncertainty/self-consistency/SUQ) or tool-augmented verifiers, which would contextualize MediHallDetector‚Äôs strengths/weaknesses.\n\n- The detector formulation explicitly requires ground truth as an input (I, P, A, GT), which limits utility in real-world deployment where GT is unavailable; this positions MediHallDetector more as an evaluator than a detector for live use.\n- MediHall Score relies on a discrete set of handpicked weights (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) without sensitivity analysis or external validation against clinician risk ratings; the mapping from category to clinical risk is plausible but uncalibrated.\n- Dataset construction partially bootstraps from LVLM-generated answers and GPT-based rewriting/counterfactual generation, risking style bias and confirming patterns that detectors may implicitly learn to recognize rather than true medical errors.\n- The method description for MediHallDetector lacks architectural and training detail (loss formulation, label formats, handling of long IRG inputs, class imbalance), and the ablation table includes confusing or unscaled values.\n\n- No inter-annotator agreement statistics (e.g., Œ∫) are reported despite multiple rounds of clinician review, making label reliability difficult to assess.\n- Possible circularity: parts of the evaluation of models via MediHall Score seem to lean on detector predictions; it is unclear when human labels vs detector-derived labels are used for scoring, which affects the fairness of comparisons.\n- Limited modality scope in IRG (CXR-dominant) and unclear distributional diversity in VQA; no out-of-distribution or cross-modality generalization tests.\n- Absence of significance testing, calibration analysis, or correlation of MediHall Score with blinded clinician risk/severity judgments to validate metric fidelity.\n- Several baselines are reported as ‚Äúnot suitable‚Äù for IRG MediHall Score due to generation format; it should be possible to sentence-segment any textual output and score it consistently.\n\n- Some tables contain clearly erroneous or inconsistent numbers (e.g., ROUGE-1 = 91.17 for a weak baseline; units and decimals are inconsistent in ablations) and figures/tables include artifacts from PDF extraction.\n- The scoring equation for IRG is truncated mid-sentence in the text; training details in the supplement are referenced but not summarized.\n- The rationale for category boundaries and concrete annotation guidelines are summarized but not operationalized with edge-case examples or decision checklists.\n\n- Recent and closely related efforts are not cited or compared: MedHEval (cause-driven medical hallucination benchmark), HEAL-MedVQA (large Med-VQA benchmark with grounding and robustness protocols), Gut-VLM (sentence-level hallucination tags and correction on endoscopy), reference-free detectors like SUQ probes, and tool-augmented detection frameworks like UNIHD. Positioning relative to these is necessary to substantiate claims of being the first domain-specific benchmark/detector.\n- No head-to-head against reference-free detectors (uncertainty/self-consistency/SUQ) or tool-augmented verifiers, which would contextualize MediHallDetector‚Äôs strengths/weaknesses.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe hierarchical taxonomy is medically motivated and arguably more actionable than object-only taxonomies. However, the severity mapping needs validation and clearer annotation protocols to ensure consistent application across annotators and tasks.Requiring ground truth for MediHallDetector limits its description as a ‚Äúdetector‚Äù; it is more accurately a supervised evaluator. For clinical deployment or pre-release screening, a reference-free or retrieval-augmented variant would be essential.The metric‚Äôs discrete weights are intuitive but arbitrary. A sensitivity analysis (vary weights, or learn them from clinician preference data) and a correlation study with independent clinician risk ratings would strengthen claims.Mixing training signals from general task data, benchmark labels, and instruction pairs is sensible. Yet, critical details (loss functions, negative sampling, class imbalance handling, sentence segmentation for IRG, temperature/decoding strategies for detector outputs) are missing; ablations are hard to interpret due to unit inconsistencies and vague method descriptors (a‚Äìf).\n- Experimental evaluation assessmentBaselines span many popular LVLMs, which is valuable. However, some metrics appear implausible (e.g., very high ROUGE-1 for a poor model), suggesting preprocessing or reporting errors.The decision to omit MediHall Score for some IRG outputs due to ‚Äúformat‚Äù is weak‚Äîsentence segmentation is generally feasible; a uniform segmentation policy should be applied for fairness.The speed/consistency comparison with GPT-3.5/4 is useful but lacks methodological detail (number of items, prompt templates, and exact metric of ‚Äúconsistency‚Äù), and does not include open, research-grade detectors as additional baselines.No robustness analyses (e.g., when IRG reports include benign but uninformative content, distribution shifts, different imaging modalities, or degraded image quality) are reported; related work shows these factors strongly affect detection performance.Statistical significance and effect sizes are absent. Given modest differences in MediHall Score across models for VQA, confidence intervals would help.\n- Comparison with related work (using the summaries provided)MedHEval emphasizes cause-driven taxonomy and evaluates mitigation strategies across modalities and contexts; this paper‚Äôs severity-based taxonomy is complementary but should acknowledge MedHEval and clarify differences (severity vs cause), dataset scale, and task coverage.HEAL-MedVQA contributes a large Med-VQA dataset with grounding and robustness protocols (textual/visual perturbations) and a localize-before-answer approach that reduces hallucination; incorporating HEAL‚Äôs protocols or reporting performance on HEAL would improve external validity.Gut-VLM conducts sentence-level hallucination detection and correction on endoscopy; while the modality differs, it is a closely aligned task and demonstrates a practical detection-and-correction training paradigm that this work could compare to or adopt in IRG settings.Reference-free detection studies (e.g., SUQ probes) and tool-augmented detection (UNIHD) report strong performance in general domains; including a medical adaptation or at least discussion/limitations would prevent over-claiming about the superiority of MediHallDetector.CoMT (chain-of-medical-thought) shows that structured supervision reduces hallucinations and uses a MediHall-like scoring; benchmarking the proposed measure against CoMT‚Äôs evaluation protocols or correlating with their human assessments would strengthen external alignment.\n- Discussion of broader impact and significanceA severity-aware metric and benchmark can help the community move beyond surface metrics toward safety-relevant evaluation, which is highly valuable in clinical AI.Risks include over-reliance on detector scores that are trained on benchmark-specific distributions, potential biases introduced by GPT-assisted data construction, and the mismatch between evaluator-with-GT vs real-world detector use cases.If curated and maintained well (with transparent annotation protocols, IAA reporting, external validation, and broader modalities), Med-HallMark could become a standard resource for medical LVLM safety evaluation.\n\n- The hierarchical taxonomy is medically motivated and arguably more actionable than object-only taxonomies. However, the severity mapping needs validation and clearer annotation protocols to ensure consistent application across annotators and tasks.\n- Requiring ground truth for MediHallDetector limits its description as a ‚Äúdetector‚Äù; it is more accurately a supervised evaluator. For clinical deployment or pre-release screening, a reference-free or retrieval-augmented variant would be essential.\n- The metric‚Äôs discrete weights are intuitive but arbitrary. A sensitivity analysis (vary weights, or learn them from clinician preference data) and a correlation study with independent clinician risk ratings would strengthen claims.\n- Mixing training signals from general task data, benchmark labels, and instruction pairs is sensible. Yet, critical details (loss functions, negative sampling, class imbalance handling, sentence segmentation for IRG, temperature/decoding strategies for detector outputs) are missing; ablations are hard to interpret due to unit inconsistencies and vague method descriptors (a‚Äìf).\n\n- Baselines span many popular LVLMs, which is valuable. However, some metrics appear implausible (e.g., very high ROUGE-1 for a poor model), suggesting preprocessing or reporting errors.\n- The decision to omit MediHall Score for some IRG outputs due to ‚Äúformat‚Äù is weak‚Äîsentence segmentation is generally feasible; a uniform segmentation policy should be applied for fairness.\n- The speed/consistency comparison with GPT-3.5/4 is useful but lacks methodological detail (number of items, prompt templates, and exact metric of ‚Äúconsistency‚Äù), and does not include open, research-grade detectors as additional baselines.\n- No robustness analyses (e.g., when IRG reports include benign but uninformative content, distribution shifts, different imaging modalities, or degraded image quality) are reported; related work shows these factors strongly affect detection performance.\n- Statistical significance and effect sizes are absent. Given modest differences in MediHall Score across models for VQA, confidence intervals would help.\n\n- MedHEval emphasizes cause-driven taxonomy and evaluates mitigation strategies across modalities and contexts; this paper‚Äôs severity-based taxonomy is complementary but should acknowledge MedHEval and clarify differences (severity vs cause), dataset scale, and task coverage.\n- HEAL-MedVQA contributes a large Med-VQA dataset with grounding and robustness protocols (textual/visual perturbations) and a localize-before-answer approach that reduces hallucination; incorporating HEAL‚Äôs protocols or reporting performance on HEAL would improve external validity.\n- Gut-VLM conducts sentence-level hallucination detection and correction on endoscopy; while the modality differs, it is a closely aligned task and demonstrates a practical detection-and-correction training paradigm that this work could compare to or adopt in IRG settings.\n- Reference-free detection studies (e.g., SUQ probes) and tool-augmented detection (UNIHD) report strong performance in general domains; including a medical adaptation or at least discussion/limitations would prevent over-claiming about the superiority of MediHallDetector.\n- CoMT (chain-of-medical-thought) shows that structured supervision reduces hallucinations and uses a MediHall-like scoring; benchmarking the proposed measure against CoMT‚Äôs evaluation protocols or correlating with their human assessments would strengthen external alignment.\n\n- A severity-aware metric and benchmark can help the community move beyond surface metrics toward safety-relevant evaluation, which is highly valuable in clinical AI.\n- Risks include over-reliance on detector scores that are trained on benchmark-specific distributions, potential biases introduced by GPT-assisted data construction, and the mismatch between evaluator-with-GT vs real-world detector use cases.\n- If curated and maintained well (with transparent annotation protocols, IAA reporting, external validation, and broader modalities), Med-HallMark could become a standard resource for medical LVLM safety evaluation.\n\n### Questions for Authors\n\n- Does MediHallDetector strictly require ground-truth (GT) at inference? If so, do you have a reference-free variant or retrieval-augmented alternative for real-world deployment where GT is absent?\n- What are the inter-annotator agreement statistics (e.g., Cohen‚Äôs/Fleiss‚Äô Œ∫) for hallucination category labels at utterance- and sentence-level? How often did the lead clinician override disagreements?\n- How were the MediHall Score weights chosen for each severity level? Did you conduct sensitivity analyses or collect clinician preference data correlating categories with clinical risk to justify the weighting?\n- For the baseline results reported with MediHall Score, did you use human annotations or MediHallDetector predictions? If the latter, how do you avoid circularity when comparing MediHallDetector to other models?\n- How did you prevent training‚Äìtest leakage between the detector‚Äôs SFT data and the benchmark‚Äôs evaluation set, given you mix ‚ÄúMed-HallMark data‚Äù into detector training?\n- Why are MediHall Scores omitted for IRG outputs from some models due to ‚Äúgeneration format‚Äù? Can you standardize sentence segmentation and apply the same scoring pipeline to all models?\n- Beyond CXR and the datasets used (SLAKE, VQA-RAD, MIMIC, OpenI), do you plan to include other modalities (CT, MRI, ultrasound) and clinical domains? What is the current modality/pathology distribution?\n- Can you provide detailed training settings for MediHallDetector (losses, label encoding, class weights/imbalance handling, tokenization/segmentation for IRG) and release code/checkpoints to ensure reproducibility?\n- Have you validated that MediHall Score correlates with blinded clinician assessments of clinical risk/harm for generated reports? If so, please report correlation or agreement statistics.\n- Several tables contain values that appear erroneous or unit-inconsistent (e.g., ROUGE-1 ‚âà 91, ablation ACC/Recall units). Can you audit and correct these numbers and clarify metric scaling?\n\n### Overall Assessment\n\nThis paper addresses an important and underexplored problem: how to detect and quantify clinically meaningful hallucinations in medical LVLM outputs. The proposed benchmark, severity-aware metric, and dedicated detector constitute a useful package that can push evaluation beyond shallow text overlap. The taxonomy is intuitive and clinician-informed, the dataset spans both VQA and IRG with granular labels, and the experiments cover many popular LVLMs. However, several core aspects need strengthening before publication at a top venue: (i) the detector‚Äôs reliance on ground truth limits its practical detection utility; (ii) the MediHall Score weights and category boundaries lack validation and sensitivity analysis; (iii) inter-annotator agreement and leakage controls are not reported; (iv) comparisons miss closely related, recent medical hallucination benchmarks and detection methods; and (v) parts of the empirical reporting appear inconsistent or error-prone. With clearer methodological details, reliability measures, external validation (e.g., correlation with clinical risk ratings), broader modality coverage, and head-to-head comparisons against contemporary medical hallucination resources and detectors, this work could become a valuable reference point for the community.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 2,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Med-HallMark*, a comprehensive benchmark for detecting and evaluating hallucinations in medical large vision‚Äìlanguage models (LVLMs). It introduces three core components: a clinician-informed hierarchical taxonomy of hallucination severity, the *MediHall Score* for quantifying hallucination impact, and the *MediHallDetector*, a fine-tuned LVLM designed to classify hallucination categories using multimodal and textual inputs. The work spans both medical VQA and imaging report generation tasks, includes multi-level clinician annotation, and compares multiple baseline LVLMs. Overall, the paper is clearly written and addresses a critical gap in safety-centered evaluation for medical AI, though several technical, methodological, and reporting issues limit its current robustness and generalizability.  \n\n**Major Comments**  \n1. **Detector and Metric Design** ‚Äì The *MediHallDetector* requires ground truth input, which restricts its use to evaluation rather than real-world detection. The *MediHall Score* relies on unvalidated, manually chosen weights; no sensitivity or clinician risk calibration analyses are provided.  \n2. **Dataset and Training Details** ‚Äì The benchmark partially uses LVLM- and GPT-generated text, potentially introducing stylistic bias. Key detector specifications (architecture, loss functions, class imbalance handling, IRG segmentation) are underdescribed, and reported ablations contain unscaled or inconsistent metrics.  \n3. **Annotation and Evaluation Reliability** ‚Äì No inter-annotator agreement is reported despite multi-clinician labeling. It is unclear when human versus detector labels are applied in scoring, creating potential circularity. Significant testing, calibration, and external validation (e.g., correlation with clinician harm ratings) are missing.  \n4. **Scope and Comparisons** ‚Äì The dataset is CXR-dominant with limited modality diversity. The paper omits comparison to recent closely related works (MedHEval, HEAL-MedVQA, Gut-VLM, SUQ probes, UNIHD) and lacks direct testing against reference-free or tool-augmented detectors.  \n5. **Presentation and Reporting Issues** ‚Äì Tables contain implausible or inconsistent numbers; one scoring equation is truncated; and annotation guidelines lack clarity on boundary cases. These issues hinder reproducibility and interpretation.\n\n**Minor Comments**  \n- Some baseline evaluations omit *MediHall Score* on formatting grounds though sentence segmentation could normalize outputs.  \n- Provide full training hyperparameters and code/checkpoints for reproducibility.  \n- Correct numerical inconsistencies (e.g., ROUGE anomalies, unit mismatches).  \n- Clarify data splits and leakage prevention between benchmark and detector training.  \n- Expand discussion of modalities (CT, MRI, ultrasound) and future dataset extensions.\n\n**Summary Paragraph**  \nThis is a timely and ambitious effort addressing the safety evaluation of medical LVLMs through a unified benchmark, severity-aligned metric, and specialized detector. The work‚Äôs clinical grounding and comprehensive scope are strong assets. However, validation gaps in the proposed metric, dependence on ground-truth inputs, incomplete methodological detail, and missing comparisons to contemporaneous benchmarks and detectors undermine current credibility. Addressing these concerns‚Äîparticularly validation of weighting schemes, inter-annotator reliability, numerical accuracy, and broader modality coverage‚Äîwould substantially strengthen the paper‚Äôs contribution and impact.  \n\n**Decision Recommendation**  \n**Major Revision.** The study offers notable value for medical AI safety research but requires clarification, methodological validation, and comparative rigor before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: \"Detecting and Evaluating Medical Hallucinations in Large Vision Language Models\"\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical problem of hallucinations in Large Vision Language Models (LVLMs) when applied to medical contexts, where such errors can have serious clinical consequences. The authors identify a significant gap in the field: while hallucination detection has received attention in general LVLM research, no dedicated methods or benchmarks exist for the medical domain. To bridge this gap, the authors introduce Med-HallMark, the first benchmark specifically designed for medical hallucination detection that provides multi-task hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. They also propose MediHall Score, a new medical evaluative metric that assesses hallucinations through a severity-based hierarchical scoring system, and MediHallDetector, a novel medical LVLM engineered for precise hallucination detection. Through extensive experimental evaluations, the authors establish baselines for popular LVLMs, demonstrating that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and that MediHallDetector significantly outperforms general-purpose models like GPT-4 in hallucination detection tasks. The work represents a foundational contribution to ensuring the reliability of LVLMs in high-stakes medical applications.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The paper addresses a critical and previously neglected problem in medical AI, where hallucinations can have serious clinical consequences. This represents a significant contribution to the field.\n- The hierarchical hallucination categorization is particularly valuable, moving beyond technical classifications to consider clinical severity and impact on decision-making, which better aligns with medical needs.\n- The Med-HallMark benchmark is well-constructed with thoughtful multi-task support, diverse question types (conventional, confidence-weakening, counterfactual), and thorough annotation that addresses limitations of existing medical benchmarks.\n- The MediHall Score effectively addresses a key limitation of traditional metrics by distinguishing between different severity levels of hallucinations, providing clinically meaningful evaluation.\n- The MediHallDetector model demonstrates clear superiority over general-purpose models in hallucination detection, with rigorous ablation studies supporting the design choices.\n\n**Limitations:**\n- The clinical validation of the hallucination categorization system is insufficient. While clinicians assisted in creating the categories, there's no evidence of validation with practicing clinicians regarding how these categories actually impact real-world medical decisions.\n- The benchmark focuses almost exclusively on radiology (X-rays, CT scans), with limited representation of other medical imaging modalities like MRI, ultrasound, or pathology slides, limiting the approach's generalizability to broader medical contexts.\n- The paper lacks assessment of how these hallucinations would impact actual clinical workflows or patient outcomes, making it difficult to fully evaluate the importance of the different hallucination categories in practice.\n- The English-only limitation significantly restricts the practical applicability of the tools, as medical practice occurs in many languages globally.\n\n### Minor Comments\n\n**Strengths:**\n- The presentation of hierarchical categorization with clear examples is accessible to both technical and medical readers.\n- The comprehensive experimental setup with various question types provides a thorough evaluation of model robustness.\n- The authors' commitment to reproducibility through resource release and detailed methodological documentation is commendable.\n- The comparison between different models across multiple dimensions (inference time, consistency, accuracy) provides valuable practical insights.\n\n**Limitations:**\n- The paper could better address how ambiguous cases (where medical experts might disagree on interpretation) would be handled within the hallucination framework.\n- There's no comparison to human performance on the hallucination detection tasks, which would provide a meaningful baseline.\n- Some results are presented in the main paper while others are relegated to the appendix, making the narrative flow less cohesive.\n- The use of GPT-4 for creating counterfactual questions may introduce bias, as GPT-4 itself can hallucinate.\n\n## 3. Evaluation Summary\n\n**Significance:** The work addresses a critical safety issue in medical AI with direct implications for patient care. As LVLMs become more integrated into healthcare, ensuring their reliability is paramount. The paper establishes foundational tools for hallucination detection in medical contexts, making it highly significant. However, the narrow focus on radiology somewhat limits the breadth of impact.\n\n**Innovation:** The paper is highly innovative in creating the first medical hallucination-specific benchmark and evaluation framework. The hierarchical categorization of hallucinations by clinical severity rather than technical characteristics represents a novel approach that better serves medical needs. The dedicated hallucination detection model also introduces a new direction in medical AI safety research.\n\n**Evaluation:** The evaluation is thorough in comparing multiple models across various metrics and question types, with strong ablation studies for the detector. However, the lack of clinical validation and real-world impact assessment weakens the evaluation. The comparison to human performance on hallucination detection would have strengthened the methodology.\n\n**Reproducibility:** The authors have made significant efforts toward reproducibility by releasing resources and providing detailed information about data construction and model training. The appendix includes comprehensive details about benchmark construction and experimental setup, meeting TMI's high standards for reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision (Reject/Resubmit)**\n\nThis paper makes a significant contribution to an important and underexplored area of medical AI safety. The foundational nature of the work‚Äîestablishing the first medical hallucination benchmark and evaluation framework‚Äîmakes it a valuable addition to the field. However, the current version has limitations that need to be addressed before publication.\n\nThe authors should:\n1. Expand the discussion of clinical validation, providing evidence of how the hallucination categories were validated with practicing clinicians for real-world relevance\n2. Address the narrow focus on radiology by either expanding the benchmark to include other modalities or more clearly articulating the scope limitations\n3. Strengthen the discussion of how hallucination severity maps to clinical impact with input from medical professionals\n4. Expand the future work section to address the language limitation more comprehensively\n\nThese revisions would significantly strengthen an already valuable contribution to ensuring the safety and reliability of AI in medical applications. The work has strong potential to become a foundational reference in the field once these concerns are adequately addressed.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 3,
          "specificity": 2,
          "correctness": 2,
          "constructiveness": 2,
          "stance": 2,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript ‚ÄúDetecting and Evaluating Medical Hallucinations in Large Vision Language Models‚Äù investigates hallucination behavior in large vision-language models (LVLMs) applied to medical imaging. The authors identify the absence of medical-domain benchmarks and propose three contributions: (1) **Med-HallMark**, the first benchmark tailored to detect and evaluate medical hallucinations, incorporating multi-task support and hierarchical hallucination categorization; (2) **MediHall Score**, a severity-based hierarchical evaluation metric; and (3) **MediHallDetector**, a dedicated detection model shown to outperform general-purpose LVLMs such as GPT‚Äë4. The paper presents extensive evaluations illustrating that MediHall Score and MediHallDetector yield a more nuanced and clinically grounded understanding of hallucinations. The topic is timely and addresses a critical aspect of medical AI reliability.  \n\n**Major Comments**  \n1. **Clinical validation:** The hallucination categorization system lacks validation with practicing clinicians. Although clinicians contributed to category creation, there is no demonstration of its alignment with real-world medical decision-making.  \n2. **Domain scope:** The dataset is dominated by radiology images (X‚Äëray, CT) with minimal inclusion of other modalities (MRI, ultrasound, pathology), limiting generalizability across medical imaging.  \n3. **Clinical relevance:** The study does not assess how hallucinations impact clinical workflows or patient outcomes, which makes the practical utility of the categories hard to gauge.  \n4. **Language limitations:** The current English-only design reduces global applicability in multilingual medical contexts.  \n\n**Minor Comments**  \n- The hierarchical taxonomy is clearly explained with accessible examples for both clinical and technical audiences.  \n- The experimental design is comprehensive, including diverse question types and well-documented setup, supporting reproducibility.  \n- Results presentation could be more cohesive; moving key findings from the appendix to the main text would improve readability.  \n- The framework does not discuss handling of ambiguous medical cases or disagreement between experts.  \n- The benchmark lacks a baseline comparison to human annotators.  \n- Using GPT‚Äë4 to generate counterfactual questions may introduce bias from model hallucinations.  \n\n**Summary Paragraph**  \nOverall, the manuscript tackles an important and underexplored challenge in medical AI safety. Its contributions‚Äîthe first medical hallucination benchmark, a severity-aware scoring metric, and a dedicated detection model‚Äîare innovative and potentially foundational. The work is methodologically thorough and reproducible but limited by its narrow clinical domain, absent validation with practitioners, and missing analysis of real-world clinical impact. These aspects should be addressed to realize the paper‚Äôs full potential and ensure translational relevance.  \n\n**Decision Recommendation**  \n**Major Revision (Reject and Resubmit)** ‚Äì The work is significant and promising but requires stronger clinical validation, expanded modality coverage or clearer scope definition, discussion of clinical relevance, and treatment of language limitations before it is ready for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe authors tackle the problem of hallucinations in large vision‚Äëlanguage models (LVLMs) used for medical tasks, where erroneous outputs may have serious clinical ramifications. To address this, they introduce a benchmark called Med‚ÄëHallMark that includes medical visual‚Äëquestion‚Äëanswering and imaging‚Äëreport generation tasks, supporting both open‚Äëended and closed‚Äëended formats. The benchmark categorises hallucinations into five hierarchical severity levels. Based on this taxonomy, they propose a new metric, the MediHall‚ÄØScore, which assigns numeric values according to the identified hallucination category for each response. They also present a detection model, MediHallDetector, trained with multitask supervision on conventional medical vision‚Äëlanguage data, Med‚ÄëHallMark instances, and hallucination‚Äëfocused instruction pairs. Baseline experiments on several existing LVLMs are reported to illustrate the benchmark‚Äôs utility and the detector‚Äôs purported advantage over standard metrics in recognising different hallucination types.  \n\n---\n\n**## General feedback**  \n\n- **Significance:** The manuscript correctly points out that hallucinations in medical LVLMs can lead to unsafe clinical decisions, and a domain‚Äëspecific benchmark together with a dedicated metric does fill an evident gap (Abstract; Sec‚ÄØ3).  \n- **Innovation:** The hierarchical hallucination taxonomy and the MediHall‚ÄØScore are presented as novel adaptations for medical text generation (Fig‚ÄØ1c, Sec‚ÄØ4). In practice, however, the taxonomy largely reproduces existing object/attribute/relational categories with added severity labels, and the authors provide no empirical evidence that these severity levels correspond to clinically meaningful distinctions.  \n- **Evaluation:** Extensive baseline results are shown (Tables‚ÄØ1‚Äë4,‚ÄØ6‚Äë8) along with a human‚Äëagreement study (Fig‚ÄØ2b,c). Yet the paper lacks statistical significance testing, clear definitions of ‚Äúagreement,‚Äù and any correlation analysis between the MediHall‚ÄØScore and established metrics, which weakens the claim of improvement.  \n- **Reproducibility:** Although the authors state that code and data are open‚Äësource, they omit URLs, versioning information, and licensing terms. Crucial hyper‚Äëparameters (e.g., batch size, data splits, prompting templates) are relegated to the supplement (Sec‚ÄØ5, App‚ÄØA.2‚ÄëA.3), making replication less straightforward.  \n\n---\n\n**## Specific comments/critiques**  \n\n- The benchmark is limited to English texts; there is no discussion of multilingual applicability (A.6) or strategies to mitigate this limitation.  \n- Confidence‚Äëweakening prefixes are manually crafted (Fig‚ÄØ4) but the manuscript does not quantify their effect on model confidence or output quality.  \n- Counter‚Äëfactual questions are generated by GPT‚Äë4 without grounding in the associated images (Fig‚ÄØ5). The proportion of automatically generated versus human‚Äëcurated items is unclear, raising concerns about annotation reliability.  \n- The hierarchical score intervals (e.g., catastrophic‚ÄØ=‚ÄØ0.0‚Äì0.2, critical‚ÄØ=‚ÄØ0.2‚Äì0.4, ‚Ä¶) are presented without justification linking them to clinical risk, and no validation against radiologist judgments is provided (Sec‚ÄØ4).  \n- Comparisons with GPT‚Äë3.5/4 (Fig‚ÄØ2c) are based on only 300 inferences and lack statistical testing; confidence intervals for the reported 93.88‚ÄØ% consistency are absent.  \n- The ablation study (Table‚ÄØ5) reports accuracy and recall for each hallucination type but omits precision, F1, and uncertainty estimates, limiting insight into trade‚Äëoffs.  \n- Details of the dataset split (training/validation/test) for Med‚ÄëHallMark are not explicitly described (Sec‚ÄØ3.4), hindering reproducibility.  \n- Tables‚ÄØ1‚Äë4 mix traditional metrics with the MediHall‚ÄØScore yet provide no correlation analyses to demonstrate that the new score captures hallucination severity more effectively than BLEU, ROUGE, or BERTScore.  \n- The ethical statement claims ‚Äúno new user research‚Äù while also mentioning a ‚Äúdouble‚Äëblind restriction‚Äù that prevents submission of ethics approvals; this inconsistency requires clarification (Sec‚ÄØD).  \n\n---\n\n**## A suggested decision**  \n\n**Major Revision**  \n\nThe work addresses an important problem and introduces potentially useful resources, but substantial methodological and reporting issues need to be resolved before the manuscript can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates hallucination phenomena in large vision‚Äìlanguage models (LVLMs) applied to medical imaging tasks, emphasizing their potential clinical risks. To address these issues, the authors introduce a benchmark, **Med‚ÄëHallMark**, which covers both visual‚Äëquestion‚Äëanswering and imaging‚Äëreport generation tasks, encompassing five hierarchical levels of hallucination severity. They further develop a quantitative metric, **MediHall‚ÄØScore**, and a detection model, **MediHallDetector**, intended to classify and assess hallucinations based on this taxonomy. Baseline experiments on various LVLMs and a human‚Äëagreement study are presented to demonstrate the benchmark‚Äôs utility. While the topic is timely and relevant, several conceptual, methodological, and reproducibility concerns limit the current strength of the study.  \n\n---\n\n**Major Comments**  \n1. **Significance and novelty:** The focus on hallucination detection in medical LVLMs is relevant, and the combination of a benchmark and dedicated metric fills a clear gap. However, the proposed hierarchy of hallucination severity largely reuses object/attribute/relational distinctions common in prior work, with the added severity levels lacking empirical validation for clinical relevance.  \n2. **Evaluation design:** Although the manuscript reports extensive baseline studies (Tables‚ÄØ1‚Äë4,‚ÄØ6‚Äë8) and a human‚Äëagreement experiment, statistical significance testing, definitions of ‚Äúagreement,‚Äù and correlation analyses with established metrics are missing. These omissions weaken claims of improvement.  \n3. **Reproducibility:** The paper lacks URLs, versioning, and licensing for the stated open‚Äësource resources. Critical hyper‚Äëparameters and prompting details are confined to the appendix, making reproduction difficult. Dataset splits for Med‚ÄëHallMark are not explicitly described.  \n4. **Benchmark limitations:** The dataset is restricted to English without discussion of multilingual generalization. Counter‚Äëfactual questions generated by GPT‚Äë4 are not image‚Äëgrounded, and the human‚Äëto‚Äëautomatic annotation ratio is unspecified, raising reliability concerns.  \n5. **Metric validation:** The MediHall‚ÄØScore intervals are not justified in terms of clinical risk, nor validated with expert (radiologist) judgments. Correlation analyses with established metrics such as BLEU, ROUGE, or BERTScore are absent.  \n6. **Experimental reporting:** Comparisons with GPT‚Äë3.5/4 rely on 300 inferences and omit confidence intervals; ablation results exclude precision, F1, and uncertainty analysis.  \n7. **Ethical documentation:** The ethics statement contains an inconsistency regarding human data usage and ethics approvals that requires clarification.  \n\n---\n\n**Minor Comments**  \n- The effect of manually designed confidence‚Äëweakening prefixes is not quantified.  \n- Figures and tables could include explicit statistical indicators (confidence intervals, p‚Äëvalues).  \n- Clarify the proportion of auto‚Äëgenerated versus human‚Äëcurated questions.  \n- Ensure consistent terminology and provide concise cross‚Äëreferences between sections.  \n\n---\n\n**Summary Paragraph**  \nOverall, the submission tackles an important and under‚Äëexplored safety issue in medical vision‚Äëlanguage modeling and introduces potentially valuable resources. Nonetheless, the novelty of the taxonomy is limited, and methodological rigor‚Äîparticularly in statistical evaluation, validation of the scoring scale, and reproducibility documentation‚Äîrequires improvement. The contribution would be strengthened by clearer empirical validation, transparent data and code release, and more thorough reporting of experimental uncertainty.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper addresses a significant problem and proposes useful direction, but substantial revisions in methodology, validation, and documentation are needed before it can be recommended for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Dingkang Yang",
      "Dongling Xiao",
      "Jiawei Chen",
      "Ke Li",
      "Lihua Zhang",
      "Mingcheng Li",
      "Shunli Wang",
      "Tong Wu",
      "Xiaolu Hou",
      "Yue Jiang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_e3a42a649db419e8ec7cf4e6173f60905540569e.pdf",
    "remote_url": "https://openreview.net/pdf/e3a42a649db419e8ec7cf4e6173f60905540569e.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning to Teach: Improving Mean Teacher in Semi-supervised Medical Image Segmentation with Dynamic Decay  Modulation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "transfer learning, meta learning, and lifelong learning"
    ],
    "keywords": [
      "Meta learning",
      "Medical image segmentation",
      "semi-supervised learning"
    ],
    "abstract": "Medical image segmentation is essential in medical diagnostics but is hindered by the scarcity of labeled three-dimensional imaging data, which requires costly expert annotations. Semi-supervised learning (SSL) addresses this limitation by utilizing large amounts of unlabeled data alongside limited labeled samples. The Mean Teacher model, a prominent SSL method, enhances performance by employing an Exponential Moving Average (EMA) of the student model to form a teacher model, where the EMA decay coefficient is critical. However, using a fixed coefficient fails to adapt to the evolving training dynamics, potentially restricting the model's effectiveness. In this paper,\nwe propose Meta MeanTeacher, a novel framework that integrates meta-learning to dynamically adjust the EMA decay coefficient during training. We approach proposed Dynamic Decay Modulation (DDM) module in our Meta MeanTeacher framework, which captures the representational capacities of both student and teacher models. DDM heuristically learns the optimal EMA decay coefficient by taking the losses of the student and teacher networks as inputs and updating it through pseudo-gradient descent on a meta-objective. This dynamic adjustment allows the teacher model to more effectively guide the student as training progresses.\nExperiments on two datasets with different modalities, i.e., CT and MRI, show that Meta MeanTeacher consistently outperforms traditional Mean Teacher methods with fixed EMA coefficients. Furthermore, integrating Meta MeanTeacher into state-of-the-art frameworks like UA-MT, AD-MT, and PMT leads to significant performance enhancements, achieving new state-of-the-art results in semi-supervised medical image segmentation.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces the Meta Mean Teacher framework, a novel approach to improve semi-supervised medical image segmentation. Traditional Mean Teacher models use a fixed Exponential Moving Average (EMA) decay coefficient to update the teacher model, but this fixed value often limits model effectiveness. Meta Mean Teacher addresses this limitation by introducing a Dynamic Decay Modulation (DDM) module that adaptively adjusts the EMA decay coefficient based on training dynamics. This dynamic adjustment optimizes the student-teacher learning process, enabling better performance in tasks with limited labeled data.\n\nKey contributions of this work include:\n\n1. Adaptive EMA Decay: The DDM module optimizes the EMA decay coefficient, enhancing the model's adaptability and enabling it to capture richer training representations.\n2. Plug-and-Play Architecture: Meta Mean Teacher is designed to integrate seamlessly into existing models, improving performance across various Mean Teacher-based methods.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. Dynamic Adaptability: By incorporating the Dynamic Decay Modulation (DDM) module, the framework dynamically adjusts the EMA decay coefficient (Œ±) during training. This adaptability ensures that the teacher model evolves effectively with the student model, allowing more precise guidance as training progresses. This approach addresses a common limitation in fixed-coefficient Mean Teacher models, which often fail to account for varying training dynamics.\n\n2. Plug-and-Play Module: The Meta Mean Teacher framework is designed as a modular system, making it highly compatible with existing models based on the Mean Teacher architecture. This modularity allows easy integration into various semi-supervised frameworks like UA-MT, AD-MT, and PMT.\n\n3. Enhanced Stability and Robustness: The framework benefits from the Mean Teacher method‚Äôs inherent stability due to EMA but improves upon it by learning an optimal decay coefficient through meta-learning techniques.\n\n### Weaknesses\n\n1. High Computational Overhead: The adaptive EMA adjustment via DDM introduces complexity and requires more computational resources. The dynamic adjustment process, which includes cloning and iterative updates of both teacher and student models, may not be feasible for real-time or resource-limited applications, especially when processing large 3D medical imaging data.\n\n2. Limited Exploration of Other Adaptive Techniques: While the paper focuses on dynamically adjusting the EMA decay coefficient, other hyperparameters (like learning rates or loss weight factors) could also impact model performance in semi-supervised learning. The focus on only one parameter might restrict the overall optimization potential, as additional adjustments could further enhance the segmentation quality.\n\n### Questions\n\n1. On the impact of Œ±=0.01: Why does the model show improvement when Œ± is set to 0.01? This result seems to contradict the explanation provided in Section 3.1.\n\n2. The use of fixed Œ± in the ablation experiments in Section 4.3: In Section 4.3, why was the average fixed Œ± method chosen for comparison? From Figure 1, we can see that lower Œ± values ‚Äã‚Äã(such as 0.03, 0.05, and 0.1) significantly degrade the performance. In contrast, Œ± of 0.97 achieves performance higher than 0.84. Doesn't this general average comparison seem a bit biased?\n\n3. The impact of Œ± greater than 0.5: From Table 1, we can see that when Œ± is greater than 0.5, its impact on performance becomes less significant. Test whether randomly selected Œ± values ‚Äã‚Äãgreater than 0.5 are beneficial, thereby potentially improving the results?\n\n4. Suspicion about the data in Section 4.4 compared with other methods: Why is your experimental setting different from that in \"Alternative Diversified Teaching of Semi-Supervised Medical Image Segmentation\", but most of the state-of-the-art data (including LA and Pancreas-NIH datasets) are the same as the data in that paper? Does this indicate that the data is directly borrowed?\n\n5. The m symbol in Figure 2: What does \"m\" mean in Figure 2? Is this symbol redundant?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents the *Meta Mean Teacher* framework, which aims to enhance semi-supervised medical image segmentation by introducing a meta-learning mechanism for adaptive model updating. Unlike traditional Mean Teacher models that rely on a fixed Exponential Moving Average (EMA) decay coefficient, this approach integrates a Dynamic Decay Modulation (DDM) module to adjust the EMA coefficient based on training dynamics. This adaptive mechanism intends to improve stability and training efficiency, particularly in settings with limited labeled data. Overall, the paper is clearly written and introduces a technically coherent methodology.\n\n**Major Comments**  \n1. **Computational Complexity:** The DDM module increases the computational burden, as its adaptive EMA adjustment requires model cloning and iterative updates for both student and teacher networks. This may hinder scalability and make the method unsuitable for real-time or resource-constrained applications, especially in large-scale 3D medical image segmentation.  \n2. **Scope of Adaptation:** The paper focuses solely on adapting the EMA decay coefficient, neglecting other important hyperparameters such as the learning rate or loss weighting factors. Exploring additional adaptive mechanisms could yield further performance gains and a more comprehensive optimization strategy.  \n3. **Experimental Clarifications:** Several points in the experimental results require clarification. Specifically:  \n   - Why does the model perform better when Œ±=0.01, which appears inconsistent with the theoretical explanation in Section 3.1?  \n   - In Section 4.3, the choice of using the average fixed Œ± for comparison seems biased, given that lower Œ± values (e.g., 0.03‚Äì0.1) degrade performance while higher Œ± (e.g., 0.97) performs better.  \n   - Table 1 suggests diminishing returns when Œ±>0.5; the benefit of random Œ± values above this threshold could be tested.  \n   - The experimental setup in Section 4.4 resembles that of the ‚ÄúAlternative Diversified Teaching‚Äù paper, though the datasets cited are identical‚Äîclarify whether data were reused or newly processed.  \n   - In Figure 2, the meaning of the ‚Äúm‚Äù symbol should be explained or removed if redundant.\n\n**Minor Comments**  \n- Clarify notation and provide consistent symbol definitions throughout figures.  \n- Ensure all parameter values mentioned in text match those shown in tables and figures.\n\n**Summary Paragraph**  \nThis work introduces a conceptually appealing adaptive extension to the Mean Teacher framework, offering improved stability and generalizability through meta-learned EMA decay. Its modular, plug-and-play design is a practical strength. However, concerns remain regarding computational cost, limited exploration of other adaptive factors, and unclear presentation of some experimental choices and data correspondences. With these issues addressed, the paper would offer a more robust contribution to semi-supervised learning in medical imaging.\n\n**Decision Recommendation**  \nMajor Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper explores the EMA decay coefficient within the MT semi-supervised framework, fully tapping into the potential of the MT framework. Additionally, it introduces a novel meta-learning strategy to dynamically find the optimal EMA decay coefficient during the training process. Experiments conducted on two medical image datasets demonstrate that this method achieves superior performance.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. This paper introduces a novel meta-learning strategy to adjust the EMA decay coefficient, fully tapping into the potential of the MT semi-supervised framework.\n2. This paper introduces a strategy to adjust the EMA decay coefficient to improve semi-supervised segmentation performance, which could be a meaningful contribution to this field.\n3. The extensive experimental results show the effectiveness of the proposed method.\n\n### Weaknesses\n\n1. I have not observed many innovative aspects in the application of meta-learning to the field of semi-supervised medical image segmentation. Part of the reason for this is the clarity of the writing; it is currently unclear what significant differences exist between the proposed DDM and previous meta-learning strategies. If there are no substantial differences, then the methodological contribution of this approach appears to be quite limited.\n2. Could the authors explain what potential drawbacks a fixed EMA decay coefficient might have on the MT framework, particularly in the context of medical image processing?\n3. The motivation is unclear. I do not understand why a dynamic change in $\\alpha$ would have a greater advantage compared to a fixed value. $\\alpha$ can be understood as the weight distribution between the teacher model‚Äôs parameters and the student model‚Äôs parameters during the iterative update process, with the teacher model‚Äôs weight being overwhelmingly dominant. I question the assumption that dynamically varying $\\alpha$ between 0.95 and 0.99 is necessarily better than a fixed value of 0.97. Could you provide a plot showing how $\\alpha$ changes dynamically over training iterations in the experiments?\n4. In Equation 2, what criteria does DDM use to derive Œ±m? Is there a relationship between $\\alpha_m$ and these two losses? For example, if the teacher model has a lower loss, should $\\alpha_m$ be larger? Please explain.\n5. The notation in the paper is somewhat confusing. In Equation 1, what are the differences between $\\Theta_s^*$ and $\\Theta_s$, and between $\\Theta_s$ and $\\theta_s$? Additionally, what is meant by meta data $\\mathcal{D}_m$, and how does it differ from labeled data and unlabeled data? Furthermore, the $\\mathcal{L}_m$ formula is missing; I suggest adding it.\n6. What is the initial value of $\\alpha$? Has an ablation study been conducted to verify the impact of $\\alpha$?\n7. The authors mention that DDM can be promoted as a plug-and-play component for different models. However, I think DDM has limitations. For instance, how would DDM be applied to commonly used pseudo-labeling methods based on CPS (Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision)?\n\n### Questions\n\nPlease refer to the weakness.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the EMA decay coefficient within the Mean Teacher (MT) semi-supervised learning framework and proposes a meta-learning-based strategy to dynamically optimize this coefficient during training. The study aims to enhance semi-supervised medical image segmentation performance and reports superior results on two datasets. Overall, the paper presents a clear motivation and an interesting adaptation of meta-learning to this context, although certain methodological aspects and explanations remain insufficiently elaborated.\n\n**Major Comments**  \n1. The claimed methodological novelty is unclear. The distinction between the proposed Dynamic Decay Mechanism (DDM) and prior meta-learning strategies in semi-supervised segmentation is not adequately justified. Without clear differences, the contribution appears limited.  \n2. The motivation for dynamically varying the EMA decay coefficient (Œ±) rather than using a fixed value is not fully explained. A theoretical or empirical rationale is needed, along with a plot showing Œ± variations over training iterations.  \n3. The paper should elaborate on the potential drawbacks of a fixed EMA coefficient within the MT framework, particularly in medical imaging applications.  \n4. In Equation (2), the criteria used by DDM to derive Œ±‚Çò should be clarified. The relationship between Œ±‚Çò and the losses of the teacher and student models is not explicit; for example, whether Œ±‚Çò increases when the teacher loss decreases.  \n5. Notation requires clarification. The distinctions among Œò‚Çõ*, Œò‚Çõ, and Œ∏‚Çõ are unclear, as are the definitions of meta data ùîá‚Çò versus labeled/unlabeled data. The formula for the meta-loss ùìõ‚Çò is missing and should be included.  \n6. The initial value of Œ± and its influence should be discussed. An ablation study examining the effect of Œ± initialization would strengthen the paper.  \n7. Although the authors claim DDM is a plug-and-play component, its applicability to other semi-supervised methods (e.g., CPS-based pseudo-labeling) is uncertain and should be discussed.\n\n**Minor Comments**  \n- Improve clarity and consistency in mathematical notation.  \n- Ensure all symbols are defined when first introduced.  \n- Revise text for improved readability and precision.\n\n**Summary Paragraph**  \nThe paper proposes a meta-learning extension to dynamically adjust EMA decay in the MT semi-supervised framework, supported by experimental evidence of performance gains. Strengths include an interesting application direction and promising empirical results. However, the work‚Äôs novelty and methodological distinctions are insufficiently clear, and several definitions, motivations, and notations need refinement. Further theoretical justification and improved presentation would enhance the paper‚Äôs impact.\n\n**Decision Recommendation**  \nMajor revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents the 'Meta Mean Teacher', an approach for semi-supervised medical image segmentation. Building on the Mean Teacher model, which leverages exponential moving average (EMA) to create a stable teacher model from a student model, this framework introduces the Dynamic Decay Modulation (DDM) module. DDM dynamically adjusts the EMA decay coefficient based on both the student and teacher losses, improving the model's adaptability during training.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nThe paper addresses semi-supervised learning in medical image segmentation with a novel meta-learning approach, introducing the Dynamic Decay Modulation (DDM) module to adjust the EMA decay coefficient dynamically. \n\nThe paper strengthens its empirical evaluation by testing on three datasets, covering different imaging modalities.\n\n### Weaknesses\n\nWhile the paper builds on the Mean Teacher model, which is well-established in semi-supervised learning, it may lack substantial novelty as the framework mainly modifies an existing approach. Although the Dynamic Decay Modulation (DDM) module adds a new layer of adaptability, many similar extensions to Mean Teacher already exist, potentially limiting the paper's contribution to novel methodology.\n\n\nThe experimental scope appears limited as it only includes limited number of baseline methods, i.e. Mean Teacher variations like UAMT with UNet and VNet, models that have already been well-explored in this context. The paper‚Äôs experiments may be restricted by a limited range of labeled-to-unlabeled data ratios, which does not fully capture the model‚Äôs performance across different semi-supervised settings. Testing with a wider variety of label-scarcity scenarios would offer more robust insights into the framework's adaptability and practical applicability in real-world cases where data availability varies.\n\n### Questions\n\n(1) How do you ensure that comparisons are fair in semi-supervised learning scenarios? For example, I understand that in some cases, we can control the percentage of labeled and unlabeled data, such as using 5% or 10% labeled data. However, the feature distribution of labeled and unlabeled data cannot be guaranteed to be the same.\n\n\n(2) The exclusive use of VNet as the backbone may limit the generalizability of the results, as it does not reflect performance across more commonly used architectures like UNet or newer ViT-based UNets.\n\n(3) In Table 2, I observe that VNet‚Äôs performance is significantly lower than others when only 5% of data is labeled, but it is only slightly lower when 10% is labeled. Could you explain why this discrepancy occurs? Additionally, could you provide more results for cases with 20%, 50%, 80%, and 90% labeled data, if available?\n\n(4) In table 3, why VNet outperforms UA-MT when 20% are labeled.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Meta Mean Teacher*, a semi-supervised learning approach for medical image segmentation. It extends the established Mean Teacher framework by incorporating a *Dynamic Decay Modulation (DDM)* module, which adaptively adjusts the exponential moving average (EMA) decay coefficient based on teacher‚Äìstudent loss dynamics. The aim is to improve training stability and model adaptability under varied supervision levels. The paper is clearly written and evaluated on three datasets spanning different imaging modalities.\n\n**Major Comments**  \n1. **Novelty and Contribution**: While the study proposes a dynamic modulation of the EMA decay parameter, its novelty appears limited. The approach largely modifies the existing Mean Teacher framework, and similar adaptive extensions have been reported previously. Consequently, the methodological contribution may not represent a substantial advance.  \n2. **Experimental Design and Comparisons**: The empirical evaluation, though performed on multiple datasets, includes only a narrow set of baseline methods‚Äîmainly Mean Teacher variants such as UAMT with UNet and VNet. This constrains the assessment of the proposed method‚Äôs comparative performance.  \n3. **Range of Semi-Supervised Scenarios**: Experiments employ a restricted range of labeled-to-unlabeled data ratios, which might not fully reveal the model‚Äôs robustness across varying data scarcity conditions. Testing additional label proportions (e.g., 20%, 50%, 80%, 90%) would provide a more comprehensive picture.  \n4. **Fairness of Comparison**: Clarification is needed on how fairness is maintained when partitioning data into labeled and unlabeled sets, given potential differences in feature distributions.  \n5. **Architecture Dependence**: The exclusive use of VNet as the backbone model limits the generalizability of results. Inclusion of more common or recent architectures (e.g., UNet, ViT-based UNet) would strengthen conclusions.  \n\n**Minor Comments**  \n- In Table‚ÄØ2, the reason for VNet‚Äôs performance discrepancy between 5% and 10% labeled data should be discussed.  \n- In Table‚ÄØ3, it is unclear why VNet outperforms UA-MT with 20% labeled data; further explanation would improve clarity.  \n\n**Summary Paragraph**  \nThe work targets an important challenge in semi-supervised medical image segmentation and benefits from clear exposition and multi-dataset validation. However, its main innovation‚Äîthe DDM module‚Äîrepresents an incremental refinement to the Mean Teacher framework, and the limited range of baselines and label ratios constrains the strength of empirical evidence. Broader evaluation and architectural diversity would be needed for stronger validation.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n**1. Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a methodological innovation in *semi-supervised medical image segmentation*, introducing a *meta-learned dynamic decay mechanism* for the Mean Teacher framework. The contribution squarely targets algorithmic and training methodology, relevant to TMI‚Äôs focus on methodological advances in image analysis rather than pure application studies.\n\n**2. Novelty & Contribution Level**  \nThe proposed *Dynamic Decay Modulation (DDM)* and *Meta Mean Teacher* framework represent a conceptual and technical contribution: using meta-learning to adapt a critical hyperparameter (EMA decay) during training. While the Mean Teacher and meta-learning foundations are not new, their integration for dynamic control of the EMA coefficient is a novel idea with plausible generality. The incremental benefit (~0.5‚Äì1.5% improvement in Dice or Jaccard) is modest but consistent, demonstrating methodological value.  \n‚Üí **Assessment:** Methodologically moderate-to-strong novelty relative to existing semi-supervised segmentation paradigms.\n\n**3. Technical and Experimental Rigor**  \nThe experimental design includes:  \n- Comparisons on two standard 3D datasets (LA MRI, Pancreas-NIH CT).  \n- Ablation testing between fixed, teacher-guided, and student-guided DDM update strategies.  \n- Integrations into multiple Mean Teacher derivatives (UA-MT, AD-MT, PMT).  \nThe details of optimization and hyperparameters are adequately stated. However, documentation of *dataset splits*, *statistical significance of improvements*, *runtime overhead*, and *reproducibility assurances* (e.g., code sharing specifics) are limited. The theoretical justification adds conceptual clarity but remains informal‚Äînot rigorous mathematics.  \n\n**4. Clarity and Presentation**  \nOverall clear and fluent English with a professional structure (abstract, introduction, method, experiments). Figures and tables are informative. Minor issues: formatting glitches (e.g., subscript/superscript spacing, occasionally inconsistent symbol rendering), some sentence-level redundancy, and overlength explanations. Grammar is good but readability could be improved by tighter phrasing and clearer figure captions.\n\n**5. Ethical and Reproducibility Compliance**  \nDatasets used (LA, Pancreas-NIH) are public and anonymized. There is no mention of institutional review board (IRB) approvals or patient data handling details, but expected for public datasets. The authors *state* that code will be released upon acceptance‚ÄîTMI recommends release at submission or acceptance. Reproducibility would benefit from inclusion of random seeds and precise training-validation-test partitioning.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe paper proposes *Meta Mean Teacher*, a new semi-supervised segmentation framework combining the classical Mean Teacher approach with *Dynamic Decay Modulation (DDM)*. DDM employs a small meta-learned network that dynamically adjusts the exponential moving average (EMA) decay coefficient (Œ±) between student and teacher models based on their respective losses. The authors introduce a ‚Äústudent-guided‚Äù meta-learning strategy, theoretically argue its advantage, and empirically validate it on two 3D medical image segmentation benchmarks (LA MRI and Pancreas CT). The approach serves as a plug-and-play extension to current Mean Teacher‚Äìstyle methods, yielding consistent but moderate performance improvements.\n\n**2. Strengths**\n- Clear methodological scope relevant to semi-supervised medical image segmentation.  \n- Conceptually elegant integration of meta-learning for dynamic hyperparameter control.  \n- Broad validation across multiple MT-based architectures.  \n- Theoretical motivation clarifies underlying intuition.  \n- Demonstrated generality on multiple imaging modalities.  \n\n**3. Weaknesses**\n- Incremental gains: improvements are small relative to added complexity.  \n- Limited statistical analysis or robustness studies (no variance over multiple runs).  \n- Theoretical analysis is heuristic rather than formal proof-based.  \n- No exploration of runtime or computational burden of the DDM (an important consideration for practical adoption).  \n- Only two datasets; broader validation (other segmentation tasks or modalities) would strengthen claims.  \n- Reproducibility could be improved by earlier public code release and complete dataset split documentation.\n\n**4. Major Comments**\n1. **Quantitative significance** ‚Äì Provide variance/error bars or multiple runs to demonstrate robustness of improvements. Are observed gains statistically significant given stochastic training variance?  \n2. **Complexity analysis** ‚Äì Report computational cost, training time overhead, and parameter count associated with DDM.  \n3. **Generalization** ‚Äì Consider adding experiments on a different anatomical task or at least cross-validation between modalities to claim methodological generality.  \n4. **Clarity on meta-learning setup** ‚Äì Expand explanation of meta-dataset selection: how is *Dm* separated from labeled/unlabeled data, and how does this affect data efficiency?  \n5. **Code and reproducibility** ‚Äì Strongly recommend providing a GitHub repository link or detailed pseudocode, per TMI reproducibility policy.  \n6. **Theoretical exposition** ‚Äì The current ‚Äúfunction space‚Äù argument is qualitative. Strengthen with a clearer mathematical formulation or intuition figures.  \n\n**5. Minor Comments**\n- Fix isolated formatting errors (e.g., missing spaces in formulas and variable superscripts).  \n- In Figures 1‚Äì3, ensure high-resolution and clearer captions explaining all abbreviations.  \n- Recheck references for consistent IEEE style capitalization.  \n- Eliminate redundant sentences in Introduction (lines 056‚Äì080 overlap with Abstract).  \n- Clarify units and voxel size descriptions consistently (LA dataset section).\n\n**6. Overall Recommendation**  \n**Major Revision** ‚Äì The work presents a promising and conceptually novel enhancement to Mean Teacher‚Äìbased semi-supervised segmentation frameworks with methodological relevance for TMI. However, the manuscript needs additional experimental rigor (statistical significance, efficiency analysis) and clearer methodological exposition to reach publishable standard.\n\n**7. Confidence Level:** 4/5  \n(High confidence in technical assessment; moderate uncertainty about expected generalization beyond tested datasets.)\n\n---\n\n**Reviewer Summary Comment to Editor**  \nThe submission introduces an interesting meta-learning‚Äìbased mechanism for dynamic teacher-student consistency adaptation, fitting within TMI‚Äôs methodology scope. The idea is novel enough to warrant consideration, though improvements are numerically small and the paper lacks some reproducibility and statistical analyses expected for an IEEE TMI-level contribution. With a strong revision focusing on experimental substantiation and presentation polish, it may reach publishable quality.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a semi-supervised medical image segmentation framework termed *Meta Mean Teacher*, which integrates *Dynamic Decay Modulation (DDM)* into the traditional Mean Teacher model. Through a meta-learning mechanism, DDM dynamically adjusts the exponential moving average (EMA) decay coefficient between student and teacher models, aiming to improve consistency training. The study demonstrates its applicability to two 3D benchmark datasets (LA MRI and Pancreas-NIH CT) and several Mean Teacher derivatives. The paper is clearly written and technically sound, though improvements are modest and certain methodological details and reproducibility aspects require strengthening.  \n\n**Major Comments**  \n1. **Quantitative Robustness** ‚Äì The performance gains are small (roughly 0.5‚Äì1.5% improvement), lacking statistical significance analyses or variance estimates. Multiple runs with standard deviations or error bars should be included to demonstrate robustness.  \n2. **Computational Complexity** ‚Äì The method adds a meta-learning module, but there is no analysis of training time, memory, or parameter overhead. Reporting these details would clarify trade-offs between performance and efficiency.  \n3. **Generalization Scope** ‚Äì Validation is limited to two datasets and two anatomical tasks; adding more diverse segmentation tasks or cross-validation between modalities would strengthen claims of generality.  \n4. **Meta-learning Protocol** ‚Äì The meta-dataset configuration remains underexplained. The authors should clarify how the meta-set is distinguished from labeled and unlabeled data and how it influences sample efficiency.  \n5. **Reproducibility and Code Availability** ‚Äì The paper states that code will be released, but details such as dataset partitions, random seeds, and precise splits should be explicitly provided to ensure reproducibility.  \n6. **Theoretical Justification** ‚Äì The current theoretical description is qualitative. A clearer mathematical explanation or illustrative figure would help substantiate the conceptual claims.  \n\n**Minor Comments**  \n- Correct small formatting errors (spacing and superscripts in formulas).  \n- Ensure high-resolution figures with complete, self-contained captions and abbreviation definitions.  \n- Avoid redundancy in the Introduction overlapping with the Abstract.  \n- Maintain consistent terminology and voxel size descriptions for datasets.  \n- Standardize references in citation format and capitalization.  \n\n**Summary Paragraph**  \nThis work provides a conceptually novel yet incremental contribution by applying meta-learning to dynamically regulate teacher‚Äìstudent consistency in semi-supervised segmentation. The framework is well-motivated, reasonably validated, and relevant to methodological development in medical image analysis. However, the limited statistical evaluation, absence of complexity analysis, and incomplete reproducibility documentation hinder full assessment of its impact. Addressing these issues along with minor presentation revisions would significantly enhance the paper‚Äôs rigor and clarity.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper shows technical promise and conceptual originality but requires substantial additional analysis and clarification before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses semi-supervised medical image segmentation by proposing Meta Mean Teacher, which dynamically adjusts the Exponential Moving Average (EMA) decay coefficient in Mean Teacher frameworks. The authors introduce a Dynamic Decay Modulation (DDM) module that learns optimal EMA coefficients through meta-learning, using student and teacher model losses as inputs. The method employs a student-guided meta-learning approach where DDM parameters are updated via pseudo-gradient descent on meta-data. Experiments on LA Heart (MRI) and Pancreas-NIH (CT) datasets show improvements when DDM is integrated into existing frameworks (UA-MT, AD-MT, PMT), with reported performance gains of 0.91% Dice improvement on LA dataset with 5% labeled data.\n\n## Weaknesses\n\n‚Ä¢ **Theoretical Analysis Lacks Rigor**\n  - The probability argument in Equation 3 (Page 6, lines 274-275) states P[Œ∏* ‚àà span(Œ∏t, Œ∏s)] ‚âà 0 without mathematical justification or dimensionality assumptions\n  - The Universal Approximation Theorem application in Equation 5 (Page 6, line 284) ignores practical constraints like network capacity and training dynamics\n  - The claim that teacher-guided approach is theoretically inferior (Page 6, Section 3.2) lacks formal proof and contradicts empirical evidence in some cases\n\n‚Ä¢ **Experimental Design and Evaluation Issues**\n  - Table 1 (Page 7) shows DDM degrading performance for UA-MT on multiple metrics (95HD: +11.88, ASD: +3.33), contradicting the method's claimed universality\n  - Missing MCF results for 5% labeled data (Table 2, Page 8) and 10% labeled data (Table 3, Page 9) prevent comprehensive comparison\n  - Algorithm 1 (Page 5) lacks specification of meta-iteration count, learning rates Œ∑D, and convergence criteria, making reproduction difficult\n\n‚Ä¢ **Technical Implementation Concerns**\n  - The DDM architecture details are not specified beyond \"small neural network, typically MLP\" (Page 4, lines 195-196), limiting reproducibility\n  - The bilevel optimization in Equation 1 (Page 5, line 253) may suffer from optimization instability, but no convergence analysis is provided\n  - The meta-data splitting strategy is not described, potentially leading to data leakage between training and meta-learning phases\n\n‚Ä¢ **Limited Scope and Generalizability**\n  - Only two datasets tested, both from medical imaging domain, limiting evidence of broader applicability claimed in abstract\n  - The method requires additional meta-data partitioning, reducing already scarce labeled data available for semi-supervised learning\n  - No computational overhead analysis provided despite introducing additional meta-learning iterations and DDM forward passes\n\n## Suggestions for Improvement\n\n‚Ä¢ **Strengthen Theoretical Foundation**\n  - Provide rigorous mathematical proof for the probability claim in Equation 3 with explicit dimensionality assumptions and geometric constraints\n  - Replace the informal Universal Approximation Theorem argument with analysis specific to EMA constraints and finite network capacity\n  - Develop formal convergence guarantees for the bilevel optimization problem or provide empirical convergence analysis across different datasets\n\n‚Ä¢ **Address Experimental Inconsistencies**\n  - Investigate and explain why DDM degrades UA-MT performance on boundary metrics (Table 1), potentially through additional ablation studies or method refinement\n  - Complete missing baseline comparisons (MCF results) and ensure all competing methods are evaluated under identical experimental conditions\n  - Specify all hyperparameters in Algorithm 1 including meta-iteration counts, learning schedules, and provide sensitivity analysis for key parameters\n\n‚Ä¢ **Enhance Technical Clarity and Reproducibility**\n  - Provide detailed DDM architecture specifications including layer dimensions, activation functions, and initialization strategies\n  - Analyze and discuss potential optimization instabilities in the bilevel formulation, possibly including gradient clipping or other stabilization techniques\n  - Clearly describe the meta-data partitioning strategy and verify no information leakage occurs between different learning phases\n\n‚Ä¢ **Expand Experimental Validation**\n  - Evaluate on additional datasets beyond medical imaging (e.g., natural images, other semi-supervised domains) to support generalizability claims\n  - Develop strategies to minimize meta-data requirements or demonstrate performance when meta-data is very limited\n  - Provide comprehensive computational cost analysis including training time, memory usage, and inference overhead compared to baseline methods",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *Meta Mean Teacher*, a semi-supervised medical image segmentation method that dynamically adjusts the Exponential Moving Average (EMA) decay coefficient within Mean Teacher frameworks. The central contribution is the *Dynamic Decay Modulation* (DDM) module, which employs meta-learning to optimize EMA coefficients based on student‚Äìteacher loss signals. The paper claims consistent performance gains across variations of existing frameworks (UA-MT, AD-MT, PMT) on the LA Heart (MRI) and Pancreas-NIH (CT) datasets, reporting modest Dice score improvements. While the proposed idea of adaptive EMA modulation is clear and potentially useful, the theoretical grounding, experimental completeness, and implementation transparency require significant enhancement.  \n\n**Major Comments**  \n1. **Theoretical Rigor** ‚Äì The probability statement in Equation 3 lacks justification or dimensional assumptions; the invocation of the Universal Approximation Theorem (Eq. 5) neglects practical limits such as network capacity; and the claim that teacher-guided methods are theoretically inferior is made without proof or consistent empirical support.  \n2. **Experimental Design** ‚Äì Reported results in Table 1 show that DDM can degrade performance (e.g., UA-MT on 95HD and ASD metrics), which challenges claims of universality. Missing results for MCF under certain label settings hinder comprehensive comparison, and Algorithm 1 omits meta-iteration counts, learning rates, and convergence criteria, limiting reproducibility.  \n3. **Implementation Details** ‚Äì DDM architecture is described only as a ‚Äúsmall MLP,‚Äù providing insufficient information for replication. The bilevel optimization setup may be unstable, yet no convergence or stability analysis is offered. The meta-data partitioning strategy is unspecified, raising potential for data leakage.  \n4. **Scope and Generalizability** ‚Äì Validation is limited to two medical imaging datasets. No computational cost analysis or discussion of the reduced effective labeled data caused by meta-data partitioning is provided.  \n\n**Minor Comments**  \n- Report missing experimental baselines uniformly in Tables 2‚Äì3.  \n- Clarify algorithm notation and parameter definitions for readability.  \n- Check for consistency in figure and table labeling.  \n\n**Summary Paragraph**  \nOverall, the approach is conceptually promising but undermined by weak theoretical justification, incomplete evaluations, and insufficient implementation detail. Broader validation, explicit hyperparameter specification, and rigorous analysis are needed to establish the method‚Äôs reliability and generality.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper presents an interesting idea but requires substantive theoretical, experimental, and reproducibility improvements before it can be recommended for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces Meta Mean Teacher (Meta MT), a semi-supervised learning framework designed to enhance the performance of Mean Teacher (MT) models in medical image segmentation by dynamically adjusting the Exponential Moving Average (EMA) decay coefficient. The proposed Dynamic Decay Modulation (DDM) module uses meta-learning to adaptively determine the EMA decay coefficient based on the losses of both student and teacher models. The authors claim that this approach improves the representation capabilities of the student model, leading to better segmentation performance. The manuscript is well-written and provides detailed experimental results on two datasets (LA Heart and Pancreas-NIH), demonstrating consistent improvements over traditional MT methods.\n\n###\n\n## Major Comments\n1. Novelty and Positioning. While the proposed DDM module offers a novel way to dynamically adjust the EMA decay coefficient, the manuscript could benefit from a clearer distinction from existing works on dynamic hyperparameter tuning in semi-supervised learning. Specifically, the authors should highlight how their approach differs from and potentially improves upon recent methods that also employ dynamic adjustments in similar contexts.\n\n2. Evaluation Design. The evaluation is performed on two datasets with different modalities, which is commendable. However, the experiments are limited to a fixed number of iterations (6000) and a specific baseline model (VNet). The authors should consider extending the evaluation to include more diverse architectures and training conditions to validate the robustness and generalizability of their approach.\n\n3. Comparisons. The comparisons with state-of-the-art methods are thorough, but the manuscript should also include a discussion on how the proposed framework compares to other semi-supervised techniques that do not rely on the Mean Teacher framework. This would provide a more comprehensive view of the method's advantages and limitations.\n\n4. Reproducibility. Although the authors state they will release the code upon acceptance, the manuscript lacks detailed descriptions of the training protocols, preprocessing steps, and hyperparameters used. Providing complete methodological details is crucial for ensuring reproducibility and allowing others to build upon the work.\n\n###\n\n## Minor Comments\n1. Figures and Tables. Some figures (e.g., Fig. 3) are cluttered and could benefit from a more streamlined presentation, highlighting fewer representative slices with zoomed-in regions to improve readability.\n   \n2. Notation and Definitions. The notation introduced in Section 2.1 is inconsistent and lacks sufficient explanation. For example, the forward operator is denoted differently in various parts of the manuscript.\n   \n3. Acronyms. Several acronyms (e.g., \"R=4\") are used without clear definitions, which may confuse readers unfamiliar with the context.\n   \n4. Typographical Issues. There are typographical errors such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7) that need to be corrected.\n\n###\n\n## Summary Paragraph\n(Focusing on 4 TMI Editorial Criteria)\nThe manuscript addresses a significant clinical challenge: improving medical image segmentation with limited labeled data. The proposed Meta Mean Teacher framework, incorporating a Dynamic Decay Modulation (DDM) module, is a technically innovative approach to enhance the Mean Teacher model's performance. The experimental results on two datasets show consistent improvements over traditional methods, indicating the potential significance of the work. However, the evaluation could be strengthened by including a broader range of datasets and models to fully establish generalizability. The reproducibility of the approach is also a concern due to incomplete methodological details. Overall, while the idea holds promise, the current evidence partially meets the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include a wider range of methods and datasets, provide complete methodological details for reproducibility, and clarify the novelty and positioning of their work relative to existing literature.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **Meta Mean Teacher (Meta MT)**, a semi-supervised learning framework aimed at improving Mean Teacher (MT) models for medical image segmentation. The key innovation is the **Dynamic Decay Modulation (DDM)** module, which employs meta-learning to adaptively adjust the Exponential Moving Average (EMA) decay coefficient based on student and teacher losses. This mechanism is designed to enhance the representational ability of the student model and, consequently, segmentation accuracy. The paper is clearly written and reports detailed experiments on two datasets (LA Heart and Pancreas-NIH), demonstrating consistent performance gains over standard MT approaches.  \n\n---\n\n**Major Comments**  \n1. **Novelty and Positioning:** The concept of dynamically updating the EMA decay coefficient is interesting, but the distinction from prior work on dynamic hyperparameter tuning in semi-supervised settings is not sufficiently articulated. The manuscript should better clarify how DDM differs from recent adaptive approaches and what unique contributions it brings beyond existing methods.  \n\n2. **Evaluation Design:** Although the experiments span two distinct imaging modalities, the evaluation scope is limited to a single architecture (VNet) and a fixed training regime (6000 iterations). Incorporating additional architectures, longer training schedules, or other conditions would strengthen claims of robustness and generalizability.  \n\n3. **Comparisons:** The comparisons with state-of-the-art Mean Teacher variants are comprehensive, yet discussion is lacking about how Meta MT performs relative to other semi-supervised segmentation approaches not based on the MT paradigm. Such comparisons would situate the work more effectively within the broader literature.  \n\n4. **Reproducibility:** The paper omits detailed methodological specifications, such as data preprocessing, hyperparameter settings, and training protocols. Providing these details is essential for reproducibility and for enabling follow-up research, even if code release is planned upon acceptance.  \n\n---\n\n**Minor Comments**  \n1. **Figures and Tables:** Some figures (e.g., Fig.‚ÄØ3) appear cluttered. Simplifying layouts or zooming into representative areas could enhance clarity.  \n2. **Notation:** Section‚ÄØ2.1 contains inconsistent notation; for instance, the forward operator symbol varies in different parts.  \n3. **Acronyms:** Certain acronyms (e.g., ‚ÄúR=4‚Äù) are introduced without definition.  \n4. **Typos:** Correct typographical errors such as ‚Äúk‚Äëspacce‚Äù and ‚Äúundersampling maskes.‚Äù  \n\n---\n\n**Summary Paragraph**  \nThis study tackles an important challenge in semi-supervised medical image segmentation by proposing a meta-learning-based mechanism to improve teacher‚Äìstudent consistency. The approach shows promising quantitative results, suggesting potential practical impact. However, questions remain regarding novelty clarification, breadth of evaluation, and the reproducibility of experimental details. Addressing these limitations would considerably strengthen the manuscript‚Äôs contribution and reliability.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The authors should clarify methodological novelty, broaden experimental validation, and provide full reproducibility details before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## LEARNING TO TEACH: IMPROVING MEAN TEACHER IN SEMI-SUPERVISED MEDICAL IMAGE SEGMENTATION WITH DYNAMIC DECAY MODULATION\n\n### Summary\n\nThe paper proposes Meta Mean Teacher, a semi-supervised medical image segmentation framework that dynamically adjusts the EMA decay coefficient Œ± of the teacher model via a learned Dynamic Decay Modulation (DDM) module. DDM is trained with a student-guided bilevel/meta-learning objective that takes the student‚Äôs and teacher‚Äôs supervised losses as inputs to output Œ± at each iteration. Experiments on LA (MRI) and Pancreas-NIH (CT) show consistent gains over fixed-Œ± Mean Teacher and modest improvements when plugging DDM into MT-based methods (UA-MT, PMT, AD-MT).\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a simple, plug-and-play mechanism to adapt the EMA decay Œ± online using a learned mapping from student/teacher loss signals.Casts Œ± selection as a meta-learned hyperparameter within a bilevel optimization setup, which is a principled framing and aligns with recent meta-learning for hyperparameters.The student-guided meta-objective is a meaningful choice: optimize Œ± to improve the student‚Äôs downstream supervised performance, consistent with meta-pseudo-labeling concepts.\n- Experimental rigor and validationEvaluates on two standard 3D medical segmentation datasets (LA and Pancreas-NIH) with commonly used label fractions.Tests integration into strong MT-based frameworks (UA-MT, PMT, AD-MT) and includes an ablation comparing student-guided vs teacher-guided meta-updates.Provides sensitivity evidence that Œ± matters (Figure 1), motivating a dynamic approach.\n- Clarity of presentationPresents a pipeline diagram and an algorithmic sketch that clarify the two-phase (student-learning and meta-learning) procedure.Clearly states the role of DDM and how Œ± is injected into EMA updates.\n- Significance of contributionsAddresses a widely used but under-discussed hyperparameter (EMA decay) in teacher‚Äìstudent semi-supervised segmentation.If robust and general, the idea could be useful broadly for MT variants in medical imaging.\n\nTechnical novelty and innovation\n\n- Introduces a simple, plug-and-play mechanism to adapt the EMA decay Œ± online using a learned mapping from student/teacher loss signals.\n- Casts Œ± selection as a meta-learned hyperparameter within a bilevel optimization setup, which is a principled framing and aligns with recent meta-learning for hyperparameters.\n- The student-guided meta-objective is a meaningful choice: optimize Œ± to improve the student‚Äôs downstream supervised performance, consistent with meta-pseudo-labeling concepts.\n\nExperimental rigor and validation\n\n- Evaluates on two standard 3D medical segmentation datasets (LA and Pancreas-NIH) with commonly used label fractions.\n- Tests integration into strong MT-based frameworks (UA-MT, PMT, AD-MT) and includes an ablation comparing student-guided vs teacher-guided meta-updates.\n- Provides sensitivity evidence that Œ± matters (Figure 1), motivating a dynamic approach.\n\nClarity of presentation\n\n- Presents a pipeline diagram and an algorithmic sketch that clarify the two-phase (student-learning and meta-learning) procedure.\n- Clearly states the role of DDM and how Œ± is injected into EMA updates.\n\nSignificance of contributions\n\n- Addresses a widely used but under-discussed hyperparameter (EMA decay) in teacher‚Äìstudent semi-supervised segmentation.\n- If robust and general, the idea could be useful broadly for MT variants in medical imaging.\n\n### Weaknesses\n\n- Technical limitations or concernsMissing critical implementation details: DDM architecture (size/activations), how Œ± is constrained to [0,1] (e.g., sigmoid/clipping), number of unrolled steps, meta-iterations per update, and how gradients are stabilized through the unrolled optimization.‚ÄúMeta data‚Äù split is unspecified: size, selection strategy, and whether it reduces the already scarce labeled pool; risk of data leakage if validation/test data are inadvertently used.The theoretical analysis is informal: key claims about EMA‚Äôs ‚Äúprobability zero‚Äù to pass through Œ∏* and unavoidable approximation error are not rigorous and do not reflect practical behavior (e.g., EMA as regularized ensembling rather than optimality in parameter space).\n- Experimental gaps or methodological issuesInconsistent improvements: several configurations with DDM degrade 95HD/ASD compared to the corresponding baseline (e.g., UA-MT+DDM on LA 5%, Pancreas 10%; PMT+DDM on Pancreas 20%; AD-MT+DDM on Pancreas 20% 95HD). This conflicts with statements that DDM ‚Äúoutperforms across all four metrics.‚ÄùMissing baselines: no comparison to common dynamic Œ± schedules (ramp-up/increasing Œ± over training) or tuned schedules, which are standard in MT literature; fixed Œ± grid search is reported only in a single figure without systematic schedule comparisons.Reproducibility: single runs with no variance estimates; no report of seeds, standard deviations, or statistical tests.Fairness concerns: inference averages ‚Äútwo networks‚Äù (student and teacher); it is unclear whether all baselines also used the same ensembling at test time.Computational overhead is not quantified; unrolling through student updates in 3D segmentation is typically memory-intensive.\n- Clarity or presentation issuesSeveral typos, truncated sentences, equation/notation artifacts (‚Äúa‚Äù vs Œ±, broken phrases), and incomplete tables (e.g., missing MCF entries; inconsistent 95HD notation like ‚Äú6.48+0.03‚Äù).Important training details are missing: augmentation protocols, meta-batch composition, frequency of meta-updates, Œª schedule, confidence thresholds, and patching/inference specifics for all methods.\n- Missing related work or comparisonsLacks discussion of Meta Pseudo Labels (MPL; Pham et al., 2020) which uses a student-supervised meta-objective for the teacher‚Äîhighly relevant to the student-guided rationale.Recent MT variants in medical/dense prediction that modify ema/teacher mechanics (e.g., CE-MT competitive ensembling; Dual Teacher alternating EMAs; PLGDF multi-scale consistency with sharpening) are not discussed, limiting context.Bilevel hyperparameter optimization references are included but practical connections to the present formulation (e.g., convergence, truncation bias) are not elaborated.\n\nTechnical limitations or concerns\n\n- Missing critical implementation details: DDM architecture (size/activations), how Œ± is constrained to [0,1] (e.g., sigmoid/clipping), number of unrolled steps, meta-iterations per update, and how gradients are stabilized through the unrolled optimization.\n- ‚ÄúMeta data‚Äù split is unspecified: size, selection strategy, and whether it reduces the already scarce labeled pool; risk of data leakage if validation/test data are inadvertently used.\n- The theoretical analysis is informal: key claims about EMA‚Äôs ‚Äúprobability zero‚Äù to pass through Œ∏* and unavoidable approximation error are not rigorous and do not reflect practical behavior (e.g., EMA as regularized ensembling rather than optimality in parameter space).\n\nExperimental gaps or methodological issues\n\n- Inconsistent improvements: several configurations with DDM degrade 95HD/ASD compared to the corresponding baseline (e.g., UA-MT+DDM on LA 5%, Pancreas 10%; PMT+DDM on Pancreas 20%; AD-MT+DDM on Pancreas 20% 95HD). This conflicts with statements that DDM ‚Äúoutperforms across all four metrics.‚Äù\n- Missing baselines: no comparison to common dynamic Œ± schedules (ramp-up/increasing Œ± over training) or tuned schedules, which are standard in MT literature; fixed Œ± grid search is reported only in a single figure without systematic schedule comparisons.\n- Reproducibility: single runs with no variance estimates; no report of seeds, standard deviations, or statistical tests.\n- Fairness concerns: inference averages ‚Äútwo networks‚Äù (student and teacher); it is unclear whether all baselines also used the same ensembling at test time.\n- Computational overhead is not quantified; unrolling through student updates in 3D segmentation is typically memory-intensive.\n\nClarity or presentation issues\n\n- Several typos, truncated sentences, equation/notation artifacts (‚Äúa‚Äù vs Œ±, broken phrases), and incomplete tables (e.g., missing MCF entries; inconsistent 95HD notation like ‚Äú6.48+0.03‚Äù).\n- Important training details are missing: augmentation protocols, meta-batch composition, frequency of meta-updates, Œª schedule, confidence thresholds, and patching/inference specifics for all methods.\n\nMissing related work or comparisons\n\n- Lacks discussion of Meta Pseudo Labels (MPL; Pham et al., 2020) which uses a student-supervised meta-objective for the teacher‚Äîhighly relevant to the student-guided rationale.\n- Recent MT variants in medical/dense prediction that modify ema/teacher mechanics (e.g., CE-MT competitive ensembling; Dual Teacher alternating EMAs; PLGDF multi-scale consistency with sharpening) are not discussed, limiting context.\n- Bilevel hyperparameter optimization references are included but practical connections to the present formulation (e.g., convergence, truncation bias) are not elaborated.\n\n### Detailed Comments\n\n- Technical soundness evaluationBilevel formulation is appropriate, but the paper needs to specify:The number of unrolled student steps during meta-updates and how higher-order gradients are handled (exact vs truncated vs stop-gradient).How Œ± is constrained (e.g., Œ± = sigmoid(h(¬∑))) and initialized; whether Œ± is clipped to avoid instability (e.g., Œ± close to 1.0 early may stall teacher).Stability controls (e.g., regularization of Œ± changes, smoothing Œ± over steps) and whether DDM uses additional signals (gradient norms, teacher‚Äìstudent disagreement).The theoretical discussion about EMA‚Äôs limitations is heuristic. A stronger, more relevant argument would analyze how Œ± affects pseudo-label drift/stability and student generalization (e.g., bias‚Äìvariance or confirmation bias trade-offs), or empirically diagnose where student-guided vs teacher-guided choices differ (e.g., teacher/student calibration and noise rates).\n- Experimental evaluation assessmentPlease provide:Exact meta-data protocol: how D_m is chosen (from the labeled set or a held-out subset), its size, and whether it is disjoint from validation/test.Multi-seed results with mean¬±std for all tables; small label fractions are high-variance.Compute and memory overhead of DDM (time per iteration, GPU hours), and the number of meta-iterations per outer step; compare to baseline MT.A fair Œ±-schedule baseline: e.g., linearly increasing Œ± to 0.99/0.999, and adaptive schedules used in prior MT segmentation works (many increase Œ± over training). Also report the best single fixed Œ± per dataset from grid search as an oracle baseline.Ablations on DDM design: inputs (include L_u and/or disagreement/uncertainty?), architecture size, Œ± activation and regularization, update frequency, and using both labeled and unlabeled batches to predict Œ±.Clarify evaluation:Whether all compared methods used the same test-time ensembling (student+teacher average).Whether Œª schedules, augmentations, and inference protocols match those of the reproduced baselines (UA-MT, PMT, AD-MT).Resolve the discrepancies where DDM worsens 95HD/ASD; if DDM improves region overlap but harms boundary metrics, analyze and mitigate (e.g., boundary-aware losses, uncertainty masking of low-confidence pseudo-labels).\n- Comparison with related work (using the summaries provided)MPL (meta teacher/student) is conceptually close: the student‚Äôs supervised loss after learning from the teacher provides the meta-signal. Your work differs by meta-learning a scalar Œ± via a small network, rather than the teacher itself. A direct discussion and empirical contrast to MPL-style objectives would strengthen positioning.CE-MT and Dual Teacher show alternatives to reduce teacher‚Äìstudent coupling by diversifying teacher updates; PLGDF sharpens/mixes pseudo-labels and uses multi-scale consistency. These are contemporary, MT-adjacent strategies. A paragraph situating DDM relative to these‚Äîespecially how dynamic Œ± complements or competes with competitive/dual teachers and advanced pseudo-label management‚Äîwould help readers.Bilevel HPO theory (e.g., 1806.04910) supports your formulation; however, please discuss practical aspects such as convergence with truncated unrolling, meta-learning rates, and sensitivity.\n- Discussion of broader impact and significanceThe idea is appealing due to its simplicity and portability: a learned Œ± could be adopted across many MT variants. However, without robust variance analysis and compute reporting, it is hard to assess the cost‚Äìbenefit trade-off in 3D settings.The occasional degradation in boundary metrics suggests DDM may prioritize stability/overlap but not contour fidelity; highlighting this trade-off and proposing remedies (e.g., boundary-aware L_m, uncertainty masking in L_u, or including edge-aware terms in the meta-loss) would increase practical value.\n\nTechnical soundness evaluation\n\n- Bilevel formulation is appropriate, but the paper needs to specify:The number of unrolled student steps during meta-updates and how higher-order gradients are handled (exact vs truncated vs stop-gradient).How Œ± is constrained (e.g., Œ± = sigmoid(h(¬∑))) and initialized; whether Œ± is clipped to avoid instability (e.g., Œ± close to 1.0 early may stall teacher).Stability controls (e.g., regularization of Œ± changes, smoothing Œ± over steps) and whether DDM uses additional signals (gradient norms, teacher‚Äìstudent disagreement).\n- The theoretical discussion about EMA‚Äôs limitations is heuristic. A stronger, more relevant argument would analyze how Œ± affects pseudo-label drift/stability and student generalization (e.g., bias‚Äìvariance or confirmation bias trade-offs), or empirically diagnose where student-guided vs teacher-guided choices differ (e.g., teacher/student calibration and noise rates).\n\n- The number of unrolled student steps during meta-updates and how higher-order gradients are handled (exact vs truncated vs stop-gradient).\n- How Œ± is constrained (e.g., Œ± = sigmoid(h(¬∑))) and initialized; whether Œ± is clipped to avoid instability (e.g., Œ± close to 1.0 early may stall teacher).\n- Stability controls (e.g., regularization of Œ± changes, smoothing Œ± over steps) and whether DDM uses additional signals (gradient norms, teacher‚Äìstudent disagreement).\n\nExperimental evaluation assessment\n\n- Please provide:Exact meta-data protocol: how D_m is chosen (from the labeled set or a held-out subset), its size, and whether it is disjoint from validation/test.Multi-seed results with mean¬±std for all tables; small label fractions are high-variance.Compute and memory overhead of DDM (time per iteration, GPU hours), and the number of meta-iterations per outer step; compare to baseline MT.A fair Œ±-schedule baseline: e.g., linearly increasing Œ± to 0.99/0.999, and adaptive schedules used in prior MT segmentation works (many increase Œ± over training). Also report the best single fixed Œ± per dataset from grid search as an oracle baseline.Ablations on DDM design: inputs (include L_u and/or disagreement/uncertainty?), architecture size, Œ± activation and regularization, update frequency, and using both labeled and unlabeled batches to predict Œ±.\n- Clarify evaluation:Whether all compared methods used the same test-time ensembling (student+teacher average).Whether Œª schedules, augmentations, and inference protocols match those of the reproduced baselines (UA-MT, PMT, AD-MT).Resolve the discrepancies where DDM worsens 95HD/ASD; if DDM improves region overlap but harms boundary metrics, analyze and mitigate (e.g., boundary-aware losses, uncertainty masking of low-confidence pseudo-labels).\n\n- Exact meta-data protocol: how D_m is chosen (from the labeled set or a held-out subset), its size, and whether it is disjoint from validation/test.\n- Multi-seed results with mean¬±std for all tables; small label fractions are high-variance.\n- Compute and memory overhead of DDM (time per iteration, GPU hours), and the number of meta-iterations per outer step; compare to baseline MT.\n- A fair Œ±-schedule baseline: e.g., linearly increasing Œ± to 0.99/0.999, and adaptive schedules used in prior MT segmentation works (many increase Œ± over training). Also report the best single fixed Œ± per dataset from grid search as an oracle baseline.\n- Ablations on DDM design: inputs (include L_u and/or disagreement/uncertainty?), architecture size, Œ± activation and regularization, update frequency, and using both labeled and unlabeled batches to predict Œ±.\n\n- Whether all compared methods used the same test-time ensembling (student+teacher average).\n- Whether Œª schedules, augmentations, and inference protocols match those of the reproduced baselines (UA-MT, PMT, AD-MT).\n- Resolve the discrepancies where DDM worsens 95HD/ASD; if DDM improves region overlap but harms boundary metrics, analyze and mitigate (e.g., boundary-aware losses, uncertainty masking of low-confidence pseudo-labels).\n\nComparison with related work (using the summaries provided)\n\n- MPL (meta teacher/student) is conceptually close: the student‚Äôs supervised loss after learning from the teacher provides the meta-signal. Your work differs by meta-learning a scalar Œ± via a small network, rather than the teacher itself. A direct discussion and empirical contrast to MPL-style objectives would strengthen positioning.\n- CE-MT and Dual Teacher show alternatives to reduce teacher‚Äìstudent coupling by diversifying teacher updates; PLGDF sharpens/mixes pseudo-labels and uses multi-scale consistency. These are contemporary, MT-adjacent strategies. A paragraph situating DDM relative to these‚Äîespecially how dynamic Œ± complements or competes with competitive/dual teachers and advanced pseudo-label management‚Äîwould help readers.\n- Bilevel HPO theory (e.g., 1806.04910) supports your formulation; however, please discuss practical aspects such as convergence with truncated unrolling, meta-learning rates, and sensitivity.\n\nDiscussion of broader impact and significance\n\n- The idea is appealing due to its simplicity and portability: a learned Œ± could be adopted across many MT variants. However, without robust variance analysis and compute reporting, it is hard to assess the cost‚Äìbenefit trade-off in 3D settings.\n- The occasional degradation in boundary metrics suggests DDM may prioritize stability/overlap but not contour fidelity; highlighting this trade-off and proposing remedies (e.g., boundary-aware L_m, uncertainty masking in L_u, or including edge-aware terms in the meta-loss) would increase practical value.\n\n### Questions for Authors\n\n- How is the meta dataset D_m defined and sized? Is it a strict hold-out from the labeled set, and does its use further reduce supervision for the main training? Any risk of leakage with validation/test data?\n- How is Œ± constrained to [0,1]? What activation or clipping do you use, and how is Œ± initialized and regularized to avoid instability (e.g., Œ±‚Üí1 too early)?\n- How many inner student update steps are unrolled when computing ‚àá_{Œ∏^D} L_m? Do you use exact higher-order gradients or truncation/stop-gradients? What is the meta-update frequency?\n- What is the computational and memory overhead of DDM (per-iteration time, total GPU hours) relative to the baselines on LA and Pancreas?\n- At inference time you average student and teacher predictions. Did all baselines use the same ensembling? If not, can you report results without ensembling for a fair comparison?\n- Several rows show degraded 95HD/ASD with DDM (e.g., UA-MT+DDM on LA 5% and Pancreas 10%). Can you analyze failure modes and propose adjustments (e.g., boundary-aware meta-loss, uncertainty masking)?\n- How does DDM compare against simple Œ± schedules (e.g., linear/cosine ramp-up to 0.99/0.999) and tuned schedules? Please include these as strong baselines.\n- Why did you choose to feed only supervised losses to DDM? Have you tried including unsupervised loss, teacher‚Äìstudent disagreement, or uncertainty as inputs?\n- How sensitive are results to the DDM architecture, learning rate, and the choice of Œª? Please provide robustness analyses.\n- How does the proposed student-guided approach relate empirically and conceptually to Meta Pseudo Labels? Can you cite and contrast with MPL, and clarify the key differences in optimization target and compute?\n\n### Overall Assessment\n\nThis is a timely and promising idea: dynamically meta-learning the EMA decay coefficient addresses an important but under-explored component of Mean Teacher. The plug-and-play DDM concept is attractive, and the student-guided bilevel framing is sensible. However, the current presentation lacks essential implementation and evaluation detail, contains several clarity issues and inconsistencies, and overstates claims relative to the tables (not all metrics improve, sometimes substantially worse). To be competitive at a top-tier venue, the paper needs a more rigorous and transparent experimental section (multi-seed statistics, compute overhead, stronger baselines including Œ± schedules, ablations of DDM design), clearer positioning against closely related meta-teacher work (MPL) and recent MT variants in medical segmentation, and a tighter, more accurate narrative around when and why DDM helps. With these improvements and a refined theoretical or empirical justification of the student-guided choice, the contribution could be impactful for MT-based semi-supervised segmentation.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Meta Mean Teacher*, a semi‚Äësupervised medical image segmentation approach that learns to adapt the exponential moving average (EMA) decay coefficient of the teacher model through a *Dynamic Decay Modulation* (DDM) module. The DDM adjusts the decay parameter Œ± at each iteration using a student‚Äëguided bilevel/meta‚Äëlearning objective driven by the student and teacher losses. Evaluations on LA (MRI) and Pancreas‚ÄëNIH (CT) datasets demonstrate improvements over fixed‚ÄëŒ± Mean Teacher baselines and modest gains when DDM is integrated into established Mean Teacher variants (UA‚ÄëMT, PMT, AD‚ÄëMT). The paper is generally clear and well‚Äëillustrated, with appropriate motivation for dynamically adapting Œ±.\n\n---\n\n**Major Comments**\n\n1. **Technical Details and Theoretical Rigor**  \n   - Key implementation specifics are missing: DDM architecture (layer size/activations), how Œ± is bounded within [0,1], the number of unrolled student steps, frequency of meta‚Äëupdates, and stabilization of meta‚Äëgradients.  \n   - The ‚Äúmeta data‚Äù split is undefined; it is unclear whether it reduces the already limited labeled pool or risks leakage from validation/test sets.  \n   - The theoretical discussion of EMA optimality is heuristic and not rigorous, misrepresenting EMA as parameter optimization rather than an ensemble regularization mechanism.  \n\n2. **Experimental Design and Reproducibility**  \n   - Reported improvements are inconsistent; several settings show degraded 95HD/ASD scores. This contradicts claims of uniform superiority.  \n   - Missing baselines include dynamic Œ± schedules (e.g., ramp‚Äëup schemes) and tuned Œ± values from grid search across datasets.  \n   - Only single runs are shown without variance or statistical testing, and it is unclear whether test‚Äëtime ensembling is applied consistently across baselines.  \n   - Computational and memory overhead are unspecified despite the heavy cost of unrolled 3D segmentation meta‚Äëupdates.\n\n3. **Comparative Context and Related Work**  \n   - The paper omits discussion of *Meta Pseudo Labels* (MPL, Pham‚ÄØet‚ÄØal.,‚ÄØ2020), which is conceptually close to the student‚Äëguided meta‚Äëobjective.  \n   - Recent Mean Teacher extensions (CE‚ÄëMT, Dual Teacher, PLGDF) that modify teacher dynamics are also not addressed, limiting context.  \n   - Connections to bilevel hyperparameter optimization theory are cited but not discussed with respect to convergence or truncation bias.  \n\n4. **Clarity and Presentation**  \n   - Several typographical errors, incomplete tables, and notation inconsistencies (e.g., ‚Äúa‚Äù vs Œ±) hinder readability.  \n   - Essential training details such as augmentation strategy, meta‚Äëbatch composition, Œª schedule, and inference setup are missing.\n\n---\n\n**Minor Comments**\n\n- Specify how Œ± is initialized and regularized to prevent unstable updates.  \n- Provide multi‚Äëseed averages and standard deviations for tables.  \n- Quantify compute overhead (iterations/time, GPU hours) relative to baselines.  \n- Analyze cases where DDM improves region overlap but harms boundary metrics, and consider boundary‚Äëaware or uncertainty‚Äëmasked losses.  \n- Contrast empirically with MPL by showing conceptual and practical differences in meta‚Äësignal formulation.  \n\n---\n\n**Summary Paragraph**  \nThe idea of dynamically learning the EMA decay within a Mean Teacher framework is technically appealing, lightweight, and potentially generalizable. The manuscript clearly articulates motivation and demonstrates some empirical benefits. However, the current version lacks critical implementation transparency, systematic baselines, statistical robustness, and contextual discussion, making it difficult to assess reproducibility and significance. The core concept appears sound and relevant but needs stronger validation, controlled comparisons, and clearer reporting to be convincing.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nFurther methodological clarification, broader experimental baselines, and a more rigorous empirical and comparative analysis are required before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"LEARNING TO TEACH: IMPROVING MEAN TEACHER IN SEMI-SUPERVISED MEDICAL IMAGE SEGMENTATION WITH DYNAMIC DECAY MODULATION\"\n\n## 1. Opening Paragraph\n\nThe paper addresses the critical challenge of limited labeled data in medical image segmentation by proposing a novel framework that enhances the Mean Teacher approach in semi-supervised learning. The authors identify that the fixed Exponential Moving Average (EMA) decay coefficient in traditional Mean Teacher methods fails to adapt to evolving training dynamics, potentially limiting performance. They propose Meta Mean Teacher, which incorporates a Dynamic Decay Modulation (DDM) module that uses meta-learning to dynamically adjust the EMA decay coefficient throughout training. The DDM module takes student and teacher network losses as inputs and updates the decay coefficient through pseudo-gradient descent on a meta-objective. The authors demonstrate that their approach consistently outperforms traditional Mean Teacher methods with fixed EMA coefficients across two diverse medical datasets (LA Heart and Pancreas-NIH) with different imaging modalities (MRI and CT). Furthermore, integrating Meta Mean Teacher into state-of-the-art frameworks (UA-MT, AD-MT, PMT) achieves new state-of-the-art results, with improvements of up to 1.58% in Jaccard coefficient and significant reductions in 95HD and ASD metrics across various labeled data proportions (5%, 10%, 20%). The visualizations provided in Figure 3 clearly illustrate the segmentation quality improvements over baseline methods.\n\n## 2. Major and Minor Comments\n\n**Major Comments:**\n\n1. **Insufficient validation of student-guided vs. teacher-guided approach:** While the paper provides a strong theoretical justification for the student-guided meta-learning approach, the empirical validation is limited to a single ablation study (Table 1) on the LA dataset. More comprehensive experiments across both datasets and different labeled data proportions would strengthen the claim that student-guided learning is consistently superior to teacher-guided learning.\n\n2. **Lack of computational complexity analysis:** The paper does not discuss the additional computational cost of the meta-learning approach compared to standard Mean Teacher methods. Since medical imaging applications often face computational constraints, understanding the training time overhead and resource requirements of the proposed method is crucial for practical implementation.\n\n3. **Incomplete comparison with state-of-the-art:** The paper claims to achieve new state-of-the-art results but does not comprehensively compare with all recent semi-supervised approaches that don't use the Mean Teacher framework. Some alternative methods may offer comparable or better performance without the added complexity of dynamic decay modulation.\n\n4. **Insufficient description of meta-dataset:** The paper lacks critical details about how the meta-dataset is selected from the training data for the meta-learning phase. This information is essential for reproducibility and understanding potential data leakage issues that might affect results.\n\n5. **Lack of insight into dynamic decay behavior:** The paper would benefit from visualizations or analysis showing how the DDM module dynamically adjusts the decay coefficient during training, which would provide deeper understanding of the method's operation and help validate the authors' claims about adapting to training dynamics.\n\n**Minor Comments:**\n\n1. The paper would benefit from additional ablation studies on the DDM module architecture (e.g., different MLP sizes, activation functions) to demonstrate the robustness of the design choices.\n\n2. The explanation of \"pseudo-gradient descent\" mechanism could be made more accessible for readers without extensive meta-learning background.\n\n3. The theoretical analysis in Section 3.2 is somewhat abstract and could be strengthened with concrete mathematical examples or visualizations.\n\n4. Figure 1's caption states it shows results on the LA Heart dataset, but the x-axis is labeled \"Weight\" without clear connection to EMA decay coefficient Œ±, potentially causing confusion for readers.\n\n5. The discussion of limitations in Section 5 is somewhat superficial; expanding on scenarios where dynamic decay modulation might not provide benefits would strengthen the paper's credibility.\n\n## 3. Evaluation Against TMI Editorial Criteria\n\n**Significance:** The work addresses a critical problem in medical imaging‚Äîlimited labeled data‚Äîwhich is foundational for clinical applications. The Mean Teacher framework is widely used in medical image segmentation, and improving its performance through dynamic decay modulation has high potential impact across various medical imaging domains. The demonstrated performance gains with limited labeled data directly address a key bottleneck in deploying AI in healthcare.\n\n**Innovation:** The paper presents a novel approach to dynamically adjusting the EMA decay coefficient using meta-learning, which represents a significant conceptual advance over fixed-coefficient approaches. The student-guided meta-learning strategy provides an innovative perspective on optimizing this critical parameter, and the demonstration that it outperforms teacher-guided approaches offers new theoretical insights into the Mean Teacher framework.\n\n**Evaluation:** The evaluation is comprehensive across two diverse datasets with different modalities and varying proportions of labeled data. The use of four complementary metrics (Dice, Jaccard, 95HD, ASD) provides a thorough assessment of segmentation quality. However, the meta-dataset selection process is not clearly described, and the lack of analysis showing how the decay coefficient evolves during training limits the depth of the evaluation.\n\n**Reproducibility:** The paper provides reasonable implementation details including network architecture (VNet), training parameters, and optimization settings. However, key details about the DDM module architecture and meta-dataset selection are missing, which would be necessary for exact replication. The promise to release code upon acceptance helps address this concern, but the paper should still include more methodological specifics for immediate reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision (Reject/Resubmit)**\n\nThe paper presents a valuable contribution to semi-supervised medical image segmentation with a novel approach to enhancing the Mean Teacher framework. The core idea is sound, the methodology is well-structured, and the results demonstrate consistent improvements over existing methods. However, several critical issues require substantial revision before the paper can be accepted. Specifically, the authors need to: (1) provide more comprehensive validation of the student-guided approach; (2) analyze the computational cost of their method; (3) clarify the meta-dataset selection process; (4) include visualizations of how the decay coefficient evolves during training; and (5) expand the comparison with state-of-the-art methods. Addressing these concerns would significantly strengthen the paper's contribution and ensure the claims are adequately supported by empirical evidence. I recommend major revision with the expectation that the authors can adequately address these points in a revised submission.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes *Meta Mean Teacher*, a meta-learning framework designed to enhance the Mean Teacher paradigm for semi-supervised medical image segmentation, particularly under limited labeled data conditions. The key contribution is the introduction of a Dynamic Decay Modulation (DDM) module that adaptively adjusts the Exponential Moving Average (EMA) decay coefficient based on evolving training dynamics. The DDM module optimizes this decay value using pseudo-gradient descent guided by student and teacher network losses. Experiments on two distinct medical imaging datasets (LA Heart and Pancreas-NIH) demonstrate consistent performance gains over traditional Mean Teacher variants and further improvements when integrated into existing frameworks such as UA-MT, AD-MT, and PMT. The reported results include notable increases in segmentation metrics and improved boundary accuracy across varying proportions of labeled data.\n\n---\n\n**Major Comments**  \n1. **Empirical validation of student-guided approach:** The superiority of the student-guided meta-learning strategy is supported by only one ablation study (on LA dataset). Broader validation across both datasets and label proportions is needed to substantiate this claim.  \n2. **Computational cost analysis:** The paper omits discussion of the computational overhead introduced by the meta-learning component. Quantifying additional training time or resource demands is essential for assessing feasibility in medical imaging contexts.  \n3. **Comparisons with other methods:** Although the work claims state-of-the-art results, comparisons are largely confined to Mean Teacher variants. Evaluation against other contemporary semi-supervised approaches not based on Mean Teacher would clarify the method‚Äôs relative advantages.  \n4. **Meta-dataset details:** The procedure for constructing the meta-dataset used in training is insufficiently described. Clear explanation is required to ensure reproducibility and prevent ambiguity regarding potential data leakage.  \n5. **Dynamic decay analysis:** The paper would benefit from visualization or quantitative tracking of how the DDM module adjusts the EMA decay during training to evidence its adaptive behavior.\n\n---\n\n**Minor Comments**  \n1. Include ablation studies on DDM architecture parameters (e.g., MLP size, activations) to confirm design robustness.  \n2. Simplify the description of the pseudo-gradient descent mechanism for readers less familiar with meta-learning.  \n3. Strengthen the theoretical section (3.2) with concrete examples or illustrative figures.  \n4. Clarify Figure‚ÄØ1: the label ‚ÄúWeight‚Äù should directly relate to the EMA decay coefficient Œ±.  \n5. Expand the limitations section to discuss scenarios where dynamic decay modulation may offer limited benefit.\n\n---\n\n**Summary Paragraph**  \nOverall, the study makes a technically meaningful and potentially impactful contribution to semi-supervised medical image segmentation. Its principal strength lies in adapting a key Mean Teacher parameter through meta-learning, offering a novel and conceptually elegant improvement. Nonetheless, the empirical validation, evaluation breadth, and methodological transparency require enhancement. Additional experiments, clearer computational analysis, and visual evidence of decay modulation dynamics would appreciably strengthen the work‚Äôs reliability and reproducibility.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The study is promising and methodologically sound but requires significant revisions to substantiate claims, clarify implementation details, and broaden evaluation before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThis manuscript tackles the well‚Äëknown limitation of using a fixed exponential moving average (EMA) decay coefficient in Mean‚ÄëTeacher semi‚Äësupervised segmentation of 3‚ÄëD medical images. The authors introduce **Meta Mean Teacher**, which incorporates a Dynamic Decay Modulation (DDM) module that learns the EMA coefficient through meta‚Äëlearning based on both student and teacher losses. Experiments are performed on two volumetric datasets‚Äîa left‚Äëatrial MRI collection (100 volumes) and a pancreatic CT set (82 volumes)‚Äîusing a VNet backbone. The evaluation includes Dice, Jaccard, 95‚ÄØ% Hausdorff Distance and Average Surface Distance. Reported results indicate that the DDM‚Äëaugmented models achieve modest but consistent improvements (up to ~1.3‚ÄØ% absolute Dice gain) and reductions in distance‚Äëbased metrics compared with fixed‚ÄëŒ± baselines. The primary contribution is a plug‚Äëand‚Äëplay framework that adapts the EMA decay coefficient during training, thereby enhancing the teacher‚Äëstudent interaction in semi‚Äësupervised segmentation.\n\n---\n\n## General feedback  \n\n- **Relevance:** The sensitivity of the EMA decay to the dynamics of training is an important issue for semi‚Äësupervised segmentation when only a small number of annotated 3‚ÄëD volumes are available.  \n- **Originality:** Employing meta‚Äëlearning to adapt the EMA coefficient represents a novel twist on the classic Mean‚ÄëTeacher paradigm. Although the DDM itself is a relatively simple MLP that maps loss information to a scalar, the idea of learning the decay schedule on the fly is commendable.  \n- **Experimental scope:** The authors evaluate two distinct imaging modalities and benchmark against several strong baselines, which adds confidence to the empirical claims. However, the study would benefit from reporting statistical significance, variance, and perhaps an additional external dataset to further substantiate the findings. Some reported numbers (e.g., the 95‚ÄØ% Hausdorff Distance for UA‚ÄëMT+DDM in Table‚ÄØ2) appear at odds with the described improvements and should be clarified.  \n- **Reproducibility:** While the overall methodology is clear, details concerning the DDM hyper‚Äëparameters (architecture, learning rates, number of meta‚Äëiterations), the construction of the meta‚Äëdataset, and the computational overhead of the meta‚Äëlearning step are missing. Making the code available only after acceptance limits immediate verification, but providing more implementation specifics in the manuscript would greatly aid reproducibility.\n\n---\n\n## Specific comments / suggestions for improvement  \n\n1. **DDM architecture description:** The paper states that the DDM is ‚Äúa small neural network, typically an MLP,‚Äù yet it does not enumerate the number of layers, hidden units, activation functions, or total parameters (see ¬ß3.2). Adding these details would allow readers to gauge model complexity and facilitate replication.  \n\n2. **Meta‚Äëdataset handling:** The source, size, and splitting strategy for the meta‚Äëdataset \\(D_m\\) are not specified, leaving uncertainty about potential overlap with the primary training set (Algorithm‚ÄØ1). Clarifying how \\(D_m\\) is constructed and ensuring it is independent from the training data would strengthen the methodological rigor.  \n\n3. **Definition of ‚Äúpseudo‚Äëgradient descent‚Äù:** This term is introduced without a formal definition or citation in ¬ß3.1, which makes the update rule ambiguous. Providing a precise mathematical formulation or referencing prior work that employs this concept would improve reproducibility.  \n\n4. **Metric inconsistency:** In Table‚ÄØ2, the 95‚ÄØ% Hausdorff Distance for UA‚ÄëMT+DDM appears to increase from 13.71 to 25.59, contradicting the narrative of improvement. This likely reflects a typographical error; correcting the table will restore confidence in the reported results.  \n\n5. **Statistical analysis:** All tables present single mean values. Including standard deviations, confidence intervals, or results of statistical tests (e.g., paired t‚Äëtests) would help assess whether the ‚â§‚ÄØ1‚ÄØ% Dice improvements are statistically meaningful.  \n\n6. **Broader ablation study:** The current ablation compares fixed‚ÄØŒ±, teacher‚Äëguided DDM, and student‚Äëguided DDM. Adding a baseline that uses a simple scheduled Œ± (e.g., linear or cosine decay) would clarify whether the meta‚Äëlearning component offers a genuine advantage over conventional decay schedules.  \n\n7. **Training cost assessment:** The manuscript does not quantify the additional runtime or memory requirements introduced by the meta‚Äëlearning phase. Reporting these overheads would allow readers to evaluate the practicality of deploying the proposed method in real‚Äëworld settings.  \n\n8. **Baseline diversity:** Recent semi‚Äësupervised approaches such as FixMatch, contrastive learning frameworks, or other dynamic‚Äëteacher strategies are absent from the comparative study. Including these methods could provide a more comprehensive benchmark and highlight the relative strengths of Meta Mean Teacher.  \n\n9. **Code availability:** Stating that the code will be released only after acceptance limits transparency. Providing a public repository (or at least a link to a pre‚Äërelease version) would enable the community to verify and build upon the work sooner.\n\n---\n\n## Suggested decision  \n\n**Reject**  \n\n*Rationale:* While the paper addresses a pertinent problem and proposes an innovative meta‚Äëlearning based solution, the current manuscript lacks sufficient methodological detail, statistical validation, and comprehensive benchmarking to meet the journal‚Äôs standards for reproducibility and scientific rigor. Addressing the points above would greatly enhance the work‚Äôs impact and suitability for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the limitation of using a fixed exponential moving average (EMA) decay in Mean‚ÄëTeacher frameworks for semi‚Äësupervised 3‚ÄëD medical image segmentation. It proposes a Meta Mean Teacher approach that dynamically learns the EMA decay coefficient through a Dynamic Decay Modulation (DDM) module using meta‚Äëlearning informed by both student and teacher losses. Experiments on two volumetric datasets‚Äîleft‚Äëatrial MRI and pancreatic CT‚Äîdemonstrate modest but consistent performance gains over fixed‚ÄëŒ± baselines. The paper is clearly written and addresses a relevant problem, though several methodological and experimental aspects require clarification and expansion to ensure reproducibility and robustness of the reported findings.  \n\n---\n\n**Major Comments**  \n1. **Methodological detail and reproducibility:** The description of the DDM is insufficient. Information such as the number of layers, hidden units, activation functions, and specific hyper‚Äëparameters is missing, hindering reproducibility. The construction and independence of the meta‚Äëdataset \\(D_m\\) also remain unclear, as does the definition of ‚Äúpseudo‚Äëgradient descent.‚Äù  \n2. **Experimental validity:** Statistical analyses (variance, confidence intervals, or significance tests) are absent, making it difficult to judge whether the reported ‚â§‚ÄØ1‚ÄØ% performance improvements are meaningful.  \n3. **Metric inconsistency:** The 95‚ÄØ% Hausdorff Distance values for UA‚ÄëMT+DDM in Table‚ÄØ2 appear inconsistent with the stated improvements, suggesting a possible typographical or reporting error.  \n4. **Comparative scope:** Evaluation is limited to a narrow set of baselines. Including alternative decay schedules (e.g., linear or cosine) and recent semi‚Äësupervised frameworks such as FixMatch or contrastive approaches would better contextualize the contribution.  \n5. **Resource analysis:** The additional computational cost of the meta‚Äëlearning phase is not quantified, making practical utility difficult to assess.  \n6. **Code and transparency:** Deferring code release until after acceptance reduces immediate verifiability and transparency.  \n\n---\n\n**Minor Comments**  \n- Include standard deviations or confidence intervals in result tables.  \n- Ensure that all symbols and terms (e.g., ‚Äúpseudo‚Äëgradient descent‚Äù) are clearly defined or referenced.  \n- Verify numerical consistency across tables and correct typographical errors.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper presents an original and relevant enhancement to Mean‚ÄëTeacher methods by learning an adaptive EMA decay through meta‚Äëlearning. The contribution is conceptually interesting but lacks sufficient methodological transparency, robust statistical validation, and comprehensive comparative analysis. Addressing these issues would considerably improve the scientific rigor and credibility of the work.  \n\n---\n\n**Decision Recommendation**  \n**Reject.** The submission introduces a promising idea but currently falls short in methodological detail, evaluation depth, and reproducibility to warrant acceptance in its present form.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chen Chen",
      "Le Wang",
      "Ning Gao",
      "Sanping Zhou"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a2ba00a164f58076d0fbf59a182260f8e32b0178.pdf",
    "remote_url": "https://openreview.net/pdf/a2ba00a164f58076d0fbf59a182260f8e32b0178.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Conformal confidence sets for biomedical image segmentation",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Deep learning",
      "neural networks",
      "uncertainty quantification",
      "confidence sets"
    ],
    "abstract": "We develop confidence sets which provide spatial uncertainty guarantees for the output of a black-box machine learning model designed for image segmentation. To do so we adapt conformal inference to the imaging setting, obtaining thresholds on a calibration dataset based on the distribution of the maximum of the transformed logit scores within and outside of the ground truth masks. We prove that these confidence sets, when applied to new predictions of the model, are guaranteed to contain the true unknown segmented mask with desired probability. We show that learning appropriate score transformations on an independent learning dataset before performing calibration is crucial for optimizing performance. We illustrate and validate our approach on polyps colonscopy, brain imaging and teeth datasets. To do so we obtain the logit scores from deep neural networks trained for polyps, brain mask and tooth segmentation segmentation. We show that using distance and other transformations of the logit scores allows us to provide tight inner and outer confidence sets for the true masks whilst controlling the false coverage rate.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper proposes a conformal prediction based method to quantify the uncertainty for medical image segmentation. The proposed method is particularly designed for pre-trained segmentation models which notoriously make overconfident and wrong predictions. The proposed method learns thresholds using the maximum logit scores from a calibration set for the inside and outside of the ground truth masks and apply them on the logit scores of the test image to return conformalized segmentation prediction which guarantees to include the ground truth segmentation. The paper shows that naively learning the outside thresholds on max logits is not optimal and propose to transform the scores using a distance to make sure that far away pixels have lower scores. The method is validated on a single dataset for polyp segmentation and the results show that the proposed method produces conformal sets with narrower boundaries compared to using scores which are not transformed.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 1\n\n### Strengths\n\n- The idea of using transformed max logit scores is simple but quite effective strategy to produces conformal segmentation sets.\n- The presented experiments show the effectiveness of the method compared to using non-transformed logits.\n\n### Weaknesses\n\n1- Although I found the proposed idea of transforming max logit scores interesting, I don't think that the paper presents enough contribution to be presented in ICLR. The idea of applying conformal prediction to max logits for inside and outside of the boundaries is a direct extension of initial conformal prediction methods developed for segmentation, and applying transformations based on distance is an intuitive choice to refine predicted boundaries.\n\n2- The paper does not present any comparisons with the existing conformal prediction works for image segmentation.\n\n[1] Mossina et al. Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty, CVPR Workshops, 2024,\n\n3- The method is evaluated on only a single dataset. Multiple datasets should be included to make sure that the performance generalizes across datasets.\n\n4- In many segmentation tasks, we are interested in segmenting multiple structures. The paper only focuses on binary segmentation. I think the method should be validated on multi-class setting to make sure that it is also applicable in that setting.\n\n5- The explanation of how the method is applied at test time could also be clearer. As I understand it, during testing, the method applies the inner threshold on max logits to find inner boundaries, then applies a distance transformation based on each pixel‚Äôs distance from these inner boundaries, and finally applies an outer boundary threshold. However, the exact steps of the algorithm during test time need more clarification.\n\n6- In conventional uncertainty quantification algorithms for segmentation such as [2, 3] the uncertainty is quantified by the variance of the segmentation samples generated from the posterior distribution. How can the quantification be done in this case? Is it the margin between the inner and outer boundaries? Is the uncertainty quantified by the algorithm correlates with the uncertainty in the input image? For example, does the method output larger margins when there is greater disagreement between the segmentations of different experts?  \n\n[2] Kohl et al. A Probabilistic U-Net for Segmentation of Ambiguous Images\n[3] Erdil et al. MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\n\n7- The margin between the inner and outer boundaries appears quite large and there can be many unplausible segmentations within this area. For practical applications, an uncertainty quantification method should ideally produce a set of plausible segmentation samples within this margin, rather than simply indicating a large margin that may or may not include the ground truth segmentation. How could one obtain a plausible segmentation sample from this margin?\n\n### Questions\n\n- How does the results generalize to other datasets and segmentation of multiple structures?\n- How does the uncertainty quantified by the proposed method relates with the real uncertainty (assuming it can be measured by the disagreement between multiple experts)?\n- How one can use the proposed method in a practical application? Can we get samples of plausible segmentations within the margin outputted by the algorithm?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a conformal prediction‚Äìbased framework for uncertainty quantification in medical image segmentation, specifically targeting pre-trained segmentation models that tend to produce overconfident outputs. The approach calibrates thresholds for pixels inside and outside ground-truth masks using maximum logit scores, optionally transformed by distance to penalize pixels far from boundaries. The method is tested on a single dataset for polyp segmentation, showing that distance-transformed logits yield narrower conformal segmentation boundaries than untransformed scores. Overall, the paper is clearly written and technically sound, but its novelty and scope appear limited.\n\n**Major Comments**  \n1. The main contribution‚Äîapplying conformal prediction to max logits and refining thresholds using distance‚Äîis incrementally extending existing conformal segmentation methods. The conceptual advance may not meet the standard required for a high-impact venue.  \n2. The work lacks comparison with prior conformal prediction approaches for segmentation, such as Mossina et al. (CVPRW 2024), making it difficult to assess the relative benefit of the proposed technique.  \n3. Evaluation is limited to a single dataset (polyp segmentation), which restricts conclusions about generalization. Additional datasets are needed.  \n4. The method is only demonstrated on binary segmentation. Validation on multi-class segmentation tasks would strengthen applicability.  \n5. The test-time procedure is not entirely clear. The description of how inner thresholds, distance transformations, and outer thresholds interact could be elaborated for reproducibility.  \n6. The paper should clarify how uncertainty is quantified. Is it represented by the margin between inner and outer boundaries, and does it correlate with annotation variability (e.g., expert disagreement)?  \n7. The large margins observed between inner and outer boundaries may encompass implausible segmentations. A discussion or mechanism to constrain these to plausible structures would increase practical relevance.\n\n**Minor Comments**  \n- Improve algorithmic clarity in the test-time description.  \n- Ensure all referenced works (e.g., [1‚Äì3]) are properly cited and discussed.  \n- Expand the explanation of how margin width relates to uncertainty interpretation.\n\n**Summary Paragraph**  \nThe manuscript presents a simple and interpretable adaptation of conformal prediction for segmentation uncertainty but contributes limited methodological novelty beyond prior work. Its empirical validation, while promising, remains narrow in scope and lacks broad comparative and multi-class analysis. Clarifications on the algorithmic pipeline and clearer uncertainty interpretation would strengthen the study.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper is technically correct but requires expanded evaluation, stronger comparative analysis, and clearer exposition to support its contribution level.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors formally present an approach that aims at inferring uncertainty margins to segmentations. They propose either take the logit score of a CNN and to threshold it to obtain this margin, or to threshold at a certain distance to the predicted segmentation. Threshold and type of margin (logit score / distance) is to be identified experimentally for a given dataset. Experiments on one public dataset  are shown (containing still images from minmally invasive surgery).\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* The authors present the problem in a formal manner, relating it to existing work.  \n* The overall problem addressed is relevant.\n\n### Weaknesses\n\n* The motivation for the scores functions (logit, distance, ...) is weak. The necessity to choose the type and to even mix them gives the overall approach a bit of a heuristic touch. (While I do understand that you would consider your contribution here to be in the formal derivation of underlying theory, i.e., very much the opposite of a heuristic.)\n* The experiments only provide insights into one very narrow application. they are merely fulfilling the purpose of an illustation of the problem, but not a validation.\n\n### Questions\n\n* You are testing on public data. Has your pretrained polyp segmentation algorithm been trained on the same public data? \n* Are there any susequent video frames in the dataset, or images of the same polyp / patient? If there are, did you stratify your training / testing set accordingly?  \n* Please remove the reference to tumors throughout the paper. Polyps may be precursors to tumors, but they aren't any. \n* You are using a dataset from different centers, there may be systematic differences in how the polyp areas are annotated - some annotators being more inclusive with respect to surrounding tissue, others being less. How does this variability impact on your measure? \n*  I might have missed it but what is the accuracy of your underlying segmentation algorithm? I would be under the impression that it is a well performing algorithm on a rather easy segmentation task? How does your approach relate to extrema in algorithmic performance, i.e., perfect segmentations or complete misses? \n* You are stating \"In order to make efficient use of the data available, the learning dataset can in fact contain some or all of the data used to train the image segmentor.\" Your training data may be fairly overfittet impacting on your logit score and, hence, your choice of margin (logit/distance, thresholds). Wouldn't it be a safer approach to generate cross-validated logit functions and use them in the comparison?\n* I understand that the primary contribution of this study is the theory offered. Still, you are stressing that your algorithm is a very lightweight addition to any pretrained segmentation algorithm. And there are a lot of standard computer vision / biomedical image data sets for segmentation available, as well as pretrained algorithms. Would you be able to generate segmentations maps for predefined certainty levels, and compare these levels with the testing performances across a larger set of applications? It would be quite convincing, if e.g., your 90% certainty map of the outer margin would indeed include 90% pixels of a test set or lead to a sufficiently large overlap (that has previously been defined) in 90% of all test cases.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3\n\n### Details Of Ethics Concerns\n\nnone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a formal method for estimating uncertainty margins in image segmentation tasks. The approach involves thresholding either the logit scores or the distance from the predicted segmentation to identify uncertain regions, with the optimal threshold and margin type determined experimentally for each dataset. Experiments are demonstrated on a public dataset of still images from minimally invasive surgery. The problem addressed is relevant and the manuscript presents it in a formal and structured manner, although the experimental evaluation is limited.\n\n**Major Comments**  \n1. **Motivation and Theoretical Foundation** ‚Äì The justification for the choice of scoring functions (logit vs. distance) is insufficient. The flexibility to select or combine them gives the proposed method a heuristic character, which appears contrary to the authors‚Äô intent of providing a theoretical contribution.  \n2. **Experimental Scope and Validation** ‚Äì The experiments are very limited, using only one narrow application. They serve more as an illustration than a true validation of the proposed approach.  \n3. **Training and Data Management** ‚Äì Clarification is needed regarding whether the pretrained polyp segmentation algorithm was trained on the same public dataset used in testing. The authors should also specify whether there is overlap between training and test data, particularly if images of the same patient or polyp are included.  \n4. **Dataset Composition and Annotation Variability** ‚Äì The influence of inter-center differences in annotation style (more or less inclusive delineation of polyp areas) on the derived uncertainty measures should be addressed.  \n5. **Baseline Performance and Overfitting** ‚Äì Information is missing on the baseline segmentation accuracy. Given that the segmentation task may be relatively easy, it is unclear how the proposed measure behaves under extreme performance conditions (perfect or failed segmentations). Further, using the same data to train both the segmentor and uncertainty model risks overfitting; cross-validated logit functions might provide a safer evaluation.  \n6. **Generalizability** ‚Äì Since the approach is described as lightweight and potentially adaptable, evaluation on a broader range of datasets would strengthen the empirical claims, for example by comparing predicted uncertainty levels against observed segmentation accuracy.\n\n**Minor Comments**  \n- Remove all references to ‚Äútumors,‚Äù as polyps are not themselves tumors.  \n- Minor typographical errors and phrasing inconsistencies can be corrected for improved readability.\n\n**Summary Paragraph**  \nThe paper addresses a relevant problem and is presented with adequate formalism. However, the methodological motivation appears weak, and the validation is restricted to a single dataset without adequate controls for overfitting or annotation bias. Broader experiments and clearer articulation of theoretical rationale would be necessary to support the claims.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial improvements in justification, experimental validation, and dataset handling are required before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors propose a conformal prediction method that computes confidence sets with spatial uncertainty guarantees in image segmentation from any machine learning model. They illustrate the usefulness of the proposed method on medical images.\n\n### Soundness: 4\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper is well-written and clear, although it took a second read-through to fully understand. The proposed method seems to work very well, and the presented experiments are convincing.\n\n### Weaknesses\n\nI am missing more quantitative results. For instance, aggregated coverage scores (e.g., mean; or other metrics, e.g., evaluate Equations 1 and 2) for the different versions on more than one dataset. This comparison should then also include some existing methods, to illustrate the relative strengths of different methods.\n\nAs just mentioned, for the results to be more convincing, I would also like to see examples on more than just one dataset.\n\nAlso, there must be other score transformation functions that could also be evaluated. Testing a couple more could strengthen the results and make it more convincing.\n\n### Questions\n\n- Couldn't a related/similar smooth distance be defined using kernels?\n- What is called \"original scores\", is this when you use the identity score transformation?\n- What are the dashed lines in Figures 4 and 5?\n\nMajor comments:\n- Add labels and/or legends to the rows and columns of the figures.\n\nMinor comments:\n- The word \"polyp\" is misspelled in different ways in almost every instance. Do check this.\n- It says \"... the set a side [num] images ...\", or something similar, a few times. Check the grammar there.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a conformal prediction framework for image segmentation that generates confidence sets with spatial uncertainty guarantees applicable to any machine learning model. The proposed approach is applied to medical imaging data to demonstrate its practical utility. Overall, the paper is well written and generally clear, though some aspects required rereading for full comprehension. The experimental results are convincing, and the method appears to perform effectively across the presented examples.\n\n**Major Comments**  \n1. **Quantitative Evaluation** ‚Äì The paper would benefit from more comprehensive quantitative results, such as aggregated coverage scores (mean or alternative metrics) derived from Equations 1 and 2. Results across multiple datasets would make the evidence more robust.  \n2. **Comparative Analysis** ‚Äì Including comparisons with existing methods would help contextualize the proposed approach and highlight its relative advantages.  \n3. **Dataset Diversity** ‚Äì The evaluation is currently limited to a single dataset. Testing the method on additional datasets would strengthen confidence in its generalizability.  \n4. **Alternative Transformations** ‚Äì The authors should explore other possible score transformation functions beyond the one discussed to assess sensitivity and confirm the method‚Äôs flexibility.  \n5. **Figures** ‚Äì Figures would benefit from clearer labeling, including legends and explicit identifiers for rows and columns. In Figures 4 and 5, the meaning of the dashed lines should be explained.\n\n**Minor Comments**  \n- Verify spelling of the word ‚Äúpolyp,‚Äù which appears inconsistently.  \n- Correct minor grammatical issues such as ‚Äúset a side‚Äù ‚Üí ‚Äúset aside.‚Äù  \n- Clarify terminology: confirm whether ‚Äúoriginal scores‚Äù refers to the identity score transformation.  \n- Consider whether a related smooth distance function could be defined via kernels.\n\n**Summary Paragraph**  \nThe paper presents a promising and well-articulated method with clear motivation and convincing preliminary experiments. However, the evaluation lacks breadth in datasets, quantitative analyses, and baseline comparisons. Addressing these points would substantially enhance the strength and generalizability of the findings.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The study is promising, but substantial additional experiments and clarifications are needed before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The paper is clearly methodological rather than application‚Äëoriented. It develops a new mathematical and statistical framework for constructing spatial **conformal confidence sets** for image segmentation outputs, with theoretical coverage guarantees. This relates directly to quantitative imaging analysis and statistical image inference, fitting well within the IEEE‚ÄØTMI scope.\n\n2. **Novelty & Contribution Level**  \nThe central novelty is the *adaptation of conformal inference* to generate *spatially aware uncertainty regions* for segmentation masks, including:\n- Use of image‚Äëlevel exchangeability to obtain rigorous finite‚Äësample guarantees.\n- Derivation of inner and outer confidence sets with probabilistic coverage of the full segmentation mask, not just pixelwise or FDR‚Äëcontrolled uncertainty.\n- Introduction of *learned score transformations* (notably the distance transformation) to improve the geometric tightness of the sets.\n- Theoretical results (Theorems‚ÄØ2.1‚Äì2.8) linking coverage control to Hausdorff distance and bounding‚Äëbox formulations.  \nThis is a meaningful methodological advance beyond prior ‚Äúpixelwise conformal‚Äù or ‚Äúrisk‚Äëcontrolled‚Äù approaches.  \n\n3. **Technical & Experimental Rigor**  \nTheory is formally developed with proofs, assumptions stated, and multiple validation studies on independent datasets (polyps, brain, teeth). Experiments demonstrate both coverage control at nominal levels and comparative efficiency of score transformations.  \nPotential issues: limited discussion of computational complexity, sensitivity to segmenter quality, and reliance on reasonably large calibration datasets. Reproducibility seems supported through code availability.\n\n4. **Clarity & Presentation**  \nThe manuscript is generally clear and well‚Äëstructured, though mathematically dense. Figures are numerous and informative, though some legends and notations could be streamlined. Minor English style and formatting corrections would improve readability.\n\n5. **Ethical & Reproducibility Compliance**  \nAll datasets are publicly available and anonymized. The text explicitly mentions compliance and provides code for replication. No ethical concerns detected.\n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThe authors propose a general framework for constructing conformal confidence sets that provide spatially resolved uncertainty guarantees for biomedical image segmentation. Instead of pixel‚Äëlevel conformal p‚Äëvalues, the method calibrates thresholds based on maxima of transformed logit scores within and outside ground‚Äëtruth masks. Distance‚Äëbased and other transformations can be learned on an auxiliary dataset to produce tight outer and inner bounds. Theoretical coverage results are proven under exchangeability. Experiments on colon‚Äëpolyp, brain‚Äëmask, and dental CT segmentations validate finite‚Äësample coverage and show practical utility for uncertainty quantification.\n\n**2. Strengths**\n- Strong theoretical grounding with well‚Äëdefined probabilistic guarantees.  \n- Novel extension of conformal inference to spatial segmentation masks.  \n- Demonstrated generality across different imaging modalities and neural‚Äënetwork backbones.  \n- Practical insights on transformation learning (e.g., distance transform) that markedly improve confidence‚Äëset precision.  \n- Clear link to clinical uncertainty needs and easy computational implementation.\n\n**3. Weaknesses**\n- Some derivations assume ideal exchangeability and independence between segmenter and calibration data; discussion of violations (e.g., domain shift) is brief.  \n- Evaluation focuses mainly on qualitative visualizations and coverage‚Äërate plots; limited quantitative metrics on set tightness versus segmentation accuracy.  \n- The method‚Äôs scalability to 3‚ÄëD volumetric data is not fully evaluated.  \n- Writing densely mathematical; minor editing needed for accessibility to broader TMI readership.\n\n**4. Major Comments**\n1. **Exchangeability and Independence** ‚Äì elaborate on how violations (e.g., correlated calibration/test subjects, retraining same network) might affect validity, and potential remedies (weighted conformal, domain adaptation).  \n2. **3‚ÄëD and Computational Cost** ‚Äì extend discussion/experiments to full volumetric data efficiency to demonstrate feasibility for standard medical‚Äëimage sizes.  \n3. **Quantitative Evaluation of Efficiency** ‚Äì include standardized metrics (Dice of inner/outer bounds, average Hausdorff width) to better quantify practical informativeness.  \n4. **Transformation Learning Strategy** ‚Äì clarify selection criteria, hyperparameters, and avoid any circularity when limited data are available.  \n5. **Potential Clinical Integration** ‚Äì briefly discuss how these spatial uncertainty bounds could be used in downstream decision‚Äëmaking or uncertainty visualization interfaces.\n\n**5. Minor Comments**\n- Correct small typographical and spacing errors (e.g., ‚Äúpolyps colonscopy,‚Äù ‚Äúacheived‚Äù).  \n- Improve figure captions to be self‚Äëcontained.  \n- Consolidate repetitive references to sections/appendices for conciseness.  \n- Ensure consistent notation for Œ±‚ÇÅ,‚ÄØŒ±‚ÇÇ across figures and text.\n\n**6. Overall Recommendation**  \n**Major Revision**  \nThe manuscript offers significant theoretical and methodological innovation of clear relevance to TMI, but a stronger empirical section, clearer exposition of assumptions, and expanded discussion of computational/clinical aspects would materially strengthen it before publication.\n\n**7. Confidence Level:** 5 (high confidence ‚Äî topic and methods align with reviewer expertise).",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a methodological framework for constructing spatial conformal confidence sets that provide uncertainty quantification for image segmentation outputs. Drawing from conformal inference, the approach generates spatially aware inner and outer confidence regions for segmentation masks with theoretical finite‚Äësample coverage guarantees. The work is clearly written and mathematically rigorous, though dense in presentation. Its contributions are foundational to quantitative imaging analysis, combining theoretical proofs with empirical validation across several biomedical imaging datasets.\n\n---\n\n**Major Comments**  \n1. **Exchangeability and Independence Assumptions** ‚Äì Theoretical validity hinges on the assumption of exchangeability and independence between the segmentation model and calibration datasets. The discussion of how violations, such as correlated calibration/test subjects or retraining the same model, affect coverage is brief. Further elaboration and potential remedies (e.g., weighted conformal or domain adaptation strategies) would enhance robustness.  \n2. **Evaluation of 3‚ÄëD Data and Computational Cost** ‚Äì The paper demonstrates the method on multiple 2‚ÄëD datasets but does not fully assess efficiency or scalability in volumetric (3‚ÄëD) contexts. Adding such experiments or a complexity analysis would improve practical applicability.  \n3. **Quantitative Metrics of Efficiency** ‚Äì Experimental evaluation relies primarily on coverage‚Äërate plots and visual results. Including quantitative measures such as Dice overlap of inner/outer bounds or average Hausdorff width would better convey the informativeness of the proposed uncertainty sets.  \n4. **Transformation Learning Details** ‚Äì The distance‚Äëbased and other learned score transformations are central to performance. The manuscript should clarify the selection process, relevant hyperparameters, and how data limitations are handled to avoid circularity.  \n5. **Clinical Integration** ‚Äì A short discussion on the potential downstream use of spatial confidence sets for visualization or decision support would contextualize the method‚Äôs practical utility.\n\n---\n\n**Minor Comments**  \n- Correct typographical and spacing errors (e.g., ‚Äúpolyps colonscopy,‚Äù ‚Äúacheived‚Äù).  \n- Improve figure captions to be more self‚Äëcontained and concise.  \n- Ensure consistent notation (Œ±‚ÇÅ,‚ÄØŒ±‚ÇÇ) across figures and text.  \n- Reduce redundancy between main text and appendices.  \n- Minor English and formatting edits could enhance readability.\n\n---\n\n**Summary Paragraph**  \nOverall, the paper represents a strong theoretical advance in uncertainty quantification for segmentation, integrating conformal methods with spatial modeling. It is well‚Äëmotivated and experimentally validated across several datasets, with reproducible results and publicly available code. The main weaknesses lie in limited quantitative evaluation, restricted discussion of computational scalability, and dense exposition. Addressing these points would substantially improve accessibility and empirical completeness.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work is innovative and methodologically solid but requires stronger empirical evaluation, expanded discussion of assumptions and scalability, and improved clarity before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper develops conformal confidence sets for biomedical image segmentation that provide spatial uncertainty guarantees with probabilistic coverage. The authors adapt split-conformal inference by computing thresholds on calibration data based on the distribution of maximum transformed logit scores within and outside ground truth masks. The method constructs inner and outer confidence sets guaranteed to contain the true segmented mask with desired probability under exchangeability assumptions (Assumption 1, Section 2.1). Key theoretical contributions include marginal coverage results (Theorems 2.1-2.2), joint coverage guarantees (Theorem 2.6), and analysis of distance transformations showing improved performance when segmentation models are accurate (Theorem 2.8). The approach is validated on three biomedical datasets: polyp colonoscopy (Section 3), brain imaging (Section 4), and teeth segmentation (Section 5), demonstrating that distance transformations of predicted masks provide tighter outer confidence sets compared to raw logit scores.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical notation inconsistencies and clarity issues**\n  - The notation switches between MÃÇ(X) and MÀÜ(X) for predicted masks without explanation (pages 2-3)\n  - Definition of sign(A,v) in distance transformation (page 5, line 252) could be clearer about boundary cases\n  - The relationship between score function s and predicted mask MÃÇ(X) is stated as assumed but never formally defined (page 2, lines 108-109)\n\n‚Ä¢ **Limited theoretical analysis of score transformation optimality**\n  - No theoretical guidance provided for selecting optimal transformations beyond distance transforms (Section 2.4)\n  - Theorem 2.8 only covers distance transformations but applications use other methods like smoothing (Section 5, teeth segmentation)\n  - The learning dataset approach lacks theoretical justification for when including training data is appropriate (pages 4-5, lines 216-228)\n\n‚Ä¢ **Experimental validation limitations**\n  - Small dataset sizes limit generalizability, particularly brain imaging (524 subjects, Section 4) and teeth (598 subjects, Section 5)\n  - No comparison with existing conformal methods for image segmentation beyond brief mention of Angelopoulos et al. (2024) approach\n  - Validation procedure uses only 1000 random splits which may not capture full variability in coverage performance (Figure 4, Section 3.3)\n\n‚Ä¢ **Insufficient analysis of computational complexity and scalability**\n  - Algorithm 1 (page 16) shows computational steps but provides no complexity analysis\n  - Runtime reported only for downscaled polyp data (0.03 seconds) with limited scalability assessment (page 10, lines 537-539)\n  - No discussion of memory requirements for large medical images or 3D volumes\n\n‚Ä¢ **Incomplete treatment of practical implementation challenges**\n  - Limited guidance on hyperparameter selection (e.g., FWHM values for smoothing kernels in teeth application)\n  - No analysis of sensitivity to calibration set size or composition\n  - Assumption 2 (independence of scores) may be violated when learning dataset overlaps with training data but this is not addressed theoretically\n\n## Suggestions for Improvement\n\n‚Ä¢ **Standardize mathematical notation and improve clarity**\n  - Use consistent notation throughout (either MÃÇ(X) or MÀÜ(X)) and define all symbols upon first use\n  - Provide explicit definition of sign(A,v) with examples for boundary cases\n  - Formally define the relationship between score function s(X) and predicted mask MÃÇ(X) in the setup section\n\n‚Ä¢ **Develop theoretical framework for transformation selection**\n  - Provide theoretical analysis or guidelines for selecting optimal score transformations beyond distance transforms\n  - Extend theoretical results to cover smoothing and other transformations used in applications\n  - Establish formal conditions for when training data can be safely included in learning datasets without violating independence\n\n‚Ä¢ **Strengthen experimental evaluation**\n  - Include larger datasets or multi-center validation to improve generalizability claims\n  - Add quantitative comparisons with existing conformal image segmentation methods (e.g., Mossina et al. 2024, Bates et al. 2021)\n  - Increase validation runs beyond 1000 or provide statistical analysis of coverage estimate uncertainty\n\n‚Ä¢ **Add comprehensive computational analysis**\n  - Provide theoretical or empirical complexity analysis for all algorithmic components\n  - Include scalability experiments on larger images and 3D volumes with memory usage analysis\n  - Compare computational efficiency with alternative conformal approaches\n\n‚Ä¢ **Enhance practical implementation guidance**\n  - Develop systematic approaches for hyperparameter selection with validation on learning datasets\n  - Analyze sensitivity to calibration set characteristics (size, distribution, quality)\n  - Address potential violations of Assumption 2 when learning and training data overlap, possibly through empirical validation or relaxed theoretical conditions",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a conformal inference framework for biomedical image segmentation that produces spatial uncertainty maps with probabilistic coverage guarantees. By adapting split-conformal methods, the authors construct inner and outer confidence sets based on distributions of transformed logit scores on calibration data. The paper contributes theoretical results on marginal and joint coverage, explores the use of distance transformations to improve the tightness of confidence sets, and validates the approach on three biomedical datasets (polyp, brain, teeth). Overall, the work is mathematically rigorous and addresses an important problem, though several aspects of notation, theoretical completeness, and empirical evaluation require refinement for greater clarity and robustness.  \n\n**Major Comments**  \n1. **Clarity and Notation Issues** ‚Äì The mathematical notation is inconsistent (e.g., both MÃÇ(X) and MÀÜ(X) for predicted masks) and certain definitions are unclear, such as sign(A,v) and the relationship between the score function s(X) and the predicted mask. These ambiguities affect readability and precision.  \n2. **Limited Theory for Transformation Selection** ‚Äì Beyond distance transformations, the paper offers no theoretical guidance on choosing or justifying alternative score transformations, even though other transformations (e.g., smoothing) are applied in experiments. The inclusion of training data in the learning dataset also lacks theoretical justification regarding independence assumptions.  \n3. **Experimental Validation Constraints** ‚Äì Dataset sizes are relatively small, and no direct comparisons to other conformal segmentation methods are included. The reliance on 1000 random splits may not sufficiently capture performance variability.  \n4. **Computational and Scalability Analysis** ‚Äì Algorithmic complexity is not analyzed, runtime data are limited to one downscaled example, and there is no discussion of memory or scalability for high-resolution or 3D images.  \n5. **Implementation Considerations** ‚Äì The paper lacks practical guidance on hyperparameter selection, calibration set sensitivity, and treatment of independence violations due to overlapping datasets.  \n\n**Minor Comments**  \n- Standardize all notation and clearly define each symbol at first appearance.  \n- Clarify the definition of sign(A,v) with examples for boundary cases.  \n- Provide more details in figure captions and ensure consistent terminology across sections.  \n\n**Summary Paragraph**  \nThis study offers a promising and well-motivated method for uncertainty quantification in image segmentation grounded in conformal inference theory. Its theoretical contributions are novel, but the current version would benefit from clearer notation, theoretical extensions beyond distance transformations, and stronger empirical and computational evaluations. Improved practical guidance would also aid reproducibility and adoption.  \n\n**Decision Recommendation**  \nMajor revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces a novel approach for constructing conformal confidence sets for the output of black-box machine learning models used in biomedical image segmentation tasks. The method leverages conformal inference to provide spatial uncertainty guarantees, utilizing transformed logit scores and distance transformations to generate tight inner and outer confidence sets. The authors demonstrate their approach across three distinct datasets‚Äîpolyps colonscopy, brain imaging, and teeth segmentation‚Äîhighlighting the flexibility and applicability of their method. The manuscript is well-written and provides a thorough theoretical foundation, alongside empirical validation.\n\n## Major Comments\n1. Novelty and Positioning: The manuscript offers a compelling contribution to the field of uncertainty quantification in image segmentation by adapting conformal inference techniques to provide spatial uncertainty guarantees. However, the novelty of the approach relative to existing literature on conformal inference for image segmentation is not fully articulated. The authors should more explicitly discuss how their method distinguishes itself from previous work, particularly in terms of the specific contributions of the distance transformations and the optimization of score transformations.\n\n2. Empirical Validation Scope: The empirical validation is conducted on three datasets, which is commendable. However, the datasets are somewhat specialized (polyps, brain imaging, teeth), and the manuscript could benefit from broader validation across a wider range of biomedical imaging tasks. Additionally, while the manuscript provides extensive validation on the datasets used, the authors should consider including more diverse types of segmentation tasks to further substantiate the generalizability of their approach.\n\n3. Reproducibility: The manuscript states that Matlab code will be provided, which is a positive step towards reproducibility. However, the manuscript could benefit from a more detailed description of the experimental setup, including specifics on the preprocessing steps, model training procedures, and parameter settings. Providing a more comprehensive guide on how to reproduce the results would enhance the manuscript's utility and credibility.\n\n4. Comparison with Baselines: While the manuscript compares the proposed method against baselines such as logit scores and bounding box scores, it would be beneficial to include comparisons against more recent and sophisticated methods in the domain of uncertainty quantification for image segmentation. This would provide a clearer understanding of the relative strengths and weaknesses of the proposed approach.\n\n## Minor Comments\n1. Clarity of Figures: Some figures, such as Figure 2, are somewhat cluttered. Simplifying the presentation by showing fewer representative slices with zoomed-in regions could improve readability and clarity.\n\n2. Notation Explanation: The notation introduced in Section 2.1 is dense and could be explained more thoroughly. In particular, the forward operator is denoted inconsistently, which might confuse readers unfamiliar with the terminology.\n\n3. Acronym Definitions: Several acronyms are used without definition (e.g., \"R=4\"). Defining these acronyms would improve the accessibility of the manuscript.\n\n4. Typographical Errors: There are a few typographical errors throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). These should be corrected to maintain the professionalism of the document.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in biomedical imaging: providing reliable uncertainty quantification for the output of deep learning models used in image segmentation. The proposed approach, which adapts conformal inference techniques to provide spatial uncertainty guarantees, is technically sound and potentially impactful. The theoretical foundations are robust, and the empirical validation, although somewhat limited in scope, demonstrates the feasibility and effectiveness of the approach. However, the manuscript could benefit from broader validation and more detailed comparisons with recent methods. Reproducibility is addressed through the provision of code, but the manuscript would benefit from a more detailed description of the experimental setup. Overall, the manuscript meets many of the standards for publication in TMI but would benefit from addressing the identified limitations.\n\n## Decision Recommendation\nMajor revision. The authors should expand their empirical validation to cover a broader range of biomedical imaging tasks, provide more detailed comparisons with recent methods, and enhance the reproducibility of their results through a more comprehensive description of the experimental setup.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a novel method for constructing conformal confidence sets for outputs of black-box machine learning models applied to biomedical image segmentation. The approach leverages conformal inference to generate spatial uncertainty guarantees, using transformed logit scores and distance transformations to produce tight inner and outer confidence regions. The method is evaluated on three datasets‚Äîpolyp colonoscopy, brain imaging, and teeth segmentation‚Äîdemonstrating its flexibility and applicability. The paper is clearly written, grounded in solid theoretical analysis, and complemented by empirical validation.\n\n**Major Comments**  \n1. **Novelty and Positioning**: While the paper advances uncertainty quantification in image segmentation through conformal inference, the novelty relative to existing studies is not fully clarified. The authors should explicitly state how their contributions‚Äîparticularly the role of distance transformations and score transformation optimization‚Äîdiffer from prior works in conformal inference for segmentation.  \n2. **Empirical Validation Scope**: The experiments on three datasets are valuable but relatively narrow in domain specialization. The work would be strengthened by including additional biomedical segmentation tasks to demonstrate broader generalizability and applicability.  \n3. **Reproducibility**: The plan to release Matlab code is commendable. However, reproducibility could be improved by providing more details on the experimental procedures, including preprocessing, training setup, and parameter settings. A more comprehensive description would enhance transparency and replicability.  \n4. **Comparison with Baselines**: Current comparisons focus on simpler baselines such as logit and bounding box scores. Including stronger recent baselines in uncertainty quantification for image segmentation would help contextualize the strengths and limitations of the proposed method.\n\n**Minor Comments**  \n1. **Figures**: Some figures (e.g., Figure 2) appear cluttered. Simplifying visualizations or focusing on representative slices with zoomed-in areas would improve clarity.  \n2. **Notation**: The notation in Section 2.1 is dense and inconsistently applied to the forward operator. Greater consistency and clearer explanations are recommended.  \n3. **Acronyms**: Some acronyms (e.g., ‚ÄúR=4‚Äù) are undefined. Full definitions would improve accessibility for readers.  \n4. **Typos**: Minor typographical errors such as ‚Äúk-spacce‚Äù and ‚Äúundersampling maskes‚Äù should be corrected.\n\n**Summary Paragraph**  \nThis work provides a technically sound and potentially impactful approach to quantifying spatial uncertainty in biomedical image segmentation. The theoretical contributions are strong, and the empirical results substantiate feasibility, though they could be expanded for broader validation. Enhancing comparisons with modern baselines and strengthening reproducibility details would make the contribution more robust. With these improvements, the manuscript would align well with publication standards.\n\n**Decision Recommendation**  \n**Major Revision.** The authors should broaden empirical validation, include comparisons with more recent methods, and increase experimental reproducibility through expanded methodological details.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Conformal CONFIDENCE SETS FOR BIOMEDICAL IMAGE SEGMENTATION\n\n### Summary\n\nThe paper proposes a split-conformal framework to produce inner and outer confidence sets for binary image segmentation that provide image-wise probabilistic guarantees: the inner set contains no false positives and the outer set contains all true positives with user-chosen confidence. The core idea is to calibrate thresholds on the maxima of suitably transformed score maps inside/outside the ground-truth mask; a key practical contribution is to use distance-transformations of the predicted masks to obtain much tighter outer sets, with a theoretical link to the Hausdorff distance. The method is validated on polyp, brain, and teeth segmentation, with extensive coverage calibration on polyps and ablations on score transformations.\n\n### Strengths\n\n- Technical novelty and innovationThe formulation of inner/outer image-wise confidence sets using maxima of transformed score maps is intuitive and general, with clean conformal guarantees under exchangeability.The use of distance transforms tied to the predicted mask to create outer confidence sets is a simple, effective idea; the Hausdorff-distance‚Äìbased result (Theorem 2.8) provides an appealing theoretical rationale linking segmentation accuracy and set tightness.The joint-coverage construction via either Bonferroni splitting or a joint max-statistic is a useful design choice that practitioners can tune for asymmetrical inner/outer priorities.The generalization to any increasing combination function offers a flexible path to adapt beyond the maximum.\n- Experimental rigor and validationFor polyps, the paper performs 1000 random calibration/test splits over a large held-out pool (n=1500) and reports empirical coverage curves spanning a range of confidence levels, which strongly supports nominal validity.The transformation-learning split (learning vs calibration vs test) is clearly motivated to avoid post-selection bias, and the demonstrated gains of distance transforms are consistent across examples.Efficiency analyses (diameter ratios and area-based measures) substantiate the practical benefits of specific transformations for inner vs outer sets.\n- Clarity of presentationThe conceptual distinction between inner, outer, and joint coverage is well explained, including the analogy to FWER vs FDR in multiple testing.The pipeline and the role of the learning dataset for choosing transformations are presented in a way that is actionable for practitioners.The appendices broaden the theoretical scope (increasing combination functions, risk-control view).\n- Significance of contributionsReliable, image-wise uncertainty quantification for segmentation with finite-sample guarantees is important for biomedical deployments where over/under-segmentation has clinical consequences.The proposed scheme is simple to implement, computationally lightweight, and compatible with existing black-box segmenters, which increases its potential impact.\n\n- The formulation of inner/outer image-wise confidence sets using maxima of transformed score maps is intuitive and general, with clean conformal guarantees under exchangeability.\n- The use of distance transforms tied to the predicted mask to create outer confidence sets is a simple, effective idea; the Hausdorff-distance‚Äìbased result (Theorem 2.8) provides an appealing theoretical rationale linking segmentation accuracy and set tightness.\n- The joint-coverage construction via either Bonferroni splitting or a joint max-statistic is a useful design choice that practitioners can tune for asymmetrical inner/outer priorities.\n- The generalization to any increasing combination function offers a flexible path to adapt beyond the maximum.\n\n- For polyps, the paper performs 1000 random calibration/test splits over a large held-out pool (n=1500) and reports empirical coverage curves spanning a range of confidence levels, which strongly supports nominal validity.\n- The transformation-learning split (learning vs calibration vs test) is clearly motivated to avoid post-selection bias, and the demonstrated gains of distance transforms are consistent across examples.\n- Efficiency analyses (diameter ratios and area-based measures) substantiate the practical benefits of specific transformations for inner vs outer sets.\n\n- The conceptual distinction between inner, outer, and joint coverage is well explained, including the analogy to FWER vs FDR in multiple testing.\n- The pipeline and the role of the learning dataset for choosing transformations are presented in a way that is actionable for practitioners.\n- The appendices broaden the theoretical scope (increasing combination functions, risk-control view).\n\n- Reliable, image-wise uncertainty quantification for segmentation with finite-sample guarantees is important for biomedical deployments where over/under-segmentation has clinical consequences.\n- The proposed scheme is simple to implement, computationally lightweight, and compatible with existing black-box segmenters, which increases its potential impact.\n\n### Weaknesses\n\n- Technical limitations or concernsThe maximum-based calibration is intrinsically conservative (single extreme pixel can trigger failure), which can inflate outer sets unless transformations are carefully chosen; while distance transforms help, the paper could more systematically explore alternative aggregators (e.g., top-k max, spatial quantiles).Assumption 2 (independence of s, fI, fO from calibration/test data) is crucial; the text suggests the learning set can include training data, but a clearer, stricter data-separation protocol is needed to ensure no leakage into calibration/test when fine-tuning transformations.Edge cases (empty masks or full-foreground images) are not explicitly handled in the definitions of œÑi and Œ≥i; the maximum over an empty index set should be defined and implemented carefully to avoid invalid scores.\n- Experimental gaps or methodological issuesBaseline comparisons miss several recent conformal segmentation approaches beyond bounding boxes and conformal risk control, including morphological dilation/erosion CP methods that require only binary masks, and spatially structured CP methods; empirical head-to-head comparisons would strengthen claims of efficiency.The brain and teeth applications are less thoroughly quantified than polyps (e.g., fewer coverage curves, fewer ablations/baselines), which makes it harder to generalize the conclusions.Calibration robustness to dataset shift is not assessed; the polyps pool aggregates multiple datasets, but explicit cross-dataset calibration/testing would be informative.\n- Clarity or presentation issuesTheorem/proof snippets contain typographical artifacts from PDF extraction (e.g., corrupted Greek letters and symbols in Theorems 2.1/2.2 proofs); while not fatal, they impede readability and should be corrected.Several implementation details are deferred or implicit (e.g., precise definition of E(A) and distance transforms in 3D, handling of anisotropic spacing), which matter for biomedical imaging.Some figures/tables show placeholder text or artifacts (‚ÄúTABLE AREA‚Äù, blurred numbers); the camera-ready should replace these with finalized figures and numerical tables.\n- Missing related work or comparisonsMinimal discussion and no empirical comparison with: (i) morphological conformal approaches for segmentation that calibrate dilation/erosion radii to guarantee coverage of a fraction of the ground truth (e.g., nested-set CP with morphological operators), (ii) methods controlling inner false positive proportions via conformal calibration of score thresholds/erosions, and (iii) spatially structured or representation-space conformal methods that tighten sets by leveraging correlated uncertainty. Positioning with respect to these is important for a top-tier venue.\n\n- The maximum-based calibration is intrinsically conservative (single extreme pixel can trigger failure), which can inflate outer sets unless transformations are carefully chosen; while distance transforms help, the paper could more systematically explore alternative aggregators (e.g., top-k max, spatial quantiles).\n- Assumption 2 (independence of s, fI, fO from calibration/test data) is crucial; the text suggests the learning set can include training data, but a clearer, stricter data-separation protocol is needed to ensure no leakage into calibration/test when fine-tuning transformations.\n- Edge cases (empty masks or full-foreground images) are not explicitly handled in the definitions of œÑi and Œ≥i; the maximum over an empty index set should be defined and implemented carefully to avoid invalid scores.\n\n- Baseline comparisons miss several recent conformal segmentation approaches beyond bounding boxes and conformal risk control, including morphological dilation/erosion CP methods that require only binary masks, and spatially structured CP methods; empirical head-to-head comparisons would strengthen claims of efficiency.\n- The brain and teeth applications are less thoroughly quantified than polyps (e.g., fewer coverage curves, fewer ablations/baselines), which makes it harder to generalize the conclusions.\n- Calibration robustness to dataset shift is not assessed; the polyps pool aggregates multiple datasets, but explicit cross-dataset calibration/testing would be informative.\n\n- Theorem/proof snippets contain typographical artifacts from PDF extraction (e.g., corrupted Greek letters and symbols in Theorems 2.1/2.2 proofs); while not fatal, they impede readability and should be corrected.\n- Several implementation details are deferred or implicit (e.g., precise definition of E(A) and distance transforms in 3D, handling of anisotropic spacing), which matter for biomedical imaging.\n- Some figures/tables show placeholder text or artifacts (‚ÄúTABLE AREA‚Äù, blurred numbers); the camera-ready should replace these with finalized figures and numerical tables.\n\n- Minimal discussion and no empirical comparison with: (i) morphological conformal approaches for segmentation that calibrate dilation/erosion radii to guarantee coverage of a fraction of the ground truth (e.g., nested-set CP with morphological operators), (ii) methods controlling inner false positive proportions via conformal calibration of score thresholds/erosions, and (iii) spatially structured or representation-space conformal methods that tighten sets by leveraging correlated uncertainty. Positioning with respect to these is important for a top-tier venue.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe exchangeability-based split-conformal construction for inner/outer sets is standard and appears correct; the quantile definitions align with inductive conformal calibration conventions, though the notation should be cleaned up and matched to a precise order-statistic formula.The generalization to increasing combination functions is appropriate and potentially impactful; concrete examples (e.g., top-k maxima, spatially weighted maxima, cluster-wise maxima) would be valuable, along with brief guidance on their effect on power/conservativeness.Theorem 2.8 connecting distance-transformed scores to Hausdorff distance is a meaningful theoretical anchor; however, it assumes an accurate boundary-extraction routine (marching squares/cubes) and an appropriate metric œÅ. Please detail assumptions on grid spacing, isotropy, and how boundary discretization affects the bound in practice.Joint coverage via max(œÑ,Œ≥) is valid; however, ‚Äúpivotality‚Äù concerns raised in Remark 2.7 deserve a quantified treatment (e.g., simple simulation showing when joint-max calibration is inferior to Bonferroni splits and how transformation scaling mitigates imbalance).\n- Experimental evaluation assessmentPolyps: The repeated-split coverage curves and efficiency plots are strong and convincing; thank you for reporting uncertainty bands. Please add numerical summaries (mean coverage ¬± CI at canonical levels 0.8/0.9/0.95) and mean set sizes (inner/outer) to complement the plots.Brain and teeth: Include the same systematic coverage/effectiveness plots as for polyps; add standard segmentation metrics for the base models and correlate them with band tightness (beyond the brief discussion in the appendix).Edge-case analysis: Report behavior on images with empty ground-truth masks or tiny structures (where maxima/quantiles can be unstable). Also show the impact of different metrics œÅ (Euclidean vs chessboard vs physical spacing-aware metrics) on outer-set size.Computational considerations: The stated runtimes are promising; add runtime break-down for distance transform computations and 3D cases, and memory considerations for large volumes.\n- Comparison with related work (using the summaries provided)Conformal risk control (Bates 2021; Angelopoulos et al. 2024): The paper correctly notes that risk-control methods target expected loss (e.g., FN rate) rather than full-set coverage. It would help to include a direct empirical comparison on polyps using a representative risk function (as in the cited works) to quantify the trade-off between tighter outer sets vs stricter guarantees.Morphology-based conformal segmentation (e.g., nested dilation CP; inner-mask FP control via erosion/thresholding): These methods offer black-box applicability (no logits) and sometimes more coherent margins; please discuss and, if possible, include a baseline that calibrates morphological dilation/erosion for outer/inner sets, respectively, on the same datasets. This would help position the proposed distance-transform method relative to black-box CP alternatives.Spatially structured CP (e.g., CONSIGN) and representation-space CP for downstream metrics (e.g., COMPASS): While these target different uncertainty objects (set-valued masks via low-dim subspaces or metric intervals), they address the same broad need‚Äîspatially coherent, efficient uncertainty. A short discussion clarifying scope (full mask coverage vs expected-risk vs metric coverage), assumptions (need for multiple stochastic samples vs single deterministic map), and potential hybridization opportunities would strengthen the context.\n- Discussion of broader impact and significanceThe image-wise guarantee is clinically meaningful (FWER-like protection), especially for critical applications where even one missed or spurious region matters. The distance-transform trick makes this practically viable in settings where score maps are unreliable near boundaries.Practical deployment will hinge on calibration under shift; the current approach assumes exchangeability. Consider adding a brief outline for detecting calibration drift and re-calibrating under shift (e.g., covariate-shift CP, importance-weighted calibration, or lightweight adaptive recalibration strategies).The framework is model-agnostic and adds minimal overhead, making it a good candidate for integration into clinical pipelines. Detailed guidance for choosing fI/fO by task (including smoothing and morphology) would further aid practitioners.\n\n- The exchangeability-based split-conformal construction for inner/outer sets is standard and appears correct; the quantile definitions align with inductive conformal calibration conventions, though the notation should be cleaned up and matched to a precise order-statistic formula.\n- The generalization to increasing combination functions is appropriate and potentially impactful; concrete examples (e.g., top-k maxima, spatially weighted maxima, cluster-wise maxima) would be valuable, along with brief guidance on their effect on power/conservativeness.\n- Theorem 2.8 connecting distance-transformed scores to Hausdorff distance is a meaningful theoretical anchor; however, it assumes an accurate boundary-extraction routine (marching squares/cubes) and an appropriate metric œÅ. Please detail assumptions on grid spacing, isotropy, and how boundary discretization affects the bound in practice.\n- Joint coverage via max(œÑ,Œ≥) is valid; however, ‚Äúpivotality‚Äù concerns raised in Remark 2.7 deserve a quantified treatment (e.g., simple simulation showing when joint-max calibration is inferior to Bonferroni splits and how transformation scaling mitigates imbalance).\n\n- Polyps: The repeated-split coverage curves and efficiency plots are strong and convincing; thank you for reporting uncertainty bands. Please add numerical summaries (mean coverage ¬± CI at canonical levels 0.8/0.9/0.95) and mean set sizes (inner/outer) to complement the plots.\n- Brain and teeth: Include the same systematic coverage/effectiveness plots as for polyps; add standard segmentation metrics for the base models and correlate them with band tightness (beyond the brief discussion in the appendix).\n- Edge-case analysis: Report behavior on images with empty ground-truth masks or tiny structures (where maxima/quantiles can be unstable). Also show the impact of different metrics œÅ (Euclidean vs chessboard vs physical spacing-aware metrics) on outer-set size.\n- Computational considerations: The stated runtimes are promising; add runtime break-down for distance transform computations and 3D cases, and memory considerations for large volumes.\n\n- Conformal risk control (Bates 2021; Angelopoulos et al. 2024): The paper correctly notes that risk-control methods target expected loss (e.g., FN rate) rather than full-set coverage. It would help to include a direct empirical comparison on polyps using a representative risk function (as in the cited works) to quantify the trade-off between tighter outer sets vs stricter guarantees.\n- Morphology-based conformal segmentation (e.g., nested dilation CP; inner-mask FP control via erosion/thresholding): These methods offer black-box applicability (no logits) and sometimes more coherent margins; please discuss and, if possible, include a baseline that calibrates morphological dilation/erosion for outer/inner sets, respectively, on the same datasets. This would help position the proposed distance-transform method relative to black-box CP alternatives.\n- Spatially structured CP (e.g., CONSIGN) and representation-space CP for downstream metrics (e.g., COMPASS): While these target different uncertainty objects (set-valued masks via low-dim subspaces or metric intervals), they address the same broad need‚Äîspatially coherent, efficient uncertainty. A short discussion clarifying scope (full mask coverage vs expected-risk vs metric coverage), assumptions (need for multiple stochastic samples vs single deterministic map), and potential hybridization opportunities would strengthen the context.\n\n- The image-wise guarantee is clinically meaningful (FWER-like protection), especially for critical applications where even one missed or spurious region matters. The distance-transform trick makes this practically viable in settings where score maps are unreliable near boundaries.\n- Practical deployment will hinge on calibration under shift; the current approach assumes exchangeability. Consider adding a brief outline for detecting calibration drift and re-calibrating under shift (e.g., covariate-shift CP, importance-weighted calibration, or lightweight adaptive recalibration strategies).\n- The framework is model-agnostic and adds minimal overhead, making it a good candidate for integration into clinical pipelines. Detailed guidance for choosing fI/fO by task (including smoothing and morphology) would further aid practitioners.\n\n### Questions for Authors\n\n- How do you handle images with empty ground-truth masks (Y ‚â° 0) or masks covering the full image (Y ‚â° 1) when defining œÑi and Œ≥i? What are the exact conventions for maxima over empty index sets, and how do they affect coverage validity?\n- Can you provide a precise, reproducible definition of the boundary operator E(A) for 2D and 3D (marching squares vs marching cubes), and discuss the effect of anisotropic pixel/voxel spacing on the distance transform and Theorem 2.8?\n- How sensitive are the results to the choice of distance metric œÅ and to discretization? Have you tried spacing-aware Euclidean distances in 3D brain datasets, and how do they change outer-set sizes and empirical coverage?\n- Could you include baseline comparisons with morphological CP approaches (calibrated dilation for outer sets, erosion/score-thresholding for inner sets) and with a risk-control outer-set baseline, on the polyps dataset?\n- When learning fI and fO, what is the exact data partitioning strategy to ensure independence from calibration/test? If training and learning sets overlap in images, how do you guard against leakage into calibration/test via fine-tuned transformations?\n- For joint coverage, did you measure empirical joint coverage under the Œ±1 + Œ±2 split versus the joint-max approach, and how did the two compare across datasets?\n- Could you report ablations for alternative combination functions (e.g., top-k maxima or spatial quantiles) to mitigate the conservativeness of the plain max, and comment on any changes to validity/efficiency?\n- In the teeth and brain studies, can you add the same repeated-split coverage curves and efficiency metrics as for polyps to substantiate generality?\n\n### Overall Assessment\n\nThis paper addresses an important need in medical image segmentation: simple, valid, and interpretable uncertainty quantification at the image level. The proposed inner/outer conformal confidence sets are technically sound, easy to implement, and supported by a pragmatic insight‚Äîdistance transforms‚Äîthat materially tightens outer sets, with a supporting Hausdorff distance result. The polyps study is thorough and convincingly demonstrates nominal coverage and efficiency gains from appropriate score transformations. However, for a top-tier venue, the experimental scope and positioning relative to closely related conformal segmentation baselines (morphological CP, conformal risk control, spatially structured CP) should be strengthened with direct comparisons and broader quantification (especially for brain and teeth). Clarifying edge cases, 3D/spacing details, and formalizing the data-splitting protocol will improve reproducibility and perceived rigor. Overall, the work is promising and impactful; with the suggested additions and cleanup, it would be a strong candidate for publication.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a split-conformal framework for constructing inner and outer confidence sets in binary biomedical image segmentation, offering image-wise probabilistic guarantees with finite-sample validity. The method calibrates thresholds on extrema of transformed score maps, using distance-transformed predictions to achieve tighter outer sets with a theoretical connection to the Hausdorff distance. Validation includes several segmentation tasks (polyps, brain, teeth) with extensive calibration experiments and ablation studies. The paper is generally clear, technically sound, and presents a practical and theoretically grounded contribution to uncertainty quantification in segmentation.  \n\n**Major Comments**  \n1. **Technical scope and limitations**: The maximum-based calibration is conservative, potentially inflating outer sets; exploration of alternative aggregators (e.g., top‚Äëk max or spatial quantiles) would improve efficiency. Assumption 2 on data independence requires a clearer definition to avoid data leakage when transformations are tuned. Edge cases such as empty or full masks are not explicitly treated and should be formally defined for validity.  \n2. **Experimental breadth and comparisons**: The evaluation focuses mainly on polyps; the brain and teeth results are less developed. Baselines omit several relevant conformal segmentation approaches, including morphological dilation/erosion and spatially structured CP methods. Adding these comparisons would better establish empirical efficiency. Data‚Äëshift robustness is also untested; explicit cross‚Äëdataset calibration would strengthen applicability.  \n3. **Notation and implementation clarity**: Some theorem proofs include corrupted symbols, and several implementation steps (e.g., the boundary operator \\(E(A)\\), distance transforms in 3D, anisotropic spacing) require precise specification.  \n4. **Positioning in related work**: The paper should more clearly contrast its method with morphological conformal approaches, risk‚Äëcontrol conformal prediction, and spatially structured or representation‚Äëspace variants, emphasizing distinctive assumptions and objectives.  \n5. **Reproducibility and theoretical detail**: Clarify the calibration protocol, boundary discretization assumptions, and the effect of grid resolution on Theorem‚ÄØ2.8. Quantitative investigation of the joint‚Äëcoverage strategy (Bonferroni vs joint‚Äëmax) would illuminate trade‚Äëoffs.  \n\n**Minor Comments**  \n- Correct typographic artifacts in theorems and replace placeholder or blurred figures/tables.  \n- Add numerical summaries of coverage and set sizes at standard confidence levels.  \n- Provide computational breakdowns for distance transforms, especially in 3D.  \n- Discuss behavior on images with small or empty foregrounds and specify how maxima over empty sets are handled.  \n\n**Summary Paragraph**  \nThe work offers a principled and efficient approach to conformal uncertainty estimation for segmentation, combining theoretical soundness with convincing empirical calibration‚Äîparticularly for polyp data. Its strengths lie in technical novelty, clear presentation, and potential clinical relevance. The main weaknesses are incomplete comparative analysis, limited generalization beyond one dataset, and some missing implementation clarifications. Addressing these issues would improve reproducibility and contextual positioning while preserving the method‚Äôs practical appeal.  \n\n**Decision Recommendation**  \n**Major revision** ‚Äì technically solid and promising but requires stronger comparative evaluation, clarification of edge cases and implementation details, and improved discussion of related approaches before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical challenge of providing formal uncertainty quantification for biomedical image segmentation models, which is essential for safe clinical deployment of AI systems in medical imaging. The authors develop a novel approach using conformal inference to generate spatial confidence sets with probabilistic guarantees for segmentation outputs. Their key innovation lies in adapting conformal methods to handle the spatial dependencies in images by using score transformations (particularly distance transformations) of neural network outputs to create tight inner and outer confidence sets. Through rigorous theoretical analysis and extensive empirical validation across three diverse medical imaging domains (polyps colonoscopy, brain MRI, and dental X-ray segmentation), the authors demonstrate that their approach provides meaningful spatial uncertainty bounds while strictly controlling false coverage rates. The results show that distance-transformed scores significantly outperform raw logit scores, yielding confidence sets that are both theoretically valid and practically useful for identifying regions where segmentation predictions are reliable.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n1. **Limited comparison to alternative uncertainty quantification methods**: While the paper effectively compares to bounding box approaches (e.g., And√©ol et al., 2023), it lacks direct comparison to other state-of-the-art uncertainty quantification methods for medical segmentation such as Monte Carlo dropout, deep ensembles, or Bayesian approaches. Including such comparisons would better position the method within the broader uncertainty quantification landscape and demonstrate its relative advantages.\n\n2. **Practical clinical implications could be strengthened**: The paper would benefit from more concrete examples of how these confidence sets would inform clinical decision-making. Specifically, how would practitioners interpret and act upon the inner/outer confidence sets in real clinical workflows? What is the added value compared to standard segmentation outputs with simple confidence thresholds?\n\n3. **Computational considerations for 3D volumes**: The distance transformation step, while effective, may present computational challenges for large 3D medical volumes. The paper should discuss the computational complexity of the approach, especially for 3D segmentation tasks, and potentially suggest optimizations for clinical deployment.\n\n### Minor Comments\n\n1. **Clarification of learning dataset requirements**: The paper mentions setting aside a learning dataset for score transformation optimization but doesn't clearly specify the minimum size requirements or how performance varies with learning dataset size. Adding this information would help practitioners implement the method in resource-constrained settings.\n\n2. **Visualization consistency**: Some figures (e.g., Figure A16) could benefit from more consistent labeling and clearer distinction between different score transformation methods to facilitate direct comparison.\n\n3. **Algorithm implementation details**: While Algorithm 1 provides a good overview, more specific implementation details (e.g., exact parameter choices for the distance transformation, computational time for different image sizes) would enhance reproducibility.\n\n4. **Discussion of model performance dependency**: The paper notes that confidence set precision correlates with segmentation model quality (Section 6), but could elaborate more on how the approach performs when segmentation quality is poor, which is common in challenging clinical cases.\n\n## 3. Summary Evaluation\n\n**Significance**: This work addresses a critical gap in medical AI - the lack of formal uncertainty guarantees for segmentation outputs. Providing spatial confidence sets with probabilistic coverage is highly significant for clinical applications where segmentation errors can have serious consequences. The approach is applicable across multiple medical imaging domains, enhancing its potential impact on clinical practice.\n\n**Innovation**: The paper makes a substantial theoretical contribution by adapting conformal inference to provide spatial confidence sets for image segmentation, moving beyond pixel-wise confidence measures. The use of distance transformations to optimize confidence set tightness is particularly innovative, and the formal connection between spatial confidence sets and multiple testing error rates provides valuable conceptual framing.\n\n**Evaluation**: The authors provide thorough empirical validation across three diverse medical imaging datasets, with extensive visualizations and quantitative metrics. The coverage rate validation with 1000 resamples is particularly strong. However, the evaluation would be strengthened by direct comparisons to alternative uncertainty quantification methods beyond bounding box approaches.\n\n**Reproducibility**: The paper provides clear theoretical foundations and algorithmic descriptions, with supplementary code available for implementation. The detailed appendices and comprehensive visualization of results significantly enhance reproducibility. Including more specific implementation parameters and computational requirements would further strengthen this aspect.\n\n## 4. Decision Recommendation\n\nMinor Revision\n\nThe paper presents a significant and well-executed contribution to uncertainty quantification in medical image segmentation with strong theoretical foundations and thorough empirical validation. The approach addresses a critical need in clinical AI deployment by providing spatial confidence sets with formal guarantees. While the paper is fundamentally sound and makes a valuable contribution to the field, the major comments regarding comparison to alternative uncertainty quantification methods and clarification of practical clinical implications should be addressed to strengthen its impact and utility for the medical imaging community. The minor comments are relatively straightforward to address and would further improve the paper's clarity and reproducibility. Overall, this work has high potential to influence both research and clinical practice in medical AI if these revisions are incorporated.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of formal uncertainty quantification in biomedical image segmentation, a crucial requirement for clinically reliable AI systems. It introduces a novel conformal inference‚Äìbased framework that generates spatial confidence sets with probabilistic coverage guarantees. The approach adapts conformal methods to spatial data by applying distance transformations to neural network outputs, thereby producing inner and outer confidence sets that control false coverage rates. The work is supported by rigorous theoretical analysis and extensive experiments across multiple medical imaging domains (polyp detection in colonoscopy, brain MRI, and dental X-ray segmentation). The findings show that distance-transformed scores yield tighter and more interpretable confidence sets than alternative scoring schemes, offering both theoretical validity and practical utility.\n\n---\n\n**Major Comments**  \n1. **Comparison to alternative uncertainty methods:** The paper compares its approach primarily to bounding box‚Äìbased methods but omits direct comparisons with other uncertainty quantification techniques such as Monte Carlo dropout, deep ensembles, or Bayesian models. Including these would better establish the method‚Äôs relative advantages and contextualize its contribution.  \n2. **Clinical interpretation and use:** The manuscript would benefit from a clearer discussion of how clinicians might use these confidence sets in practice and what specific decisions they could inform beyond typical segmentation confidence thresholds.  \n3. **Computational cost for 3D volumes:** The distance transformation step may become demanding for large 3D datasets. A discussion of computational complexity and potential optimization strategies would clarify feasibility in clinical workflows.\n\n---\n\n**Minor Comments**  \n1. Clarify the minimum recommended size of the learning dataset used for score transformation optimization, and note how performance scales with its size.  \n2. Improve consistency and labeling in visualizations (e.g., Figure A16) to ease cross-method comparison.  \n3. Provide more details on algorithm implementation, including parameter choices and computational times for different image sizes, to enhance reproducibility.  \n4. Elaborate briefly on how performance degrades when base segmentation models yield poor outputs, as this is common in difficult clinical cases.\n\n---\n\n**Summary Paragraph**  \nThe paper offers a significant theoretical and practical advance in providing formal spatial confidence guarantees for segmentation models, an important step toward trustworthy deployment in clinical settings. Its innovative use of distance-transformed conformal scores is well motivated and validated through comprehensive empirical studies. While the study demonstrates strong coverage performance and reproducibility, its impact would be strengthened by broader comparisons to existing uncertainty quantification methods, clearer articulation of clinical benefits, and more detailed computational analysis.\n\n---\n\n**Decision Recommendation**  \n**Minor Revision** ‚Äî The work is original, rigorous, and potentially influential. Addressing the major comments regarding comparative evaluation and clinical applicability, along with the minor clarifications, would consolidate its contribution and value to the medical imaging community.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles an important gap in biomedical image segmentation: the absence of formal uncertainty quantification for deep‚Äëlearning models, which can produce overconfident predictions that jeopardize clinical decisions. The authors assess their approach on three distinct domains‚Äîcolonoscopy polyp images, brain‚ÄëMRI skull‚Äëstripping, and dental radiographs‚Äîusing pretrained segmentation networks that output per‚Äëpixel logits. Their method relies on split‚Äëconformal inference to generate inner and outer confidence sets. Thresholds are obtained from the distribution of the maximum transformed scores inside and outside the ground‚Äëtruth masks on a calibration set. Score transformations are learned on a separate dataset; the identity transformation is used for the inner set, while a distance‚Äëtransform is applied to the outer set to improve tightness. Experiments show that the constructed confidence sets achieve the nominal coverage probabilities on held‚Äëout test data, demonstrating that the technique can provide probabilistic spatial guarantees for segmentation outputs.\n\n---\n\n## General feedback  \n\n- **Significance:**  Offering distribution‚Äëfree, spatially resolved uncertainty guarantees for black‚Äëbox segmenters addresses a pressing need in medical imaging, where unchecked confidence can lead to harmful clinical outcomes.  \n- **Innovation:**  The use of the **maximum of transformed logits** for split‚Äëconformal calibration, together with an explicit **distance‚Äëtransform score** for tighter outer sets, represents a novel departure from earlier pixel‚Äëwise p‚Äëvalue or FDR‚Äëbased methods.  \n- **Evaluation:**  The authors present experiments on three datasets and show that the nominal coverage levels are met (see Figures‚ÄØ4 and‚ÄØA16).  Nevertheless, the paper does not include quantitative comparisons with existing conformal segmentation baselines or other uncertainty‚Äëestimation techniques, and there is no statistical testing of the reported improvements.  \n- **Reproducibility:**  While MATLAB code and a demonstration are mentioned in the supplement, crucial details‚Äîsuch as precise dataset versions, random‚Äëseed handling, hyper‚Äëparameters for the distance transform, and the exact protocol for constructing the learning set‚Äîare missing, making it difficult for others to replicate the work.\n\n---\n\n## Specific comments / critiques  \n\n- **Assumption‚ÄØ1 (exchangeability) not justified:**  The coverage guarantee hinges on exchangeability of the image‚Äëpair sequence, yet the manuscript does not discuss possible violations (e.g., subject‚Äëlevel correlation, scanner heterogeneity) or their effect on the theoretical guarantees (Section‚ÄØ2.1).  \n- **Baseline comparisons absent:**  No results are shown against state‚Äëof‚Äëthe‚Äëart conformal segmentation approaches (e.g., Angelopoulos‚ÄØet‚ÄØal., 2021; Mossina‚ÄØet‚ÄØal., 2024) or more conventional uncertainty methods (Monte‚ÄëCarlo dropout, deep ensembles).  Consequently, the relative advantage of the proposed confidence sets remains unclear.  \n- **Coverage evaluation lacks statistical rigor:**  Figures‚ÄØ4 and‚ÄØA16 report mean coverage over 1‚ÄØ000 resamples but omit confidence intervals or hypothesis tests, so the variability and significance of differences between score transformations cannot be assessed.  \n- **Efficiency metrics are under‚Äëreported:**  Tightness is illustrated through diameter ratios (Figure‚ÄØ5) without accompanying statistical summaries (e.g., mean‚ÄØ¬±‚ÄØstandard deviation) or clinically meaningful measures such as Hausdorff distance or Intersection‚Äëover‚ÄëUnion between the confidence sets and the ground truth.  \n- **Score‚Äëtransformation learning procedure undefined:**  Section‚ÄØ3.1 references a ‚Äúlearning dataset‚Äù used to select the best transformation, yet provides no algorithmic description (cross‚Äëvalidation scheme, loss function, optimization details), hindering reproducibility of the transformation selection step.  \n- **Reproducibility gaps:**  Exact split proportions (e.g., ‚Äú1798 polyp images ‚Üí 298 learning, 1000 calibration, 500 test‚Äù), random‚Äëseed strategy, hardware specifications, and software versions are omitted; the supplemental code lacks documentation of these essential details.  \n- **Computational cost claim unsupported:**  The statement that calibrating thresholds takes‚ÄØ0.03‚ÄØs on a downscaled polyp set (Section‚ÄØ6) does not specify the hardware used, the data dimensionality, or how the method scales to full‚Äëresolution 3D volumes.  \n- **Typographical and notation issues:**  The manuscript contains numerous formatting glitches (e.g., ‚Äú_il‚Äë_ _lustrate_‚Äù, ‚Äú_c on‚Äë sequences‚Äù) and inconsistent symbols (œÑ·µ¢,‚ÄØŒ≥·µ¢,‚ÄØf_I,‚ÄØf_O,‚ÄØŒª_I) that impede readability and suggest insufficient proofreading.  \n- **No analysis of failure cases or clinical integration:**  The paper does not present examples where the confidence sets miss the ground‚Äëtruth mask or become excessively large, nor does it discuss concrete clinical workflows or user‚Äëstudy evaluations that would substantiate the claimed practical impact.\n\n---\n\n## A suggested decision  \n\n**Reject**  \n\nThe manuscript makes an interesting contribution, but the lack of baseline comparisons, absence of statistical analysis, and several reproducibility shortcomings prevent the claims from being adequately supported.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses an important need in biomedical image segmentation: the lack of formal uncertainty quantification for deep‚Äëlearning models, which can lead to overconfident and potentially unsafe clinical predictions. The authors propose a split‚Äëconformal approach to generate inner and outer confidence sets around segmentation outputs, using transformed per‚Äëpixel logits from pretrained networks. Experiments on three tasks‚Äîcolonoscopy polyp segmentation, brain‚ÄëMRI skull‚Äëstripping, and dental‚Äëradiograph segmentation‚Äîdemonstrate that the proposed method attains nominal coverage on held‚Äëout data. Overall, the paper presents an appealing concept with promising results, but its empirical and methodological support remains incomplete.  \n\n**Major Comments**  \n1. **Novelty and contribution** ‚Äì The integration of split‚Äëconformal calibration with distance‚Äëtransformed logits is conceptually innovative compared with pixel‚Äëwise or FDR‚Äëbased calibration methods.  \n2. **Exchangeability assumption** ‚Äì The theoretical guarantees depend on data exchangeability, but the manuscript does not discuss possible violations (e.g., inter‚Äësubject correlation, scanner variability) or their implications (Section‚ÄØ2.1).  \n3. **Lack of baseline comparisons** ‚Äì No quantitative results are provided against existing conformal segmentation methods (e.g., Angelopoulos‚ÄØet‚ÄØal.,‚ÄØ2021; Mossina‚ÄØet‚ÄØal.,‚ÄØ2024) or established uncertainty approaches such as Monte‚ÄëCarlo dropout or deep ensembles, leaving the relative advantage unclear.  \n4. **Statistical analysis of coverage** ‚Äì Mean coverage values are shown without confidence intervals or tests of significance, limiting the rigor of claims about calibration quality.  \n5. **Evaluation metrics** ‚Äì Tightness is illustrated only by diameter ratios; more complete measures such as mean‚ÄØ¬±‚ÄØstandard deviation, Hausdorff distance, or Intersection‚Äëover‚ÄëUnion would better quantify efficiency and clinical relevance.  \n6. **Reproducibility issues** ‚Äì Key implementation details are missing, including dataset splits, random‚Äëseed policies, hyper‚Äëparameters for distance transforms, and the algorithmic procedure for learning score transformations. The supplementary code lacks documentation of these aspects.  \n7. **Computational efficiency** ‚Äì The reported 0.03‚ÄØs calibration time lacks hardware context and scaling analysis for larger 3D data, making performance claims uncertain.  \n8. **Practical validation** ‚Äì No failure‚Äëcase analysis or clinical‚Äëworkflow demonstration is provided to substantiate the real‚Äëworld applicability of the proposed confidence sets.  \n\n**Minor Comments**  \n- Numerous typographical and formatting inconsistencies occur (e.g., ‚Äú_il‚Äë_ _lustrate_‚Äù, ‚Äú_c‚ÄØon‚Äësequences_‚Äù) and inconsistent notation (œÑ·µ¢,‚ÄØŒ≥·µ¢,‚ÄØf_I,‚ÄØf_O,‚ÄØŒª_I). These should be corrected for clarity and professionalism.  \n\n**Summary Paragraph**  \nThe study introduces a conceptually appealing framework for distribution‚Äëfree, spatially coherent uncertainty quantification in segmentation, and it demonstrates that nominal coverage can be achieved on test data. However, the absence of baseline comparisons, rigorous statistical evaluation, and detailed reproducibility information undermines confidence in the empirical claims. Clarifying theoretical assumptions, expanding evaluation metrics, and improving documentation would significantly strengthen the manuscript.  \n\n**Decision Recommendation**  \n**Reject** ‚Äì The work presents interesting ideas but lacks sufficient experimental validation, statistical analysis, and reproducibility to support acceptance at this stage.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Samuel Davenport"
    ],
    "url": "pdfs/iclr.cc-2025-conference_1bdddb81fd08be607c052dee4379dfe48532b252.pdf",
    "remote_url": "https://openreview.net/pdf/1bdddb81fd08be607c052dee4379dfe48532b252.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Is classification all you need for radiology report generation?",
    "status": "completed",
    "evaluators": [
      "Luping",
      "Yixuan"
    ],
    "primary_area": [
      "other topics in machine learning (i.e., none of the above)"
    ],
    "keywords": [
      "radiology report generation",
      "large language model",
      "multi-modalities"
    ],
    "abstract": "Automatic radiology report generation is an advanced medical assistive technology capable of producing coherent reports based on medical images, akin to a radiologist. However, current generative methods exhibit a notable gap in clinical metrics when compared to medical image classification. Recently, leveraging diagnostic results to improve report quality has emerged as a promising approach. We are curious whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. To investigate this question, this study designs an evaluation framework that integrates long-tail scenarios and summarizes potential combinations of LLM-based report generation models. We assess the impact of classification on report quality across four benchmarks. Initially, we introduce LLM-based language and clinical metrics and develop a pipeline to evaluate the model's performance on both in-domain and out-of-distribution (OOD) long-tail scenarios. Subsequently, we conduct a systematic evaluation of all potential model combinations. Our findings reveal that: 1) the impact of classification on report quality is positively correlated with the performance of classifiers, but the gap still exists, and 2) while classification can enhance report quality in in-domain long-tail scenarios, its benefits for OOD scenarios are limited.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nCurrent RRG systems exhibit a notable gap in clinical metrics when compared to classification models. The authors investigate whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. Key findings show that classification helps improve report quality in in-distribution settings but exhibits limited benefits in OOD scenarios.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper evaluates whether classification models can help improve the quality of radiology reports. This is a novel and interesting insight, with potential for aiding in real-world clinical workflows.\n2. The authors perform extensive evaluations across different strategies for incorporating classification information as well as in-domain vs. OOD settings. \n3. The authors also introduce novel LLM-based metrics for assessing the quality of radiology reports, which evaluate how well reports capture long-tail observations. This metric is an interesting contribution and has potential for aiding future works in the domain of radiology report generation.\n\n### Weaknesses\n\nThe key weakness of this paper is inadequate evaluations, as discussed below.\n\n1. **Inadequate evaluations of classifier:** The incorporation of the classifier is inadequately evaluated, making it difficult to understand the settings in which incorporating a classifier is useful, as detailed below. The paper could have benefitted from more nuanced insights on when including the classifier helps vs. detracts from the quality of generated reports.\n\n    a. **Classifier Performance:** Although the incorporation of the classifier is the key contribution of this paper, the authors do not provide any evaluations on the quality of the actual classifier. What conditions does the classifier perform well on and which conditions does the classifier perform worse on? Does this correlate with report generation performance on the subset of images with particular conditions when the classifier is included? Does improving the performance of the classifier improve report generation quality? In OOD settings, the evaluated classifier displays poor performance  (12.0-14.0 F1 points), and as a result, it is not surprising that incorporating the classifier will not result in benefits to the RRG model in OOD settings; does this result change if the classifier is instead trained on the PadChest or IU X-ray datasets? All of these questions are critical for understanding when/how classification helps in RRG, but none of these are evaluated.\n\n    b. **Upper Bound:** It would be useful to establish an upper bound on performance of the report generation model by utilizing an ‚Äúoracle‚Äù classifier (i.e. a hypothetical classifier that predicts every condition correctly). This would establish whether classification, in the optimal setting, makes useful contributions to the report generation task. As of now, the evaluations are limited by the poor performance of the evaluated classifier, and the key findings/takeaways from the paper are based on this specific classifier.\n\n2. **Inadequate evaluations of proposed metrics:** Although several new metrics are presented as key contributions of this paper, the quality of these metrics is not evaluated. How accurate is the OpenAI GPT-4o model at extracting disease categories? Did the authors check for false negatives / positives in this extraction procedure? Does this metric align with ground-truth labels for the conditions where labels are provided? Evaluating the quality of the metrics via a dataset like ReXVal would have been useful [1].\n\n[1] https://physionet.org/content/rexval-dataset/1.0.0/\n\n### Questions\n\nMy questions are detailed above in the weaknesses section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether integrating classification models‚Äîcovering both common and rare diseases‚Äîcan improve the robustness and quality of radiology report generation (RRG) systems. The study finds that while classification-based approaches enhance report quality for in-distribution data, they provide limited benefits in out-of-distribution (OOD) scenarios. The paper presents extensive evaluations across different classifier integration strategies and introduces novel large language model (LLM)-based metrics for assessing report quality, particularly in terms of capturing long-tail disease observations. Overall, the manuscript is clear and addresses an important question in clinical image analysis, though several aspects of the evaluation require strengthening.  \n\n**Major Comments**  \n1. **Evaluation of Classifier Integration:** The investigation into classifier incorporation is insufficient. The paper does not explore under what conditions including a classifier improves or diminishes RRG performance. More granular analysis of when classification aids report quality would significantly improve interpretability.  \n2. **Classifier Performance Details:** Despite the classifier being central to the study‚Äôs contribution, there is no detailed assessment of its own performance. The results lack clarity regarding which conditions the classifier handles well or poorly and whether these trends correlate with report generation quality. The classifier performs poorly in OOD settings (12‚Äì14 F1 points), making it unsurprising that no gains are observed in those cases. The study would benefit from experiments using classifiers trained on alternative datasets (e.g., PadChest, IU X-ray) to assess generalizability and the relationship between classifier accuracy and report quality.  \n3. **Need for an Upper Bound:** Introducing an ‚Äúoracle‚Äù classifier that predicts all conditions accurately could establish a meaningful upper bound for RRG performance, clarifying whether classification, in principle, can enhance generation quality beyond current limitations.  \n4. **Evaluation of Proposed Metrics:** The new LLM-derived metrics are proposed as key contributions but are not validated. The authors should assess how accurately GPT-4o extracts disease categories, check for false positives/negatives, and compare with ground truth where available‚Äîpotentially using datasets such as ReXVal‚Äîto determine metric reliability.  \n\n**Minor Comments**  \n- Clarify whether the LLM-based metrics correlate with human or clinical evaluations.  \n- Some references to datasets and model names could be more explicitly linked to experimental setups.  \n\n**Summary Paragraph**  \nThis work addresses an interesting and relevant question about leveraging classification for more robust radiology report generation. Its strengths include novelty, thoughtful motivation, and the introduction of innovative LLM-based evaluation metrics. However, the study‚Äôs impact is limited by insufficient validation of both the classifier and the proposed metrics. A more comprehensive evaluation‚Äîparticularly analyzing classifier performance, exploring alternate datasets, and validating the new metrics‚Äîwould substantially strengthen the conclusions.  \n\n**Decision Recommendation**  \n**Major Revision.** The submission presents promising ideas but requires stronger empirical validation and methodological clarity before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper explores the use of classification models in enhancing radiology report generation, specifically examining if a classifier encompassing all possible radiological findings can improve the robustness of generated reports. The study finds that while classification models improve report quality in in-domain scenarios, their impact is limited in out-of-distribution (OOD) settings, especially for long-tail disease categories. The authors propose a comprehensive evaluation framework with novel LLM-based metrics and conduct experiments across multiple benchmarks, concluding that classification aids in in-domain accuracy but may introduce errors in OOD cases due to model biases.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\nThe study tackles a critical challenge in medical AI‚Äîhow to generate reliable and clinically relevant radiology reports that generalize well across diverse clinical scenarios. By showing that classification models can enhance in-domain performance while introducing potential errors in OOD settings, this paper offers valuable insights into the limitations and trade-offs of using classification-aided generation models. The paper is well-structured, with each section building logically upon the previous one. The problem formulation, methodology, and experimental setup are clearly articulated, enhancing accessibility for readers. The experimental design is thorough, incorporating multiple benchmarks and a diverse set of evaluation metrics, including novel LLM-based metrics, to assess model performance across both in-domain and OOD settings. The rigorous use of benchmarks and detailed comparisons with existing methods underscore the study's robustness and the reliability of its findings.\n\n### Weaknesses\n\nThe primary weakness of this work lies in its motivation, or key argument, which has already been discussed in a prior publication that is not referenced in this submission. The previous study, Medical Report Generation Is A Multi-label Classification Problem (Fan et al., IEEE MedAI 2024), introduces a similar concept: that classification accuracy significantly impacts the quality of medical report generation. Both works share the same foundational idea, though they differ in specific model designs and classification categories. From a performance perspective, the previous work, which uses a cross-attention approach, demonstrates better outcomes and introduces the concept of ceiling performance. This concept suggests that the theoretical upper bound for this approach would be achieved if all ground-truth labels were provided to the model, given that real-world classifiers cannot reach 100% accuracy. \n\nWhile this submission‚Äôs strength lies in its extensive experiments, broader metrics, and a variety of models, its core insights are similar to the prior work, without addressing ceiling performance or providing a comparison to it. Additionally, the reason why the 28 classification categories used in the previous study outperform disease-based categories is not examined here. These points, along with further experimental and discussion-based additions, would significantly enhance the paper.\n\n### Questions\n\nplease read the weakness part.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the role of classification models in enhancing automatic radiology report generation. Specifically, it examines whether introducing a classifier covering a full range of radiological findings improves the robustness and quality of generated reports. The study introduces an evaluation framework incorporating novel LLM-based metrics and tests multiple models across several benchmarks. Results indicate that classification models improve in-domain accuracy but have limited benefits‚Äîand sometimes negative effects‚Äîin out-of-distribution (OOD) contexts, particularly for rare disease categories. Overall, the paper is clearly written, methodologically sound, and well organized, presenting its rationale, experimental design, and results in a coherent manner.\n\n**Major Comments**  \n1. **Novelty and Relation to Prior Work**: The central motivation and conceptual contribution are closely aligned with an existing study, *Medical Report Generation Is a Multi-label Classification Problem* (Fan et al., IEEE MedAI 2024). That prior work already articulated the key idea that classification accuracy influences the quality of generated medical reports. Although the current paper differs in its choice of model architectures and classification taxonomies, its fundamental premise is not sufficiently distinct. The absence of explicit discussion or citation of the earlier study weakens the claim of novelty.  \n2. **Evaluation Scope**: The previous work introduced the concept of ceiling performance, representing an upper bound achievable if ground truth labels were perfectly known. This important consideration is not addressed here, leaving unclear how far current results are from this theoretical upper limit.  \n3. **Category Design**: The reason for differing outcomes between disease-based classification categories (used here) and the 28-class label scheme (used in the prior study) is not investigated. A comparative or analytical discussion of this difference would clarify design trade-offs and contribute to a more robust understanding of classification effects.\n\n**Minor Comments**  \n- Overall presentation and logical flow are strong.  \n- Experimental coverage is comprehensive, with clear methodological descriptions and diverse evaluation metrics.  \n- No ethical concerns are noted.  \n\n**Summary Paragraph**  \nThis is a technically competent and clearly presented study addressing an important problem in medical AI. Its multi-benchmark evaluation and metric design are noteworthy strengths. However, the paper‚Äôs conceptual contribution overlaps substantially with prior work, lacking discussion of ceiling performance and omitting analysis comparing disease-based and category-based classification schemes. Addressing these issues would enhance the paper‚Äôs originality and contextual positioning within existing literature.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The study is solidly executed but requires stronger differentiation from prior work and additional analysis to justify its conceptual contribution.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper studies the question: will a classifier enhance LLM-based report generation models' performance in long-tail OOD scenarios? The authors explore different architectures on the combination of vision encoders, classifiers, and LLM, and perform lots of experiments. They get the findings that classification can enhance report quality in in-domain long-tail scenarios but is bounded by the performance of the classifier.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 3\n\n### Strengths\n\n1. Many research focuses on  LLM-based report generation models, which is an important task for the medical AI domain, the question this paper investigated may provide some useful information for future research.  \n2. The research comprehensively evaluates different architectural combinations (vision encoders, classifiers, and LLMs) across multiple diverse datasets (MIMIC-CXR, CXR-LT, PadChest, and IU X-Ray).\n\n### Weaknesses\n\n1. The comparison between the four baseline models lacks analysis. The metrics give inconsistent results, how could the author get the results that C+V+LLM is better than V+LLM, and the comparison between Refining and C+V+LLM, giving statistical significance analysis of the results would be helpful.\n\n2. The poor performance of the 'Expanding' approach may be due to the prompt rather than inherent limitations.  I do not find details about the prompts used (apologize if I overlooked them). The 'Expanding' prompt should include standard medical report templates to prove a fair comparison.\n\n3. The classifier only provides binary disease presence information without crucial details like location and severity. This limitation makes hallucinations in LLM-generated reports inevitable when using classification-only inputs. Instead of comparing with these inherently limited training-free approaches, the paper would be more valuable if it explored different variants of end-to-end Vision + LLM full training. \n\n4. The paper's title 'Is classification all you need for radiology report generation?' raises a clear question, but fails to provide the answer. While the findings show that classification helps with in-domain cases but struggles with OOD scenarios, the paper leaves crucial questions unanswered: Is better classification the solution? Or should we focus on pursuing end-to-end training approaches? If the answer is ' we need better classification models', then everyone knows it.\n\n### Questions\n\n1.  Have you optimized the Expanding and Refining prompt? we can't tell whether the performance difference is due to this method not being good enough or the prompt being the problem.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nN/A",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 3,
          "source": "ai",
          "comment": "The main comments focused primarily on textual analysis, and the discussions within the textual analysis were general, resulting in a bias in the overall review's performance regarding specificity."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether incorporating a classifier can improve large language model (LLM)-based radiology report generation, particularly in long-tail and out-of-distribution (OOD) scenarios. The authors test different combinations of vision encoders, classifiers, and LLMs across multiple datasets (MIMIC-CXR, CXR-LT, PadChest, and IU X-Ray). The study is clearly framed around an important question for the medical AI community and presents extensive experimental evaluations. While the paper‚Äôs motivation is relevant and the experiments are comprehensive, several issues limit the interpretability and conclusiveness of the findings.  \n\n**Major Comments**  \n1. **Comparison and Statistical Analysis** ‚Äì The comparisons among the four baseline models are insufficiently analyzed. Reported metrics yield inconsistent conclusions, making it unclear how the authors determine that C+V+LLM outperforms V+LLM. A statistical significance analysis of the results would strengthen the claims, particularly in comparing the Refining and C+V+LLM settings.  \n2. **Prompt Design and Fairness** ‚Äì The poor performance of the ‚ÄúExpanding‚Äù approach may stem from suboptimal prompt design rather than inherent model limitations. The paper lacks details about the prompts used. Including standardized medical report templates within the prompts could create a fairer comparison.  \n3. **Classifier Limitations and Model Design** ‚Äì The classifier provides only binary disease presence information, omitting critical localization and severity cues. This restriction likely induces hallucinations in generated reports and undermines the classifier‚Äôs utility. The paper would benefit from exploring end-to-end Vision + LLM training rather than relying solely on constrained classification inputs.  \n4. **Unresolved Research Question** ‚Äì The title poses an explicit question‚Äî‚ÄúIs classification all you need for radiology report generation?‚Äù‚Äîbut the conclusion remains ambiguous. The study shows classification aids in-domain performance but not OOD robustness. The paper does not address whether improved classifiers are sufficient or whether fundamentally different training paradigms are required.  \n\n**Minor Comments**  \n- Clarify whether the Expanding and Refining prompts were optimized; current results cannot distinguish method inadequacy from prompt effects.  \n- Minor wording and structural edits could improve clarity, especially in describing architectures and datasets.  \n\n**Summary Paragraph**  \nThe study explores an important topic and employs multiple datasets and model configurations, providing a valuable empirical viewpoint. However, unclear comparative analysis, insufficient description of prompts, and limited classifier expressiveness constrain interpretability. The work‚Äôs overall contribution is informative but inconclusive regarding the central research question.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** ‚Äì  \n   ‚úÖ The paper fits the methodological scope of *IEEE Transactions on Medical Imaging*. It introduces new benchmark design, evaluation metrics, and architectural design space analysis for *automatic radiology report generation (ARRG)* ‚Äî a problem at the intersection of multimodal learning and medical imaging. Although it couples vision-language modeling with NLP, the study is anchored in imaging methodology (classifier‚Äìencoder‚ÄìLLM integration, long-tail robustness, and clinically oriented metrics), making it relevant to TMI‚Äôs focus on imaging informatics and quantitative analysis.\n\n2. **Novelty & Contribution Level** ‚Äì  \n   The novelty lies primarily in an **evaluation and analysis framework** rather than a fundamentally new model. Contributions include:  \n   - An **LLM-based evaluation framework** combining traditional and ‚Äúclinical‚Äù metrics with LLM-derived metrics for nuanced assessment.  \n   - A **systematic exploration of model design space** (vision encoder, classifier, LLM) and training paradigms (end-to-end, refining, expanding).  \n   - Empirical findings on the limited benefit of classifiers for out-of-distribution long-tail radiology data.  \n   While conceptually interesting, much of the architecture and methodology (Swin, DINO, Vicuna, Llama-based modules) build on existing components. Hence, the innovation is more **empirical and benchmarking-oriented** than algorithmic.\n\n3. **Technical and Experimental Rigor** ‚Äì  \n   - The experiments are thorough: multiple datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray), carefully separated in-domain vs. OOD testing, and use of robust baselines.  \n   - Metrics and ablation studies are exhaustively detailed, with appropriate reporting of macro/micro F1, BLEU/ROUGE, and LLM-based metrics.  \n   - However, **quantitative uncertainty or statistical significance** testing is not reported, and many analyses rely on heuristic interpretation of observed trends.  \n   - Some aspects (e.g., use of GPT-4-based evaluation) introduce subjectivity and reproducibility concerns, since API-based results can vary.\n\n4. **Clarity and Presentation** ‚Äì  \n   - The manuscript is generally clear and well-structured, though lengthy with some redundancy across sections.  \n   - Figures and tables are informative but dense; Figure 1 and the multi-page tables may require reformatting for journal readability.  \n   - Minor grammar and phrasing issues appear but do not hinder comprehension.  \n   - Equation-level or theoretical formulations are minimal; the work is largely experimental.\n\n5. **Ethical and Reproducibility Compliance** ‚Äì  \n   - The authors explicitly use **public datasets** (MIMIC-CXR, PadChest, IU X-Ray) with proper citations.  \n   - All datasets are de-identified; ethical compliance appears adequate.  \n   - Code availability is not explicitly stated‚Äîreproducibility could be strengthened by releasing model code, prompt templates, and evaluation scripts, especially for LLM-based scoring.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n1. **Summary**  \n   The manuscript investigates whether improved image classification performance translates into higher-quality radiology report generation. The authors propose a comprehensive **benchmark and evaluation framework** that tests multiple combinations of vision encoders, classifiers, and large language models (LLMs). They introduce **two LLM-based evaluation metrics** addressing the shortcomings of traditional language and clinical metrics. Through experiments on four datasets (in-domain and OOD, long-tail distributions), the study concludes that while classification outputs can improve report quality for common in-domain cases, their benefits do not generalize to out-of-distribution long-tail pathologies, sometimes even degrading performance.\n\n2. **Strengths**  \n   - Timely topic addressing the **integration of large language models in medical imaging**.  \n   - Systematic exploration and well-structured benchmark that could serve the community.  \n   - Insightful empirical observation clarifying the limited utility of diagnostic classifiers for OOD generation.  \n   - Introduction of LLM-based metrics that qualitatively enhance the evaluation of generated clinical text.  \n   - Extensive ablation and dataset diversity improve credibility of trends.\n\n3. **Weaknesses**  \n   - Limited *methodological novelty*: the framework largely reuses established architectures and metrics, with limited theoretical advancement.  \n   - Reproducibility depends heavily on proprietary LLM APIs (GPT‚Äë4), reducing accessibility.  \n   - Lacks formal statistical analysis or uncertainty quantification for comparative results.  \n   - Discussion of the *mechanistic reasons* for classification‚Äìgeneration gaps remains descriptive; no modeling insight or mathematical analysis is provided.  \n   - Some contextual framing (e.g., clinical interpretability, alignment with diagnostic reasoning) could be expanded to emphasize imaging science impact.\n\n4. **Major Comments**  \n   1. Clarify the **novelty boundary**: is this primarily an evaluation paper or a modeling contribution? The abstract and title may overstate methodological innovation relative to the actual scope.  \n   2. Strengthen **methodological rationale** for new LLM-based metrics‚Äîdescribe how they are validated against expert judgment or inter-rater consistency.  \n   3. Discuss **statistical significance and variability** in reported results; include confidence intervals or standard deviations where possible.  \n   4. Provide **code and pipeline availability** for LLM-based evaluation; otherwise, the benchmark cannot be adopted or verified.  \n   5. Improve **discussion of clinical relevance**: how might these findings inform the next generation of clinically trustworthy ARRG systems?  \n   6. Consider reducing redundancy in Results/Appendix and enhancing visual clarity (condensed tables, key examples).  \n\n5. **Minor Comments**  \n   - Carefully proofread for minor grammatical inconsistencies (‚Äúaim to,‚Äù ‚Äúwould improve‚Äù) and typographical errors.  \n   - In Figure 1, ensure architectural variants are legible in grayscale print.  \n   - Explicitly define all abbreviations at first use (e.g., ICL, RJ-n).  \n   - Move dataset preprocessing details to supplementary material for length reduction.  \n   - Update the references to the latest *TMI* or *MedIA* works on multimodal LLM evaluation if available.\n\n6. **Overall Recommendation** ‚Äì **Major Revision**  \n   The work is thorough, relevant, and potentially impactful as a community benchmark, but it falls short of TMI‚Äôs threshold for publication-ready novelty and methodological generality in its present form. Clarify conceptual contributions, enhance reproducibility, and solidify empirical rigor to justify acceptance.\n\n7. **Confidence Level** ‚Äì **4 / 5**  \n   Confident in the assessment; findings and claims are well-supported but rely on external LLM-based evaluation that may evolve.\n\n---\n\n**Summary Decision:** *Major Revision ‚Äî potentially valuable benchmark contribution; requires stronger articulation of novelty, reproducible methodology, and quantitative rigor to meet IEEE TMI standards.*",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 5,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a comprehensive benchmark and evaluation framework for automatic radiology report generation (ARRG), examining whether improvements in image classification performance yield higher-quality clinical reports. It integrates multiple combinations of vision encoders, classifiers, and large language models (LLMs), and introduces two LLM-based evaluation metrics aimed at addressing the limitations of traditional linguistic and clinical measures. The study is clear, well-structured, and thoroughly experimental, though somewhat lengthy and occasionally redundant. Overall, it contributes valuable empirical insights into multimodal medical imaging evaluation but offers limited methodological novelty.  \n\n**Major Comments**  \n1. **Clarify contribution scope:** Explicitly distinguish whether the work constitutes a methodological innovation or primarily an evaluation benchmark. The title and abstract may overstate innovation.  \n2. **Validation of LLM-based metrics:** Provide evidence of how these new metrics align with expert judgments or inter-rater consistency, as justification for their validity beyond heuristic comparison.  \n3. **Statistical rigor:** Incorporate measures of uncertainty (e.g., confidence intervals, standard deviations) and statistical significance testing to support claims of performance differences.  \n4. **Reproducibility and code sharing:** Clarify code and pipeline availability, particularly for the LLM-based evaluation module; proprietary GPT-based dependencies limit accessibility.  \n5. **Clinical interpretability:** Expand on the implications of results for clinical deployment and diagnostic trustworthiness in ARRG systems.  \n6. **Presentation improvements:** Reduce redundancy in results and appendices, condense large tables, and improve figure readability for grayscale print.  \n\n**Minor Comments**  \n- Proofread to correct minor grammatical and typographical errors.  \n- Ensure all abbreviations (e.g., ICL, RJ‚Äën) are defined on first use.  \n- Move dataset preprocessing details to supplementary material to streamline the main text.  \n- Update references to include recent related works on multimodal LLM evaluation.  \n- Reformat Figure 1 for improved clarity and legibility.  \n\n**Summary Paragraph**  \nThis manuscript addresses a timely and relevant problem at the intersection of vision-language modeling and medical imaging. Its strengths include systematic benchmarking, comprehensive ablation studies, and diverse datasets, which yield credible empirical conclusions about the limited utility of diagnostic classifiers for out-of-distribution report generation. However, its contribution is primarily empirical rather than methodological; reproducibility and statistical validation require substantial improvement. With clearer articulation of novelty, stronger justification for the introduced metrics, and enhanced transparency of methods, the work could become a valuable community benchmark.  \n\n**Decision Recommendation:** **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper investigates whether incorporating classification information can improve the robustness of automatic radiology report generation (ARRG), particularly for long-tail and out-of-distribution scenarios. The authors design an evaluation framework that combines LLM-based report generation models with diagnostic classifiers, proposing four baseline architectures: vision+LLM, classifier+vision+LLM, expanding, and refining approaches (Figure 1). They introduce LLM-based evaluation metrics including LLM-RadJudge and Long-tailed & OOD F1 scores to assess performance beyond conventional language and clinical metrics (Section 2.3.2). Experiments across four datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray) reveal that while classification information improves report quality in in-domain scenarios, it provides limited benefits for out-of-distribution long-tail cases when using powerful foundation models like Llama 3.1 70B and GPT-4 (Tables 2, 4, 5).\n\n## Weaknesses\n\n‚Ä¢ **Insufficient Mathematical Rigor in Model Formulation**\n  - The paper lacks formal mathematical definitions for the core components (vision encoder, classifier, LLM integration) described in Section 2.2, making it difficult to assess technical soundness\n  - No explicit loss functions or optimization objectives are provided for the different training paradigms (end-to-end vs. training-free), which is critical for reproducibility\n  - The connector architecture is only briefly mentioned as \"multi-layer perceptron (MLP) with GELU activations\" (Section 2.1) without mathematical specifications\n\n‚Ä¢ **Limited Experimental Design and Statistical Analysis**\n  - Tables 1-2 show results without confidence intervals, standard deviations, or significance tests, undermining the reliability of claimed improvements\n  - The choice of 100 and 200 long-tail categories for classifier training (Table 2) appears arbitrary with no justification or ablation study\n  - Case studies are limited to individual examples (Figure 3, Figure 5) without systematic analysis across the test sets\n\n‚Ä¢ **Questionable Evaluation Framework Validity**\n  - The LLM-based metrics (LLM-RadJudge, Long-tailed & OOD F1) rely on GPT-4o for ground truth extraction (Section 2.3.2), introducing potential bias and circularity in evaluation\n  - The paper acknowledges that \"varying granularity in observation names across datasets requires semantic transformation\" but doesn't address how this affects metric reliability\n  - No validation of the proposed metrics against radiologist assessments or existing clinical standards\n\n‚Ä¢ **Unclear Methodology and Reproducibility Issues**\n  - The data preprocessing pipeline lacks detail, particularly for the OOD datasets where Spanish reports are translated to English using GPT-4 (Section 2.3.1)\n  - Implementation details are relegated to the appendix with insufficient description of hyperparameter selection and model training procedures\n  - The \"refining\" approach methodology is not clearly explained in the main text, making it difficult to understand how classification information is integrated\n\n‚Ä¢ **Weak Analysis of Core Research Question**\n  - The title question \"Is classification all you need?\" is not systematically addressed through controlled experiments isolating classification effects\n  - The paper conflates multiple variables (model architecture, training paradigm, dataset domain) making it difficult to attribute performance differences to classification integration\n  - Finding 2 states limitations for OOD scenarios but doesn't provide sufficient analysis of why this occurs or potential solutions\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance Mathematical Formulation and Technical Clarity**\n  - Provide formal mathematical definitions for each model component including input/output dimensions and transformation functions\n  - Specify explicit loss functions and optimization objectives for each training paradigm with proper mathematical notation\n  - Include detailed architectural specifications for the connector and classification integration mechanisms\n\n‚Ä¢ **Strengthen Experimental Rigor and Statistical Validation**\n  - Report confidence intervals and conduct statistical significance tests for all performance comparisons across tables\n  - Provide systematic ablation studies justifying the choice of long-tail category numbers and other key hyperparameters\n  - Expand case studies to include quantitative analysis across larger sample sets rather than cherry-picked examples\n\n‚Ä¢ **Validate and Improve Evaluation Framework**\n  - Validate LLM-based metrics against expert radiologist evaluations to establish reliability and remove potential circular reasoning\n  - Address the acknowledged issues with cross-dataset observation granularity through systematic semantic alignment studies\n  - Include traditional clinical validation metrics alongside the proposed LLM-based approaches\n\n‚Ä¢ **Clarify Methodology and Ensure Reproducibility**\n  - Move essential implementation details from appendix to main text and provide comprehensive hyperparameter justification\n  - Describe the data preprocessing pipeline in detail, particularly for cross-lingual translation and its potential impact on results\n  - Provide clear algorithmic descriptions of the \"refining\" approach and other training-free methods\n\n‚Ä¢ **Systematically Address the Core Research Question**\n  - Design controlled experiments that isolate the effect of classification information while holding other variables constant\n  - Provide deeper analysis of why classification helps in-domain but fails for OOD scenarios, potentially through attention visualization or ablation studies\n  - Restructure the paper to more directly answer the title question with clear experimental evidence and theoretical justification",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 4,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether incorporating classification information can enhance the robustness of automatic radiology report generation (ARRG), with particular attention to long-tail and out-of-distribution (OOD) scenarios. The authors design an evaluation framework integrating diagnostic classifiers with large language models (LLMs), proposing four variant architectures‚Äîvision+LLM, classifier+vision+LLM, expanding, and refining. They further introduce new LLM-based metrics (LLM-RadJudge and Long-tailed & OOD F1) to complement existing evaluation measures. Experiments across multiple datasets suggest that classification integration improves in-domain performance but offers limited benefits for OOD tasks when powerful LLMs are used. While the study explores an important question, multiple aspects of its technical formulation, evaluation validity, and methodological clarity warrant further refinement.  \n\n**Major Comments**  \n1. **Mathematical and Technical Insufficiency** ‚Äì The model components and integration schemes are described qualitatively without formal mathematical definitions, making it difficult to assess soundness. Loss functions and optimization objectives are absent, and the connector architecture is insufficiently specified.  \n2. **Experimental Design and Statistical Rigor** ‚Äì The reported results lack confidence intervals, standard deviations, or statistical significance testing. Key experimental choices, such as the number of long-tail categories, are not justified or ablated. Case studies are anecdotal rather than comprehensive.  \n3. **Evaluation Framework Validity** ‚Äì The new metrics depend on GPT-4o for ground truth extraction, introducing potential evaluation bias and circularity. Dataset-level semantic transformations are mentioned but not systematically evaluated. No validation is provided against clinical or expert benchmarks.  \n4. **Methodological Clarity and Reproducibility** ‚Äì The preprocessing pipeline, particularly the translation of Spanish reports, is insufficiently detailed. Implementation details and hyperparameter selections are poorly documented, and the refining approach remains unclear.  \n5. **Analysis of Core Research Question** ‚Äì The central inquiry (‚ÄúIs classification all you need?‚Äù) is not addressed through controlled or isolated experiments. Multiple variables are entangled, limiting interpretability, and the reasons behind OOD limitations are underexplored.  \n\n**Minor Comments**  \n- Greater prominence could be given to architectural schematics and algorithmic descriptions.  \n- Some figures and tables would benefit from clearer labeling and statistical summaries.  \n- Clarify acronyms and standardize notation throughout.  \n\n**Summary Paragraph**  \nThe paper raises a meaningful problem and offers creative evaluation concepts but falls short in theoretical rigor, methodological completeness, and empirical validation. Strengthening mathematical formulation, statistical support, and reproducibility documentation would make the findings more credible and actionable. The current presentation does not convincingly answer the central research question or substantiate the claimed robustness improvements.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial methodological and evaluative refinements are required before the work can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript investigates the impact of incorporating classification information into large language model (LLM)-based radiology report generation systems. The authors hypothesize that integrating a classifier trained on long-tailed and rare diseases can enhance the robustness of generated reports. They evaluate this hypothesis using four benchmarks and a comprehensive set of metrics, including traditional language and clinical metrics, as well as novel LLM-based metrics. The results reveal that while classification information improves report quality in in-domain scenarios, it has limited benefits in out-of-distribution (OOD) settings. The study highlights the potential of classification in improving report generation but also underscores the challenges faced by LLMs in handling long-tail and OOD data.\n\n## Major Comments\n1. Novelty and Positioning: The manuscript contributes to the ongoing discussion on the role of classification in report generation, but it could benefit from a clearer distinction from related works. The authors should more explicitly compare their approach with recent studies that have explored the integration of classification and LLMs in medical imaging tasks. Additionally, the manuscript should provide a more detailed analysis of how the proposed method differs from existing techniques in terms of both conceptual design and empirical performance.\n\n2. Evaluation Design: The experiments are conducted on a diverse set of datasets, including in-domain and OOD scenarios. However, the evaluation could be strengthened by including more detailed case studies and analyses of the generated reports. Specifically, qualitative evaluations of the generated reports in OOD scenarios could provide deeper insights into the limitations and potential biases of the models. Moreover, the inclusion of additional clinical metrics, such as those that assess the impact of the generated reports on clinical decision-making, would further bolster the manuscript's contributions.\n\n3. Comparisons: The manuscript compares the proposed approach against several state-of-the-art baselines, but it lacks comparisons with recent diffusion-based and transformer-based methods that have shown promising results in medical image synthesis and report generation. Including these comparisons would provide a more comprehensive view of the proposed method's performance relative to the latest advancements in the field.\n\n4. Reproducibility: The manuscript states that code will be released, but the description of the training protocols, preprocessing steps, and model hyperparameters is insufficient for full reproducibility. Providing a more detailed description of the experimental setup, including the exact configurations and data preprocessing pipelines, would enhance the transparency and reproducibility of the work.\n\n## Minor Comments\n1. Clarity of Figures: Figures 3 and 5 are somewhat cluttered. Simplifying the presentation by showing fewer representative examples with zoomed-in regions would improve readability.\n   \n2. Notation Consistency: The notation for the vision encoder and classification tokens is inconsistent. Clarifying these notations and ensuring consistency throughout the manuscript would aid in understanding the model architecture.\n   \n3. Acronym Definitions: Several acronyms (e.g., \"R=4\") are used without definition. Providing definitions for all acronyms would improve clarity.\n   \n4. Typographical Errors: Minor typographical issues such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7) should be corrected.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in the field of automatic radiology report generation by investigating the impact of classification information on report quality. The proposed adaptation of LLMs to incorporate diagnostic results from classifiers is technically sound and offers valuable insights into the strengths and limitations of current approaches. The evaluation framework is comprehensive and includes both traditional and novel metrics, providing a thorough assessment of the proposed method. However, the evaluation could be strengthened with additional qualitative analyses and comparisons to recent diffusion-based and transformer-based methods. The reproducibility of the approach is currently limited due to incomplete methodological details, which needs to be addressed. Overall, while the manuscript makes a meaningful contribution to the field, there is room for improvement in terms of comprehensive evaluation and reproducibility.\n\n## Decision Recommendation\nMajor revision. The authors should expand their comparative analysis to include recent diffusion-based and transformer-based methods, conduct more detailed qualitative evaluations, and provide complete methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 5,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai",
          "comment": "The major comments primarily focus on textual analysis, lacking analysis of tables and figures."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript examines the integration of classification information into large language model (LLM)-based radiology report generation systems. The authors hypothesize that using a classifier trained on long-tailed and rare diseases can improve the robustness and clinical accuracy of generated reports. Their evaluation spans four benchmarks and a wide range of metrics, encompassing both conventional language/clinical measures and newer LLM-based metrics. The findings indicate that classification information enhances report quality in in-domain scenarios but yields limited benefits in out-of-distribution (OOD) contexts. Overall, the study provides an informative perspective on the interplay between classification and report generation, while identifying ongoing challenges in applying LLMs to long-tail and OOD medical data.  \n\n**Major Comments**  \n1. **Novelty and Positioning:** While the work is relevant to current efforts in combining classification and generative modeling, its originality would be clearer with a more explicit differentiation from prior studies. The authors should discuss in greater depth how their approach diverges from existing techniques conceptually and empirically, including comparisons to related literature on classifier-augmented LLMs in medical imaging.  \n2. **Evaluation Design:** The experiments include both in-domain and OOD datasets, but the evaluation would benefit from richer qualitative analyses of the generated reports, especially for OOD cases. Examples or case studies could reveal model limitations and potential biases. Additionally, the inclusion of metrics reflecting clinical decision impact would strengthen the contribution.  \n3. **Comparisons:** Although several baselines are included, the paper omits recent diffusion-based and transformer-based models that have achieved strong results in related imaging and reporting tasks. Adding these comparisons would provide a fuller perspective on performance.  \n4. **Reproducibility:** The promise to release code is positive, but the methodological description currently lacks necessary details. More explicit reporting of training protocols, hyperparameters, and preprocessing steps is required for reproducibility.  \n\n**Minor Comments**  \n- **Figures:** Figures 3 and 5 are visually dense; displaying fewer but more focused examples could improve clarity.  \n- **Notation:** The notation for vision encoders and classification tokens is inconsistent; aligning notation would aid comprehension.  \n- **Acronyms:** Several acronyms (e.g., ‚ÄúR=4‚Äù) are undefined; all should be clarified at first use.  \n- **Typos:** Minor typographical errors (e.g., ‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù) should be corrected.  \n\n**Summary Paragraph**  \nThis study contributes valuable insights into how classification-guided LLMs can improve radiology report generation, emphasizing their effectiveness in-domain and limitations on OOD data. The methodological design and metric coverage are strong, yet the comparative analysis and qualitative evaluation remain incomplete. Enhancing clarity, reproducibility, and the breadth of comparisons will improve the paper‚Äôs impact.  \n\n**Decision Recommendation**  \n**Major Revision.** The authors should expand comparative experiments, incorporate qualitative analyses, and detail methodological aspects to ensure full reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Is CLASSIFICATION ALL YOU NEED FOR RADIOLOGY REPORT GENERATION?\n\n### Summary\n\nThe paper investigates whether improving multi-label classifiers is sufficient to close the gap between medical image classification and radiology report generation, particularly under long-tailed and out-of-distribution (OOD) conditions. The authors propose a design space for LLM-based report generation (vision encoder, classifier, LLM) and a robustness-oriented evaluation framework that augments standard language/clinical metrics with LLM-based metrics, including an LLM judge and an LLM-extracted long-tail/OOD F1. Experiments across MIMIC-CXR (in-domain), CXR-LT (long-tail), PadChest and IU X-Ray (OOD) show that classification can improve in-domain clinical metrics but offers limited help OOD and can even mislead LLMs when classifier errors are amplified.\n\n### Strengths\n\n- Technical novelty and innovationSystematic design space enumeration for LLM-based radiology report generation, covering end-to-end and training-free paradigms and multiple ways of injecting classifier information (tokens, prompts, refinement).Introduction of LLM-based evaluation components that target limitations of n-gram and label-set‚Äìrestricted clinical metrics; the attempt to evaluate long-tail/OOD disease coverage via LLM extraction is timely.Clear articulation of the central hypothesis (‚Äúis classification all you need?‚Äù) and analysis showing nuanced behavior across in-domain vs OOD and long-tail regimes.\n- Experimental rigor and validationBroad benchmarking across four datasets, including both in-domain (MIMIC, CXR-LT) and OOD (PadChest, IU X-Ray), and multiple backbones (Swin, ViT, DINOv2, RAD-DINO) and LLMs (Phi-3, Vicuna, Llama 3.1).Ablations on representation of classification information ([CLS] tokens vs prompts) and LLM scaling; case studies to interpret failure modes.Explicit long-tail emphasis aligns with community benchmarks (CXR-LT), addressing an important gap.\n- Clarity of presentationThe conceptual design space and training vs training-free dichotomy are explained in a way that unifies disparate prior methods.The main findings are crisply summarized: classification helps in-domain; limited or negative OOD impact.\n- Significance of contributionsProvides evidence-based guidance to the community that naive integration of classifier outputs may not generalize OOD and can amplify errors for rare findings, a critical insight for clinical deployment.Encourages more robust evaluation practices and careful use of classifier-derived signals, with potential to influence future architectures.\n\nTechnical novelty and innovation\n\n- Systematic design space enumeration for LLM-based radiology report generation, covering end-to-end and training-free paradigms and multiple ways of injecting classifier information (tokens, prompts, refinement).\n- Introduction of LLM-based evaluation components that target limitations of n-gram and label-set‚Äìrestricted clinical metrics; the attempt to evaluate long-tail/OOD disease coverage via LLM extraction is timely.\n- Clear articulation of the central hypothesis (‚Äúis classification all you need?‚Äù) and analysis showing nuanced behavior across in-domain vs OOD and long-tail regimes.\n\nExperimental rigor and validation\n\n- Broad benchmarking across four datasets, including both in-domain (MIMIC, CXR-LT) and OOD (PadChest, IU X-Ray), and multiple backbones (Swin, ViT, DINOv2, RAD-DINO) and LLMs (Phi-3, Vicuna, Llama 3.1).\n- Ablations on representation of classification information ([CLS] tokens vs prompts) and LLM scaling; case studies to interpret failure modes.\n- Explicit long-tail emphasis aligns with community benchmarks (CXR-LT), addressing an important gap.\n\nClarity of presentation\n\n- The conceptual design space and training vs training-free dichotomy are explained in a way that unifies disparate prior methods.\n- The main findings are crisply summarized: classification helps in-domain; limited or negative OOD impact.\n\nSignificance of contributions\n\n- Provides evidence-based guidance to the community that naive integration of classifier outputs may not generalize OOD and can amplify errors for rare findings, a critical insight for clinical deployment.\n- Encourages more robust evaluation practices and careful use of classifier-derived signals, with potential to influence future architectures.\n\n### Weaknesses\n\n- Technical limitations or concernsThe LLM-based long-tail/OOD F1 relies on GPT-4o extraction and ad-hoc label mappings, which introduces uncertainty, potential subjectivity, and limited reproducibility without extensive details or calibration checks.Lack of uncertainty-aware integration (e.g., calibration, abstention, confidence gating) despite the finding that misclassifications can mislead LLMs; this undercuts the conclusion that classification ‚Äúhelps in-domain but not OOD‚Äù without testing guardrails.The switch from DICOM to JPG inputs (for compatibility) is acknowledged but not quantified; this could degrade subtle findings and confound conclusions about gaps between generation and classification.\n- Experimental gaps or methodological issuesLimited comparison to several strong contemporary methods focused on clinical fidelity and long-tail robustness, such as LM-RRG (clinical RL), RA-RRG (retrieval-augmented generation), and concept-alignment pipelines like RadAlign; results would be more compelling with these baselines.No statistical significance analysis, error bars, or calibration metrics; correlation claims between classifier quality and report outcomes would benefit from controlled sweeps with quantified variance.The PadChest OOD evaluation is based on a small random sample (n=500) and English translation via GPT-4, which may shift label distributions and semantics relative to the original; the impact of translation is not analyzed.\n- Clarity or presentation issuesSeveral tables have missing values, formatting artifacts, and unexplained metric names (e.g., RGER vs RadGraph F1), reducing interpretability and reproducibility.Definitions of LT-100/LT-200 label sets, their coverage, and mapping across datasets are under-specified, making it hard to assess OOD label-space mismatch.\n- Missing related work or comparisonsLimited discussion and empirical comparison with recent pipelines that (a) align visual concepts to diagnostic criteria (RadAlign), (b) use RL with clinical rewards (LM-RRG), or (c) employ retrieval/quality-control auditing (RA-RRG; auditing frameworks) to mitigate hallucinations and noisy classifier propagation.The CXR-LT 2024 challenge highlights stronger long-tail practices (reweighting, ensembles, synthetic augmentation, calibration); these strategies are not tested here to see if they change the central finding.\n\nTechnical limitations or concerns\n\n- The LLM-based long-tail/OOD F1 relies on GPT-4o extraction and ad-hoc label mappings, which introduces uncertainty, potential subjectivity, and limited reproducibility without extensive details or calibration checks.\n- Lack of uncertainty-aware integration (e.g., calibration, abstention, confidence gating) despite the finding that misclassifications can mislead LLMs; this undercuts the conclusion that classification ‚Äúhelps in-domain but not OOD‚Äù without testing guardrails.\n- The switch from DICOM to JPG inputs (for compatibility) is acknowledged but not quantified; this could degrade subtle findings and confound conclusions about gaps between generation and classification.\n\nExperimental gaps or methodological issues\n\n- Limited comparison to several strong contemporary methods focused on clinical fidelity and long-tail robustness, such as LM-RRG (clinical RL), RA-RRG (retrieval-augmented generation), and concept-alignment pipelines like RadAlign; results would be more compelling with these baselines.\n- No statistical significance analysis, error bars, or calibration metrics; correlation claims between classifier quality and report outcomes would benefit from controlled sweeps with quantified variance.\n- The PadChest OOD evaluation is based on a small random sample (n=500) and English translation via GPT-4, which may shift label distributions and semantics relative to the original; the impact of translation is not analyzed.\n\nClarity or presentation issues\n\n- Several tables have missing values, formatting artifacts, and unexplained metric names (e.g., RGER vs RadGraph F1), reducing interpretability and reproducibility.\n- Definitions of LT-100/LT-200 label sets, their coverage, and mapping across datasets are under-specified, making it hard to assess OOD label-space mismatch.\n\nMissing related work or comparisons\n\n- Limited discussion and empirical comparison with recent pipelines that (a) align visual concepts to diagnostic criteria (RadAlign), (b) use RL with clinical rewards (LM-RRG), or (c) employ retrieval/quality-control auditing (RA-RRG; auditing frameworks) to mitigate hallucinations and noisy classifier propagation.\n- The CXR-LT 2024 challenge highlights stronger long-tail practices (reweighting, ensembles, synthetic augmentation, calibration); these strategies are not tested here to see if they change the central finding.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe central claim‚Äîthat classification improves in-domain report quality but has limited OOD benefits and can amplify errors‚Äîis plausible and supported by aggregate metrics and cases. However, the current integration strategies are relatively naive (token injection, prompts, basic refining), and the study omits uncertainty-aware fusions (e.g., confidence gating, selective abstention, calibration, ensembling). Without these, the conclusion may overstate the limits of classification-informed generation.The proposed LLM-based metrics are directionally useful, but they demand rigorous controls: inter-rater reliability between LLM labelers and human experts, stability across prompts/temperature, ontology normalization across datasets, and handling of near-synonyms and hierarchies (e.g., lobe vs lung). Absent these, metric variance may be high.\n- Experimental evaluation assessmentDataset coverage is appropriate, but the OOD samples for PadChest are small and potentially confounded by translation. Consider evaluating directly in Spanish with clinically tuned NER/labelers or validating that translation preserves mention-level labels.The ablation on [CLS] vs promptifying classification information is informative; however, the study would benefit from:A calibrated thresholding study (e.g., tune per-class thresholds using dev sets; ECE/Brier scores; temperature scaling/Dirichlet calibration) and analysis of how confidence gating affects generation outcomes and OOD robustness.Selective prediction experiments that trade coverage for accuracy (auditing frameworks), e.g., deferring to human when classifier vs text labels disagree, as in modular auditing approaches.Controlled sweeps of classifier strength (e.g., synthetic noise injection or different backbones and training regimes) to quantify the monotonic relationship between classifier F1 and generation quality more precisely (confidence intervals, regression analyses).The decision to use MIMIC-CXR-JPG rather than DICOM for vision inputs should be supported by quantitative comparisons; subtle pathologies can be degraded by lossy transformations.\n- Comparison with related work (using the summaries provided)Relative to MAIRA-1, this work focuses more on evaluation design and classifier integration than on pushing SOTA generation quality. MAIRA-1 demonstrates gains from domain-specific vision encoders and deeper adapters; those design choices are touched on here but not fully exploited.LM-RRG shows clinical reward optimization (RadCliQ) can improve clinical fidelity; integrating such a reward in your setting might change the interplay between classification cues and textual outputs, especially for rare findings.RadAlign and the long-tailed node-based generation approach explicitly align image features with clinically meaningful concepts and retrieve exemplars; these methods aim to reduce hallucination and improve grounding. Including them as baselines or incorporating their ideas (e.g., concept tokens aligned via contrastive losses, RAG) would enrich the study and potentially mitigate the amplification of classifier errors you observe.RA-RRG demonstrates that key-phrase retrieval plus an LLM can yield strong clinical metrics at low training cost; your ‚ÄúExpanding/Refining‚Äù baselines are related but lack a trained semantic retriever and multi-view combination. Incorporating a learned retriever and phrase normalization could provide a stronger training-free baseline.The CXR-LT (2023/2024) benchmark papers emphasize long-tail remedies (loss reweighting, ensembles, synthetic tail augmentation) and show material gains on tail classes. Evaluating such remedies within your integration schemes could test whether stronger rare-class classifiers change your OOD conclusion.\n- Discussion of broader impact and significanceThe paper makes an important practical point: injecting classifier outputs into LLMs without safeguards can systematically propagate and amplify false positives/negatives, particularly for rare classes and OOD data. This highlights the need for confidence-aware fusion, abstention/deferral policies, and modular auditing to ensure clinical safety.The proposed LLM-based metrics may catalyze broader adoption of semantically aware evaluation; however, they must be standardized, open-sourced, and validated against expert annotations to be trusted for benchmarking.\n- Reproducibility and reportingProvide complete details on LT-100/LT-200 construction, class lists, mapping across datasets, prompt templates, LLM inference settings (temperature, system prompts), and GPT-4o extraction prompts and postprocessing. Release code/prompts and, if possible, pseudo-labels or label-mapping dictionaries.Add significance testing and confidence intervals; report per-class head/medium/tail breakdowns in tables; include calibration metrics and error analysis stratified by view count and image quality.\n\nTechnical soundness evaluation\n\n- The central claim‚Äîthat classification improves in-domain report quality but has limited OOD benefits and can amplify errors‚Äîis plausible and supported by aggregate metrics and cases. However, the current integration strategies are relatively naive (token injection, prompts, basic refining), and the study omits uncertainty-aware fusions (e.g., confidence gating, selective abstention, calibration, ensembling). Without these, the conclusion may overstate the limits of classification-informed generation.\n- The proposed LLM-based metrics are directionally useful, but they demand rigorous controls: inter-rater reliability between LLM labelers and human experts, stability across prompts/temperature, ontology normalization across datasets, and handling of near-synonyms and hierarchies (e.g., lobe vs lung). Absent these, metric variance may be high.\n\nExperimental evaluation assessment\n\n- Dataset coverage is appropriate, but the OOD samples for PadChest are small and potentially confounded by translation. Consider evaluating directly in Spanish with clinically tuned NER/labelers or validating that translation preserves mention-level labels.\n- The ablation on [CLS] vs promptifying classification information is informative; however, the study would benefit from:A calibrated thresholding study (e.g., tune per-class thresholds using dev sets; ECE/Brier scores; temperature scaling/Dirichlet calibration) and analysis of how confidence gating affects generation outcomes and OOD robustness.Selective prediction experiments that trade coverage for accuracy (auditing frameworks), e.g., deferring to human when classifier vs text labels disagree, as in modular auditing approaches.Controlled sweeps of classifier strength (e.g., synthetic noise injection or different backbones and training regimes) to quantify the monotonic relationship between classifier F1 and generation quality more precisely (confidence intervals, regression analyses).\n- The decision to use MIMIC-CXR-JPG rather than DICOM for vision inputs should be supported by quantitative comparisons; subtle pathologies can be degraded by lossy transformations.\n\n- A calibrated thresholding study (e.g., tune per-class thresholds using dev sets; ECE/Brier scores; temperature scaling/Dirichlet calibration) and analysis of how confidence gating affects generation outcomes and OOD robustness.\n- Selective prediction experiments that trade coverage for accuracy (auditing frameworks), e.g., deferring to human when classifier vs text labels disagree, as in modular auditing approaches.\n- Controlled sweeps of classifier strength (e.g., synthetic noise injection or different backbones and training regimes) to quantify the monotonic relationship between classifier F1 and generation quality more precisely (confidence intervals, regression analyses).\n\nComparison with related work (using the summaries provided)\n\n- Relative to MAIRA-1, this work focuses more on evaluation design and classifier integration than on pushing SOTA generation quality. MAIRA-1 demonstrates gains from domain-specific vision encoders and deeper adapters; those design choices are touched on here but not fully exploited.\n- LM-RRG shows clinical reward optimization (RadCliQ) can improve clinical fidelity; integrating such a reward in your setting might change the interplay between classification cues and textual outputs, especially for rare findings.\n- RadAlign and the long-tailed node-based generation approach explicitly align image features with clinically meaningful concepts and retrieve exemplars; these methods aim to reduce hallucination and improve grounding. Including them as baselines or incorporating their ideas (e.g., concept tokens aligned via contrastive losses, RAG) would enrich the study and potentially mitigate the amplification of classifier errors you observe.\n- RA-RRG demonstrates that key-phrase retrieval plus an LLM can yield strong clinical metrics at low training cost; your ‚ÄúExpanding/Refining‚Äù baselines are related but lack a trained semantic retriever and multi-view combination. Incorporating a learned retriever and phrase normalization could provide a stronger training-free baseline.\n- The CXR-LT (2023/2024) benchmark papers emphasize long-tail remedies (loss reweighting, ensembles, synthetic tail augmentation) and show material gains on tail classes. Evaluating such remedies within your integration schemes could test whether stronger rare-class classifiers change your OOD conclusion.\n\nDiscussion of broader impact and significance\n\n- The paper makes an important practical point: injecting classifier outputs into LLMs without safeguards can systematically propagate and amplify false positives/negatives, particularly for rare classes and OOD data. This highlights the need for confidence-aware fusion, abstention/deferral policies, and modular auditing to ensure clinical safety.\n- The proposed LLM-based metrics may catalyze broader adoption of semantically aware evaluation; however, they must be standardized, open-sourced, and validated against expert annotations to be trusted for benchmarking.\n\nReproducibility and reporting\n\n- Provide complete details on LT-100/LT-200 construction, class lists, mapping across datasets, prompt templates, LLM inference settings (temperature, system prompts), and GPT-4o extraction prompts and postprocessing. Release code/prompts and, if possible, pseudo-labels or label-mapping dictionaries.\n- Add significance testing and confidence intervals; report per-class head/medium/tail breakdowns in tables; include calibration metrics and error analysis stratified by view count and image quality.\n\n### Questions for Authors\n\n- How were the LT-100 and LT-200 label sets defined precisely, and how do they map to the label spaces of PadChest and IU X-Ray? Can you share the ontology mappings and coverage statistics?\n- For the GPT-4o-based long-tail/OOD F1, what prompts, temperatures, and normalization rules did you use? How consistent are extractions across repeated runs, and how do they correlate with human annotations?\n- Did you evaluate classifier calibration (ECE, Brier, temperature scaling) and test confidence gating/abstention when injecting classifier outputs into the LLM? If so, how did coverage‚Äìquality trade-offs change?\n- What was the impact of translating PadChest reports into English on label distributions and metrics? Did you try native Spanish labelers or concept normalization (e.g., UMLS/RadGraph mapping) to avoid translation artifacts?\n- Why did you choose MIMIC-CXR-JPG rather than DICOM or the MAIRA-1 pipeline‚Äôs DICOM pathway? Do you have quantitative evidence that this choice does not adversely affect clinical findings?\n- Can you provide statistical significance and error bars for the main comparisons (especially the ‚Äúclassification helps in-domain‚Äù result) and per-class head/medium/tail analyses?\n- Did you experiment with retrieval augmentation (e.g., exemplar reports) or concept-alignment losses (as in RadAlign) to reduce classifier-error amplification? If not, what challenges prevented this?\n- How do your results change when using auditing pipelines that reconcile image-derived labels with text-derived labels and defer disagreements (e.g., Warr et al.)?\n- Are multi-view studies handled explicitly? If not, could multi-view fusion alter your conclusions about classifier vs LLM contributions?\n- Will you release code, prompts, and the LLM-judge evaluation scripts to facilitate replication and standardization?\n\n### Overall Assessment\n\nThis is a timely, thoughtful analysis paper that asks an important question for clinically reliable radiology report generation: does a stronger classifier suffice? The study‚Äôs main finding‚Äîthat classification boosts in-domain metrics but offers limited or even negative OOD benefits without safeguards‚Äîis valuable and likely to influence practice. The work is strengthened by its design space framing, long-tail/OOD emphasis, and ablations on how to inject classifier information.\n\nHowever, the evaluation pipeline has methodological gaps that limit the conclusiveness of the claims. The LLM-based long-tail/OOD metric needs more rigorous standardization and validation; OOD evaluation is small and confounded by translation; key modern baselines (RL-aligned, retrieval-augmented, concept-aligned) are not included; and uncertainty-aware fusion and auditing, which are directly relevant to your conclusions, are not tested. Table/reporting artifacts also impede clarity and reproducibility.\n\nWith a more rigorous and transparent evaluation (calibration, abstention, larger OOD sets without translation, statistical tests), stronger baselines, and inclusion of safety mechanisms (gating/auditing), this paper could reach top-tier impact as a benchmark/analysis contribution. As it stands, it is a useful and thought-provoking step that surfaces critical pitfalls, but it needs additional experimental depth and cleaner reporting to meet the standards of a top-tier venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 3,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 2,
          "source": "ai",
          "comment": "Parts of the main comments are broadly descriptive and do not offer subjective/critical opinions to the manuscript."
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates whether improving multi-label classifiers can close the performance gap between medical image classification and radiology report generation, focusing on long-tailed and out-of-distribution (OOD) settings. It explores a systematic design space for LLM-based report generation, integrating visual encoders, classifiers, and large language models. A robustness-oriented evaluation framework is introduced, augmenting conventional clinical and language metrics with LLM-based assessments such as an ‚ÄúLLM judge‚Äù and LLM-extracted tail/OOD F1 scores. Experiments across multiple datasets (MIMIC-CXR, CXR-LT, PadChest, IU X-Ray) show that classification improvements benefit in-domain performance but have limited or adverse OOD effects, as classifier errors may mislead the LLM.\n\n**Major Comments**  \n1. **Technical rigor and novelty:** The study‚Äôs systematic exploration of classifier‚ÄìLLM integration is commendable, yet its evaluation metrics rely on GPT-4o label extraction and heuristic mappings that may reduce reproducibility. The absence of calibration checks or inter-rater validation weakens confidence in the proposed long-tail/OOD metric.  \n2. **Methodological gaps:** No uncertainty-aware mechanisms such as calibration, confidence gating, or abstention are tested, despite their direct relevance to the identified failure mode. The claim that classification helps only in-domain could be overstated without these controls.  \n3. **Experimental limitations:** OOD evaluation is based on a small, translated PadChest subset, potentially altering label semantics. The switch from DICOM to JPG is unquantified and could affect clinical fidelity.  \n4. **Missing baselines:** Key recent methods‚ÄîLM-RRG (RL-aligned generation), RA-RRG (retrieval-augmented), and RadAlign (concept-based alignment)‚Äîare not compared, limiting the contextual depth of results.  \n5. **Statistical reporting:** The paper lacks significance tests, variance analyses, and calibration metrics. Controlled sweeps over classifier strength and confidence effects would reinforce the core claims.  \n6. **Clarity and reproducibility:** Incomplete tables, undefined acronyms (e.g., RGER), and missing definitions for LT-100/LT-200 sets reduce interpretability. Precise label mappings, prompts, and extraction details should be released for replication.\n\n**Minor Comments**  \n- Clean up table formatting and fill missing values.  \n- Include per-class head/mid/tail results and calibration metrics.  \n- Quantify the impact of JPG over DICOM inputs.  \n- Clarify prompt templates, temperatures, and extraction consistency for GPT-4o metrics.  \n- Discuss the translation artifacts from Spanish PadChest to English evaluation.  \n\n**Summary Paragraph**  \nThis is a timely, carefully framed study that raises an important question about the role of classification in report generation. It contributes a valuable design-space framework and highlights that classifier integration can help in-domain but harm OOD generalization. Yet, methodological weaknesses‚Äîespecially reliance on unvalidated LLM metrics, limited baselines, small OOD samples, and absence of uncertainty-aware fusion‚Äîundermine the conclusiveness of findings. The paper would benefit from stronger statistical analysis, transparent protocols, and inclusion of confidence- or retrieval-based mechanisms to ensure robustness and clinical safety.\n\n**Decision Recommendation**  \n**Major Revision.** The work offers significant insights but requires improved methodological rigor, expanded baselines, validation of LLM-derived metrics, and clearer reporting before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"IS CLASSIFICATION ALL YOU NEED FOR RADIOLOGY REPORT GENERATION?\"\n\n## 1. Opening Paragraph\n\nThis paper investigates the significant performance gap between medical image classification and radiology report generation, where current generative methods lag approximately 20% behind classification models in diagnostic accuracy. The authors explore whether incorporating classification information, particularly from models trained on long-tailed and rare disease distributions, can improve the quality and diagnostic accuracy of automatically generated radiology reports. They design a comprehensive evaluation framework that integrates long-tail scenarios and systematically evaluates different combinations of vision encoders, classifiers, and LLMs across both in-domain (MIMIC-CXR, CXR-LT) and out-of-distribution (PadChest, IU X-Ray) datasets. Their findings reveal that while classification information can enhance report quality in in-domain long-tail scenarios (with Macro F1 improvements of ~2-3%), its benefits for out-of-distribution scenarios are limited, and in some cases, incorporating classifier outputs can even degrade performance due to error propagation. The authors demonstrate that the \"Refining\" approach (using classification information to refine existing generated reports) outperforms direct expansion methods, but the persistent performance gap between classification and generation suggests that classification alone is not sufficient for high-quality report generation.\n\n## 2. Major and Minor Comments\n\n### Major Strengths:\n- The paper presents a well-designed evaluation framework that properly addresses the long-tail distribution challenges in medical imaging, which is a critical issue in clinical applications.\n- Introduction of LLM-based metrics (beyond traditional language and clinical metrics) provides more nuanced assessment of report quality, particularly for long-tail and rare conditions.\n- The systematic exploration of different model architectures (end-to-end vs. training-free approaches) offers valuable insights for future model development in the field.\n- Clear demonstration of the limitations of using classification information for OOD scenarios addresses an important gap in current understanding.\n- Comprehensive ablation studies examining the impact of different components (vision encoders, MLP layers, LLM scales) provide practical guidance for implementation.\n\n### Major Limitations:\n- The paper identifies the performance gap between classification and generation but offers limited exploration of why this gap persists beyond the hypothesis about missing descriptive details in classification information.\n- The evaluation is limited to chest X-rays; the generalizability of findings to other imaging modalities (CT, MRI) is unclear and should be addressed.\n- While the paper shows that classification information can introduce errors, it lacks a detailed analysis of specific error patterns and failure modes, which would strengthen the practical implications of the findings.\n- The paper could benefit from more concrete recommendations about when and how to optimally incorporate classification information for different clinical scenarios.\n\n### Minor Comments:\n- Figure 2 could be better labeled to clarify the distinction between \"w/ [IMG CLS] token\" and \"w/o [IMG CLS] token\" conditions, as the current presentation may be confusing to readers unfamiliar with vision transformer architectures.\n- Some sections, particularly the Related Work, could be more concise while maintaining necessary context.\n- The paper mentions the potential for LLMs to selectively use additional data during fine-tuning but doesn't explore this direction in depth, which would have strengthened the discussion.\n- More specific guidance on computational trade-offs between different approaches would be valuable for clinical implementation.\n\n## 3. Summary Evaluation\n\n**Significance**: This work addresses a critical problem in medical AI - the substantial performance gap between image classification and report generation. Understanding the conditions under which classification information improves report quality has direct implications for developing more accurate clinical decision support systems. The findings help set realistic expectations about the benefits of classification-based approaches and have practical value for the medical imaging community.\n\n**Innovation**: The paper introduces a novel evaluation framework with LLM-based metrics specifically designed for assessing report generation in long-tail scenarios. The systematic exploration of the model design space and clear categorization of approaches (end-to-end vs. training-free) provide valuable new insights. The finding that classification information helps in-domain but not OOD scenarios is counterintuitive and represents an important contribution to the field.\n\n**Evaluation**: The evaluation is thorough across multiple dimensions - different model architectures, vision encoders, LLM scales, and datasets (including both in-domain and OOD long-tail scenarios). The use of both traditional and LLM-based metrics provides a comprehensive assessment of model performance. However, the paper would be strengthened by a more detailed analysis of specific error patterns when classification information is incorporated.\n\n**Reproducibility**: The paper provides sufficient implementation details, including training procedures, hyperparameters, and dataset descriptions. The ablation studies are well-documented, and the model variants are clearly defined. While the authors describe their experimental setup comprehensively, explicit mention of code and model checkpoint availability would further enhance reproducibility.\n\n## 4. Decision Recommendation\n\nMinor Revision\n\nThe paper presents a well-conducted study with significant findings about the limitations of using classification information for radiology report generation. The evaluation is thorough, the methodology sound, and the findings have practical implications for the field. The authors have addressed an important clinical problem with a systematic approach that advances our understanding of when and how classification information can improve report generation. While there are areas for improvement (particularly in error analysis and generalizability), these can be addressed through minor revisions. The paper makes a valuable contribution to understanding the relationship between classification and report generation in medical imaging and merits publication in IEEE Transactions on Medical Imaging after addressing the specific concerns outlined in the major and minor comments.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 5,
          "specificity": 4,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates the performance gap between medical image classification and radiology report generation, where generative models lag approximately 20% behind classification models in diagnostic accuracy. The authors examine whether classification-derived information, especially from models trained on long-tailed and rare disease distributions, can enhance automated report quality and diagnostic performance. They propose an evaluation framework covering in-domain (MIMIC-CXR, CXR-LT) and out-of-distribution (PadChest, IU X-Ray) datasets, exploring multiple encoder-classifier-LLM configurations. Results indicate that while classification information benefits in-domain long-tail cases (yielding modest Macro-F1 gains), it offers limited or even negative effects in out-of-distribution settings due to potential error propagation. The study also shows that refining reports using classification outputs performs better than directly expanding reports but concludes that classification alone cannot close the performance gap in report generation.  \n\n**Major Comments**  \n1. The paper presents a robust evaluation framework addressing long-tail distribution issues, providing a valuable contribution for clinical applications.  \n2. Use of LLM-based metrics extends evaluation beyond traditional measures, offering nuanced insights into report quality for rare conditions.  \n3. Systematic exploration of architectures‚Äîcontrasting end-to-end and training-free approaches‚Äîproduces meaningful guidance for future work.  \n4. The analysis convincingly highlights the limitations of classification information in out-of-distribution cases.  \n5. However, the paper provides limited investigation into the root causes of the persistent gap between classification and generation, beyond hypothesizing missing descriptive detail.  \n6. The study focuses solely on chest X-rays; generalizability to other modalities (CT, MRI) is uncertain.  \n7. A more detailed analysis of specific failure modes and recommendations for optimal integration of classification information across clinical contexts would strengthen the practical impact.  \n\n**Minor Comments**  \n- Clarify labeling in Figure‚ÄØ2 to distinguish the ‚Äúw/ [IMG‚ÄØCLS] token‚Äù and ‚Äúw/o [IMG‚ÄØCLS] token‚Äù conditions.  \n- The Related Work section could be more concise.  \n- The discussion briefly mentions potential fine-tuning with additional data but does not analyze it; elaboration would enhance completeness.  \n- More explicit discussion of computational trade-offs between approaches would assist practitioners.  \n\n**Summary Paragraph**  \nOverall, this work addresses a timely and clinically relevant question about the interaction between classification and report generation in medical imaging. Its systematic framework, multi-dataset evaluation, and integration of LLM-based metrics represent notable methodological strengths. The findings‚Äîthat classification signals moderately improve in-domain results yet fail to generalize‚Äîprovide valuable reality checks for researchers pursuing hybrid architectures. The work is technically sound and well-documented, though it would benefit from expanded discussion of persistent error patterns, generalizability to other modalities, and clearer figure presentation.  \n\n**Decision Recommendation**  \n**Minor Revision.** The study is rigorous, well-motivated, and of practical importance. With minor clarifications and expanded discussion as noted above, the paper will make a strong contribution to understanding the relationship between classification and report generation in medical imaging.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe authors explore whether a comprehensive classification of radiological findings‚Äîincluding long‚Äëtailed and rare diseases‚Äîcan strengthen automatic radiology report generation (ARRG). They train their models on the MIMIC‚ÄëCXR dataset and evaluate them on four benchmarks (MIMIC‚ÄëCXR, CXR‚ÄëLT, PadChest, and IU‚ÄØX‚ÄëRay), constructing an evaluation framework that captures both in‚Äëdomain and out‚Äëof‚Äëdistribution (OOD) long‚Äëtail scenarios. A design space for LLM‚Äëbased report generation is defined, consisting of a vision encoder, a classifier, and a large language model (LLM). The study compares several configurations‚Äîvision‚Äëonly, classifier‚Äëaugmented, and refined prompting‚Äîand reports results using conventional language and clinical metrics together with two LLM‚Äëderived metrics. The findings indicate that classification can improve report quality for in‚Äëdomain long‚Äëtail cases in proportion to classifier performance, but offers limited advantage for OOD cases and may even magnify classifier errors in long‚Äëtail settings. The principal contribution is a systematic analysis of how classifiers influence LLM‚Äëdriven ARRG across a variety of evaluation conditions.\n\n---  \n\n## General feedback  \n\n- **Significance:** The investigation of how diagnostic classification might bolster ARRG addresses a clinically relevant challenge, particularly for rare pathologies where reliability is essential.  \n- **Innovation:** The paper presents three notable elements: (i) a clear taxonomy of LLM‚Äëbased report generators (see Figure‚ÄØ1), (ii) two novel LLM‚Äëderived evaluation metrics, and (iii) an extensive empirical assessment of classifier‚Äëreport interactions. While the taxonomy is helpful, the underlying concept of feeding classifier outputs to an LLM has been explored in recent literature (e.g., Wang‚ÄØet‚ÄØal.,‚ÄØ2023; Zhao‚ÄØet‚ÄØal.,‚ÄØ2024), which narrows the novelty margin.  \n- **Evaluation:** The authors employ four datasets and a broad suite of language, clinical, and LLM‚Äëbased metrics (Tables‚ÄØ1‚Äë5). However, the analysis would benefit from statistical significance testing, confidence intervals, and an independent radiologist assessment. Moreover, the OOD subsets are relatively small (e.g., 500 PadChest images and 756 IU‚ÄëX‚ÄëRay images), limiting the robustness of the conclusions.  \n- **Reproducibility:** Training specifics such as optimizer, learning‚Äërate schedule, and number of epochs are documented (Appendix‚ÄØF), yet key details‚Äîrandom seeds, precise data splits for the long‚Äëtail subsets, prompt templates, and the configuration of the LLM‚ÄëRadJudge metric‚Äîare missing. Additionally, the code and model checkpoints have not been released, which hampers reproducibility.\n\n---  \n\n## Specific comments/critiques  \n\n- **Title and framing:** The current title, ‚ÄúIS CLASSIFICATION ALL YOU NEED FOR,‚Äù is incomplete and could mislead readers into believing the paper proves classification alone is sufficient. A clearer phrasing that reflects the paper‚Äôs evaluative focus would improve comprehension.  \n- **Typographical and formatting issues:** Throughout the manuscript there are stray numbers (e.g., ‚Äú054‚Äë077‚Äù), broken tables, and duplicated figure captions. These distractions detract from readability and suggest the need for careful editorial polishing.  \n- **Statistical analysis:** The manuscript does not present any significance testing (e.g., paired t‚Äëtests, bootstrapping) to substantiate claims such as ‚Äúthe impact of classification on report quality is positively correlated with classifier performance.‚Äù Incorporating such analyses would strengthen the evidential basis of the results.  \n- **OOD experiment robustness:** The PadChest and IU‚ÄëX‚ÄëRay evaluations each rely on a single random sample (500 and 756 images, respectively) without reporting variability. Consequently, reported macro‚ÄëF1 improvements (e.g., 7.5‚ÄØ% versus 12.8‚ÄØ%) may not be reliable. Providing multiple splits or confidence intervals would address this concern.  \n- **Human evaluation:** While the authors argue that traditional clinical metrics have limitations, they do not include a radiologist or expert study to validate the proposed LLM‚Äëbased metrics (LLM‚ÄëRadJudge, disease‚Äëextraction). Adding such validation would greatly enhance the credibility of these new metrics.  \n- **Details of ‚Äúexpanding‚Äù and ‚Äúrefining‚Äù paradigms:** The description of how classification prompts are tokenized, how many tokens are added, and how the LLM processes them is lacking. More granular information is required for other researchers to reproduce the experiments.  \n- **Discussion of [CLS] tokens:** Figure‚ÄØ2 shows a minimal effect of adding image‚ÄØ[CLS] tokens, yet the text claims this contradicts earlier work (Kim‚ÄØ2021; Touvron‚ÄØ2021) without investigating possible reasons (e.g., token alignment, fine‚Äëtuning strategy). A deeper analysis would clarify this discrepancy.  \n- **LLM‚ÄëRadJudge discriminative power:** The RJ‚Äë1/RJ‚Äë2 scores are close to zero across all methods, offering little differentiation, and no comparison with radiologist judgments is provided. Further validation is needed to demonstrate the metric‚Äôs usefulness.  \n- **Reproducibility gaps:** The lack of publicly available code, trained checkpoints, random seeds, and exact prompt templates‚Äîespecially for the LLM‚Äëbased metrics‚Äîdoes not meet the reproducibility standards expected by the journal. Addressing these gaps would be essential for future adoption.  \n\n---  \n\n## A suggested decision  \n\nReject.  \n\nThe manuscript offers a systematic analysis of classifier influence on LLM‚Äëdriven ARRG, but it falls short in terms of novelty, statistical rigor, and reproducibility. Consequently, it does not yet meet the criteria for publication in its current form.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 5,
          "constructiveness": 4,
          "stance": 3,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript examines how diagnostic classification of radiological findings‚Äîincluding rare and long‚Äëtailed diseases‚Äîaffects automatic radiology report generation (ARRG) using large language models (LLMs). The authors define a structured design space comprising a vision encoder, classifier, and LLM, and evaluate different configurations on four datasets that encompass both in‚Äëdomain and out‚Äëof‚Äëdistribution (OOD) conditions. The study introduces two new LLM‚Äëbased evaluation metrics and provides a detailed empirical assessment of classifier‚Äëreport interactions. Overall, the paper is clearly written and systematically organized, but its methodological and statistical rigor are limited, and novelty beyond existing literature appears modest.  \n\n---\n\n**Major Comments**  \n1. **Novelty and Contribution:** While the taxonomy of LLM‚Äëbased generators and the inclusion of new LLM‚Äëderived metrics are helpful, the foundational concept of integrating classifier outputs with LLMs has prior precedents, reducing the perceived originality.  \n2. **Evaluation Design:** The experiments cover four datasets with varied metrics, yet lack statistical significance testing, confidence intervals, or variance estimates, which weakens the reliability of the reported improvements. Sample sizes for OOD datasets (PadChest and IU‚ÄëX‚ÄëRay) are small and drawn from a single split, limiting generalizability.  \n3. **Validation of Proposed Metrics:** The new metrics (LLM‚ÄëRadJudge and disease‚Äëextraction) are not supported by expert or radiologist validation, and their scores show limited discriminative capacity. Incorporating a human study would substantiate their value.  \n4. **Reproducibility Issues:** Important implementation details‚Äîrandom seeds, long‚Äëtail split definitions, prompt templates, and LLM‚ÄëRadJudge configurations‚Äîare omitted, and no code or checkpoints are released, hindering replication.  \n5. **Technical Description:** The mechanisms of ‚Äúexpanding‚Äù and ‚Äúrefining‚Äù classification prompts and the handling of [CLS] tokens are insufficiently described. The limited discussion of contradictory findings relative to prior work leaves some technical claims unexplained.  \n\n---\n\n**Minor Comments**  \n- The title is incomplete and potentially misleading about the work‚Äôs scope; a more descriptive phrasing is advised.  \n- Formatting and typographical inconsistencies (e.g., stray numbers, broken tables, repeated captions) reduce readability and should be corrected.  \n- Reporting statistical analyses (e.g., paired tests or bootstrapping) would support the claim of a correlation between classifier performance and report quality.  \n\n---\n\n**Summary Paragraph**  \nThe manuscript addresses an important question regarding the role of classification in enhancing LLM‚Äëbased report generation and is commendable for its comprehensive experimental scope and clear structural taxonomy. However, the contribution is limited by modest methodological innovation, insufficient statistical validation, lack of expert evaluation, and incomplete reproducibility information. Strengthening these aspects would be essential for a convincing demonstration of the proposed approach‚Äôs value.  \n\n---\n\n**Decision Recommendation**  \n**Reject.** The study provides informative analyses but does not yet reach the required levels of novelty, statistical support, and reproducibility for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Bo Wang",
      "Dongsheng Li",
      "Guanzhou Ke",
      "XINYANG JIANG",
      "Xiaoli Wang",
      "Xufang Luo",
      "Yang Yu",
      "Yifan Yang",
      "Zilong Wang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_d7f9b710e10bdd36261f817262f60d57d47921bf.pdf",
    "remote_url": "https://openreview.net/pdf/d7f9b710e10bdd36261f817262f60d57d47921bf.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "SeLoRA: Self-Expanding Low-Rank Adaptation of Latent Diffusion Model for Medical Image Synthesis",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to computer vision, audio, language, and other modalities"
    ],
    "keywords": [
      "Text-to-Image Synthesis",
      "Low-Rank Adaptation",
      "Medical Imaging",
      "Parameter Efficient Finetuning"
    ],
    "abstract": "The persistent challenge of medical image synthesis posed by the scarcity of annotated data and the need to synthesize \"missing modalities\" for multi-modal analysis, underscored the imperative development of effective synthesis methods. Recently, the combination of Low-Rank Adaptation (*LoRA*) with latent diffusion models (LDMs) has emerged as a viable approach for efficiently adapting pre-trained large language models, in the medical field. However, the direct application of *LoRA* assumes uniform ranking across all linear layers, overlooking the significance of different weight matrices, and leading to sub-optimal outcomes. Prior works on *LoRA* prioritize the reduction of trainable parameters, and there exists an opportunity to further tailor this adaptation process to the intricate demands of medical image synthesis. In response, we present *SeLoRA*, a Self-Expanding Low-Rank Adaptation module, that dynamically expands its ranking across layers during training, strategically placing additional ranks on crucial layers, to allow the model to elevate synthesis quality where it matters most. Our analysis shows that *SeLoRA* strikes the best balance between synthesis quality and training efficiency. The proposed method not only enables LDMs to fine-tune on medical data efficiently but also empowers the model to achieve improved image quality with minimal ranking. The code of our *SeLoRA* method is publicly available at https://anonymous.4open.science/r/SeLoRA-980D.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a new parameter-efficient fine-tuning method SeLoRA (Self-Expanding Low-Rank Adaptation) for adapting Stable Diffusion to generate chest x-ray images. The main contribution of the paper lies in dynamically expanding the rank of LoRA during the training process, allowing it to adapt the rank according to the importance of different layers and thereby improving the quality of the synthesized images. The novelty of this approach is in using Fisher Information to guide the rank expansion, avoiding the limitations of the traditional LoRA method, which uses a \"uniform rank,\" especially when dealing with models (like Stable Diffusion) that have diverse weight matrix shapes. The paper demonstrates the effectiveness of SeLoRA on two chest x-ray datasets and provides detailed comparative experiments with other LoRA variants.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. \nThe papers proposes SeLoRA, a dynamic rank-expanding method using Fisher Information to guide rank expansion during fine-tuning large models with LoRA. This is novel and more applicable to models with diverse weight matrix shapes.\n2.\nExperimental results demonstrate the effectiveness of the proposed method.\n3. \nThe paper is well-organized.\n\n### Weaknesses\n\n1.\nThe contribution is vague. As a paper focusing on adaptive parameter efficient fine-tuning methods, the paper utilizes LoRA to adapt Stable Diffusion for Chest X-ray synthesis, limiting its technical contribution. For a paper dedicated to adapting foundation model like Stable Diffusion for Chest X-ray (medical image) synthesis, the exploration is also limited and does not compare with previous work (e.g. Chambon et al., 2022a;b). Through visual comparison with image displays in Chambon et al., 2022a;b, the proposed method seems at a disadvantage.\n2.\nExperiments are conducted on relatively small datasets. Large image-report paired Chest X-ray datasets exist (e.g. MIMIC-CXR). Is this because of the heavy burden of large model like Stable Diffusion, or may be also related to the proposed method? Can the authors provide training time comparisons between SeLoRA and the compared methods? Also, as the test set of IU X-RAY and Montgomery County CXR dataset has only contains 100~200 images, the validation of the effectiveness of the method is weak.\n3.\nEvaluation and explanation are insufficient. Using a CLIP model trained purely on natural images and a maximum text token length of 76 to compute CLIP-score may not faithfully reflect how good the text-image alignment is for Chest x-ray images.\nThe training/validation/testing split is strange, could the author explain why the test set only contain 4% of the data? Are Table 1 values computed with valid data or test data? This is unclear. The paper also lacks in-depth discussion of the distribution of final rank (Figure 4,5,6) and why other LoRA methods fail on Montgomery county CXR data (Figure7). How accurate can the model generate an image given the disease or abnormal findings in the text prompt? This may be revealed using pretrained Chest X-ray classification models or manually inspect a small subset of generated results.\n\nOther non-important issues:\n4.\nthe paper title is about \"medical image syntheis‚Äù; but it only focuses on chest x-ray image.\n5.\nThe formula derivation in section3.3 is unclear.\n6.\nThe Stable Diffusion model is trained to generate a resolution of 512x512, using a resolution of 224x224 may limit the performance.\n\n### Questions\n\nPlease answer the questions mentioned in the above 'weakness' section.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *SeLoRA* (Self‚ÄëExpanding Low‚ÄëRank Adaptation), a parameter‚Äëefficient fine‚Äëtuning method designed to adapt Stable Diffusion for chest X‚Äëray image generation. The central idea is to dynamically adjust the LoRA rank during training using Fisher Information, allowing adaptive capacity allocation across layers. This approach intends to overcome limitations of conventional uniform‚Äërank LoRA, particularly for models with variable weight shapes. The study evaluates SeLoRA on two chest X‚Äëray datasets and presents comparisons with existing LoRA variants. The paper is well organized and clearly written, though its contributions and evaluation depth require clarification.  \n\n**Major Comments**  \n1. **Scope and Contribution** ‚Äì The paper‚Äôs contribution appears limited. It primarily adapts existing LoRA methodology for Stable Diffusion and applies it to chest X‚Äëray synthesis without broad exploration or deeper analysis of adaptation mechanisms. Comparisons with prior medical‚Äëdomain diffusion studies (e.g., Chambon et‚ÄØal.,‚ÄØ2022a,b) are missing; visual comparisons suggest that SeLoRA may perform less favorably.  \n2. **Dataset Scale and Validation** ‚Äì Experiments rely on small datasets (IU‚ÄØX‚ÄëRAY, Montgomery County CXR) with only about 100‚Äì200 test images, weakening empirical validation. The authors should clarify whether dataset choice was constrained by the computational cost of Stable Diffusion or by SeLoRA itself, and report training‚Äëtime comparisons with baseline methods.  \n3. **Evaluation Metrics and Experimental Design** ‚Äì The CLIP‚Äëscore, computed with a model trained on natural images and limited to 76‚Äëtoken text prompts, may not accurately assess text‚Äìimage alignment for medical imagery. The data split is unclear‚Äîwhy does the test set comprise only 4% of the data? Specify whether the results in Table‚ÄØ1 correspond to validation or test data. Discussion of rank distributions (Figures‚ÄØ4‚Äì6) and failures on Montgomery County data (Figure‚ÄØ7) should be expanded. Further, the study lacks analysis of disease‚Äëspecific synthesis accuracy, which could be assessed with pretrained chest‚ÄëX‚Äëray classifiers or manual inspection.  \n\n**Minor Comments**  \n- The title mentions ‚Äúmedical image synthesis‚Äù but all experiments concern chest X‚Äërays only.  \n- Section‚ÄØ3.3‚Äôs formula derivation is unclear.  \n- Downsampling Stable Diffusion to 224√ó224 resolution may restrict performance; discuss this design choice.  \n\n**Summary Paragraph**  \nOverall, the work presents a novel adaptive LoRA mechanism that is technically sound and clearly articulated. However, the limited experimental scope, small datasets, and incomplete evaluation weaken the evidence for SeLoRA‚Äôs advantages. Comparative context with prior medical diffusion models and clearer experimental explanations are required to substantiate the claimed benefits.  \n\n**Decision Recommendation**  \n**Major Revision.** The core idea is interesting, but the contribution, experimental validation, and evaluation criteria require further clarification and expansion.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper present SeLoRA, a Self-Expanding Low-Rank Adaptation module, that dynamically expands its ranking across layers during training. The proposed method increases the rank from 1 gradually. FI-Ratio and parameter \\lambda were used to determine when to expanding the rank.  The experiment is performed with stable diffusion and X-ray dataset and synthesis the X-ray image with text prompt.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 3\n\n### Strengths\n\nThe paper introduces a parameter-efficient method to e fine-tune stable diffusion models for generating X-ray images based on text (radiology) prompts. And the proposed method can progressive expansion in the rank of LoRA. FI-Ratio is used to guiding SeLoRA to expand its rank. The rank of different layers was given. The experiment shows the result is promisingly.\n\n### Weaknesses\n\n1. Computational overhead. While SeLoRA reduces the number of trainable parameters, its dynamic rank expansion mechanism introduces additional computational complexity. Computing Fisher information increases the overhead, which may become significant for larger datasets or more complex models.\n2. Limited dataset evaluation. Experiments were limited to two 2D X-ray datasets, with no evaluation of SeLoRA‚Äôs performance on other modalities, such as MRI or CT scans. Further validation on additional modalities would help confirm the generalizability of the method.\n3. Visual result is limited. The visual result is not satisfactory, such as in Figure 8, the contrast and details are not good.\n\n### Questions\n\n1. Computational complexity analysis A comparison of training time, memory usage and FLOPs between SeLoRA, LoRA, and other variants is needed to quantify the computational trade-offs introduced by dynamic rank expansion.\n2. Evaluation on a wider range of datasets Evaluating SeLoRA on larger datasets, such as MIMIC-CXR (containing approximately 377,000 images), would provide more insights into its scalability. Future work could also validate SeLoRA on MRI, CT, or ultrasound datasets, as testing on diverse datasets would better demonstrate its robustness and versatility.\n3. Incorporating related work. The idea of dynamically adjusting the rank of the LoRA matrix in SeLoRA is conceptually similar to the recently proposed ALoRA (NAACL 2024). However, the two methods differ in implementation: ALoRA utilizes pruning and redistribution strategies, while SeLoRA relies on Fisher information as the adjustment criterion. Insights from ALoRA could provide valuable inspiration for future improvements of SeLoRA.\n4. Evaluation with medical doctor may help verify the experiment results.\nOther questions\n1. Unconventional split of the IU X-Ray dataset. The 80:16:4 split results in a relatively small test set, which could compromise the robustness of the evaluation. A more conventional split (e.g., 80:10:10) might provide more reliable insights.\n2. Small sample size in the Montgomery County CXR dataset. With only 138 samples, the Montgomery County dataset is too small for deep learning applications, which may impact the stability and generalizability of the model‚Äôs results\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **SeLoRA (Self-Expanding Low-Rank Adaptation)**, a parameter-efficient method designed to fine-tune stable diffusion models for generating X-ray images from text prompts. The key idea is a dynamic adjustment of low-rank dimensions across network layers based on the Fisher Information Ratio (FI-Ratio) and a controlling parameter Œª, allowing the rank to increase progressively during training. Experiments are conducted using two X-ray datasets. Overall, the paper is clearly structured, and the proposed approach shows potential for improving model adaptability with reduced trainable parameters, though some aspects of evaluation and analysis remain limited.  \n\n**Major Comments**  \n1. **Computational Overhead** ‚Äì While SeLoRA aims for parameter efficiency, computing Fisher information introduces additional computational cost. The impact on time and memory is not quantified, raising concerns about scalability to larger datasets or models.  \n2. **Limited Dataset Evaluation** ‚Äì Experiments are restricted to two small 2D X-ray datasets. Validation across more diverse and larger datasets (e.g., MIMIC-CXR, MRI, CT, ultrasound) would be necessary to assess generalizability and robustness.  \n3. **Quality of Generated Images** ‚Äì Visual outputs, particularly Figure 8, show low contrast and limited detail, making the results less convincing.  \n4. **Comparative Analysis** ‚Äì The review notes the need for explicit comparisons of training time, FLOPs, and memory between SeLoRA and baselines such as LoRA and related methods like ALoRA. The conceptual distinction between SeLoRA and ALoRA should also be discussed.  \n5. **Dataset Splits and Sample Sizes** ‚Äì The unconventional 80:16:4 split for the IU X-Ray dataset reduces test set reliability. The Montgomery County dataset‚Äôs small size may also limit the validity of findings.  \n6. **External Validation** ‚Äì Involvement of medical experts could strengthen the evaluation and interpretation of generated images.  \n\n**Minor Comments**  \n- Clarify certain grammatical and stylistic issues to improve readability.  \n- Maintain consistency in terminology (e.g., rank expansion, FI-Ratio).  \n\n**Summary Paragraph**  \nThe work presents an interesting extension of LoRA through dynamic rank adaptation guided by Fisher information, showing promising preliminary results for text-to-image generation in radiology. However, the scope of evaluation, computational trade-off analysis, and quality of visual outputs need substantial improvement to support the paper‚Äôs claims of generalizability and efficiency. Enhancing experimental breadth and providing more quantitative comparisons would significantly strengthen the contribution.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces SeLoRA (Self-Expanding Low-Rank Adaptation), an extension of the LoRA (Low-Rank Adaptation) technique, designed for the fine-tuning of large diffusion models specifically in medical image synthesis. The core idea is to dynamically expand the rank of low-rank matrices during training, based on a criterion derived from Fisher information. This adaptation is applied selectively across different layers, allowing for a more effective distribution of ranks that aligns with each layer's significance in the model, particularly within the denoising U-Net of the Stable Diffusion framework.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n1. The paper is well-organized and easy to follow.\n\n2. The idea of adaptive computation rank selection is interesting and highly relevant for using pre-trained models for downstream tasks in a memory efficient fashion\n\n### Weaknesses\n\n- While the paper mainly focusses on making a more efficient LoRA design as claimed by authors, there is no Analysis of Training Efficiency in the paper.\n\n- Lack of detailed explanation of the procedure of selecting hyper-parameters needed.\n\n### Questions\n\n- How is the proposed method compared to other baselines in terms of max training GPU memory, training speed, and\ntraining time costs? An analysis of these criteria would strengthen the paper.\n\n- The paper mentions thresholds (Œª and t) for triggering rank expansion. How sensitive is the method to these hyper-parameters?\n\n- In figure 7, why are the other methods not generating similar xray images? As per my understanding, they should at-least make a similar image like figure 8. \n\n- The proposed method is just compared to methods with rank pruning methods (dylora and adalora), could you mention why there is no comparing with adaptive rank selection papers? as they seem to be a similar approach to your paper.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNo ethics review needed.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **SeLoRA (Self‚ÄëExpanding Low‚ÄëRank Adaptation)**, an extension of the LoRA method for fine‚Äëtuning large diffusion models, with a focus on medical image synthesis tasks. The key contribution is a dynamic mechanism that expands the rank of low‚Äërank matrices based on Fisher information during training, distributing ranks adaptively across layers in the denoising U‚ÄëNet of a stable diffusion model. The paper is clearly written, logically structured, and introduces an idea that is relevant for improving the efficiency of fine‚Äëtuning large pre‚Äëtrained models.\n\n**Major Comments**  \n1. **Lack of Training Efficiency Analysis:** The study emphasizes improved efficiency through adaptive rank expansion; however, it does not include concrete analyses of training efficiency metrics such as GPU memory consumption, training speed, or total computation time.  \n2. **Incomplete Hyperparameter Explanation:** The procedure for selecting and tuning the method‚Äôs hyperparameters, including the thresholds (Œª and t) used for rank expansion, is insufficiently explained. Clarifying sensitivity to these settings would strengthen reproducibility.  \n3. **Limited Baseline Comparisons:** The comparisons are restricted to rank‚Äëpruning approaches (DyLoRA, AdaLoRA) but do not include adaptive rank selection methods that appear conceptually aligned. Including this category would provide a fairer evaluation of relative advantages.  \n4. **Clarification on Generated Results:** In Figure 7, the outputs from baseline methods differ notably from those of SeLoRA, though they might be expected to produce more similar X‚Äëray images. Further explanation of this discrepancy would help interpret the results.\n\n**Minor Comments**  \n- Some parameters and procedures could be more explicitly described for reproducibility.  \n- Typographical and formatting quality are good; however, figure captions could offer more interpretive detail.\n\n**Summary Paragraph**  \nOverall, the paper introduces an appealing idea of adaptive rank expansion that aligns with current efforts to fine‚Äëtune large diffusion models efficiently. The manuscript‚Äôs organization and clarity are strengths. Its main weaknesses lie in the absence of quantitative training efficiency analysis, limited methodological comparisons, and insufficient hyperparameter transparency. Addressing these issues would considerably enhance the completeness of the experimental evaluation.\n\n**Decision Recommendation**  \n**Major Revision.** The work is promising but requires additional experimental and methodological details to substantiate its claims and contextualize its contributions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI:**  \n   ‚úÖ The manuscript presents a methodological contribution to *medical image synthesis* using diffusion-based generative models. Specifically, it introduces **SeLoRA** (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning (PEFT) mechanism for adapting latent diffusion models to medical imaging tasks. The work focuses on rank-adaptive LoRA techniques, which are methodological innovations in model adaptation rather than purely application studies. The application to X-ray and histopathology image synthesis serves to validate methodological claims. Thus, the work fits well within TMI‚Äôs scope of *imaging methods and computational modeling*.\n\n2. **Novelty & Contribution Level:**  \n   The core novelty lies in introducing a **dynamic rank-expansion strategy guided by Fisher information** to improve parameter efficiency in diffusion model adaptation. Compared with prior adaptive LoRA or rank-pruning variants (DyLoRA, AdaLoRA, ALoRA), SeLoRA‚Äôs ‚Äúself-expanding‚Äù mechanism is distinct ‚Äî it allows rank growth rather than pruning and uses a theoretically motivated Fisher Information Ratio test. While LoRA-based adaptation itself is established, the proposed incremental-learning mechanism provides a methodological increment rather than a radical advance. The contribution is thus *moderately novel but technically meaningful and well-motivated.*\n\n3. **Technical and Experimental Rigor:**  \n   The algorithmic formulation (Equations 2‚Äì5, Algorithm 1) is clearly presented, and convergence considerations are discussed (Sec.‚ÄØ3.3). The evaluation is conducted on multiple public datasets (IU X-Ray, Montgomery CXR, PatchGastricADC22) and multiple resolutions, with ablation on threshold Œª. Comparisons include LoRA, AdaLoRA, and DyLoRA baselines with careful control of parameter counts. Metrics (FID, CLIP) and figures are standard and support conclusions. However:\n   - Datasets used are relatively small; external validation beyond X-ray (e.g., CT or MR) remains limited.\n   - Statistical significance or confidence intervals for FID are minimal.\n   - Limited reporting of computational cost beyond memory/time averages (no discussion of scalability or stability).\n\n4. **Clarity and Presentation:**  \n   The manuscript is clearly structured and well-written, with sufficient mathematical detail and readable figures. Minor typographical issues (spacing, capitalization, and occasional OCR artifacts like ‚ÄúS ELF -EXPANDING‚Äù) should be corrected. Figures 3‚Äì6 are informative but somewhat dense; legends could better explain abbreviations (q,‚ÄØk,‚ÄØv,‚ÄØattn1,‚ÄØattn2).  \n\n5. **Ethical and Reproducibility Compliance:**  \n   The authors state publicly available code and use of publicly released datasets (IU‚ÄØX-Ray, Montgomery County CXR, PatchGastricADC22). No identifiable patient data are introduced, and dataset use appears consistent with public repository terms; IRB exemption would be appropriate. Reproducibility is adequately supported through provided hyperparameters and algorithm pseudocode.  \n\n---\n\n**Phase 2 ‚Äì IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThis paper presents *SeLoRA*, a Self‚ÄëExpanding Low‚ÄëRank Adaptation method for fine‚Äëtuning latent diffusion models in medical image synthesis. The approach starts with a minimal rank and dynamically expands it per layer, guided by a Fisher Information Ratio (FI‚ÄëRatio) criterion. This allows the model to allocate parameters strategically where adaptation is most beneficial. SeLoRA is applied to X‚Äëray and histopathology datasets for text‚Äëto‚Äëimage synthesis, demonstrating improved Fr√©chet Inception Distance and CLIP scores relative to LoRA, AdaLoRA, and DyLoRA while using fewer trainable parameters. The paper also analyzes rank allocation patterns and training efficiency.\n\n**2. Strengths**\n- Introduces a *methodologically motivated* dynamic rank‚Äëexpansion mechanism guided by Fisher Information‚Äînovel among LoRA variants.  \n- Demonstrates consistent improvements in synthesis quality and parameter efficiency.  \n- Offers theoretical intuition for convergence and empirical ablations across Œª thresholds.  \n- Well‚Äëstructured, reproducible implementation with code availability.  \n- Applied to multiple medical imaging datasets and modalities, suggesting general applicability.\n\n**3. Weaknesses**\n- The experimental datasets are relatively small; results may not generalize to larger or clinically diverse cohorts.  \n- The presented FID/CLIP improvements, while positive, are modest on certain datasets and lack statistical testing.  \n- Limited theoretical depth beyond heuristic Fisher scoring‚Äîno formal proof of optimality or convergence.  \n- Figures and tables sometimes contain formatting anomalies; some numerical results (e.g., FID‚ÄØ64/192 units) could be better explained.  \n- Comparison restricted to LoRA variants; missing baseline vs. full fine‚Äëtuning or non‚ÄëLoRA PEFT methods in vision‚Äëlanguage diffusion settings.\n\n**4. Major Comments**\n1. **Novelty Contextualization:** Clarify how SeLoRA‚Äôs Fisher‚Äëguided expansion differs mathematically from existing ALoRA or adaptive‚Äërank LoRA variants. A concise comparison table would help establish novelty.  \n2. **Scalability Evaluation:** Include an experiment on a larger dataset (e.g., MIMIC‚ÄëCXR subset or similar) or a 3D imaging task to strengthen claims of general applicability.  \n3. **Ablation on FI‚ÄëRatio Threshold and t:** Present quantitative curves of performance vs. parameter growth to better show the efficiency trade‚Äëoff.  \n4. **Uncertainty and Statistical Reporting:** Provide mean‚ÄØ¬±‚ÄØSD for FID/CLIP across multiple runs and confidence intervals for improvements.  \n5. **Clinical Relevance Discussion:** Expand on how synthesized images could be used in downstream clinical or training pipelines, addressing potential ethical use.\n\n**5. Minor Comments**\n- Correct typographical inconsistencies and OCR artifacts (e.g., ‚ÄúFisher information Ratio‚Äù, ‚ÄúS ELF‚ÄëEXPANDING‚Äù).  \n- Improve figure readability‚Äîespecially Figures‚ÄØ3‚Äì6‚Äîby using consistent color scales and adding legends.  \n- Ensure all dataset citations (e.g., IU‚ÄØX‚ÄëRay, Montgomery CXR, PatchGastricADC22) include proper repository URLs or DOIs.  \n- Clarify units for memory/time tables and refer consistently to ‚ÄúGPU‚Äëhours‚Äù or ‚Äúmemory‚ÄØ(GB)‚Äù.  \n- Verify that all hyperparameters (Œª,‚ÄØt,‚ÄØr) are defined upon first mention in the main text.\n\n**6. Overall Recommendation**  \n**Major‚ÄØRevision.**  \nThe paper presents a promising and well‚Äëimplemented methodological contribution within TMI‚Äôs scope. However, it requires additional evidence of generalizability and more rigorous comparative and statistical analysis to meet TMI‚Äôs standards for methodological maturity. With extended validation and stronger framing of novelty, SeLoRA could be a valuable addition to the literature on parameter‚Äëefficient adaptation for medical imaging generative models.\n\n**7. Confidence Level:** 4‚ÄØ/‚ÄØ5 ‚Äî Confident in technical assessment and TMI‚ÄØscope alignment.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *SeLoRA* (Self‚ÄëExpanding Low‚ÄëRank Adaptation), a parameter‚Äëefficient fine‚Äëtuning method for diffusion-based generative models applied to medical image synthesis. The proposed approach begins with a low-rank initialization and dynamically expands the rank of LoRA modules based on a Fisher Information Ratio criterion, allocating capacity where adaptation is most beneficial. Experiments on X‚Äëray and histopathology datasets demonstrate improved synthesis quality (FID, CLIP) relative to LoRA, AdaLoRA, and DyLoRA baselines while maintaining reduced trainable parameters. The paper is clearly organized, methodologically sound, and presents mathematical and algorithmic details with clarity.  \n\n**Major Comments**  \n1. **Novelty and Differentiation:** The self‚Äëexpanding mechanism guided by Fisher information is a meaningful technical extension over existing adaptive LoRA variants. Nevertheless, the authors should clarify mathematically how this differs from other adaptive‚Äërank methods such as ALoRA and AdaLoRA. A concise comparative table could strengthen the novelty claim.  \n2. **Evaluation Scope:** Most experiments rely on relatively small datasets (IU‚ÄØX‚ÄëRay, Montgomery‚ÄØCXR, PatchGastricADC22). Additional tests on larger or more clinically diverse datasets, or on 3D imaging modalities, would better establish scalability and robustness.  \n3. **Ablation and Efficiency Analysis:** The threshold parameters (Œª,‚ÄØt) governing rank expansion warrant a more systematic analysis (e.g., performance vs. parameter growth curves).  \n4. **Statistical Reporting:** Quantitative results (FID, CLIP) should include multiple runs with mean‚ÄØ¬±‚ÄØstandard deviation or confidence intervals to assess statistical significance.  \n5. **Clinical and Ethical Context:** The discussion could further address potential downstream uses of the synthesized images and relevant ethical considerations.  \n\n**Minor Comments**  \n- Correct typographical inconsistencies and OCR artifacts (e.g., spacing and capitalization of ‚ÄúSelf‚ÄëExpanding‚Äù).  \n- Improve figure readability and ensure legends clearly define abbreviations.  \n- Include dataset source references (URLs or DOIs) where appropriate.  \n- Clarify units for memory and time comparisons and maintain consistent notation for parameters (Œª,‚ÄØt,‚ÄØr).  \n\n**Summary Paragraph**  \nThis work delivers a technically motivated and reproducible approach to adaptive low‚Äërank fine‚Äëtuning for medical diffusion models. Its main strengths are methodological clarity, theoretical grounding in Fisher information, and consistent empirical improvements across multiple datasets. However, the contribution remains incremental rather than transformative, with limited experimental breadth and incomplete statistical validation. Clarifying novelty, expanding evaluation, and refining presentation would significantly strengthen the manuscript.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper is promising and well-executed but requires broader validation and stronger evidence of generalizability and novelty before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents SeLoRA (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning method for adapting latent diffusion models to medical image synthesis. The authors address the limitation that standard LoRA applies uniform ranking across all linear layers, which may be suboptimal for the diverse weight matrix shapes in diffusion models' U-Net architecture. SeLoRA dynamically expands ranks during training using Fisher information as a guide, starting from rank r=1 and strategically placing additional ranks on crucial layers. The method is evaluated on chest X-ray datasets (IU X-RAY and Montgomery County CXR) for text-conditioned medical image synthesis. Results show that SeLoRA achieves superior FID scores and CLIP scores compared to standard LoRA, AdaLoRA, and DyLoRA while using fewer trainable parameters (Section 4, Tables 1-2). The rank allocation analysis demonstrates that SeLoRA concentrates higher ranks on attention layers where text and image embeddings interact (Figures 3-4).\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and clarity issues**\n  - The FI-Ratio definition in Equation 5 uses inconsistent notation where the denominator should logically be larger than numerator for meaningful interpretation, but the paper suggests expansion when FI-Ratio ‚â• Œª > 1, creating conceptual confusion about what constitutes \"improvement\"\n  - Algorithm 1 lacks mathematical precision in the expansion condition, particularly in how the FI-Scores are computed for the expanded matrices A‚Ä≤ and B‚Ä≤ without parameter updates\n  - The convergence analysis in Section 3.3 provides only intuitive arguments without formal mathematical proof, and the approximation of FI-Ratio lacks rigorous derivation of the assumed relationship between rank and Fisher information\n\n‚Ä¢ **Limited experimental validation and dataset scope**\n  - Evaluation is restricted to only two small chest X-ray datasets (3,955 and 138 images respectively as noted in Section 3.4), which limits generalizability claims for medical image synthesis\n  - No comparison with full fine-tuning baselines or other parameter-efficient methods beyond LoRA variants, missing important performance context for the claimed efficiency gains\n  - The PatchGastricADC22 results in Appendix show SeLoRA performing worse than LoRA on the primary FID metric (Table 6), contradicting the main narrative of consistent superiority\n  - Statistical significance testing is absent despite reporting standard deviations in Tables 1-2, making it unclear whether observed differences are statistically meaningful\n\n‚Ä¢ **Methodological concerns and incomplete analysis**\n  - The Fisher information calculation in Equation 3 uses empirical estimation over single batches, which may be noisy and unstable for reliable expansion decisions during training\n  - Hyperparameter selection (Œª=1.1, t=40) appears arbitrary with insufficient justification or sensitivity analysis beyond the limited ablation in Table 3\n  - The rank expansion strategy lacks theoretical grounding for why Fisher information should be the optimal criterion for determining when to expand, and no comparison with alternative expansion criteria is provided\n  - Missing analysis of computational overhead during training, particularly the cost of repeatedly computing FI-Scores every t steps for expansion testing\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and strengthen mathematical foundations**\n  - Reformulate the FI-Ratio definition in Equation 5 with clear explanation of why this specific ratio indicates improvement, and provide formal mathematical justification for the expansion criterion\n  - Add rigorous algorithmic details in Algorithm 1 specifying exactly how FI-Scores are computed for expanded matrices, including gradient computation procedures without parameter updates\n  - Develop formal convergence proof for the rank expansion algorithm with clear bounds, and derive the FI-Ratio approximation in Section 3.3 with explicit assumptions and mathematical steps\n\n‚Ä¢ **Expand experimental validation comprehensively**\n  - Include evaluation on larger and more diverse medical imaging datasets beyond chest X-rays to demonstrate broader applicability across medical imaging modalities and scales\n  - Add comparisons with full fine-tuning and other PEFT methods (e.g., adapters, prompt tuning) to provide complete performance context and justify the focus on LoRA-based approaches\n  - Conduct statistical significance testing for all reported metrics and provide detailed analysis of why SeLoRA underperforms on PatchGastricADC22, addressing apparent contradictions in performance claims\n  - Report comprehensive computational analysis including training time, memory usage, and FLOPs comparison across all methods with proper statistical analysis\n\n‚Ä¢ **Strengthen methodological rigor and theoretical justification**\n  - Investigate stability of Fisher information estimation and propose more robust alternatives, including analysis of how batch size and gradient noise affect expansion decisions\n  - Conduct thorough hyperparameter sensitivity analysis across multiple datasets and provide principled guidelines for selecting Œª and t values based on dataset characteristics\n  - Compare Fisher information criterion with alternative expansion strategies (e.g., gradient magnitude, loss-based criteria) and provide theoretical analysis of why Fisher information is optimal for this task\n  - Add detailed computational cost analysis of the expansion testing procedure and propose efficiency improvements to reduce the overhead of frequent FI-Score computation",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **SeLoRA (Self-Expanding Low-Rank Adaptation)**, a parameter-efficient fine-tuning approach for adapting latent diffusion models to medical image synthesis. The method aims to overcome the limitation of uniform rank assignment in standard LoRA by dynamically expanding rank allocations across layers, guided by Fisher information. Experiments conducted on two chest X-ray datasets (IU X-RAY and Montgomery County CXR) show improved FID and CLIP scores over LoRA variants while using fewer trainable parameters. The analysis further indicates that SeLoRA adaptively allocates higher ranks to attention layers where text‚Äìimage interactions occur. While the paper presents a clear motivation and promising empirical results, several theoretical, methodological, and evaluative aspects require clarification and elaboration.  \n\n**Major Comments**  \n1. **Mathematical formulation and clarity** ‚Äì The FI-Ratio in Equation 5 shows inconsistent notation, making the expansion condition conceptually unclear (the ratio‚Äôs interpretation appears inverted relative to expectations). Algorithm¬†1 lacks precision in describing how FI-Scores are computed for expanded matrices without updates. The convergence analysis in Section¬†3.3 relies on intuition without formal proof, and the approximation of the FI-Ratio lacks a rigorous derivation.  \n2. **Limited experimental validation** ‚Äì Evaluation is confined to two small chest X-ray datasets, restricting claims of generalizability. The study omits comparisons with full fine-tuning and non-LoRA adaptive methods. Results on PatchGastricADC22 (Appendix) show SeLoRA underperforming LoRA, contradicting claims of consistent improvement. Statistical significance testing is absent despite reported variances.  \n3. **Methodological concerns** ‚Äì Fisher information is estimated from single batches, potentially unstable for reliable expansion. Hyperparameter selection (Œª¬†=¬†1.1,¬†t¬†=¬†40) appears arbitrary without adequate analysis. The Fisher criterion‚Äôs suitability for rank expansion lacks theoretical justification or comparison with other criteria. Computational overhead of frequent FI-Score computation is unquantified.  \n\n**Minor Comments**  \n- Clarify notation in equations, particularly in Equation¬†5 and Algorithm¬†1.  \n- Provide explicit gradient computation steps for expanded parameters.  \n- Improve figure captions (e.g., Figures¬†3‚Äì4) for self-contained interpretation.  \n\n**Summary Paragraph**  \nOverall, the work is well-motivated and potentially significant for efficient adaptation of diffusion models. Strengths include the dynamic rank-expansion idea and empirical demonstration of parameter efficiency. However, the paper‚Äôs impact is limited by unclear mathematical formulation, narrow evaluation scope, and insufficient theoretical grounding of the Fisher information criterion. Strengthening the mathematical justification, expanding datasets and baselines, and adding rigorous statistical and computational analyses would substantially improve clarity and credibility.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces SeLoRA (Self-Expanding Low-Rank Adaptation), a method designed to fine-tune latent diffusion models (LDMs) for medical image synthesis with minimal trainable parameters. SeLoRA dynamically adjusts the rank of the low-rank decomposition matrices during training, guided by Fisher information ratios (FI-Ratios), allowing for more efficient and adaptive fine-tuning. The authors demonstrate the efficacy of SeLoRA on two datasets, IU X-Ray and Montgomery County CXR, showcasing improved synthesis quality and reduced parameter counts compared to baseline methods. The manuscript is well-written and provides a thorough analysis of the proposed approach.\n\n###\n\n## Major Comments\n1. Novelty and Positioning:\n   - Strength: The introduction of SeLoRA offers a unique approach to rank adjustment, leveraging Fisher information to guide rank expansion dynamically. This is a novel contribution that distinguishes the work from existing methods.\n   - Weakness: The manuscript could benefit from a more detailed comparison with other recent rank-adaptive methods, particularly those that also leverage Fisher information for rank selection. This would help clarify the unique contributions and improvements offered by SeLoRA.\n\n2. Evaluation Design:\n   - Strength: The evaluation covers multiple metrics (FID, CLIP Score) and includes both quantitative and qualitative analyses, providing a comprehensive assessment of SeLoRA's performance.\n   - Weakness: The evaluation is limited to two datasets, both of which are relatively small. Including a more diverse set of datasets, including larger and more complex medical imaging datasets, would strengthen the validation of SeLoRA's effectiveness.\n\n3. Comparisons:\n   - Strength: The manuscript compares SeLoRA against several relevant methods (LoRA, DyLoRA, AdaLoRA), demonstrating consistent improvements across different metrics.\n   - Weakness: The baseline comparisons lack a more extensive set of recent methods that also aim to optimize parameter efficiency. Including these would provide a more robust benchmark against which SeLoRA can be evaluated.\n\n4. Reproducibility:\n   - Strength: The authors state that the code will be made publicly available, which is a positive step towards reproducibility.\n   - Weakness: The methodological details, particularly regarding the training protocols and hyperparameters, are somewhat sparse. Providing more explicit details, such as the full training configurations and any preprocessing steps, would significantly enhance the reproducibility of the results.\n\n###\n\n## Minor Comments\n1. Figures:\n   - Figure 2 and Figure 9 could be improved by providing more representative samples and zoomed-in regions to better illustrate the differences between methods.\n   \n2. Notation and Terminology:\n   - The notation used for the forward operator and other mathematical expressions should be consistently defined and explained.\n   \n3. Acronyms:\n   - Several acronyms (e.g., FID, CLIP) are used without definition. Adding a glossary or defining these terms in the text would improve clarity.\n   \n4. Typographical Issues:\n   - There are minor typographical errors such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). These should be corrected.\n\n###\n\n## Summary Paragraph\n(focusing on 4 TMI Editorial Criteria)\n\nThe manuscript addresses the significant challenge of medical image synthesis, particularly in the context of limited annotated data. SeLoRA introduces a novel and innovative approach to parameter-efficient fine-tuning of LDMs by dynamically adjusting the rank of low-rank decomposition matrices. The evaluation, while comprehensive, could be strengthened by including a broader range of datasets and more recent comparative methods. The reproducibility of the method is somewhat compromised by the lack of detailed training protocols and hyperparameters. Overall, while SeLoRA shows promise, the current evidence falls short of fully meeting the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis, broaden the validation across a wider range of datasets, and provide more detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **SeLoRA (Self-Expanding Low-Rank Adaptation)**, a method for parameter-efficient fine-tuning of latent diffusion models (LDMs) in medical image synthesis. SeLoRA dynamically expands low-rank decomposition matrices during training, guided by Fisher information ratios (FI-Ratios), allowing the model to adaptively adjust its representational capacity. The approach is evaluated on the IU X-Ray and Montgomery County CXR datasets, demonstrating improvements in synthesis quality and parameter efficiency compared to existing methods. The paper is clearly written and provides a solid analysis of the proposed technique.  \n\n**Major Comments**  \n1. **Novelty and Positioning**  \n   - *Strengths*: The concept of using FI-Ratios to guide adaptive rank expansion is innovative and differentiates SeLoRA from existing low-rank adaptation methods.  \n   - *Weaknesses*: The positioning could be strengthened through a more detailed comparison with other rank-adaptive or Fisher information‚Äìbased approaches, clarifying the distinct contributions of SeLoRA relative to prior work.  \n\n2. **Evaluation Design**  \n   - *Strengths*: Evaluation across multiple quantitative (FID, CLIP Score) and qualitative metrics provides a well-rounded assessment of model performance.  \n   - *Weaknesses*: Validation is restricted to two relatively small datasets. Including additional, larger, or more diverse datasets would better substantiate the claimed generalizability of SeLoRA.  \n\n3. **Comparisons**  \n   - *Strengths*: Comparisons with LoRA, DyLoRA, and AdaLoRA show consistent performance gains.  \n   - *Weaknesses*: Broader inclusion of recent parameter-efficiency methods would lead to more robust benchmarking and contextualize the improvements more convincingly.  \n\n4. **Reproducibility**  \n   - *Strengths*: Public code release enhances reproducibility prospects.  \n   - *Weaknesses*: Insufficient methodological detail‚Äîparticularly regarding training configurations, hyperparameters, and preprocessing‚Äîlimits complete reproducibility. Providing fuller experimental specifications would strengthen transparency.  \n\n**Minor Comments**  \n1. Figures 2 and 9 would benefit from clearer, zoomed-in visualizations highlighting differences across methods.  \n2. Mathematical notation, especially for the forward operator, should be standardized and consistently defined.  \n3. Acronyms such as FID and CLIP should be defined upon first use or included in a glossary.  \n4. Minor typographical issues (e.g., ‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù) should be corrected.  \n\n**Summary Paragraph**  \nThis study tackles parameter-efficient fine-tuning in medical image synthesis, introducing a novel adaptive-rank mechanism for LDMs. SeLoRA demonstrates promising results but would benefit from wider empirical validation, expanded methodological clarification, and more extensive comparisons with recent alternatives. The contribution is technically sound and potentially impactful but requires additional evidence and transparency to meet publication standards.  \n\n**Decision Recommendation**  \n**Major Revision.** The authors should broaden dataset coverage, enhance comparative analysis, and expand methodological details to improve reproducibility and strengthen the manuscript‚Äôs evidential base.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## SeLoRA: SELF-EXPANDING LOW-RANK ADAPTATION OF LATENT DIFFUSION MODEL FOR MEDICAL IMAGE SYNTHESIS\n\n### Summary\n\nThis paper proposes SeLoRA, a self-expanding Low-Rank Adaptation mechanism for fine-tuning Latent Diffusion Models (Stable Diffusion v1-5) in medical image synthesis. Instead of fixing a uniform rank across layers, SeLoRA starts with rank 1 per adapter and periodically decides whether to expand rank based on a Fisher Information (FI) ratio computed between an ‚Äúoriginal‚Äù and an ‚Äúexpanded‚Äù adapter configuration, aiming to concentrate capacity on layers where it is most beneficial. Experiments on IU X-Ray, Montgomery County CXR, and a histopathology patch dataset suggest SeLoRA can match or improve fidelity and text‚Äìimage alignment with fewer trainable parameters than static LoRA and pruning-based variants.\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a simple, modular rank-expansion mechanism that operates during training, avoiding a-priori rank selection and enabling per-layer non-uniform rank allocation.Uses an FI-based criterion to gate rank growth, aligning expansion decisions with a signal of parameter informativeness.Proposes an output-preserving expansion scheme by appending a new column to A and a zero row to B, keeping current predictions unchanged at the expansion moment.\n- Experimental rigor and validationCompares against relevant LoRA variants (LoRA, DyLoRA, AdaLoRA) on multiple datasets and resolutions.Reports performance across multiple FID feature dimensions (including intermediate Inception features) and CLIP score, with results aggregated over three seeds.Provides qualitative analyses and heatmaps showing where rank is concentrated (notably in cross-attention q/k), which is consistent with intuition about conditioning.\n- Clarity of presentationThe high-level motivation and the training loop (Algorithm 1) are easy to follow.The ablation on Œª offers some insight into the aggressiveness‚Äìefficiency trade-off.Rank allocation visualizations help interpret what the method is learning.\n- Significance of contributionsAddresses a practical pain point for PEFT on diffusion models: non-uniform, layer-wise rank needs.If robust, the approach could be useful beyond medical synthesis (e.g., other conditional diffusion tasks) and complementary to orthogonal PEFT advances (quantization, Kroneckerized adapters, etc.).\n\n- Introduces a simple, modular rank-expansion mechanism that operates during training, avoiding a-priori rank selection and enabling per-layer non-uniform rank allocation.\n- Uses an FI-based criterion to gate rank growth, aligning expansion decisions with a signal of parameter informativeness.\n- Proposes an output-preserving expansion scheme by appending a new column to A and a zero row to B, keeping current predictions unchanged at the expansion moment.\n\n- Compares against relevant LoRA variants (LoRA, DyLoRA, AdaLoRA) on multiple datasets and resolutions.\n- Reports performance across multiple FID feature dimensions (including intermediate Inception features) and CLIP score, with results aggregated over three seeds.\n- Provides qualitative analyses and heatmaps showing where rank is concentrated (notably in cross-attention q/k), which is consistent with intuition about conditioning.\n\n- The high-level motivation and the training loop (Algorithm 1) are easy to follow.\n- The ablation on Œª offers some insight into the aggressiveness‚Äìefficiency trade-off.\n- Rank allocation visualizations help interpret what the method is learning.\n\n- Addresses a practical pain point for PEFT on diffusion models: non-uniform, layer-wise rank needs.\n- If robust, the approach could be useful beyond medical synthesis (e.g., other conditional diffusion tasks) and complementary to orthogonal PEFT advances (quantization, Kroneckerized adapters, etc.).\n\n### Weaknesses\n\n- Technical limitations or concernsThe FI-Ratio definition and gate condition appear internally inconsistent across the text and the convergence discussion; the sign and threshold interpretation likely contain errors.With Bnew initialized to zero and K random, gradients flow to Bnew but not to K at the decision step; thus FI for the added ‚ÄúA-side‚Äù component is initially zero, biasing FI comparisons and potentially inducing systematic expansion regardless of utility.The convergence ‚Äúupper bound‚Äù argument is heuristic and relies on an FI-Ratio that seems algebraically inverted relative to earlier definitions.\n- Experimental gaps or methodological issuesEvaluation on very small datasets (IU X-Ray ‚âà4k items, Montgomery 138 images) limits generality; stronger validation on MIMIC-CXR or larger medical corpora is needed.FID with ImageNet Inception features and generic CLIP score are known to be suboptimal for medical imaging; domain-specific metrics (e.g., XRV-FID, CheXpert-based AUROC alignment, RadGraph metrics) and radiologist ratings would strengthen claims.Fairness of parameter budget comparisons is unclear when Œª=1 yields large ranks (heatmaps show ranks >20‚Äì40 in some modules). Report exact trainable parameter counts per setting and ensure baselines are matched to the final SeLoRA parameter budget.No baselines against non-LoRA PEFT that performed well on diffusion (e.g., Bias Tuning, SV-Diff, DiffFit) and no recent adaptive-rank methods with expansion (ALoRA) or combined prune+expand (e.g., ElaLoRA, PrunedLoRA) on this task.Overhead of periodic FI evaluation (extra forward/backward) is not profiled in wall-clock time; only GPU memory is reported.\n- Clarity or presentation issuesConflicting statements about the FI-Ratio threshold (‚â• vs ‚â§) and its interpretation (Œª=1 ‚Äúaccepts any improvement‚Äù) make the core decision rule ambiguous.Some figures show unexpected colorized ‚ÄúX-ray‚Äù outputs for certain baselines, raising concerns about consistent preprocessing/decoding and fairness of qualitative comparisons.Data splits vary across sections (80/16/4 vs 70/10/20), and the code link is referenced but not provided.\n- Missing related work or comparisonsLacks experimental comparisons to: RoentGen and Chest-Diffusion (domain-adapted diffusion pipelines), and adaptive/pruning+expansion LoRA methods (ALoRA, ElaLoRA, SubLoRA, PrunedLoRA, FLoE). Discussion mentions some but empirical positioning is limited.\n\n- The FI-Ratio definition and gate condition appear internally inconsistent across the text and the convergence discussion; the sign and threshold interpretation likely contain errors.\n- With Bnew initialized to zero and K random, gradients flow to Bnew but not to K at the decision step; thus FI for the added ‚ÄúA-side‚Äù component is initially zero, biasing FI comparisons and potentially inducing systematic expansion regardless of utility.\n- The convergence ‚Äúupper bound‚Äù argument is heuristic and relies on an FI-Ratio that seems algebraically inverted relative to earlier definitions.\n\n- Evaluation on very small datasets (IU X-Ray ‚âà4k items, Montgomery 138 images) limits generality; stronger validation on MIMIC-CXR or larger medical corpora is needed.\n- FID with ImageNet Inception features and generic CLIP score are known to be suboptimal for medical imaging; domain-specific metrics (e.g., XRV-FID, CheXpert-based AUROC alignment, RadGraph metrics) and radiologist ratings would strengthen claims.\n- Fairness of parameter budget comparisons is unclear when Œª=1 yields large ranks (heatmaps show ranks >20‚Äì40 in some modules). Report exact trainable parameter counts per setting and ensure baselines are matched to the final SeLoRA parameter budget.\n- No baselines against non-LoRA PEFT that performed well on diffusion (e.g., Bias Tuning, SV-Diff, DiffFit) and no recent adaptive-rank methods with expansion (ALoRA) or combined prune+expand (e.g., ElaLoRA, PrunedLoRA) on this task.\n- Overhead of periodic FI evaluation (extra forward/backward) is not profiled in wall-clock time; only GPU memory is reported.\n\n- Conflicting statements about the FI-Ratio threshold (‚â• vs ‚â§) and its interpretation (Œª=1 ‚Äúaccepts any improvement‚Äù) make the core decision rule ambiguous.\n- Some figures show unexpected colorized ‚ÄúX-ray‚Äù outputs for certain baselines, raising concerns about consistent preprocessing/decoding and fairness of qualitative comparisons.\n- Data splits vary across sections (80/16/4 vs 70/10/20), and the code link is referenced but not provided.\n\n- Lacks experimental comparisons to: RoentGen and Chest-Diffusion (domain-adapted diffusion pipelines), and adaptive/pruning+expansion LoRA methods (ALoRA, ElaLoRA, SubLoRA, PrunedLoRA, FLoE). Discussion mentions some but empirical positioning is limited.\n\n### Detailed Comments\n\n- Technical soundness evaluationFI-Ratio definition and gate: Equation (5) defines FI-Ratio = FI-Scoreorig / FI-Scoreexp, yet the text states ‚Äúexpand when FI-Ratio ‚â• Œª,‚Äù and also that ‚ÄúŒª=1 accepts any improvement.‚Äù If FIexp > FIorig signals ‚Äúimprovement,‚Äù then FI-Ratio < 1 should trigger expansion, not ‚â•. The convergence analysis then uses a formula aligned with exp/orig (not orig/exp). Please reconcile these inconsistencies and provide a formal derivation for the chosen form and inequality direction.Gradient flow at expansion: With A‚Äô = [A K] and B‚Äô = [B; 0], the output is unchanged, but the gradient w.r.t. K at the decision step is zero (because Bnew=0), whereas gradient w.r.t. Bnew can be non-zero (depends on xK). Consequently, the ‚Äúexpanded FI-Score‚Äù may understate the value of expanding A while overstating B-side utility, biasing the decision. Consider initializing Bnew to a small Œµ and K scaled by 1/Œµ to keep AB contribution ‚âà0 while allowing non-trivial gradients to both sides, or adopt a symmetric tiny-noise initialization for both A and B with explicit output compensation.Fisher information estimation: Empirical, diagonal FI can be noisy at small batch sizes and sensitive to scale. Without smoothing (e.g., EMA over steps) or normalization by parameter count, the comparison across different-sized layers and across time may be unstable. The paper would benefit from specifying normalization, smoothing strategies, and sensitivity analyses to batch size and t.Convergence argument: The proportionality derivation for FI-Ratio depends on average FI per parameter and ignores cross-parameter interactions; it also relies on a ratio that appears inverted relative to Eq. (5). As it stands, it is heuristic. A more careful analysis or empirical convergence evidence (rank trajectories and final rank stability under multiple seeds) would be more convincing.\n- Experimental evaluation assessmentMetrics: FID based on ImageNet Inception and CLIP score are weak proxies in medical imaging. Consider domain-specific metrics used in prior work: (i) FID with in-domain encoders (e.g., DenseNet/XRV features), (ii) pathology alignment via CheXpert/XRV AUROC on synthetic images conditioned on labels, (iii) RadGraph or factENT-based text‚Äìimage alignment, and (iv) radiologist blinded rating for realism/alignment.Data scale and representativeness: Results on Montgomery County CXR (n=138) can be highly variable; IU X-Ray is relatively small and heterogeneous. Stronger validation on MIMIC-CXR (as in RoentGen and Chest-Diffusion) would better support generalization claims.Fairness of parameter budgets: You report that at Œª=1.1 SeLoRA uses fewer tunables than LoRA r=4, which is encouraging. However, the best scores in Table 3 (FID=76.4 at Œª=1) likely use much larger total rank. Please report for every experiment: total trainable parameters, average/final rank per module group (e.g., text enc, U-Net), and show performance as a function of parameter budget.Overhead and efficiency: The periodic FI testing implies extra computation. Report wall-clock training time and overhead percentage relative to LoRA, and confirm that inference-time cost (due to increased rank) remains acceptable.Qualitative consistency: Some qualitative images appear colorized for baselines but grayscale for SeLoRA. Ensure uniform postprocessing (VAE decoding, grayscale conversion, windowing) across all methods to enable fair visual comparisons.\n- Comparison with related work (using the summaries provided)RoentGen and Chest-Diffusion show that domain-adapted pipelines (larger-scale data, domain text encoders, transformer denoisers) significantly improve report-to-CXR generation. Position SeLoRA as orthogonal (PEFT) and, if possible, test SeLoRA within such pipelines to demonstrate additive benefits.Adaptive LoRA methods such as ALoRA (expansion), ElaLoRA (prune+expand with orthogonality), SubLoRA (second-order submodular pruning), and PrunedLoRA (second-order structured pruning) present alternative ways to allocate rank. Given the conceptual overlap, an empirical comparison on at least one dataset would better situate SeLoRA. Similarly, FLoE‚Äôs FI-based layer selection is related in spirit; discuss differences in FI use (per-layer gating vs constrained selection) and potential synergies.For diffusion, prior benchmarks suggest LoRA may be suboptimal relative to bias tuning, SV-Diff, or DiffFit. Including these baselines would help establish that SeLoRA improves not just over LoRA variants but also over the best-known PEFT baselines for diffusion.\n- Discussion of broader impact and significanceGenerating medical images raises safety, privacy, and downstream bias concerns. It would help to discuss safeguards (e.g., watermarking synthetic images), intended uses (data augmentation vs. clinical decision support), and the risks of hallucinated findings or device artifacts. Incorporating radiologist evaluation would provide practical insight into clinical viability.The idea of dynamic rank allocation guided by information-theoretic signals is promising beyond medical imaging; a brief note on generalization to other modalities and tasks could broaden impact.\n\n- FI-Ratio definition and gate: Equation (5) defines FI-Ratio = FI-Scoreorig / FI-Scoreexp, yet the text states ‚Äúexpand when FI-Ratio ‚â• Œª,‚Äù and also that ‚ÄúŒª=1 accepts any improvement.‚Äù If FIexp > FIorig signals ‚Äúimprovement,‚Äù then FI-Ratio < 1 should trigger expansion, not ‚â•. The convergence analysis then uses a formula aligned with exp/orig (not orig/exp). Please reconcile these inconsistencies and provide a formal derivation for the chosen form and inequality direction.\n- Gradient flow at expansion: With A‚Äô = [A K] and B‚Äô = [B; 0], the output is unchanged, but the gradient w.r.t. K at the decision step is zero (because Bnew=0), whereas gradient w.r.t. Bnew can be non-zero (depends on xK). Consequently, the ‚Äúexpanded FI-Score‚Äù may understate the value of expanding A while overstating B-side utility, biasing the decision. Consider initializing Bnew to a small Œµ and K scaled by 1/Œµ to keep AB contribution ‚âà0 while allowing non-trivial gradients to both sides, or adopt a symmetric tiny-noise initialization for both A and B with explicit output compensation.\n- Fisher information estimation: Empirical, diagonal FI can be noisy at small batch sizes and sensitive to scale. Without smoothing (e.g., EMA over steps) or normalization by parameter count, the comparison across different-sized layers and across time may be unstable. The paper would benefit from specifying normalization, smoothing strategies, and sensitivity analyses to batch size and t.\n- Convergence argument: The proportionality derivation for FI-Ratio depends on average FI per parameter and ignores cross-parameter interactions; it also relies on a ratio that appears inverted relative to Eq. (5). As it stands, it is heuristic. A more careful analysis or empirical convergence evidence (rank trajectories and final rank stability under multiple seeds) would be more convincing.\n\n- Metrics: FID based on ImageNet Inception and CLIP score are weak proxies in medical imaging. Consider domain-specific metrics used in prior work: (i) FID with in-domain encoders (e.g., DenseNet/XRV features), (ii) pathology alignment via CheXpert/XRV AUROC on synthetic images conditioned on labels, (iii) RadGraph or factENT-based text‚Äìimage alignment, and (iv) radiologist blinded rating for realism/alignment.\n- Data scale and representativeness: Results on Montgomery County CXR (n=138) can be highly variable; IU X-Ray is relatively small and heterogeneous. Stronger validation on MIMIC-CXR (as in RoentGen and Chest-Diffusion) would better support generalization claims.\n- Fairness of parameter budgets: You report that at Œª=1.1 SeLoRA uses fewer tunables than LoRA r=4, which is encouraging. However, the best scores in Table 3 (FID=76.4 at Œª=1) likely use much larger total rank. Please report for every experiment: total trainable parameters, average/final rank per module group (e.g., text enc, U-Net), and show performance as a function of parameter budget.\n- Overhead and efficiency: The periodic FI testing implies extra computation. Report wall-clock training time and overhead percentage relative to LoRA, and confirm that inference-time cost (due to increased rank) remains acceptable.\n- Qualitative consistency: Some qualitative images appear colorized for baselines but grayscale for SeLoRA. Ensure uniform postprocessing (VAE decoding, grayscale conversion, windowing) across all methods to enable fair visual comparisons.\n\n- RoentGen and Chest-Diffusion show that domain-adapted pipelines (larger-scale data, domain text encoders, transformer denoisers) significantly improve report-to-CXR generation. Position SeLoRA as orthogonal (PEFT) and, if possible, test SeLoRA within such pipelines to demonstrate additive benefits.\n- Adaptive LoRA methods such as ALoRA (expansion), ElaLoRA (prune+expand with orthogonality), SubLoRA (second-order submodular pruning), and PrunedLoRA (second-order structured pruning) present alternative ways to allocate rank. Given the conceptual overlap, an empirical comparison on at least one dataset would better situate SeLoRA. Similarly, FLoE‚Äôs FI-based layer selection is related in spirit; discuss differences in FI use (per-layer gating vs constrained selection) and potential synergies.\n- For diffusion, prior benchmarks suggest LoRA may be suboptimal relative to bias tuning, SV-Diff, or DiffFit. Including these baselines would help establish that SeLoRA improves not just over LoRA variants but also over the best-known PEFT baselines for diffusion.\n\n- Generating medical images raises safety, privacy, and downstream bias concerns. It would help to discuss safeguards (e.g., watermarking synthetic images), intended uses (data augmentation vs. clinical decision support), and the risks of hallucinated findings or device artifacts. Incorporating radiologist evaluation would provide practical insight into clinical viability.\n- The idea of dynamic rank allocation guided by information-theoretic signals is promising beyond medical imaging; a brief note on generalization to other modalities and tasks could broaden impact.\n\n### Questions for Authors\n\n- Please clarify the FI-Ratio definition and the expansion condition: is it FI-Ratio = FIorig / FIexp or the inverse, and do you expand when the ratio is ‚â§ Œª or ‚â• Œª? How does this align with the statement that ‚ÄúŒª=1 accepts any improvement‚Äù and with the convergence analysis formula?\n- With Bnew initialized to zero and K random, gradients at the expansion test time do not flow to K, only to Bnew. How do you ensure FIexp reflects the value of adding a new A-direction? Would a small non-zero initialization or compensated scaling for both A and B be more appropriate?\n- What is the exact computational overhead (wall-clock time per epoch and total training time) introduced by periodic FI testing across all modules compared to LoRA? How does it scale with t and model size?\n- For every reported experiment, could you provide the final total number of trainable parameters and the average/final rank distribution per module group (text encoder vs. U-Net), and confirm that baselines are matched to SeLoRA‚Äôs final budget?\n- How many images were generated for FID and CLIP score evaluations per test set and per seed? What sampling settings (number of diffusion steps, guidance scale) and random seeds were used? Were generation and FID calculation identical across methods?\n- Given known limitations of Inception-FID and generic CLIP in medical imaging, can you report in-domain metrics (e.g., XRV-FID, CheXpert AUROCs, RadGraph) or expert evaluations to corroborate improvements?\n- Figures show some colorized ‚ÄúX-ray‚Äù outputs for baselines. Are all methods using the same decoding and grayscale conversion? Could preprocessing differences explain some qualitative discrepancies?\n- How sensitive is SeLoRA to t (testing interval) and batch size (FI stability)? Do you use any EMA or normalization of FI across layers to reduce noise?\n- Could you release the code (the paper references ‚Äúthis link‚Äù but no URL is provided), data splits, and scripts to compute FI and perform expansion decisions for reproducibility?\n- Have you considered combining SeLoRA with pruning (e.g., adding a mechanism to remove unhelpful ranks) or comparing against expansion-capable baselines like ALoRA/ElaLoRA on at least one dataset?\n\n### Overall Assessment\n\nSeLoRA addresses a practical limitation of fixed-rank LoRA by proposing a training-time, FI-guided expansion strategy that adaptively allocates rank where it matters most. The idea is appealing, the implementation appears simple, and the qualitative rank heatmaps support the intuition that cross-attention and text-encoder q/k benefit from more capacity. However, key methodological details (FI-Ratio definition and gating condition, gradient flow through the expanded components, and the convergence argument) are currently ambiguous or internally inconsistent. Experimentally, improvements on the smallest datasets are promising but would be much more compelling with (i) domain-appropriate evaluation metrics and expert review, (ii) larger-scale validation (e.g., MIMIC-CXR), (iii) matched-parameter comparisons throughout, (iv) runtime overhead reporting, and (v) comparisons to the strongest PEFT baselines for diffusion and to recent dynamic LoRA methods that include expansion. Addressing these points would materially strengthen the paper and better position it for a top-tier venue.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **SeLoRA**, a self-expanding low-rank adaptation mechanism for fine-tuning latent diffusion models, specifically applied to medical image synthesis. The method dynamically allocates rank during training using a Fisher Information (FI)-based criterion, enabling per-layer non-uniform expansion rather than fixed-rank adaptation. Experiments on several medical imaging datasets (IU X-Ray, Montgomery CXR, histopathology patches) indicate that SeLoRA can achieve comparable or improved fidelity and text‚Äìimage alignment with fewer trainable parameters than static or pruning-based LoRA variants. The paper is clearly written overall and the motivation, approach, and visualizations are well conveyed.  \n\n---\n\n**Major Comments**  \n1. **Conceptual and Technical Consistency:** The FI-Ratio definition and gating mechanism appear inconsistent across equations and text. The direction of inequality (‚â• vs. ‚â§) and interpretation of Œª=1 are unclear, making the decision rule ambiguous. Relatedly, the convergence argument appears heuristic and based on an inverted ratio compared to the original definition.  \n2. **Gradient Flow at Expansion:** When expanding via zero-initialized components (Bnew=0, K random), gradients flow asymmetrically to Bnew but not K, biasing FI estimation and potentially forcing redundant expansion. Initialization or normalization strategies to balance gradient flow are needed.  \n3. **Experimental Coverage:** Evaluation on small datasets limits generalization. Validation on larger datasets such as MIMIC-CXR would strengthen claims. The reliance on ImageNet-FID and generic CLIP metrics is suboptimal for medical imaging; domain-specific evaluation (e.g., XRV-FID, CheXpert AUROC, RadGraph) or expert ratings are recommended.  \n4. **Baseline and Fairness Issues:** Parameter budgets are not always matched across methods, and some high-performing configurations appear to use more parameters. Comparisons should include non-LoRA PEFT baselines and recent adaptive/prune+expand LoRA methods (ALoRA, ElaLoRA, PrunedLoRA, etc.).  \n5. **Efficiency and Reporting:** The computational overhead of FI evaluation is not reported in wall-clock time, and code is not released for reproducibility.  \n6. **Presentation and Data Integrity:** Figures include inconsistent preprocessing (colorized vs. grayscale baselines) and differing data splits. Clarification and uniform preprocessing are necessary.  \n\n---\n\n**Minor Comments**  \n- Clarify equations and convergence notations; ensure all threshold conditions are consistent.  \n- Provide rank distribution per layer, total parameter counts, and runtime overhead.  \n- Confirm identical sampling settings, seeds, and preprocessing across all visual comparisons.  \n- Cite or experimentally compare to domain-adapted diffusion systems (RoentGen, Chest-Diffusion).  \n- Discuss broader impacts of synthetic medical image generation, including privacy and safety safeguards.  \n\n---\n\n**Summary Paragraph**  \nSeLoRA offers an appealing and technically simple mechanism for adaptive rank expansion that targets a key limitation of existing diffusion PEFT methods. The approach is clearly motivated and experimentally tested, but several methodological ambiguities‚Äîparticularly in the FI-Ratio formulation, gradient handling, and convergence reasoning‚Äîundermine confidence in correctness. Empirical validation remains limited in scale and diagnostic rigor, and fairness of baseline comparisons is uncertain. Strengths include a well-defined motivation, interpretability via rank heatmaps, and potential generalization to other tasks. Weaknesses concern unclear theoretical grounding, limited evaluation scope, and missing baselines. Addressing these aspects would considerably improve robustness and significance.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The work is promising and potentially valuable, but it requires clearer theoretical specification, stronger and more domain-appropriate evaluation, and fairer comparative analysis before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"SeLoRA: SELF-EXPANDING LOW-RANK ADAPTATION OF LATENT DIFFUSION MODEL FOR MEDICAL IMAGE SYNTHESIS\"\n\n## 1. Opening Paragraph\n\nThis manuscript addresses the critical challenge of medical image synthesis in resource-constrained settings where annotated data is scarce, focusing on adapting latent diffusion models using parameter-efficient fine-tuning techniques. The authors propose SeLoRA (Self-Expanding Low-Rank Adaptation), a novel method that dynamically expands the rank of LoRA modules during training based on Fisher Information analysis, rather than using a uniform rank across all layers as in conventional approaches. The core innovation lies in initializing with rank=1 and strategically expanding ranks only in crucial layers where additional capacity provides meaningful improvements in synthesis quality. The authors demonstrate that SeLoRA achieves superior image fidelity (as measured by FID scores) and better text-image alignment (CLIP scores) compared to standard LoRA, AdaLoRA, and DyLoRA across multiple medical imaging datasets (IU X-Ray, Montgomery County CXR, and PatchGastricADC22), while maintaining training efficiency. The method's ability to automatically allocate ranks to critical attention layers (particularly query and key weights in cross-attention mechanisms) is empirically validated through detailed rank allocation visualizations and ablation studies.\n\n## 2. Major and Minor Comments\n\n**Major Strengths:**\n- The core concept of dynamically expanding ranks based on Fisher Information is well-motivated and addresses a genuine limitation of uniform-rank LoRA approaches when applied to the heterogeneous layer structures of medical image diffusion models\n- Comprehensive experimental validation across multiple datasets, resolutions (224√ó224 and 384√ó384), and medical imaging modalities (X-ray and histopathology) demonstrates the method's versatility\n- The rank allocation patterns shown in Figures 3-6 provide compelling evidence that SeLoRA correctly identifies critical layers (particularly cross-attention mechanisms) for rank expansion\n- Thorough memory usage analysis (Figures 11-12) and training time comparison offer practical insights for real-world implementation\n\n**Major Limitations:**\n- The paper lacks comparison with more diverse state-of-the-art medical image synthesis methods beyond LoRA variants (e.g., traditional GAN-based approaches or other diffusion model adaptations)\n- The theoretical convergence analysis (Section 3.3) is somewhat hand-wavy and would benefit from more rigorous mathematical treatment with formal proofs\n- The selection of Œª=1.1 as default threshold appears arbitrary without sufficient justification for why this value works best across different datasets and modalities\n- Limited discussion of computational complexity in terms of FLOPs, which would complement the memory usage analysis\n\n**Minor Strengths:**\n- Clear visualization of the rank expansion mechanism in Figure 1\n- Inclusion of qualitative results across different medical imaging modalities (X-ray and histopathology)\n- Detailed ablation study on the impact of threshold parameter Œª (Table 3)\n- Public availability of code, which enhances reproducibility\n\n**Minor Limitations:**\n- Some figures (e.g., Figure 2) would benefit from clearer annotations to highlight key differences between methods\n- The paper mentions \"more sample results in Appendix\" but doesn't specify exact locations for all supplementary materials\n- Minor inconsistencies in notation (e.g., sometimes using \"r\" for rank, sometimes not)\n- The memory usage plots (Figures 11-12) could be improved with clearer labeling of expansion events\n\n## 3. Evaluation Against TMI Editorial Criteria\n\n**Significance (High):** The work addresses a critical problem in medical imaging‚Äîthe scarcity of annotated data for training deep learning models. By improving parameter-efficient fine-tuning of diffusion models for medical image synthesis, the approach has significant potential impact on enabling AI applications in resource-limited clinical settings. The application to multiple medical imaging modalities (X-ray and histopathology) further strengthens its significance for the medical imaging community.\n\n**Innovation (High):** The self-expanding rank mechanism represents a meaningful advancement beyond existing LoRA variants. The Fisher Information-based criterion for rank expansion is particularly innovative, providing an automated method to allocate resources where they matter most. The work effectively bridges concepts from efficient fine-tuning with the specific needs of medical image synthesis, where data scarcity makes parameter efficiency crucial.\n\n**Evaluation (Good):** The evaluation is comprehensive, including quantitative metrics across multiple datasets and resolutions, qualitative comparisons, memory usage analysis, and ablation studies. However, the evaluation would be strengthened by including human expert evaluation of synthesized images and comparison with a broader range of baselines beyond LoRA variants. The inclusion of higher-resolution experiments (384√ó384) and cross-modality validation on histopathology images demonstrates thoroughness.\n\n**Reproducibility (Good):** The paper provides sufficient implementation details, including hyperparameters, training protocols, and code availability. The inclusion of memory usage plots, training time analysis, and the worst-case training time formula (Section D.2.1) significantly enhances reproducibility. The public release of code is a strong point for reproducibility, though the paper would benefit from more specific information about the computational environment used.\n\n## 4. Decision Recommendation\n\n**Minor Revision**\n\nThe manuscript presents a novel and well-executed approach to parameter-efficient fine-tuning for medical image synthesis that makes a valuable contribution to the field. The core innovation of dynamically expanding ranks based on Fisher Information analysis is both theoretically sound and empirically validated across multiple medical imaging datasets and modalities. While the paper is generally well-written and scientifically rigorous, I recommend minor revisions to address the limitations noted above, particularly strengthening the theoretical analysis, providing more justification for the selected threshold parameter, and expanding the comparison with broader state-of-the-art methods. These revisions would further enhance the paper's impact and clarity without requiring substantial additional experimentation. The work is well-positioned for acceptance in TMI after these minor revisions.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *SeLoRA* (Self-Expanding Low-Rank Adaptation), a parameter-efficient fine-tuning method for latent diffusion models aimed at improving medical image synthesis in data-scarce environments. The approach dynamically expands the ranks of LoRA modules during training based on Fisher Information, rather than using a fixed rank across layers. Beginning with rank‚ÄØ=‚ÄØ1, SeLoRA selectively increases rank in layers where greater capacity meaningfully enhances performance. Experiments on multiple medical imaging datasets (IU‚ÄØX-Ray, Montgomery County‚ÄØCXR, PatchGastricADC22) demonstrate superior image fidelity (FID) and improved text‚Äìimage alignment (CLIP) compared to LoRA, AdaLoRA, and DyLoRA, while maintaining efficiency. The method‚Äôs automatic rank allocation to critical attention layers is validated through visualizations and ablation studies. Overall, the manuscript is clearly presented and addresses a well-motivated problem in parameter-efficient model adaptation for medical imaging.  \n\n**Major Comments**  \n1. The core idea of adaptive rank expansion based on Fisher Information is well‚Äëjustified and effectively addresses limitations of uniform‚Äërank LoRA, showing both novelty and practical value.  \n2. Experimental validation is extensive across multiple datasets, resolutions, and modalities, illustrating SeLoRA‚Äôs robustness.  \n3. Comparative baselines are limited to LoRA variants; inclusion of broader state‚Äëof‚Äëthe‚Äëart medical image synthesis approaches (e.g., GAN‚Äë or diffusion‚Äëbased methods) would strengthen the study.  \n4. The theoretical convergence discussion (Section‚ÄØ3.3) is informal; more rigorous mathematical treatment and formal proofs would improve credibility.  \n5. The choice of Œª‚ÄØ=‚ÄØ1.1 as a default threshold seems arbitrary and needs stronger justification.  \n6. Computational complexity in terms of FLOPs is not discussed, which would complement the current memory and training‚Äëtime analysis.  \n\n**Minor Comments**  \n- Figure‚ÄØ1 effectively depicts rank expansion, but Figure‚ÄØ2 needs clearer annotations to highlight key methodological differences.  \n- Some references to supplementary figures and tables are vague; clearer directions to appendices are recommended.  \n- Minor notation inconsistencies (use of ‚Äúr‚Äù for rank) should be corrected.  \n- Memory usage plots (Figures‚ÄØ11‚Äì12) would benefit from clearer labeling of expansion events.  \n\n**Summary Paragraph**  \nThis is a strong contribution addressing a significant challenge in adapting diffusion models for medical image synthesis under data constraints. The innovation of Fisher Information‚Äëdriven rank expansion is compelling and well‚Äëvalidated experimentally. The work excels in methodological clarity, breadth of evaluation, and reproducibility through code release and detailed analyses. Remaining issues include limited comparison breadth, underdeveloped theoretical discussion, and incomplete justification of hyperparameters. With these minor revisions, the manuscript would be ready for publication.  \n\n**Decision Recommendation**  \n**Minor Revision.** The paper is technically sound, original, and well‚Äëexecuted, requiring only modest revisions related to theoretical rigor, parameter justification, and expanded comparative analysis.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe manuscript tackles the well‚Äëknown problem of limited annotated medical imaging data and the consequent need to synthesize missing imaging modalities for multimodal analyses. The authors train stable‚Äëdiffusion models on several chest‚ÄëX‚Äëray collections ‚Äì the IU‚ÄØX‚ÄëRay dataset (‚âà‚ÄØ4‚ÄØk image‚Äëreport pairs), the Montgomery County CXR set (138 frontal images) ‚Äì as well as on a gastric histology patch dataset. To improve efficiency, they introduce a Self‚ÄëExpanding Low‚ÄëRank Adaptation (SeLoRA) module. SeLoRA starts with LoRA adapters of rank‚ÄØ1 and, during training, expands the rank of each linear layer according to a Fisher‚Äëinformation‚Äëbased ratio, while a special initialization scheme (zero‚Äëinit‚ÄØB / random‚Äëinit‚ÄØA) is used to keep the model output unchanged. Across the three benchmarks, SeLoRA yields lower Fr√©chet Inception Distance scores and higher CLIP similarity values than the baseline LoRA, AdaLoRA, and DyLoRA, while requiring fewer trainable parameters and comparable GPU memory consumption. The key contribution is a parameter‚Äëefficient, rank‚Äëadaptive fine‚Äëtuning strategy for latent diffusion models in the context of medical image synthesis.\n\n---\n\n**## General feedback**\n\n- **Significance:** The paper addresses an important practical bottleneck‚Äîhow to choose the rank for parameter‚Äëefficient fine‚Äëtuning (PEFT) when only a small amount of medical data is available. The topic is certainly relevant, although the experiments are confined to relatively small 2‚ÄëD datasets, which limits the immediate clinical impact.\n\n- **Innovation:** The authors propose a novel rule for rank expansion based on a Fisher‚Äëinformation ratio, together with a zero‚Äëinitialization of the B matrix and random‚Äëinitialization of A (see Sections‚ÄØ3.1‚Äì3.2). While adaptive‚Äërank ideas have appeared before (e.g., AdaLoRA, DyLoRA, ALoRA), the manuscript does not clearly explain what theoretical advantage SeLoRA has over these existing approaches.\n\n- **Evaluation:** Three datasets and three baseline methods are examined, and the reported improvements (e.g., a ~5‚ÄØ% reduction in FID from 113.04 to 65.78 on IU‚ÄØX‚ÄëRay, Table‚ÄØ1) are encouraging. However, the analysis lacks statistical testing, variance measures for SeLoRA are missing in several tables, and stronger diffusion‚Äëspecific baselines such as DreamBooth, Textual Inversion, or full‚Äëparameter fine‚Äëtuning are not included.\n\n- **Reproducibility:** A code repository is provided, which is commendable. Nevertheless, several critical hyper‚Äëparameters (the Fisher‚Äëratio threshold Œª, the testing interval t, optimizer Œ≤ values) and details of the data splits are only partially described (Table‚ÄØ4, Section‚ÄØB.1). The choice of t‚ÄØ=‚ÄØ40 is described as ‚Äúsomewhat arbitrary‚Äù without a sensitivity study, making it hard for others to reproduce the results reliably.\n\n---\n\n**## Specific comments/critiques**\n\n1. **Justification of the FI‚ÄëRatio:** Equation‚ÄØ5 defines the Fisher‚Äëinformation ratio used to decide when to expand ranks, but the paper offers only an empirical rationale and does not reference prior work on Fisher‚Äëbased model selection. A stronger theoretical grounding would improve confidence in this criterion.\n\n2. **Sensitivity to hyper‚Äëparameters:** The expansion interval (t‚ÄØ=‚ÄØ40) and the FI‚ÄëRatio threshold (Œª‚ÄØ=‚ÄØ1.1) are presented as convenient choices (Section‚ÄØ3.5). Apart from a brief comparison in Table‚ÄØ3, there is no systematic ablation or sweep of these values, leaving the robustness of SeLoRA to such settings unclear.\n\n3. **Baseline completeness:** The study does not compare against full‚Äëparameter fine‚Äëtuning or recent diffusion‚Äëspecific PEFT techniques such as DreamBooth or Textual Inversion. Without these comparisons, it is difficult to ascertain whether the observed gains stem from the adaptive rank mechanism itself or simply from the larger number of trainable parameters.\n\n4. **Statistical reporting:** While standard deviations are reported for the LoRA baselines (Tables‚ÄØ1‚Äë2), they are absent for SeLoRA and for the higher‚Äëresolution experiments (Tables‚ÄØ5‚Äë6). Confidence intervals or hypothesis tests would be essential to demonstrate that the improvements are statistically meaningful.\n\n5. **Clinical validation:** Figures‚ÄØ2,‚ÄØ9, and‚ÄØ10 showcase visually appealing results and claim better capture of pathological features. However, no blinded assessment by radiologists or pathologists is performed, so the clinical relevance of the generated images remains unverified.\n\n6. **Analysis of rank allocation:** The visualisations in Figures‚ÄØ3‚Äë6 illustrate where ranks increase during training, yet the manuscript does not relate these per‚Äëlayer changes to gradient magnitudes, FI‚ÄëScore contributions, or the final FID impact. A deeper analysis would help explain why certain layers are preferred for rank expansion.\n\n7. **Memory and runtime claims:** Reported GPU‚Äëmemory savings (~0.2‚ÄØGB, Table‚ÄØ7) and modest training‚Äëtime overhead (~2.5‚ÄØ% extra, Table‚ÄØ8) are interesting, but they are presented without measures of variability. It would be useful to see whether these benefits persist across different hardware configurations or larger models.\n\n8. **Notation inconsistencies:** Table‚ÄØ4 lists optimizer momentum as ‚ÄúŒ≤‚ÇÅ‚ÄØ=‚ÄØ0.9, Œ≤‚ÇÇ‚ÄØ=‚ÄØ0.999‚Äù, whereas Section‚ÄØ3.5 writes ‚ÄúŒ≤‚ÇÅ‚ÄØ=‚ÄØ0‚ÄØ._‚ÄØ9_, Œ≤‚ÇÇ‚ÄØ=‚ÄØ0‚ÄØ._‚ÄØ999‚Äù. The discrepancy could cause confusion and should be corrected.\n\n9. **Presentation issues:** The manuscript contains several typographical errors, broken equations, and inconsistent formatting (e.g., misplaced underscores). These issues detract from readability and may hinder reproducibility.\n\n---\n\n**## A suggested decision**\n\n**Reject**\n\n*Rationale:* Although the work presents an interesting idea for rank‚Äëadaptive fine‚Äëtuning of diffusion models, the current manuscript falls short of the journal‚Äôs standards. The limited experimental scope, insufficient statistical validation, incomplete baseline comparisons, and several reproducibility and presentation concerns prevent the paper from being ready for publication in its present form. Further development and more rigorous evaluation would be needed before the contribution can be considered for acceptance.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of limited annotated medical imaging data by exploring parameter‚Äëefficient fine‚Äëtuning for image synthesis. Specifically, it introduces Self‚ÄëExpanding Low‚ÄëRank Adaptation (SeLoRA), a diffusion‚Äëmodel variant that adaptively expands the rank of LoRA layers based on a Fisher‚Äëinformation criterion while maintaining efficiency through a specialized initialization scheme. Experiments on three datasets‚Äîtwo chest X‚Äëray sets and a gastric histology collection‚Äîdemonstrate lower Fr√©chet Inception Distance (FID) scores and higher CLIP similarity than several LoRA‚Äëbased baselines with comparable computational cost. Overall, the topic is relevant and the proposed method promising, though current limitations restrict its demonstrated impact.  \n\n**Major Comments**  \n1. **Significance and Scope:** The work targets an important issue‚Äîrank selection for parameter‚Äëefficient fine‚Äëtuning under small‚Äëdata conditions‚Äîbut the experiments cover only modest‚Äësized 2‚ÄëD datasets, reducing generalizability and clinical impact.  \n2. **Novelty and Theoretical Basis:** While the Fisher‚Äëinformation‚Äëguided rank expansion and initialization strategy are novel elements, the manuscript does not clearly establish theoretical advantages over existing adaptive‚Äërank methods (AdaLoRA, DyLoRA, ALoRA). A stronger conceptual justification of the Fisher‚Äëinformation ratio would increase confidence in the approach.  \n3. **Experimental Evaluation:** The reported ~5‚ÄØ% FID improvements are encouraging, yet statistical tests, standard deviations for SeLoRA, and comparisons with stronger diffusion‚Äëbased or full‚Äëparameter baselines (e.g., DreamBooth, Textual Inversion) are missing, limiting the strength of the evidence.  \n4. **Reproducibility:** Although code is shared, several hyper‚Äëparameters (Fisher‚Äëratio threshold Œª, expansion interval‚ÄØt, optimizer Œ≤ values) and dataset splits are only partially described. The stated choice of t‚ÄØ=‚ÄØ40 appears arbitrary, and no sensitivity analysis is provided.  \n5. **Clinical and Analytical Depth:** Visual results claim better depiction of pathological features, but no expert validation is conducted. The analysis of how layer‚Äëwise rank changes relate to gradients or model performance remains incomplete.  \n6. **Computational Claims:** Reported gains in memory and runtime are modest and lack variance measures across runs or configurations.  \n\n**Minor Comments**  \n- Clarify and align notation for optimizer parameters (Œ≤‚ÇÅ,‚ÄØŒ≤‚ÇÇ) and correct typographical and formatting inconsistencies, including equation rendering and misplaced underscores.  \n- Improve figure and table readability, and ensure all tables report variance or confidence intervals where applicable.  \n\n**Summary Paragraph**  \nThis submission proposes a potentially useful mechanism for adaptive rank growth in diffusion models, addressing a practical bottleneck in medical image generation. Strengths include a relevant problem focus, accessible implementation, and improved efficiency metrics. However, weaknesses lie in theoretical justification, statistical rigor, completeness of baseline evaluation, and limited dataset diversity. Presentation and documentation issues further reduce reproducibility. Strengthening experiments and clarifying theoretical and methodological aspects would substantially improve the work‚Äôs credibility.  \n\n**Decision Recommendation**  \n**Reject.** The idea is interesting and technically sound in concept, but the current study lacks sufficient validation, comparisons, and clarity to meet acceptance standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chengjia Wang",
      "Giorgos Papanastasiou",
      "Guang Yang",
      "Hongwei Bran Li",
      "Wei Pang",
      "Yuchen Mao"
    ],
    "url": "pdfs/iclr.cc-2025-conference_f6f8edf9c65dcb297d770a2a371e14a022229bb1.pdf",
    "remote_url": "https://openreview.net/pdf/f6f8edf9c65dcb297d770a2a371e14a022229bb1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Transformer-Based CT Anomaly Detection and Auto-Segmentation of Sparse Lung Nodules",
    "status": "not_started",
    "evaluators": [
      "Bernhard",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "Transformer",
      "CT scans",
      "lung nodules",
      "anomaly detection",
      "auto-segmentation",
      "Deformable-DETR",
      "sparse data",
      "medical imaging",
      "self-attention",
      "multi-scale learning",
      "object detection",
      "Focal Loss",
      "segmentation"
    ],
    "abstract": "Accurate segmentation of lung nodules in computed tomography (CT) scans is challenging due to extreme class imbalance, where nodules appear sparsely among healthy tissue. Lung tumor boards often review these scans manually, a time-consuming process. This paper introduces a novel two-stage approach for lung tumor segmentation by framing the problem as anomaly detection. The method is divided into two stages, allowing each model to leverage its strengths. Stage 1 focuses on region proposal, employing a custom Deformable Detection Transformer with Focal Loss to overcome class imbalance and localize sparse tumors. In Stage 2, the predicted bounding boxes are refined into pixel-wise segmentation masks using a fine-tuned variant of Meta's Segment Anything Model (SAM) for semantic segmentation. To address the challenge of nodule sparsity and improve spatial context, a 7.5 mm Maximum Intensity Projection (MIP) is applied, aiding in the differentiation between nodules, bronchioles, and vascular structures. The model achieves a Dice coefficient of 92.4%, with 95.2% sensitivity and 93.2% precision on the LUNA16 dataset, demonstrating robust performance in real-world clinical conditions where nodule sparsity is 5%.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis paper proposes a two-stage pipeline for lung nodule segmentation in CT scans, designed to support lung tumor boards by enhancing segmentation accuracy and efficiency. The first stage employs a custom Deformable Detection Transformer (DETR) architecture to detect sparse lung tumors, leveraging deformable attention to improve sensitivity to small nodules. The second stage utilizes a fine-tuned Segment Anything Model (SAM), enhanced with medical imaging capabilities (MedSAM), to refine the bounding boxes into pixel-level segmentation masks, ensuring precision in differentiating nodules from surrounding anatomy.\n\nTo address the class imbalance in CT data - where lung nodules are rare compared to healthy tissue - the framework incorporates focal loss, reducing model bias towards non-tumor areas and enhancing detection accuracy for hard-to-detect nodules. Achieving a 94.2% F1 score for bounding box prediction and a 92.4% Dice coefficient in segmentation accuracy, this pipeline demonstrates strong potential to improve clinical workflows, enhance tumor board decision-making, and contribute to better patient outcomes by streamlining nodule detection in a clinical setting.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper introduces a novel two-stage approach for lung tumor segmentation by framing the task as anomaly detection, addressing the challenges of sparse nodule identification in CT scans. This innovative structure uses a Deformable Detection Transformer (DETR) for region proposals and a fine-tuned Segment Anything Model (SAM) for precise segmentation, effectively handling class imbalance and complex image features.\n\nAdditionally, the paper‚Äôs clear, logical structure and well-explained methodology make complex concepts accessible. Detailed quantitative results further highlight the framework's effectiveness, making it a valuable and readable contribution to medical imaging and clinical decision support.\n\n### Weaknesses\n\n1. The paper‚Äôs experimental section lacks depth, with insufficient analysis to thoroughly validate the proposed method. \n\n2. There is no ablation study provided, which limits insight into how each component - such as the use of Deformable Detection Transformer (DETR), the fine-tuned Segment Anything Model (SAM), and the customized focal loss - contributes to overall performance. Without this breakdown, it‚Äôs difficult to assess which aspects of the framework are most effective. \n\n3. The paper relies solely on quantitative evaluation, omitting any qualitative assessment, such as visual comparisons among different methods, which could provide a clearer understanding of the model's segmentation accuracy and real-world applicability.\n\n4. The presentation of results is also weak, with layout issues that detract from readability and professionalism. For instance, Table 2 extends beyond the page margin, rendering the data difficult to interpret. Additionally, there are inconsistencies and errors in in-text citations, which may confuse readers and hinder the paper‚Äôs credibility. These issues in presentation and citation detract from the paper's overall clarity and polish, suggesting the need for more careful formatting and editing. There are also several grammatical errors, which make the paper somewhat challenging to read.\n\n### Questions\n\n- Why is there only quantitative evaluation/comparison provided in the manuscript? Could you provide some qualitative results, such as visual examples of segmentation outputs, to illustrate the model‚Äôs performance?\n\n- Can you elaborate on how the class imbalance was handled during training? Were any additional strategies (besides focal loss) considered or tested to further address this issue?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nNone",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage pipeline for lung nodule segmentation in CT scans, intended to support clinical tumor boards by improving segmentation accuracy and efficiency. The first stage employs a customized Deformable Detection Transformer (DETR) for detecting sparse lung tumors, while the second stage fine-tunes a Segment Anything Model (MedSAM) for precise pixel-level segmentation. To mitigate class imbalance between nodules and healthy tissue, the authors incorporate focal loss to enhance sensitivity to rare findings. Reported performance is high, suggesting potential clinical value. The paper is generally clear, with a well-structured presentation of methods and results.\n\n**Major Comments**  \n1. **Experimental Depth:** The evaluation section lacks sufficient analysis to robustly validate the proposed framework. Greater detail on experimental setup, dataset composition, and comparison with baselines would strengthen the empirical foundation.  \n2. **Ablation Study:** The absence of an ablation study limits understanding of the contributions of individual components‚Äînamely DETR detection, SAM segmentation, and focal loss. Decomposing their effects would clarify which elements drive performance gains.  \n3. **Qualitative Evaluation:** Only quantitative metrics are provided; qualitative visualizations or example segmentations would help assess segmentation quality and practical applicability.  \n4. **Result Presentation:** Formatting issues detract from readability. For instance, Table 2 extends beyond page margins, and citation inconsistencies introduce confusion. These presentation concerns, along with grammatical errors, reduce overall professionalism.  \n\n**Minor Comments**  \n- Address typographical and grammatical errors throughout the text.  \n- Standardize citation formatting and ensure figures and tables are legible within margins.  \n- Clarify explanations regarding the management of class imbalance; indicate whether strategies beyond focal loss were explored.  \n\n**Summary Paragraph**  \nOverall, the paper presents an innovative pipeline combining DETR and MedSAM for improved lung nodule detection and segmentation, offering promising results supported by strong quantitative performance. However, the lack of detailed experimental analysis, missing ablation and qualitative studies, and presentation flaws limit its current completeness. Addressing these issues would substantially enhance both scientific rigor and presentation quality.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The approach is novel and potentially impactful, but further experimental validation, qualitative analysis, and improved presentation are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists. However, the primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\nThis study is to propose a two-stage approach for lung tumor segmentation by anomaly detection including stage 1 of region proposal with deformable detection transformer with focal loss, and stage 2 with fine-tuned SAM. This study is notable for its use of the MIP (Maximum Intensity Projection) method to address issues related to nodule sparsity and spatial context. This approach is also frequently employed by radiologists.\n\n### Weaknesses\n\nThe primary concern with this paper is that all preprocessing and modeling steps are performed in 2D. When lung segmentation is conducted in 2D, it may be challenging to differentiate diseased lungs or lung cancers that are close to the thoracic wall. Additionally, for nodules with subsolid or GGO characteristics, visibility might be reduced in thicker MIP slices, suggesting that these types should be evaluated separately. Despite achieving better results than previous models, the study lacks an analysis of subclasses or an ablation study, and falls short in terms of technical novelty.\n\n### Questions\n\nPreprocessing is performed with 1 mm isocubic resolution, yet the method for generating 7.5 mm MIP using five slices in Figure 1-c needs clarification.\n\nThe authors should analyze the histogram of nodules under 10 mm from the LIDC dataset and include these results and discussions in the paper.\n\nThe paper lacks external validation, which raises concerns about the generalizability of the findings. A discussion on this limitation is recommended.\n\nIn Figure 1-b, lung segmentation is reportedly performed using Otsu segmentation; however, accuracy metrics such as DSC should be presented.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5\n\n### Details Of Ethics Concerns\n\nPublic dataset",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a two-stage framework for lung tumor segmentation based on anomaly detection. Stage one involves region proposal using a deformable detection transformer with focal loss, while stage two refines results using a fine-tuned Segment Anything Model (SAM). The paper highlights the use of Maximum Intensity Projection (MIP) to mitigate nodule sparsity and enhance spatial context, a technique commonly adopted by radiologists. Overall, the study demonstrates promising segmentation accuracy and improved performance over prior models, though it is limited in methodological innovation and completeness of analysis.\n\n**Major Comments**  \n1. **Dimensionality of Analysis** ‚Äì All preprocessing and modeling steps are conducted in 2D, which may impede accurate segmentation of tumors near the thoracic wall or reduce visibility of subsolid and ground-glass opacity (GGO) nodules. This raises concerns about the model‚Äôs capacity to generalize to more complex or subtle lesion presentations.  \n2. **Lack of Subtype Evaluation and Ablation Study** ‚Äì The work does not include subclass analyses or ablation experiments to assess individual component contributions. These omissions reduce the interpretability and confirmatory strength of the results.  \n3. **Clarification of MIP Generation** ‚Äì The process of generating a 7.5‚ÄØmm MIP from five slices (as shown in Figure‚ÄØ1-c) should be clearly described, given the mention of 1‚ÄØmm isocubic preprocessing.  \n4. **Validation and Generalizability** ‚Äì External validation is absent, leaving the robustness of the proposed approach underexplored. A discussion of this limitation is needed.  \n5. **Segmentation Accuracy Reporting** ‚Äì Figure‚ÄØ1-b includes lung segmentation via Otsu‚Äôs method, but objective accuracy metrics (e.g., Dice Similarity Coefficient) are not reported and should be provided.  \n6. **Dataset Characterization** ‚Äì The authors are encouraged to analyze and report the size distribution (e.g., nodules‚ÄØ<‚ÄØ10‚ÄØmm) from the LIDC dataset to strengthen understanding of model performance across lesion scales.\n\n**Minor Comments**  \n- Some descriptions and figure references could be clarified for readability.  \n- Ensure consistent terminology for nodule types and imaging parameters.\n\n**Summary Paragraph**  \nThis study presents a practical segmentation approach combining transformer-based detection and SAM refinement with MIP preprocessing. The method is clearly motivated and potentially useful; however, its reliance on 2D processing, absence of ablation and subclass analyses, and lack of external validation limit its innovation and reproducibility. Additional quantitative reporting and methodological clarification would improve the study‚Äôs soundness and interpretability.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis manuscript presents a novel two-stage approach for automating lung nodule segmentation using transformer models. In the data preprocessing phase, Maximum Intensity Projection (MIP) enhances spatial features, helping to distinguish nodules from bronchioles and vessels in CT images. Next, region proposal bounding boxes are generated using the Deformable-DETR model. In Stage 2, these bounding boxes are processed by the SAM model to achieve pixel-level segmentation. To address class imbalance within the dataset, focal loss is incorporated into the original DETR loss function. The results demonstrate superior performance compared to state-of-the-art (SOTA) methods.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 1\n\n### Strengths\n\nIn preprocessing, maximum intensity projection is applied across slices to enhance visibility. The two-stage framework, combining DETR and SAM models, offers a straightforward approach. Additionally, the common issue of class imbalance in medical datasets is addressed. As a result, segmentation performance is significantly improved. The paper is well organized.\n\n### Weaknesses\n\nThis manuscript lacks novel insights, as the deep learning models used in each stage are well-established, and focal loss is widely applied across various domains. Additionally, the ROI-based segmentation approach is considered somewhat conventional. There is no ablation studies on original SAM performance and some critical models to compare or discuss are missing.\n\n### Questions\n\n1. What is the technical insight of this work? The concept of ROI-based segmentation is not new, and both DETR and SAM are well-established models. As a result, the framework appears to lack novelty, which is a critical concern.\n\n2. There are several existing studies on lung nodule segmentation in CT images. For instance, the IEEE TMI paper, \"Closing the Gap between Deep Neural Network Modeling and Biomedical Decision-Making Metrics in Segmentation via Adaptive Loss Functions,\" addresses not only lung segmentation but also class imbalance. It would be beneficial for the authors to compare or discuss their work in relation to such prior studies.\n\n3. Does the model function in an end-to-end learning manner? If it does or does not, the authors should provide a discussion on the merits and limitations of the learning method used in this framework.\n\n4. Which stage of the framework provides the most significant performance improvement? \n\n5. Additionally, what is the baseline performance of the original SAM model pretrained with MedSAM? Are the datasets used in MedSAM aligned with the LUNA dataset, and how does this impact model performance?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 1\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a two-stage framework for automated lung nodule segmentation in CT images, combining transformer-based approaches. The pipeline uses Maximum Intensity Projection (MIP) in preprocessing to enhance spatial features, followed by region proposal generation using Deformable-DETR and pixel-level segmentation using the SAM model. Focal loss is embedded to mitigate class imbalance. The study reports improved segmentation performance compared with state-of-the-art methods. Overall, the manuscript is clearly written and well organized, but its originality and technical novelty are limited.  \n\n**Major Comments**  \n1. **Novelty and Contribution** ‚Äì The approach lacks substantial technical innovation since both DETR and SAM are established models, and focal loss is a widely adopted technique. The manuscript does not present new algorithmic insights or clear methodological advances.  \n2. **Comparative Evaluation** ‚Äì Important comparisons and discussion with relevant works are missing. In particular, the manuscript should reference prior studies such as those using adaptive loss functions for biomedical segmentation, which already address class imbalance and segmentation challenges.  \n3. **Ablation Studies** ‚Äì There is no ablation analysis on the original SAM performance or other critical models to demonstrate the contribution of each component. This limits understanding of how each stage contributes to the reported improvements.  \n4. **Technical Clarity** ‚Äì The paper should clarify whether the proposed system operates in an end-to-end manner and discuss the strengths and weaknesses of the adopted design.  \n5. **Performance Interpretation** ‚Äì The reviewer requests explanation of which stage provides the most performance gain and how the pretrained MedSAM baseline performs on the LUNA dataset, including any dataset alignment issues that might affect the reported results.  \n\n**Minor Comments**  \n- Overall presentation is clear, and the manuscript structure is logical.  \n- Terminology and notation are consistent; only minor clarifications are needed regarding experimental setup and dataset description.  \n\n**Summary Paragraph**  \nThe paper introduces a practically motivated but methodologically conventional pipeline for lung nodule segmentation. While the integration of MIP preprocessing, DETR region proposals, and SAM-based segmentation is effective, the work relies on existing models without offering new conceptual insights. Missing comparisons, lack of ablation studies, and insufficient discussion of design choices reduce confidence in the claimed improvements. Despite good organization and presentation, the contribution is incremental.  \n\n**Decision Recommendation**: **Reject** (due to limited novelty and insufficient experimental validation).",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI** ‚Äì  \n‚úÖ The paper presents a methodological contribution within medical imaging, focusing on a novel two-stage transformer-based framework for CT nodule segmentation. The combination of Deformable-DETR for localization and fine-tuned SAM for pixel-level segmentation constitutes a methodological pipeline, aligning with TMI‚Äôs emphasis on imaging computation rather than clinical application alone.\n\n2. **Novelty & Contribution Level** ‚Äì  \nModerate-to-strong novelty. The integration of Deformable DETR and MedSAM within a unified workflow for sparse nodule segmentation is new, especially accompanied by dedicated preprocessing (7.5mm MIP) and the use of focal loss to address extreme data imbalance. Nonetheless, each component (DETR, SAM, focal loss, MIP) is known; thus, the innovation primarily stems from architectural integration and adaptation for real-world, low-prevalence CT data. The level of methodological insight is solid but not fully foundational ‚Äî best classified as a hybrid systems contribution.\n\n3. **Technical and Experimental Rigor** ‚Äì  \nGenerally rigorous: dataset preparation and preprocessing are transparently described; the LUNA16 dataset is correctly characterized. Proper train/validation/test splitting, balance between realism and feasibility, and inclusion of augmentation are clear strengths. Some empirical concerns remain: only a single dataset (LUNA16) was used, and no cross-dataset generalization or ablation beyond Œ≥/Œ± tuning is presented. The grounding in 2D MIP versus 3D context warrants further comparison. Uncertainty analysis or statistical significance testing is absent.\n\n4. **Clarity and Presentation** ‚Äì  \nHigh clarity and polished narrative; figures are described adequately, although small inconsistencies in reference formatting and equation typography exist. Some long sentences could be edited for conciseness. The presentation style is coherent and close to IEEE standard.\n\n5. **Ethical and Reproducibility Compliance** ‚Äì  \nDataset is open (LUNA16), ensuring compliance; however, there is no explicit mention of code availability or reproducibility plan. IRB not applicable as no private patient data were used.\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n### 1. Summary\nThis manuscript introduces a two-stage transformer-based framework for automated segmentation of lung metastases on CT. Stage‚ÄØ1 employs a customized Deformable‚ÄØDETR detector with focal loss on 7.5‚ÄØmm‚ÄØMIP slices to propose bounding boxes, addressing tumor sparsity and class imbalance. Stage‚ÄØ2 refines these proposals using a fine‚Äëtuned variant of MedSAM to generate pixel‚Äëlevel masks. Tested on the LUNA16 dataset, the approach achieved a Dice‚ÄØ‚âà‚ÄØ92‚ÄØ% and sensitivity‚ÄØ‚âà‚ÄØ95‚ÄØ%, outperforming recent CNN‚Äëbased baselines. The work aims to integrate into tumor‚Äëboard workflows for decision support.\n\n### 2. Strengths\n- Strong alignment with current interests in transformer architectures for medical imaging.\n- Clear methodological articulation and preprocessing transparency (resampling, MIP generation, augmentation).\n- Quantitative improvement over published baselines on a standard dataset.\n- Addressing clinical realism by preserving low nodule prevalence in the test set.\n- Writing is accessible and figures assist conceptual understanding.\n\n### 3. Weaknesses\n- Limited validation: only one dataset and no external testing or cross‚Äëcomparison to human annotators.\n- Methodological innovation lies in pipeline integration rather than a new theoretical contribution.\n- Absence of ablation or sensitivity analyses (e.g., effect of MIP slab thickness, focal loss parameters, SAM fine‚Äëtuning depth).\n- Unclear computational cost and inference time, crucial for clinical workflows.\n- Reproducibility discussion (code/data policy) lacking.\n- The ‚Äútumor‚Äëboard‚Äù framing is more aspirational than demonstrated.\n\n### 4. Major Comments\n1. **Clarify Novelty Relative to Prior DETR/SAM Applications** ‚Äì The manuscript would benefit from explicitly differentiating how the joint DETR‚ÄìSAM pipeline qualitatively advances beyond sequential detection‚Äësegmentation schemes in existing literature.  \n2. **Include External or Cross‚ÄëValidation Dataset** ‚Äì Demonstrating performance on at least one additional dataset (e.g., LIDC‚ÄëIDRI full set, or variation in CT reconstruction kernels) is essential for TMI to assess methodological robustness.  \n3. **Ablation and Sensitivity Analysis** ‚Äì Quantify contributions from each component (MIP, focal loss, deformable attention, two‚Äëstage structure).  \n4. **Computational Efficiency** ‚Äì Provide benchmark timings (inference per scan, GPU type, memory footprint).  \n5. **Statistical Rigor** ‚Äì Report confidence intervals or statistical tests across folds; a single best Dice value is insufficient at TMI standard.  \n6. **Reproducibility Statement** ‚Äì Add explicit mention of code availability, trained model weights, and details sufficient for replication.  \n7. **Clinical Integration Claim** ‚Äì Tone down assertions regarding ‚Äútumor board application‚Äù unless validated by user study or workflow integration trial.\n\n### 5. Minor Comments\n- Correct minor grammatical inconsistencies (e.g., ‚Äúis impractical as it is time‚Äëconsuming‚Äù ‚Üí streamline).  \n- Ensure consistent superscript formatting of equations and symbols.  \n- Add journal and year details to some references; check duplicated Lin‚ÄØet‚ÄØal.‚ÄØ(2017a/2017b) citations.  \n- Figures‚ÄØ1‚Äì4 should include scale bars and resolution information.  \n- Re‚Äëevaluate section numbering (minor spacing and caption formatting per IEEE guidelines).\n\n### 6. Overall Recommendation\n**Major‚ÄØRevision** ‚Äì The paper is promising and within TMI scope but needs additional methodological validation and experimental depth (cross‚Äëdataset testing, ablation, performance metrics) to meet TMI‚Äôs rigor and generalizability standards. If these are addressed, publication could be justified as a substantive systems innovation paper.\n\n### 7. Confidence Level\n**4‚ÄØ/‚ÄØ5** ‚Äì Reviewer is confident in the assessment based on experience with transformer‚Äëbased segmentation methods and TMI publication standards.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two‚Äëstage transformer‚Äëbased framework for automated CT lung nodule segmentation. Stage‚ÄØ1 employs a customized Deformable‚ÄØDETR detector with focal loss on 7.5‚ÄØmm‚ÄØMIP slices to localize sparse nodules, while Stage‚ÄØ2 refines these proposals using a fine‚Äëtuned MedSAM for pixel‚Äëlevel segmentation. The study evaluates performance on the LUNA16 dataset, reporting high Dice (‚âà‚ÄØ92‚ÄØ%) and sensitivity (‚âà‚ÄØ95‚ÄØ%) values that exceed existing CNN‚Äëbased approaches. The paper is clearly written, methodologically coherent, and well aligned with the journal‚Äôs focus on computational imaging.  \n\n---\n\n**Major Comments**  \n1. **Clarify Novelty Relative to Prior Work** ‚Äì Elaborate how the DETR‚ÄìSAM pipeline specifically advances beyond existing sequential detection‚Äësegmentation methods to highlight the true methodological innovation.  \n2. **Dataset Limitation** ‚Äì Validation is confined to LUNA16; additional external or cross‚Äëdataset testing (e.g., LIDC‚ÄëIDRI or datasets with varying reconstruction kernels) is essential to demonstrate generalizability.  \n3. **Ablation and Sensitivity Analyses** ‚Äì Include experiments quantifying contributions from each module (MIP slab thickness, focal loss, deformable attention, two‚Äëstage design).  \n4. **Computational Efficiency** ‚Äì Report inference time per scan, GPU specifications, and memory usage to assess feasibility for clinical workflows.  \n5. **Statistical Significance** ‚Äì Provide confidence intervals or statistical testing across folds to substantiate reported improvements.  \n6. **Reproducibility and Code Availability** ‚Äì Add a reproducibility statement specifying access to source code, trained weights, and implementation details.  \n7. **Clinical Integration Claim** ‚Äì Moderate statements regarding ‚Äútumor‚Äëboard application‚Äù unless supported by a dedicated user study or applied validation.  \n\n---\n\n**Minor Comments**  \n- Revise small grammatical issues for conciseness.  \n- Standardize typography and equation formatting (e.g., superscripts).  \n- Complete missing reference details (journal, year) and correct duplicate citations.  \n- Ensure figures include scale bars and resolution information.  \n- Adjust section numbering and caption spacing to conform to IEEE template.  \n\n---\n\n**Summary Paragraph**  \nOverall, the study makes a credible systems‚Äëlevel contribution by integrating established components‚ÄîDeformable‚ÄØDETR, MedSAM, focal loss, and MIP preprocessing‚Äîinto a coherent pipeline tailored for sparse lesion segmentation. Its methodological clarity and quantitative improvements are noteworthy, but the absence of external validation, ablation detail, and statistical rigor currently limits its generalizability. Strengths include transparent preprocessing, accessible writing, and alignment with transformer‚Äëbased imaging research; weaknesses relate to experimental breadth and reproducibility transparency.  \n\n---\n\n**Decision Recommendation**  \n**Major‚ÄØRevision** ‚Äì The submission is promising and relevant but requires broader validation, ablation analyses, and stronger reproducibility measures to meet the expected standard for publication.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper presents a two-stage transformer-based approach for lung nodule segmentation in CT scans to support tumor board applications. The method addresses class imbalance challenges where nodules appear in only 5% of CT slices. Stage 1 employs a custom Deformable Detection Transformer with Focal Loss for region proposal to localize sparse tumors. Stage 2 uses a fine-tuned Segment Anything Model (SAM) initialized with MedSAM weights for pixel-wise segmentation refinement. The preprocessing pipeline includes 7.5mm Maximum Intensity Projection (MIP) to enhance nodule visibility and differentiate nodules from vascular structures. Evaluated on LUNA16 dataset, the method achieves 92.4% Dice coefficient, 95.2% sensitivity, and 93.2% precision, with an F1 score of 94.2% for region proposal. The approach demonstrates superior performance compared to existing CNN and hybrid CNN-transformer methods, particularly for medium and large nodules.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation inconsistencies and notation issues**\n  - Equation (1) presents resampling factor R with inconsistent vector notation, showing R as both a vector and individual ratios without clear dimensional specification (Page 3, lines 117-120), which creates ambiguity in implementation\n  - The computational complexity formula O(H¬≤W¬≤C) in Section 3.3 incorrectly states C=1 for grayscale images when describing feature map channels from ResNet-50 backbone (Page 4, lines 214-216), as ResNet-50 typically outputs multi-channel feature maps\n  - Focal loss parameters Œ≥=2 and Œ±t=0.25 are mentioned as optimal through hyperparameter tuning (Page 7, lines 340-343) but no systematic evaluation or comparison with other parameter combinations is provided\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - Training set contains 12.7% nodule-positive images while test set has only 5% (Page 4, lines 180-184), creating a significant domain shift that may overestimate real-world performance and compromise model generalizability\n  - Stage 2 evaluation lacks independent assessment, as Table 1 shows only overall Dice coefficient of 92.4% without breakdown by nodule size categories (Page 6, Table 1), preventing detailed analysis of segmentation quality across different tumor characteristics\n  - Comparison with existing methods in Table 2 lacks statistical significance testing and confidence intervals (Page 7, Table 2), making it difficult to assess whether reported improvements are statistically meaningful\n\n‚Ä¢ **Technical implementation and reproducibility concerns**\n  - MIP slab thickness of 7.5mm selection is mentioned as \"suitable compromise\" (Page 3, lines 138-140) but no systematic evaluation of different thickness values or quantitative justification is provided\n  - Stage 1 and Stage 2 training details are incompletely specified, with missing information about convergence criteria, early stopping, and final model selection procedures (Page 5, Section 3.5), hindering reproducibility\n  - The claim of being \"trained from scratch\" for Deformable-DETR (Page 4, line 199) contradicts the use of ResNet-50 backbone, which typically uses pre-trained weights, creating confusion about the actual training methodology\n\n‚Ä¢ **Limited clinical validation and generalizability assessment**\n  - Evaluation is restricted to LUNA16 dataset only, with no validation on independent clinical datasets or multi-center data (throughout results section), limiting evidence for real-world tumor board applicability\n  - No analysis of computational requirements, inference time, or practical deployment considerations for clinical integration (no direct evidence found in the manuscript), despite the stated tumor board application focus\n  - Missing discussion of failure cases, edge conditions, or limitations in detecting specific nodule types or anatomical locations (no direct evidence found in the manuscript), which is crucial for clinical decision support systems\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify mathematical formulations and provide rigorous notation**\n  - Revise Equation (1) to use consistent vector notation and specify dimensional units clearly, providing implementation pseudocode to eliminate ambiguity in the resampling procedure\n  - Correct the computational complexity analysis by accurately describing the multi-channel feature maps from ResNet-50 and provide detailed complexity comparison with standard DETR\n  - Include systematic hyperparameter sensitivity analysis for focal loss parameters, showing performance curves across different Œ≥ and Œ±t values with statistical error bars\n\n‚Ä¢ **Strengthen experimental design and provide comprehensive evaluation**\n  - Conduct additional experiments with matched nodule prevalence between training and test sets, and report performance degradation analysis when prevalence varies to better estimate real-world applicability\n  - Provide detailed Stage 2 evaluation breakdown by nodule size categories, including per-class Dice coefficients, hausdorff distances, and boundary accuracy metrics to enable thorough segmentation quality assessment\n  - Add statistical significance testing with confidence intervals for all performance comparisons, including bootstrap sampling or cross-validation to establish statistical validity of reported improvements\n\n‚Ä¢ **Enhance technical documentation and ensure reproducibility**\n  - Conduct systematic evaluation of MIP thickness parameters (e.g., 5mm, 7.5mm, 10mm, 15mm) with quantitative metrics showing trade-offs between nodule visibility and structural overlap\n  - Provide complete training specifications including convergence criteria, validation monitoring, learning curves, and model selection procedures, along with computational resource requirements and training time analysis\n  - Clarify the exact pre-training strategy for all model components, specify which weights are initialized versus trained from scratch, and provide detailed architecture modifications made to standard Deformable-DETR\n\n‚Ä¢ **Expand clinical validation and address practical deployment considerations**\n  - Validate the approach on additional independent datasets beyond LUNA16, including multi-center clinical data and different CT scanner manufacturers to demonstrate broader generalizability\n  - Include comprehensive computational performance analysis with inference times, memory requirements, and hardware specifications needed for practical tumor board integration\n  - Provide detailed failure case analysis, including examples of missed nodules, false positives, and challenging anatomical scenarios, along with guidelines for clinical interpretation and limitations",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a two-stage transformer-based framework for lung nodule segmentation in CT scans, aimed at supporting tumor board applications. The first stage employs a custom Deformable Detection Transformer using Focal Loss to localize sparse nodules, and the second stage refines segmentation using a fine-tuned Segment Anything Model (SAM) initialized with MedSAM weights. The approach also integrates a 7.5‚ÄØmm Maximum Intensity Projection (MIP) preprocessing step to improve nodule visibility and separation from vessel structures. Experiments on the LUNA16 dataset report strong quantitative performance‚Äî92.4% Dice coefficient, 95.2% sensitivity, and 93.2% precision‚Äîexceeding existing CNN and hybrid transformer architectures. Overall, the paper is technically ambitious and clinically motivated but exhibits issues in mathematical consistency, evaluation design, and reproducibility.  \n\n**Major Comments**  \n1. **Mathematical and Notational Issues**: The resampling factor \\(R\\) in Equation‚ÄØ(1) is inconsistently described as both a vector and individual ratios. The computational complexity expression \\(O(H^2W^2C)\\) incorrectly sets \\(C=1\\) for grayscale images, disregarding ResNet-50‚Äôs multi-channel outputs. Hyperparameter tuning of Focal Loss parameters (Œ≥‚ÄØ=‚ÄØ2,‚ÄØŒ±‚Çú‚ÄØ=‚ÄØ0.25) is mentioned but lacks systematic evaluation.  \n2. **Experimental Design and Evaluation**: Training and test sets have mismatched nodule prevalence (12.7% vs‚ÄØ5%), risking biased generalization. Stage‚ÄØ2 lacks separate analysis by nodule size, and comparisons with related work in Table‚ÄØ2 omit statistical significance testing or confidence intervals.  \n3. **Reproducibility and Implementation Details**: MIP thickness (7.5‚ÄØmm) is described qualitatively without quantitative justification. Critical training details‚Äîsuch as convergence criteria, early stopping, and model selection‚Äîare unspecified. The claim of ‚Äútraining from scratch‚Äù conflicts with the use of a pre-trained ResNet-50 backbone.  \n4. **Clinical Validation and Generalizability**: Results rely solely on LUNA16 without independent or multi-center validation. Computational cost, inference efficiency, and deployment feasibility are not discussed. The manuscript also lacks analysis of failure cases or scenarios where the method underperforms.  \n\n**Minor Comments**  \n- Improve consistency in mathematical notation and specify units or dimensionality throughout equations.  \n- Clarify labeling of tables and figures for cross-reference.  \n- Ensure terminology such as ‚Äústage,‚Äù ‚Äúmodule,‚Äù and ‚Äúpipeline‚Äù is used consistently.  \n\n**Summary Paragraph**  \nThe paper introduces an innovative two-stage transformer pipeline that shows promising segmentation results on a benchmark dataset. However, weaknesses in theoretical formulation, experimental rigor, and reporting limit reproducibility and clinical interpretability. In particular, inconsistent mathematical expressions, incomplete training descriptions, and lack of statistical and clinical validation reduce the confidence in the claimed superiority of the approach. Addressing these issues would substantially strengthen the manuscript‚Äôs technical and applied contributions.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The work has potential significance but requires comprehensive clarification of mathematical definitions, expanded experimental validation, and detailed documentation to ensure reliability and generalizability.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces a two-stage framework for the automatic segmentation of lung metastases in CT scans, aiming to assist lung tumor boards in their evaluations. The proposed method employs a custom Deformable Detection Transformer (Deformable-DETR) with Focal Loss for region proposal and a fine-tuned Segment Anything Model (SAM) for pixel-wise segmentation. The authors claim robust performance with a Dice coefficient of 92.4%, 95.2% sensitivity, and 93.2% precision on the LUNA16 dataset. The manuscript is well-written and clearly articulates the challenges of class imbalance and nodule sparsity.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: The proposed method integrates Deformable-DETR and SAM, which is a unique combination for lung nodule segmentation. However, the novelty of this approach needs further clarification. The manuscript should provide a more detailed comparison with existing works that use similar architectures or techniques for lung nodule segmentation. Additionally, the manuscript should discuss how the proposed method stands out in terms of performance and practical utility compared to existing methods.\n   \n2. Evaluation Design: The evaluation is conducted solely on the LUNA16 dataset, which is a standard benchmark but limited in scope. The authors should consider evaluating their method on additional datasets to demonstrate broader applicability and generalizability. Furthermore, the manuscript should include a comparison with state-of-the-art methods on multiple datasets to provide a comprehensive assessment of the proposed framework's performance.\n\n3. Data Preprocessing and Augmentation: The preprocessing steps, including the application of Maximum Intensity Projection (MIP), are well-described. However, the manuscript lacks details on the rationale behind choosing a 7.5mm slab thickness for MIP and how this choice affects the model's performance. Additionally, the data augmentation strategies used during training should be better explained, especially given the class imbalance issue.\n\n4. Reproducibility: While the authors mention that the code will be made available, the manuscript lacks sufficient detail on the training protocols, preprocessing steps, and model hyperparameters. Providing a clear and detailed description of these aspects is crucial for reproducibility. Additionally, the manuscript should include a section detailing the hardware and software requirements for replicating the experiments.\n\n###\n\n## Minor Comments\n1. Figures: Figures 1 and 2 are cluttered and could be improved by showing fewer representative slices with zoomed-in regions to enhance readability.\n2. Section 2.1: The introduction of notation in Section 2.1 is insufficiently explained, particularly regarding the forward operator.\n3. Acronyms: Several acronyms (e.g., \"R=4\") are used without definitions.\n4. Typographical Issues: Minor typographical errors such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7) should be corrected.\n\n###\n\n## Summary Paragraph\n(focusing on 4 TMI Editorial Criteria)\nThe manuscript addresses a significant and clinically relevant challenge: accurate segmentation of lung nodules in CT scans. The proposed two-stage framework, leveraging Deformable-DETR and SAM, is technically innovative and shows promising results on the LUNA16 dataset. However, the evaluation is limited to a single dataset, which weakens the claims of significance and generalizability. The manuscript lacks detailed comparisons with state-of-the-art methods and sufficient documentation for reproducibility. While the idea has merit, the current evidence does not fully meet the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor Revision: The authors should expand comparative analysis across multiple datasets, clarify the rationale behind certain preprocessing choices, and provide detailed methodological details to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThis manuscript presents a two-stage framework for automatic segmentation of lung metastases in CT scans, intended to support lung tumor board evaluations. The approach combines a custom Deformable Detection Transformer (Deformable-DETR) with Focal Loss for region proposal and a fine-tuned Segment Anything Model (SAM) for pixel-level segmentation. Reported results show strong performance on the LUNA16 dataset, with a Dice coefficient of 92.4%, 95.2% sensitivity, and 93.2% precision. The paper is well written, and the challenge of class imbalance and nodule sparsity is clearly articulated.  \n\n**Major Comments**  \n1. **Novelty and Positioning:** While integrating Deformable-DETR and SAM is an original combination, the degree of novelty within the broader context of existing lung nodule segmentation research remains unclear. The manuscript should include a more explicit comparison with related methods employing similar architectures and clarify the distinct advantages and contributions of the proposed framework.  \n2. **Evaluation Design:** The performance evaluation is restricted to the LUNA16 dataset. Including experiments on additional datasets would strengthen the evidence for generalizability. Comparisons with current state-of-the-art results across multiple benchmarks are also needed to substantiate the claimed performance benefits.  \n3. **Data Preprocessing and Augmentation:** Although preprocessing steps, including Maximum Intensity Projection (MIP), are described, the rationale for using a 7.5‚ÄØmm slab thickness should be explained, along with its potential impact on performance. Further detail on data augmentation methods‚Äîparticularly in relation to class imbalance‚Äîwould aid interpretability.  \n4. **Reproducibility:** The study lacks sufficient detail on training protocols, preprocessing steps, and hyperparameters. A dedicated section outlining these aspects, along with hardware and software details, would improve transparency and replicability.  \n\n**Minor Comments**  \n- Figures‚ÄØ1 and‚ÄØ2 appear cluttered; focusing on representative slices with zoomed regions could improve clarity.  \n- Notation in Section‚ÄØ2.1, especially the definition of the forward operator, requires clearer explanation.  \n- Some acronyms (e.g., ‚ÄúR‚ÄØ=‚ÄØ4‚Äù) are undefined and should be expanded on first use.  \n- Typos such as ‚Äúk-spacce‚Äù (p.‚ÄØ6) and ‚Äúundersampling maskes‚Äù (p.‚ÄØ7) should be corrected.  \n\n**Summary Paragraph**  \nOverall, the study addresses an important clinical problem and presents a promising technical approach combining transformer-based detection with a large-scale segmentation model. The reported results are encouraging, yet the reliance on a single dataset limits the claims regarding robustness and generalization. Further comparative experiments and enhanced methodological transparency are needed to meet expectations of significance and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision.** The manuscript requires expanded comparative evaluations, justification of preprocessing parameters, and comprehensive methodological documentation before being suitable for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## TRANSFORMER-BASED CT AUTO-SEGMENTATION OF LUNG METASTASES ‚Äì A TUMOR-BOARD APPLICATION\n\n### Summary\n\nThis paper proposes a two-stage pipeline for auto-segmentation of lung nodules on CT, aimed at tumor-board decision support. Stage 1 uses a Deformable-DETR detector trained with focal loss on 7.5 mm MIP slabs to localize nodules under extreme sparsity; Stage 2 uses a fine-tuned MedSAM/SAM, prompted by Stage 1 boxes, to produce pixel-wise masks on individual slices. On a processed LUNA16-derived dataset, the authors report Stage 1 precision 93.3% and sensitivity 95.2% (IoU 0.5), and a Stage 2 Dice of 92.4%.\n\n### Strengths\n\n- Technical novelty and innovationThe two-stage decomposition (transformer-based detector for sparse region proposal, then promptable segmentation) is a pragmatic way to exploit complementary strengths, and the use of MIP to mitigate slice-level sparsity is clinically motivated.Incorporating focal loss into a transformer-based detector addresses extreme class imbalance in a targeted way and is reasonable for this domain.The pipeline design and preprocessing are clearly intended for a tumor-board application, which is a meaningful end-user context.\n- Experimental rigor and validationThe split-before-augmentation protocol is explicitly stated, and some hyperparameters for both stages are reported.Reporting stratification of detection performance by size (small/medium/large) is helpful to interpret small-nodule behavior.\n- Clarity of presentationThe overall pipeline is clearly described with helpful figures and step-by-step narrative of preprocessing (resampling, Otsu lung isolation, CLAHE, MIP).The rationale for the two-stage design is well articulated, including observations that SAM alone underperforms in highly imbalanced detection settings.\n- Significance of contributionsThe work addresses an impactful clinical task ‚Äî reducing radiologist time and improving deliberations in tumor boards.If validated under standard evaluation protocols and patient-wise splits, a high-sensitivity two-stage system could be valuable as a region proposal and segmentation tool in practice.\n\nTechnical novelty and innovation\n\n- The two-stage decomposition (transformer-based detector for sparse region proposal, then promptable segmentation) is a pragmatic way to exploit complementary strengths, and the use of MIP to mitigate slice-level sparsity is clinically motivated.\n- Incorporating focal loss into a transformer-based detector addresses extreme class imbalance in a targeted way and is reasonable for this domain.\n- The pipeline design and preprocessing are clearly intended for a tumor-board application, which is a meaningful end-user context.\n\nExperimental rigor and validation\n\n- The split-before-augmentation protocol is explicitly stated, and some hyperparameters for both stages are reported.\n- Reporting stratification of detection performance by size (small/medium/large) is helpful to interpret small-nodule behavior.\n\nClarity of presentation\n\n- The overall pipeline is clearly described with helpful figures and step-by-step narrative of preprocessing (resampling, Otsu lung isolation, CLAHE, MIP).\n- The rationale for the two-stage design is well articulated, including observations that SAM alone underperforms in highly imbalanced detection settings.\n\nSignificance of contributions\n\n- The work addresses an impactful clinical task ‚Äî reducing radiologist time and improving deliberations in tumor boards.\n- If validated under standard evaluation protocols and patient-wise splits, a high-sensitivity two-stage system could be valuable as a region proposal and segmentation tool in practice.\n\n### Weaknesses\n\n- Technical limitations or concernsNovelty is limited for Stage 1: the specific integration of MIP + Deformable-DETR + focal loss for sparse nodule detection has been reported (e.g., Lung-DETR), making the contribution largely incremental; the main added element here is the SAM-based Stage 2.Some technical statements are inaccurate or underspecified: for example, the self-attention complexity provided (O(H^2W^2C) with C=1) is not representative for deformable attention and backbone features are not single-channel; the loss integration with DETR‚Äôs Hungarian matching and the box-loss terms are not specified.The MIP-to-slice decoupling and prompting strategy in Stage 2 needs more detail to ensure geometric consistency and correct supervision at the slice level.\n- Experimental gaps or methodological issuesPotential data leakage: the split is described at the slice/MIP level, not at the patient/scan level. With MIP slabs coming from the same volume, leakage across train/val/test is likely unless split is patient-wise.Non-standard evaluation for LUNA16 detection: the community uses FROC/CPM with false positives per scan;IoU@0.5precision/recall is not directly comparable and risks overstating performance.The reported dataset size after preprocessing (9,676 MIP slices across 888 scans ‚âà 11 slabs/scan) is unusually small, raising concerns about slice removal rules and representativeness; clarity is needed on how many nodules and slabs per nodule are included.Baselines and ablations are insufficient: there is no comparison to standard detectors (e.g., nnDetection) under the LUNA16 protocol, no controlled ablations for CLAHE/Otsu/MIP thickness, no comparison of SAM vs a strong UNet/VNet under identical bounding-box prompts, and no 3D segmentation baseline.Stage 2 evaluation lacks detail: it is unclear whether Dice is computed per-slice, per-nodule 3D, or image-wise; per-size Dice and failure modes (e.g., juxta-pleural nodules) are not reported.\n- Clarity or presentation issuesMetrics labeling in Table 1 is confusing: ‚ÄúAverage @ IoU 0.5 (All Areas)‚Äù with ‚ÄúPrecision‚Äù and ‚ÄúSensitivity‚Äù reads like AP/AR but is named as precision/recall; Table 2 appears to repeat Stage 1 precision/recall as ‚ÄúSpecificity/Sensitivity,‚Äù which is not standard for detection and may be misleading for segmentation.Important architectural hyperparameters are omitted (number of encoder/decoder layers, number of queries, deformable sampling points, attention heads, feature levels), limiting reproducibility.\n- Missing related work or comparisonsClosely related methods are not compared under common protocols: nnDetection on LUNA16, tri-planar learned projection detectors, and Lung-DETR (MIP + Deformable-DETR + focal loss) should be baseline references for Stage 1.For segmentation, recent ROI-guided and MIP-aware 2D/3D methods (e.g., MESAHA-Net) should be discussed and compared, at least conceptually, and ideally empirically on volume-level Dice using standard ground truth.\n\nTechnical limitations or concerns\n\n- Novelty is limited for Stage 1: the specific integration of MIP + Deformable-DETR + focal loss for sparse nodule detection has been reported (e.g., Lung-DETR), making the contribution largely incremental; the main added element here is the SAM-based Stage 2.\n- Some technical statements are inaccurate or underspecified: for example, the self-attention complexity provided (O(H^2W^2C) with C=1) is not representative for deformable attention and backbone features are not single-channel; the loss integration with DETR‚Äôs Hungarian matching and the box-loss terms are not specified.\n- The MIP-to-slice decoupling and prompting strategy in Stage 2 needs more detail to ensure geometric consistency and correct supervision at the slice level.\n\nExperimental gaps or methodological issues\n\n- Potential data leakage: the split is described at the slice/MIP level, not at the patient/scan level. With MIP slabs coming from the same volume, leakage across train/val/test is likely unless split is patient-wise.\n- Non-standard evaluation for LUNA16 detection: the community uses FROC/CPM with false positives per scan;IoU@0.5precision/recall is not directly comparable and risks overstating performance.\n- The reported dataset size after preprocessing (9,676 MIP slices across 888 scans ‚âà 11 slabs/scan) is unusually small, raising concerns about slice removal rules and representativeness; clarity is needed on how many nodules and slabs per nodule are included.\n- Baselines and ablations are insufficient: there is no comparison to standard detectors (e.g., nnDetection) under the LUNA16 protocol, no controlled ablations for CLAHE/Otsu/MIP thickness, no comparison of SAM vs a strong UNet/VNet under identical bounding-box prompts, and no 3D segmentation baseline.\n- Stage 2 evaluation lacks detail: it is unclear whether Dice is computed per-slice, per-nodule 3D, or image-wise; per-size Dice and failure modes (e.g., juxta-pleural nodules) are not reported.\n\nClarity or presentation issues\n\n- Metrics labeling in Table 1 is confusing: ‚ÄúAverage @ IoU 0.5 (All Areas)‚Äù with ‚ÄúPrecision‚Äù and ‚ÄúSensitivity‚Äù reads like AP/AR but is named as precision/recall; Table 2 appears to repeat Stage 1 precision/recall as ‚ÄúSpecificity/Sensitivity,‚Äù which is not standard for detection and may be misleading for segmentation.\n- Important architectural hyperparameters are omitted (number of encoder/decoder layers, number of queries, deformable sampling points, attention heads, feature levels), limiting reproducibility.\n\nMissing related work or comparisons\n\n- Closely related methods are not compared under common protocols: nnDetection on LUNA16, tri-planar learned projection detectors, and Lung-DETR (MIP + Deformable-DETR + focal loss) should be baseline references for Stage 1.\n- For segmentation, recent ROI-guided and MIP-aware 2D/3D methods (e.g., MESAHA-Net) should be discussed and compared, at least conceptually, and ideally empirically on volume-level Dice using standard ground truth.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe two-stage approach is sensible, but technical details need tightening. DETR-style training typically combines classification (including a ‚Äúno-object‚Äù class) with L1/GIoU losses under Hungarian matching; it is not clear how focal loss is integrated into this setup and what the relative weights are. Without this, replication and understanding of optimization behavior are difficult.The description of attention complexity conflates standard self-attention with deformable attention; deformable attention limits sampling to sparse points (with complexity closer to O(HWk) per head), which is a central justification and should be correctly stated.The MIP slab‚Üíslice mapping for Stage 2 implies that a single 2D box from a slab may correspond to a variable set of slices. The paper should specify how slices outside the slab are handled, how z-extent is determined, how boxes are cropped on slices with partial/no nodule appearance, and how negative slices within the slab contribute to training SAM.The pipeline‚Äôs lung mask erosion and removal of superior/inferior slices can suppress search space, but may also truncate juxta-pleural or apical/basal nodules; risks and safeguards should be addressed.\n- Experimental evaluation assessmentSplitting and leakage: unless explicitly patient-wise, splitting MIP slabs or slices can leak highly similar content across sets. This would inflate detection and segmentation metrics. A patient-wise split is necessary; cross-validation on the official LUNA16 folds would further increase credibility.Evaluation protocol: for LUNA16 detection, FROC and CPM are standard. Reporting those, including sensitivity at FP/scan thresholds and the CPM, is essential for comparison with prior work. IoU=0.5 P/R on images is not sufficient and is not directly comparable.Segmentation evaluation should clarify whether Dice is volumetric (3D nodule masks reconstructed across slices) or averaged per-slice. Many reported baselines in Table 2 use 3D Dice on LIDC/LUNA; the current setting (MIP-driven 2D prompts) may not be directly comparable.Ablations: present controlled ablations for (a) MIP slab thickness (e.g., 3 mm vs 7.5 mm vs 10‚Äì15 mm), (b) with/without CLAHE and Otsu masks, (c) standard DETR vs Deformable-DETR vs CNN baselines under identical preprocessing, and (d) SAM vs a strong UNet/VNet segmentation head under the same box prompts. Include small-nodule (‚â§6‚Äì7 mm) stratified analyses in Stage 2 (Dice and detection miss rate).External validation: even a modest external test (e.g., an NLST subset or a held-out LIDC center) would help assess generalization. Without this, claims about clinical robustness are tentative.\n- Comparison with related work (using the summaries provided)Lung-DETR demonstrates the exact Stage 1 combination of MIP + Deformable-DETR + focal loss on LUNA16 with similar metrics and limitations (small-nodule performance, dependency on preprocessing), highlighting limited novelty for Stage 1; the present work mainly adds a SAM-based segmentation refinement.nnDetection provides a strong, self-configuring detection baseline and reports CPM ‚âà 0.93 on LUNA16; comparisons under FROC/CPM would be informative and may challenge the superiority claims.The tri-planar learned projection approach (a learned analogue to MIP combined with a U-Net segmentation head) achieves mean sensitivity ‚âà0.959 on LUNA16 and eliminates FP-reduction stages; it provides a pertinent alternative to fixed MIP and should be discussed as a potentially more expressive variant of Stage 1 with 2D segmentation.The DETR-in-medical-imaging study (transfer of DETR design choices) shows simpler configurations can work as well or better on LUNA16, and warns against over-complicating query init/refinement; this supports the need to report and possibly simplify the current configuration, and to provide ablations on encoder depth, feature levels, and query count.For segmentation, MESAHA-Net leverages bidirectional MIPs and ROI attention to reconstruct volumetric masks efficiently with strong Dice; this is directly related to Stage 2 and should be compared or at least positioned.CASED presents a principled curriculum and hard-negative mining strategy for extreme imbalance; it may be a complementary or alternative to focal loss worth testing or acknowledging more explicitly.\n- Discussion of broader impact and significanceThe paper‚Äôs focus on tumor-board support and realistic nodule sparsity is commendable. However, clinical relevance depends on robust, patient-wise evaluation, standard metrics (FROC/CPM), and a careful false-negative analysis for small nodules. The current preprocessing (mask erosion, slice removal) and MIP dependence may introduce failure modes (juxta-pleural/apical/basal nodules, motion, calcifications) that need explicit study.Interpretability and workflow integration: region proposals with calibrated confidence and per-nodule volumetric masks could be valuable for longitudinal monitoring. Reporting calibration metrics, per-case inference time, and human factors (review time saved) would strengthen the case for tumor-board applications.\n\nTechnical soundness evaluation\n\n- The two-stage approach is sensible, but technical details need tightening. DETR-style training typically combines classification (including a ‚Äúno-object‚Äù class) with L1/GIoU losses under Hungarian matching; it is not clear how focal loss is integrated into this setup and what the relative weights are. Without this, replication and understanding of optimization behavior are difficult.\n- The description of attention complexity conflates standard self-attention with deformable attention; deformable attention limits sampling to sparse points (with complexity closer to O(HWk) per head), which is a central justification and should be correctly stated.\n- The MIP slab‚Üíslice mapping for Stage 2 implies that a single 2D box from a slab may correspond to a variable set of slices. The paper should specify how slices outside the slab are handled, how z-extent is determined, how boxes are cropped on slices with partial/no nodule appearance, and how negative slices within the slab contribute to training SAM.\n- The pipeline‚Äôs lung mask erosion and removal of superior/inferior slices can suppress search space, but may also truncate juxta-pleural or apical/basal nodules; risks and safeguards should be addressed.\n\nExperimental evaluation assessment\n\n- Splitting and leakage: unless explicitly patient-wise, splitting MIP slabs or slices can leak highly similar content across sets. This would inflate detection and segmentation metrics. A patient-wise split is necessary; cross-validation on the official LUNA16 folds would further increase credibility.\n- Evaluation protocol: for LUNA16 detection, FROC and CPM are standard. Reporting those, including sensitivity at FP/scan thresholds and the CPM, is essential for comparison with prior work. IoU=0.5 P/R on images is not sufficient and is not directly comparable.\n- Segmentation evaluation should clarify whether Dice is volumetric (3D nodule masks reconstructed across slices) or averaged per-slice. Many reported baselines in Table 2 use 3D Dice on LIDC/LUNA; the current setting (MIP-driven 2D prompts) may not be directly comparable.\n- Ablations: present controlled ablations for (a) MIP slab thickness (e.g., 3 mm vs 7.5 mm vs 10‚Äì15 mm), (b) with/without CLAHE and Otsu masks, (c) standard DETR vs Deformable-DETR vs CNN baselines under identical preprocessing, and (d) SAM vs a strong UNet/VNet segmentation head under the same box prompts. Include small-nodule (‚â§6‚Äì7 mm) stratified analyses in Stage 2 (Dice and detection miss rate).\n- External validation: even a modest external test (e.g., an NLST subset or a held-out LIDC center) would help assess generalization. Without this, claims about clinical robustness are tentative.\n\nComparison with related work (using the summaries provided)\n\n- Lung-DETR demonstrates the exact Stage 1 combination of MIP + Deformable-DETR + focal loss on LUNA16 with similar metrics and limitations (small-nodule performance, dependency on preprocessing), highlighting limited novelty for Stage 1; the present work mainly adds a SAM-based segmentation refinement.\n- nnDetection provides a strong, self-configuring detection baseline and reports CPM ‚âà 0.93 on LUNA16; comparisons under FROC/CPM would be informative and may challenge the superiority claims.\n- The tri-planar learned projection approach (a learned analogue to MIP combined with a U-Net segmentation head) achieves mean sensitivity ‚âà0.959 on LUNA16 and eliminates FP-reduction stages; it provides a pertinent alternative to fixed MIP and should be discussed as a potentially more expressive variant of Stage 1 with 2D segmentation.\n- The DETR-in-medical-imaging study (transfer of DETR design choices) shows simpler configurations can work as well or better on LUNA16, and warns against over-complicating query init/refinement; this supports the need to report and possibly simplify the current configuration, and to provide ablations on encoder depth, feature levels, and query count.\n- For segmentation, MESAHA-Net leverages bidirectional MIPs and ROI attention to reconstruct volumetric masks efficiently with strong Dice; this is directly related to Stage 2 and should be compared or at least positioned.\n- CASED presents a principled curriculum and hard-negative mining strategy for extreme imbalance; it may be a complementary or alternative to focal loss worth testing or acknowledging more explicitly.\n\nDiscussion of broader impact and significance\n\n- The paper‚Äôs focus on tumor-board support and realistic nodule sparsity is commendable. However, clinical relevance depends on robust, patient-wise evaluation, standard metrics (FROC/CPM), and a careful false-negative analysis for small nodules. The current preprocessing (mask erosion, slice removal) and MIP dependence may introduce failure modes (juxta-pleural/apical/basal nodules, motion, calcifications) that need explicit study.\n- Interpretability and workflow integration: region proposals with calibrated confidence and per-nodule volumetric masks could be valuable for longitudinal monitoring. Reporting calibration metrics, per-case inference time, and human factors (review time saved) would strengthen the case for tumor-board applications.\n\n### Questions for Authors\n\n- Was the train/val/test split performed at the patient/scan level? If not, can you re-run experiments with patient-wise splits (or the official LUNA16 folds) to rule out leakage and report FROC/CPM?\n- How exactly is focal loss integrated into the DETR training with Hungarian matching? What are the loss weights for classification vs L1/GIoU, and do you include the ‚Äúno-object‚Äù class in focal loss?\n- Please specify Deformable-DETR hyperparameters: number of encoder/decoder layers, attention heads, sampling points per head, number of feature levels, number of queries, and feature map strides.\n- How do you map a 2D MIP box back to slices for SAM prompting? Do you segment all slices in the slab? How do you handle slices with no visible nodule within the slab? How is ground-truth supervision provided at the slice level (2D masks) vs volumetric masks (3D Dice)?\n- What is the precise definition of the Stage 2 Dice metric (per-slice average, per-nodule volumetric Dice, per-image)? Can you report per-size Dice and failure cases (e.g., juxta-pleural, sub-6 mm nodules)?\n- In Table 2, are ‚ÄúSensitivity‚Äù and ‚ÄúSpecificity‚Äù computed for segmentation or copied from Stage 1 detection metrics? Please clarify and ensure apples-to-apples comparisons with cited works.\n- The dataset shrinks to 9,676 MIP images (‚âà11 per scan). What rules caused such aggressive slice removal? How many slabs per nodule remain on average, and what fraction of nodules are excluded?\n- Did you evaluate the method under standard LUNA16 detection metrics (FROC/CPM, sensitivity at fixed FP/scan)? How does Stage 1 compare to nnDetection and to tri-planar learned projection methods?\n- For SAM fine-tuning: which components are frozen vs trainable, what learning rates/epochs were used, and what was the prompt strategy during training (boxes only, points, masks)? Any ablation vs a UNet/VNet baseline under identical prompts?\n- Can you provide ablations for MIP slab thickness, CLAHE/Otsu, and sensitivity analyses for small nodules (‚â§6‚Äì7 mm) including false negative examples and confusion with vessels/bronchi?\n\n### Overall Assessment\n\nThe paper addresses an important and clinically relevant problem with a sensible two-stage design and encouraging preliminary results. However, the novelty for Stage 1 is limited relative to existing work that already integrates MIP + Deformable-DETR + focal loss on LUNA16; the added value is primarily the SAM-based segmentation refinement. More critically, the evaluation as presented is not aligned with standard LUNA16 protocols, lacks a patient-wise split (raising potential leakage), and provides limited ablation and baseline comparisons. Several technical details (loss integration, architecture hyperparameters, slice-level supervision for SAM) are underspecified, and some reported metrics/tables are confusing or potentially misaligned. With a rigorous, patient-wise, FROC/CPM-based re-evaluation, clear volumetric segmentation metrics, comprehensive ablations, and comparisons to strong baselines (nnDetection, tri-planar learned projections, UNet-based segmentation), this work could become a solid, practically useful contribution for tumor-board workflows. In its current form, it reads as a promising but preliminary study and would likely need substantial experimental strengthening and clarifications for top-tier publication.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a two-stage transformer-based pipeline for automatic segmentation of lung metastases on CT, designed to support tumor-board decision-making. Stage‚ÄØ1 employs a Deformable-DETR detector trained with focal loss on 7.5‚ÄØmm MIP slabs to identify sparse nodules, while Stage‚ÄØ2 uses a fine-tuned MedSAM/SAM model, prompted by Stage‚ÄØ1 boxes, for pixel-level segmentation. Experiments on a processed LUNA16-derived dataset yield promising detection and segmentation metrics. The work is clearly motivated by clinical relevance, with a well-articulated rationale and detailed pipeline description. Overall, it proposes a practically oriented framework with potential clinical utility, though several methodological and evaluation issues limit its current evidential strength.\n\n---\n\n**Major Comments**  \n1. **Novelty and Technical Details** ‚Äì Stage‚ÄØ1‚Äôs design (MIP‚ÄØ+‚ÄØDeformable-DETR‚ÄØ+‚ÄØfocal‚ÄØloss) largely mirrors prior work such as Lung-DETR, making the novelty incremental; the principal addition is the SAM-based segmentation refinement. Some technical descriptions are inaccurate or incomplete (e.g., attention complexity, loss integration with Hungarian matching, unspecified architectural hyperparameters). The MIP-to-slice prompting in Stage‚ÄØ2 lacks sufficient detail for reproducibility or assurance of geometric consistency.  \n2. **Evaluation Design and Data Splitting** ‚Äì The dataset appears split at slice/MIP rather than patient level, risking leakage across sets. Standard LUNA16 metrics (FROC/CPM) are not reported, impeding fair comparison. The dataset size and slice-removal rules need clarification to assess representativeness.  \n3. **Baselines and Ablations** ‚Äì The study omits crucial baselines (e.g., nnDetection, tri-planar learned projection detectors, UNet/VNet segmenters) and parameter ablations (MIP thickness, preprocessing variants, segmentation head comparison). These are necessary to understand contribution and performance robustness.  \n4. **Segmentation Metrics and Reporting** ‚Äì The Dice metric definition (per-slice vs‚ÄØ3D) is unclear, and stage-specific metrics appear inconsistently labeled. Small-nodule and failure-case analyses are absent.  \n5. **Related Work and Reproducibility** ‚Äì Closely related approaches (nnDetection, Lung-DETR, MESAHA-Net) are not convincingly compared or contextualized. Omitted model parameters hinder reproducibility.  \n6. **Potential Clinical and Methodological Risks** ‚Äì Preprocessing choices (mask erosion, slab cropping) may remove clinically significant nodules. Without patient-wise evaluation and external validation, claims of clinical robustness remain tentative.\n\n---\n\n**Minor Comments**  \n- Figures and preprocessing descriptions are clear, but metrics labeling in Tables‚ÄØ1‚Äì2 is confusing and terminology inconsistent (precision vs‚ÄØAP, sensitivity vs‚ÄØspecificity).  \n- Report all Deformable-DETR configuration details (encoder/decoder layers, heads, queries).  \n- Clarify the computation of Stage‚ÄØ2 Dice and whether segmentation results correspond to volumetric or slice-level targets.  \n- Discuss computational cost, per-case inference time, and calibration for tumor-board integration.  \n\n---\n\n**Summary Paragraph**  \nThis study proposes a conceptually sound and well-described two-stage transformer pipeline with convincing clinical motivation and solid preliminary results. Its strengths lie in clear presentation, task relevance, and thoughtful sensor design choices. However, the experimental framework lacks alignment with standard protocols, adequate ablations, and baseline comparisons, while some methodological details remain underspecified. The paper would benefit substantially from patient-wise and FROC-based validation, inclusion of stronger baselines, explicit architectural parameters, and a clearer description of segmentation evaluation. At present, the contribution is promising but preliminary, requiring significantly stronger experimental evidence and clarification to meet publication standards.\n\n---\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThe manuscript addresses the critical challenge of accurately segmenting lung nodules in CT scans, which is complicated by extreme class imbalance where nodules appear sparsely among healthy tissue (typically only 0-5% of slices contain nodules). The authors propose a novel two-stage approach framing the problem as anomaly detection, where Stage 1 employs a custom Deformable Detection Transformer with Focal Loss to overcome class imbalance and localize sparse tumors, and Stage 2 refines bounding boxes into pixel-wise segmentation masks using a fine-tuned variant of Meta's Segment Anything Model (SAM). To address nodule sparsity and improve spatial context, the authors incorporate a 7.5mm Maximum Intensity Projection (MIP) that aids in differentiating nodules from bronchioles and vascular structures. The approach demonstrates strong performance with a Dice coefficient of 92.4%, 95.2% sensitivity, and 93.2% precision on the LUNA16 dataset, significantly outperforming existing methods as shown in their comparative analysis.\n\n## 2. Major and Minor Comments\n\n### Major Strengths\n- The two-stage approach effectively addresses extreme class imbalance by separating detection (Stage 1) and segmentation (Stage 2), allowing each model to operate within its strengths\n- The custom Deformable DETR architecture with Focal Loss specifically designed for sparse nodule detection represents a meaningful advancement over previous approaches\n- The integration of MIP as a preprocessing step is clinically relevant and well-justified, improving spatial context for nodule differentiation\n- The paper demonstrates state-of-the-art performance (92.4% Dice coefficient) that surpasses existing methods by a significant margin\n\n### Major Limitations\n- The manuscript lacks sufficient discussion on model generalizability across different CT scanner types and acquisition protocols, which is critical for clinical adoption\n- There is no detailed analysis of false positives/negatives in clinical context - what is the clinical impact of the 7.6% missed nodules, particularly for small nodules?\n- The training methodology doesn't explicitly address challenges with different nodule types (e.g., ground glass vs solid nodules), which have different clinical implications\n- The paper would benefit from an ablation study to quantify the contribution of each component (MIP, Focal Loss, etc.) to the final performance\n\n### Minor Strengths\n- Excellent visualization of the processing pipeline in Figure 1 clearly demonstrates the MIP enhancement effect\n- The architectural diagrams (Figures 2-4) provide clear, informative depictions of the two-stage approach\n- The comparison with existing methods in Table 2 is comprehensive and well-presented\n- The methodology section is well-structured with appropriate technical details\n\n### Minor Limitations\n- Stage 2 implementation details are less thoroughly described compared to Stage 1, particularly regarding SAM fine-tuning specifics\n- Figure 4 could benefit from more detailed annotations to better illustrate the model's performance on challenging cases\n- Computational time metrics are missing, which is important for evaluating clinical workflow integration potential\n- The methodology for achieving exactly 5% nodule sparsity in the test set could be described more clearly\n\n## 3. Evaluation According to TMI Editorial Criteria\n\n**Significance**: The work addresses a clinically significant problem with clear potential to improve tumor board workflows. The authors effectively justify the clinical relevance of automating lung nodule segmentation, noting how manual segmentation slows decision-making and increases resource demands. The integration of this tool into clinical tumor board settings could enhance workflow efficiency, improve patient outcomes, and reduce costs, making the work highly significant for medical imaging applications.\n\n**Innovation**: The paper presents a novel integration of transformer architectures specifically tailored for the challenge of sparse lung nodule segmentation. While individual components (Deformable DETR, SAM) have been used previously, their combination with MIP preprocessing and custom Focal Loss for this specific application represents a meaningful innovation. The two-stage approach that frames nodule segmentation as anomaly detection is particularly innovative and addresses a key limitation in existing approaches.\n\n**Evaluation**: The evaluation is generally thorough, using appropriate metrics (Dice coefficient, sensitivity, precision) on the standard LUNA16 benchmark. The comparison with existing methods in Table 2 is comprehensive and strengthens the paper's claims. However, the evaluation would benefit from more detailed analysis of failure cases, particularly for small nodules (up to 7mm), and lacks an ablation study to quantify component contributions. External validation on a different dataset would strengthen the clinical relevance claims.\n\n**Reproducibility**: The methodology is well-documented with clear descriptions of data preprocessing, network architecture, and training parameters. Specific implementation details (ResNet-50 backbone, AdamW optimizer with learning rates, batch size) are provided. However, some details for Stage 2 implementation and the exact MIP implementation parameters could be more precise. The paper would benefit from referencing code availability to enhance reproducibility.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nWhile this paper presents a promising approach with strong performance results, several critical revisions are required before acceptance. The authors should:\n1. Provide a detailed analysis of failure cases, particularly false negatives for small nodules, with clinical context\n2. Include an ablation study to quantify the contribution of each component (MIP, Focal Loss, etc.)\n3. Address model generalizability across different CT scanner types and acquisition protocols\n4. Add computational time metrics to assess clinical workflow integration potential\n5. Clarify the specific methodology for handling different nodule types (ground glass vs solid)\n\nThe work has significant potential to advance the field and improve clinical practice, but these revisions are necessary to meet TMI's standards for clinical relevance, methodological rigor, and reproducibility. I encourage the authors to address these concerns comprehensively in their revision.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript focuses on improving automatic lung nodule segmentation in CT scans, tackling the difficulty of extreme class imbalance where nodules appear in only a small fraction of slices. The study introduces a two-stage pipeline that conceptualizes the task as anomaly detection. In Stage 1, a custom Deformable Detection Transformer with Focal Loss aims to localize rare nodules, while Stage 2 refines these detections into pixel-wise segmentation masks using a fine-tuned variant of Meta‚Äôs Segment Anything Model (SAM). The authors also employ a 7.5‚ÄØmm Maximum Intensity Projection (MIP) to enhance spatial context and distinguish nodules from bronchioles and vascular structures. Evaluated on the LUNA16 dataset, the model reports strong performance (Dice‚ÄØ=‚ÄØ92.4%, Sensitivity‚ÄØ=‚ÄØ95.2%, Precision‚ÄØ=‚ÄØ93.2%) and outperforms existing approaches according to their comparisons.\n\n**Major Comments**  \n1. The two-stage design effectively separates detection and segmentation, addressing class imbalance and leveraging the strengths of each network.  \n2. The custom Deformable DETR with Focal Loss and MIP preprocessing constitutes a meaningful and clinically relevant contribution.  \n3. Despite strong performance, the paper lacks discussion on model generalizability across scanner types and acquisition protocols, which is crucial for clinical use.  \n4. Failure case analysis is incomplete; the absence of detailed examination of false positives/negatives‚Äîparticularly small missed nodules‚Äîlimits assessment of clinical impact.  \n5. The method does not address performance differences across nodule types (e.g., ground-glass vs. solid), which may affect diagnostic applicability.  \n6. An ablation study quantifying the contributions of MIP, Focal Loss, and SAM would strengthen claims regarding method components.  \n7. Computational efficiency and runtime are not reported, which is important for determining clinical workflow suitability.\n\n**Minor Comments**  \n- Figures 1‚Äì4 clearly convey the system architecture, though Figure‚ÄØ4 could include richer annotations for challenging examples.  \n- The description of Stage‚ÄØ2 (SAM fine-tuning) should match the detail provided for Stage‚ÄØ1.  \n- Clarify the method used to achieve 5% nodule sparsity in the test data.  \n- Referencing or releasing code could further support reproducibility.\n\n**Summary Paragraph**  \nThis work addresses an important clinical imaging problem and proposes a technically innovative and well-motivated two-stage solution that integrates transformer-based detection and SAM-based segmentation. Its strong quantitative results and clear clinical relevance make it promising. However, the current version lacks essential analyses on generalizability, component contribution, computational cost, and clinical error interpretation. Addressing these areas would substantially improve methodological rigor and the paper‚Äôs applicability to real-world settings.\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The method is innovative and promising, but further experiments and clarifications are required to meet standards of clinical validity, reproducibility, and completeness.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe manuscript tackles the well‚Äëknown problem of severe class imbalance when segmenting lung nodules on CT, a difficulty that hampers efficient tumor‚Äëboard review. The authors work with the LUNA16 cohort (888 thoracic CT scans) which are resampled to 1‚ÄØmm¬≥ voxels and processed using Otsu thresholding, CLAHE, and a 7.5‚ÄØmm maximum‚Äëintensity projection (MIP) to make nodules more conspicuous. Their methodology consists of a two‚Äëstage pipeline: the first stage uses a custom Deformable‚ÄëDETR trained with focal loss to generate bounding‚Äëbox proposals, and the second stage refines those boxes into voxel‚Äëwise masks via a fine‚Äëtuned Segment‚ÄëAnything Model (SAM) that receives the boxes as prompts. On the held‚Äëout test set the authors report a Dice of 92.4‚ÄØ%, sensitivity of 95.2‚ÄØ% and precision of 93.3‚ÄØ%. The principal novelty claimed is the combination of a transformer‚Äëbased detector with a promptable segmentation model to cope with the sparsity of lung nodules in a clinically realistic workflow.\n\n---\n\n**## General feedback**\n\n- **Relevance** ‚Äì The problem of extreme class imbalance (‚âà5‚ÄØ% nodule prevalence) is genuine, and an automated solution could indeed lessen manual workload during tumor‚Äëboard discussions.  \n- **Originality** ‚Äì Merging Deformable‚ÄëDETR with a SAM‚Äëstyle segmentation head and introducing a thin‚Äëslice MIP as a radiologist‚Äëinspired preprocessing step are interesting ideas.  \n- **Empirical validation** ‚Äì The study is confined to a single public dataset (LUNA16). No external or multi‚Äëcenter test set is provided, making it difficult to assess generalizability. Moreover, the baselines reported in Table‚ÄØ2 appear to be taken from earlier publications without re‚Äëcomputing them on the identical split, which raises concerns about the fairness of the comparison. The manuscript also lacks any statistical testing or confidence intervals for the reported metrics.  \n- **Reproducibility** ‚Äì Several crucial training details (e.g., the fine‚Äëtuning schedule for SAM, the full set of focal‚Äëloss hyper‚Äëparameters, data‚Äëaugmentation specifics, random‚Äëseed information for the split) are omitted. The authors do not make code, pretrained weights, or preprocessing scripts publicly available.\n\n---\n\n**## Specific comments/critiques**\n\n1. **Potential data leakage** ‚Äì Section‚ÄØ3.2 states a 70/20/10 split but does not clarify whether the division is performed at the patient level or whether a fixed random seed is used. Without this information, inadvertent overlap between training and test sets cannot be ruled out.  \n2. **Baseline comparison** ‚Äì Table‚ÄØ2 lists Dice, sensitivity and specificity for prior CNN/U‚ÄëNet approaches, yet it is unclear whether these numbers were recomputed on the same test split or copied from the original works. This ambiguity undermines the claim of superiority.  \n3. **Ablation of the MIP step** ‚Äì The manuscript argues that the 7.5‚ÄØmm MIP improves discrimination (Figure‚ÄØ1), but no experiment isolates the effect of MIP by training the pipeline without it. Consequently, the assertion remains unsupported.  \n4. **Focal‚Äëloss tuning** ‚Äì Only Œ±‚ÄØ=‚ÄØ0.25 and Œ≥‚ÄØ=‚ÄØ2 are reported as ‚Äúoptimal‚Äù in Section‚ÄØ3.6, with no description of the hyper‚Äëparameter search or analysis of how these values influence recall for small nodules.  \n5. **SAM fine‚Äëtuning specifics** ‚Äì Section‚ÄØ3.5 mentions a ‚ÄúDice‚ÄëCrossEntropy loss‚Äù but omits the optimizer, learning‚Äërate schedule, number of epochs, batch size, and the relative weighting of the two loss components. Replicating the fine‚Äëtuning procedure is therefore impracticable.  \n6. **Statistical rigor** ‚Äì Table‚ÄØ1 provides point estimates (Dice‚ÄØ=‚ÄØ92.4‚ÄØ%, etc.) without confidence intervals or hypothesis testing against the baselines, leaving the statistical significance of the improvements ambiguous.  \n7. **Runtime considerations** ‚Äì The claim of ‚Äúefficient clinical deployment‚Äù (Section‚ÄØ3.5) is not backed by any measurement of inference time, GPU memory consumption, or throughput, which are essential for assessing practical feasibility.  \n8. **Performance on small nodules** ‚Äì The ‚ÄúSmall Areas‚Äù entry in Table‚ÄØ1 shows notably lower precision and recall, yet the manuscript offers no discussion of mitigation strategies or the potential clinical ramifications of missing small lesions.  \n9. **Availability of resources** ‚Äì No statement is made regarding the release of source code, pretrained models, or an online demo. For a method that relies on transformer architectures, such transparency is increasingly expected.  \n10. **Presentation issues** ‚Äì The related‚Äëwork section contains stray numbers (e.g., ‚Äú054‚Äë064‚Äù), and several figures (2‚Äì4) lack essential annotations such as scale bars or axis labels, which hampers interpretation.  \n11. **Loss weighting** ‚Äì The combined Dice‚ÄëCrossEntropy loss is introduced without specifying the weighting factor that balances overlap versus boundary accuracy; this omission further obscures reproducibility.  \n12. **Data augmentation details** ‚Äì While Section‚ÄØ3.4 mentions ‚Äúbasic augmentations,‚Äù it does not clarify whether rotations, scalings, or intensity perturbations were applied to the MIP images, an omission that could affect the model‚Äôs robustness to variability in clinical data.\n\n---\n\n**## A suggested decision**\n\n**Reject** ‚Äì The manuscript falls short of the journal‚Äôs standards in several respects: insufficient external validation, lack of ablation studies, questionable baseline comparisons, absence of statistical significance testing, and missing reproducibility information. Addressing these deficiencies would require extensive additional experiments and documentation.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the challenge of severe class imbalance in lung nodule segmentation from CT scans, proposing a two‚Äëstage approach that integrates a Deformable‚ÄëDETR detector trained with focal loss and a fine‚Äëtuned Segment‚ÄëAnything Model (SAM) for voxel‚Äëlevel refinement. Preprocessing includes Otsu thresholding, CLAHE, and a 7.5‚ÄØmm maximum‚Äëintensity projection (MIP) to enhance nodule visibility. Using the LUNA16 dataset, the authors report a Dice score of 92.4‚ÄØ%, sensitivity of 95.2‚ÄØ%, and precision of 93.3‚ÄØ%. The primary contribution is the combination of a transformer‚Äëbased detector with a prompt‚Äëdriven segmentation model aimed at handling sparse targets in a clinical workflow. While conceptually appealing, the study presents several methodological and reproducibility issues that limit its current reliability.  \n\n---\n\n**Major Comments**  \n1. **Evaluation scope and generalizability** ‚Äì Experiments are limited to the LUNA16 dataset without external or multi‚Äëcenter validation, reducing confidence in the model‚Äôs robustness across populations.  \n2. **Baseline fairness** ‚Äì Results from previous CNN/U‚ÄëNet methods appear to be taken directly from prior publications rather than recomputed on the same split, compromising the fairness of the comparison.  \n3. **Data leakage risk** ‚Äì The data split (70/20/10) is not stated to be at the patient level, nor is a random seed mentioned; possible overlap between training and test sets cannot be excluded.  \n4. **Lack of ablation and statistical testing** ‚Äì The claimed benefit of the MIP preprocessing step is unverified because no ablation study is performed. Reported metrics lack confidence intervals or hypothesis testing against baselines.  \n5. **Incomplete methodological details** ‚Äì Key parameters of focal‚Äëloss tuning, SAM fine‚Äëtuning (optimizer, schedule, batch size, loss weighting), and data augmentation are omitted, hindering reproducibility.  \n6. **Runtime and practicality** ‚Äì No inference‚Äëtime or hardware‚Äëefficiency measurements are provided despite claims of clinical efficiency.  \n7. **Performance on small nodules** ‚Äì Lower scores for small regions are reported but not analyzed or addressed.  \n8. **Transparency** ‚Äì Source code, pretrained weights, and preprocessing scripts are not made available.  \n\n---\n\n**Minor Comments**  \n- Typographical and formatting inconsistencies exist in the related‚Äëwork section (e.g., stray numbers ‚Äú054‚Äë064‚Äù).  \n- Figures‚ÄØ2‚Äì4 lack scale bars and axis labels, limiting interpretability.  \n- Clarify the weighting factor used in the Dice‚ÄëCrossEntropy loss combination.  \n\n---\n\n**Summary Paragraph**  \nThe manuscript proposes an imaginative integration of transformer‚Äëbased detection and promptable segmentation for lung nodule analysis, reflecting a potentially useful direction for addressing class imbalance. However, the absence of external validation, incomplete methodological disclosure, unclear baseline handling, and lack of statistical rigor substantially weaken the credibility and reproducibility of the reported results. Substantial additional experimentation, documentation, and transparency would be necessary before the contribution could be considered reliable.  \n\n---\n\n**Decision Recommendation**  \n**Reject** ‚Äì The study suffers from insufficient validation, missing methodological details, unverified claims, and inadequate reproducibility, requiring major additional work to meet publication standards.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Hooman Ramezani"
    ],
    "url": "pdfs/iclr.cc-2025-conference_3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "remote_url": "https://openreview.net/pdf/3b2e1c08161048b8ede6314001a4010437994508.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "A New 3D Image Block Ranking Method Using Axial, Coronal and Sagittal Image Patch Rankings for Explainable Medical Imaging",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "interpretability and explainable AI"
    ],
    "keywords": [
      "convolutional neural networks",
      "feature selection",
      "gradcam",
      "medical imaging",
      "disease diagnosis",
      "image classification"
    ],
    "abstract": "Although a 3D Convolutional Neural Network (CNN) has been applied to explainable\nmedical imaging in recent years, understanding the relationships among input\n2D image patches, input 3D image blocks, extracted feature maps, top-ranked\nfeatures, heatmaps, and final diagnosis remains a significant challenge. To help\naddress this important challenge, firstly, we create a new 2D Grad-CAM-based\nmethod using feature selection to produce explainable 2D heatmaps with a small\nnumber of highlighted image patches corresponding to top-ranked features. Secondly,\nwe design a new 2D image patch ranking algorithm that leverages the newly\ndefined feature matrices and relevant statistical data from numerous heatmaps to\nreliably rank axial patches, coronal patches, and sagittal patches. Thirdly, we create\na novel 3D image block ranking algorithm to generate a ‚ÄúBlock Ranking Map\n(BRM)‚Äù by using the axial patch ranking scores, coronal patch ranking scores, and\nsagittal patch ranking scores. Lastly, we develop a hybrid 3D image block ranking\nalgorithm to generate a reliable hybrid BRM by using different block ranking\nscores generated by the 3D image block ranking algorithm using different top feature\nsets. The associations between brain areas and a brain disease are reliably\ngenerated by using hybrid information from ChatGPT and relevant publications.\nThe simulation results using two different 3D data sets indicate that the novel hybrid\n3D image block ranking algorithm can identify top-ranked blocks associated\nwith important brain areas related to AD diagnosis and autism diagnosis. A doctor\nmay conveniently use the hybrid BRM with axial, coronal, and sagittal views\nto better understand the relationship between the top-ranked blocks and medical\ndiagnosis, and then can efficiently and effectively make a rational and explainable\nmedical diagnosis.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nAuthors present a method for modifying Grad-CAM feature attribution maps that is able to identify the most important 'blocks' in a 3D MRI brain image. The method is applied to output features of a trained CNN by sectioning each 3D image axis into an equal number of patches. The features from Grad-CAM are passed through a feature selection (FS) method consisting of a combination of standard library functions including recursive feature elimination (RFE). The resulting `k` ranked features are accumulated axis- (or patch-) wise, and the patches from different combinations of FS methods are passed into final step that aggregates the 2D features into 3D block features. The blocks that contain the most patches with highest ranking features are selected as the most important blocks. Authors evaluate their method by verifying that the selected blocks correspond with those that are know to be important in the literature, as well as asking Chat-GPT. Author's provide clinical reasoning to explain the attributions.\n\n### Soundness: 3\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* The target of the Authors' work is important and moves towards a more explainable and trustworthy result for use by clinicians.\n* Authors are thorough in their definitions and attempt to give the reader the detail to reproduce their work.\n\n### Weaknesses\n\n* In general, this paper is badly formatted and overly verbose. The notation is difficult to keep consistent and often badly defined. Further comments to this are made in the `Questions` section below.\n* Authors spend very little time reviewing prior work and setting their method in context - Authors should add explanations of related prior work including methods for using statistical analysis of Grad-CAM attribution and saliency maps to identify significant regions in input data.\n* Authors inexplicably use Chat-GPT to test the reliability of their method by asking it to verify the important brain regions associated with AD - there is no explanation for this, and no reason to be doing that instead of actually asking clinicians.\n* Authors present their method only on a single 3D MRI dataset and do not discuss its applicability to other modalities, or indeed any other domain. Authors cannot claim that this is a method for \"Explainable Medical Imaging\" when results on a single dataset are reported.\n\n### Questions\n\n### Major Comments\n* The definitions and notation presented in the paper are cumbersome, and difficult to read. This starts with the statement `P (H-bar/H) x (W-bar/W) patches for P=HW` (line 92) that is repeated several times (187, 214 etc.) which could be replaced by a less obtuse definition of patch size. Authors also do not define _n_ in this Section 2. Authors should better define their variables. Additionally, the cumbersome notation in definitions 1-6 is largely unnecessary - Authors can simplify this section by removing superfluous 'definitions' and describing the process through which they yield the ranking matrices: this will avoid repetition of the `i` and `j` indices and shorten this bloated paper.\n* On a similar note, Authors introduce additional notation for the 'top feature map' `T^Q`. This terminology is confusing. It is not a top \"feature map\", but rather an aggregated \"top k features\" map combining the Grad-CAM features selected by 'some feature selection method'. Authors should consider re-wording this.\n* Lines 221-234 - Authors present their 8 steps for Image Block Ranking algorithm. This is presented badly - the reader is capable of understanding that the same steps are applied to the 3 different axis without making each step of this method so verbose. Figure 1 shows this much better in fact. Authors should describe steps 1-7 on a single axis to improve readability.\n* Authors show a visualization of important blocks identified by their method in Figure 2b. The full Grad-CAM output without feature selection is shown in Figure 3b. It is evident that applying some thresholding to the full feature map, and even just applying the brain-boundary regional constraints, would yield a similar map to their own. Can author's comment on the significant differences between using the Grad-CAM values directly in this way, rather than the additional steps in their method? It would have been helpful to show some quantitative comparison in their results given this is supposed to be an extension of Grad-CAM -based methods.\n* Why did Authors use Chat-GPT in their evaluations? What is the benefit of this over asking the clinicians which Authors repeated claim that this system is aimed at helping?\n* Have Authors used this method on other domains or modalities to demonstrate its effectiveness?\n\n### Minor Comments\n* The paragraph from lines 57 contains a lot of repetition and should be pared down: this sentiment is reflected in many parts of the paper. The reader is clear that having better explainability is important and that a clinician can use this information to inform their diagnosis.\n* Line 147 \"... makes a more impact on the decision\" - to what decision are the Authors referring?\n* Line 148 \"... we use a trained CNN to generate L heatmaps...\" - the language here is confusing: Authors are referring to the model on which they are performing Grad-CAM, not a random trained model that generates heatmaps.\n* Table 2 is unclear - why is much of the table blank?\n* Line 489 - this is confusing: what is meant by the 'the 9th ... patches are shown'?. There are 16x16 patches in each of the 3 axis images shown, and a single patch at (6, 10, 9) is highlighted.\n* Section 4.1 - it is absolutely unnecessary to bloat this section with the indicies that correspond to the brain boundary - put this in the appendix, or show it as an image if the Authors feel it adds to their explanation.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4\n\n### Details Of Ethics Concerns\n\nI question the validity of using Chat-GPT in this work - but no significant Ethics concerns.",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a method to modify Grad-CAM feature attribution maps to identify the most salient 3D regions (‚Äúblocks‚Äù) within brain MRI images for Alzheimer‚Äôs disease interpretation. The approach applies recursive feature elimination (RFE) and related feature selection strategies to Grad-CAM outputs from a CNN, aggregating the results into ranked 3D blocks. The authors validate the importance of these regions using both literature sources and Chat-GPT, and provide clinical reasoning supporting the identified areas. The topic of explainable medical imaging is relevant and potentially useful for clinicians, and the work provides implementation detail that could support reproducibility. However, the manuscript is difficult to read, poorly formatted, and lacks sufficient contextualization within prior literature.  \n\n**Major Comments**  \n1. **Notation and Definitions** ‚Äì The mathematical notation and definitions are cumbersome and repetitive (e.g., patch definitions and indices in Section 2). Variables such as _n_ are undefined, and many statements could be simplified. The numerous ‚Äúdefinitions‚Äù duplicate explanations that could be described procedurally.  \n2. **Terminology Confusion** ‚Äì The introduced term ‚Äútop feature map‚Äù (_T^Q_) is misleading, as it represents an aggregation of top _k_ Grad-CAM features rather than an actual feature map. Clarification is needed.  \n3. **Algorithm Description** ‚Äì The Image Block Ranking algorithm (lines 221‚Äì234) is overly verbose. Describing the process for one axis would suffice, as Figure‚ÄØ1 already conveys this clearly.  \n4. **Evaluation and Comparisons** ‚Äì Figures‚ÄØ2b and‚ÄØ3b suggest that simple thresholding of Grad-CAM maps or applying anatomical masks could yield similar results. A quantitative comparison to baseline Grad-CAM maps would strengthen the evaluation.  \n5. **Use of Chat-GPT** ‚Äì Employing Chat-GPT to verify brain region importance lacks justification. Consultation with clinicians would have been more appropriate given the stated clinical motivation.  \n6. **Scope and Generalizability** ‚Äì The method is demonstrated only on one MRI dataset, with no discussion of performance on other modalities or domains. Claims of applicability to ‚ÄúExplainable Medical Imaging‚Äù are therefore overstated.  \n\n**Minor Comments**  \n- Several sections (e.g., lines 57‚ÄØff.) are repetitive and could be condensed.  \n- Lines‚ÄØ147‚Äì148 contain unclear phrasing about ‚Äúimpact on the decision‚Äù and ‚Äútrained CNN to generate L heatmaps.‚Äù  \n- Table‚ÄØ2 is partially blank and needs explanation.  \n- Line‚ÄØ489 contains an unclear reference to the ‚Äú9th patch.‚Äù  \n- Section‚ÄØ4.1 includes extensive index listings that would be better placed in an appendix or figure.  \n\n**Summary Paragraph**  \nOverall, the paper tackles an important problem‚Äîenhancing interpretability of CNNs for medical imaging‚Äîand provides detailed implementation information. Strengths include attention to clinical motivation and reproducibility. Major weaknesses lie in presentation, clarity of notation, insufficient literature review, unnecessary methodological complexity, and limited evaluation restricted to a single dataset. The unconventional use of Chat-GPT for validation further undermines the evaluation rigor.  \n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper attempt to provide a more explainable 3D GradCam map by combining ideas of GradCam in each of the respective projections (coronal, axial, sagittal) of MRI data with feature selection concepts. The feature maps produced in the maxpooling convolutional layers are used to derive various representations such as heatmap matrics, feature matrices, and the values within them are ranked individually per view and then combined into a 3D block ranking. The authors claim the resulting visualization gives better indication of the disease and demonstrate this on ADNI data for Alzheimer's disease. \n\nThe paper is poorly written and many of the details seem to be automatically written through a translation software or perhaps LLM judging by the language used. For example, reading the abstract had a lot of details  that typically seen in results sections later rather than focusing on a high-level summary of the approach.  Another example is a sentence in line 164-165 which reads \"Different from traditional CAM-based methods without FS a new FS-Grad-CAM methods uses a FS method to select the top k features from m flattened features.\" - Is this referring to their proposed approach. Normally we would phrase it as \"Unlike traditional CAM-based methods, we propose a new method called FS-Grad_CAM where we employ a feature selection method to select the top K feature from m flattened features first before applying GradCAM.\" \n\n Many details are unclear including the novelty with respect to other 3DGradCam methods (see eblow). Overall, it needs a major rewrite and using clinically relevant terminology with better motivation of the healthcare problem addressed.\n\n### Soundness: 1\n\n### Presentation: 1\n\n### Contribution: 1\n\n### Strengths\n\nPaper is about explainable AI showing clinicians relevant features useful for diagnosis in the 3D MRI images by taking slices in multiple views and offering top-ranked patches that are correlated with disease and understand the relationship between the top blocks and the decisions made by 3D CNN. As such, the paper attempts an important problem, namely, making the disease classification more explainable to clinicians. The familiar mechanisms of GradCam are used and attempt is made to process complex MRI datasets in multiple views. If the method could be clearly explained, one could even see value in the technique for 3DGradCam in general although other 3D GradCam tools are available. The main argument appears to be that GradCam should be applied after feature selection in the feature maps.\n\n### Weaknesses\n\nAs mentioned above, the paper is poorly written to determine if the idea being proposed is a variation of 3D gradCam. No comparisons are made to any methods to even see its merit. Several open questions arise (see below). \nThere are several tools available for 3DGradCam such as these. If they are not relevant for comparison, it would be helpful to at least explain how your method differs from these. \nhttps://github.com/fitushar/3D-Grad-CAM\nhttps://www.researchgate.net/publication/357899396_Automated_grading_of_enlarged_perivascular_spaces_in_clinical_imaging_data_of_an_acute_stroke_cohort_using_an_interpretable_3D_deep_learning_framework\n\nThe paper in current problem needs a full rewrite starting with explanation of the MRI disease visualization problem, the role of existing 3D GradCam and the need for feature selection prior to GradCam. The whole idea of class activation maps was to allow us to see the rationale for the classification with the visualization itself in a way doing regional feature selection. By applying a separate feature selection operator a priori, what would be the impact on the gradient operators and the resulting activation maps?\n\nChatGPT is briefly mentioned and it is not clear what it is being used for.\nWhat does it mean to say ChatGPT is used to verify if a brain area is associated with a disease? What is the prompt used? What are the input, only text or text and image, is a bounding box and a prompt given as input. How accurate is ChatGPT in identifying  the brain areas associated with the disease. All these should be added to explain the use of ChatGPT.\n\n### Questions\n\n1. What is the rationale for a top-ranked 3Dimage block being correlated with diseases? Real-life experiences with 2d heat maps alone indicate they are not always a reliable indicator of a disease. Since a disease may be seen in some view better than other views, the method of fusion is important between the 3 views. Provide more discussion or justification for the correlation between top-ranked blocks and diseases, and the fusion method would help clarify better. \n\n2. How is the ranking of the blocks done? In general, what does ranking mean in your context, is it just a matter of selecting high-valued entries in the feature and heat map matrices?\n\n3. What is the purpose of ChatGPT in the work? It is said it is to verify if a brain area is associated with a disease. How does it work? Are both prompt text and image supplied as input? How accurate is ChatGPT in identifying  the brain areas associated with the disease. This discussion is brief and unconvincing so elaborating on how exactly ChatGPT is used and what its inputs are would explain this section better.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes an explainable AI method, termed FS‚ÄëGrad‚ÄëCAM, for visualizing disease‚Äërelated regions in 3D MRI data. It combines Grad‚ÄëCAM‚Äëbased saliency mapping in three orthogonal projections‚Äîcoronal, axial, and sagittal‚Äîwith a feature selection step intended to highlight the most informative activations. The goal is to provide clinicians with interpretable 3D visualizations correlating image regions to Alzheimer‚Äôs disease using ADNI data. Although the topic is important and addresses explainability in medical imaging, the current presentation is unclear and difficult to follow. Much of the phrasing appears machine‚Äëtranslated or automatically generated, hindering comprehension and evaluation of the technical contribution.  \n\n**Major Comments**  \n1. **Clarity and Writing Quality:** The paper requires substantial rewriting for clarity and grammatical correctness. Sentences often appear disordered or translated, obscuring meaning (e.g., lines 164‚Äì165). The abstract mixes methodological and result descriptions instead of providing a concise overview of the approach.  \n2. **Novelty and Relation to Existing Work:** The contribution relative to established 3D Grad‚ÄëCAM methods is not well delineated. The paper should clearly describe how FS‚ÄëGrad‚ÄëCAM differs from available implementations such as those cited (e.g., GitHub 3D‚ÄëGrad‚ÄëCAM repositories and prior interpretable 3D CNN frameworks).  \n3. **Justification of Feature Selection Step:** The rationale for applying feature selection prior to Grad‚ÄëCAM is insufficiently explained. Since Grad‚ÄëCAM intrinsically highlights important regions, the added value or theoretical motivation for an explicit feature selection step needs discussion.  \n4. **Evaluation and Comparisons:** No quantitative or qualitative comparison is made with baseline 3D Grad‚ÄëCAM methods, making it impossible to assess the merit of the proposed visualization.  \n5. **Use of ChatGPT:** The inclusion of ChatGPT as a verification tool is vague. The manuscript should specify what inputs (text, image, or region coordinates) are provided, the prompting method, and how correctness of the returned associations is evaluated.  \n6. **Interpretation of Ranked Blocks:** The logic linking top‚Äëranked 3D blocks to disease relevance and the method for fusing results across three anatomical planes require elaboration.  \n\n**Minor Comments**  \n- Define acronyms and parameters (e.g., \\(k\\), \\(m\\)) upon first use.  \n- Revise figures and captions to clarify what each heatmap or block represents.  \n- Ensure clinically appropriate terminology and consistent formatting.  \n\n**Summary Paragraph**  \nThe paper tackles an important problem‚Äîenhancing explainability of 3D CNNs in medical imaging‚Äîbut its current presentation obscures the technical innovations and experimental validation. The central ideas of combining Grad‚ÄëCAM with feature selection could have potential if more clearly justified and empirically supported. Major rewriting, methodological clarification, and comparative evaluation are essential before the work can be properly assessed.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The study requires substantial clarification, methodological justification, and improved writing before it can be considered further.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a feature-selected (FS) Grad-CAM method to generate more focused explainable heatmaps with smaller highlighted areas. Additionally, a novel 2D image patch ranking algorithm was developed to reliably rank patches along the axial, sagittal, and coronal axes using features extracted by FS-Grad-CAM. These ranked scores are then used to create a Block Ranking Map (BRM) via a newly developed 3D block ranking algorithm. The resulting block-ranked scores are further refined through a novel hybrid 3D block ranking algorithm to produce a reliable hybrid BRM. The method was validated on Alzheimer‚Äôs Disease (AD) data and identified the top 10 ranked blocks associated with AD.\n\n### Soundness: 1\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\nThe paper used the 2D images in axial, sagittal and coronal axes to rank the 3D images, which makes the research novel.  This paper has detailed explanation on the algorithm, which makes the method replication easier.\n\n### Weaknesses\n\n1. Lack of competition: The cited papers (He et al., 2019 and Selvaraju et al., 2017) in the introduction part have compared their new algorithm (Grad-CAM) with other common machine learning algorithm (e.g. logistic regression) to demonstrate their better performance in the binary classification. But this paper doesn‚Äôt have compared with any baseline to demonstrate its superiority. There exists 3D medical imaging visual explanation algorithm (e.g. Respond-CAM) which might be a good benchmark to compare with.\n2. Lack of generality: The paper only evaluated on one dataset, which cannot guarantee the generality of the proposed method. More tests are needed to justify the statement.\n\n### Questions\n\n1. It would be great if any qualitative/quantitative comparison with similar algorithm could be added to help the readers to better evaluate the performance of the proposed method.\n2. It would be ideal if more dataset could be used to evaluate the performance of the proposed method (e.g. LUNA 16 lung nodule dataset)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a feature-selected Grad-CAM (FS-Grad-CAM) framework designed to generate more focused and interpretable heatmaps with smaller highlighted areas in medical imaging. Building on this, the authors propose a 2D image patch ranking algorithm to order patches along the axial, sagittal, and coronal axes, followed by a 3D block ranking algorithm to create a Block Ranking Map (BRM). A hybrid 3D block ranking algorithm is further introduced to refine the BRM. The method is applied to Alzheimer‚Äôs Disease (AD) data to identify the top-ranked brain regions associated with AD. The paper is described as well explained algorithmically, supporting reproducibility and clarity.  \n\n**Major Comments**  \n1. **Lack of Baseline Comparison:** The study does not include comparisons against other established algorithms or baselines to demonstrate performance superiority. Previous works such as He et al. (2019) and Selvaraju et al. (2017) have compared Grad-CAM variants with traditional classifiers (e.g., logistic regression), while relevant 3D visual explanation algorithms like Respond-CAM are not included as benchmarks. Incorporating such baselines would strengthen the validation.  \n2. **Limited Generality and Evaluation Scope:** The proposed method has been evaluated only on one dataset (AD). Broader testing across multiple datasets would be necessary to assess the generality and robustness of the method. The current evidence does not fully justify general applicability.  \n3. **Quantitative and Qualitative Evaluation:** The review requests additional qualitative and quantitative comparisons with similar algorithms to contextualize performance and enhance interpretability of the results.  \n\n**Minor Comments**  \n- The paper‚Äôs technical explanations are clear and facilitate potential replication.  \n- Consider extending the evaluation to another dataset (e.g., LUNA16 lung nodule dataset) to substantiate claims of generality.  \n- No ethical concerns were identified.  \n\n**Summary Paragraph**  \nOverall, the paper introduces a novel and technically detailed framework for explainable 3D medical image interpretation, offering innovation in its multi-axis ranking approach. However, the absence of baseline comparisons and single-dataset validation limit the assessment of its superiority and generalizability. The methodology presentation is clear, but further empirical evidence would be required for stronger support.  \n\n**Decision Recommendation:** Major Revision",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1 ‚Äì Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript proposes new algorithms for *explainable 3D medical imaging*, including a feature‚Äëselection‚Äëbased Grad‚ÄëCAM method, a 2D patch‚Äëranking algorithm, and a hybrid 3D image‚Äëblock ranking framework integrating axial, coronal, and sagittal views. These are methodological contributions to medical image analysis and visualization of learned representations‚Äîsquarely within the methodological scope of *IEEE Transactions on Medical Imaging*.\n\n2. **Novelty & Contribution Level**  \nThe integration of feature selection into Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM) and the multiview fusion of 2D patch rankings to form a 3D ‚ÄúBlock Ranking Map‚Äù represent incremental but nontrivial extensions of existing explainability strategies.  \nHowever, the novelty is partly algorithmic engineering rather than theoretical innovation. The extensive definition of matrices and ranking functions demonstrates effort toward formalization, yet conceptual advances beyond existing explainability frameworks (e.g., Grad‚ÄëCAM++, Score‚ÄëCAM, SHAP‚ÄëCAM) are modest. Claims about clinical interpretability through ChatGPT‚Äëassisted verification are unconventional and not well‚Äëfounded scientifically.  \n‚Üí **Borderline for TMI novelty**: between a methodological extension and an application framework.\n\n3. **Technical and Experimental Rigor**  \nThe manuscript includes substantial mathematical notation but limited quantitative validation. There is no ablation or statistical significance test demonstrating that FS‚ÄëGrad‚ÄëCAM or hybrid block ranking improve interpretability or classification beyond baseline CAM methods. The use of ChatGPT to ‚Äúverify‚Äù brain‚Äëregion associations is not scientifically valid for peer‚Äëreviewed evidence. Dataset use (ADNI, Autism datasets) is appropriate, but reproducibility (code, parameters, number of subjects per split, model hyperparameters) is sparse. Overall rigor is *medium*.\n\n4. **Clarity and Presentation**  \nThe paper is generally readable, yet lengthy, repetitive, and formally dense. There are numerous typographic inconsistencies (spacing, equation numbering, variable notation). Figures and tables are described but might be visually crowded. The hierarchical organization could be streamlined. English is adequate but could benefit from editing for conciseness and academic tone.\n\n5. **Ethical and Reproducibility Compliance**  \nPublic datasets (ADNI, Kaggle autism) are cited appropriately. No mention is made of IRB or subject consent, though likely not needed for secondary anonymized data. No code availability statement. Use of generative AI (‚ÄúChatGPT used to verify clinical associations‚Äù) raises *ethical and methodological concern*‚Äîshould be omitted or reframed as literature-based validation only.\n\n---\n\n**Phase 2 ‚Äì IEEE TMI Review Report**\n\n1. **Summary**  \nThe authors introduce a feature‚Äëselection‚Äëbased Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM) to generate more interpretable 2D heatmaps by focusing on top‚Äëranked features, a five‚Äëfactor 2D patch‚Äëranking algorithm that combines multiple feature and activation matrices, and a hybrid 3D image‚Äëblock ranking approach that fuses axial, coronal, and sagittal patch rankings into a 3D Block Ranking Map (BRM). Applied to ADNI and an autism brain‚ÄëMRI dataset, the framework identifies small 3D ‚Äúimportant blocks‚Äù purportedly associated with disease-related brain regions.\n\n2. **Strengths**\n- Addresses explainability of 3D CNNs‚Äîa current and significant challenge in medical deep learning.\n- Proposes a structured approach combining feature selection and multiview fusion.  \n- Provides detailed definitions and reproducible conceptual framework.  \n- Demonstrates feasibility on two public 3D brain‚ÄëMRI datasets.  \n- Potentially useful as a visualization and interpretability aid.\n\n3. **Weaknesses**\n- Limited scientific novelty; largely an extension of existing Grad‚ÄëCAM/Fusion ideas.  \n- Validation lacks quantitative metrics for interpretability or diagnostic utility.  \n- Reliance on ChatGPT outputs to confirm neuroanatomical associations undermines scientific rigor.  \n- The methods are excessively complicated without theoretical justification and not benchmarked against recent explainability methods.  \n- Writing and formatting need significant refinement.  \n- No error bars, cross‚Äëvalidation, or statistical tests; reproducibility not demonstrated.\n\n4. **Major Comments**\n1. **Quantitative evaluation** ‚Äì Include comparative experiments vs. standard Grad‚ÄëCAM, Grad‚ÄëCAM++, and other explainability methods using objective interpretability metrics or human expert assessment.  \n2. **Remove ChatGPT verification** ‚Äì Replace with expert review or established neuroimaging literature only. Discussion should emphasize model‚Äëderived findings rather than AI text confirmation.  \n3. **Ablation and Sensitivity** ‚Äì Demonstrate contribution of each component: FS‚ÄëGrad‚ÄëCAM, patch ranking factors, and hybrid fusion, ideally with measurable improvement.  \n4. **Method transparency** ‚Äì Provide clear mathematical formulation for the ranking function `f(¬∑)` and weighting scheme; otherwise the method is not reproducible.  \n5. **Visualization and clarity** ‚Äì Condense definitions, properly label equations, unify notation, and illustrate the pipeline with one concise figure.  \n6. **Statistical rigor** ‚Äì Report classification performance with standard deviation over multiple splits and include significance tests.  \n7. **Broader applicability** ‚Äì Evaluate on at least one non‚Äëbrain dataset or discuss generalization to other modalities.  \n\n5. **Minor Comments**\n- Grammar and style require editing for conciseness.  \n- Check repeated references (e.g., Zhou et‚ÄØal. 2024a/b).  \n- Ensure consistent variable naming in equations.  \n- Figures should have readable fonts and color legends.  \n- Provide code/parameter details in Supplementary Material.  \n\n6. **Overall Recommendation:**  \n**Major Revision (borderline below TMI novelty threshold).**  \nWhile the problem is relevant and the approach is conceptually coherent, the paper in current form lacks quantitative evidence, clear methodological novelty, and professional presentation required for *IEEE TMI*. If the authors provide rigorous comparative validation, remove unscientific elements, and substantially clarify exposition, it might reach publishable quality.\n\n7. **Confidence Level:** 4/5 ‚Äì High confidence in assessment based on domain familiarity with deep‚Äëlearning explainability and medical imaging methods.\n\n---\n\n**Summary Statement to the Editor:**  \nThis manuscript tackles an important problem‚Äîexplainable 3D deep learning in neuroimaging‚Äîthrough a highly detailed but somewhat convoluted ranking‚Äëbased approach. Its novelty is moderate and technical rigor insufficient for immediate acceptance at *IEEE TMI*, but with major revision focusing on quantitative and methodological soundness, it could evolve into a useful contribution to interpretability in medical imaging.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a framework for explainable 3D medical image analysis, comprising a feature‚Äëselection‚Äëbased Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM), a 2D patch‚Äëranking algorithm, and a hybrid 3D image‚Äëblock ranking module that integrates axial, coronal, and sagittal views into a unified Block Ranking Map. The work aims to enhance interpretability of convolutional neural networks applied to brain MRI from public datasets. Overall, the topic is timely and relevant to explainable AI in medical imaging, but the manuscript‚Äôs presentation is lengthy, its methodological advances incremental, and its experimental validation limited.  \n\n**Major Comments**  \n1. **Scientific novelty** ‚Äì The approach extends existing Grad‚ÄëCAM and fusion concepts rather than introducing a distinct theoretical innovation. The contribution lies mainly in algorithmic engineering and integration.  \n2. **Quantitative validation** ‚Äì The study lacks objective evaluation of interpretability improvement. No ablation, statistical testing, or comparison to baseline methods (Grad‚ÄëCAM, Grad‚ÄëCAM++, SHAP‚ÄëCAM, etc.) is presented.  \n3. **ChatGPT‚Äëbased verification** ‚Äì Use of a generative model to confirm clinical relevance is methodologically unsound; expert review or literature support should replace it.  \n4. **Method transparency and reproducibility** ‚Äì Details on ranking functions, weighting schemes, hyperparameters, and data splits are insufficient. No code or reproducibility statement is provided.  \n5. **Statistical rigor** ‚Äì Reported classification results omit cross‚Äëvalidation, error bars, or significance analysis, limiting trust in performance claims.  \n6. **Clarity and structure** ‚Äì The manuscript is text‚Äë and notation‚Äëheavy, with inconsistent variables, lengthy definitions, and crowded figures. A streamlined description and unified notation are needed.  \n7. **Broader applicability** ‚Äì The work is confined to brain MRI; evaluation or discussion of generalization to other modalities would strengthen scope.  \n\n**Minor Comments**  \n- Grammar and formatting should be edited for conciseness and consistency.  \n- Ensure uniform equation numbering and resolve repeated citations.  \n- Improve figure readability (fonts, color labels).  \n- Include parameter details and code availability in supplementary materials.  \n- Ethics section should clarify dataset use and remove unsupported claims about AI‚Äëbased validation.  \n\n**Summary Paragraph**  \nThe study addresses an important issue‚Äîexplainability in 3D CNNs for neuroimaging‚Äîthrough a structured yet complex multiview ranking approach. It demonstrates feasibility and conceptual effort but exhibits modest novelty, weak empirical validation, and presentation issues. The methods require clearer articulation, stronger statistical and comparative evaluation, and elimination of unscientific verification steps before it can be considered robust. The work has potential value as an interpretability aid if substantially revised.  \n\n**Decision Recommendation**  \n**Major Revision.** The paper presents a relevant problem but lacks quantitative evidence, methodological rigor, and clarity needed for acceptance in its current form.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper addresses explainability challenges in 3D medical imaging by proposing a novel ranking method for 3D image blocks. The approach combines a new FS-Grad-CAM method with feature selection to generate explainable 2D heatmaps, followed by a 2D patch ranking algorithm using five informative matrices (feature distribution, feature ranking, average feature ranking, heatmap activation, and heatmap strength matrices). The core contribution is a hybrid 3D image block ranking algorithm that integrates axial, coronal, and sagittal patch rankings to create Block Ranking Maps (BRMs). The method is evaluated on ADNI dataset (982 3D brain images) for Alzheimer's disease diagnosis and an autism dataset (286 3D images), achieving testing accuracies of 0.9309-0.9897. Results show that top-ranked blocks correspond to brain regions associated with AD and autism diagnosis, verified through ChatGPT and literature citations. The hybrid BRM enables visualization across three anatomical views for medical interpretation.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical notation inconsistencies and unclear formulations**\n  - Definition 6 appears twice (lines 153 and 162) with different content, creating confusion about heatmap count matrix versus average feature ranking matrix\n  - Equation complexity in Definition 5 (lines 147-150) with floor division notation ¬µ = ‚åäIp/n‚åã lacks clear explanation of how feature index mapping works\n  - The monotonically non-decreasing function Œ∏ij = f(dij, r‚Å∞ij, rÃÑij, uij, sij) in Algorithm 1 (line 225) is never explicitly defined, making reproducibility impossible\n\n‚Ä¢ **Insufficient experimental validation and comparison**\n  - No comparison with existing 3D explainability methods beyond brief mentions of 3DGradCAM (line 176), limiting assessment of relative performance\n  - Evaluation relies heavily on subjective verification through ChatGPT and literature matching (Tables 1-2, pages 7-9) rather than quantitative explainability metrics\n  - Testing accuracies vary significantly (0.9309-0.9897, line 317) across axial/coronal/sagittal views without explanation of this variance or its impact on final rankings\n\n‚Ä¢ **Methodological complexity without clear justification**\n  - The five-factor ranking approach (Algorithm 1, lines 216-228) lacks ablation studies showing individual factor contributions or necessity\n  - Multiple feature selection methods are combined sequentially (Chi2, mutual_info_classif, f_regression, f_classif, RFE, lines 350-356) without justification for this specific ordering\n  - Hybrid ranking requires \"different conditions\" (Algorithm 3, line 296) but these conditions are not systematically defined or validated\n\n‚Ä¢ **Limited scope and generalizability concerns**\n  - Evaluation limited to brain imaging applications (AD and autism) without demonstration on other organ systems or pathologies\n  - Dataset preprocessing constraints (resizing to 64√ó64√ó64, extracting middle slices 22-41, lines 307-309) may not generalize to clinical workflows\n  - Heavy reliance on \"ebrains\" software tool for brain region identification (line 339) creates dependency on specific external tools\n\n## Suggestions for Improvement\n\n‚Ä¢ **Clarify and standardize mathematical formulations**\n  - Resolve the duplicate Definition 6 by renumbering and clearly distinguishing between heatmap count matrix and average feature ranking matrix concepts\n  - Provide explicit mathematical formulation for the ranking function Œ∏ij = f(dij, r‚Å∞ij, rÃÑij, uij, sij) with specific weights or combination rules\n  - Add detailed explanation of the feature index mapping process in Definition 5, including worked examples showing how ¬µ values translate to spatial coordinates\n\n‚Ä¢ **Strengthen experimental validation framework**\n  - Include quantitative comparisons with at least 2-3 existing 3D explainability methods using standard metrics like localization accuracy or faithfulness scores\n  - Replace subjective ChatGPT verification with established ground truth datasets or expert radiologist annotations for brain region validation\n  - Investigate and explain the accuracy variance across anatomical views, potentially through cross-validation or statistical significance testing\n\n‚Ä¢ **Justify and validate methodological design choices**\n  - Conduct ablation studies removing individual factors from the five-factor ranking to demonstrate each component's necessity and contribution\n  - Provide theoretical or empirical justification for the specific sequential ordering of feature selection methods, or compare against alternative orderings\n  - Define the \"different conditions\" in Algorithm 3 systematically and validate their contribution to ranking stability through sensitivity analysis\n\n‚Ä¢ **Expand evaluation scope and reduce external dependencies**\n  - Demonstrate method applicability on non-brain imaging datasets (lung, liver, or cardiac imaging) to establish broader medical imaging utility\n  - Evaluate performance under different preprocessing parameters (various resizing dimensions, slice selection strategies) to assess robustness\n  - Reduce dependency on \"ebrains\" software by incorporating alternative brain atlases or developing atlas-independent validation approaches",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a method to improve explainability in 3D medical imaging through a hybrid 3D block ranking algorithm. The approach integrates a novel FS‚ÄëGrad‚ÄëCAM technique, multiple feature selection procedures, and a five‚Äëfactor ranking pipeline to produce Block Ranking Maps (BRMs) across axial, coronal, and sagittal views. Evaluations on Alzheimer‚Äôs disease (ADNI dataset) and autism datasets demonstrate high reported accuracies (0.93‚Äì0.99) and suggest alignment of top‚Äëranked regions with known disease‚Äërelated brain areas. The study offers a detailed technical pipeline but contains several issues in mathematical clarity, experimental design, and validation rigor.  \n\n**Major Comments**  \n1. **Mathematical clarity and notation** ‚Äì Duplicate Definition 6 entries appear with conflicting meanings. Equations in Definitions 5‚Äì6 and Algorithm 1 contain ambiguous notation (e.g., floor division in ¬µ = ‚åäIp/n‚åã, undefined ranking function Œ∏ij = f(...)), hindering reproducibility.  \n2. **Experimental validation and comparison** ‚Äì Performance results lack comparison against established 3D explainability methods beyond minimal mention of 3DGradCAM. The validation approach relies on ChatGPT‚Äëbased region verification and literature citations rather than standard quantitative metrics. Accuracy varies widely across anatomical planes without clear rationale.  \n3. **Methodological justification** ‚Äì The necessity of all five ranking factors and multiple sequential feature selection methods is not demonstrated. Algorithm 3 refers to ‚Äúdifferent conditions‚Äù for hybrid ranking without formal definition or validation.  \n4. **Scope and generalizability** ‚Äì Experiments are limited to brain imaging (AD and autism) and depend on fixed preprocessing (resizing, slice selection), which may restrict applicability. Heavy reliance on external ‚Äúebrains‚Äù software introduces tool‚Äëspecific constraints.  \n\n**Minor Comments**  \n- Renumber and clarify duplicated definitions.  \n- Provide explicit examples for index mapping and variable roles in equations.  \n- Improve explanation of accuracy variance and reporting format in Tables 1‚Äì2.  \n- Ensure consistent terminology across algorithms and factor names.  \n\n**Summary Paragraph**  \nOverall, the paper proposes an ambitious and technically elaborate framework for interpretable 3D medical image analysis. Strengths include the hybrid integration of multi‚Äëview rankings and a structured attempt at interpretability visualization. However, unclear mathematics, insufficient comparative evaluation, and limited methodological justification weaken the study‚Äôs rigor and reproducibility. Extending validation to other modalities and clarifying formulations would substantially strengthen the contribution.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The manuscript requires substantial clarification of formulations, strengthened quantitative validation, and justified methodological choices before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript presents a novel 3D image block ranking method for explainable medical imaging, particularly for Alzheimer's disease (AD) and autism diagnosis. The proposed method integrates 2D image patch rankings from axial, coronal, and sagittal views to generate a hybrid Block Ranking Map (BRM). This BRM aims to highlight the most relevant 3D image blocks for medical diagnoses by leveraging feature selection (FS) and Grad-CAM-based heatmaps. The authors demonstrate the effectiveness of their approach using the ADNI dataset and an autism dataset, showcasing its potential for identifying key brain areas associated with these diseases.\n\n## Major Comments\n1. Novelty and Contribution: While the manuscript introduces a comprehensive framework for ranking 3D image blocks, it does not sufficiently contextualize its contribution relative to existing work on explainable AI in medical imaging. The authors need to clarify how their method stands out from other recent advancements in feature selection and 3D CNN interpretation, particularly those that also integrate multi-view information.\n\n2. Evaluation Design: The evaluation is primarily conducted on two datasets (ADNI and autism), which limits the generalizability of the findings. Including additional datasets and varying types of medical conditions would strengthen the claim of broad applicability. Additionally, the manuscript should discuss the limitations of the current datasets and how they might affect the reliability of the results.\n\n3. Comparisons with Baselines: The manuscript lacks detailed comparisons with other state-of-the-art methods for explainable 3D medical imaging. Including comprehensive baselines would provide a clearer picture of the method's advantages and limitations. The authors should consider including methods like 3D Grad-CAM and other explainable AI techniques that have been recently published.\n\n4. Reproducibility: The manuscript mentions that code will be made available, but the methodology section lacks sufficient detail regarding the training protocols, preprocessing steps, and hyperparameters used. Providing a more thorough description of these aspects is crucial for reproducibility.\n\n## Minor Comments\n1. Figures: Figures 2 and 3 are somewhat cluttered and could benefit from a more focused presentation, possibly by highlighting key regions and reducing the number of displayed patches.\n   \n2. Notation Consistency: There are inconsistencies in the notation used throughout the manuscript, particularly in the mathematical sections. Standardizing these notations would enhance clarity.\n\n3. Acronyms and Terminology: Acronyms such as \"FS\" and \"BRM\" are used without initial definition, which can confuse readers unfamiliar with the terminology. Providing a glossary or defining terms upon first use would be beneficial.\n\n4. Typographical Errors: Minor typographical errors exist throughout the manuscript, such as \"sagittal image with indices (i, j)\" instead of \"(i, j)\" being properly formatted. These should be corrected for clarity.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in explainable medical imaging by proposing a novel method for ranking 3D image blocks using multi-view 2D patches. The innovation lies in integrating feature selection and Grad-CAM-based heatmaps to generate a hybrid BRM, which can potentially aid in medical diagnosis. However, the evaluation is limited to two datasets, which may restrict the generalizability of the findings. The manuscript would benefit from a more comprehensive comparison with existing methods and clearer distinctions from prior work. Despite these limitations, the idea is promising and could contribute significantly to the field of explainable AI in medical imaging once these issues are addressed.\n\n## Decision Recommendation\nMajor Revision. The authors should expand the comparative analysis, broaden the scope of their validation across diverse datasets, and provide detailed methodological descriptions to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a 3D image block ranking approach designed for explainable medical imaging, with applications to Alzheimer‚Äôs disease (AD) and autism diagnosis. The proposed framework combines 2D patch rankings from axial, coronal, and sagittal views to form a hybrid Block Ranking Map (BRM) that identifies key 3D image regions relevant to diagnosis. The method incorporates feature selection and Grad-CAM-based heatmaps, and its performance is demonstrated on ADNI and autism datasets. Overall, the approach addresses an important topic in explainable AI for medical imaging and presents a technically coherent framework with promising potential.\n\n**Major Comments**  \n1. **Novelty and Contribution:** The manuscript‚Äôs contribution is not clearly differentiated from prior explainable AI methods in medical imaging. The authors should better position their work relative to existing studies, especially those employing multi-view integration, feature selection, or 3D CNN interpretation, to clarify the novelty of their approach.  \n2. **Evaluation Design:** The experiments are limited to two datasets (ADNI and autism), constraining generalizability. Extending the evaluation to include additional datasets or conditions and discussing dataset limitations would enhance the robustness of the conclusions.  \n3. **Comparisons with Baselines:** The work lacks systematic comparisons with state-of-the-art explainability methods, such as 3D Grad-CAM and other contemporary approaches. Including such baselines would help quantify the advantages and limitations of the proposed technique.  \n4. **Reproducibility:** While the authors note that code will be shared, the methodology section omits specific details on training protocols, preprocessing steps, and hyperparameters. Expanded descriptions are necessary to support independent replication.\n\n**Minor Comments**  \n1. **Figures:** Figures 2 and 3 appear overcrowded. Simplifying layouts or highlighting key regions would improve readability.  \n2. **Notation:** Some inconsistencies occur in mathematical notation; standardization is recommended.  \n3. **Terminology:** Acronyms such as ‚ÄúFS‚Äù and ‚ÄúBRM‚Äù should be defined upon first mention or included in a glossary.  \n4. **Typos:** Minor formatting and typographical issues (e.g., coordinate notation) should be corrected.\n\n**Summary Paragraph**  \nOverall, the study proposes a promising and structured approach for enhancing explainability in 3D medical imaging. Its integration of feature selection and Grad-CAM-based methods could yield useful diagnostic insights. However, limited dataset diversity, insufficient baseline comparisons, and incomplete methodological details reduce the current strength of evidence. Addressing these issues would substantively improve the manuscript‚Äôs clarity, reproducibility, and impact.\n\n**Decision Recommendation:** **Major Revision.** The manuscript requires stronger contextualization, broader evaluation, and improved methodological transparency before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A New 3D Image Block Ranking Method Using Axial, Coronal, and Sagittal Image Patch Rankings for Explainable Medical Imaging\n\n### Summary\n\nThe paper proposes a pipeline for explainable 3D medical imaging that ranks 3D image blocks by aggregating 2D patch rankings from axial, coronal, and sagittal views. It introduces (i) a feature-selection‚Äìaware Grad-CAM variant (FS‚ÄëGrad‚ÄëCAM), (ii) several ‚Äúinformative‚Äù feature/heatmap matrices, (iii) a five-factor 2D patch ranking scheme, and (iv) a hybrid 3D block ranking algorithm that averages multiple runs using different feature-selection sets. Experiments on resized ADNI MRI (three-class) and an autism dataset (binary) qualitatively map top-ranked blocks to neuroanatomical regions the authors claim are associated with AD and ASD.\n\n### Strengths\n\n- Technical novelty and innovationThe idea of combining planewise (axial/coronal/sagittal) patch rankings to produce a 3D block ranking map (BRM) is intuitively appealing and practical for clinicians who read 2D slices.Introducing auxiliary matrices (feature selection map, accumulation/distribution matrices, heatmap activation/strength) provides a structured vocabulary for discussing patch-level evidence.The hybrid aggregation across multiple feature-selection procedures aims to mitigate instability/bias in saliency and selection steps.\n- Experimental rigor and validationTwo datasets (ADNI, an ASD dataset) are considered, showing an intent to test generality.The framework attempts to link top-ranked blocks to brain regions via a standard brain reference tool (ebrains), which is a step toward anatomical interpretability.\n- Clarity of presentationThe pipeline is laid out in an 8-step framework figure, with named subcomponents and intermediate outputs, which helps the reader follow the intended data flow.Several definitions make explicit the objects manipulated (e.g., feature binary matrix, heatmap activation matrix).\n- Significance of contributionsExplainability for volumetric neuroimaging is an important area; any method that helps clinicians navigate 3D volumes through ranked 2D views has potential practical value.A systematic attempt to reduce saliency map clutter via feature selection before Grad‚ÄëCAM addresses a recognized pain point of CAM methods (diffuse, non-specific heatmaps).\n\n- The idea of combining planewise (axial/coronal/sagittal) patch rankings to produce a 3D block ranking map (BRM) is intuitively appealing and practical for clinicians who read 2D slices.\n- Introducing auxiliary matrices (feature selection map, accumulation/distribution matrices, heatmap activation/strength) provides a structured vocabulary for discussing patch-level evidence.\n- The hybrid aggregation across multiple feature-selection procedures aims to mitigate instability/bias in saliency and selection steps.\n\n- Two datasets (ADNI, an ASD dataset) are considered, showing an intent to test generality.\n- The framework attempts to link top-ranked blocks to brain regions via a standard brain reference tool (ebrains), which is a step toward anatomical interpretability.\n\n- The pipeline is laid out in an 8-step framework figure, with named subcomponents and intermediate outputs, which helps the reader follow the intended data flow.\n- Several definitions make explicit the objects manipulated (e.g., feature binary matrix, heatmap activation matrix).\n\n- Explainability for volumetric neuroimaging is an important area; any method that helps clinicians navigate 3D volumes through ranked 2D views has potential practical value.\n- A systematic attempt to reduce saliency map clutter via feature selection before Grad‚ÄëCAM addresses a recognized pain point of CAM methods (diffuse, non-specific heatmaps).\n\n### Weaknesses\n\n- Technical limitations or concernsThe core scoring functions are not specified: Œ∏ij = f(dij, r^0ij, rÃÑij, uij, sij) and œÜijk = f(Œ∏jk, Œ∏ik, Œ∏ij) are left undefined beyond ‚Äúmonotonically non-decreasing.‚Äù This makes the method unreproducible and precludes technical scrutiny of stability, calibration, and sensitivity.The FS‚ÄëGrad‚ÄëCAM formulation is not rigorously justified; it is unclear how masking to selected flattened features interacts with Grad‚ÄëCAM‚Äôs channel-wise weighting and whether the forward/backward passes are consistent with Grad‚ÄëCAM‚Äôs assumptions.Block selection requires Œ∏ from all three planes to be positive, which may systematically exclude clinically relevant blocks at the periphery or in anisotropic acquisitions.\n- Experimental gaps or methodological issuesLikely subject leakage: the train/test splits appear to be performed at the 2D slice level (e.g., 19,640 slices, 70/30 split), so slices from the same subject may be in both sets. This can inflate reported slice-level accuracies and any downstream saliency statistics. Subject-wise splits are standard and necessary.No quantitative evaluation of localization: there is no atlas-overlap or ROI‚Äëbased metric demonstrating that top-ranked blocks concentrate in known AD/ASD regions versus random baselines or competing XAI methods. Reliance on qualitative inspection and ChatGPT assertions is inadequate for validation.No comparison to strong baselines: methods like 3D‚ÄëGrad‚ÄëCAM, AXIAL‚Äëstyle 3D attention fusion, or SHAP‚Äëbased patch selection (e.g., sMRI‚ÄëPatchNet) are not compared quantitatively.Use of training data to generate heatmaps and patch rankings risks bias; cross-validated or held-out saliency generation is not described.\n- Clarity or presentation issuesNotation is often ambiguous or inconsistent (e.g., repeated definition numbers, reuse of H/W both for image size and patch size, unclear mapping from flattened features back to channels/spatial positions).The CNN architectures, training regimes, and feature-selection hyperparameters are unspecified, making replication difficult.Claims such as ‚Äúmore interpretable than Grad‚ÄëCAM‚Äù are not supported by quantitative metrics (e.g., region concentration, deletion/insertion curves).\n- Missing related work or comparisonsClosely related multi-plane explainability/fusion for MRI (e.g., AXIAL) and end-to-end 3D explainable models (e.g., 3D‚ÄëResAttNet with 3D‚ÄëGrad‚ÄëCAM) are not engaged with in a comparative, quantitative way.Patch selection explainability (e.g., SHAP in sMRI‚ÄëPatchNet) is highly relevant but not directly compared; 2.5D fusion literature shows systematic approaches to leveraging multi-view context.\n\n- The core scoring functions are not specified: Œ∏ij = f(dij, r^0ij, rÃÑij, uij, sij) and œÜijk = f(Œ∏jk, Œ∏ik, Œ∏ij) are left undefined beyond ‚Äúmonotonically non-decreasing.‚Äù This makes the method unreproducible and precludes technical scrutiny of stability, calibration, and sensitivity.\n- The FS‚ÄëGrad‚ÄëCAM formulation is not rigorously justified; it is unclear how masking to selected flattened features interacts with Grad‚ÄëCAM‚Äôs channel-wise weighting and whether the forward/backward passes are consistent with Grad‚ÄëCAM‚Äôs assumptions.\n- Block selection requires Œ∏ from all three planes to be positive, which may systematically exclude clinically relevant blocks at the periphery or in anisotropic acquisitions.\n\n- Likely subject leakage: the train/test splits appear to be performed at the 2D slice level (e.g., 19,640 slices, 70/30 split), so slices from the same subject may be in both sets. This can inflate reported slice-level accuracies and any downstream saliency statistics. Subject-wise splits are standard and necessary.\n- No quantitative evaluation of localization: there is no atlas-overlap or ROI‚Äëbased metric demonstrating that top-ranked blocks concentrate in known AD/ASD regions versus random baselines or competing XAI methods. Reliance on qualitative inspection and ChatGPT assertions is inadequate for validation.\n- No comparison to strong baselines: methods like 3D‚ÄëGrad‚ÄëCAM, AXIAL‚Äëstyle 3D attention fusion, or SHAP‚Äëbased patch selection (e.g., sMRI‚ÄëPatchNet) are not compared quantitatively.\n- Use of training data to generate heatmaps and patch rankings risks bias; cross-validated or held-out saliency generation is not described.\n\n- Notation is often ambiguous or inconsistent (e.g., repeated definition numbers, reuse of H/W both for image size and patch size, unclear mapping from flattened features back to channels/spatial positions).\n- The CNN architectures, training regimes, and feature-selection hyperparameters are unspecified, making replication difficult.\n- Claims such as ‚Äúmore interpretable than Grad‚ÄëCAM‚Äù are not supported by quantitative metrics (e.g., region concentration, deletion/insertion curves).\n\n- Closely related multi-plane explainability/fusion for MRI (e.g., AXIAL) and end-to-end 3D explainable models (e.g., 3D‚ÄëResAttNet with 3D‚ÄëGrad‚ÄëCAM) are not engaged with in a comparative, quantitative way.\n- Patch selection explainability (e.g., SHAP in sMRI‚ÄëPatchNet) is highly relevant but not directly compared; 2.5D fusion literature shows systematic approaches to leveraging multi-view context.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe FS-Grad-CAM approach needs a clear derivation: selecting top flattened features implies selecting specific spatial positions across channels; Grad‚ÄëCAM weights are channel-wise, whereas flattened selection mixes channel and location. If the FS layer zeroes non-selected positions, gradients and weights computed thereafter differ from standard Grad‚ÄëCAM, potentially leading to inconsistent attributions. A formal statement of the forward pass, masking operation, and backprop path is needed.The five-factor patch score Œ∏ij is central to the proposal but lacks a mathematical definition. Without normalization, weighting, or an aggregation rule (e.g., convex combination, learned weights, rank fusion), properties such as robustness to noisy heatmaps, dominance of any single factor, and calibration across planes can‚Äôt be assessed.Requiring Œ∏ > 0 across all three planes may be too strict. Consider probabilistic fusion (e.g., product/sum of normalized evidences) with confidence weighting rather than hard intersection, especially for anisotropic data or partial coverage.\n- Experimental evaluation assessmentData splitting: Subject-wise split is a must (as in AXIAL and 3D‚ÄëResAttNet). Redo all experiments with subject-level k‚Äëfold CV to avoid leakage. The very high sagittal slice accuracy (0.9897) is a red flag for leakage.Quantitative localization: Report overlap statistics with atlases (e.g., hippocampus, parahippocampus, amygdala for AD; cerebellar and IFG regions for ASD). Metrics can include: proportion of top‚ÄëK blocks falling within pre-specified ROIs, expected hit rate vs random, and aggregated heatmap intensity within ROIs versus outside. Compare FS‚ÄëGrad‚ÄëCAM vs standard Grad‚ÄëCAM and against AXIAL‚Äëstyle voxel attention fusion.Ablations: (1) With/without FS before Grad‚ÄëCAM; (2) different k values (e.g., 50, 100, 250) and their effect on sparsity and overlap metrics; (3) define and vary the Œ∏ aggregation rule; (4) with/without the ‚Äúthree-plane intersection‚Äù constraint; (5) effect of hybrid averaging across FS methods.External validity: For AD, evaluate with subject-wise CV and, ideally, on a separate cohort or a held-out ADNI subset (e.g., ADNI-2/3). For ASD, use a recognized dataset (e.g., ABIDE) or provide more details and QC on the Kaggle dataset, including preprocessing, motion/quality filtering.Statistical testing: When claiming region associations, include statistical comparisons (e.g., permutation tests comparing the observed count of top‚ÄëK blocks in ROI R to a null of randomly sampled blocks matched for brain coverage). Reporting p-values, CIs, and effect sizes is necessary.\n- Comparison with related work (using the summaries provided)AXIAL (2407.02418) learns planewise slice importances and composes a voxel‚Äëlevel 3D attention map, then quantitatively evaluates region overlap. Your BRM concept is similar in spirit (multi-plane fusion), but lacks a learned attention mechanism and, crucially, quantitative ROI validation. Adopting AXIAL‚Äôs evaluation protocol (region-wise overlap metrics) would substantially strengthen your claims.3D‚ÄëResAttNet (2008.04024) offers end‚Äëto‚Äëend 3D classification with 3D‚ÄëGrad‚ÄëCAM. A direct comparison (localization focus and region concentration metrics) would clarify whether the proposed FS‚ÄëGrad‚ÄëCAM and BRM pipeline offers advantages over standard 3D saliency.sMRI‚ÄëPatchNet (2302.08967) uses SHAP for principled patch selection and demonstrates that a small number of interpretable patches can support strong classification, with atlas mapping. This is highly aligned with your goals; at minimum, compare the spatial concentration and anatomical plausibility of your top‚ÄëK blocks to SHAP-selected patches on ADNI.The 2.5D segmentation study (2010.06163) highlights practical tradeoffs in multi‚Äëview fusion. Your approach fits the multi-view aggregation paradigm; leveraging its insights (e.g., weighted fusion, attention across views) and citing computational tradeoffs would improve positioning.\n- Discussion of broader impact and significanceThe problem is important: explaining 3D CNN/2.5D decisions at region/block level could help clinicians. However, relying on ChatGPT to support brain region associations in a medical context is inappropriate. Clinical claims should be grounded in systematic literature review and quantitative overlap analyses. With rigorous subject-wise CV, robust localization metrics, and well‚Äëdefined scoring functions, this line of work could become a useful toolkit for interpretable neuroimaging.\n\n- The FS-Grad-CAM approach needs a clear derivation: selecting top flattened features implies selecting specific spatial positions across channels; Grad‚ÄëCAM weights are channel-wise, whereas flattened selection mixes channel and location. If the FS layer zeroes non-selected positions, gradients and weights computed thereafter differ from standard Grad‚ÄëCAM, potentially leading to inconsistent attributions. A formal statement of the forward pass, masking operation, and backprop path is needed.\n- The five-factor patch score Œ∏ij is central to the proposal but lacks a mathematical definition. Without normalization, weighting, or an aggregation rule (e.g., convex combination, learned weights, rank fusion), properties such as robustness to noisy heatmaps, dominance of any single factor, and calibration across planes can‚Äôt be assessed.\n- Requiring Œ∏ > 0 across all three planes may be too strict. Consider probabilistic fusion (e.g., product/sum of normalized evidences) with confidence weighting rather than hard intersection, especially for anisotropic data or partial coverage.\n\n- Data splitting: Subject-wise split is a must (as in AXIAL and 3D‚ÄëResAttNet). Redo all experiments with subject-level k‚Äëfold CV to avoid leakage. The very high sagittal slice accuracy (0.9897) is a red flag for leakage.\n- Quantitative localization: Report overlap statistics with atlases (e.g., hippocampus, parahippocampus, amygdala for AD; cerebellar and IFG regions for ASD). Metrics can include: proportion of top‚ÄëK blocks falling within pre-specified ROIs, expected hit rate vs random, and aggregated heatmap intensity within ROIs versus outside. Compare FS‚ÄëGrad‚ÄëCAM vs standard Grad‚ÄëCAM and against AXIAL‚Äëstyle voxel attention fusion.\n- Ablations: (1) With/without FS before Grad‚ÄëCAM; (2) different k values (e.g., 50, 100, 250) and their effect on sparsity and overlap metrics; (3) define and vary the Œ∏ aggregation rule; (4) with/without the ‚Äúthree-plane intersection‚Äù constraint; (5) effect of hybrid averaging across FS methods.\n- External validity: For AD, evaluate with subject-wise CV and, ideally, on a separate cohort or a held-out ADNI subset (e.g., ADNI-2/3). For ASD, use a recognized dataset (e.g., ABIDE) or provide more details and QC on the Kaggle dataset, including preprocessing, motion/quality filtering.\n- Statistical testing: When claiming region associations, include statistical comparisons (e.g., permutation tests comparing the observed count of top‚ÄëK blocks in ROI R to a null of randomly sampled blocks matched for brain coverage). Reporting p-values, CIs, and effect sizes is necessary.\n\n- AXIAL (2407.02418) learns planewise slice importances and composes a voxel‚Äëlevel 3D attention map, then quantitatively evaluates region overlap. Your BRM concept is similar in spirit (multi-plane fusion), but lacks a learned attention mechanism and, crucially, quantitative ROI validation. Adopting AXIAL‚Äôs evaluation protocol (region-wise overlap metrics) would substantially strengthen your claims.\n- 3D‚ÄëResAttNet (2008.04024) offers end‚Äëto‚Äëend 3D classification with 3D‚ÄëGrad‚ÄëCAM. A direct comparison (localization focus and region concentration metrics) would clarify whether the proposed FS‚ÄëGrad‚ÄëCAM and BRM pipeline offers advantages over standard 3D saliency.\n- sMRI‚ÄëPatchNet (2302.08967) uses SHAP for principled patch selection and demonstrates that a small number of interpretable patches can support strong classification, with atlas mapping. This is highly aligned with your goals; at minimum, compare the spatial concentration and anatomical plausibility of your top‚ÄëK blocks to SHAP-selected patches on ADNI.\n- The 2.5D segmentation study (2010.06163) highlights practical tradeoffs in multi‚Äëview fusion. Your approach fits the multi-view aggregation paradigm; leveraging its insights (e.g., weighted fusion, attention across views) and citing computational tradeoffs would improve positioning.\n\n- The problem is important: explaining 3D CNN/2.5D decisions at region/block level could help clinicians. However, relying on ChatGPT to support brain region associations in a medical context is inappropriate. Clinical claims should be grounded in systematic literature review and quantitative overlap analyses. With rigorous subject-wise CV, robust localization metrics, and well‚Äëdefined scoring functions, this line of work could become a useful toolkit for interpretable neuroimaging.\n\n### Questions for Authors\n\n- How exactly is Œ∏ij computed? Please provide a precise mathematical definition, including normalization, weighting between the five factors, and any hyperparameters.\n- How exactly is œÜijk computed from Œ∏jk, Œ∏ik, Œ∏ij? Is it a product, sum, min/max, or learned fusion? How do you normalize scores across planes?\n- Were splits subject-wise or slice-wise? If slice-wise, please redo with subject-wise cross‚Äëvalidation; otherwise specify the subject stratification protocol and ensure no leakage.\n- Are the heatmaps and Œ∏/œÜ computed on training data or held-out data? If training data were used, how do you mitigate bias? Can you report results using only test (held-out) saliencies?\n- What is the CNN architecture used per plane (layers, channels, activation, training hyperparameters)? How were 64 feature maps obtained and why 16√ó16 spatial size?\n- In FS‚ÄëGrad‚ÄëCAM, do you zero non-selected features in the forward pass before computing gradients? How do you reconcile flattened feature selection with Grad‚ÄëCAM‚Äôs channel-wise weights?\n- Why require Œ∏ > 0 in all three planes for block selection? Have you evaluated soft fusion without this hard constraint, and what was the impact on localization metrics?\n- Can you provide quantitative localization results (e.g., percentage of top‚ÄëK blocks overlapping hippocampus/parahippocampus) compared to baselines (standard Grad‚ÄëCAM, AXIAL‚Äëstyle attention, SHAP‚Äëpatch selection)?\n- How sensitive are the results to the choice of k (number of selected features) and to the specific sequence of feature selection methods? Any stability analysis across random seeds?\n- What preprocessing was applied to ADNI (bias field correction, registration to MNI, skull‚Äëstripping)? How were ‚Äúout-of-brain‚Äù patches determined using ebrains, and is that step reproducible?\n- For the ASD dataset, please detail dataset provenance, preprocessing, quality control, and subject-wise splits. Have you validated on ABIDE or another public dataset?\n- Will you release code and a minimal reproducible example to compute Œ∏, œÜ, PRM, and BRM?\n\n### Overall Assessment\n\nThe paper tackles an important, clinically relevant problem: producing interpretable 3D block‚Äëlevel explanations from multi‚Äëplane 2D CNN evidence. The proposed vocabulary of feature/heatmap matrices and the idea to average multiple feature-selection runs to stabilize rankings are interesting. However, in its current form the work has substantial methodological and validation gaps that prevent a rigorous assessment. Most critically, the central scoring functions are undefined, splits appear to suffer from subject leakage, FS‚ÄëGrad‚ÄëCAM is insufficiently specified, and there is no quantitative localization evaluation or comparison to strong baselines such as 3D‚ÄëGrad‚ÄëCAM, AXIAL-style multi-plane attention, or SHAP-based patch selection. The reliance on ChatGPT to support neuroanatomical claims further weakens the scientific grounding. With subject-wise evaluation, explicit definitions of Œ∏ and œÜ, quantitative atlas-overlap metrics, ablations, and fair baselines, the contribution could become a useful addition to explainable neuroimaging workflows. As it stands, the paper is not yet ready for a top‚Äëtier venue but has a promising direction if the above issues are addressed.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces a framework for explainable 3D medical image analysis that ranks volumetric image blocks based on aggregated 2D rankings from axial, coronal, and sagittal planes. Core contributions include a feature‚Äëselection‚Äìaware Grad‚ÄëCAM variant (FS‚ÄëGrad‚ÄëCAM), several auxiliary feature and heatmap matrices, a five‚Äëfactor patch ranking scheme, and a hybrid aggregation over multiple feature‚Äëselection sets. Experiments on ADNI MRI (three‚Äëclass) and an autism dataset (binary) aim to highlight blocks corresponding to disease‚Äërelated brain regions. The topic is significant and the presentation conceptually clear, employing diagrams and explicit matrix definitions that illustrate the data flow.  \n\n**Major Comments**  \n1. **Methodological specification:** The main scoring functions (Œ∏ and œÜ) are undefined beyond monotonicity assumptions, rendering the approach unreproducible. FS‚ÄëGrad‚ÄëCAM requires formal derivation‚Äîits interaction with Grad‚ÄëCAM‚Äôs channel‚Äëwise weighting and the masking of flattened features is unclear. The rule requiring Œ∏ positive across all planes may omit relevant peripheral regions.  \n2. **Experimental protocol:** The reported slice‚Äëlevel splits likely mix slices from the same subject across training and test sets, leading to potential data leakage and inflated accuracies. Subject‚Äëwise cross‚Äëvalidation is essential.  \n3. **Quantitative validation:** No quantitative localization metrics or region‚Äëoverlap analyses are presented. The reliance on qualitative interpretation and language‚Äëmodel‚Äëgenerated anatomical claims is insufficient. Region‚Äëwise statistics or atlas‚Äëbased comparisons (e.g., hippocampus overlap) are needed.  \n4. **Baselines and ablations:** The study lacks quantitative comparisons against established alternatives such as 3D‚ÄëGrad‚ÄëCAM, AXIAL‚Äëstyle attention fusion, and SHAP‚Äëbased patch selection (sMRI‚ÄëPatchNet). Ablations on feature selection, k‚Äëparameter, and fusion rules would clarify robustness.  \n5. **Reproducibility and clarity:** CNN architectures, feature‚Äëselection hyperparameters, and training details are not described. Notation is inconsistent, and reuse of symbols (H/W) causes confusion.  \n6. **Comparative context:** Literature on multi‚Äëplane explainability and 2.5D fusion is cited but not systematically compared. Adopting evaluation protocols from prior work (e.g., AXIAL) would strengthen positioning.  \n\n**Minor Comments**  \n- Define all variables and normalization factors for Œ∏ij and œÜijk.  \n- Replace qualitative descriptions with formal or tabular quantitative data.  \n- Differentiate between patch and feature dimensions in notation.  \n- Provide dataset provenance, preprocessing, and subject‚Äësplit details.  \n- Clarify whether saliency maps are generated using held‚Äëout test data.  \n\n**Summary Paragraph**  \nThe study addresses an important problem‚Äîinterpretable region identification in volumetric neuroimaging‚Äîthrough a novel multi‚Äëplane aggregation concept. The framework‚Äôs terminology and structured pipeline aid understanding, and the idea has practical appeal for clinical interpretability. Nevertheless, core methodological ambiguities, unverified assumptions, possible data leakage, and a lack of quantitative or baseline comparisons undermine the scientific rigor required for publication. Strengthening the mathematical formulation, applying subject‚Äëwise evaluation, and including statistical localization metrics would substantially improve the contribution.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The concept is promising and innovative, but significant methodological clarification, reproducibility detail, and quantitative validation are required before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report: A NEW 3D IMAGE BLOCK RANKING METHOD USING AXIAL, CORONAL, AND SAGITTAL IMAGE PATCH RANKINGS FOR EXPLAINABLE MEDICAL IMAGING\n\n## 1. Opening Paragraph\n\nThe paper addresses the critical challenge of interpreting 3D medical imaging results from deep learning models, specifically focusing on understanding the relationship between 3D image blocks, extracted features, and final medical diagnoses. The authors propose a novel 3D image block ranking framework that leverages 2D axial, coronal, and sagittal patch analysis to generate a Block Ranking Map (BRM) for explainable medical imaging. Their approach consists of four key components: (1) developing a new FS-Grad-CAM method with feature selection to produce more interpretable heatmaps, (2) designing a 2D image patch ranking algorithm that utilizes feature matrices and statistical information from multiple heatmaps, (3) creating a 3D image block ranking algorithm that integrates patch rankings from all three anatomical planes, and (4) developing a hybrid ranking approach that combines results from multiple feature selection methods for increased robustness. The authors demonstrate their method on two medical imaging applications‚ÄîAlzheimer's disease diagnosis using the ADNI dataset and autism diagnosis‚Äîshowing that their approach can successfully identify top-ranked blocks corresponding to brain regions known to be associated with these conditions. The visualizations provide clinicians with axial, coronal, and sagittal views of important blocks, facilitating more explainable diagnosis.\n\n## 2. Major and Minor Comments\n\n### Major Strengths:\n- The paper presents a novel and practical approach to 3D medical image interpretation by leveraging 2D slice analysis from multiple anatomical planes, which aligns better with clinical workflows than direct 3D analysis.\n- The integration of feature selection with Grad-CAM (FS-Grad-CAM) creates more interpretable heatmaps by focusing on top-ranked features rather than using all features indiscriminately.\n- The hybrid block ranking approach that combines results from multiple feature selection techniques appears methodologically sound and provides more reliable results.\n- The validation using real medical datasets (ADNI and autism dataset) with cross-verification of findings through literature review demonstrates clinical relevance.\n- The visualizations of the Block Ranking Map with multi-planar views are clinically intuitive and could be valuable for practitioners.\n\n### Major Limitations:\n- The paper lacks quantitative comparison with existing 3D explainable AI methods, making it difficult to assess the true advancement over current state-of-the-art techniques.\n- The validation methodology relies on ChatGPT for identifying disease-associated brain regions, which introduces potential bias and lacks scientific rigor; systematic literature review by domain experts would be more appropriate.\n- Insufficient details are provided about implementation (exact network architecture, training parameters, computational requirements) to ensure reproducibility.\n- There is no clinical evaluation of whether the proposed method actually improves diagnostic accuracy or clinician decision-making compared to existing approaches.\n- The paper doesn't clearly establish the clinical utility of the approach or how it would integrate into actual medical workflows.\n\n### Minor Strengths:\n- The detailed mathematical formulation of feature matrices and ranking algorithms provides strong theoretical grounding.\n- The approach of using multiple 2D views to inform 3D interpretation is intuitive and aligns well with how clinicians naturally examine medical images.\n- The figures illustrating the BRM and patch rankings are clear and effectively communicate the method's output.\n\n### Minor Limitations:\n- Some technical details in the appendices could be better integrated into the main text for improved flow.\n- Limited discussion of computational complexity and runtime performance, which would be important for clinical deployment.\n- The paper could benefit from more discussion about generalizability to other medical imaging modalities beyond brain MRI.\n- The description of the \"ebrains\" software tool integration is somewhat vague and could be clarified.\n\n## 3. Evaluation Summary\n\n### Significance:\nThe work addresses an important problem in medical AI‚Äîthe need for explainable models in clinical decision-making. Understanding which regions of medical images contribute to AI diagnoses is crucial for clinical adoption. The proposed method has potential clinical significance as it provides a way to visualize and interpret 3D medical imaging results in a manner consistent with clinical anatomical thinking. However, the actual clinical impact isn't fully demonstrated, as there's no evaluation of how this method affects diagnostic accuracy or decision-making for medical professionals in practice.\n\n### Innovation:\nThe paper presents a novel approach by combining 2D patch rankings from multiple anatomical planes to generate 3D block rankings. The FS-Grad-CAM method and hybrid block ranking algorithm represent meaningful innovations. The systematic use of feature selection to improve interpretability is a key contribution. However, the innovation is more in the combination and medical application of existing techniques rather than completely novel methodologies, which somewhat limits its theoretical novelty.\n\n### Evaluation:\nThe evaluation is comprehensive in terms of technical validation on two medical datasets, showing that the method identifies brain regions associated with AD and autism. However, the evaluation has significant limitations: (1) no quantitative comparison with state-of-the-art explainable AI methods, (2) questionable methodology relying on ChatGPT for validation, (3) no clinical evaluation of utility for medical professionals, and (4) limited information about the robustness of the approach across different conditions. The quantitative results (e.g., classification accuracies) are presented but not well contextualized against existing methods.\n\n### Reproducibility:\nThe paper provides a good theoretical foundation and describes the methodology in detail, which supports reproducibility. However, significant gaps remain: (1) lack of specific implementation details (network architecture, exact training parameters), (2) the feature selection rule in Appendix A is dataset-specific and may not generalize, (3) no code or data availability statement, and (4) the complex coordinate mapping process could be described more clearly. These factors substantially reduce the reproducibility of the work.\n\n## 4. Decision Recommendation\n\n**Major Revision (reject/resubmit)**\n\nThis paper presents a promising approach to explainable 3D medical imaging with potential clinical value. The core methodology is well-structured and addresses an important problem in medical AI. However, several major issues must be addressed before the paper can be accepted:\n\n1. The authors must provide quantitative comparisons with existing 3D explainable AI methods to establish the true advancement of their approach.\n2. The reliance on ChatGPT for validation of brain region-disease associations should be replaced with rigorous systematic review methods involving medical domain experts.\n3. More detailed implementation information is required to ensure reproducibility, including network architecture details and training parameters.\n4. The clinical utility of the approach needs better demonstration, ideally with input from medical professionals regarding its practical value.\n\nThe paper would benefit from a more critical analysis of its limitations and a clearer demonstration of its advantages over existing approaches. With these improvements, the paper could make a valuable contribution to the field of explainable medical imaging. I recommend major revisions with the expectation that the authors will address these critical concerns before the manuscript can be reconsidered for publication.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a framework for explainable 3D medical image interpretation by integrating axial, coronal, and sagittal 2D patch rankings into a unified 3D Block Ranking Map (BRM). The method employs four components: (1) an FS‚ÄëGrad‚ÄëCAM approach with feature selection for more interpretable heatmaps; (2) a 2D image patch ranking algorithm based on feature matrices and statistical information; (3) a 3D image block ranking procedure combining multi‚Äëplane patch results; and (4) a hybrid ranking technique aggregating outcomes from multiple feature‚Äëselection methods. Demonstrations on Alzheimer‚Äôs disease (ADNI dataset) and autism data show that the approach highlights brain regions previously linked to these conditions. The paper is clearly presented, with well‚Äëstructured visualizations that aid interpretability for clinical users.\n\n---\n\n**Major Comments**  \n1. **Comparative Evaluation:** The paper does not include quantitative comparisons with current 3D explainable AI approaches, limiting assessment of advancement over existing methods.  \n2. **Validation Methodology:** Reliance on a language model (ChatGPT) to confirm disease‚Äërelated brain regions undermines scientific rigor. Validation should instead use systematic expert review.  \n3. **Implementation Transparency:** Key details‚Äînetwork architecture, training parameters, and computational requirements‚Äîare missing, hindering reproducibility.  \n4. **Clinical Utility:** There is no demonstration that the proposed method improves diagnostic accuracy or decision‚Äëmaking. Its integration into routine clinical workflow remains unclear.  \n5. **Generalizability:** The manuscript does not address applicability beyond brain MRI, leaving uncertainty about broader relevance.\n\n---\n\n**Minor Comments**  \n- Some algorithmic details placed in the appendices would be better summarized in the main text.  \n- Computational complexity and runtime should be reported.  \n- Clarification is needed about the integration of the ‚Äúebrains‚Äù tool.  \n- Figures are clear and effectively communicate multi‚Äëplanar results, though captions could specify clinical context.\n\n---\n\n**Summary Paragraph**  \nThe study tackles a clinically important problem‚Äîimproving interpretability of AI‚Äëbased 3D imaging. Its integration of feature‚Äëselected Grad‚ÄëCAM and plane‚Äëwise patch rankings represents a practical contribution with potential for clinical insight. However, the lack of quantitative baseline comparison, dependence on non‚Äëexpert validation, incomplete implementation reporting, and missing assessment of clinical benefit considerably weaken the empirical strength and reproducibility of the work. While the conceptual idea is promising and the visualization outputs are strong, substantial revisions are required to establish methodological rigor and demonstrate tangible improvement over prior art.\n\n---\n\n**Decision Recommendation:** **Major Revision (resubmit after substantial revision)**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the problem of interpreting how 2‚ÄëD image patches, 3‚ÄëD image blocks, feature maps, and final predictions are linked in 3‚ÄëD convolutional neural networks applied to medical imaging. By extracting axial, coronal, and sagittal slices from whole‚Äëbrain volumes, the authors propose a feature‚Äëselection‚Äëbased Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM) that produces heat‚Äëmaps highlighting a limited set of highly ranked features. They then introduce a five‚Äëcomponent patch‚Äëranking scheme that evaluates patches according to feature distribution, rank, activation frequency, and strength. Scores from the three orthogonal views are merged via a 3‚ÄëD block‚Äëranking algorithm, and several block‚Äëranking outcomes are averaged to obtain a hybrid block‚Äëranking map (BRM). Experiments on the ADNI cohort (982 subjects) and an autism cohort (286 subjects) report classification accuracies in the range of 0.93‚Äì0.99 and list the highest‚Äëranking blocks, which the authors claim correspond to brain regions implicated in Alzheimer‚Äôs disease and autism. The primary claimed contributions are the FS‚ÄëGrad‚ÄëCAM heat‚Äëmap generation, the patch‚Äëranking procedure, and the hybrid 3‚ÄëD block‚Äëranking framework for producing explainable diagnoses.\n\n---\n\n## General feedback  \n\n- **Significance:** Clarifying the decision‚Äëmaking of 3‚ÄëD CNNs is certainly relevant for clinical adoption, yet the manuscript mainly assembles a collection of ad‚Äëhoc ranking formulas. It does not advance the theory or practice of explainability in a principled way. (Abstract; Fig.‚ÄØ1)  \n- **Innovation:** The novelty is confined to the formal definitions (Defs‚ÄØ1‚Äë6) and the five‚Äëfactor patch‚Äëranking rule (Alg.‚ÄØ1). These extensions appear incremental relative to standard Grad‚ÄëCAM and lack a solid theoretical justification or empirical motivation. (Sec.‚ÄØ2; Alg.‚ÄØ1)  \n- **Evaluation:** The reported results consist of the underlying CNN classification accuracies (0.93‚Äì0.99) and qualitative tables of ‚Äútop‚Äëranked blocks‚Äù with their alleged anatomical relevance (Tables‚ÄØ1‚Äë6, Fig.‚ÄØ2, Fig.‚ÄØ3). The study omits any baseline explainability method, quantitative localisation metric, statistical testing, or ablation analysis of the individual components. (Sec.‚ÄØ4)  \n- **Reproducibility:** Several essential details are absent: the exact CNN hyper‚Äëparameters, the precise feature‚Äëselection pipeline (the rule in App.‚ÄØA appears arbitrary), the code or model checkpoints, and the mapping from block indices to physical coordinates (App.‚ÄØC). Without these, independent replication is unlikely. (App.‚ÄØA‚ÄëC)  \n\n---\n\n## Specific comments/critiques  \n\n1. The manuscript introduces a series of matrices (feature‚Äëbinary, accumulation, distribution, ranking, heat‚Äëmap activation/strength) without clarifying their necessity or demonstrating how each contributes to improved interpretability. (Sec.‚ÄØ2)  \n2. FS‚ÄëGrad‚ÄëCAM substitutes the conventional Grad‚ÄëCAM weighting (Eq.‚ÄØ2‚Äë3) but provides no empirical evidence that restricting the explanation to a reduced feature set yields more faithful or reliable heat‚Äëmaps. (Sec.‚ÄØ2.4)  \n3. The five‚Äëfactor patch‚Äëranking function \\( \\theta_{ij}=f(d_{ij},r_{ij},u_{ij},s_{ij}) \\) is described merely as ‚Äúmonotonically non‚Äëdecreasing‚Äù; the concrete functional form is omitted, rendering the algorithm non‚Äëdeterministic. (Alg.‚ÄØ1)  \n4. The selection of top‚Äëk features (k‚ÄØ=‚ÄØ250 or 100) and the order of feature‚Äëselection methods (Chi‚Äësquare ‚Üí mutual information ‚Üí ‚Ä¶) appear arbitrary. No ablation or sensitivity analysis regarding the choice of k or the FS pipeline is presented. (Sec.‚ÄØ4.1)  \n5. Validation relies on visual inspection of heat‚Äëmaps (Fig.‚ÄØ2) and manual literature lookup to link blocks with brain regions (Tables‚ÄØ1,‚ÄØ2). No quantitative localisation metric (e.g., intersection‚Äëover‚Äëunion with expert‚Äëannotated ROIs) or statistical significance testing is reported. (Fig.‚ÄØ2; Tables‚ÄØ1‚Äë2)  \n6. The manuscript mentions using ChatGPT to ‚Äúverify‚Äù the relevance of identified brain regions. The prompts, validation of the model‚Äôs responses, and the impact of this step on scientific rigor are not disclosed. (Sec.‚ÄØ4.2)  \n7. There is no comparison with established explainability approaches such as vanilla Grad‚ÄëCAM, 3‚ÄëD Grad‚ÄëCAM, attention‚Äëbased maps, or SHAP, precluding any assessment of the purported advantage of the hybrid BRM. (Abstract; Fig.‚ÄØ1)  \n8. Computational cost and runtime of the multi‚Äëstage pipeline (feature selection, FS‚ÄëGrad‚ÄëCAM, patch ranking, block aggregation) are omitted, yet such information is crucial for evaluating clinical feasibility. (Missing performance data)  \n9. The authors do not provide code, model weights, or detailed preprocessing specifications (voxel spacing, intensity normalisation, train‚Äëtest splits), which is inconsistent with the reproducibility expectations of *Medical Image Analysis*. (App.‚ÄØB‚ÄëC)  \n\n---\n\n## A suggested decision  \n\n**Reject**  \n\n*Rationale:* The manuscript does not offer a sufficiently novel methodological contribution, and its evaluation lacks the rigor, quantitative analysis, and comparative baselines required to substantiate the claimed benefits. Moreover, the absence of critical implementation details severely undermines reproducibility.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript investigates methods for interpreting 3‚ÄëD convolutional neural networks applied to medical imaging by relating 2‚ÄëD image patches, 3‚ÄëD blocks, feature maps, and predictions. The authors propose a feature‚Äëselection‚Äëbased Grad‚ÄëCAM (FS‚ÄëGrad‚ÄëCAM) that highlights a limited set of top‚Äëranked features across axial, coronal, and sagittal views, followed by a five‚Äëcomponent patch‚Äëranking scheme and a hybrid block‚Äëranking map (BRM) for aggregating results in 3‚ÄëD. Experiments on Alzheimer‚Äôs and autism datasets report high classification accuracies (0.93‚Äì0.99) and visual correspondences between top‚Äëranked blocks and known brain regions. While the goal of improving interpretability of CNN decisions in neuroimaging is relevant and timely, the contribution largely builds on existing Grad‚ÄëCAM techniques with additional heuristic ranking steps, and the presentation lacks the methodological depth and evaluation rigor expected for such claims.  \n\n---\n\n**Major Comments**  \n1. The work primarily assembles ad‚Äëhoc ranking formulas rather than introducing a principled advance in model explainability.  \n2. Novelty is limited to incremental formal definitions and the five‚Äëfactor patch‚Äëranking rule, which lacks theoretical or empirical justification.  \n3. Evaluation focuses on classification accuracy and qualitative visualization; no baselines, quantitative localization metrics, ablation studies, or statistical tests are provided.  \n4. Key implementation details‚ÄîCNN hyper‚Äëparameters, feature‚Äëselection procedure, code availability, and mapping from block indices to anatomical coordinates‚Äîare missing, hindering reproducibility.  \n5. Numerous matrices and ranking operations are introduced without demonstrating their individual necessity or contribution to interpretability.  \n6. FS‚ÄëGrad‚ÄëCAM modifies the standard Grad‚ÄëCAM weighting but offers no evidence that restricting explanations to selected features improves fidelity.  \n7. The five‚Äëfactor ranking function is underspecified, making the algorithm non‚Äëdeterministic.  \n8. Parameter choices such as the number of top‚Äëk features and the sequence of feature‚Äëselection methods appear arbitrary and untested.  \n9. Validation relies entirely on visual inspection and literature lookup without quantitative regional overlap or significance testing.  \n10. The use of ChatGPT for region verification is insufficiently documented, raising concerns about methodological rigor.  \n11. No comparison is made to other explainability methods, preventing assessment of relative advantage.  \n12. Runtime and computational cost are not reported, leaving clinical feasibility unassessed.  \n\n---\n\n**Minor Comments**  \n- Improve clarity in definitions of ranking matrices and notation consistency.  \n- Provide explicit equations for the monotonic function used in patch ranking.  \n- Detail preprocessing parameters (voxel spacing, normalization, data splits).  \n- Correct minor ambiguities in figure and table references; indicate missing performance tables where applicable.  \n\n---\n\n**Summary Paragraph**  \nOverall, the study addresses an important interpretability problem but does not demonstrate methodological novelty or experimental rigor sufficient to substantiate its claims. The absence of baselines, quantitative evaluation, and essential implementation details severely limits the credibility and reproducibility of results. While the manuscript presents ambitious integration of feature selection and Grad‚ÄëCAM visualization, the approach remains heuristic and insufficiently validated.  \n\n---\n\n**Decision Recommendation**  \n**Reject** ‚Äì The contribution is incremental, experimental evaluation is weak, and the reproducibility of results cannot be ensured.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Luna Zhang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_727c34225dfc91a5f7681403d6688e41dc3bcb20.pdf",
    "remote_url": "https://openreview.net/pdf/727c34225dfc91a5f7681403d6688e41dc3bcb20.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Build your own cell: Diffusion Models for Multichannel 3D Microscopy Image Generation",
    "status": "not_started",
    "evaluators": [
      "Tolga",
      "Justin"
    ],
    "primary_area": [
      "applications to physical sciences (physics, chemistry, biology, etc.)"
    ],
    "keywords": [
      "3D Diffusion Models"
    ],
    "abstract": "Three-dimensional (3D) cellular morphology is a critical indicator of cellular function, disease states, and drug responses. However, capturing and interpreting the complex relationships between cell shape, treatment conditions, and their biological implications remains a challenge. To address this, we present \"Build Your Own Cell'' (BYOC), a multichannel 3D generative framework that combines vector quantisation and diffusion models to synthesise biologically realistic 3D cell structures. BYOC captures intricate morphological changes induced by different drug treatments, enabling high-throughput in silico simulations and screening of cell shapes in response to varied conditions. This novel framework represents a significant step towards accelerating pre-clinical drug development by synthesising high-resolution, biologically realistic 3D cells, potentially reducing reliance on labour-intensive experimental studies. By ensuring phenotypic consistency between cell and nucleus volumes through joint modelling, BYOC provides high-fidelity reconstructions that could facilitate downstream analyses, including drug efficacy evaluation and mechanistic studies.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe paper introduces a multi-channel 3D diffusion model designed for generating two-channel cell images from volumetric fluorescence microscopy data. By focusing on the coupling of the two channels within the diffusion process, the model aims to improve the quality of generated dual-channel 3D cell images. The results presented show an improvement over the current state-of-the-art in this area.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- Addresses a challenging and pertinent problem in the field of biomedical microscopy, specifically in cellular imaging.\n- The overall motivation behind the proposed methodological enhancements is generally clear.\n- The experimental outcomes demonstrate promising improvements over existing methods.\n\n### Weaknesses\n\n- The biological rationale behind the model is not thoroughly convincing or well-articulated.\n- Some concrete methodological choices lack clear motivation or detailed explanation, leading to potential confusion (e.g. a clear motivation why and how to use VQGANs would be nice).\n- Some details are missing or inadequately explained in the formal equations and overall framework.\n- The manuscript tends to be imprecise in its language, which affects clarity and understanding.\n- The conclusion lacks specificity regarding the contributions, limitations and future directions of the methods-aspects of the work.\n\n### Questions\n\n1. How could biological or mechanistic understanding arise from generative models in your context? Can you expand and provide a stronger motivation for this idea?\n2. You mention that \"GANs often struggle with generating coherent latent representations.\" Since GANs do not inherently produce latent representations in the same way as e.g. Variational Autoencoders, could you clarify what \"coherent latent representations\" means in the context of GANs, and how this specifically relates to your proposed method's advantages?\n3. The claim that multiple color channels can be treated as distinct modalities is not clearly explained in my opinion but is crucial to the suggested method. Do you have examples from related work where color channels have been treated as distinct modalities? Could you explain the biological basis for considering cell and nucleus channels as separate modalities?\n4. In Equation 1, are the variables h,w,d the same dimensions as H,W,D? If not, what is their relationship? Similarly, in Equation 2, the depth dimension d seems to be omitted‚Äîwas this intentional or a typo? Please add a brief explanation of these variables and their relationships directly after the equations.\n5. How does the simultaneous recovery of both channels relate specifically to latent diffusion? Can you provide a specific example or illustration of how the simultaneous recovery process works in your model, and how it differs from standard latent diffusion approaches?\n6. What is the reason for using unquantized embeddings in your framework? If they drift from the codebook vectors, how does this affect the model, and what is the underlying motivation?\n7. In Equation 12, the variable t should be defined. Additionally, in Equation 13, what exactly is Œº_Œ∏_cn(‚ãÖ) computing‚Äîonly the mean or is there an associated variance? If not, what is the variance of your Gaussian?\n8. There is an existing WNet in medical imaging literature [1]. To avoid confusion, would you consider renaming your model?\n9. Can you provide more precise details about your dual-channel 3D architecture, perhaps with references or a schematic in the supplementary material?\n10. On page 6, you state that attention mechanisms are \"strategically placed\" to focus on regions of interest. Could you elaborate on the strategy behind their placement and how regions of interest are determined?\n11. The numerical differences in Table 1 are hard to interpret without context. Could you explain or hint to what these differences mean in terms of image quality and their significance in your application? Can you provide a brief interpretation guide for the FID and MMD scores, perhaps indicating what range of differences would be considered significant in this context? You could also include a qualitative comparison of images corresponding to different score ranges to help readers understand the practical implications of these differences.\n12. Could you provide more context about the ResNet50 model used‚Äîfor instance, what type of medical images it was trained on?\n\n**Additional Feedback for Improvement:**\n\n- In Figure 1, please explain what the rows and columns represent to enhance understanding.\n- In the introduction, you mention that \"single-cell data is often high-dimensional and inconsistent in size.\" Could you clarify whether this inconsistency refers to the images, cells, biological structures, or image resolutions?\n- In the Related Work section, the statement about discriminative frameworks needing a \"deep understanding of the underlying input distribution\" is unclear. Providing an example or reference could help clarify this point.\n- It might be beneficial to first introduce and describe the dataset before delving into implementation details like volume padding.\n- Please specify the size and resolution of the microscopy images. Are the single-cell images crops from larger stacks, or are they the direct output from the microscope?\n- Consider citing relevant works such as the 3D U-Net architecture [2] to situate your work within the existing literature.\n- The conclusion would be stronger if it discussed potential methodological developments and acknowledged limitations of the current approach.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 6\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a multi-channel 3D diffusion model for generating dual-channel cell images from volumetric fluorescence microscopy data. By incorporating interactions between the two image channels within the diffusion process, the approach aims to enhance the realism and fidelity of generated 3D cellular images. The paper is generally well motivated, with a clear focus on improving dual-channel image generation in biomedical microscopy, and shows quantitative gains over existing methods. However, the exposition and biological rationale could be clearer to fully support the technical contributions.\n\n---\n\n**Major Comments**  \n1. The biological motivation for the model is not convincingly articulated. The link between the proposed generative approach and potential biological or mechanistic insight remains unclear.  \n2. Several methodological design choices lack sufficient justification or technical explanation, such as the rationale and implementation details of the VQGAN component.  \n3. Certain equations and variables are ambiguous or incomplete (e.g., definitions and relationships of *h, w, d* vs. *H, W, D*; omission of the depth term in Equation 2; missing definitions of *t* and Œº_Œ∏_cn(‚ãÖ); unclear variance terms).  \n4. The paper should elaborate on how simultaneous recovery of both channels operates within latent diffusion and how this differs from conventional latent diffusion models.  \n5. The role and placement of attention mechanisms are mentioned but not sufficiently specified in terms of strategy or identification of regions of interest.  \n6. Numerical results (FID and MMD) in Table 1 lack interpretive context; readers would benefit from a discussion of what score differences mean in practice, including potential qualitative comparisons.  \n7. The term ‚Äúcoherent latent representations‚Äù in relation to GANs requires clarification, as GANs do not produce latent representations in the same manner as VAEs.  \n8. To avoid confusion with the existing WNet architecture in medical imaging, a name change may be advisable.\n\n---\n\n**Minor Comments**  \n- Clarify what the rows and columns in Figure‚ÄØ1 represent.  \n- Specify whether the size inconsistency in single-cell data refers to images, cells, or imaging resolutions.  \n- The statement on discriminative frameworks requiring ‚Äúdeep understanding of the input distribution‚Äù would benefit from a supporting reference.  \n- Present the dataset description before discussing implementation details such as volume padding.  \n- Provide image size/resolution information and indicate whether data are cropped or raw microscope outputs.  \n- Consider referencing works such as 3D‚ÄØU‚ÄëNet for contextual grounding.  \n- The conclusion should more clearly acknowledge limitations and future directions.\n\n---\n\n**Summary Paragraph**  \nThis work addresses a relevant and technically demanding problem in 3D fluorescence microscopy image generation. The concept of modeling inter-channel coupling within a diffusion framework is promising and supported by encouraging experimental improvements. Nonetheless, insufficient explanation of the biological premise, limited methodological clarity, and incomplete equation definitions reduce the work‚Äôs overall soundness and readability. With improved articulation of the rationale, reformulated equations, stronger methodological justification, and clearer interpretation of results, the paper could make a more substantial contribution.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The paper demonstrates solid motivation and preliminary results but needs substantial clarification and expansion of the methodological and interpretive components before it can be accepted.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper presents the novice Build Your Own Model (BYOL), a multichannel generative framework leveraging the diffusion model, to generate a population of simulated 3D multichannel data that shows the morphological changes in cells perturbed by drug treatments. The model captures the relation between the nuclear and cytoplasmic channels used for model training when generating the simulated images and presents a high spatial resolution of the images. The authors benchmarked the model against already available models like GAN-based models and MedicalDiffusion, useful for 3D image generation for the same test case, and found the best overall model performance.\n\n### Soundness: 3\n\n### Presentation: 4\n\n### Contribution: 2\n\n### Strengths\n\nThe model outperforms existing models, generating nuanced morphological changes due to perturbations like drug treatments. Compared to existing models, it can also accurately capture the 3D resolved morphology of the cellular tags. The model is best at generating cellular data and matches real data.\n\n### Weaknesses\n\nThe model captures the morphological changes associated with the perturbations it has been trained on but has not yet been shown to be generalizable to different cell types and drug treatments. This has been marked as a future prospect of the study. This is important in biological studies as tagging and imaging the markers are expensive for generating training data and a very important domain where biology communities would benefit.\n\n### Questions\n\nThe metrics used to evaluate the model are good for evaluating the overall model performance. But do the metrics evaluate the inter- and intra-channel prediction accuracy? The authors stated that it is biologically relevant and an improvement brought by the work. But how can you evaluate this specific aspect using relevant metrics from a biological point of view?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 8\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Build Your Own Model (BYOL)*, a multichannel generative framework based on diffusion models for synthesizing 3D multichannel cellular images under various drug perturbations. The approach models relationships between nuclear and cytoplasmic channels to generate high‚Äìspatial-resolution synthetic data that reflects morphological changes in treated cells. The paper benchmarks BYOL against existing generative approaches, including GAN-based models and MedicalDiffusion, reporting superior overall performance. The study is clearly presented, though some questions remain regarding generalizability and biological validation.  \n\n**Major Comments**  \n1. **Generalizability** ‚Äì The model‚Äôs capacity to generalize beyond the specific cell types and drug treatments used for training has not been demonstrated. The authors acknowledge this limitation and mention it as future work, yet broader validation is essential for biological applicability and for reducing the cost of new tagging and imaging campaigns.  \n2. **Evaluation Metrics** ‚Äì While the chosen metrics effectively assess overall model performance, it is unclear whether they capture inter- and intra-channel prediction accuracy, a key contribution claimed by the authors. Further clarification on how the evaluation reflects biological relevance would strengthen the study.  \n3. **Biological Utility** ‚Äì The model‚Äôs potential impact on the biology community is high, but the review notes that current experiments remain limited in scope. Demonstrating usefulness across cell types or different perturbations would enhance confidence in the method‚Äôs applicability.  \n\n**Minor Comments**  \n- The manuscript presentation is generally good, though further clarification of metric selection and interpretability from a biological standpoint would improve clarity.  \n- No ethical issues were identified.  \n\n**Summary Paragraph**  \nOverall, the study provides a technically strong contribution by extending diffusion models to biologically meaningful multichannel 3D image generation. Strengths include high-quality image synthesis and improved performance over prior models. Weaknesses involve limited generalization and insufficient evaluation of biologically relevant inter-channel accuracy. Addressing these would increase the paper‚Äôs robustness and practical relevance.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Recommended revisions include expanding validation to additional biological settings and clarifying evaluation metrics to substantiate claims of biological realism and generalization.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe authors combine vector quantized GANs to learn representations of microscopy images of cells and develop a denoising diffusion model for latent representations. By combining vector quantized representations and the process of diffusion, they seek to generate 3D images of cells that belong to the distribution of realistic microscopy images.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n* Strategy: The strategy of using diffusion modeling to improve the accuracy of prediction of GANs is promising.\n\n### Weaknesses\n\n* Incorrect assumptions about microscopy image data: Microscopy images often consist of more than two channels and many of them cannot be just binned into cells and nuclei. The authors seem to be familiar with medical imaging datasets but unaware of datasets such as cell painting (JUMP, CHAMMI), human protein atlas, and virtual staining. These datasets illustrate that microscopy data often consists of channels that encode multiple organelles and cellular compartments.\n* Lack of 3D predictions: Although the paper claims to be the first to build a 3D generative model of microscopy images, all the presented data is 2D. The authors should show orthogonal slices of generated volumes.\n* Relevance of metrics: Fre ÃÅchet Inception Distance and Maximum Mean Discrepancy seem reasonable. However, the authors do not clarify how these metrics may be affected by the typical failure modes of GANs, such as hallucinations of spurious cellular processes.\n\n### Questions\n\n* What is the effect of the diffusion on the quantized codebook? The way diffusion is used during inference was not apparent from the text or figures.\n* Does the approach work only with a specified number of input channels?\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents a method that integrates vector-quantized GANs with denoising diffusion models to learn latent representations of microscopy images of cells, aiming to generate realistic 3D images within the distribution of observed microscopy data. The approach seeks to combine the generative power of diffusion processes with the compact representation capacity of VQ-GANs. While the concept is promising, the manuscript‚Äôs clarity could be improved, and there are substantive concerns regarding the assumptions about microscopy data, the scope of results, and the choice and interpretation of evaluation metrics.  \n\n**Major Comments**  \n1. **Assumptions about microscopy image data:** The paper assumes microscopy images typically contain only two channels (cells and nuclei), which is inaccurate. Many datasets‚Äîsuch as Cell Painting (JUMP, CHAMMI), the Human Protein Atlas, and virtual staining datasets‚Äîinclude multiple channels capturing diverse organelles and subcellular compartments. The authors‚Äô limited framing suggests insufficient consideration of the broader range of microscopy data types.  \n2. **3D generation claim:** Despite the claim of presenting a 3D generative model for microscopy imaging, the paper only shows 2D data. To substantiate the claim, the authors should include orthogonal slices or other evidence of generated 3D volumes.  \n3. **Evaluation metrics and interpretation:** The use of Fr√©chet Inception Distance and Maximum Mean Discrepancy is standard, but the authors do not explain how these metrics respond to common GAN failure modes, such as hallucinated cellular structures. Clarifying this relationship would strengthen the evaluation.  \n4. **Methodological clarity:** The role of diffusion during inference and its effect on the quantized codebook is unclear. A clearer description or schematic would improve reproducibility. Additionally, it should be specified whether the approach depends on a fixed number of input channels.  \n\n**Minor Comments**  \n- The manuscript would benefit from improved description of implementation steps and figures to clarify the workflow between the VQ-GAN and diffusion components.  \n\n**Summary Paragraph**  \nOverall, the submission proposes a potentially useful integration of diffusion models with vector-quantized GANs for microscopy image generation. The innovation lies in the combination strategy, which could improve representational accuracy. However, the work currently suffers from questionable assumptions about input data, unsubstantiated claims regarding 3D results, and insufficient methodological detail. Improved data awareness and clearer exposition would enhance the paper‚Äôs impact and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI ‚Äì ‚úÖ**\n   The paper presents a new generative modeling framework for multichannel 3D fluorescence microscopy, integrating vector quantization (VQ) with diffusion models to synthesize realistic volumetric cellular images. The methodological emphasis on multichannel latent diffusion and inter-channel consistency places it squarely within TMI‚Äôs methodological domain, particularly under *imaging physics/AI-based image formation, synthesis, and reconstruction*.\n\n2. **Novelty & Contribution Level ‚Äì Moderate to Strong**\n   - The **‚Äúlibrary of codebooks‚Äù** for separate biological channels combined with a **multichannel diffusion stage** is a novel integration not previously reported in 3D cell imagery synthesis.\n   - The pragmatic innovation lies in demonstrating *joint-channel generative modeling* for cell and nucleus morphology under drug perturbations.\n   - While the technical blocks (VQGAN, DDPM, UNet) are established, their combination for *multichannel 3D biological imaging* is distinctive and offers generalizable methodological value.\n   - Novelty is more architectural and conceptual rather than theoretical; however, it is sufficient for TMI if rigorously validated.\n\n3. **Technical and Experimental Rigor ‚Äì Good but with Gaps**\n   - Experiments are systematic: dataset description is clear; baselines and metrics are standard (FID, MMD using Med3D features).\n   - Implementation details and ablations are comprehensive.\n   - Missing aspects: uncertainty quantification, statistical significance testing, and evaluation of biological validity beyond pixel-level similarity (e.g., morphological metrics or expert scoring). Dataset splits are consistent, but no external validation set.\n   - Ethical clarity on dataset provenance (cell line, IRB, data sharing license) could be strengthened.\n\n4. **Clarity and Presentation ‚Äì Very Good**\n   - The manuscript is clearly structured with logical flow and meticulous mathematical description.\n   - Figures are likely informative (though text indicates heavy visual content).\n   - Minor typographic and stylistic issues (spacing, capitalization, figure referencing).\n\n5. **Ethical and Reproducibility Compliance ‚Äì Partially Addressed**\n   - GitHub link placeholder suggests intention for open code release.\n   - Dataset origin (melanoma cells) is described but without explicit IRB or licensing statement.\n   - No mention of code availability assurance under anonymization review.\n\n---\n\n**Phase 2: IEEE TMI Review Report**\n\n**1. Summary**  \nThe manuscript, *‚ÄúBuild Your Own Cell: Diffusion Models for Multichannel 3D Microscopy Image Generation,‚Äù* introduces BYOC ‚Äî a generative framework combining VQGAN encoding with a multichannel 3D diffusion process. The model uses independent codebooks for each biological channel (cell, nucleus) and jointly denoises these latent spaces via a dual-channel 3D UNet in the diffusion loop. Evaluations on a 7k-cell light-sheet fluorescence dataset of drug-treated melanoma cells demonstrate that BYOC outperforms GAN-based and diffusion-based baselines (HA-GAN, Œ±-GAN, MedicalDiffusion) on quantitative similarity metrics (FID, MMD) and yields visually realistic multi-channel 3D reconstructions. Ablation experiments confirm the benefit of channel-specific codebooks for morphological consistency.\n\n**2. Strengths**\n- Clear methodological contribution integrating vector quantization and conditional diffusion into a multichannel 3D framework.\n- Comprehensive experimental setup with multiple baselines and ablation studies.\n- Strong qualitative demonstrations of biologically faithful reconstructions across different drug treatments.\n- Potentially impactful for in-silico high-throughput phenotyping and drug-screening applications.\n\n**3. Weaknesses**\n- Biological validation remains limited to visual inspection and distribution-level metrics; lacks expert annotation or morphological feature comparison.\n- Evaluation confined to one cell line and specific drug treatments ‚Äî limited generalizability.\n- No uncertainty analysis or robustness study (e.g., to noise, resolution changes).\n- Insufficient discussion of computational scalability and inference time, relevant for translational use.\n- Ethical/compliance clarifications are minimal (dataset license, data sharing, human/animal sample policy).\n\n**4. Major Comments**\n1. **Validation Beyond Distribution Metrics:**  \n   Extend evaluation with domain-relevant morphological descriptors (e.g., cell/nucleus volume ratios, shape descriptors) or expert morphological score comparisons. This would strengthen biomedical relevance.  \n2. **Dataset and Compliance Clarification:**  \n   Include explicit details on dataset availability, ethical approval (if needed for biological specimens), and licensing terms.  \n3. **Code and Reproducibility:**  \n   Confirm that training and inference code (VQGAN + DDPM pipeline) will be made available; current placeholder link is insufficient for TMI reproducibility standards.  \n4. **Generalizability and Scalability:**  \n   Discuss limitations for other modalities (e.g., confocal, electron microscopy) and computational resource demands.  \n5. **Quantitative Uncertainty and Repeatability:**  \n   Consider assessing variance across multiple random seeds or training repeats to ensure stability given small batch sizes.\n\n**5. Minor Comments**\n- Correct typographical inconsistencies (e.g., ‚ÄúD IFFUSION,‚Äù ‚ÄúC ¬∏ ic ¬∏ek‚Äù) and unify capitalization.  \n- Figure numbering and captions should be concise and self-contained.  \n- Include clear axis labels and units for all quantitative plots.  \n- Reference formatting should adhere to IEEE style (currently hybrid arXiv/journal forms).  \n- Clarify how 3D FID/MMD was computed (feature extraction pipeline).\n\n**6. Overall Recommendation ‚Äì Major Revision**  \nThe work presents a promising, methodologically meaningful step forward in 3D multichannel biomedical image generation, fitting IEEE TMI‚Äôs methodological innovation scope. However, additional experimental validation and stronger reproducibility/ethical transparency are required before acceptance.\n\n**7. Confidence Level ‚Äì 4/5**  \nThe methodological domain and evaluation strategies are within the reviewer‚Äôs expertise; confidence is high in assessing novelty and rigor, moderate for biological relevance interpretations.\n\n---\n\n**Summary Recommendation:**  \n‚ñ∂ **Major Revision** ‚Äî The framework is innovative and potentially impactful but needs bolstered validation (morphological realism assessment, reproducibility details) and improved ethical/data disclosure to meet IEEE TMI expectations.",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Build Your Own Cell (BYOC)*, a generative framework for 3D multichannel fluorescence microscopy images that integrates vector quantization with diffusion modeling. The approach introduces channel-specific latent codebooks for cell and nucleus signals, jointly denoised through a 3D multichannel diffusion process. The study aims to generate realistic volumetric cellular imagery under drug perturbations and demonstrates quantitative and qualitative improvements over existing GAN and diffusion-based models. Overall, the paper is clearly written and methodologically sound, though several aspects of validation, compliance, and reproducibility need further attention.\n\n---\n\n**Major Comments**  \n1. **Validation Beyond Distribution Metrics** ‚Äì Evaluation relies on distribution-based measures (FID, MMD) and visual inspection. Including morphological or cell-structure-based metrics, or expert validation, would substantially enhance the biomedical credibility of the results.  \n2. **Dataset and Ethical Clarifications** ‚Äì Provide explicit information on dataset origin, licensing, and ethical approval related to biological samples. Current documentation of provenance and compliance is insufficient.  \n3. **Reproducibility and Code Availability** ‚Äì Confirm that full training and inference code for the VQGAN‚ÄìDDPM pipeline will be released. The current placeholder link does not meet reproducibility expectations.  \n4. **Generalization and Scalability** ‚Äì Discuss computational requirements, scalability to other imaging modalities, and potential constraints of the current architecture.  \n5. **Uncertainty and Stability Analysis** ‚Äì Quantify model variability across random seeds or training runs to assess stability and robustness.  \n6. **Evaluation Scope** ‚Äì Experiments cover a single cell line and specific drug treatments, limiting generalizability. Consider external validation or cross-condition testing to strengthen claims.\n\n---\n\n**Minor Comments**  \n- Correct typographic and capitalization errors (e.g., inconsistent spacing and special characters).  \n- Ensure figure captions are self-contained and that all axes are labeled with units.  \n- Standardize reference formatting and citation style.  \n- Provide a clear description of how 3D FID and MMD metrics were computed (feature extraction details).  \n\n---\n\n**Summary Paragraph**  \nThe study contributes a novel combination of vector quantization and diffusion modeling for multichannel 3D image synthesis, yielding realistic and coherent reconstructions. Its strengths lie in methodological integration, comprehensive ablation studies, and clear presentation. However, weaknesses include limited biological validation, incomplete compliance statements, and a lack of reproducibility assurance. Addressing these issues would significantly enhance the rigor and impact of the work.\n\n---\n\n**Decision Recommendation: Major Revision**  \nThe manuscript is innovative and promising but requires stronger experimental validation, explicit compliance details, and confirmation of reproducibility before it can be recommended for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThe paper presents \"Build Your Own Cell\" (BYOC), a generative framework for synthesizing biologically realistic 3D multichannel cellular structures. The method addresses the challenge of generating high-resolution 3D cell and nucleus volumes that maintain inter-channel consistency and biological accuracy. BYOC combines vector quantization using a library of channel-specific codebooks with multichannel diffusion models. The approach uses adapted VQGAN architecture to learn separate codebooks for cell and nucleus channels (Section 3.1), followed by a DualChannelUNet-based diffusion process to generate coherent multichannel volumes (Section 3.2). The authors evaluate their method on over 7,083 metastatic melanoma cells treated with three different drugs, demonstrating superior performance compared to baselines including MedicalDiffusion, HA-GAN, and W-GAN across FID and MMD metrics (Table 1). The framework aims to enable high-throughput in silico drug screening and morphological analysis.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical Formulation Inconsistencies**\n  - Equation 1 contains a clear error where both encoded representations are defined as E(xnuc), when one should be E(xcell), compromising mathematical clarity and reproducibility\n  - The notation switches inconsistently between ÀÜz and zq without clear distinction in meaning (Equations 3-4)\n  - The joint reverse process notation pŒ∏zÀÜqcell zÀÜqnuc lacks proper mathematical formatting and subscript clarity (Section 3.2)\n\n‚Ä¢ **Limited Experimental Validation Scope**\n  - Dataset restricted to single cell type (melanoma) and only three drug treatments, limiting generalizability claims made in the abstract and introduction\n  - No comparison with recent 3D medical generation methods beyond MedicalDiffusion (2023), missing important baselines from the evolving field\n  - Evaluation metrics (FID, MMD) computed on averaged channels rather than preserving multichannel structure, potentially masking inter-channel consistency issues (Section 4.3)\n  - Cross-validation details mentioned in Table 1 but methodology not described in Section 4.1\n\n‚Ä¢ **Technical Architecture Justification Gaps**\n  - No ablation study comparing the proposed DualChannelUNet against standard 3D UNet architectures for multichannel processing\n  - Library of codebooks concept lacks comparison with joint multichannel codebook approaches beyond the basic \"unimodal\" baseline (Figure 6)\n  - Missing analysis of computational overhead introduced by separate codebooks versus unified representations\n  - Spatial and depth-wise attention mechanisms mentioned (Section 3.2) but no evaluation of their specific contribution\n\n‚Ä¢ **Biological Validation Deficiencies**\n  - No quantitative analysis of biological plausibility metrics such as nucleus-to-cell volume ratios or spatial positioning accuracy\n  - Generated samples show \"slightly smoother\" fine details (page 8) but this degradation is not quantitatively assessed\n  - Claims of \"biologically realistic\" synthesis lack validation from domain experts or comparison with established biological morphology benchmarks\n  - Inter-channel consistency evaluation relies only on visual inspection rather than quantitative spatial relationship metrics\n\n## Suggestions for Improvement\n\n‚Ä¢ **Correct Mathematical Formulations**\n  - Fix Equation 1 to properly define ·∫ëcell = E(xcell) and ·∫ënuc = E(xnuc) with consistent notation throughout\n  - Standardize notation between ·∫ë and zq with clear definitions of when each representation is used\n  - Reformat the joint reverse process notation with proper mathematical typesetting and clear subscript definitions\n\n‚Ä¢ **Expand Experimental Validation**\n  - Include additional cell types and drug treatments to demonstrate generalizability across biological contexts\n  - Add comparisons with recent 3D generative methods and multichannel synthesis approaches from 2023-2024 literature\n  - Compute FID and MMD metrics on full multichannel volumes rather than channel-averaged versions to properly assess inter-channel consistency\n  - Provide detailed cross-validation methodology and statistical significance testing\n\n‚Ä¢ **Strengthen Technical Justification**\n  - Conduct ablation studies comparing DualChannelUNet against standard 3D UNet with multichannel inputs\n  - Compare library of codebooks against joint multichannel codebook learning with multiple architectural variants\n  - Report computational costs and memory requirements for separate versus unified codebook approaches\n  - Quantitatively evaluate the contribution of spatial and depth-wise attention mechanisms through targeted ablations\n\n‚Ä¢ **Enhance Biological Validation**\n  - Implement quantitative biological metrics such as nucleus-cytoplasm ratio, spatial positioning accuracy, and morphological feature preservation\n  - Quantify the \"smoother details\" observation with texture and fine-structure preservation metrics\n  - Collaborate with cell biology experts for qualitative validation or compare against established morphological databases\n  - Develop inter-channel spatial relationship metrics to quantitatively assess biological consistency beyond visual inspection",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *Build Your Own Cell* (BYOC), a generative framework designed to synthesize biologically realistic 3D multichannel cellular structures. The approach integrates vector quantization via channel-specific codebooks with a multichannel diffusion process, employing an adapted VQGAN and DualChannelUNet architecture. The system is evaluated on a dataset of over 7,000 metastatic melanoma cells under three drug treatments and shows superior quantitative performance to MedicalDiffusion, HA-GAN, and W-GAN. The work aims to facilitate in silico drug screening and morphological analysis. While the paper is generally well written and presents an interesting framework, several mathematical, experimental, technical, and biological validation issues limit its reproducibility and generalizability.  \n\n**Major Comments**  \n1. **Mathematical Formulation Inconsistencies:** Equation 1 incorrectly defines both encoded representations as *E(xnuc)* instead of distinguishing between *E(xcell)* and *E(xnuc)*. Notation alternates between ·∫ë and zq without clear differentiation, and the joint reverse process expression lacks proper subscripts and mathematical clarity.  \n2. **Limited Experimental Validation:** Evaluation is restricted to a single cell type and three drug treatments, constraining generalizability. Comparisons omit several recent 3D medical generation baselines, and FID/MMD metrics are averaged over channels rather than computed on multichannel volumes. Cross-validation procedures are not adequately described.  \n3. **Insufficient Technical Justification:** The DualChannelUNet and separate codebook designs are not supported by ablation studies or computational cost analyses. The benefits of spatial and depth-wise attention mechanisms are asserted but untested.  \n4. **Biological Validation Deficiencies:** Claims of biological realism are not quantitatively supported; metrics such as nucleus-to-cell ratios or spatial accuracy are missing. The evaluation of inter-channel consistency relies solely on visual inspection, and qualitative comments about smoother textures are not measured.\n\n**Minor Comments**  \n- Ensure consistent mathematical notation and proper equation formatting.  \n- Clarify cross-validation methodology and dataset splits.  \n- Label figures and tables clearly to match text references.  \n\n**Summary Paragraph**  \nOverall, the paper presents a promising multichannel generative approach with clear potential applications in biological image synthesis. However, formulation errors, limited validation scope, missing ablation studies, and absent quantitative biological evaluations hinder confidence in the reported results. Addressing these issues would substantially strengthen the work‚Äôs validity and reproducibility.  \n\n**Decision Recommendation:** Major Revision.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript introduces \"Build Your Own Cell\" (BYOC), a novel multichannel 3D generative framework designed to synthesize biologically realistic cell structures. The method integrates vector quantization with diffusion models to capture intricate morphological changes induced by different drug treatments, aiming to facilitate high-throughput in silico simulations and screening of cell shapes. The authors present a thorough methodology and demonstrate the framework's ability to generate high-resolution, biologically realistic 3D cell structures, which could accelerate pre-clinical drug development. The manuscript is well-written and presents a clear rationale for the proposed approach.\n\n## Major Comments\n1. Novelty and Positioning: The proposed method is innovative in its use of vector quantization and diffusion models to handle multichannel 3D data. However, the manuscript could benefit from a more detailed discussion of how BYOC differentiates itself from existing generative models in the domain of 3D biological data synthesis. There are several recent works on diffusion models for 3D data, and a clearer delineation of the unique contributions of BYOC would strengthen the manuscript.\n\n2. Evaluation Design: The evaluation is conducted on a single dataset of metastatic melanoma cells treated with three different drugs. While this provides a solid foundation, the validation could be strengthened by including additional datasets and cell types to demonstrate the generalizability of the method. Moreover, a comparison with more recent state-of-the-art methods would provide a clearer picture of the method's performance relative to the latest advancements.\n\n3. Comparisons: The quantitative evaluation includes several baseline models, but the inclusion of more recent diffusion-based models would provide a more comprehensive comparison. Additionally, while the qualitative evaluation shows promising results, a more rigorous statistical analysis (e.g., significance tests) would help substantiate the claims of superiority over existing methods.\n\n4. Reproducibility: The authors state that the code will be released, but the current manuscript lacks sufficient detail regarding the training protocols, preprocessing steps, and hyperparameters. Providing a more detailed description of these aspects is crucial for ensuring that the results can be independently reproduced.\n\n## Minor Comments\n1. Figures: Some figures are cluttered and could benefit from a more organized presentation. For instance, Figure 4 could be improved by showing fewer representative slices with zoomed-in regions.\n   \n2. Notation: The notation introduced in Section 2.1 is sometimes inconsistent and lacks sufficient explanation. Clarifying these points would enhance the readability of the manuscript.\n\n3. Acronyms: Several acronyms are used without definition, such as \"R=4\". Providing definitions for these acronyms would aid readers who are unfamiliar with the terminology.\n\n4. Typographical Issues: Minor typographical errors exist throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). Careful proofreading would eliminate these issues.\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in generating biologically realistic 3D cellular structures, which is crucial for accelerating pre-clinical drug development. The proposed BYOC framework represents a promising innovation by integrating vector quantization with diffusion models to handle multichannel 3D data. However, the evaluation is somewhat limited to a single dataset, and comparisons with more recent state-of-the-art methods are needed to fully establish the method's novelty and superiority. Additionally, while the reproducibility statement is positive, the manuscript lacks sufficient methodological detail to ensure independent reproduction. Overall, the manuscript has the potential to make a substantial contribution to the field, but it requires further refinement in terms of validation and reproducibility.\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include more recent state-of-the-art methods, strengthen the validation across different datasets and cell types, and provide a more detailed description of the training protocols and hyperparameters to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Build Your Own Cell* (BYOC), a multichannel 3D generative framework designed to synthesize biologically realistic cell morphologies. BYOC combines vector quantization with diffusion models to capture subtle structural variations under different drug treatments, with potential applications in large-scale in silico screening for pre‚Äëclinical studies. The paper is coherently written, clearly explains its motivation, and convincingly articulates the relevance of realistic 3D cell modeling for biomedical research.\n\n**Major Comments**  \n1. **Novelty and Positioning:** The integration of vector quantization and diffusion models for multichannel 3D data is a valuable contribution. However, the manuscript would benefit from a more explicit discussion contrasting BYOC with existing generative approaches for 3D biological data. Highlighting the technical and conceptual distinctions from recent 3D diffusion-based works would help clarify the unique aspects of this framework.  \n2. **Evaluation Design:** Current evaluation is conducted on a single dataset comprising metastatic melanoma cells treated with three drugs. To demonstrate broader utility, additional datasets and diverse cell types should be included. Incorporating comparisons with recent state‚Äëof‚Äëthe‚Äëart methods would provide stronger evidence for generalizability and competitive performance.  \n3. **Comparative Analysis:** The quantitative evaluation includes established baselines but omits newer diffusion or transformer‚Äëbased generative models. Expanding comparisons and applying statistical significance tests would strengthen the claims of improved performance and enhance the rigor of the evidence presented.  \n4. **Reproducibility:** While code release is promised, the current text omits key implementation details such as preprocessing, training procedures, and hyperparameters. Providing this information is essential for independent verification and replication of results.\n\n**Minor Comments**  \n- **Figures:** Several figures, notably Figure¬†4, appear visually dense; simplifying layouts or including zoomed‚Äëin regions could improve clarity.  \n- **Notation:** Some symbols in Section¬†2.1 are inconsistently defined; clearer explanations are needed.  \n- **Acronyms:** Undefined abbreviations (e.g., ‚ÄúR=4‚Äù) should be expanded on first use.  \n- **Typos:** Minor spelling issues (e.g., ‚Äúk‚Äëspacce,‚Äù ‚Äúundersampling maskes‚Äù) require correction.\n\n**Summary Paragraph**  \nThis work presents an innovative approach to 3D cellular modeling with clear potential impact on virtual cell simulations and drug discovery. Its conceptual design is sound, and results are promising; however, the evidence base remains narrow, relying on a limited dataset and incomplete comparisons with current methods. Improved experimental breadth, detailed methodological documentation, and enhanced clarity in presentation would significantly strengthen the manuscript‚Äôs contribution.\n\n**Decision Recommendation**  \n**Major Revision.** The authors should extend comparative evaluations, validate across additional datasets and cell types, and include comprehensive methodological details to ensure reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## BUILD YOUR OWN CELL: DIFFUSION MODELS FOR MULTICHANNEL 3D MICROSCOPY IMAGE GENERATION\n\n### Summary\n\nThis paper proposes BYOC, a two-stage generative framework for multichannel 3D fluorescence microscopy volumes of single cells. It learns per-channel VQGAN codebooks for the cell body and nucleus and then performs joint denoising diffusion on the unquantized 3D latents with a dual‚Äëchannel 3D U‚ÄëNet so that channels influence each other during synthesis. On a dataset of ‚àº7k melanoma cells under three drug treatments, BYOC qualitatively outperforms a 3D diffusion baseline and achieves better FID/MMD scores; ablations suggest per‚Äëchannel codebooks improve fidelity over a single shared codebook.\n\n### Strengths\n\n- Technical novelty and innovationIntroduces a ‚Äúlibrary of codebooks‚Äù with per‚Äëchannel vector quantization for multichannel 3D volumes and a joint diffusion process that conditions the reverse step for each channel on the other channel‚Äôs latent.A practical dual‚Äëchannel 3D U‚ÄëNet backbone with spatial/depthwise attention to capture inter‚Äë and intra‚Äëchannel dependencies in volumes.Sensible engineering choices (latent-space diffusion, freezing the autoencoder, normalization of latents) that make 3D multichannel synthesis feasible on moderate hardware.\n- Experimental rigor and validationBaseline comparisons include both adversarial and diffusion families (HA‚ÄëGAN, W‚ÄëGAN, Œ±‚ÄëGAN, MedicalDiffusion).Ablation on codebook designs (unimodal vs separate per channel; normalization strategies) and reporting across three drug conditions.Implementation details and hyperparameters are provided, which supports reproducibility.\n- Clarity of presentationThe high-level pipeline and motivation are clearly explained; diagrams effectively convey the two-stage setup and joint diffusion idea.The qualitative figures credibly show improved boundary sharpness and nucleus‚Äìcell spatial consistency over a strong 3D diffusion baseline.\n- Significance of contributionsAddresses an important gap: synthesizing coherent multichannel 3D cellular structures, a prerequisite for in silico phenotypic screening.Potential impact for pre-clinical experimentation, enabling data augmentation and hypothesis generation about morphology‚Äìtreatment relationships.\n\n- Introduces a ‚Äúlibrary of codebooks‚Äù with per‚Äëchannel vector quantization for multichannel 3D volumes and a joint diffusion process that conditions the reverse step for each channel on the other channel‚Äôs latent.\n- A practical dual‚Äëchannel 3D U‚ÄëNet backbone with spatial/depthwise attention to capture inter‚Äë and intra‚Äëchannel dependencies in volumes.\n- Sensible engineering choices (latent-space diffusion, freezing the autoencoder, normalization of latents) that make 3D multichannel synthesis feasible on moderate hardware.\n\n- Baseline comparisons include both adversarial and diffusion families (HA‚ÄëGAN, W‚ÄëGAN, Œ±‚ÄëGAN, MedicalDiffusion).\n- Ablation on codebook designs (unimodal vs separate per channel; normalization strategies) and reporting across three drug conditions.\n- Implementation details and hyperparameters are provided, which supports reproducibility.\n\n- The high-level pipeline and motivation are clearly explained; diagrams effectively convey the two-stage setup and joint diffusion idea.\n- The qualitative figures credibly show improved boundary sharpness and nucleus‚Äìcell spatial consistency over a strong 3D diffusion baseline.\n\n- Addresses an important gap: synthesizing coherent multichannel 3D cellular structures, a prerequisite for in silico phenotypic screening.\n- Potential impact for pre-clinical experimentation, enabling data augmentation and hypothesis generation about morphology‚Äìtreatment relationships.\n\n### Weaknesses\n\n- Technical limitations or concernsThe ‚Äúfirst 3D fluorescence cell generative model‚Äù claim is too strong; prior works have synthesized 3D fluorescence microscopy volumes or used generative pipelines for 3D cell imagery, albeit with different conditioning setups or modality focus.The diffusion objective is under-specified (noise vs x0 prediction, variance parameterization); Eq. (13) omits variance and training target details, and Eq. (12) has a likely typographical error in the forward process.Training separate models per drug undermines the claim of a unified multichannel generator and limits generalization/controllability across treatments.\n- Experimental gaps or methodological issuesFID/MMD are computed on channel-averaged volumes using Med3D features trained on MR/CT; this discards inter-channel structure and uses a domain-mismatched feature extractor, weakening the support for the central inter-channel consistency claim.No quantitative biology-aware validation (e.g., nucleus-inside-cell constraint violations, nucleus-to-cell volume ratio, sphericity/elongation, protrusion counts) or expert studies; this is critical for the stated downstream aims.Inconsistency between ‚Äú80/20 split‚Äù and ‚Äú5-fold cross-validation‚Äù in reported experiments; unclear what exact splits were used for metrics and model selection.Fairness of multichannel adaptations for baselines is not fully documented (architectural capacity, training stability, and whether baselines were also trained per drug).\n- Clarity or presentation issuesNotational errors in Sec. 3.1 (both latents fed with E(x_nuc); typos in dimensions and variable names), minor rendering artifacts, and missing definitions (e.g., the reverse variance schedule).Ablation section text is partially garbled; the definitions of ‚ÄúAbsolute,‚Äù ‚ÄúCell,‚Äù and ‚ÄúNucleus‚Äù normalization variants require clearer explanation.\n- Missing related work or comparisonsMissing discussion of recent scalable 3D diffusion approaches and memory-efficient designs (e.g., wavelet-domain diffusion for 3D volumes, cascaded/amortized latent 3D diffusion) that could be relevant baselines or complementary.Limited connection to microscopy-specific generative pipelines and multichannel quantization literature.\n\n- The ‚Äúfirst 3D fluorescence cell generative model‚Äù claim is too strong; prior works have synthesized 3D fluorescence microscopy volumes or used generative pipelines for 3D cell imagery, albeit with different conditioning setups or modality focus.\n- The diffusion objective is under-specified (noise vs x0 prediction, variance parameterization); Eq. (13) omits variance and training target details, and Eq. (12) has a likely typographical error in the forward process.\n- Training separate models per drug undermines the claim of a unified multichannel generator and limits generalization/controllability across treatments.\n\n- FID/MMD are computed on channel-averaged volumes using Med3D features trained on MR/CT; this discards inter-channel structure and uses a domain-mismatched feature extractor, weakening the support for the central inter-channel consistency claim.\n- No quantitative biology-aware validation (e.g., nucleus-inside-cell constraint violations, nucleus-to-cell volume ratio, sphericity/elongation, protrusion counts) or expert studies; this is critical for the stated downstream aims.\n- Inconsistency between ‚Äú80/20 split‚Äù and ‚Äú5-fold cross-validation‚Äù in reported experiments; unclear what exact splits were used for metrics and model selection.\n- Fairness of multichannel adaptations for baselines is not fully documented (architectural capacity, training stability, and whether baselines were also trained per drug).\n\n- Notational errors in Sec. 3.1 (both latents fed with E(x_nuc); typos in dimensions and variable names), minor rendering artifacts, and missing definitions (e.g., the reverse variance schedule).\n- Ablation section text is partially garbled; the definitions of ‚ÄúAbsolute,‚Äù ‚ÄúCell,‚Äù and ‚ÄúNucleus‚Äù normalization variants require clearer explanation.\n\n- Missing discussion of recent scalable 3D diffusion approaches and memory-efficient designs (e.g., wavelet-domain diffusion for 3D volumes, cascaded/amortized latent 3D diffusion) that could be relevant baselines or complementary.\n- Limited connection to microscopy-specific generative pipelines and multichannel quantization literature.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe two-stage design is sound: learning compact per‚Äëchannel latents with VQGAN and then modeling their joint distribution via diffusion is a principled way to handle multichannel volumes. Operating diffusion on unquantized latents preserves gradient flow and avoids discrete index modeling complexity.However, the diffusion objective is insufficiently specified: whether the network predicts Œµ, x0, or v‚Äëprediction; the choice of L1 vs L2; and how variance is handled in sampling. These details materially affect quality and speed.The normalization of latents by codebook statistics is reasonable but needs clarity: are these statistics computed on the training set only? Are they per‚Äëdrug or global? Could leakage occur?\n- Experimental evaluation assessmentMetrics: Using Med3D features and channel averaging to compute FID/MMD weakens conclusions about the main contribution (inter‚Äëchannel fidelity). Consider:Computing FID on concatenated channels or learning a multi-channel 3D feature extractor on the microscopy domain (self-supervised or supervised on this dataset).Adding biological/structural metrics: nucleus-inside-cell violation rate; overlap ratios; centroid distances; nucleus-to-cell volume ratio distribution; sphericity/elongation; filamentous protrusion statistics; treatment-specific morphometrics and distribution shift tests.Reporting MS‚ÄëSSIM or LPIPS in 3D to assess diversity vs overfitting.Protocol: Clarify whether baselines were trained per drug, and ensure consistent capacity and training budget. Resolve the discrepancy between 80/20 and 5-fold CV; if 5-fold was used, describe model selection and the number of real samples used for FID per fold.Generalization: Training per drug restricts controllability. A stronger claim would train a single conditional model (drug label as condition) and show interpolation/extrapolation between treatments.Efficiency: Provide sampling speed and memory profiles for all baselines at 64^3, and discuss feasibility of scaling to higher resolutions.\n- Comparison with related work (using the summaries provided)3D medical diffusion: Methods like wavelet-domain diffusion (memory-efficient, 3D volumes) and cascaded amortized latent diffusion for large volumes demonstrate strong 3D fidelity and efficiency; situating BYOC against these approaches would clarify trade-offs (e.g., your joint multichannel modeling vs their scaling).Conditional 2D/3D medical generative work (e.g., multitask/multi‚Äëstain diffusion) shows that jointly modeling related channels or tasks improves fidelity; BYOC aligns with this insight but should adopt stronger quantitative evidence for inter‚Äëchannel gains.3D microscopy synthesis: Prior pipelines generate 3D fluorescence data from masks or via cascaded multi‚Äëview reconstruction; while BYOC directly synthesizes multichannel 3D volumes, referencing these would contextualize novelty and motivate future conditional controls (e.g., mask‚Äëguided generation).Multichannel quantization: Prior ‚Äúmultichannel quantization‚Äù and large codebook strategies highlight alternatives to improve latent richness; discussing why per‚Äëchannel codebooks were preferred (and how codebook size/utilization affects BYOC) would strengthen the positioning.\n- Discussion of broader impact and significanceThe target application‚Äîrapid in silico exploration of treatment-induced morphology‚Äîis compelling. To move toward impact:Demonstrate that downstream analytics trained on synthetic data benefit (e.g., segmentation, phenotype classification).Quantify whether generated morphometric distributions match real distributions per treatment; test sensitivity to drug labels if a conditional model is built.Address safety: report any anatomically implausible geometries and mitigation (constraints, rejection sampling, or loss terms).\n\n- The two-stage design is sound: learning compact per‚Äëchannel latents with VQGAN and then modeling their joint distribution via diffusion is a principled way to handle multichannel volumes. Operating diffusion on unquantized latents preserves gradient flow and avoids discrete index modeling complexity.\n- However, the diffusion objective is insufficiently specified: whether the network predicts Œµ, x0, or v‚Äëprediction; the choice of L1 vs L2; and how variance is handled in sampling. These details materially affect quality and speed.\n- The normalization of latents by codebook statistics is reasonable but needs clarity: are these statistics computed on the training set only? Are they per‚Äëdrug or global? Could leakage occur?\n\n- Metrics: Using Med3D features and channel averaging to compute FID/MMD weakens conclusions about the main contribution (inter‚Äëchannel fidelity). Consider:Computing FID on concatenated channels or learning a multi-channel 3D feature extractor on the microscopy domain (self-supervised or supervised on this dataset).Adding biological/structural metrics: nucleus-inside-cell violation rate; overlap ratios; centroid distances; nucleus-to-cell volume ratio distribution; sphericity/elongation; filamentous protrusion statistics; treatment-specific morphometrics and distribution shift tests.Reporting MS‚ÄëSSIM or LPIPS in 3D to assess diversity vs overfitting.\n- Protocol: Clarify whether baselines were trained per drug, and ensure consistent capacity and training budget. Resolve the discrepancy between 80/20 and 5-fold CV; if 5-fold was used, describe model selection and the number of real samples used for FID per fold.\n- Generalization: Training per drug restricts controllability. A stronger claim would train a single conditional model (drug label as condition) and show interpolation/extrapolation between treatments.\n- Efficiency: Provide sampling speed and memory profiles for all baselines at 64^3, and discuss feasibility of scaling to higher resolutions.\n\n- Computing FID on concatenated channels or learning a multi-channel 3D feature extractor on the microscopy domain (self-supervised or supervised on this dataset).\n- Adding biological/structural metrics: nucleus-inside-cell violation rate; overlap ratios; centroid distances; nucleus-to-cell volume ratio distribution; sphericity/elongation; filamentous protrusion statistics; treatment-specific morphometrics and distribution shift tests.\n- Reporting MS‚ÄëSSIM or LPIPS in 3D to assess diversity vs overfitting.\n\n- 3D medical diffusion: Methods like wavelet-domain diffusion (memory-efficient, 3D volumes) and cascaded amortized latent diffusion for large volumes demonstrate strong 3D fidelity and efficiency; situating BYOC against these approaches would clarify trade-offs (e.g., your joint multichannel modeling vs their scaling).\n- Conditional 2D/3D medical generative work (e.g., multitask/multi‚Äëstain diffusion) shows that jointly modeling related channels or tasks improves fidelity; BYOC aligns with this insight but should adopt stronger quantitative evidence for inter‚Äëchannel gains.\n- 3D microscopy synthesis: Prior pipelines generate 3D fluorescence data from masks or via cascaded multi‚Äëview reconstruction; while BYOC directly synthesizes multichannel 3D volumes, referencing these would contextualize novelty and motivate future conditional controls (e.g., mask‚Äëguided generation).\n- Multichannel quantization: Prior ‚Äúmultichannel quantization‚Äù and large codebook strategies highlight alternatives to improve latent richness; discussing why per‚Äëchannel codebooks were preferred (and how codebook size/utilization affects BYOC) would strengthen the positioning.\n\n- The target application‚Äîrapid in silico exploration of treatment-induced morphology‚Äîis compelling. To move toward impact:Demonstrate that downstream analytics trained on synthetic data benefit (e.g., segmentation, phenotype classification).Quantify whether generated morphometric distributions match real distributions per treatment; test sensitivity to drug labels if a conditional model is built.Address safety: report any anatomically implausible geometries and mitigation (constraints, rejection sampling, or loss terms).\n\n- Demonstrate that downstream analytics trained on synthetic data benefit (e.g., segmentation, phenotype classification).\n- Quantify whether generated morphometric distributions match real distributions per treatment; test sensitivity to drug labels if a conditional model is built.\n- Address safety: report any anatomically implausible geometries and mitigation (constraints, rejection sampling, or loss terms).\n\n### Questions for Authors\n\n- Did you train one model per drug or a single conditional model with the drug label? If per-drug, why was conditional training not attempted, and how do you envision scaling to more treatments?\n- How exactly is the diffusion loss formulated (Œµ‚Äëprediction vs x0‚Äëprediction vs v‚Äëprediction) and with what loss (L1/L2)? How is the reverse variance handled (fixed vs learned)?\n- Are the codebook normalization statistics (min/max) computed on training data only? Are they per-channel and per-drug, or global?\n- For FID/MMD, what is the exact real set used: validation or test split per fold? How many real samples were used per fold, and were 5,000 generated samples matched by an equal number of real samples?\n- Why average the two channels before computing FID/MMD? Have you tried computing FID on concatenated channels or using a microscopy-domain 3D feature extractor?\n- Can you report quantitative biology-aware metrics (nucleus-in-cell violation rate, volume ratios, shape descriptors) per drug, and whether BYOC narrows the gap to real distributions vs baselines?\n- How were baselines adapted to be multichannel? Did they receive exactly two channels at input/output with comparable capacity and training time, and were they also trained per-drug?\n- What is the codebook size per channel actually used (Appendix says 1024); what is utilization, and how sensitive are results to codebook size and latent dimensionality?\n- In the ablation, what precisely do ‚ÄúAbsolute,‚Äù ‚ÄúCell,‚Äù and ‚ÄúNucleus‚Äù normalization variants mean operationally? In some cases ‚ÄúNucleus‚Äù outperforms BYOC‚Äîcan you explain why and whether this reflects dataset bias?\n- Can you quantify sampling speed and memory usage across methods, and discuss scaling to higher resolutions (e.g., 128^3 or beyond)?\n- Do you enforce any constraints to prevent the nucleus from leaving the cell boundary? If not, how often does it occur, and can BYOC‚Äôs joint diffusion be augmented with geometric constraints?\n- Will the data preprocessing scripts and trained models be released with the anonymized code? Are there plans to evaluate on other cell lines or multiorganelle channels?\n\n### Overall Assessment\n\nBYOC is a well-motivated and practically designed framework for multichannel 3D cellular synthesis that combines per-channel quantization with joint latent diffusion. The qualitative improvements are convincing, and the ablation suggests the per‚Äëchannel codebook idea is useful. However, the evaluation protocol does not yet support the strongest claims: channel-averaged FID on MR/CT features cannot verify inter-channel consistency, the model appears to be trained per drug (limiting generality and controllability), and biologically meaningful quantitative validation is missing. The methodological description has a few notational and specification gaps that should be clarified. I see solid potential and relevance for the bioimaging community; with stronger, biology-aware metrics, a conditional model across treatments, clearer baselines, and tighter methodological details, this work could be competitive at a top-tier venue. As it stands, it is a promising contribution that would benefit from substantial strengthening of the experimental evidence and positioning relative to recent 3D diffusion literature.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **Build Your Own Cell (BYOC)**, a two-stage generative framework for synthesizing multichannel 3D fluorescence microscopy images of single cells. The method learns per-channel VQGAN codebooks for cellular components and applies joint latent-space diffusion using a dual-channel 3D U‚ÄëNet to model inter‚Äëchannel relationships. Experiments on ‚àº7,000 melanoma cell volumes under multiple drug treatments indicate qualitative and quantitative advantages over existing 3D generative baselines. The paper is clearly written, with thorough motivation, lucid figures, and a well-structured technical presentation.\n\n---\n\n**Major Comments**  \n1. **Technical specification gaps** ‚Äì The diffusion objective is underdefined: the paper does not clarify whether Œµ‚Äë, x‚ÇÄ‚Äë, or v‚Äëprediction is used, nor how the loss function and variance parameterization are handled. Equations (12)‚Äì(13) contain typographical inaccuracies and omit essential details.  \n2. **Generality and experimental design** ‚Äì Training a separate model per drug condition undermines the claim of a unified generator and limits controllability. A single conditional model with drug labels could better demonstrate coherence across treatments.  \n3. **Evaluation metrics** ‚Äì FID/MMD are computed on channel‚Äëaveraged volumes using Med3D features trained on MR/CT data, which do not capture fluorescence‚Äëspecific or inter‚Äëchannel structure. This weakens evidence for improvements in cross‚Äëchannel consistency.  \n4. **Lack of biological validation** ‚Äì The study omits biologically meaningful quantitative metrics (e.g., nucleus‚Äëinside‚Äëcell integrity, volume ratios, shape descriptors) and expert evaluation, despite their importance for the stated downstream applications.  \n5. **Reproducibility and clarity** ‚Äì Inconsistencies between ‚Äú80/20 split‚Äù and ‚Äú5‚Äëfold cross‚Äëvalidation,‚Äù unclear baseline configurations, and minor notational errors in Section¬†3 hinder reproducibility. Latent normalization and codebook statistics require clarification regarding computation scope (per‚Äëdrug or global).  \n6. **Related work** ‚Äì The discussion misses connections to recent scalable and memory‚Äëefficient 3D diffusion methods (e.g., wavelet‚Äëdomain or cascaded latent models) and to microscopy‚Äëspecific generative and multichannel quantization literature.\n\n---\n\n**Minor Comments**  \n- Section¬†3.1 contains symbol and dimensional typos, duplicated latent variable references, and missing definitions (e.g., variance schedule).  \n- Figures are informative, but minor rendering artifacts are visible.  \n- The ablation table includes partly garbled terms (‚ÄúAbsolute,‚Äù ‚ÄúCell,‚Äù ‚ÄúNucleus‚Äù normalization) that require clearer definitions.  \n- Please report sampling efficiency, memory usage, and codebook utilization.  \n- Clarify release plans for data preprocessing scripts and model weights.\n\n---\n\n**Summary Paragraph**  \nOverall, BYOC offers a technically sound and innovative approach to multichannel 3D cellular image synthesis, introducing a useful per‚Äëchannel quantization and joint diffusion design. Its strengths lie in conceptual novelty, clear engineering, and potential significance for microscope data augmentation. However, the work‚Äôs central claims are weakened by incomplete diffusion specification, limited generalization across treatments, and evaluation metrics that do not directly assess inter‚Äëchannel or biological fidelity. Strengthening these aspects and expanding comparison to recent 3D diffusion research would make this contribution more compelling and verifiable.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The study is promising and well‚Äëmotivated but requires substantial clarification of the methodology, improved biologically relevant evaluation, and expanded comparative analysis before it can be considered for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for IEEE Transactions on Medical Imaging\n\n## 1. Opening Paragraph\n\nThe paper addresses the critical challenge of generating biologically realistic 3D cellular structures from multichannel fluorescence microscopy data, where maintaining phenotypic consistency between cell and nucleus channels while capturing drug-induced morphological changes remains difficult. The authors propose BYOC (Build Your Own Cell), a novel framework that combines vector quantization with diffusion modeling to synthesize 3D cell structures. The approach features a \"library of codebooks\" to independently process cell and nucleus channels while learning their interdependencies, followed by a multichannel diffusion model that preserves spatial relationships. Evaluated on 7,083 melanoma cells treated with three different drugs (Nocodazole, Binimetinib, and Blebbistatin), BYOC demonstrates superior performance over existing methods with significantly lower FID scores (1.91 vs. 2.12 for Nocodazole) and MMD metrics (8.34 vs. 9.55). The qualitative results show BYOC generates samples with better-preserved cell and nucleus boundaries and more realistic morphological characteristics, enabling high-throughput in silico screening of drug effects on cell morphology.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The \"library of codebooks\" approach is innovative and effectively addresses the critical challenge of maintaining inter-channel consistency while processing biologically distinct but related features\n- The multichannel diffusion framework successfully preserves spatial relationships between cell and nucleus, which is essential for biological validity\n- The comprehensive evaluation across three different drug treatments with both quantitative metrics (FID, MMD) and qualitative visual assessment provides strong evidence of superiority over existing methods\n- The ablation study clearly demonstrates the importance of channel-specific codebooks compared to unimodal representations\n\n**Limitations:**\n- The framework is currently limited to only two channels (cell and nucleus), which restricts applicability to more complex multichannel datasets with additional organelles or cellular compartments\n- The evaluation is restricted to melanoma cells treated with just three drugs, limiting generalizability to other cell types, conditions, or more complex drug combinations\n- The paper lacks discussion about potential biases in the synthetic data generation process that might affect downstream biological analyses\n- Computational requirements (4 GPUs with 24GB RAM) are substantial and could limit accessibility for researchers with limited resources\n\n### Minor Comments\n\n**Strengths:**\n- The visualizations (particularly Figures 4, 5, and 6) are clear and effectively demonstrate the method's capabilities and advantages\n- The framework is well-motivated with appropriate background on the biological significance of cell morphology in drug response\n- The training details and hyperparameters are thoroughly documented in the appendix, enhancing reproducibility\n\n**Limitations:**\n- Some high-resolution sample figures could benefit from more detailed annotations to highlight specific morphological features\n- The clinical relevance of the specific drug treatments could be better explained to strengthen the connection to real-world applications\n- The comparison with MedicalDiffusion lacks metrics on inference speed despite noting differences in generation time (~7s vs ~5s)\n- The limitation section could be expanded with more specific implementation strategies for future multi-channel extensions\n\n## 3. Summary Evaluation\n\n**Significance:** The work addresses a significant challenge in medical imaging and drug discovery by enabling high-throughput in silico screening of drug effects on 3D cellular morphology. The ability to generate biologically realistic cell structures with preserved inter-channel relationships has strong potential to reduce reliance on labor-intensive experimental studies, accelerating preclinical drug development pipelines. The focus on phenotypic consistency between cell and nucleus channels is particularly valuable for accurate biological interpretation.\n\n**Innovation:** The paper presents a novel approach through its \"library of codebooks\" concept that processes biological channels independently while learning their interdependencies. The extension of diffusion models to multichannel 3D cellular synthesis with channel-specific processing represents a meaningful advancement beyond existing 3D generation methods. The combination of vector quantization with diffusion modeling for this specific application is well-conceived and effectively executed.\n\n**Evaluation:** The evaluation is comprehensive, including both quantitative metrics (FID and MMD) across multiple drug treatments and thorough qualitative visual assessment. The comparison with multiple baselines (GAN-based methods and MedicalDiffusion) provides strong evidence of the method's superiority. The ablation study effectively demonstrates the importance of channel-specific codebooks. However, the evaluation could be strengthened with more diverse cell types and additional metrics related to biological validity beyond structural similarity.\n\n**Reproducibility:** The paper provides excellent documentation of implementation details, including thorough hyperparameters in the appendix, training procedures, and evaluation protocols. The authors mention their project repository (currently anonymous) and provide detailed descriptions of the experimental setup. The use of standard metrics and publicly available architectures (Med3D for feature extraction) enhances reproducibility. The comprehensive appendix with training parameters is particularly helpful for replication.\n\n## 4. Decision Recommendation\n\nMinor Revision\n\nThe paper presents a novel and well-executed approach to 3D cellular image generation with clear improvements over existing methods. The technical contribution is significant, the evaluation is thorough, and the work has potential impact on both medical imaging and drug discovery communities. While there are some limitations regarding generalizability to more complex datasets, these are appropriately acknowledged by the authors and do not substantially undermine the value of the current contribution.\n\nI recommend minor revision to:\n1) Expand the discussion about potential biases in synthetic data that might affect downstream analyses\n2) Provide more context on the clinical relevance of the specific drug treatments used\n3) Include inference speed metrics in the comparison with MedicalDiffusion\n4) Add more specific implementation strategies for future multi-channel extensions\n\nThese revisions would strengthen the paper without requiring additional experiments, and the core contribution is solid and well-prepared for publication in TMI after these modifications.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *Build Your Own Cell (BYOC)*, a framework for synthesizing biologically realistic 3D cellular structures from multichannel fluorescence microscopy data. The central goal is to preserve phenotypic consistency between cell and nuclear channels while capturing drug-induced morphological variation. BYOC integrates vector quantization with diffusion modeling through a ‚Äúlibrary of codebooks‚Äù that processes cell and nucleus channels independently yet learns their interdependencies. The model is evaluated on 7,083 melanoma cells treated with three drugs (Nocodazole, Binimetinib, and Blebbistatin), showing improved quantitative performance (lower FID and MMD scores) and enhanced visual realism compared with existing methods. Overall, the manuscript is clearly written, methodologically sound, and presents a promising direction for in silico cellular modeling.\n\n---\n\n**Major Comments**  \n**Strengths:**  \n1. The ‚Äúlibrary of codebooks‚Äù concept is innovative and effectively maintains inter-channel consistency.  \n2. The multichannel diffusion framework preserves spatial relationships critical for biological validity.  \n3. The evaluation across three drug treatments includes both quantitative (FID, MMD) and qualitative analyses, supporting claims of superiority over baselines.  \n4. The ablation study convincingly demonstrates the benefit of channel-specific codebooks.  \n\n**Limitations:**  \n1. The model currently supports only two channels, limiting applicability to datasets with additional organelles or compartments.  \n2. Experiments are restricted to melanoma cells and three drugs, constraining generalizability.  \n3. Discussion on potential biases in synthetic data generation and their impact on downstream analyses is missing.  \n4. High computational requirements (4 GPUs, 24GB RAM) may hinder accessibility.  \n\n---\n\n**Minor Comments**  \n**Strengths:**  \n- Figures 4‚Äì6 effectively visualize the model‚Äôs strengths.  \n- The paper provides adequate biological motivation and detailed training documentation, enhancing reproducibility.  \n\n**Limitations:**  \n- Some sample figures would benefit from clearer annotations of morphological features.  \n- Clinical relevance of the chosen drugs could be better contextualized.  \n- The comparison with MedicalDiffusion omits inference speed metrics despite noting runtime differences.  \n- The limitations section could include concrete strategies for expanding to multi-channel data.  \n\n---\n\n**Summary Paragraph**  \nThe study tackles an important problem in computational cell imaging with a technically novel approach that combines vector quantization and diffusion modeling. BYOC advances state of the art in preserving inter-channel biological consistency and demonstrates solid quantitative and qualitative performance. The evaluation is comprehensive and reproducible, but broader assessments across cell types, inclusion of inference efficiency, and more explicit discussion of bias and clinical context would strengthen the paper. Overall, the contribution is significant and well executed.\n\n---\n\n**Decision Recommendation**  \n**Minor Revision** ‚Äì The manuscript is strong and nearly publishable. Revisions should (1) elaborate on potential data biases, (2) clarify clinical relevance of the drug treatments, (3) report inference speed comparisons, and (4) suggest concrete strategies for multi-channel extensions. These adjustments would refine the presentation without requiring major additional experiments.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the generation of realistic synthetic 3‚ÄëD fluorescence microscopy volumes that reflect drug‚Äëinduced morphological changes. Using a collection of 7‚ÄØ083 metastatic melanoma cells imaged by light‚Äësheet microscopy (two fluorescence channels: cytoplasm and nucleus) and annotated with three pharmacological conditions (Nocodazole, Binimetinib, Blebbistatin), the authors build a pipeline that first learns separate vector‚Äëquantised (VQ) codebooks for each channel and then feeds the resulting latent tensors into a dual‚Äëchannel diffusion model (DualChannelUNet) that denoises them jointly. Performance is assessed with Fr√©chet Inception Distance (FID) and Maximum Mean Discrepancy (MMD), where the proposed method attains lower scores than several baseline generators; visual inspection is reported to show faithful reconstruction of inter‚Äëchannel structure. The primary claim is the introduction of the first multichannel 3‚ÄëD fluorescence cell generator that couples independent channel codebooks with a joint diffusion process, purportedly preserving biologically relevant relationships.\n\n---\n\n## General feedback  \n\n- **Relevance.** Producing multichannel 3‚ÄëD microscopy data is indeed a pressing need for virtual screening and augmentation of scarce biological datasets. Emphasising preservation of the nucleus‚Äëto‚Äëcell geometry is a worthwhile objective.  \n- **Originality.** Merging per‚Äëchannel VQ codebooks with a shared diffusion backbone (DualChannelUNet) represents a novel twist on recent multimodal diffusion frameworks such as MM‚ÄëDiffusion. Nevertheless, the manuscript does not sufficiently differentiate its approach from existing VQ‚ÄëGAN‚Äëdiffusion pipelines, and key architectural details‚Äîe.g., where attention modules are placed, depth of the UNet, and how the two channels are fused‚Äîare barely described.  \n- **Empirical validation.** The reported reductions in FID and MMD are encouraging, yet the evaluation is confined to these two aggregate metrics. No statistical testing (e.g., confidence intervals, hypothesis tests) is presented, and the study lacks measures that specifically target inter‚Äëchannel consistency (for instance, the ratio of nucleus volume to whole‚Äëcell volume). Moreover, the impact of the synthetic data on downstream tasks (e.g., training a classifier for drug response) is absent.  \n- **Reproducibility.** While hyper‚Äëparameters are tabulated (Tables‚ÄØ2‚Äë3) and a anonymised repository is cited, the underlying dataset is undisclosed, preprocessing steps (anisotropy correction, intensity scaling) are insufficiently explained, and the adaptation of baseline methods to a two‚Äëchannel setting is vaguely mentioned. Training curves, random seeds, and details of computational resources are omitted, which hampers independent verification.\n\n---\n\n## Specific comments / critiques  \n\n1. **Voxel geometry.** Section‚ÄØ4.1 does not report the voxel size, physical scaling, or any anisotropy present in the volumetric data, making it impossible to assess the realism of the generated samples.  \n2. **Dataset scope.** The work is limited to a single cell line (melanoma) and three drug perturbations. Generalisation to other cell types, tissues, or broader perturbation libraries remains untested, and the data are not made publicly available.  \n3. **Baseline adaptation.** The claim that baseline generators were ‚Äúadjusted to process multichannel inputs‚Äù lacks concrete description of the architectural changes, loss modifications, or hyper‚Äëparameter retuning, raising doubts about the fairness of the comparative results.  \n4. **Feature embedding choice.** Quantitative metrics rely on embeddings from Med3D networks pre‚Äëtrained on unrelated medical imaging modalities. The manuscript provides no evidence that these embeddings capture the relevant cellular morphology or inter‚Äëchannel relationships needed for a fair assessment.  \n5. **Statistical reporting.** Table‚ÄØ1 lists mean FID/MMD values obtained through five‚Äëfold cross‚Äëvalidation but omits confidence intervals or any statistical significance testing; the observed differences could be due to random variation.  \n6. **Ablation scope.** Figure‚ÄØ6 examines only variations in codebook configuration. Other critical design choices‚Äîsuch as the number of diffusion timesteps, latent dimensionality, or inclusion of attention layers‚Äîare not explored, leaving the contribution of each component ambiguous.  \n7. **Inter‚Äëchannel consistency analysis.** Beyond qualitative visual examples, there is no quantitative examination of whether synthetic nuclei occupy appropriate sub‚Äëvolumes within the generated cells (e.g., correlation of nucleus‚Äëto‚Äëcell volume ratios).  \n8. **Computational demand.** No information is given on training time, inference latency, or memory consumption for the high‚Äëresolution 3‚ÄëD diffusion pipeline, making it difficult to judge scalability to larger datasets or higher resolutions.  \n9. **Code availability.** The provided repository link is a placeholder (https://anonymous.4open.science/r/XXX) and the associated code/license are missing, preventing replication of the experiments.  \n10. **Presentation issues.** The manuscript contains numerous typographical and formatting errors (misplaced symbols, missing section headings) that obstruct readability and may conceal important methodological details.\n\n---\n\n## A suggested decision  \n\n**Major Revision**  \n\nThe study introduces a promising direction for multichannel 3‚ÄëD microscopy synthesis, but substantial concerns remain regarding methodological transparency, evaluation rigor, and reproducibility. Addressing the points above‚Äîparticularly by providing detailed architectural descriptions, robust statistical analyses, quantitative inter‚Äëchannel consistency measures, and a publicly accessible dataset/codebase‚Äîwill be essential before the manuscript can be considered for publication.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript addresses the synthesis of realistic 3‚ÄëD fluorescence microscopy volumes that reproduce drug‚Äëinduced morphological alterations in metastatic melanoma cells. It employs a dataset of more than 7‚ÄØ000 light‚Äësheet microscopy volumes across two fluorescence channels (cytoplasm and nucleus) and three pharmacological conditions. The proposed pipeline learns separate vector‚Äëquantised codebooks per channel, followed by a dual‚Äëchannel diffusion model (DualChannelUNet) that jointly denoises the latent representations. Performance is evaluated with Fr√©chet Inception Distance (FID) and Maximum Mean Discrepancy (MMD), showing improvements over baseline generators. The submission claims to be the first multichannel 3‚ÄëD fluorescence generator coupling independent channel encodings within a unified diffusion process. The paper is relevant and potentially useful but requires major improvements in methodological transparency, validation rigor, and reproducibility.\n\n---\n\n**Major Comments**  \n1. **Novelty and architectural clarity:** The combination of separate VQ codebooks and a joint diffusion backbone is creative but insufficiently distinguished from prior VQ‚ÄëGAN‚Äëdiffusion approaches. Key architectural details‚Äîattention placement, UNet depth, and channel fusion strategy‚Äîare not described.  \n2. **Experimental rigor:** Evaluation relies solely on global FID and MMD metrics without statistical tests, confidence intervals, or significance analysis. There are no quantitative measures of inter‚Äëchannel consistency, such as nucleus‚Äëto‚Äëcell volume ratios.  \n3. **Validation scope:** The study is limited to a single melanoma line under three drugs, with no examination of generalization to other cell types or perturbations.  \n4. **Baseline fairness:** Adjustments made to baseline models for multichannel input are insufficiently documented, raising concerns over comparison validity.  \n5. **Feature embeddings:** FID/MMD are computed using Med3D embeddings trained on unrelated modalities; their suitability for cellular morphology assessment is unsubstantiated.  \n6. **Ablation completeness:** Only codebook variants were tested; other critical parameters (diffusion steps, latent size, attention) were not explored.  \n7. **Reproducibility:** Dataset access, preprocessing procedures, training conditions, and computational specifications are incompletely reported. The anonymized code link is non‚Äëfunctional.  \n8. **Computational efficiency:** Training time, inference speed, and scalability are not discussed.  \n\n---\n\n**Minor Comments**  \n- Section‚ÄØ4.1 omits voxel size and anisotropy information.  \n- Multiple typographical and formatting issues hinder readability.  \n- Tables lack confidence intervals and statistical annotations.  \n\n---\n\n**Summary Paragraph**  \nThe work presents a potentially impactful technique for generating multichannel 3‚ÄëD cellular microscopy data, with promising qualitative and quantitative indications. However, critical deficiencies persist in architectural detail, evaluation depth, baseline transparency, and reproducibility. These issues limit confidence in the results and the ability to assess the true contribution. Substantial methodological clarification and expanded quantitative validation are required to realize the paper‚Äôs potential significance.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The concept is promising, but major methodological and documentation improvements are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Chris Bakal",
      "Chris Dunsby",
      "Lucas G Dent",
      "Matt De Vries",
      "Nathan Curry",
      "Olga Fourkioti",
      "Reed Naidoo"
    ],
    "url": "pdfs/iclr.cc-2025-conference_dd6f5c6509a71b82b803fd05cb68835d3440c1b0.pdf",
    "remote_url": "https://openreview.net/pdf/dd6f5c6509a71b82b803fd05cb68835d3440c1b0.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
    "status": "not_started",
    "evaluators": [
      "Luping",
      "Bernhard"
    ],
    "primary_area": [
      "foundation or frontier models, including LLMs"
    ],
    "keywords": [
      "LVLM",
      "Eye Gaze",
      "Video",
      "Medicine",
      "Medical Image",
      "Chest X-ray",
      "Chest X-ray Report Generation"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists‚Äô eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. The video, featuring a red gaze point overlaid on CXR images, emphasizes regions of focused attention during interpretation. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with a video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 25.4% on Impression generation task and on average 7.9% for all tasks using scaled evaluation metrics. Our approach enhanced open-domain LVLM models, when combined with exemplar reports for in-context learning, outperform medical models as well as those specifically trained for CXR report generation on the benchmark dataset. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks, pointing out a new effective approach of utilising LVLMs in healthcare and beyond.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThis article introduces a novel prompting method called RadEyeVideo, which presents radiologists‚Äô eye-tracking data as a video sequence to capture the temporal and spatial order of their gaze (i.e., scan paths). The authors evaluated the effectiveness of this approach in chest X-ray report generation and disease diagnosis tasks using large vision-language models (LVLMs) with video input capabilities. Results show that incorporating eye-gaze video prompts improved model performance by 25.4% on the Impression generation task, with an average performance increase of 7.9% across all tasks.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The RadEyeVideo method converts radiologists‚Äô eye-tracking data into video sequences for use in chest X-ray report generation and diagnostic tasks, significantly enhancing the performance of general large vision-language models (LVLMs). Experimental results indicate a 25.4% improvement in the Impression generation task and an average improvement of 7.9% across all tasks, outperforming models specifically designed for medical applications.\n2. Comprehensive Evaluation of Eye-Tracking Prompting Methods: This study presents the first thorough evaluation of various eye-tracking integration techniques, including static heatmaps, eye-tracking text prompts, and dynamic video prompts. The results indicate that RadEyeVideo outperforms the other methods in terms of diagnostic accuracy and clinical relevance, establishing a new standard for eye-tracking data prompts.\n\n### Weaknesses\n\n- Methodological Clarity: a) The paper lacks sufficient detail about the video prompt integration process, particularly how the video information is encoded and processed by the LVLMs. The technical implementation of combining video sequences with textual prompts could be more thoroughly explained. b) There's limited discussion of potential alternatives to full video sequence processing that might achieve similar results with lower computational costs.\n\n- Data Collection and Generalizability Limitations: a) The methodology requires synchronized eye-tracking data collection during radiologist readings, which is resource-intensive and difficult to scale. b) The approach may not generalize well to scenarios where real-time eye-tracking data is unavailable or impractical to collect. c) The current evaluation is limited to a relatively small dataset (2,298 CXR images), raising questions about broader applicability.\n\n- Computational Efficiency Concerns: The direct use of complete eye-tracking video sequences as prompts likely increases computational overhead significantly. While the authors mention sampling k frames (typically 16) from the total sequence, there's limited analysis of the computational trade-offs or optimal sampling strategies. The method may be computationally prohibitive for real-time clinical applications.\n\n### Questions\n\nPlease address the concern raised in weakness part.\nMoreover, there are some questions may help you to address the weakness.\n1) What is the computational overhead of processing video prompts compared to traditional image-only or text-only prompts?\n2) Have you explored more efficient alternatives to using complete video sequences, such as key frame selection or compressed representations? 3) How do you envision this approach being implemented in real-world clinical settings where real-time eye-tracking data may not be available? 4) Have you considered alternative methods for generating synthetic eye-tracking data that could make the approach more broadly applicable? 5)\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RadEyeVideo*, a novel prompting approach that leverages radiologists‚Äô eye-tracking data represented as video sequences to convey temporal and spatial attention patterns (‚Äúscan paths‚Äù). The method is applied to chest X-ray report generation and diagnostic classification tasks using large vision‚Äìlanguage models (LVLMs) with video input capabilities. Experiments demonstrate notable performance gains‚Äî25.4% improvement on the Impression generation task and an average increase of 7.9% across all tasks. The paper is clearly written and presents a compelling exploration of integrating expert gaze data into multimodal learning.  \n\n**Major Comments**  \n1. **Methodological Clarity** ‚Äì The technical description of how video prompts are encoded and integrated within LVLMs lacks sufficient detail. Specifically, the mechanisms combining video sequences with textual prompts should be specified more clearly. Additionally, alternative, lower-cost representations approximating full sequence processing are not discussed.  \n2. **Data Collection and Generalizability** ‚Äì The framework relies on synchronized eye-tracking during radiologist readings, making data acquisition resource-intensive and potentially limiting scalability. Its applicability to datasets or clinical environments lacking real-time gaze recordings remains uncertain. Furthermore, the evaluation relies on a relatively small sample (2,298 chest X-ray images), which restricts evidence for generalization.  \n3. **Computational Efficiency** ‚Äì Employing full eye-tracking videos likely imposes significant computational demands. Although the paper mentions sampling a limited number of frames (e.g., 16), it does not analyze trade-offs or optimal sampling strategies. This limitation raises concerns about feasibility for real-time clinical use.  \n\n**Minor Comments**  \n- Clarify details of video encoding and prompt-generation steps for reproducibility.  \n- Discuss possible substitutes or compressed representations that reduce computational load.  \n- Provide more explanation of how the proposed approach could function where eye-tracking data are unavailable.  \n\n**Summary Paragraph**  \nOverall, the study presents an original concept that effectively demonstrates the potential of dynamic eye-tracking prompts for LVLM-based radiology tasks. The main strengths are its innovative integration of gaze information and promising experimental results. However, methodological transparency, computational feasibility, and generalizability remain insufficiently addressed, limiting practical adoption and reproducibility.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The contribution is promising but requires substantial clarification and discussion of efficiency, scalability, and methodological integration before it can be accepted.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nTo enhance the reliability of large vision-language models (LVLMs) in real clinical environments, this study proposes a novel video prompting method called RadEyeVideo, which integrates radiologists' eye-tracking data as video sequences, capturing the spatial and temporal dynamics of gaze. Red fixation points are superimposed on CXR images to highlight the areas that doctors pay attention to, so as to dynamically present the doctor's eye movement path. Improve the ability of radiologists or AI models to diagnose chest diseases by providing rich contextual information.\n\n### Soundness: 2\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n1. The proposed method, RadEyeVideo, combines video, text, and eye movement data to realize the fusion of multi-modal information and improve the accuracy and efficiency of diagnosis.\n2. The authors conducted a thorough assessment of various eye-tracking integration techniques, providing strong empirical support for their claims.\n\n### Weaknesses\n\n1. In Figure 2, the authors illustrate different prompting methods; however, the figure does not clearly distinguish between the use of text descriptions and video inputs. This ambiguity makes it challenging to understand whether the authors used text descriptions to guide the prompt along with the video input or if they only provided video data. \n2. The author's experiments conducted on only one dataset are clearly insufficient in terms of persuasiveness. This limitation may affect the generalizability and applicability of the research findings. To enhance the credibility of the study, it is recommended that the authors validate their approach on different datasets.\n3. When the authors use radiologist's eye-tracking sequences as video input, this approach does enhance the model's understanding of prior knowledge in the diagnostic reading process to a certain extent. However, converting eye-tracking data into video format substantially increases the number of tokens, leading to significant computational resource consumption and longer inference times for the model.\n\n### Questions\n\n1. The RadEyeVideo method integrates the eye movement data of the radiologist as a video sequence to capture the spatiotemporal dynamics of his gaze. However, the article does not explain in detail how this video sequence is specifically generated, for example, how the eye movement data is sampled and converted into video frames, and how these frames contain temporal and spatial information.\n2. In Section 2.3 INPUT REPRESENTATION, to fit the input requirements of LVLM, the authors uniformly sampled the video sequence and selected 16 frames as inputs. However, the article does not explain why 16 frames were chosen and whether this choice had a significant impact on model performance. In addition, whether the impact of using more or fewer frames on model performance has been explored is also a question worth exploring. This helps to further understand the effect of video data length and quality on model performance.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RadEyeVideo*, a novel video prompting method designed to improve the reliability of large vision‚Äìlanguage models (LVLMs) in clinical radiology. The approach integrates radiologists‚Äô eye-tracking data as video sequences to capture both spatial and temporal gaze dynamics, displaying fixation points on chest X-rays to reflect attention patterns during diagnosis. The goal is to enhance diagnostic accuracy by providing richer contextual information to radiologists or AI models. Overall, the paper is clearly written and demonstrates an innovative use of multimodal data, though several methodological and experimental issues limit its persuasiveness.  \n\n**Major Comments**  \n1. **Clarity of Experimental Setup** ‚Äì Figure 2, illustrating various prompting methods, lacks clear differentiation between text descriptions and video inputs. It is unclear whether text is used alongside video or if only video serves as the input. Clearer explanation and labeling are essential for reproducibility.  \n2. **Dataset Limitations** ‚Äì Experiments are conducted on a single dataset, which undermines the generalizability of results. Validation on multiple datasets is strongly recommended to strengthen the study‚Äôs applicability and credibility.  \n3. **Computational Overhead** ‚Äì Transforming eye-tracking data into video format substantially increases the number of input tokens, resulting in heavy computational cost and slower inference. The trade-off between performance gain and computational efficiency should be analyzed.  \n4. **Methodological Details** ‚Äì The paper does not describe how raw eye-tracking data are sampled and converted into video frames to preserve temporal and spatial information. This lack of detail hinders reproducibility.  \n5. **Choice of Frame Sampling** ‚Äì Section 2.3 reports uniform sampling of 16 frames, but the rationale for this selection is not explained. Discussion of how varying frame counts influence performance would clarify design choices and data sensitivity.  \n\n**Minor Comments**  \n- Figure legends should be revised for clarity to differentiate modalities.  \n- Some sentences could be made more precise, particularly in describing data preprocessing steps.  \n\n**Summary Paragraph**  \nThe proposed RadEyeVideo framework effectively explores multimodal fusion of text, image, and eye-tracking data for diagnostic tasks and provides thorough empirical comparisons of integration strategies. However, ambiguities in method description, limited dataset validation, and high computational cost substantially weaken the strength of its conclusions. While the idea is promising and well-presented, the study would benefit from clearer methodology and broader experimental validation.  \n\n**Decision Recommendation**  \n**Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThe paper introduces an innovative approach that incorporates radiologists' eye-tracking data as video sequences. This method captures both spatial and temporal patterns in gaze, which provides a more accurate representation of radiologists' attention during chest X-ray interpretation. The approach was evaluated using three general-domain LVLMs, showing significant improvements.\n\n### Soundness: 3\n\n### Presentation: 3\n\n### Contribution: 2\n\n### Strengths\n\n- RadEyeVideo's use of video-based eye-gaze data is a unique contribution that effectively captures the temporal and spatial dynamics of radiologists' focus. I like this idea.\n- The study demonstrates substantial improvements, particularly in impression generation, highlighting RadEyeVideo's effectiveness in enhancing diagnostic tasks.\n- The language is clearly presented. The authors use precise and concise language so that the reader can easily understand the methodology, and results of the study.\n\n### Weaknesses\n\n- Although this idea is interesting, it still relies on temporal and spatial information in the inference phase, which is difficult to apply to real clinical scenarios. Do the authors consider involving multiple information inputs only in the training phase and simulating zero-shot scenarios as much as possible in the inference phase?\n- The study‚Äôs findings are limited by the small size of the MIMIC-Eye dataset, which may not fully capture the variability in real-world clinical settings, raising questions about the generalizability of the results.\n- The comparisons primarily focus on selected models with minimal tuning for this domain. Including a wider range of task-specific medical LVLMs could provide a more comprehensive evaluation.\n\n### Questions\n\n- Why the paper lacks an section of related work? e.g., some recent evaluation work of Med-LVLMs [1,2,3]\n- The formats of reference is weird. e.g., in Line 311-321. Please check it.\n\n[1] Gu Z, Yin C, Liu F, et al. MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context[J]. arXiv preprint arXiv:2407.02730, 2024.\n\n[2] Jiang Y, Chen J, Yang D, et al. MedThink: Inducing Medical Large-scale Visual Language Models to Hallucinate Less by Thinking More[J]. arXiv preprint arXiv:2406.11451, 2024.\n\n[3] Xia P, Chen Z, Tian J, et al. CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models[J]. arXiv preprint arXiv:2406.06007, 2024.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents an innovative approach, RadEyeVideo, that incorporates radiologists‚Äô eye-tracking data represented as video sequences. By modeling both spatial and temporal gaze patterns, the method aims to more accurately reflect radiologists‚Äô visual attention during chest X-ray interpretation. The authors evaluate this approach using three general-domain large vision-language models (LVLMs), reporting substantial improvements‚Äîparticularly in impression generation. Overall, the manuscript is clearly written, with concise language that effectively communicates the methodology and results.\n\n**Major Comments**  \n1. **Practical applicability:** While the use of spatiotemporal gaze data is conceptually strong, the reliance on such information during the inference phase limits feasibility in real clinical settings. The reviewer suggests testing whether gaze information could be restricted to the training phase, simulating zero-shot inference scenarios.  \n2. **Dataset limitations:** The evaluation relies on the small MIMIC-Eye dataset, which may not adequately represent the diversity of real-world data and thus limits the generalizability of the findings.  \n3. **Evaluation scope:** Comparisons are primarily made with a small set of general-domain LVLMs with minimal domain-specific tuning. Including additional medical-domain LVLMs would strengthen the assessment of RadEyeVideo‚Äôs performance and provide greater context for its advantages.  \n4. **Related work:** The paper lacks a dedicated related work section. Discussion of recent medical LVLM evaluation studies, such as MedVH, MedThink, and CARES, would situate the contribution more effectively within the current literature.\n\n**Minor Comments**  \n- Reference formatting is inconsistent (e.g., lines 311‚Äì321) and should be standardized throughout.  \n- Language and presentation are generally clear, but some organizational issues, such as missing section headers, could be addressed for completeness.\n\n**Summary Paragraph**  \nThis paper introduces a novel method with clear technical and conceptual contributions, particularly in integrating temporal gaze data into vision-language modeling. The reviewer finds the idea novel and well presented, noting its potential to enhance diagnostic tasks. However, practical limitations in inference, restricted dataset size, and a narrow comparative evaluation reduce the strength of the conclusions. A more extensive validation and incorporation of related work would improve the study‚Äôs robustness and contextual grounding.\n\n**Decision Recommendation**: **Major Revision**",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase‚ÄØ1‚ÄØ‚Äì Analytical‚ÄØand‚ÄØScope‚ÄØAudit**\n\n**1.‚ÄØScope‚ÄØ&‚ÄØRelevance‚ÄØto‚ÄØTMI**  \nThe manuscript presents *RadEyeVideo*, a method to incorporate radiologists‚Äô eye‚Äëtracking data into large vision‚Äëlanguage models (LVLMs) for chest‚ÄØX‚Äëray report generation and diagnosis. The work sits at the interface of computer vision, multimodal machine learning, and medical imaging.  It proposes a new data representation (eye‚Äëgaze videos) and evaluates several multimodal prompting strategies.  \n‚úÖ‚ÄØ**Within scope**‚ÄØof *IEEE‚ÄØTMI*: it proposes a methodological innovation for human‚Äëin‚Äëthe‚Äëloop medical image analysis with structured experiments on image‚Äìlanguage models.\n\n**2.‚ÄØNovelty‚ÄØ&‚ÄØContribution‚ÄØLevel**  \nThe primary novelty lies in encoding gaze fixation sequences as *video prompts* to capture temporal order, contrasting with prior static heatmaps or textual cues. While use of eye‚Äëtracking data for medical imaging AI is known, extending it to video‚Äëbased prompts for LVLMs is a distinctive but *incremental* conceptual step rather than a fundamental algorithmic advance. The contribution is therefore methodological at the data‚Äërepresentation and prompting‚Äëdesign level, not at the level of new model architectures or learning theory.  \nOverall novelty: **moderate**‚Äîpotentially interesting for the community if empirical gains are validated more rigorously.\n\n**3.‚ÄØTechnical‚ÄØand‚ÄØExperimental‚ÄØRigor**  \n- **Dataset**: built from MIMIC‚ÄëEye (‚âà2.3‚ÄØk images). The authors acknowledge limited sample size and potential train/test overlap; nonetheless, methodology to mitigate bias is described (alpha/beta splits).  \n- **Baselines**: several state‚Äëof‚Äëthe‚Äëart LVLMs, including medical and general‚Äëdomain ones, are compared.  \n- **Metrics**: both lexical (ROUGE,‚ÄØBERTScore) and radiology‚Äëspecific (CheXbert,‚ÄØRadGraph,‚ÄØRaTEScore) used with a scaled‚Äëscore composite.  \n- **Ablation**: frame‚Äëcount, gaze filtering, and in‚Äëcontext learning were examined.  \n- **Weaknesses**: limited statistical analysis‚Äîno confidence intervals or significance testing; clinical relevance of improvements not formally studied; dataset imbalance remains; ethical statement minimal. Reproducibility seems adequate (code indicated).  \n\n**4.‚ÄØClarity‚ÄØand‚ÄØPresentation**  \nManuscript is generally well organized, with clear figures, equations, and tables. Wording is occasionally verbose; minor grammatical inconsistencies appear but readability is high. Quantitative results are exhaustive, perhaps excessively so for main text.\n\n**5.‚ÄØEthical‚ÄØand‚ÄØReproducibility‚ÄØCompliance**  \nData use complies with MIMIC‚ÄëEye licensing; anonymization stated. Code availability noted. No patient‚Äëidentifiable content. Ethical‚ÄØ&‚ÄØreproducibility information are satisfactory.\n\n---\n\n**Phase‚ÄØ2‚ÄØ‚Äì‚ÄØIEEE‚ÄØTMI‚ÄØReview‚ÄØReport**\n\n**1.‚ÄØSummary**  \nThe paper proposes *RadEyeVideo*, which treats radiologists‚Äô eye‚Äëfixation traces as short videos overlaid on chest‚ÄØX‚Äëray images and feeds these into video‚Äëcapable LVLMs. The goal is to capture both spatial and temporal attention signals to enhance report generation and diagnostic accuracy. Evaluations on the MIMIC‚ÄëEye dataset demonstrate up to‚ÄØ25‚ÄØ% improvement in impression‚Äëgeneration metrics compared with baseline prompting and better performance than some medical‚Äëdomain LVLMs.\n\n**2.‚ÄØStrengths**\n- Creative use of expert gaze as dynamic spatio‚Äëtemporal conditioning signal.  \n- Comprehensive comparison against multiple gaze‚Äëintegration modes (heatmap, text, video).  \n- Ablation studies clarify design choices (frame count, gaze filtering, in‚Äëcontext learning).  \n- Solid presentation and connection to human‚ÄëAI collaboration literature.  \n\n**3.‚ÄØWeaknesses**\n- Dataset small, with possible overlap of training and evaluation splits.  \n- Improvement margins reported without statistical validation; significance uncertain.  \n- Method acts mainly as data‚Äërepresentation engineering, not deeper modeling innovation.  \n- Experimental scope limited to chest‚ÄØX‚Äërays; claims of domain generality not demonstrated.  \n- Clinical impact not assessed through radiologist review or reader study.  \n\n**4.‚ÄØMajor‚ÄØComments**\n1.‚ÄØ**Validation‚ÄØDepth**‚ÄØ‚Äì‚ÄØProvide statistical significance (e.g., bootstrap CIs, t‚Äëtests) on improvement percentages and discuss variability across cases.  \n2.‚ÄØ**Dataset‚ÄØBias‚ÄØand‚ÄØOverlap**‚ÄØ‚Äì‚ÄØElaborate on how training/testing contamination was prevented when reusing MIMIC‚ÄëEye images from MIMIC‚ÄëCXR splits.  \n3.‚ÄØ**Clinical‚ÄØUtility**‚ÄØ‚Äì‚ÄØQuantify whether textual improvements correspond to higher diagnostic correctness or fewer critical hallucinations verified by domain experts.  \n4.‚ÄØ**Generality‚ÄØClaims**‚ÄØ‚Äì‚ÄØRestrict or further substantiate broader‚Äëdomain statements; showing results on another modality (e.g., CT) or public benchmark would strengthen TMI‚Äëlevel impact.  \n5.‚ÄØ**Reproducibility**‚ÄØ‚Äì‚ÄØConsider releasing the full gaze‚Äëvideo generation pipeline and scripts for metric scaling to ensure others can replicate the reported gains.\n\n**5.‚ÄØMinor‚ÄØComments**\n- Several typographical errors (‚Äúgeneartion,‚Äù ‚ÄúE‚ÄØNHANCING‚Äù) should be corrected.  \n- Figure‚ÄØcaptions could be more concise; ensure all acronyms are defined upon first use.  \n- Some references are preprints; when possible, replace with peer‚Äëreviewed versions.  \n- Supplementary quantitative tables could be moved to appendix or shared data repository to streamline the main text.  \n\n**6.‚ÄØOverall‚ÄØRecommendation**\n**Major‚ÄØRevision** ‚Äì The paper addresses an interesting methodological direction aligning with TMI‚Äôs emphasis on human‚Äëcentered imaging AI. However, the current study‚Äôs small‚Äëscale evaluation and limited novelty evidence require stronger experimental validation and clearer positioning relative to existing human‚Äëeye‚Äëguided models before acceptance.\n\n**7.‚ÄØConfidence‚ÄØLevel:**‚ÄØ4‚ÄØ/‚ÄØ5  \n(Reviewer is familiar with LVLMs and eye‚Äëtracking‚ÄØAI; moderate‚Äëto‚Äëhigh confidence in technical assessment.)",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces *RadEyeVideo*, a method that integrates radiologists‚Äô eye‚Äëtracking information into large vision‚Äëlanguage models for automated chest‚ÄØX‚Äëray interpretation and report generation. By encoding gaze patterns as short ‚Äúvideo prompts,‚Äù the approach aims to capture both spatial and temporal aspects of expert attention, enhancing the contextual grounding of generated reports. The paper presents structured experiments using the MIMIC‚ÄëEye dataset and compares multiple gaze‚Äëintegration strategies. Overall, the manuscript is clearly written, well organized, and generally reproducible, with figures and tables that effectively convey results. The quality of presentation is strong, though occasionally verbose.\n\n---\n\n**Major Comments**  \n1. **Validation and Statistical Rigor** ‚Äì Reported performance improvements (e.g., up to 25‚ÄØ%) lack statistical significance tests. Including confidence intervals or hypothesis testing would strengthen the reliability of claims.  \n2. **Dataset Limitations and Potential Overlap** ‚Äì The MIMIC‚ÄëEye dataset is small (~2.3k images) and may include overlap between training and test splits. Clarify the separation strategy and discuss potential impacts on bias.  \n3. **Novelty and Contribution Scope** ‚Äì The main innovation lies in data‚Äërepresentation and prompting design rather than architectural or algorithmic development. Novelty is moderate and should be positioned accordingly.  \n4. **Clinical and Practical Relevance** ‚Äì Improvements are measured through textual and automated metrics but not validated by radiologists or clinical outcome assessments. Clarify whether gains translate into diagnostic utility.  \n5. **Claimed Generality** ‚Äì Assertions of broad applicability are unsupported. Demonstrating results on a different modality or dataset would substantiate these claims.  \n6. **Reproducibility Details** ‚Äì While code availability is noted, consider releasing the full gaze‚Äëvideo generation pipeline and metric‚Äëscaling scripts to ensure replicability.  \n\n---\n\n**Minor Comments**  \n- Correct typographical errors (e.g., ‚Äúgeneartion,‚Äù ‚ÄúE‚ÄØNHANCING‚Äù).  \n- Define all acronyms on first use; streamline figure captions.  \n- Replace preprint citations with peer‚Äëreviewed versions where available.  \n- Consider moving detailed quantitative tables to supplementary materials for clarity.  \n- Ethical statement is brief; expand on data use compliance as space allows.  \n\n---\n\n**Summary Paragraph**  \nThis study explores a creative approach to embedding radiologist gaze dynamics into multimodal AI systems for chest‚ÄØX‚Äëray reporting. Strengths include original use of temporal gaze cues, comprehensive ablations, and clear presentation. However, limited dataset size, modest conceptual novelty, lack of statistical validation, and absence of clinically verified impact constrain the strength of the conclusions. Enhanced evaluation breadth and statistical rigor would substantially improve the work‚Äôs contribution and credibility.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The paper presents a promising direction for human‚Äëin‚Äëthe‚Äëloop imaging AI, but requires stronger validation, expanded evidence of generality, and clearer differentiation from prior gaze‚Äëbased studies before it can be accepted.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper introduces RadEyeVideo, a novel approach that integrates radiologists' eye-tracking data as video sequences to enhance large vision-language models (LVLMs) for chest X-ray analysis. Unlike existing methods that use static heatmaps or textual prompts, RadEyeVideo captures both spatial and temporal dynamics of radiologists' gaze patterns by creating videos with red dots overlaid on CXR images, where dot duration reflects fixation time (Section 2.2, Equations 1-3). The authors evaluate three general-domain video-capable LVLMs (LongVA, VideoLLaMA2, LLaVA-OneVision) on report generation and diagnosis tasks using the MIMIC-Eye dataset (2,298 CXR images with eye-gaze annotations). Results show RadEyeVideo achieves up to 25.4% improvement on Impression generation and 7.9% average improvement across all tasks compared to baseline methods (Table 3). The enhanced general-domain models outperform specialized medical LVLMs like CheXagent and CXR-LLaVA on benchmark datasets.\n\n## Weaknesses\n\n‚Ä¢ **Limited Dataset Scale and Generalizability**: The evaluation relies on a small dataset of only 2,298 images from MIMIC-Eye, with the beta split containing merely 92 images for robust evaluation (Table 1, Page 6). This severely limits statistical power and generalizability claims. The authors acknowledge this limitation (Page 10) but proceed with evaluation on both training and test splits, potentially introducing data contamination issues that could inflate reported performance gains.\n\n‚Ä¢ **Inconsistent Mathematical Formulation and Implementation Details**: The video construction methodology lacks precision in several key aspects. Equation 2 defines frame rate as 10 fps but provides conflicting information about total frame calculation (Fi = ti √ó fps vs Fi = ti √ó 10). The uniform sampling strategy in Equation 3 uses floor division that could lead to frame loss, and the fixed 5-pixel gaze radius lacks justification or sensitivity analysis. The filtering criterion using average fixation duration (Equation 1) appears arbitrary without empirical validation.\n\n‚Ä¢ **Weak Experimental Design and Evaluation Methodology**: The scaled evaluation metric based on CheXagent performance (Equation 7) creates circular reasoning since CheXagent is simultaneously used as both a baseline competitor and the normalization reference. The evaluation lacks statistical significance testing despite small sample sizes, and the temperature parameter of 0 eliminates randomness assessment. The choice of 16 frames appears arbitrary, supported only by a limited ablation study (Figure 4) without theoretical justification.\n\n‚Ä¢ **Insufficient Baseline Comparisons and Analysis**: The paper compares against limited eye-gaze integration methods and lacks comparison with recent human-AI collaboration approaches in medical imaging. The heatmap implementation details are poorly described (Page 7), making reproduction difficult. Performance improvements show high variance across models and tasks (Table 3), with VideoLLaMA2 showing negligible improvement (0.6%) and inconsistent patterns that undermine the claimed superiority of the video approach.\n\n## Suggestions for Improvement\n\n‚Ä¢ **Expand Dataset Scale and Validation Strategy**: Conduct evaluation on larger, more diverse datasets beyond MIMIC-Eye to establish generalizability. Implement proper cross-validation with completely held-out test sets to avoid contamination issues. Include multi-institutional validation data to demonstrate robustness across different radiologist populations and imaging protocols.\n\n‚Ä¢ **Clarify and Validate Mathematical Formulations**: Provide consistent and precise mathematical notation throughout, particularly for frame rate calculations and sampling strategies. Conduct sensitivity analysis for key hyperparameters including gaze radius size, filtering thresholds, and frame numbers. Justify the average-duration filtering criterion through empirical analysis of gaze pattern significance and provide theoretical foundation for uniform sampling approach.\n\n‚Ä¢ **Strengthen Experimental Design with Rigorous Statistics**: Replace the circular CheXagent-based scaling metric with independent evaluation standards or multiple reference points. Include statistical significance testing with confidence intervals and effect size calculations. Implement multiple temperature settings to assess model reliability and add cross-model validation to strengthen claims about video-based prompting superiority.\n\n‚Ä¢ **Comprehensive Baseline Analysis and Methodological Transparency**: Include comparisons with recent human-AI collaboration methods in medical imaging and provide detailed implementation specifications for all baseline approaches. Conduct thorough ablation studies examining individual components of the video creation pipeline and provide qualitative analysis of failure cases to better understand method limitations and boundary conditions.",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents *RadEyeVideo*, a new method that incorporates radiologists‚Äô eye-tracking data as dynamic video sequences to enhance large vision-language models (LVLMs) in chest X-ray interpretation. Unlike existing static or text-based gaze integration methods, this approach encodes temporal and spatial gaze dynamics as video inputs. The study evaluates three general-domain video-capable LVLMs (LongVA, VideoLLaMA2, and LLaVA-OneVision) using the MIMIC-Eye dataset for report generation and diagnosis tasks. Reported results indicate notable quantitative improvements over baseline models and even specialized medical LVLMs. The paper is clearly written and provides a novel perspective on using gaze information for medical image reasoning; however, several methodological and experimental issues limit the robustness and generalizability of the conclusions.  \n\n**Major Comments**  \n1. **Dataset Scale and Generalizability** ‚Äì The evaluation uses a relatively small dataset (2,298 images), with the beta split containing only 92 images, significantly limiting statistical strength. The use of overlapping training and test data suggests potential contamination that may inflate performance metrics.  \n2. **Mathematical and Implementation Inconsistencies** ‚Äì The video construction equations present internal inconsistencies regarding frame-rate definitions and frame calculation methods. The uniform sampling with floor division may lose frames, and fixed parameters such as the 5-pixel gaze radius lack sensitivity testing or empirical justification.  \n3. **Experimental Design Weaknesses** ‚Äì The performance scaling based on CheXagent introduces circular reasoning, as CheXagent is also a comparison baseline. There is no significance testing despite small samples, and the choice of parameters (e.g., 16 frames, temperature 0) appears arbitrary or inadequately justified.  \n4. **Limited Baseline and Analysis Depth** ‚Äì Comparisons omit several relevant gaze-based or collaborative human-AI methods. Some baselines (e.g., heatmap-based) lack sufficient implementation details for replication, and performance gains are inconsistent across models and tasks.  \n\n**Minor Comments**  \n- Clarify notations in Equations 1‚Äì3 and harmonize parameter units.  \n- Provide more detailed figure captions and clearer presentation of ablation results.  \n- Improve description of dataset splits and evaluation metrics for reproducibility.  \n\n**Summary Paragraph**  \nOverall, the paper offers an innovative approach to integrating radiologists‚Äô gaze dynamics through video-based prompts for LVLMs. Nonetheless, its impact is constrained by small-scale evaluation, ambiguous methodological descriptions, and limited statistical rigor. Addressing these issues through clearer formulations, stronger data protocols, and more comprehensive comparisons would substantially enhance the credibility of the findings.  \n\n**Decision Recommendation**  \n**Major Revision.** The proposed idea is promising, but significant methodological and evaluative refinements are needed before the results can be considered reliable.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\nThis manuscript presents RadEyeVideo, a novel approach that integrates radiologists' eye-gaze data as video sequences to enhance the performance of general-domain Large Vision-Language Models (LVLMs) in chest X-ray (CXR) analysis. The authors argue that capturing the sequential and temporal dynamics of eye movements can provide deeper insights into radiologists' decision-making processes, thereby improving the accuracy and clinical relevance of CXR report generation and diagnosis. The manuscript includes a comprehensive evaluation of RadEyeVideo against existing methods, demonstrating significant performance gains across various metrics.\n\n## Major Comments\n1. Novelty and Positioning: While RadEyeVideo introduces a compelling method for integrating eye-gaze data into LVLMs, the manuscript could benefit from a clearer distinction from existing work on multimodal human-in-the-loop (HITL) systems. The authors should provide a more detailed discussion of how their approach compares to and builds upon recent advancements in this area.\n\n2. Evaluation Design: The evaluation is primarily conducted on the MIMIC-Eye dataset, which, while valuable, may not fully capture the diversity of real-world CXR interpretations. Including a broader range of datasets or validation on prospective CXR data could strengthen the generalizability of the findings. Additionally, the evaluation metrics could be expanded to include qualitative assessments by radiologists to provide a more comprehensive understanding of the method's clinical utility.\n\n3. Comparison Baselines: The manuscript includes a thorough comparison with various prompting methods, including heatmaps and fixation text. However, the inclusion of a broader range of state-of-the-art LVLMs specifically trained for CXR analysis would provide a more robust benchmark for assessing RadEyeVideo's performance improvements.\n\n4. Reproducibility: The authors state that the code will be made available, which is commendable. However, the manuscript would benefit from a more detailed description of the experimental setup, including the preprocessing pipeline, model training parameters, and specific configurations used for the various LVLMs. This would facilitate independent replication and further development of the method.\n\n## Minor Comments\n1. Figure Quality: Figures 3 and 5 could be improved by providing a clearer comparison between different prompting methods. Zooming in on specific regions of interest and providing more detailed annotations would enhance readability.\n\n2. Notational Consistency: The manuscript occasionally uses inconsistent notation, particularly in describing the eye-gaze data processing steps. Clarifying these definitions and ensuring consistent usage throughout the document would improve clarity.\n\n3. Acronym Definitions: Some acronyms, such as \"RADEYEVIDEO,\" are introduced without full definitions. Providing a glossary or defining these terms at their first appearance would aid reader comprehension.\n\n4. Typographical Issues: Minor typographical errors, such as \"CXRMate\" (p. 11) and \"CXRMate does not support textual prompts for in-context learning, so its vanilla performance is reported instead\" (p. 12), should be corrected for consistency.\n\n## Summary Paragraph\nThe manuscript addresses a significant and clinically relevant challenge: enhancing the performance of general-domain LVLMs in CXR analysis through the integration of radiologists' eye-gaze data. The proposed RadEyeVideo method introduces a novel and innovative approach by capturing both spatial and temporal dynamics of eye movements. The evaluation demonstrates substantial performance improvements across multiple metrics, suggesting the method's potential to bridge the gap between general-purpose and task-specific models. However, the evaluation could be strengthened by incorporating a broader range of datasets and validation scenarios. The reproducibility of the approach is addressed through the provision of code, but more detailed methodological descriptions are needed to ensure independent replication. Overall, the manuscript makes a valuable contribution to the field of AI-assisted diagnostics, though further refinement in evaluation and reproducibility is recommended.\n\n## Decision Recommendation\nMajor revision. The authors should expand the evaluation to include a broader range of datasets and validation scenarios, clarify comparisons with existing work, and provide more detailed methodological descriptions to enhance reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **RadEyeVideo**, an approach that integrates radiologists‚Äô eye-gaze data as video sequences to enhance the performance of general-domain Large Vision-Language Models (LVLMs) for chest X-ray (CXR) interpretation. By modeling the temporal and sequential dynamics of eye movements, the method aims to capture radiologists‚Äô diagnostic reasoning and improve both report generation and disease detection accuracy. The paper presents comprehensive experiments demonstrating notable performance gains over existing techniques and discusses potential implications for clinical decision support. Overall, the study is clearly written and targets an important problem in AI-assisted radiology.  \n\n**Major Comments**  \n1. **Novelty and Positioning**: While the method is promising, the distinction from existing multimodal human-in-the-loop (HITL) systems could be articulated more clearly. The authors should expand the discussion on how RadEyeVideo advances or differentiates itself from prior multimodal LVLMs and gaze-based learning approaches.  \n2. **Evaluation Design**: The experiments rely mainly on the MIMIC-Eye dataset, which provides limited diversity for assessing clinical generalizability. Validation on additional datasets or prospective CXR data would strengthen the evidence. Including qualitative feedback from radiologists could further enhance the demonstration of clinical relevance.  \n3. **Comparison Baselines**: The study compares RadEyeVideo to various prompting methods but omits direct comparison with other state-of-the-art LVLMs developed specifically for CXR analysis. Incorporating such baselines would improve the robustness of the performance claims.  \n4. **Reproducibility**: Although the authors indicate that code will be released, additional experimental details are needed‚Äîsuch as preprocessing steps, model training parameters, and configuration specifics‚Äîto enable independent replication.  \n\n**Minor Comments**  \n- **Figures**: Figures 3 and 5 would benefit from clearer visual comparisons, possibly through zoomed-in examples and detailed annotations.  \n- **Notation**: Some inconsistency exists in the terminology used for eye-gaze processing; these should be standardized throughout.  \n- **Acronyms**: All abbreviations, including ‚ÄúRADEYEVIDEO,‚Äù should be defined upon first mention or summarized in a glossary.  \n- **Typos**: Minor typographical issues, such as ‚ÄúCXRMate‚Äù (p.‚ÄØ11‚Äì12), need correction for consistency.  \n\n**Summary Paragraph**  \nThis study tackles a meaningful problem‚Äîimproving LVLMs for CXR analysis by embedding radiologists‚Äô visual search patterns through spatiotemporal modeling. The proposed method is innovative and yields promising quantitative gains, but the work would benefit from broader validation, clearer differentiation from related research, and fuller methodological transparency to ensure reproducibility. With these refinements, the manuscript could make a strong contribution to the field of medical vision-language modeling.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Recommended to address evaluation breadth, comparative analysis, and methodological detail before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## RadEyeVideo: Enhancing GENERAL-DOMAIN LARGE VISION LANGUAGE MODEL FOR CHEST X-RAY ANALYSIS WITH VIDEO REPRESENTATIONS OF EYE GAZE\n\n### Summary\n\nThe paper proposes RadEyeVideo, a prompting strategy that converts radiologists‚Äô fixation traces into short videos (moving red dot over the CXR) to provide both spatial and temporal gaze information to general-domain video-capable LVLMs. Evaluated on MIMIC-Eye for findings generation, impression summarization, and differential diagnosis, the method shows consistent gains over ‚Äúno gaze‚Äù baselines and often outperforms heatmap and textual-gaze prompts, with the largest improvements observed on impression summarization. The authors also provide code to recreate gaze videos (‚ÄúMIMIC-Eye-Video‚Äù) and ablations on frames, fixation filtering, and in-context exemplars.\n\n### Strengths\n\n- Technical novelty and innovationUsing video representations of expert gaze as a visual prompt for LVLMs is a simple, elegant idea that preserves scanpath order and duration, going beyond prior heatmap and text encodings.The approach leverages existing video interfaces of general-domain LVLMs without fine-tuning, offering a practical, lightweight pathway to incorporate human expertise.\n- Experimental rigor and validationThe work compares three gaze encodings (heatmap, text, video) across multiple video-capable LVLMs and includes medical LVLM baselines, offering a broad perspective.Ablations on frame count, fixation filtering, and in-context learning provide initial insight into design choices and their effects.\n- Clarity of presentationThe motivation for preserving temporal order is clearly articulated; the video construction pipeline is easy to reproduce.Prompts and experimental settings are described with reasonable detail; code availability for dataset reconstruction is useful for reproducibility.\n- Significance of contributionsResults suggest that expert gaze, when encoded as a video, can boost general-purpose LVLMs to rival or surpass task-specific medical models on key report-generation metrics.The paper encourages a promising line of human-in-the-loop prompting for clinical AI without retraining models.\n\n- Using video representations of expert gaze as a visual prompt for LVLMs is a simple, elegant idea that preserves scanpath order and duration, going beyond prior heatmap and text encodings.\n- The approach leverages existing video interfaces of general-domain LVLMs without fine-tuning, offering a practical, lightweight pathway to incorporate human expertise.\n\n- The work compares three gaze encodings (heatmap, text, video) across multiple video-capable LVLMs and includes medical LVLM baselines, offering a broad perspective.\n- Ablations on frame count, fixation filtering, and in-context learning provide initial insight into design choices and their effects.\n\n- The motivation for preserving temporal order is clearly articulated; the video construction pipeline is easy to reproduce.\n- Prompts and experimental settings are described with reasonable detail; code availability for dataset reconstruction is useful for reproducibility.\n\n- Results suggest that expert gaze, when encoded as a video, can boost general-purpose LVLMs to rival or surpass task-specific medical models on key report-generation metrics.\n- The paper encourages a promising line of human-in-the-loop prompting for clinical AI without retraining models.\n\n### Weaknesses\n\n- Technical limitations or concernsThe study does not include critical controls to isolate the value of sequence information (e.g., shuffled scanpaths, duration-preserving but order-randomized videos, or order-preserving but location-scrambled controls).The impression task appears to take as input the ground-truth findings text (eq. 5), conflating summarization from text with image-grounded impression generation; this may inflate performance and complicate comparisons with prior work that conditions only on images.Details for heatmap construction (alpha blending, kernel size, opacity, image occlusion impact) and textual-gaze formatting are insufficient to ensure fair, optimized baselines.\n- Experimental gaps or methodological issuesEvaluation relies heavily on a small ‚Äúbeta‚Äù split (62‚Äì92 cases) and a much larger ‚Äúalpha‚Äù set that overlaps MIMIC-CXR training data; contamination risks remain high for medical baselines and possibly for some general models trained on web data containing MIMIC-derived content.No statistical significance testing or confidence intervals are reported, despite small test sizes and modest deltas in some settings.The composite ‚Äúscaled to CheXagent = 100‚Äù score averages heterogeneous metrics (lexical and clinical) and can be distorted when denominators are small (e.g., CXRMate‚Äôs very high ‚Äúscaled‚Äù findings). This makes across-model and across-task comparisons difficult to interpret.For diagnosis, the use of CheXbert micro-F1 over ‚Äúall abnormalities‚Äù to assess differential diagnosis is under-specified; it is unclear how generated free text is mapped to labels and whether this aligns with the intended diagnostic task.\n- Clarity or presentation issuesEq. (7) text says the denominator is ‚ÄúLLaVA-Med‚Äù while the formula and narrative refer to CheXagent‚Äîthis inconsistency should be fixed.Table 2 contains unclear modality support markings (parentheses, X/checkmarks) that need standardization and clearer explanation.The ‚ÄúOverall‚Äù metric definitions (which metrics are included, weighting) should be explicitly enumerated.\n- Missing related work or comparisonsRecent gaze‚ÄìLVLM or gaze‚ÄìVLP efforts are not adequately discussed, e.g., EGMA (gaze-guided VLP), MedGaze (scanpath prediction), CoRaX (gaze video for error correction), and broader visual prompting surveys; these would help situate the contribution.Discussion lacks comparisons or references to report-generation dialog systems such as RaDialog and how impression-from-findings vs image-only settings relate to prior evaluation protocols.\n\n- The study does not include critical controls to isolate the value of sequence information (e.g., shuffled scanpaths, duration-preserving but order-randomized videos, or order-preserving but location-scrambled controls).\n- The impression task appears to take as input the ground-truth findings text (eq. 5), conflating summarization from text with image-grounded impression generation; this may inflate performance and complicate comparisons with prior work that conditions only on images.\n- Details for heatmap construction (alpha blending, kernel size, opacity, image occlusion impact) and textual-gaze formatting are insufficient to ensure fair, optimized baselines.\n\n- Evaluation relies heavily on a small ‚Äúbeta‚Äù split (62‚Äì92 cases) and a much larger ‚Äúalpha‚Äù set that overlaps MIMIC-CXR training data; contamination risks remain high for medical baselines and possibly for some general models trained on web data containing MIMIC-derived content.\n- No statistical significance testing or confidence intervals are reported, despite small test sizes and modest deltas in some settings.\n- The composite ‚Äúscaled to CheXagent = 100‚Äù score averages heterogeneous metrics (lexical and clinical) and can be distorted when denominators are small (e.g., CXRMate‚Äôs very high ‚Äúscaled‚Äù findings). This makes across-model and across-task comparisons difficult to interpret.\n- For diagnosis, the use of CheXbert micro-F1 over ‚Äúall abnormalities‚Äù to assess differential diagnosis is under-specified; it is unclear how generated free text is mapped to labels and whether this aligns with the intended diagnostic task.\n\n- Eq. (7) text says the denominator is ‚ÄúLLaVA-Med‚Äù while the formula and narrative refer to CheXagent‚Äîthis inconsistency should be fixed.\n- Table 2 contains unclear modality support markings (parentheses, X/checkmarks) that need standardization and clearer explanation.\n- The ‚ÄúOverall‚Äù metric definitions (which metrics are included, weighting) should be explicitly enumerated.\n\n- Recent gaze‚ÄìLVLM or gaze‚ÄìVLP efforts are not adequately discussed, e.g., EGMA (gaze-guided VLP), MedGaze (scanpath prediction), CoRaX (gaze video for error correction), and broader visual prompting surveys; these would help situate the contribution.\n- Discussion lacks comparisons or references to report-generation dialog systems such as RaDialog and how impression-from-findings vs image-only settings relate to prior evaluation protocols.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe core method‚Äîrendering gaze as a moving point with frame counts proportional to dwell time and uniformly sampling to a fixed frame budget‚Äîis sound and easy to reproduce. The fixed 5-pixel dot may be small relative to the (unspecified) input resolution; a brief analysis of dot size and visibility would be valuable.The assumption that longer fixations carry more diagnostic signal is reasonable, but the current filtering threshold (t_i > mean) is simplistic; a percentile-based threshold or model-agnostic saliency weighting could be tested.A key missing control is a ‚Äúshuffled‚Äù condition that maintains the same set of fixation positions and durations but randomizes temporal order. This would directly test the hypothesis that sequential order per se contributes to the gains. Additional controls:Order-only vs location-only ablations (preserve order but jitter positions; preserve positions/durations but randomize order).A static ‚Äúcumulative‚Äù heatmap baseline carefully tuned to avoid occlusion.\n- Experimental evaluation assessmentSplits and contamination: Using the MIMIC-Eye training (‚Äúalpha‚Äù) data for evaluation introduces leakage with baselines trained on MIMIC-CXR; although the authors report alpha/beta separately, conclusions should be drawn primarily from the beta split and/or an external set (e.g., REFLACX/EGD-CXR). A leave-one-reader-out or cross-institution setup would strengthen claims of generalizability.Metrics and scaling: Averaging scaled metrics across dissimilar measures (ROUGE-L, BERTScore, CheXbert F1, RadGraph F1, RaTEScore) can obscure where gains truly come from. Please report raw clinical metric values (CheXbert, RadGraph, RaTEScore) and provide per-metric deltas with confidence intervals. Consider reporting composite metrics separately for lexical and clinical categories, rather than collapsing to a single ‚ÄúOverall‚Äù.Impression task protocol: If ground-truth findings are input, this becomes a text summarization task rather than an image-grounded impression generation. Provide a separate evaluation where impression is generated from images (+gaze) only, aligning with standard practice and fairer comparisons to image-conditioned baselines.Diagnosis evaluation: Clarify how diagnostic entities are extracted from the generated text, how CheXbert is applied to references vs predictions, and whether a radiology NER/normalization (e.g., SapBERT mapping) is used. Current framing of ‚Äúdifferential diagnosis‚Äù versus CheXbert label sets needs alignment.Statistical rigor: Include CIs or bootstrap resampling and hypothesis testing across model‚Äìprompt conditions, especially on the beta split.\n- Comparison with related work (using the summaries provided)EGMA (eye-gaze‚Äìguided pretraining) demonstrates that gaze can improve patch-sentence alignment; your work complements this by showing prompting-time benefits on report generation and diagnosis without finetuning. A discussion of training-time vs prompting-time gaze use would enrich the narrative.Kim et al. (heatmap/text prompting) is a direct comparator; emphasizing that RadEyeVideo preserves sequential order addresses a known limitation in those encodings.MedGaze (scanpath prediction) and CoRaX (gaze-video‚Äìguided error correction) both leverage spatio-temporal gaze signals. Citing them would position RadEyeVideo among broader gaze-temporal approaches, distinguishing your focus on LVLM prompting for report/diagnosis.GazeGNN encodes dwell but not temporal order; your results can be discussed as evidence that temporal dynamics matter at inference time for generative tasks.Visual prompting surveys could help contextualize how your ‚Äúvideo gaze prompt‚Äù fits within the growing literature on non-text prompts for MLLMs/LVLMs.\n- Discussion of broader impact and significanceThe work highlights a low-cost, training-free avenue to inject expert attention patterns into LVLMs, which could be impactful in data-scarce clinical settings. It also opens the door to synthetic gaze augmentation (e.g., predicted scanpaths) where real gaze is unavailable.Risks and biases: Gaze reflects reading strategies that vary by expertise and institution, potentially encoding biases (e.g., search patterns differing by radiologist seniority). Eye overlays may also occlude subtle findings; rigorous analysis of occlusion effects should be included. Privacy considerations around gaze traces warrant deeper discussion (linkability to specific readers, work patterns).Reproducibility is aided by code for video construction; stronger open evaluation would come from publishing full per-case outputs and metrics, and testing on external datasets.\n\n- The core method‚Äîrendering gaze as a moving point with frame counts proportional to dwell time and uniformly sampling to a fixed frame budget‚Äîis sound and easy to reproduce. The fixed 5-pixel dot may be small relative to the (unspecified) input resolution; a brief analysis of dot size and visibility would be valuable.\n- The assumption that longer fixations carry more diagnostic signal is reasonable, but the current filtering threshold (t_i > mean) is simplistic; a percentile-based threshold or model-agnostic saliency weighting could be tested.\n- A key missing control is a ‚Äúshuffled‚Äù condition that maintains the same set of fixation positions and durations but randomizes temporal order. This would directly test the hypothesis that sequential order per se contributes to the gains. Additional controls:Order-only vs location-only ablations (preserve order but jitter positions; preserve positions/durations but randomize order).A static ‚Äúcumulative‚Äù heatmap baseline carefully tuned to avoid occlusion.\n\n- Order-only vs location-only ablations (preserve order but jitter positions; preserve positions/durations but randomize order).\n- A static ‚Äúcumulative‚Äù heatmap baseline carefully tuned to avoid occlusion.\n\n- Splits and contamination: Using the MIMIC-Eye training (‚Äúalpha‚Äù) data for evaluation introduces leakage with baselines trained on MIMIC-CXR; although the authors report alpha/beta separately, conclusions should be drawn primarily from the beta split and/or an external set (e.g., REFLACX/EGD-CXR). A leave-one-reader-out or cross-institution setup would strengthen claims of generalizability.\n- Metrics and scaling: Averaging scaled metrics across dissimilar measures (ROUGE-L, BERTScore, CheXbert F1, RadGraph F1, RaTEScore) can obscure where gains truly come from. Please report raw clinical metric values (CheXbert, RadGraph, RaTEScore) and provide per-metric deltas with confidence intervals. Consider reporting composite metrics separately for lexical and clinical categories, rather than collapsing to a single ‚ÄúOverall‚Äù.\n- Impression task protocol: If ground-truth findings are input, this becomes a text summarization task rather than an image-grounded impression generation. Provide a separate evaluation where impression is generated from images (+gaze) only, aligning with standard practice and fairer comparisons to image-conditioned baselines.\n- Diagnosis evaluation: Clarify how diagnostic entities are extracted from the generated text, how CheXbert is applied to references vs predictions, and whether a radiology NER/normalization (e.g., SapBERT mapping) is used. Current framing of ‚Äúdifferential diagnosis‚Äù versus CheXbert label sets needs alignment.\n- Statistical rigor: Include CIs or bootstrap resampling and hypothesis testing across model‚Äìprompt conditions, especially on the beta split.\n\n- EGMA (eye-gaze‚Äìguided pretraining) demonstrates that gaze can improve patch-sentence alignment; your work complements this by showing prompting-time benefits on report generation and diagnosis without finetuning. A discussion of training-time vs prompting-time gaze use would enrich the narrative.\n- Kim et al. (heatmap/text prompting) is a direct comparator; emphasizing that RadEyeVideo preserves sequential order addresses a known limitation in those encodings.\n- MedGaze (scanpath prediction) and CoRaX (gaze-video‚Äìguided error correction) both leverage spatio-temporal gaze signals. Citing them would position RadEyeVideo among broader gaze-temporal approaches, distinguishing your focus on LVLM prompting for report/diagnosis.\n- GazeGNN encodes dwell but not temporal order; your results can be discussed as evidence that temporal dynamics matter at inference time for generative tasks.\n- Visual prompting surveys could help contextualize how your ‚Äúvideo gaze prompt‚Äù fits within the growing literature on non-text prompts for MLLMs/LVLMs.\n\n- The work highlights a low-cost, training-free avenue to inject expert attention patterns into LVLMs, which could be impactful in data-scarce clinical settings. It also opens the door to synthetic gaze augmentation (e.g., predicted scanpaths) where real gaze is unavailable.\n- Risks and biases: Gaze reflects reading strategies that vary by expertise and institution, potentially encoding biases (e.g., search patterns differing by radiologist seniority). Eye overlays may also occlude subtle findings; rigorous analysis of occlusion effects should be included. Privacy considerations around gaze traces warrant deeper discussion (linkability to specific readers, work patterns).\n- Reproducibility is aided by code for video construction; stronger open evaluation would come from publishing full per-case outputs and metrics, and testing on external datasets.\n\n### Questions for Authors\n\n- For impression generation, do all models receive the ground-truth findings text as input (eq. 5)? If so, can you report a complementary setting where impression is generated from the image (+gaze) only, to enable fairer comparison with past work?\n- Did you run a ‚Äúshuffled scanpath‚Äù control (same fixation set and durations, randomized order) or other order/location ablations to isolate the benefit of temporal order?\n- How exactly are diagnosis outputs evaluated with CheXbert? Is CheXbert applied to the generated text and compared to CheXbert labels of the reference report? How are synonyms, negations, and non-CheXpert findings handled?\n- How were in-context exemplar reports selected? Are they drawn from the same alpha/beta distributions, and is there any risk of overlap with evaluation cases? Did you match exemplars by pathology or randomly?\n- Please detail the heatmap generation (kernel/opacity/alpha blending/resolution) and whether you ensured minimal occlusion of diagnostically critical structures. Could suboptimal heatmap rendering explain underperformance?\n- What was the CXR resolution fed to each model, and how large (in pixels or degrees of visual angle) was the red dot relative to the model‚Äôs input size? Did you test alternative dot sizes or visual encodings (e.g., trailing path, variable-radius indicating dwell)?\n- Can you provide raw per-metric results (CheXbert F1, RadGraph F1, RaTEScore) and significance tests for beta split results? The current scaled ‚ÄúOverall‚Äù averages are hard to interpret.\n- Beyond MIMIC-Eye, did you try any external validation (e.g., REFLACX or EGD-CXR) to assess generalization and reduce concerns about MIMIC-CXR overlap with baselines?\n\n### Overall Assessment\n\nThis paper presents a simple and practical idea‚Äîencoding clinician gaze as a short video‚Äîand demonstrates that it can meaningfully improve general-domain LVLMs on CXR report-related tasks without training. The approach is well-motivated, the implementation is clear, and the empirical gains, especially on impression summarization, are promising. At the same time, the evaluation has notable weaknesses: reliance on a small test split and an overlapping training split, lack of statistical testing, heterogeneous and potentially distorting metric scaling, and an impression-task setup that appears to use ground-truth findings as input. Crucially, the claim that temporal order matters would be much stronger with shuffled-order controls and clearer, raw clinical metrics on an uncontaminated test set.\n\nOverall, I see this as a solid, original prompting contribution with practical value, but in its current form it falls short of top-tier standards due to methodological and reporting gaps. Addressing contamination concerns, adding order-specific ablations, clarifying the impression task conditioning, and reporting raw clinical metrics with significance on a clean test set would substantially strengthen the paper and its claims.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **RadEyeVideo**, a prompting strategy that transforms radiologists‚Äô fixation traces into short gaze videos to supply both spatial and temporal cues to video-capable large vision‚Äìlanguage models (LVLMs) for chest X‚Äëray interpretation. The method is evaluated on the MIMIC‚ÄëEye dataset for findings generation, impression summarization, and differential diagnosis, showing consistent advantages over baselines without gaze input and over heatmap- or text‚Äëbased gaze encodings. The paper is clearly written, with a transparent motivation for temporally preserving gaze information, detailed methodological description, and accompanying code release facilitating reproducibility.\n\n---\n\n**Major Comments**  \n1. **Controls for Sequence Information** ‚Äì The study lacks key control experiments to isolate the contribution of temporal order (e.g., shuffled scanpaths or duration‚Äëpreserving but order‚Äërandomized videos). Without these, the benefit attributed to sequential gaze information remains untested.  \n2. **Impression-Task Protocol** ‚Äì Equation (5) suggests the impression task conditions on ground‚Äëtruth findings text rather than the image alone, making the task a text summarization problem rather than image‚Äëgrounded impression generation. This design may inflate performance and hinders fair comparison with prior image-only baselines.  \n3. **Baseline Construction and Fairness** ‚Äì Details for heatmap generation (alpha blending, kernels, opacity, occlusion handling) and textual‚Äëgaze formatting are insufficient, limiting reproducibility and fairness across gaze modalities.  \n4. **Dataset Splits and Contamination** ‚Äì The evaluation heavily depends on an ‚Äúalpha‚Äù split overlapping with MIMIC‚ÄëCXR training data, raising contamination concerns for medical and general baselines. Conclusions should rest primarily on the smaller but cleaner ‚Äúbeta‚Äù set, or on an external dataset.  \n5. **Statistical and Metric Rigor** ‚Äì The study reports no significance tests or confidence intervals despite small sample sizes. The composite ‚Äúscaled to CheXagent = 100‚Äù measure mixes heterogeneous metrics and can distort relative comparisons. Raw and per‚Äëmetric results (e.g., CheXbert, RadGraph, RaTEScore) with confidence intervals are needed.  \n6. **Diagnosis Evaluation Clarity** ‚Äì Mapping from generated text to CheXbert labels is under‚Äëspecified; the definition of ‚Äúdifferential diagnosis‚Äù in this context requires clearer justification.  \n7. **Presentation and Consistency** ‚Äì Equation (7) inconsistently cites denominators (‚ÄúLLaVA‚ÄëMed‚Äù vs. CheXagent); Table 2‚Äôs modality markings and ‚ÄúOverall‚Äù metric composition should be clearly described.  \n8. **Related Work** ‚Äì Discussion should include recent gaze‚Äëbased VLP and LVLM approaches (e.g., EGMA, MedGaze, CoRaX), as well as report‚Äëgeneration dialogue systems like RaDialog, to better contextualize contributions.  \n9. **Additional Analyses** ‚Äì Evaluate the influence of red‚Äëdot size, fixation filtering thresholds, and possible occlusion of findings. Include shuffled or ablated controls (order‚Äëonly vs. location‚Äëonly) to reinforce claims about the importance of temporal sequence.  \n10. **Broader Impact Considerations** ‚Äì Address variability and potential bias in gaze behaviors across expertise levels, privacy issues in gaze traces, and possible occlusion artifacts introduced by gaze overlays.\n\n---\n\n**Minor Comments**  \n- Clarify heatmap rendering parameters to ensure equitable baseline comparison.  \n- Correct labeling inconsistencies in formulae and tables, and standardize notation.  \n- Explicitly list which metrics are included in the ‚ÄúOverall‚Äù score and their weights.  \n- Provide external validation results where possible to support generalization.  \n- Several references to visual prompting and gaze‚Äëguided models could be added to situate the work more completely.\n\n---\n\n**Summary Paragraph**  \nThis paper proposes an elegant, lightweight method that exploits video representations of expert gaze to enhance LVLMs without additional training. The idea is clear and its implementation straightforward, offering practical value and reproducibility through code release. Nonetheless, the evaluation design leaves notable uncertainties: potential dataset contamination, reliance on derived rather than raw metrics, and absence of controls that prove temporal order is key. Strengthening experimental rigor, clarifying task inputs, reporting unscaled clinical metrics, and expanding discussion to recent related research would substantially increase the manuscript‚Äôs credibility and impact.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The concept and motivation are strong, but methodological and reporting issues must be addressed before the contribution can be considered robust.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# Review Report for \"RADEYEVIDEO: ENHANCING GENERAL-DOMAIN LARGE VISION LANGUAGE MODEL FOR CHEST X-RAY ANALYSIS WITH VIDEO REPRESENTATIONS OF EYE GAZE\"\n\n## 1. Opening Paragraph\n\nThis manuscript addresses the limitation of current approaches that integrate radiologists' eye gaze information into large vision-language models (LVLMs) for chest X-ray (CXR) analysis. While previous methods have incorporated eye gaze through static heatmaps or textual prompts, they fail to preserve the temporal sequence of eye movements that could provide valuable context about how radiologists prioritize different regions during interpretation. The authors propose RadEyeVideo, a novel approach that represents eye gaze as dynamic video sequences where a red dot follows the radiologist's actual fixation pattern across the CXR image. This method captures both spatial attention and temporal dynamics of gaze, creating a more comprehensive representation of expert visual behavior. The authors evaluate their approach on three general-domain LVLMs (LongVA, VideoLLaMA2, and LLaVA-OneVision) across CXR report generation (findings and impressions) and diagnosis tasks. Their results demonstrate significant performance improvements, with RadEyeVideo achieving up to 25.4% gain on the Impression generation task and an average 7.9% improvement across all tasks. Remarkably, when combined with exemplar reports for in-context learning, general-domain LVLMs with RadEyeVideo outperform medical models specifically trained for CXR analysis, including the state-of-the-art CXRMate model in the Impression section.\n\n## 2. Major and Minor Comments\n\n### Major Comments\n\n**Strengths:**\n- The paper presents a novel and conceptually simple approach that addresses a clear gap in current eye-gaze integration methods by preserving temporal dynamics.\n- The comprehensive evaluation across multiple models, tasks, and metrics provides strong empirical evidence for the effectiveness of the proposed method.\n- The ablation studies (frame numbers, gaze point filtering, in-context learning) provide valuable insights into the design choices and enhance methodological rigor.\n- The demonstration that general-domain LVLMs with RadEyeVideo can outperform medical-specific models is a significant contribution with practical implications for clinical AI deployment.\n\n**Limitations:**\n- The evaluation dataset is quite small, particularly the beta set with only 92 images, which limits the statistical power of the findings. The paper mentions this limitation but doesn't sufficiently address how it might affect the reliability of the reported performance gains.\n- The diagnosis task results are inconsistent across models (e.g., performance degradation with VideoLLaMA2), and the paper doesn't adequately explore why this occurs or discuss clinical implications of these inconsistencies.\n- While the paper claims the temporal sequence is critical, it doesn't provide direct evidence that the temporal aspect specifically (rather than just the visual salience of the video format) drives the performance improvement. A controlled experiment comparing video with shuffled gaze sequences would strengthen this claim.\n\n### Minor Comments\n\n**Strengths:**\n- The figures effectively illustrate the differences between prompting methods and showcase the quality improvements in generated reports.\n- The detailed description of the gaze video construction process provides clear methodology for replication.\n- The inclusion of computational efficiency analysis (Appendix A.1) addresses a practical concern about video processing overhead.\n\n**Limitations:**\n- The manuscript would benefit from more detailed discussion about the clinical relevance of the performance improvements beyond metric scores.\n- Some figures (e.g., Figure 3) contain text that is difficult to read at the resolution provided, making it hard to fully assess the qualitative improvements claimed.\n- The paper could better clarify why the video approach outperforms the fixation text method, given that both contain temporal information (though in different formats).\n- The discussion section would be strengthened by addressing potential implementation challenges in clinical settings where real-time eye-tracking might not be feasible.\n\n## 3. Evaluation Against TMI Editorial Criteria\n\n**Significance (High):** This work addresses an important challenge in medical AI‚Äîbridging the gap between general-purpose and medical-specific models. The demonstrated ability of general-domain LVLMs with RadEyeVideo to outperform medical models has significant implications for clinical deployment, as it reduces the need for specialized medical model training while maintaining or improving performance. The approach could be extended beyond CXR to other medical imaging modalities, potentially impacting various diagnostic workflows.\n\n**Innovation (High):** The core innovation of representing eye gaze as video sequences rather than static heatmaps or text is conceptually novel and effectively addresses the limitation of previous methods that discard temporal dynamics. The paper clearly demonstrates how this approach captures the expert's visual search strategy in a way that provides more meaningful context to LVLMs. The comparison across multiple gaze integration methods provides a comprehensive benchmark that advances the field.\n\n**Evaluation (Moderate-High):** The evaluation is generally strong, with multiple models, comprehensive metrics (both general lexical and radiology-specific), and ablation studies. However, the small test set size (92 images) limits the statistical robustness of the findings. While the paper includes thorough metric reporting, it would be strengthened by more qualitative analysis of the clinical relevance of the improvements and deeper investigation into why certain models benefit more than others from the video approach.\n\n**Reproducibility (High):** The paper provides clear methodological details for recreating the gaze videos and specifies all hyperparameters. The authors mention code availability in supplementary materials, and the use of publicly available models and datasets (MIMIC-CXR) enhances reproducibility. The computational efficiency analysis in the appendix further supports practical implementation.\n\n## 4. Recommendation\n\n**Major Revision (Reject/Resubmit)**\n\nWhile this paper presents a novel and potentially impactful approach with strong empirical results, several concerns need to be addressed before acceptance. The primary issues are the limited dataset size for evaluation and the lack of direct evidence that the temporal sequence (rather than just the visual format) drives performance improvements. The authors should:\n\n1. Provide additional analysis that specifically isolates the contribution of temporal information (e.g., comparing with shuffled gaze sequences)\n2. Address the inconsistency in diagnosis task performance across models with deeper analysis\n3. Include a more thorough discussion of clinical relevance beyond metric scores\n4. Expand the discussion on implementation challenges in real clinical settings\n\nThese revisions would strengthen the paper's methodological rigor and clinical relevance, making it a more compelling contribution to the field. The core idea is promising and the current results are encouraging, but additional analysis is needed to fully substantiate the claims about the importance of temporal gaze sequence and the clinical applicability of the approach.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**\n\nThe manuscript presents *RadEyeVideo*, a method for enhancing general-domain large vision‚Äìlanguage models (LVLMs) in chest X-ray (CXR) analysis by integrating radiologists‚Äô eye gaze as dynamic video representations. Unlike prior approaches using static heatmaps or text-based prompts, this method retains the temporal progression of gaze, potentially capturing radiologists‚Äô interpretive workflows. The authors evaluate RadEyeVideo across three LVLMs (LongVA, VideoLLaMA2, and LLaVA-OneVision) on report generation and diagnostic tasks. Reported results show up to 25.4% improvement in impression generation and an average 7.9% gain overall, with general-domain LVLMs augmented by RadEyeVideo surpassing medical-specific models such as CXRMate. The manuscript is clearly written, methodologically detailed, and addresses an important problem in multimodal medical AI.\n\n---\n\n**Major Comments**\n\n1. **Conceptual and Empirical Strengths**  \n   - The approach is conceptually simple yet novel, addressing the overlooked aspect of temporal dynamics in gaze-based model conditioning.  \n   - The experiments are extensive, covering multiple models, metrics, and ablations (e.g., frame number, gaze filtering, in-context learning), which increases confidence in the findings.  \n   - Demonstrating that general-domain LVLMs can outperform domain-specialized models has notable implications for cost-effective clinical AI deployment.\n\n2. **Methodological and Analytical Limitations**  \n   - The test dataset is small‚Äîparticularly the 92-image beta set‚Äîraising concerns about statistical robustness. While acknowledged, its influence on result stability is not thoroughly examined.  \n   - Diagnosis results vary across models (e.g., degraded performance with VideoLLaMA2), and the causes or implications of these inconsistencies are insufficiently analyzed.  \n   - Although the importance of maintaining temporal gaze sequence is central to the paper‚Äôs claim, no controlled experiment distinguishes temporal ordering from general visual salience. A shuffled-sequence comparison could clarify this.\n\n---\n\n**Minor Comments**\n\n- Figures effectively illustrate differences between gaze representations, but some (e.g., Figure 3) are difficult to read due to small text.  \n- The methodology section is detailed and facilitates reproducibility; inclusion of computational efficiency analysis is appreciated.  \n- The discussion would benefit from addressing clinical relevance beyond metric gains and recognizing potential implementation constraints in settings lacking real-time eye-tracking.  \n- Clarify why gaze video outperforms fixation-text prompts, given that both encode temporal information differently.\n\n---\n\n**Summary Paragraph**\n\nOverall, this work is significant and innovative in bridging general-domain and medical LVLMs through dynamic gaze integration. The methodological design and evaluation are strong, though constrained by a small dataset and limited interpretive analysis of certain results. The paper‚Äôs contributions are reproducible, well-documented, and suggest promising directions for broader medical imaging applications. Strengthening evidence for the role of temporal sequence and providing deeper discussion of clinical applicability would markedly improve the paper‚Äôs impact.\n\n---\n\n**Decision Recommendation**\n\n**Major Revision.**  \nThe study offers a promising and well-executed approach, but acceptance should await additional analyses validating the temporal component‚Äôs contribution, addressing performance inconsistencies, and expanding clinical context discussion.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**## A summary of the paper**\n\nThe manuscript tackles the under‚Äëexploited information contained in radiologists‚Äô eye‚Äëgaze sequences when interpreting chest X‚Äërays (CXRs). Using the MIMIC‚ÄëEye resource, which supplies fixation coordinates and dwell times for 2,298 posterior‚Äëanterior CXRs, the authors generate a video prompt‚Äînamed **RadEyeVideo**‚Äîthat superimposes a red dot at the gaze location and samples frames in proportion to fixation duration. This construction preserves both spatial and temporal aspects of visual attention. Three publicly available large vision‚Äëlanguage models (LVLMs) capable of ingesting video input (LongVA, VideoLLaMA2, and LLaVA‚ÄëOneVision) are then evaluated on report‚Äëgeneration (Findings, Impression) and disease‚Äëdiagnosis tasks, with in‚Äëcontext exemplar reports supplied. Compared with a baseline that omits eye‚Äëgaze information, the RadEyeVideo prompt yields up to a 25.4‚ÄØ% improvement for Impression generation and an average increase of 7.9‚ÄØ% across all evaluated tasks. The main contribution is the introduction of a dynamic eye‚Äëgaze video prompting strategy that injects expert visual attention into existing LVLMs, demonstrating measurable gains in CXR reporting and diagnosis.\n\n---\n\n**## General feedback**\n\n- **Significance:** Demonstrating that temporal eye‚Äëgaze data can boost off‚Äëthe‚Äëshelf LVLMs for radiology is an encouraging step toward more human‚Äëcentered AI. It suggests a path for broader adoption of general‚Äëpurpose multimodal models within clinical workflows.  \n\n- **Innovation:** Encoding scan‚Äëpaths as a video prompt is a fresh perspective compared with earlier static heat‚Äëmaps or textual representations of gaze. The approach is conceptually elegant, yet it relies on a relatively simple overlay and uniform frame sampling; no new model architecture or training objective is introduced.  \n\n- **Evaluation:**  \n  - The experimental dataset is modest (‚âà2.3‚ÄØk images) and the ‚Äúbeta‚Äù validation/test split contains only **92** images (Table‚ÄØ1). This raises concerns about statistical robustness and the possibility of data leakage, given that the split mixes training and test images.  \n  - Performance is reported using a **scaled‚Äëmetric** (Eq.‚ÄØ7) anchored to CheXagent, which inflates the numbers and makes it difficult to gauge the raw improvements; raw metric values are not consistently presented.  \n  - The reported average gain (~8‚ÄØ%) is modest, and some cases show a performance drop (e.g., diagnosis with VideoLLaMA2). No confidence intervals, p‚Äëvalues, or bootstrapped estimates are provided.  \n  - The ablation study (Figure‚ÄØ4) examines only frame count, gaze‚Äëpoint filtering, and the choice of in‚Äëcontext exemplars; other design choices‚Äîsuch as dot size, motion smoothing, or alternative sampling strategies‚Äîremain unexplored.  \n\n- **Reproducibility:** The authors state that code for reconstructing the eye‚Äëgaze videos is included in the supplementary material, yet the raw gaze data cannot be released because of MIMIC‚ÄëCXR licensing restrictions. Detailed preprocessing steps (e.g., fixation‚Äëduration threshold, fps‚ÄØ=‚ÄØ10, radius‚ÄØ=‚ÄØ5‚ÄØpx), LVLM inference hyper‚Äëparameters, and the scaling procedure are described only in narrative form, which hampers exact replication.\n\n---\n\n**## Specific comments / critiques**\n\n1. **Dataset split and potential leakage** ‚Äì Section‚ÄØ3.1 describes a mixture of the MIMIC‚ÄëCXR training split with the test split (Œ±/Œ≤), and the Œ≤ split comprises just 92 images (Table‚ÄØ1). A clean, fully held‚Äëout test set would help avoid contamination and enable reliable statistical analysis.  \n\n2. **Metric scaling methodology** ‚Äì Equation‚ÄØ7 uses CheXagent as a reference to produce scaled scores (Table‚ÄØ3). This non‚Äëstandard scaling inflates the numbers and masks the true magnitude of improvement. The manuscript should also report the raw ROUGE, BERTScore, CheXbert, RadGraph, and RaTEScore values alongside the scaled versions.  \n\n3. **Statistical significance absent** ‚Äì No confidence intervals, p‚Äëvalues, or bootstrapping are presented for the reported 7.9‚ÄØ% average gain or the 25.4‚ÄØ% boost on Impression, despite the very small test set. Providing statistical validation is essential to substantiate these claims.  \n\n4. **Limited ablation scope** ‚Äì Figure‚ÄØ4 investigates only the number of frames (16 selected) and a simple duration‚Äëbased fixation filter (Eq.‚ÄØ1). Other video‚Äëencoding parameters‚Äîdot radius, colour, motion smoothing, frame‚Äërate variations‚Äîare not examined, leaving open whether the observed gains are specific to the chosen settings.  \n\n5. **Baseline coverage** ‚Äì The comparison includes CheXagent, CXRMate, CXR‚ÄëLLaVA, and LLaVA‚ÄëMed, but omits several recent state‚Äëof‚Äëthe‚Äëart medical LVLMs (e.g., MedChat‚ÄëXL, Flamingo‚ÄëMed). Moreover, the paper does not evaluate these models after fine‚Äëtuning on the eye‚Äëgaze data, which would give a fuller picture of the benefit provided by the video prompt.  \n\n6. **Clinical relevance unclear** ‚Äì A 7‚Äì8‚ÄØ% increase on scaled metrics may not translate into clinically meaningful improvements. The manuscript would benefit from an error analysis linking metric gains to changes in critical decisions (e.g., reduction of missed pneumonias).  \n\n7. **Privacy and bias considerations** ‚Äì Although an ethical statement is present, the paper does not discuss the privacy implications of releasing processed gaze trajectories nor the potential bias stemming from a single radiologist cohort. These issues warrant further discussion.  \n\n8. **Reproducibility details missing** ‚Äì No concrete repository URL is provided, and the preprocessing pipeline (frame‚Äësampling formula Eq.‚ÄØ2, fixation‚Äëfiltering threshold Eq.‚ÄØ1) is described only in prose. Supplying a runnable script with clear documentation would greatly aid replication.  \n\n9. **Presentation quality** ‚Äì The manuscript contains several typographical errors (e.g., ‚ÄúSingificant‚Äù, ‚Äúpoint‚Äëf‚Äù), inconsistent naming (RADEYEVIDEO vs. RadEyeVideo), and fragmented numbered line blocks that hinder readability. Figures‚ÄØ1,‚ÄØ2,‚ÄØ5,‚ÄØ6 and Tables‚ÄØ3‚Äë9 are crowded, making extraction of key numbers difficult.\n\n---\n\n**## A suggested decision**\n\n**Reject**  \nIn its present form the manuscript falls short of the originality and methodological rigor required for publication. Substantial revisions‚Äîparticularly regarding dataset handling, statistical validation, broader baselines, and reproducibility‚Äîwould be needed before it could be reconsidered.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 0,
          "specificity": 0,
          "correctness": 0,
          "constructiveness": 0,
          "stance": 0,
          "source": ""
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript explores how radiologists‚Äô eye‚Äëgaze information can be leveraged to enhance large vision‚Äëlanguage models (LVLMs) for chest X‚Äëray (CXR) interpretation. Using the MIMIC‚ÄëEye dataset containing fixation coordinates and dwell times, the authors generate dynamic video prompts (‚ÄúRadEyeVideo‚Äù) that embed gaze patterns as temporally weighted overlays. These videos are input to several off‚Äëthe‚Äëshelf LVLMs‚ÄîLongVA, VideoLLaMA2, and LLaVA‚ÄëOneVision‚Äîfor both report generation and disease‚Äëdiagnosis tasks. The study reports up to a 25‚ÄØ% improvement in certain metrics compared with static baselines. The paper is conceptually appealing and clearly written overall, but several methodological and reporting deficiencies undermine confidence in the results.\n\n---\n\n**Major Comments**  \n1. **Dataset Split and Leakage Risk** ‚Äì The Œ±/Œ≤ splits merge training and testing data, with the Œ≤ split including only 92 images. Such small and mixed partitions jeopardize statistical validity and may introduce data contamination. A fully held‚Äëout test set is needed.  \n2. **Metric Scaling Methodology** ‚Äì The use of a scaled metric (Eq.‚ÄØ7) anchored to CheXagent inflates performance values and obscures true gains. Raw scores for ROUGE, BERTScore, CheXbert, RadGraph, and RaTEScore should be reported alongside the scaled results.  \n3. **Lack of Statistical Significance** ‚Äì Reported improvements (7.9‚ÄØ% average, 25.4‚ÄØ% peak) are not supported by confidence intervals, p‚Äëvalues, or bootstrapping, making it difficult to assess robustness.  \n4. **Limited Ablation Study** ‚Äì Only frame count and gaze‚Äëpoint filtering are analyzed. Parameters such as dot radius, color, frame rate, and motion smoothing are not investigated, leaving unclear which design factors drive the observed effect.  \n5. **Incomplete Baseline Coverage** ‚Äì Several advanced medical LVLMs are omitted, and no experiments examine fine‚Äëtuning these models on the gaze data. Broader comparisons would strengthen claims of generality.  \n6. **Clinical Relevance** ‚Äì The modest metric gains may not correspond to meaningful clinical improvements; an error analysis linking metrics to diagnostic outcomes would be valuable.  \n7. **Privacy and Bias** ‚Äì The manuscript does not adequately discuss privacy risks of releasing gaze trajectories or the potential bias from a single radiologist cohort.  \n8. **Reproducibility Limitations** ‚Äì The description of preprocessing and scaling is largely narrative, with no accessible repository or executable scripts, hindering replication.\n\n---\n\n**Minor Comments**  \n- Various typographical and naming inconsistencies (e.g., ‚ÄúSingificant,‚Äù inconsistent capitalization of RadEyeVideo).  \n- Crowded figures and tables (particularly Figs.‚ÄØ1‚Äì2,‚ÄØ5‚Äì6; Tables‚ÄØ3‚Äì9) make numerical interpretation difficult.  \n- Presentation would benefit from streamlined formatting and clearer labeling of in‚Äëtext references.\n\n---\n\n**Summary Paragraph**  \nThe work introduces an intuitive method for incorporating expert gaze patterns into multimodal models and demonstrates potentially useful performance gains. However, weaknesses in dataset handling, unorthodox evaluation metrics, limited ablation scope, and missing reproducibility details substantially reduce confidence in the findings. While the concept of human‚Äëinformed video prompting is intriguing, the current empirical evidence and presentation do not meet the expected standard of rigor.\n\n---\n\n**Decision Recommendation**  \n**Reject** ‚Äì The manuscript requires major methodological and statistical revisions, expanded baselines, and improved reproducibility before reconsideration.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Honghan Wu",
      "Jinge Wu",
      "Yunsoo Kim"
    ],
    "url": "pdfs/iclr.cc-2025-conference_a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "remote_url": "https://openreview.net/pdf/a0b2ca0250a54184bdc4f3a8b21c5719e2ee4f83.pdf",
    "venue_id": "iclr.cc-2025-conference"
  },
  {
    "title": "Learning Robust Representations for Medical Images via Unifying (Self-)Supervisions",
    "status": "completed",
    "evaluators": [
      "Yixuan",
      "Guang/Zhenxuan"
    ],
    "primary_area": [
      "unsupervised, self-supervised, semi-supervised, and supervised representation learning"
    ],
    "keywords": [
      "medical image pre-training",
      "medical image representation learning"
    ],
    "abstract": "Pre-training medical image encoder to provide robust, task-agnostic representations is highly valuable, as it enhances the understanding of medical images and is important for performing many data-scarce analysis tasks. Current pre-training works are unable to integrate various types of supervisions, including self-supervision and external supervision such as segmentation annotations, while they are highly valuable for medical image understanding. Therefore, in this paper, we take the first step toward exploring unifying all common types of supervisions into a pre-training framework through a same scalable way. This require the pre-training framework being both unified, for accommodating diverse data and extensible, and effective, for making heterogeneous data synergistically assist unknown downstream tasks. To this end, we propose UmiF, whose principle is that once converted into token embeddings in a unified space, all diverse supervisions can be effectively utilized via contrastive learning and mask modeling with a same way. With UmiF, we pre-train on 1.66M samples from 14 public datasets, significantly surpassing previous efforts in terms of the dataset scale. We obtain and release the UmiF model, which achieved state-of-the-art performance across various downstream tasks, including classification, segmentation, and detection, retrieval and VQA.",
    "decision": "Reject",
    "reviews": [
      {
        "text": "### Summary\n\nThe Authors propose UmiF, a framework that aims to unify pre-training of any arbitrary Image-Label pair to train a robust encoder for any modality. To achieve this the authors propose three tokenizers, that each allows embedding to a shared unified token space (One Image, One Text, One Image Labels/Segmentation). These image / label tokens are either left split or merged to a certain degree, before being used for SSL training through contrastive training in a CLIP'esque fashion or through a reconstruction task. \nThey train their model on a wide variety of paired pre-training datasets and evaluate it on a broad set of downstream tasks, highlighting the final performance of UmiF's method.\n\nWhile innovative, the experiments are insufficient to highlight the proposed methodology. The authors stack a) a larger pre-training dataset b) Token mixing c) Masked and Contrastive losses together and don't provide experiments that disentangle which part brings performance and which part does not. Moreover, the presentation and language used in this paper are of insufficient quality and need a lot of work.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 3\n\n### Strengths\n\nThe proposal to not only include Image-Report pairs but also Images with other Supervision signals is an interesting and to my knowledge novel premise for the medical domain. Their idea of mixing image and supervision tokens are also innovative.\nMoreover, the amount of experiments conducted is broad, highlighting the generality of the ViT feature extractor.\n\n### Weaknesses\n\nWhile the premises are highly interesting the paper in it's current form has some issues:\n1. **Stacking of contributions** Currently the authors stack a variety of things on-top and don't ablate it properly, namely a) a larger pre-training dataset b) the token mixing block and c) the multiple SSL losses. Currently, it is impossible for a reader to know if their methodology is better downstream than the competing methods, as they create a much larger training data corpus. Maybe it's the masked reconstruction component, maybe not.\n2. **Presentation** The presentation of the paper as of right now is poor. It was very hard to read, as the language quality leaves a lot of room for improvement and should be checked by an English speaker to rework the manuscript. Moreover, Fig. 1 does a bad job of explaining the method contributing to difficulties in understanding the proposed method. Figures in the appendix are badly presented: Fig. 2 text is way too large.  Table 3 does not have a caption, The description of the Image-Segment Dataset (Section A.4) is basically non-existent and should be filled accordingly. \n3. **Reproducibility** Currently the author's don't sufficiently explain their configuration of their methods. How was _r_ chosen ? In the text it is mentioned sometimes 1 and sometimes random between [0,1]. How did the authors split their data? Was there a train-test split during pre-training and fine-tuning? Was there different weightings between the losses?\n\n### Minor Points\n- The ablation of _r_ values does not contain r of 0.9 and 1.0 - It's mentioned in the text that these performed substantially worse, but I would like to have these values included in the table. Moreover this table should provide not only RSNA 1% AUC values. The downstream adaptation are just learning of a linear-layer so please provide ablations on more datasets and all values to show if the mixing of tokens actually provides a benefit.\n- The distinction between what this paper does relative to other paper's feels not well worked out. It would help a lot to see what makes this work distinct.\n- Similarities to MedUniC Paper. This paper's Table 2 is very similar to their Table 2 -- I would prefer to highlight this in the caption. \n- There are so many typos in this manuscripts. E.g. spellings of baseline methods:  MedKLIIP/MedKILP/MedKLIP. It feels like no one proof-read this paper ever.\n- The Algorithm 1 is way too text heavy. If the authors want to go into detail about the sampling of their datasets they should move this into a separate algorithm to keep readability high.\n- The authors mention the importance of sampling smaller dataset more regularly but provide no results. Would be great to see an ablation table on this claim in the appendix.\n\n### Questions\n\n- Q1: Did you ablate the importance of using the masking and the contrastive loss by themselves?\n- Q2: How was _r_ chosen?\n- Q3: Shouldn't r be symmetric around 0.5? r=0 would just flip image embeddings to supervision and the other way around. Is this correct, or if not why not? \n- Q4: Have you tried replacing the mixing with a standard masking/token drop-out layer? Would be interesting to see if one actually has to mix tokens or if the dropping of tokens provides a similar regularizing effect.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 4",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a framework designed to unify the pre-training of diverse image‚Äìlabel pairs to produce a robust, modality-agnostic encoder. The approach employs three tokenizers to embed images, text, and image labels/segmentations into a shared token space for self-supervised learning using contrastive and reconstruction objectives. The authors train on multiple paired datasets and evaluate on a range of downstream tasks to demonstrate generality. While the conceptual premise is novel and the experimental coverage broad, the paper suffers from weak presentation and limited experimental analysis that prevents a clear understanding of each component‚Äôs contribution to overall performance.\n\n**Major Comments**  \n1. **Disentangling Contributions** ‚Äì The method combines several elements (large pre-training dataset, token mixing, and multiple self-supervised losses) without adequate ablation. The lack of controlled experiments makes it unclear which components drive performance improvements relative to baselines.  \n2. **Presentation Quality** ‚Äì The manuscript is difficult to read due to poor language and unclear explanations. Figures are inconsistently formatted and inadequately described. Figure‚ÄØ1 fails to clearly convey the model workflow; Figure‚ÄØ2 text is oversized; Table‚ÄØ3 lacks a caption; and the dataset description in Appendix‚ÄØA.4 is incomplete.  \n3. **Reproducibility and Experimental Detail** ‚Äì Important configuration details are missing. The selection of the hyperparameter *r* is inconsistently described (sometimes fixed, sometimes random). The data splitting strategy for pre-training and fine-tuning is unclear, as are the weighting schemes for different losses. These omissions limit replicability.\n\n**Minor Comments**  \n- Include additional *r* values (e.g., 0.9 and 1.0) in the ablation table and report full downstream results on more datasets.  \n- Clarify how this work differs from related methods, especially MedUniC, and explicitly note similarities in Table‚ÄØ2.  \n- Correct numerous typographical errors, especially inconsistent spellings of baseline names.  \n- Simplify Algorithm‚ÄØ1 or split it for readability.  \n- Provide ablation results for the sampling frequency of smaller datasets.  \n\n**Summary Paragraph**  \nThe concept of unifying visual pre-training across different supervision types is original and promising for the medical domain. However, the current version lacks clarity, methodological transparency, and sufficient ablation to support its claims. The writing quality and figure presentation require significant improvement for readers to follow the technical contributions. These issues collectively reduce both interpretability and reproducibility, overshadowing the paper‚Äôs innovative potential.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The framework is interesting and broadly applicable, but substantial revisions to experimental analysis and presentation are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper presents UmiF, a pre-training framework for medical image encoders that integrates multiple types of supervision, including self-supervision and annotations like segmentation labels, into a unified approach. UmiF creates a common embedding space with a token grouping strategy to leverage diverse data types for various downstream tasks. Pre-trained on 1.66 million samples from 14 public datasets, UmiF was evaluated in classification, segmentation, detection, retrieval, and VQA tasks.\n\n### Soundness: 2\n\n### Presentation: 2\n\n### Contribution: 2\n\n### Strengths\n\n- The paper introduces unifying representations from multiple supervisions into a single embedding space for self-supervised learning and proposes a grouping strategy for mixed learning of representation vectors.\n\n- The model‚Äôs effectiveness is validated through evaluations across four different downstream tasks.\n\n### Weaknesses\n\n- **Contrastive Learning Design Concerns**: The design of the contrastive learning setup after grouping raises questions. According to the paper, a positive pair is represented by \\(f1_i, f0_j\\) where i and j are indices from different data points, meaning \\(f1_i\\) and \\(f0_j\\) are from different samples. Typically, a positive pair should be \\(f1_i, f0_i\\), where both elements come from the same sample, making the current approach unclear.\n\n- **Unfair Comparisons in Downstream Tasks**: There are substantial fairness issues in the downstream task comparisons. Competing models, such as Med-Unic and MGCA, are pre-trained on datasets with 380K and 217K samples respectively, whereas this study uses 1.66 million data pairs, including 1 million images. The model‚Äôs performance advantage in downstream tasks may stem from this large data disparity, making it difficult to attribute improvements solely to the proposed pre-training strategy.\n\n- **Performance in Table 2**: In Table 2, despite using more training data and supervision than Med-Unic, the proposed model does not achieve the best performance, which raises questions about the efficiency of the approach.\n\n- **Limited Ablation Study on Parameter r**: In the ablation study on the parameter r, only 1% of the RSNA dataset is used, rather than the full dataset, and no similar experiments are conducted on other datasets. It is unclear if the chosen r value on RSNA is robust and generalizable to other tasks, as this limited evaluation does not provide strong evidence of robustness.\n\n- **Inconsistencies Between Text and Figures**: There are inconsistencies between the text and figures. For instance, the text describes vector groups as Group 1 and Group 0, but the figure labels them as Group 1 and Group 2. \n\n- **CLS Token Generation Unclear**: The generation of the CLS token information is not clearly explained.  According to the figure, the CLS token appears to be an output of the Flexible Token Grouping, but the paper does not specify how the CLS token is produced. Further clarification on this process would improve understanding.\n\n### Questions\n\nplease refer to Weaknesses\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 3\n\n### Confidence: 5",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified pre‚Äëtraining framework for medical image encoders that combines multiple supervision types‚Äîincluding self‚Äësupervision and annotated signals such as segmentation labels‚Äîwithin a shared embedding space. The approach leverages a token‚Äëgrouping strategy to align heterogeneous data for diverse downstream applications. The model is pre‚Äëtrained on 1.66‚ÄØmillion samples from 14‚ÄØpublic datasets and evaluated across classification, segmentation, detection, retrieval, and VQA tasks. The paper is generally clear in describing its objectives and experiments, and it demonstrates broad empirical coverage.\n\n---\n\n**Major Comments**  \n1. **Contrastive Learning Setup** ‚Äì The design of the contrastive learning stage is unclear. The paper indicates that a positive pair consists of \\(f1_i, f0_j\\) from different data points, whereas positive pairs should typically share the same sample (i.e., \\(f1_i, f0_i\\)). Clarification is required to understand the intended formulation.  \n2. **Fairness of Downstream Comparisons** ‚Äì Comparisons with Med‚ÄëUnic and MGCA may not be equitable. Those methods were pre‚Äëtrained on substantially smaller datasets (380‚ÄØK and‚ÄØ217‚ÄØK samples) compared to UmiF‚Äôs‚ÄØ1.66‚ÄØmillion pairs, including‚ÄØ1‚ÄØmillion images. The reported advantage may therefore partly reflect dataset scale rather than modeling improvements.  \n3. **Performance in Table‚ÄØ2** ‚Äì Despite using more data and supervision, UmiF does not surpass all baselines such as Med‚ÄëUnic. This calls into question whether the proposed framework is efficiently leveraging its additional information.  \n4. **Ablation on Parameter‚ÄØr** ‚Äì The ablation uses only‚ÄØ1% of the RSNA dataset, with no replication on other datasets. Thus, the chosen‚ÄØr‚ÄØvalue‚Äôs robustness and generality are uncertain.  \n5. **Text‚ÄìFigure Inconsistencies** ‚Äì The text names vector groups as ‚ÄúGroup‚ÄØ1‚Äù and ‚ÄúGroup‚ÄØ0,‚Äù whereas the figure shows ‚ÄúGroup‚ÄØ1‚Äù and ‚ÄúGroup‚ÄØ2,‚Äù leading to confusion.  \n6. **CLS‚ÄØToken Description** ‚Äì The process generating the CLS‚ÄØtoken remains ambiguous. The figure suggests it arises from Flexible Token Grouping, but the text does not specify how it is produced.\n\n---\n\n**Minor Comments**  \n- Alignment between text and figures should be checked for consistent terminology.  \n- Clarify the notation and dataflow in diagrams to improve readability.\n\n---\n\n**Summary Paragraph**  \nThe paper‚Äôs main strength lies in proposing a unified pre‚Äëtraining strategy that integrates heterogeneous supervision signals for medical image analysis and in demonstrating results on multiple downstream tasks. However, key methodological elements‚Äîparticularly the contrastive learning formulation, fairness of comparisons, and limited ablation coverage‚Äîundermine the strength of the empirical claims. Some inconsistencies and unclear descriptions also affect presentation clarity.\n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The framework is promising, but methodological ambiguities and unbalanced evaluations need substantial clarification and additional experiments before the contribution can be adequately assessed.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "### Summary\n\nThis paper introduces a multi-supervision unification strategy for medical image pretraining. The method allows report, segmentation, and classification (+ some others) types of supervision to jointly train one representation. The used modality is Chest X-ray (CXR). The authors collected a large-scale dataset sourced from the public domain, reaching 1M images and 1.66M supervision labels. It is reported the model, namely, UmiF reaches SOTA for a number of downstream tasks.\n\n### Soundness: 2\n\n### Presentation: 1\n\n### Contribution: 2\n\n### Strengths\n\n1. The authors have gone through tremendous effort in collecting and inventorying the datasets. I can imagine the implementation wouldn't be easy to iron out the differences in the datasets and put them together to train one model. For that, I believe the significance of the paper should be pointed out.\n\n2. the benchmarking is comprehensive, ranging over the common medical image analysis tasks.\n\n### Weaknesses\n\n1. The clarity of the paper is a concern. Many places in the text lack proper explanation and are somewhat confusing.  For example, L230 \"This interesting design allows more diverse views and enriches the learning tasks with many possibilities, surpassing previous VL learning approaches.\", The authors should clearly state why it is interesting. What are the many possibilities? What are the other diverse views (isn't the modality just CXR)? What evidence indicates your method surpasses the previous VL learning approach?\n2. Furthermore, Sec 3.2, perhaps the most important section in the paper is not well written, I've read it a few times and I still don't believe I have grasped the exact approach. \n3. I find Figure 1 hard to follow, the quantities in Sec 3.2 should be mapped to the figure. I also don't get the colour coding in Figure 1 for those tokens. The yellow/blue/no boundary cubes are also a very confusing way of presentation.\n4. The improvement over the previous state-of-the-art is marginal around 1 point in various measurements. As the authors claim a large-scale dataset of 1.66M image-supervision pairs vs \"previous effort of mostly limited to 380K image-report pairs or 838K images\", it is worth rethinking whether the effort spent on training such a large model on the twice amount of data makes sense.\n5. The title claims \"learning robust representation for medical images ...\", medical images are not just CXR, I would recommend claiming a lesser scope unless common modalities such as MRI/CT are also used.\n6. In Sec 3.1, the authors use \"modality abstraction\", which sounds cool but I would say it is actually confusing, the procedure is a label format conversion.\n\n### Questions\n\nPlease address weaknesses #2&4. The paper could use some professional editing services.\n\n### Flag For Ethics Review\n\n- No ethics review needed.\n\n### Rating: 5\n\n### Confidence: 3",
        "reviewer": "human",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a multi-supervision unification framework, termed **UmiF**, for medical image pretraining using chest X-ray (CXR) data. The approach integrates multiple types of supervision‚Äîreport, segmentation, and classification‚Äîto jointly train a single representation. The authors compile a large-scale dataset comprising approximately 1 million images and 1.66 million supervision labels from publicly available sources. Experimental results indicate that UmiF achieves state-of-the-art performance across several downstream medical imaging tasks. While the topic and ambition of the work are notable, the clarity and explanatory depth of key methodological sections raise concerns about comprehensibility and interpretability.  \n\n**Major Comments**  \n1. **Clarity and Explanation**: Several parts of the manuscript, such as the description around line 230 and Section 3.2, lack clear justification or illustrative explanation. Phrases like ‚Äúinteresting design‚Äù and ‚Äúmore diverse views‚Äù are vague and not sufficiently supported by evidence or examples. The reviewer reports difficulty understanding the exact approach after multiple readings.  \n2. **Presentation of Figures**: Figure 1 is difficult to follow. The quantitative elements described in Section 3.2 should be directly mapped to the figure, and the current color coding (yellow/blue/no boundary cubes) adds confusion rather than clarity.  \n3. **Magnitude of Improvement**: The reported performance gains over state-of-the-art methods are minimal (approximately one point across metrics). Given the significant effort and data scale increase‚Äîfrom 380K to 1.66M image-label pairs‚Äîthe practical justification for this large-scale training should be reconsidered.  \n4. **Scope of Claims**: The title suggests representation learning for ‚Äúmedical images‚Äù broadly, but experiments use only chest X-rays. The claim should be limited accordingly unless additional modalities such as MRI or CT are included.  \n5. **Terminology**: The term ‚Äúmodality abstraction‚Äù in Section 3.1 is potentially misleading and appears to refer primarily to label format conversion.  \n\n**Minor Comments**  \n- The paper could benefit from professional language editing to improve readability.  \n\n**Summary Paragraph**  \nThe study introduces an ambitious and comprehensive data unification for medical image pretraining, demonstrating strong effort in dataset curation and broad benchmarking. However, core concerns about clarity of exposition, interpretability of figures, and limited performance improvements relative to data scale reduce the paper‚Äôs overall impact. Refining presentation and clarifying methodology would significantly strengthen the work.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äî The concept and dataset scale are promising, but substantial revisions to improve clarity, presentation, and scope are necessary before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "**Phase 1: Analytical and Scope Audit**\n\n1. **Scope & Relevance to TMI**  \n‚úÖ The manuscript presents a methodological contribution in medical imaging: a unified pre‚Äëtraining framework (UmiF) for medical image encoders that integrates multiple supervision types (self‚Äësupervision, segmentation, classification, and vision‚Äëlanguage signals). This is model- and representation-learning methodology directly within TMI scope.\n\n2. **Novelty & Contribution Level**  \nThe work claims to be the first to unify four major forms of supervision in medical image pre‚Äëtraining. The central innovations are (1) a unified token space for diverse modalities (image, text, segmentation mask) and (2) a ‚Äúflexible token grouping‚Äù strategy enabling various contrastive and masked modeling tasks simultaneously. While the conceptual idea‚Äîunifying multimodal supervision‚Äîis inspired by recent general-domain vision‚Äëlanguage models (e.g., OFA, BEiT‚Äëstyle unification), its systematic adaptation to heterogeneous medical datasets constitutes a meaningful advance for the domain. The degree of novelty is moderate‚Äëto‚Äëhigh for medical imaging methodology.\n\n3. **Technical and Experimental Rigor**  \nTechnical formulation is sound and internally consistent. Loss functions for contrastive and consistency learning are detailed. Experimental evaluation is broad‚Äîcovering classification, segmentation, detection, VQA, and zero‚Äëshot tasks‚Äîon public datasets totaling 1.66‚ÄØM pairs. Comparisons to state‚Äëof‚Äëthe‚Äëart (MGCA, Med‚ÄëUniC, MRM) are appropriate. A potential concern is the uniform data harmonization using GPT‚Äë4 translation and report generation, which may introduce bias or unverified text quality; this should be discussed more quantitatively. Reproducibility could also be strengthened by specifying tokenization vocabulary sizes, exact backbone configuration, and ablation hyperparameters.\n\n4. **Clarity and Presentation**  \nThe manuscript is generally well structured and readable. Major figures and tables are clear, though minor grammar issues remain (‚Äúto to acquire‚Äù, ‚Äúbeing both unified, for accommodating diverse data and extensible, and effective‚Äù etc.). Sections are logically organized; however, some sentences are overly long, and acronyms (e.g., UmiF) could be introduced earlier.\n\n5. **Ethical and Reproducibility Compliance**  \nAll datasets are public, and IRB risk is minimal. Authors state intent to release code and checkpoints upon acceptance and clearly acknowledge limitations and bias risks‚Äîappropriate under TMI policy.\n\n---\n\n**Phase 2: IEEE‚ÄØTMI Review Report**\n\n**1. Summary**  \nThis paper proposes *UmiF*, a unified medical‚Äëimage pre‚Äëtraining framework that jointly leverages self‚Äësupervision (contrastive and mask modeling) and external supervision (text reports, captions, class labels, segmentation masks) to learn robust task‚Äëagnostic representations. The approach converts all modalities into a shared token space via modality‚Äëspecific tokenizers and processes them through a single Vision Transformer. A flexible token‚Äëgrouping mechanism randomly partitions tokens across modalities to form paired views for contrastive learning and mask modeling, effectively fusing heterogeneous supervision types. Pre‚Äëtraining on 1.66‚ÄØM samples from 14 datasets, UmiF achieves consistent state‚Äëof‚Äëthe‚Äëart performance across five downstream tasks.\n\n**2. Strengths**\n- Clear methodological innovation in unifying diverse supervision sources under one scalable backbone.  \n- Comprehensive empirical validation across multiple downstream tasks and data‚Äëefficiency regimes.  \n- Solid conceptual grounding linking self‚Äësupervised and vision‚Äëlanguage paradigms.  \n- Demonstrates potential for generalized medical foundation models.\n\n**3. Weaknesses**\n- Limited theoretical justification for why flexible token grouping yields synergy; mostly empirical.  \n- Heavy reliance on GPT‚Äë4‚Äëgenerated or translated reports introduces potential data and linguistic bias not formally evaluated.  \n- Experiments restricted to 2D X‚Äëray data; unclear how easily the approach extends to other modalities (CT, MRI, pathology).  \n- Some reproducibility details (data splits, tokenizer parameters, random seeds) are missing.  \n- Writing quality could be improved for clarity and grammar.\n\n**4. Major Comments**\n1. **Clarify the contribution boundary** ‚Äì Provide more explicit comparison to general-domain unified frameworks (OFA, BEiT, Painter), explaining which design changes are uniquely required for medical data.  \n2. **Quantify generated‚Äëtext quality** ‚Äì Include metrics (BLEU, ROUGE, or radiologist manual ratings) to confirm the reliability of GPT‚Äëgenerated translations and synthetic reports.  \n3. **Explain scalability beyond X‚Äëray** ‚Äì Discuss adaptations needed for volumetric or multi‚Äëchannel modalities.  \n4. **Expand reproducibility details** ‚Äì Document tokenizer vocabulary sizes, ViT configuration, and sampling probabilities.  \n5. **Statistical analysis** ‚Äì Include standard deviation or confidence intervals for performance metrics to support claims of superiority.\n\n**5. Minor Comments**\n- Proofread for redundant words and grammatical errors.  \n- In tables, denote backbone sizes consistently (e.g., ViT‚ÄëB/16).  \n- Add visual examples showing outputs from the grouping strategy to aid intuitive understanding.  \n- Verify citation format consistency.  \n- Clarify timing and ratio of text versus mask tokens during training.\n\n**6. Overall Recommendation:** **Major Revision**  \nThe paper presents an interesting and potentially impactful methodological contribution within TMI scope. However, it requires better justification of novelty relative to existing multimodal frameworks, quantitative validation of synthetic data quality, and detailed reproducibility information. With these revisions, it could reach publishable quality.\n\n**7. Confidence Level:** 4‚ÄØ/‚ÄØ5  \n\n**Summary Decision:** *Major Revision ‚Äì Methodologically promising, requires stronger experimental transparency and clarification of novelty and generalization scope.*",
        "reviewer": "ai/gpt-5-chat-latest",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 2,
          "constructiveness": 4,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified medical image pre‚Äëtraining framework designed to integrate multiple forms of supervision‚Äîself‚Äësupervised, segmentation, classification, and vision‚Äëlanguage‚Äîinto a single representation‚Äëlearning paradigm. It converts diverse inputs into a shared token space through modality‚Äëspecific tokenizers and employs a flexible token‚Äëgrouping mechanism for contrastive and masked modeling objectives. The work is methodologically well aligned with the field of medical image analysis and presents a moderate‚Äëto‚Äëhigh level of novelty for this domain. The manuscript is clearly structured and largely readable, though minor grammatical and stylistic issues remain.\n\n---\n\n**Major Comments**  \n1. **Clarify Contribution vs. Prior Frameworks** ‚Äì Differentiate UmiF more explicitly from general‚Äëdomain unified models such as OFA, BEiT, or Painter, detailing which architectural or training modifications are specifically motivated by medical data characteristics.  \n2. **Evaluate Generated Text Quality** ‚Äì Since the study relies on GPT‚Äë4‚Äëproduced or translated radiology reports, provide quantitative or qualitative validation (e.g., BLEU, ROUGE, or expert review) to assess the reliability of these text sources and potential bias.  \n3. **Scalability Across Modalities** ‚Äì Experiments focus on 2D X‚Äëray datasets. Discuss how the framework and tokenization strategy would generalize to other imaging modalities such as CT, MRI, or pathology slides.  \n4. **Reproducibility Details** ‚Äì Report tokenizer vocabulary sizes, backbone configuration, sampling probabilities, and hyperparameter settings. Include random seeds and data‚Äësplit information to improve transparency.  \n5. **Statistical Reporting** ‚Äì Add confidence intervals or standard deviations for performance metrics to substantiate the performance claims and facilitate fair comparison.  \n6. **Justification for the Token‚ÄëGrouping Mechanism** ‚Äì Provide clearer theoretical or empirical explanation for why flexible token grouping produces synergistic effects beyond simple multi‚Äëtask learning.\n\n---\n\n**Minor Comments**  \n- Revise grammatical inconsistencies (e.g., ‚Äúto to acquire‚Äù) and overly long sentences.  \n- In tables, standardize model designations (e.g., ViT‚ÄëB/16).  \n- Include illustrative examples of the grouping operation for intuition.  \n- Ensure consistent citation formatting and acronym introduction.  \n- Clarify the proportion and scheduling of text versus mask tokens during training.\n\n---\n\n**Summary Paragraph**  \nThis study presents a compelling framework that unifies heterogeneous supervision signals for medical image pre‚Äëtraining and demonstrates broad empirical strength across five downstream tasks. The conceptual contribution and experimental scope are substantial, yet further validation of synthetic text data, clearer distinction from general‚Äëdomain models, and enhanced reproducibility documentation are required. Addressing these revisions will considerably strengthen the manuscript‚Äôs transparency, credibility, and general applicability.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe work is promising and methodologically sound but necessitates additional clarification, validation, and completeness before being ready for acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Synopsis of the paper\n\nThis paper proposes UmiF, a unified medical image pre-training framework that integrates diverse supervision types including self-supervision, paired reports, captions, segmentation annotations, and classification labels. The method converts all input modalities (radiology, language, segmentation masks) into a unified token space using modality-specific tokenizers, then applies a flexible token grouping strategy to enable contrastive learning and mask modeling simultaneously. The authors compile a large-scale dataset of 1.66M samples from 14 public datasets, significantly exceeding previous efforts. UmiF uses Vision Transformer as backbone and trains with both alignment loss for contrastive learning and consistency loss for mask modeling. Experiments demonstrate state-of-the-art performance across multiple downstream tasks including classification, segmentation, detection, retrieval, and VQA on medical imaging datasets.\n\n## Weaknesses\n\n‚Ä¢ **Mathematical formulation clarity and consistency issues**\n  - Equation 1 uses inconsistent indexing notation where positive pairs are denoted as (f1i, f0j) but the loss function ‚Ñì(fi, fj) uses different subscript conventions without clear mapping\n  - The binary sampling strategy in Section 3.2 lacks mathematical rigor in defining the probability distribution for sampling r values beyond stating \"certain probability\" and \"randomly sampled from [0, 1]\"\n  - Equation 2's consistency loss formulation doesn't clearly specify the relationship between teacher network input Concat(Xi, Xs) and the masked student inputs X0, X1\n\n‚Ä¢ **Insufficient technical novelty and contribution clarity**\n  - The core tokenization approach directly borrows from existing works (Section 3.1 acknowledges similarity to Wang et al., 2022b; Zhang et al., 2023) without substantial methodological innovation\n  - The flexible token grouping strategy, while presented as novel, essentially reduces to standard vision-language learning when r=1 (page 5, lines 218-223), suggesting limited conceptual advancement\n  - The unification claim is overstated given that only three modality types are handled, and the abstraction is relatively straightforward (converting coordinates to masks, labels to text templates)\n\n‚Ä¢ **Experimental design and evaluation limitations**\n  - Table 2-5 comparisons include methods with different backbone architectures (ResNet-50 vs ViT-B/16), making performance comparisons potentially unfair without proper architectural controls\n  - The ablation study in Table 7 only examines single supervision types versus all combined, lacking systematic analysis of pairwise combinations to understand synergistic effects\n  - Zero-shot evaluation is limited to a single dataset (CXP500 with 500+ images), providing insufficient evidence for generalization claims across diverse medical imaging scenarios\n\n‚Ä¢ **Dataset processing and experimental rigor concerns**\n  - The reliance on GPT-4 for Spanish-to-English translation and report generation (Section 2, Table 1) introduces potential biases and quality inconsistencies not adequately validated\n  - Algorithm 1 and Section 4.1 provide insufficient implementation details regarding the \"certain probability\" for r=1 setting, batch construction specifics, and convergence criteria\n  - The segmentation mask generation process (bottom of Figure 1) lacks technical details about color assignment, resolution matching, and potential information loss during coordinate-to-mask conversion\n\n## Suggestions for Improvement\n\n‚Ä¢ **Enhance mathematical rigor and notation consistency**\n  - Standardize indexing notation throughout Equation 1 by clearly defining the relationship between batch indices i,j and the positive pair construction\n  - Provide explicit probability distributions for the flexible token grouping strategy, including mathematical formulation of the sampling process for r values\n  - Clarify the mathematical relationship in Equation 2 between complete teacher inputs and masked student inputs, potentially with a formal algorithm description\n\n‚Ä¢ **Strengthen technical contributions and positioning**\n  - Conduct deeper analysis comparing the proposed tokenization approach with existing methods, highlighting specific advantages beyond scalability\n  - Provide theoretical or empirical analysis of when and why the flexible grouping strategy outperforms standard vision-language approaches beyond the r=1 case\n  - Expand the modality abstraction framework to handle additional medical data types (e.g., 3D volumes, multi-sequence MRI) to better support the \"unified\" framework claims\n\n‚Ä¢ **Improve experimental comprehensiveness and fairness**\n  - Include systematic comparisons using identical backbone architectures across all baseline methods, or provide separate architectural ablation studies\n  - Design comprehensive ablation studies examining all pairwise and subset combinations of supervision types to quantify individual and synergistic contributions\n  - Expand zero-shot evaluation to multiple datasets and include few-shot learning scenarios to better demonstrate generalization capabilities across medical imaging domains\n\n‚Ä¢ **Strengthen dataset validation and implementation transparency**\n  - Validate GPT-4 generated translations and reports through expert radiologist review or automated quality metrics comparison with original annotations\n  - Provide complete implementation details including hyperparameter sensitivity analysis, convergence behavior, and computational requirements for reproducibility\n  - Include technical validation of the coordinate-to-mask conversion process, potentially comparing with direct coordinate-based supervision methods to justify the design choice",
        "reviewer": "ai/claude-sonnet-4-20250514",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 4,
          "constructiveness": 5,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **UmiF**, a unified medical image pre-training framework that integrates multiple supervision types‚Äîself-supervision, paired reports, captions, segmentation masks, and classification labels‚Äîwithin a single learning structure. The approach uses modality-specific tokenizers to map diverse inputs into a shared token space and applies a flexible token grouping strategy enabling both contrastive and mask modeling objectives. The study compiles a large-scale dataset of 1.66M samples across 14 public sources and reports state-of-the-art performance on various downstream medical imaging tasks. While the manuscript is ambitious and well-motivated, several issues related to mathematical formulation, novelty, and evaluation rigor limit its current clarity and impact.  \n\n---\n\n**Major Comments**  \n1. **Mathematical and Notational Clarity**  \n   - Equations contain inconsistent indexing and undefined notation, particularly in Equation 1 and the sampling-based strategy of Section 3.2.  \n   - The probability distribution for the variable *r* is vaguely defined, and the relationship between teacher and student network inputs in Equation 2 remains unclear.  \n\n2. **Technical Novelty and Conceptual Contribution**  \n   - The core tokenization process closely follows prior works (e.g., Wang et‚ÄØal., 2022b; Zhang et‚ÄØal., 2023) with limited methodological innovation.  \n   - The proposed flexible token grouping reduces to standard vision-language setups when *r‚ÄØ=‚ÄØ1*, suggesting only modest conceptual advancement.  \n   - The claim of ‚Äúunification‚Äù appears overstated, as the framework currently supports only three modality types through straightforward transformations.  \n\n3. **Experimental Design and Evaluation Limitations**  \n   - Reported comparisons use inconsistent backbone architectures, weakening performance fairness.  \n   - The ablation study assesses only individual versus combined supervision, omitting analyses of intermediate or pairwise combinations.  \n   - Zero-shot evaluation is confined to a single small dataset, providing insufficient evidence of generalization.  \n\n4. **Dataset Processing and Implementation Transparency**  \n   - The use of GPT‚Äë4 for translation and report generation may introduce unverified biases.  \n   - Implementation details (e.g., sampling probability, batch construction, convergence criteria) are inadequately documented.  \n   - The conversion from coordinates to segmentation masks is insufficiently described, leaving potential for information loss.  \n\n---\n\n**Minor Comments**  \n- Ensure consistent notation and indexing across formulas.  \n- Define all sampling probabilities explicitly.  \n- Improve figure captions and clarify mask generation in Figure‚ÄØ1.  \n\n---\n\n**Summary Paragraph**  \nOverall, the paper addresses an important objective‚Äîintegrating diverse supervision types for medical imaging pre-training‚Äîand demonstrates promising performance across tasks. However, its impact is limited by unclear mathematical definitions, modest methodological novelty, incomplete ablation analyses, and insufficient documentation of data processing and implementation details. Strengthening theoretical justification, ensuring fair experimental comparisons, and improving transparency would substantially enhance the manuscript‚Äôs rigor and reproducibility.  \n\n---\n\n**Decision Recommendation**  \n**Major Revision** ‚Äì Substantial clarification and additional experiments are required before the contribution can be convincingly evaluated.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## Opening Paragraph\n### This manuscript introduces UmiF, a unified medical image pre-training framework that integrates various types of supervisions, including self-supervisions and external supervisions such as segmentation annotations and class labels. UmiF aims to pre-train robust, task-agnostic representations for medical images by converting diverse input pairs into a unified token space and employing a novel flexible token grouping strategy for contrastive learning and mask modeling. The authors claim that UmiF surpasses previous efforts in terms of dataset scale and performance across multiple downstream tasks. The manuscript is well-written, with a clear articulation of the motivation and technical details.\n\n###\n\n## Major Comments\n1. Novelty and Positioning: While UmiF introduces a unique approach to integrating multiple supervisions, the novelty is somewhat diminished by existing works that have explored similar ideas. The authors should provide a clearer distinction between UmiF and existing methods, especially those that incorporate multi-modal pre-training or multi-task learning. The manuscript lacks a thorough discussion on how UmiF's contributions are differentiated from related work, which could strengthen the argument for its significance.\n   \n2. Evaluation Design: The evaluation is conducted on a diverse set of datasets, which is commendable. However, the authors should consider expanding the evaluation to include additional medical imaging modalities beyond 2D X-rays, such as CT scans or MRI images. This would provide a more comprehensive assessment of UmiF's versatility and generalization capabilities. Additionally, the inclusion of more real-world, prospective datasets would bolster the practical applicability of the proposed framework.\n\n3. Comparisons: The manuscript compares UmiF against a range of baselines, but some relevant recent works, particularly those focusing on vision-language pre-training and multi-modal contrastive learning, are missing. Including these comparisons would offer a more comprehensive view of UmiF's performance relative to state-of-the-art methods. The authors should ensure that the chosen baselines are representative of the latest advancements in the field.\n\n4. Reproducibility: The authors mention that the code and model checkpoints will be released upon acceptance, which is positive. However, the manuscript could benefit from a more detailed description of the training protocols, preprocessing steps, and hyperparameters. Providing this information upfront would enhance the reproducibility of the results and facilitate future research.\n\n###\n\n## Minor Comments\n1. Figures and Tables: Figures 1 and 2 are somewhat cluttered and could be improved by showing fewer representative examples with zoomed-in regions to enhance readability.\n2. Notation and Terminology: Some notation is introduced without sufficient explanation, particularly in Section 2.1. Clarifying these definitions would aid in the understanding of the technical details.\n3. Acronyms: Several acronyms are used without definition, such as \"ROC\" and \"SIIM.\" Providing a glossary or defining these terms would improve clarity.\n4. Typographical Issues: There are a few typographical errors throughout the manuscript, such as \"k-spacce\" (p. 6) and \"undersampling maskes\" (p. 7). These should be corrected for a polished presentation.\n\n###\n\n## Summary Paragraph\nThe manuscript addresses a significant challenge in medical image analysis: creating robust, task-agnostic representations that can generalize well across various downstream tasks. UmiF's approach to integrating diverse supervisions into a unified framework is innovative and technically sound. However, the evaluation could be strengthened by including a broader range of datasets and modalities. The reproducibility of the results is contingent on the completeness of the methodological details, which currently lack some crucial information. Overall, while the idea holds promise, the manuscript would benefit from expanded comparisons and a more comprehensive evaluation to fully meet the standards of rigor expected for publication in TMI.\n\n###\n\n## Decision Recommendation\nMajor revision. The authors should expand the comparative analysis to include more recent and relevant methods, broaden the evaluation to include a wider range of datasets and modalities, and provide detailed methodological information to ensure reproducibility.",
        "reviewer": "ai/hkust-reviewer",
        "metrics": {
          "coverage": 3,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "human"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript presents **UmiF**, a unified medical image pre-training framework that combines multiple forms of supervision, including self-supervised, segmentation-based, and class-label guidance. Its goal is to generate robust, task-agnostic image representations by mapping heterogeneous data into a shared token space and applying a flexible token grouping strategy for contrastive learning and mask modeling. The paper is well-organized and clearly explains the motivation and technical approach, positioning UmiF as an advance in large-scale medical image pre-training.\n\n---\n\n**Major Comments**  \n1. **Novelty and Positioning**: While the integration of multiple supervision sources is interesting, the novelty is somewhat constrained due to prior work exploring similar concepts. The distinction between UmiF and related frameworks‚Äîparticularly in multi-modal or multi-task pre-training‚Äîshould be articulated more clearly. A deeper discussion situating the method within existing literature would strengthen its claimed contributions.  \n2. **Evaluation Design**: The evaluation covers several datasets, which is a strong point, but is limited primarily to 2D X-ray images. Expanding the experiments to include modalities such as CT and MRI would better demonstrate generalization and robustness. Incorporating real-world or prospective datasets could further validate the practical utility of UmiF.  \n3. **Comparative Analysis**: The comparison section omits some recent and relevant baselines, notably those involving vision-language or multi-modal contrastive approaches. Including these would provide a more complete and current performance benchmark. The authors should ensure that the baselines represent the latest state-of-the-art.  \n4. **Reproducibility**: Although code release upon acceptance is noted, the manuscript lacks sufficient methodological transparency. Detailed descriptions of training protocols, preprocessing steps, and hyperparameters would enhance reproducibility and usefulness for the research community.\n\n---\n\n**Minor Comments**  \n1. **Figures and Tables**: Figures 1 and 2 appear crowded; simplifying layouts or adding zoomed regions would improve clarity.  \n2. **Notation**: Some symbols introduced in Section 2.1 are insufficiently defined; clearer explanations are needed.  \n3. **Acronyms**: Terms such as ‚ÄúROC‚Äù and ‚ÄúSIIM‚Äù are undefined and should be clarified or listed in a glossary.  \n4. **Typos**: Minor errors (e.g., ‚Äúk-spacce,‚Äù ‚Äúundersampling maskes‚Äù) should be corrected.\n\n---\n\n**Summary Paragraph**  \nOverall, the manuscript proposes a promising framework for unified medical image pre-training and is presented with technical clarity. Its main strengths lie in the ambition to unify multiple supervision sources and in the sound methodological formulation. However, the claims of generalization and superiority would be more convincing with broader multi-modality experiments, updated comparisons, and fuller methodological transparency. Addressing these issues would substantively improve the paper‚Äôs rigor and reproducibility.\n\n---\n\n**Decision Recommendation**  \n**Major Revision.** The authors should expand the comparative analysis, include additional imaging modalities and datasets, and provide more detailed implementation information to substantiate the proposed framework‚Äôs contributions.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## LEARNING ROBUST REPRESENTATIONS FOR MEDICAL IMAGES VIA UNIFYING (SELF-)SUPERVISIONS\n\n### Summary\n\nThis paper proposes UmiF, a unified pre-training framework for medical image encoders that integrates heterogeneous supervision signals‚Äîself-supervision, paired language (reports/captions), classification labels, and segmentation annotations‚Äîthrough a single, scalable approach. The key idea is to convert all signals into a shared token space (radiology images, language, and segmentation masks) and apply a flexible token grouping strategy that yields two complementary views for contrastive learning and masked modeling using a single ViT backbone. Pre-trained on 1.66M pairs from 14 public datasets, UmiF reports strong or state-of-the-art performance on multiple downstream tasks, including classification, segmentation, detection, zero-shot classification, retrieval, and VQA.\n\n### Strengths\n\n- Technical novelty and innovationUnifies multiple supervision types (self-supervision, reports/captions, labels, segmentation) into one tokenized space and a single backbone without bespoke task-specific heads.The flexible token grouping ratio r elegantly generalizes CLIP-style cross-modal contrastive learning to mixed-modality token subsets and integrates masked modeling with a BYOL-style teacher, providing a principled mechanism to span from pure VL to mixed intra/inter-modal self-supervision.Treating segmentation annotations as an image modality to enable an image-centric tokenization path is a pragmatic and extensible design.\n- Experimental rigor and validationEvaluates across a broad suite of downstream tasks: linear classification, semantic segmentation, object detection, zero-shot classification, and VQA.Includes ablations on token grouping probability and supervision-type inclusion to probe the synergy of heterogeneous signals.\n- Clarity of presentationPresents a coherent high-level framework with clear motivation for unification and a conceptual figure illustrating tokenization, grouping, and training objectives.Clearly lists datasets and pretraining setup at a high level.\n- Significance of contributionsAddresses a highly relevant problem in medical imaging: leveraging diverse, small, and heterogeneously annotated datasets for robust pretraining.If released, the reported model could serve as a community resource across multiple downstream tasks, potentially reducing the need for task-specific pretraining.\n\nTechnical novelty and innovation\n\n- Unifies multiple supervision types (self-supervision, reports/captions, labels, segmentation) into one tokenized space and a single backbone without bespoke task-specific heads.\n- The flexible token grouping ratio r elegantly generalizes CLIP-style cross-modal contrastive learning to mixed-modality token subsets and integrates masked modeling with a BYOL-style teacher, providing a principled mechanism to span from pure VL to mixed intra/inter-modal self-supervision.\n- Treating segmentation annotations as an image modality to enable an image-centric tokenization path is a pragmatic and extensible design.\n\nExperimental rigor and validation\n\n- Evaluates across a broad suite of downstream tasks: linear classification, semantic segmentation, object detection, zero-shot classification, and VQA.\n- Includes ablations on token grouping probability and supervision-type inclusion to probe the synergy of heterogeneous signals.\n\nClarity of presentation\n\n- Presents a coherent high-level framework with clear motivation for unification and a conceptual figure illustrating tokenization, grouping, and training objectives.\n- Clearly lists datasets and pretraining setup at a high level.\n\nSignificance of contributions\n\n- Addresses a highly relevant problem in medical imaging: leveraging diverse, small, and heterogeneously annotated datasets for robust pretraining.\n- If released, the reported model could serve as a community resource across multiple downstream tasks, potentially reducing the need for task-specific pretraining.\n\n### Weaknesses\n\n- Technical limitations or concernsThe segmentation supervision is relatively small (‚âà2K samples) compared to the overall corpus; claims about segmentation-driven gains may be confounded by the broader framework rather than true exploitation of segmentation annotations.Using RGB mask images for segmentation supervision is simple but may be suboptimal; it may encode class colors as strong priors without principled spatial alignment objectives.The design omits local alignment objectives (e.g., patch-to-token grounding) seen in related VLP works and may underutilize fine-grained supervision.\n- Experimental gaps or methodological issuesFairness of comparisons: UmiF pretrains on a substantially larger and more heterogeneous dataset than some baselines; apples-to-apples comparisons (e.g., pretrain on a single standardized corpus like MIMIC-CXR as in BenchX) are missing.Token grouping ablations are limited in scope (primarily one metric/setting); broader analysis across tasks and datasets would better justify the design.The ablation ‚Äúonly one supervision type‚Äù is confounded by drastically different data scales (e.g., segment data 2K vs. others hundreds of thousands), making ‚Äúsynergy‚Äù conclusions difficult to isolate from sample size effects.The approach to object detection using YOLOv3 with a ViT backbone is under-specified; how features are adapted is unclear, raising concerns about reproducibility.\n- Clarity or presentation issuesThe contrastive loss description ambiguously denotes positive pairs as (f1_i, f0_j); it should clarify that positives are derived from the same input pair (i=j) and specify projection heads if any.Limited details on the BYOL teacher (momentum, predictors, projection heads) and on the training recipe (initialization of ViT and tokenizers, augmentation policies).The batch sampling strategy first claims balancing across datasets but then samples proportionally to dataset size, which contradicts the balancing goal.\n- Missing related work or comparisonsLimited discussion of several close and recent efforts that unify or structure multimodal medical pretraining (e.g., IMITATE, CXR-CLIP, MedFLIP, BenchX framework and findings, DeViDe‚Äôs multi-granularity alignment, and MDT/IRENE-style unified token encoders).No controlled comparisons under standardized protocols (e.g., BenchX) to disentangle method gains from pretraining corpus differences.\n\nTechnical limitations or concerns\n\n- The segmentation supervision is relatively small (‚âà2K samples) compared to the overall corpus; claims about segmentation-driven gains may be confounded by the broader framework rather than true exploitation of segmentation annotations.\n- Using RGB mask images for segmentation supervision is simple but may be suboptimal; it may encode class colors as strong priors without principled spatial alignment objectives.\n- The design omits local alignment objectives (e.g., patch-to-token grounding) seen in related VLP works and may underutilize fine-grained supervision.\n\nExperimental gaps or methodological issues\n\n- Fairness of comparisons: UmiF pretrains on a substantially larger and more heterogeneous dataset than some baselines; apples-to-apples comparisons (e.g., pretrain on a single standardized corpus like MIMIC-CXR as in BenchX) are missing.\n- Token grouping ablations are limited in scope (primarily one metric/setting); broader analysis across tasks and datasets would better justify the design.\n- The ablation ‚Äúonly one supervision type‚Äù is confounded by drastically different data scales (e.g., segment data 2K vs. others hundreds of thousands), making ‚Äúsynergy‚Äù conclusions difficult to isolate from sample size effects.\n- The approach to object detection using YOLOv3 with a ViT backbone is under-specified; how features are adapted is unclear, raising concerns about reproducibility.\n\nClarity or presentation issues\n\n- The contrastive loss description ambiguously denotes positive pairs as (f1_i, f0_j); it should clarify that positives are derived from the same input pair (i=j) and specify projection heads if any.\n- Limited details on the BYOL teacher (momentum, predictors, projection heads) and on the training recipe (initialization of ViT and tokenizers, augmentation policies).\n- The batch sampling strategy first claims balancing across datasets but then samples proportionally to dataset size, which contradicts the balancing goal.\n\nMissing related work or comparisons\n\n- Limited discussion of several close and recent efforts that unify or structure multimodal medical pretraining (e.g., IMITATE, CXR-CLIP, MedFLIP, BenchX framework and findings, DeViDe‚Äôs multi-granularity alignment, and MDT/IRENE-style unified token encoders).\n- No controlled comparisons under standardized protocols (e.g., BenchX) to disentangle method gains from pretraining corpus differences.\n\n### Detailed Comments\n\n- Technical soundness evaluationThe unified tokenization and single-backbone design is sound and aligns with generalist transformer directions (OFA, BEiT-3, Painter), and the token grouping mechanism is a neat way to span discrete supervision regimes with one knob r. However, the absence of explicit local alignment objectives may limit the exploitation of fine-grained segmentation signals. The BYOL-style masked modeling with a full-view teacher is reasonable, but important implementation details are missing (e.g., projection/predictor heads, EMA rate), which significantly influence performance and stability.Converting bounding boxes to color-coded masks provides a simple path to harmonize modalities, but introducing fixed color encodings risks shortcut learning; masking/augmentation schemes for masks and analysis of color-coded bias would strengthen the approach.\n- Experimental evaluation assessmentThe breadth of downstream tasks is a strong point. Still, conclusions about superiority would be more convincing with standardized, controlled comparisons. BenchX highlights that protocol differences can invert method rankings; a MIMIC-only pretraining variant or reporting within BenchX would isolate methodological gains from data scale.The segmentation/detection improvements are promising, but without clearer architectural details (e.g., how ViT acts as a YOLO backbone) and stronger segmentation supervision (e.g., leveraging CheXmask or other pixel-level resources), it is hard to credit the improvements specifically to segmentation-aware pretraining rather than general large-scale pretraining and the mixed-view objectives.The ablations on supervision mixtures are informative but confounded by disparate sample sizes; a controlled ablation with matched sample counts per supervision type (or up/downsampling to equalize) would better support the ‚Äúsynergy‚Äù claim.Zero-shot classification and VQA improvements are modest but consistent; please report prompt templates and calibration protocols for zero-shot to ensure reproducibility and fair comparisons.\n- Comparison with related work (using the summaries provided)IMITATE introduces hierarchical text‚Äìvision alignment and a clinical-informed contrastive loss to avoid hard negatives among semantically similar cases; UmiF could benefit from similar ‚Äúsoft target‚Äù ideas to better handle near-duplicate or synonym-rich labels and cross-dataset concept drift.CXR-CLIP uses multi-view and modality-specific contrastive terms (image‚Äìimage and text‚Äìtext) to strengthen study-level invariance; UmiF‚Äôs token grouping could incorporate intra-modality contrastive terms as additional regularizers.MedFLIP integrates MAE-style masking in VLP and uses SVD-based alignment; UmiF‚Äôs masked modeling via BYOL might be complemented with structured alignment losses or entity-aware supervision to bolster VL grounding.DeViDe shows substantial zero-shot gains through knowledge augmentation and multi-granularity alignment; while UmiF aims to be task-agnostic, considering patch‚Äìtext local alignment for improved grounding could further enhance fine-grained performance.IRENE/MDT demonstrates unified token processing for heterogeneous clinical data with explicit bi-directional cross-attention; UmiF‚Äôs single-backbone approach is conceptually similar but lacks explicit cross-attention blocks or analyses that show token-level fusion quality. Discussing such architectural trade-offs would clarify the unique contribution of the flexible grouping mechanism.BenchX underscores the necessity of standardized pretraining/evaluation to fairly compare MedVLP methods; including a MIMIC-only pretraining comparison or reporting within BenchX would substantially strengthen claims of SOTA.\n- Discussion of broader impact and significanceUnifying diverse medical supervisions within a single, extensible framework is impactful, especially for medical domains where annotations are heterogeneous and sparse. If models and code are released, UmiF could serve as a common foundation for diverse downstream tasks, reducing the proliferation of task-specific pretraining schemes. Ethical considerations around synthetic text generation (e.g., GPT-4 generated reports) and potential biases introduced by multi-institutional data mixing should be further detailed, including procedures to avoid test-set leakage and to handle multilingual consistency.\n\nTechnical soundness evaluation\n\n- The unified tokenization and single-backbone design is sound and aligns with generalist transformer directions (OFA, BEiT-3, Painter), and the token grouping mechanism is a neat way to span discrete supervision regimes with one knob r. However, the absence of explicit local alignment objectives may limit the exploitation of fine-grained segmentation signals. The BYOL-style masked modeling with a full-view teacher is reasonable, but important implementation details are missing (e.g., projection/predictor heads, EMA rate), which significantly influence performance and stability.\n- Converting bounding boxes to color-coded masks provides a simple path to harmonize modalities, but introducing fixed color encodings risks shortcut learning; masking/augmentation schemes for masks and analysis of color-coded bias would strengthen the approach.\n\nExperimental evaluation assessment\n\n- The breadth of downstream tasks is a strong point. Still, conclusions about superiority would be more convincing with standardized, controlled comparisons. BenchX highlights that protocol differences can invert method rankings; a MIMIC-only pretraining variant or reporting within BenchX would isolate methodological gains from data scale.\n- The segmentation/detection improvements are promising, but without clearer architectural details (e.g., how ViT acts as a YOLO backbone) and stronger segmentation supervision (e.g., leveraging CheXmask or other pixel-level resources), it is hard to credit the improvements specifically to segmentation-aware pretraining rather than general large-scale pretraining and the mixed-view objectives.\n- The ablations on supervision mixtures are informative but confounded by disparate sample sizes; a controlled ablation with matched sample counts per supervision type (or up/downsampling to equalize) would better support the ‚Äúsynergy‚Äù claim.\n- Zero-shot classification and VQA improvements are modest but consistent; please report prompt templates and calibration protocols for zero-shot to ensure reproducibility and fair comparisons.\n\nComparison with related work (using the summaries provided)\n\n- IMITATE introduces hierarchical text‚Äìvision alignment and a clinical-informed contrastive loss to avoid hard negatives among semantically similar cases; UmiF could benefit from similar ‚Äúsoft target‚Äù ideas to better handle near-duplicate or synonym-rich labels and cross-dataset concept drift.\n- CXR-CLIP uses multi-view and modality-specific contrastive terms (image‚Äìimage and text‚Äìtext) to strengthen study-level invariance; UmiF‚Äôs token grouping could incorporate intra-modality contrastive terms as additional regularizers.\n- MedFLIP integrates MAE-style masking in VLP and uses SVD-based alignment; UmiF‚Äôs masked modeling via BYOL might be complemented with structured alignment losses or entity-aware supervision to bolster VL grounding.\n- DeViDe shows substantial zero-shot gains through knowledge augmentation and multi-granularity alignment; while UmiF aims to be task-agnostic, considering patch‚Äìtext local alignment for improved grounding could further enhance fine-grained performance.\n- IRENE/MDT demonstrates unified token processing for heterogeneous clinical data with explicit bi-directional cross-attention; UmiF‚Äôs single-backbone approach is conceptually similar but lacks explicit cross-attention blocks or analyses that show token-level fusion quality. Discussing such architectural trade-offs would clarify the unique contribution of the flexible grouping mechanism.\n- BenchX underscores the necessity of standardized pretraining/evaluation to fairly compare MedVLP methods; including a MIMIC-only pretraining comparison or reporting within BenchX would substantially strengthen claims of SOTA.\n\nDiscussion of broader impact and significance\n\n- Unifying diverse medical supervisions within a single, extensible framework is impactful, especially for medical domains where annotations are heterogeneous and sparse. If models and code are released, UmiF could serve as a common foundation for diverse downstream tasks, reducing the proliferation of task-specific pretraining schemes. Ethical considerations around synthetic text generation (e.g., GPT-4 generated reports) and potential biases introduced by multi-institutional data mixing should be further detailed, including procedures to avoid test-set leakage and to handle multilingual consistency.\n\n### Questions for Authors\n\n- How are positive pairs chosen exactly for the SimCLR loss‚Äîare f1 and f0 from the same input pair (i=j)? Are there projection/predictor heads before computing similarities, and if so, what are their architectures?\n- What are the momentum, predictor, and projection head details for the BYOL teacher‚Äìstudent setup? Is there stop-gradient on the teacher, and what EMA decay is used?\n- How is the ViT backbone adapted for YOLOv3 in the detection experiments? Please detail the feature map conversion, strides, and any added necks/heads to ensure reproducibility and fair comparisons.\n- Did you ensure that downstream test images (e.g., RSNA, SIIM) were excluded from the pretraining corpus? Please describe the de-duplication and patient-level filtering protocols to avoid leakage.\n- For datasets where you generated synthetic reports from class labels or used GPT-4 translations/polishing, what prompts and quality controls were used? Do these synthetic texts risk overstating VL performance by injecting near-deterministic label phrases?\n- Can you provide controlled ablations where each supervision type (report/caption/class/segment) is matched for sample count to isolate the effect of supervision diversity from dataset scale?\n- Why was segmentation supervision limited to ~2K samples? Have you considered incorporating large-scale pseudo-segmentation resources (e.g., CheXmask) to better assess the patch-level benefits of your framework?\n- Could you evaluate UmiF in a standardized setting (e.g., MIMIC-only pretraining as in BenchX) to disentangle corpus-scale effects from methodological gains?\n\n### Overall Assessment\n\nThis paper tackles an important and challenging problem‚Äîunifying heterogeneous medical supervision sources in a single pretraining framework‚Äîand presents an intuitively appealing approach via a unified token space and a flexible token grouping mechanism that subsumes cross-modal contrast and masked modeling. The breadth of downstream evaluations and generally strong performance are compelling, and the framework appears extensible. However, the empirical validation falls short of fully supporting the strongest claims: comparisons are not controlled for pretraining corpus size/heterogeneity, some implementation details critical for reproducibility and interpretation are missing, and several ablations are confounded by large disparities in supervision sample sizes. Strengthening fairness and clarity‚Äîthrough standardized pretraining comparisons (e.g., MIMIC-only/BenchX), more rigorous ablations (balanced supervision subsets, broader token-grouping analyses), and fuller methodological detail‚Äîwould make the contribution more robust for a top-tier venue. Overall, the idea is promising and of high potential value to the community; with the suggested additions and clarifications, the work could be a strong candidate for publication.",
        "reviewer": "ai/stanford",
        "metrics": {
          "coverage": 4,
          "specificity": 3,
          "correctness": 4,
          "constructiveness": 3,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF**, a unified pre-training framework for medical image encoders that integrates multiple types of supervision‚Äîself-supervised, language (reports or captions), classification, and segmentation‚Äîwithin a single scalable model. All modalities are transformed into a shared token space, allowing contrastive and masked modeling through a single ViT backbone. Pre-trained on 1.66M image-language pairs across 14 datasets, the framework demonstrates strong performance on a wide range of downstream tasks, including classification, segmentation, detection, zero-shot transfer, retrieval, and VQA. The paper is clearly organized, with sound motivation for unifying heterogeneous supervisions, and presents its conceptual design and experiments coherently.\n\n---\n\n**Major Comments**\n\n1. **Technical soundness and novelty**  \n   - The unified tokenization and single-backbone setup is technically sound and aligns with recent generalist transformer approaches. However, the omission of local alignment objectives (e.g., patch‚Äìtoken grounding) limits exploitation of fine-grained segmentation signals.  \n   - While the token grouping mechanism elegantly interpolates supervision regimes, important implementation details (projection heads, momentum, EMA) are absent, affecting reproducibility.  \n   - The use of RGB segmentation masks may introduce color priors rather than spatial understanding.\n\n2. **Experimental design and fairness**  \n   - Comparisons are not strictly controlled: UmiF pretrains on larger and more heterogeneous data than baseline methods, limiting claims of superiority.  \n   - The ablations on supervision mixtures and grouping are informative but confounded by differing data scales; balanced-sample experiments would better assess ‚Äúsynergy‚Äù effects.  \n   - Details are missing on how the ViT backbone integrates with YOLOv3 for detection, leaving reproducibility gaps.\n\n3. **Relation to prior work**  \n   - The discussion omits several closely related unified or multimodal frameworks (e.g., IMITATE, CXR-CLIP, MedFLIP, DeViDe, BenchX, MDT/IRENE). Adding such comparisons, ideally under standardized protocols, would strengthen positioning and clarify distinct contributions.\n\n4. **Broader impact and ethical aspects**  \n   - The unification approach is well-motivated for the medical domain but would benefit from discussion of potential biases in synthetic or multilingual data, test set leakage safeguards, and governance of mixed-institution datasets.\n\n---\n\n**Minor Comments**\n\n- Clarify notation in the contrastive loss: confirm that positives come from the same pair (i‚ÄØ=‚ÄØj).  \n- Specify architectural and training details for the BYOL teacher‚Äìstudent setup (momentum, projection/predictor heads, stop-gradient policy).  \n- The sampling description (balanced vs. proportional to dataset size) is inconsistent.  \n- Report prompt templates and calibration protocols for zero-shot evaluation.  \n- Minor editorial clarifications on figure labels and dataset references would improve readability.\n\n---\n\n**Summary Paragraph**  \nOverall, UmiF provides a conceptually unified framework with notable breadth in experimental evaluation and convincing potential for broad application across medical imaging tasks. The approach‚Äôs strength lies in its generality and elegant integration of heterogeneous supervision. However, several claims are weakened by limited ablation control, absent technical details, and lack of standardized comparisons to contemporary baselines. Addressing experimental fairness and transparency, along with discussion of related multimodal pretraining efforts, would substantially reinforce the work‚Äôs rigor and reproducibility.\n\n---\n\n**Decision Recommendation:** **Major Revision**  \nThe paper presents an innovative and promising direction but requires more controlled experiments, methodological clarification, and contextual grounding to justify its strong performance claims and ensure reproducibility.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "# IEEE Transactions on Medical Imaging Review Report\n\n## 1. Opening Paragraph\n\nThis paper addresses the critical challenge of pre-training medical image encoders to provide robust, task-agnostic representations that can enhance understanding of medical images and support diverse downstream analysis tasks with limited data. Current pre-training methods fail to integrate various types of supervisions (including self-supervision and external supervision like segmentation annotations, reports, and classification labels) that collectively could significantly improve medical image understanding. The authors propose UmiF (Unified medical image pre-training framework), a novel approach that unifies all common supervision types through a unified token space and flexible token grouping strategy. The framework processes diverse inputs (image-report, image-caption, image-class, and image-segment pairs) by converting them into token embeddings and randomly splitting tokens into groups for contrastive learning and mask modeling. Pre-trained on an unprecedented scale of 1.66M samples from 14 public datasets, UmiF demonstrates state-of-the-art performance across multiple downstream tasks including classification, segmentation, detection, retrieval, and visual question answering, showing significant improvements over existing methods by leveraging the synergistic effects of heterogeneous supervision signals.\n\n## 2. Major and Minor Comments\n\n### Major Strengths:\n- The paper presents a truly unified framework that can effectively integrate all common supervision types (self-supervision, language, and segmentation) through a cohesive approach rather than specialized designs for each data type\n- The large-scale pre-training dataset (1.66M samples from 14 public datasets) significantly surpasses previous efforts and provides a valuable resource for the community\n- The flexible token grouping strategy is innovative and effectively enables diverse learning tasks from a unified framework\n- Comprehensive evaluation across multiple downstream tasks with thorough ablation studies demonstrates the framework's versatility\n- The work addresses a critical need in medical imaging where diverse annotation types exist but are rarely leveraged together\n\n### Major Limitations:\n- The framework is only validated on chest X-ray images without discussion of how it might generalize to other medical imaging modalities (e.g., MRI, CT)\n- Limited discussion of computational requirements (training time, memory usage, inference speed) which is critical for clinical deployment\n- Insufficient exploration of clinical applicability and how the framework could be integrated into real-world clinical workflows\n- No analysis of performance variations across different disease categories or severity levels, which is clinically relevant\n- The paper doesn't sufficiently address potential limitations in low-resource settings or how the approach might handle noisy annotations\n\n### Minor Strengths:\n- Clear and well-designed figures that effectively illustrate the framework architecture and data processing\n- Thorough ablation studies on the flexible token grouping strategy that provide valuable insights\n- Good documentation of dataset processing and standardization procedures\n- The release of code and model checkpoints enhances reproducibility and community impact\n\n### Minor Limitations:\n- Some technical sections (particularly Section 3) could be made more accessible to non-expert readers\n- The abstract could be more concise while preserving key methodological details\n- Limited discussion about the optimal ratio of different supervision types in the pre-training data\n- Figure captions could be more descriptive to help readers interpret visual elements without constant reference to the main text\n\n## 3. Evaluation Summary\n\n**Significance**: The work addresses a highly significant problem in medical imaging AI - the inability of current pre-training methods to integrate diverse supervision signals. By unifying various supervision types into one framework, UmiF can leverage the complementary strengths of different annotation types, which is particularly valuable in medical domains where data is scarce but diverse annotations exist. This approach has potential to substantially improve the performance of medical AI systems across numerous applications.\n\n**Innovation**: The paper demonstrates high innovation through its unified framework that processes all common supervision types through a single scalable approach. The flexible token grouping strategy is particularly novel, enabling the same architecture to handle diverse learning tasks without modality-specific designs. The conceptual advance of treating all supervisions as token embeddings in a unified space represents a significant departure from previous medical pre-training methods that were limited to specific supervision types.\n\n**Evaluation**: The evaluation is comprehensive and rigorous, with state-of-the-art results across multiple downstream tasks (classification, segmentation, detection, VQA). The ablation studies on the flexible grouping strategy provide valuable insights into the framework's design choices. However, additional analysis of failure cases, performance across different disease categories, and computational requirements would strengthen the evaluation further.\n\n**Reproducibility**: The paper scores highly on reproducibility with detailed descriptions of dataset processing, pre-training configuration, and model architecture. The authors provide thorough documentation of their data processing pipeline and plan to release code and model checkpoints. The framework design is clearly explained, and the use of public datasets ensures others can replicate the results.\n\n## 4. Decision Recommendation\n\n**Major Revision**\n\nThis paper presents a valuable contribution to medical imaging AI with its unified pre-training framework that can leverage diverse supervision signals. The strengths are substantial: novel approach, comprehensive evaluation, large-scale dataset, and strong results across multiple tasks. However, several limitations need to be addressed before acceptance:\n\n1. The authors should expand their discussion of how UmiF might generalize to other medical imaging modalities beyond chest X-rays\n2. Computational requirements analysis should be added to assess practical deployment feasibility\n3. The clinical applicability section should be expanded to discuss potential integration into clinical workflows\n4. Additional analysis of performance across different disease categories and severity levels would strengthen the clinical relevance\n5. The presentation should be made more accessible to a broader medical imaging audience\n\nWith these revisions, this paper has the potential to become an important contribution to the field of medical imaging AI, addressing a critical need for unified pre-training frameworks that can leverage the diverse annotations available in medical datasets.",
        "reviewer": "ai/rpi",
        "metrics": {
          "coverage": 5,
          "specificity": 5,
          "correctness": 4,
          "constructiveness": 4,
          "stance": 4,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript introduces **UmiF (Unified medical image pre-training framework)**, which aims to address the challenge of developing task-agnostic medical image encoders by integrating diverse supervision types‚Äîincluding self-supervision, language-based supervision, and segmentation annotations‚Äîinto a single unified framework. The approach converts heterogeneous data pairs (image-report, image-caption, image-class, and image-segment) into a shared token space, enabling both contrastive learning and masked modeling through a flexible token grouping strategy. Pre-trained on a large-scale dataset of 1.66 million images from 14 public sources, UmiF achieves state-of-the-art results across multiple downstream tasks (classification, segmentation, detection, retrieval, and visual question answering). The manuscript is clearly written, well-organized, and presents comprehensive empirical validation.  \n\n**Major Comments**  \n1. **Scope and Generalization** ‚Äì Validation is limited to chest X-ray data; the paper does not discuss how UmiF might generalize to other imaging modalities such as MRI or CT.  \n2. **Computational Analysis** ‚Äì The study lacks discussion of training efficiency, memory usage, and inference time, which are key considerations for clinical adoption.  \n3. **Clinical Relevance** ‚Äì The manuscript could better address how the framework might integrate into real-world clinical workflows or impact clinical decision-making.  \n4. **Data Analysis Granularity** ‚Äì There is no analysis of performance differences across disease categories or severity levels, which would be important for clinical interpretation.  \n5. **Low-Resource and Noisy Settings** ‚Äì Potential limitations in low-resource conditions or with noisy labels are not addressed.  \n\n**Minor Comments**  \n- Some technical sections, especially Section 3, could be simplified for non-expert readers.  \n- The abstract could be more concise without losing methodological clarity.  \n- The manuscript could expand on the optimal mix of supervision types in pre-training.  \n- Figure captions would benefit from more descriptive explanations to aid comprehension without constant cross-referencing.  \n\n**Summary Paragraph**  \nOverall, this work tackles a significant and timely problem in medical image pre-training by unifying multiple supervision signals through a single, flexible framework. Its conceptual innovation‚Äîthe unified token embedding space and grouping strategy‚Äîdemonstrates notable originality and leads to strong empirical results across tasks. The evaluation is extensive, with well-documented ablations and clear evidence of improved performance, and reproducibility is enhanced by detailed documentation and publicly available datasets. The main areas for improvement relate to generalization beyond chest X-rays, computational efficiency, clinical applicability, and interpretability across disease categories. Addressing these points would strengthen the work‚Äôs robustness and relevance to broader clinical practice.  \n\n**Decision Recommendation:** **Major Revision**  \nThe paper presents promising and innovative contributions with strong empirical results, but it requires additional analysis and expanded discussion of generalization, computational demands, and clinical integration before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      },
      {
        "text": "## A summary of the paper  \n\nThe manuscript tackles the challenge of building task‚Äëagnostic encoders for medical imaging by introducing a unified pre‚Äëtraining framework that can exploit a wide variety of supervisory signals. These signals include self‚Äësupervision, paired radiology reports, image captions, classification labels, and segmentation masks. The authors pre‚Äëtrain on a massive corpus of 1.66‚ÄØmillion images drawn from 14 publicly available chest‚ÄëX‚Äëray collections, covering image‚Äëreport, image‚Äëcaption, image‚Äëclass, and image‚Äësegment pairings. All inputs are transformed into a common token embedding space using modality‚Äëspecific tokenizers. A flexible token‚Äëgrouping strategy then creates two complementary token groups that serve both as positive pairs for contrastive learning and as incomplete views for mask‚Äëmodeling, all processed by a single Vision Transformer backbone. The resulting encoder is evaluated on several downstream tasks‚Äîlinear classification, semantic segmentation, object detection, zero‚Äëshot classification, and visual‚Äëquestion‚Äëanswering‚Äîwith performance reported across the board. The principal contributions are (i) the token‚Äëbased unified framework, (ii) the large‚Äëscale multi‚Äësupervision pre‚Äëtraining, and (iii) the public release of the pretrained encoder.\n\n---\n\n## General feedback  \n\n- **Significance:** Bringing heterogeneous medical annotations together in a single pre‚Äëtraining pipeline addresses a genuine bottleneck in the field, where many public datasets are small and fragmented.  \n- **Innovation:** The proposed ‚Äúunified token space‚Äù and the ‚Äúflexible token‚Äëgrouping‚Äù mechanisms are conceptually straightforward yet novel for medical imaging. While reminiscent of multimodal contrastive and MAE approaches (e.g., CLIP, MAE), the manuscript would benefit from a clearer articulation of the specific advantages over those existing baselines.  \n- **Evaluation:** The authors present an extensive suite of downstream experiments, and UmiF often surpasses prior methods (see Tables‚ÄØ2‚Äì5). However, the experimental description lacks details on dataset splits, statistical significance testing, and a comprehensive set of baselines for detection and VQA.  \n- **Reproducibility:** Important training particulars‚Äîsuch as learning‚Äërate schedules, loss‚Äëweighting, token‚Äëgrouping probability schedules, masking ratios, and random seeds‚Äîare omitted. Moreover, the code and model release is conditional on acceptance, which limits immediate verification.\n\n---\n\n## Specific comments / critiques  \n\n- **Token‚Äëgrouping hyper‚Äëparameters:** The manuscript does not specify the probability of setting‚ÄØr‚ÄØ=‚ÄØ1 nor the full distribution of‚ÄØr. Only a limited ablation (Table‚ÄØ6) with a 0.2 probability is shown, achieving the best RSNA 1‚ÄØ% classification AUC. A more thorough investigation, including possible annealing schedules, would strengthen the analysis.  \n- **Mask‚Äëmodeling details:** Equation‚ÄØ(2) introduces a BYOL‚Äëstyle consistency loss, yet the paper omits crucial information such as mask size, the proportion of tokens masked, and the weighting of this loss relative to the contrastive alignment loss. Providing these details is essential for reproducibility (Section‚ÄØ3.3).  \n- **Segmentation tokenization and contribution:** Segmentation masks are treated as a third image modality (RGB masks ‚Üí patches), but no ablation isolates the effect of this design choice. Table‚ÄØ7 mixes all supervision types without a ‚Äúimage‚ÄØ+‚ÄØsegmentation only‚Äù comparison, leaving the contribution of segmentation unclear.  \n- **Dataset documentation:** Table‚ÄØ1 contains placeholder entries, lacks clear column headings, and does not provide sample counts for several modalities or preprocessing specifics (resolution, normalization). This makes it difficult to assess the scale and diversity of the data.  \n- **Baseline fairness:** In Tables‚ÄØ2 and‚ÄØ3, comparisons combine CNN‚Äëbased and ViT‚Äëbased baselines that differ in model size and pre‚Äëtraining data volume, potentially biasing the reported improvements. A controlled baseline‚Äîe.g., same ViT architecture and identical pre‚Äëtraining data‚Äîwould provide a fairer benchmark.  \n- **Zero‚Äëshot protocol:** Table‚ÄØ4 reports AUC on CXP500 but does not describe how prompts are constructed, which text encoder is used, or how the results compare to recent CLIP‚Äëstyle medical models such as MedCLIP, which achieve higher scores.  \n- **VQA evaluation methodology:** Table‚ÄØ5 presents accuracy without clarifying whether open‚Äëended answers are evaluated with exact match or a softer scoring scheme, nor does it detail the answer vocabulary or the fine‚Äëtuning of the decoder.  \n- **Synthetic report quality:** The appendix outlines GPT‚Äë4 prompts employed to translate Spanish reports and generate synthetic reports, yet no quantitative assessment of translation quality‚Äîor its impact on downstream performance‚Äîis provided.  \n- **Statistical reporting:** The manuscript lacks confidence intervals, standard deviations, or significance tests for any downstream metric (AUC, Dice, mAP), leaving the robustness of the reported gains uncertain.  \n- **Bias analysis:** While the ethics statement mentions potential regional bias due to reliance on public datasets, no empirical analysis (e.g., performance across institutions or imaging devices) is presented.\n\n---\n\n## A suggested decision  \n\n**Major Revision** ‚Äì The work demonstrates considerable promise and addresses an important problem, but several substantive issues‚Äîparticularly regarding methodological detail, baseline fairness, and reproducibility‚Äîmust be resolved before the manuscript can be accepted.",
        "reviewer": "ai/fudan",
        "metrics": {
          "coverage": 4,
          "specificity": 4,
          "correctness": 3,
          "constructiveness": 3,
          "stance": 3,
          "source": "ai"
        },
        "harmonization": [
          {
            "text": "**Opening Paragraph**  \nThe manuscript proposes a unified pre‚Äëtraining framework to develop task‚Äëagnostic encoders for medical imaging. By integrating multiple supervisory signals‚Äîself‚Äësupervision, paired radiology reports, image captions, classification labels, and segmentation masks‚Äîthe approach aims to leverage a large corpus of 1.66‚ÄØmillion chest X‚Äëray images from 14 public datasets. All inputs are converted into a shared token space via modality‚Äëspecific tokenizers, followed by a flexible token‚Äëgrouping strategy for contrastive and masked‚Äëmodeling objectives within a single Vision Transformer backbone. The model is evaluated on diverse downstream tasks, showing strong performance. Overall, the study contributes a significant attempt toward scalable, multi‚Äësupervised pre‚Äëtraining, though several methodological omissions limit its reproducibility and interpretability.  \n\n**Major Comments**  \n1. **Significance and Innovation:** Integrating heterogeneous medical annotations within one pre‚Äëtraining framework addresses a real challenge. The ‚Äúunified token space‚Äù and ‚Äúflexible token‚Äëgrouping‚Äù mechanisms are novel but require clearer explanation of advantages compared with established multimodal approaches.  \n2. **Evaluation Design:** While the experimental scope is broad, critical details such as dataset splits, statistical significance testing, and a broader set of detection and VQA baselines are missing.  \n3. **Reproducibility:** Essential training parameters (e.g., learning‚Äërate schedules, loss weights, masking ratios, random seeds) are omitted, and public release of code and models is contingent on acceptance.  \n4. **Token‚Äëgrouping and Mask‚Äëmodeling Details:** The probability distribution of the grouping parameter‚ÄØr, mask size, proportion of masked tokens, and loss‚Äëweighting are unspecified, hindering reproducibility.  \n5. **Segmentation and Multi‚Äësupervision Contributions:** The effect of adding segmentation masks as an image modality is not isolated; ablations covering this aspect are absent.  \n6. **Dataset Documentation:** Table‚ÄØ1 is incomplete, lacking column labels, numerical counts per modality, and preprocessing details.  \n7. **Baseline Fairness:** Comparative results mix CNN‚Äë and ViT‚Äëbased models of differing capacities and data scales; a controlled baseline would strengthen validity.  \n8. **Zero‚Äëshot and VQA Protocols:** Prompts, text encoder details, and evaluation protocols for these tasks are insufficiently described.  \n9. **Synthetic Report and Translation Quality:** The method for generating synthetic reports is reported but not quantitatively assessed for quality or its effect on downstream results.  \n10. **Statistical and Bias Analysis:** No confidence intervals, standard deviations, or bias assessments are provided to substantiate robustness or fairness claims.  \n\n**Minor Comments**  \n- Clarify notations and parameters in key equations.  \n- Ensure consistency and completeness in tables (especially Table‚ÄØ1).  \n- Define acronyms upon first use and check formatting for clarity.  \n\n**Summary Paragraph**  \nThe study introduces an ambitious and conceptually strong approach for unified multi‚Äësource pre‚Äëtraining in medical imaging. Its methodological integration and dataset scale represent meaningful contributions. However, incomplete experimental documentation, unclear methodological specifics, and insufficient fairness and reliability analyses weaken the overall rigor. Addressing these issues would substantially improve clarity, reproducibility, and trust in the reported benefits.  \n\n**Decision Recommendation**  \n**Major Revision** ‚Äì The work shows high potential and relevance, but substantial clarification and methodological reinforcement are required before acceptance.",
            "model": "gpt-5-chat-latest",
            "similarity_scores": []
          }
        ]
      }
    ],
    "authors": [
      "Dongsheng Li",
      "Hoifung Poon",
      "Lili Qiu",
      "Naoto Usuyama",
      "Sheng Zhang",
      "XINYANG JIANG",
      "Xiaoxuan He",
      "Xufang Luo",
      "Yifan Yang",
      "Yuqing Yang",
      "Zilong Wang"
    ],
    "url": "pdfs/iclr.cc-2025-conference_901796e0b8b3eeca3d767c044473e3b38b2d8cc1.pdf",
    "remote_url": "https://openreview.net/pdf/901796e0b8b3eeca3d767c044473e3b38b2d8cc1.pdf",
    "venue_id": "iclr.cc-2025-conference"
  }
]