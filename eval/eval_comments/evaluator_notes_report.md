# Evaluator Notes Analysis Report (Peer-Review Quality) — Markdown

## 1) Executive Summary

Across the evaluator notes, the dominant signal is **“AI-likeness” being inferred from style + lack of paper-specific grounding** rather than from any single tell. The most frequent criticisms cluster around:

- **Generic / template-driven / checklist-like feedback** (often described as “superficial,” “high-level,” “routine,” “too perfect,” or “over-complete”).
- **Factual errors / hallucinations / misreadings** (e.g., pointing out non-existent typos, incorrect claims about what the paper contains, invented dataset details, irrelevant technical issues).
- **Style markers** associated with AI assistance (e.g., **uniform tone**, **overly polished phrasing**, **structured paragraphs**, frequent **em-dashes**).
- A countervailing theme: **high-quality, paper-specific, technically grounded critiques** are repeatedly described as **human-like**, even when the review source is labeled as AI—suggesting evaluators reward *specificity, alignment, and prioritization* more than origin.

Evaluator behavior differs substantially: some evaluators (notably **Justin**) use a small set of repeated labels (“missed the big picture,” “unreasonable expectations,” “hallucination”), while others (notably **Luping** and **Guang**) provide nuanced, multi-criterion judgments (specificity, prioritization, correctness, tone, domain grounding).

---

## 2) Descriptive Statistics

### 2.1 Total number of comments
- **Observed in provided excerpt:** **~314 comments** (the prompt states 314; the excerpt appears consistent with that scale).
- **Note:** This report treats the provided list as the dataset; minor discrepancies can occur if the excerpt is truncated or duplicated.

### 2.2 Distribution of comments by Evaluator (qualitative + approximate)
From the excerpt, comment volume is highly imbalanced:

- **Luping**: very high volume (many long, detailed assessments; likely the largest share).
- **Justin**: very high volume (many short, repeated-tag comments).
- **Guang**: high volume (many medium-length, style+correctness judgments).
- **Tolga**: moderate volume (style/tone/stance + hallucination notes).
- **Yixuan**: lower-to-moderate volume (focus on correctness, overlap, and “textual analysis only”).
- **Bernhard**: very low volume (a few short notes, mostly “superficial” / “AI summary”).

**Interpretation:** The dataset is dominated by a few evaluators, which strongly shapes theme frequencies (e.g., “missed the big picture” is heavily driven by Justin).

### 2.3 Distribution of comments by Review Source

#### Human vs AI (high-level)
- **Human-labeled review_source:** substantial portion.
- **AI-labeled review_source:** substantial portion, spread across:
  - `ai/gpt-5-chat-latest`
  - `ai/claude-sonnet-4-20250514`
  - `ai/hkust-reviewer`
  - `ai/stanford`
  - `ai/rpi`
  - `ai/fudan`

**Important nuance:** Many comments evaluate *human-sourced reviews as “AI-like”* (AI-assisted or AI-generated), and many AI-sourced reviews are judged “human-like.” So “review_source” is not the same as “perceived authorship.”

#### By AI model family (qualitative)
Mentions suggest:
- **GPT-5**: often described as either (a) generic/templated when shallow, or (b) impressively aligned and “human-like” when detailed.
- **Claude Sonnet**: frequently described as **polished**, **uniform**, sometimes “exhaustive,” occasionally suspected AI due to structure.
- **HKUST/Stanford/RPI/Fudan variants**: mixed; some are praised as paper-aligned and human-like, others flagged for hallucinations, generic checklists, or mismatched datasets.

---

## 3) Thematic Analysis

Below are the main themes that recur across evaluators, with **typical phrasing** and **examples** drawn from the dataset. (Counts are presented as **relative frequency** rather than exact totals because multiple themes often occur in one comment and the dataset is evaluator-imbalanced.)

### Theme A — Generic / Template-driven / Checklist-like
**Description:** Feedback that could apply to many papers; overly structured; “routine” concerns; exhaustive enumeration without prioritization.

**Common phrases:**
- “generic phrasing,” “template-like,” “checklist,” “formulaic,” “routine,” “too perfect,” “uniform tone,” “evenly structured paragraphs”

**Examples:**
- Guang (human): “generic phrasing and template-like structure suggest notable AI assistance.”
- Luping (ai/rpi): “extremely comprehensive, enumerating every plausible concern… (dataset, math, metrics, reproducibility, ethics).”
- Guang (ai/gpt-5): “relies heavily on generic evaluation criteria… factually inaccurate… indicating… generated by AI.”

**What it signals to evaluators:** AI generation or AI assistance, especially when paired with low specificity.

---

### Theme B — Superficiality / High-level / Lack of Depth
**Description:** Comments are shallow, don’t engage with core technical mechanisms, or focus on surface presentation.

**Common phrases:**
- “superficial,” “high-level,” “lacks depth,” “not in-depth,” “vague,” “summary-like”

**Examples:**
- Bernhard (human): “very superficial feedback, no methodological depth”
- Guang (ai/rpi): “comprehensive, it lacks depth and remains superficial”
- Luping (ai/fudan): “remains quite general… fewer concrete technical references”

**Interpretation:** Often used as a proxy for “AI-like,” but also used to describe weak human reviews.

---

### Theme C — Factual Errors / Hallucinations / Misalignment with Manuscript
**Description:** Claims that are incorrect, fabricated, or irrelevant; references that don’t match; invented typos; wrong dataset details.

**Common phrases:**
- “hallucination,” “factually incorrect,” “errors… do not seem to exist,” “misaligned,” “misunderstands”

**Examples:**
- Guang (human): “Some of the errors pointed out… seem to be nonexistent… look more like an AI review.”
- Yixuan (human): points out reviewer claimed undisclosed parameters that are actually stated.
- Tolga (ai/hkust-reviewer): “clear hallucinations about k-space and undersampling… irrelevant to the manuscript.”
- Justin (many): “hallucination; missed the big picture; off-topic comments”

**Interpretation:** This is one of the strongest negative signals; evaluators treat it as highly diagnostic of AI generation *or* careless reviewing.

---

### Theme D — Lack of Paper-Specificity / Missing Concrete Evidence
**Description:** Feedback not grounded in tables/figures/experiments; doesn’t cite sections; doesn’t engage with actual method.

**Common phrases:**
- “not grounded in concrete details,” “limited paper-specific insight,” “does not engage with… actual methodology,” “lacking analysis of tables and figures”

**Examples:**
- Guang (human): “lacks paper-specific technical evidence… AI-generated assessment”
- Yixuan (repeated across sources): “major comments primarily focus on textual analysis, lacking analysis of tables and figures.”

**Interpretation:** Closely tied to “generic/template” but distinct: even a well-written review is penalized if it doesn’t touch concrete artifacts.

---

### Theme E — Tone / Stance / Human “imperfection”
**Description:** Notes about harshness, leniency, bias, mismatch between stance and criticisms, or “personal judgment.”

**Common phrases:**
- “lenient stance,” “harsh tone,” “bias,” “personal opinion,” “skeptical stance,” “human imperfection”

**Examples:**
- Tolga (human): “Stance: A bit lenient given the major comments…”
- Luping (ai/rpi): “tone… harsher than the raised issues… like human imperfection.”
- Guang (human): “sharp, opinionated points… looks more like a human review.”

**Interpretation:** Interestingly, *tone inconsistency* is sometimes treated as a **human marker**, while *uniform politeness* is treated as **AI-like**.

---

### Theme F — Style Markers of AI Assistance (polish, em-dash, structure)
**Description:** Evaluators repeatedly cite stylistic tells: em-dashes, overly polished prose, uniform paragraphing, “template headings.”

**Examples:**
- Guang: “polished, template-driven wording strongly suggests AI-assisted writing”
- Yixuan: “The ‘—’ is commonly used in an AI review.”
- Guang (ai/rpi): “frequent use of em-dash sentence structures… resemble AI-generated writing.”

**Interpretation:** Style alone rarely decides the judgment; it becomes decisive when paired with genericness or factual errors.

---

### Theme G — Unreasonable / Off-topic Requests; “Missed the big picture”
**Description:** Requests that don’t improve science/presentation; misunderstanding scope; focusing on irrelevant issues.

**Examples:**
- Justin (many): “unreasonable expectations; unnecessary requests…”
- Justin: “missed the big picture; off-topic comments”
- Guang: “misunderstanding of double-blind conventions… placeholders (‘XXX’)”

**Interpretation:** Often used as a quality critique; sometimes linked to AI hallucination or shallow reading.

---

## 4) Comparative Analysis (Crucial)

### 4.1 Human vs. AI (by `review_source`)

#### When `review_source = human`
Evaluators frequently still label the writing as AI-like. The most common criticisms are:

- **Template/generic structure** (“polite,” “balanced,” “too perfect,” “checklist-like”)
- **Superficiality / high-level feedback**
- **Factual errors** (especially “errors that don’t exist,” incorrect claims about acronyms/typos, wrong assumptions)

**Notable pattern:** Human-sourced reviews are often suspected of **AI assistance** rather than fully AI generation—phrased as “AI-assisted polishing,” “template-driven wording,” “AI at least for the summary.”

#### When `review_source = AI/*`
Critiques split into two regimes:

1) **AI reviews criticized as generic/superficial**  
   - “broad coverage but uniform depth,” “routine,” “summary-like,” “checklist”

2) **AI reviews praised as human-like when they show:**
   - **paper-specific technical engagement**
   - **correctness and alignment**
   - **prioritization of key issues**
   - **opinionated, experience-driven judgment**

**Key takeaway:** The evaluators’ *quality criteria* (specificity, correctness, prioritization) dominate over the label of origin. AI systems can “pass” when they avoid hallucinations and engage concretely.

---

### 4.2 Are AI reviews more likely to be called “generic” or “superficial”?
**Yes, but with an important caveat:**
- “Generic/superficial” is used heavily for AI-sourced reviews **and** for human-sourced reviews suspected of AI assistance.
- The label “AI-like” is strongly associated with **genericness + uniform structure**, regardless of true source.

So the dataset reflects **perceived AI-ness**, not just AI provenance.

---

### 4.3 Are human reviews more likely to have “tone” issues or “brevity”?
**Tone/stance mismatch** is more often described as a **human trait**:
- “harsh relative to issues,” “lenient despite major flaws,” “bias/negligence,” “personal opinion,” “skeptical stance”

**Brevity** appears in two ways:
- Some evaluators treat short, shallow reviews as low-quality (often AI-like).
- Others treat selective focus and uneven emphasis as human-like (a kind of “human imperfection” / prioritization).

---

### 4.4 Model Comparison (GPT vs Claude vs others)

Because the dataset is qualitative and not balanced, treat these as *observed tendencies in evaluator language*:

#### GPT (`ai/gpt-5-chat-latest`)
- Praised when: “careful reading,” “technically valid,” “well-targeted concerns,” “human-like reasoning.”
- Criticized when: “conservative and general,” “templated,” “generic evaluation criteria,” occasional factual inaccuracies.

**Implied profile:** Higher variance—can look very human when detailed, but still penalized for genericness.

#### Claude (`ai/claude-sonnet-4-20250514`)
- Frequently flagged for: **polished tone**, **uniform formality**, **exhaustive structure**, “AI-assisted writing.”
- Also sometimes praised as: “deep and accurate understanding,” “paper-specific,” “expert-level.”

**Implied profile:** Strong writing polish is a double-edged sword—readability increases, but “AI smell” increases.

#### Other AI sources (`hkust-reviewer`, `stanford`, `rpi`, `fudan`)
- Mixed outcomes; evaluators often judge them by:
  - **alignment with manuscript** (no mismatched datasets, no invented details)
  - **depth vs checklist**
  - **presence/absence of hallucinated citations or irrelevant technical claims**

---

## 5) Evaluator Analysis (who focuses on what)

### Bernhard
- Very short notes; strong emphasis on **superficiality** and **AI summary markers** (“The study is…”, “minimum summary is AI”).

### Guang
- Highly sensitive to **template tone**, **generic phrasing**, **paper-misalignment**, and **fabricated errors**.
- Often distinguishes “AI-assisted polishing” vs “human expert reasoning.”
- Uses “human-like” as a quality label when reviews show **personal judgment** and **sharp, grounded critique**.

### Justin
- Extremely consistent taxonomy:  
  **“missed the big picture,” “unreasonable expectations,” “comments do not suggest expertise,” “hallucination,” “off-topic.”**
- Less focus on stylistic tells; more on **review usefulness and validity**.

### Luping
- Most nuanced and criteria-driven:
  - **prioritization vs exhaustive checklist**
  - **paper-specific technical depth**
  - **domain-aware reasoning**
  - acknowledges ambiguity: “hard to tell AI vs human,” “topic could cause superficiality”
- Often treats **selective focus + contextual judgment** as human-like.

### Tolga
- Focus on:
  - **stance calibration** (lenient/harsh mismatch)
  - **word choice / style** (“fancy/heavy”)
  - **hallucinations**
  - **domain-specific references** as a human marker
  - meta-signals like referring to self as “the reviewer” / “the review” in third person.

### Yixuan
- Strong emphasis on **correctness checks** and **coverage of tables/figures**.
- Repeated critique: reviews focus on **textual analysis only**.
- Notes stylistic marker: em-dash (“—”) as AI-associated.

---

## 6) Conclusion: Implications for Using AI in Peer Review

This dataset suggests evaluators do not reject AI reviews *because they are AI*; they reject reviews that **behave like low-effort AI**: generic, uniformly structured, and insufficiently grounded. The most damaging failure mode is **hallucination / factual misalignment**, which immediately undermines trust.

**Practical implications:**
- AI can be valuable when it produces **paper-specific, technically grounded, prioritized critiques**—these are repeatedly labeled “human-like,” even when the source is AI.
- The biggest risks are:
  1) **Fabricated or incorrect claims** (including invented typos, wrong dataset details, irrelevant technical issues)
  2) **Checklist reviews** that cover everything shallowly without prioritization
  3) **Over-polished uniform tone** that triggers “AI smell,” especially when paired with generic content

**Operational recommendation (based on evaluator signals):**
- If AI is used, enforce guardrails that require:
  - explicit references to **specific sections/figures/tables/claims**
  - a **top-3 prioritized concerns** section (to avoid checklist feel)
  - a **factuality check** step (e.g., “quote the manuscript text you are criticizing”)
  - discouraging stylistic over-regularization (reduce template rigidity; allow natural variance)

Overall, the qualitative feedback implies AI-assisted peer review is most acceptable when it behaves like a careful expert: **correct, specific, and selective**, rather than comprehensive-but-generic.