\documentclass{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{float}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, linktoc=all}

\title{AI Reviewer Evaluation Report}
\author{Automated Analysis}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
This report presents a comprehensive evaluation of AI reviewers compared to human performance.

\section{Score Statistics}

\subsection{Score Distributions}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/human_vs_ai_boxplot.png}
    \caption{Distribution of Review Scores (Human vs AI).}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/radar_human_vs_ai.png}
    \caption{Average Score Profile: Human vs AI.}
\end{figure}

\subsection{Per-Evaluator Statistics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/evaluator_boxplot.png}
    \caption{Score Distribution by Evaluator.}
\end{figure}

\subsection{Per-Evaluator per Metric Statistics}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/evaluator_per_metric_boxplot.png}
    \caption{Score Distribution: Evaluator per Metric.}
\end{figure}

\section{Statistical Significance Tests}

\subsection{Methodology}
We confirm performance differences using Mann-Whitney U (unpaired), Wilcoxon Signed-Rank (paired), and assess variance equality with Levene's Test. Effect size is measured by Cliff's Delta.

\subsection{Global Analysis (Human vs All AI)}

\begin{table}[H]
\centering
\caption{Statistical Significance (Overall)}
\label{tab:sig_global}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.4257$ & $0.3154$ & $0.0888$ & -0.042 \\
Specificity & $0.0117^{*}$ & $0.3046$ & $0.8810$ & +0.133 \\
Correctness & $0.4954$ & $0.6326$ & $0.6067$ & +0.036 \\
Constructiveness & $0.5667$ & $0.9367$ & $0.9975$ & +0.030 \\
Stance & $0.3668$ & $0.3751$ & $0.8857$ & -0.047 \\
\bottomrule
\end{tabular}

\end{table}

\subsection{Per-Model Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/radar_models.png}
    \caption{Performance Profile per AI Model.}
\end{figure}
\subsubsection{Model: claude-sonnet-4-20250514}

\begin{table}[H]
\centering
\caption{Significance: Human vs claude-sonnet-4-20250514}
\label{tab:sig_claude-sonnet-4-20250514}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.9777$ & $0.8441$ & $0.7283$ & +0.003 \\
Specificity & $0.1302$ & $0.7492$ & $0.6723$ & +0.130 \\
Correctness & $0.4365$ & $0.6947$ & $0.5289$ & +0.067 \\
Constructiveness & $0.2614$ & $0.2963$ & $0.7862$ & +0.097 \\
Stance & $0.8436$ & $0.3610$ & $0.9078$ & -0.017 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: fudan}

\begin{table}[H]
\centering
\caption{Significance: Human vs fudan}
\label{tab:sig_fudan}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.3471$ & $0.1423$ & $0.6293$ & -0.081 \\
Specificity & $0.1898$ & $0.5494$ & $0.6125$ & +0.113 \\
Correctness & $0.6484$ & $0.4506$ & $0.7832$ & -0.039 \\
Constructiveness & $0.8420$ & $0.9754$ & $0.7737$ & -0.017 \\
Stance & $0.3708$ & $0.0734$ & $0.3934$ & -0.077 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: gpt-5-chat-latest}

\begin{table}[H]
\centering
\caption{Significance: Human vs gpt-5-chat-latest}
\label{tab:sig_gpt-5-chat-latest}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.9845$ & $0.7646$ & $0.2558$ & -0.002 \\
Specificity & $0.1254$ & $0.5295$ & $0.6841$ & +0.132 \\
Correctness & $0.3352$ & $0.6136$ & $0.4886$ & +0.083 \\
Constructiveness & $0.9905$ & $0.7808$ & $0.9468$ & +0.001 \\
Stance & $0.9647$ & $0.6004$ & $0.8854$ & -0.004 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: hkust-reviewer}

\begin{table}[H]
\centering
\caption{Significance: Human vs hkust-reviewer}
\label{tab:sig_hkust-reviewer}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.0351^{*}$ & $0.0202^{*}$ & $0.0339^{*}$ & -0.181 \\
Specificity & $0.4722$ & $0.8503$ & $0.1826$ & +0.062 \\
Correctness & $0.9030$ & $0.4923$ & $0.5603$ & -0.011 \\
Constructiveness & $0.3904$ & $0.7538$ & $0.6046$ & -0.074 \\
Stance & $0.5353$ & $0.5007$ & $0.3936$ & -0.053 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: rpi}

\begin{table}[H]
\centering
\caption{Significance: Human vs rpi}
\label{tab:sig_rpi}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.1210$ & $0.4776$ & $0.0914$ & +0.133 \\
Specificity & $0.0168^{*}$ & $0.2015$ & $0.7371$ & +0.206 \\
Correctness & $0.9914$ & $0.4817$ & $0.6390$ & +0.001 \\
Constructiveness & $0.3183$ & $0.6419$ & $0.9853$ & +0.086 \\
Stance & $0.2791$ & $0.1057$ & $0.9523$ & -0.093 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: stanford}

\begin{table}[H]
\centering
\caption{Significance: Human vs stanford}
\label{tab:sig_stanford}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.1512$ & $0.0842$ & $0.7931$ & -0.124 \\
Specificity & $0.0747$ & $0.4609$ & $0.2799$ & +0.153 \\
Correctness & $0.1846$ & $0.5616$ & $0.7711$ & +0.114 \\
Constructiveness & $0.3043$ & $0.4573$ & $0.5177$ & +0.088 \\
Stance & $0.6400$ & $0.8720$ & $0.4670$ & -0.040 \\
\bottomrule
\end{tabular}

\end{table}


\section{Turing Test Analysis (AI Detection)}
Evaluators were asked to guess if the review was written by AI or Human. We present the confusion matrices below.

\begin{table}[H]
\centering
\caption{Turing Test Performance Metrics}
\label{tab:tur_metrics}
\begin{tabular}{lrrrr}
\toprule
Evaluator & Accuracy & Precision & Recall & F1 \\
\midrule
Bernhard & 0.667 & 0.667 & 1.000 & 0.800 \\
Guang & 0.378 & 0.536 & 0.500 & 0.517 \\
Justin & 0.600 & 0.658 & 0.833 & 0.735 \\
Luping & 0.478 & 0.741 & 0.333 & 0.460 \\
Tolga & 0.478 & 0.667 & 0.433 & 0.525 \\
Yixuan & 0.567 & 0.769 & 0.500 & 0.606 \\
Overall & 0.528 & 0.661 & 0.600 & 0.629 \\
\bottomrule
\end{tabular}

\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/turing_cm_overall.png}
    \caption{Overall Confusion Matrix (AI Detection).}
\end{figure}
\subsection{Per-Evaluator Confusion Matrices}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{plots/turing_cm_evaluators_combined.png}
    \caption{Confusion Matrices per Evaluator.}
\end{figure}

\section{Inter-Evaluator Agreement}
Cohen's Kappa and Gwet's AC2 agreement between evaluators on review scores (discretized).

\subsection{Cohen's Kappa}

\begin{table}[H]
\centering
\caption{Pairwise Cohen's Kappa Agreement}
\label{tab:kappa}
\begin{tabular}{lllllll}
\toprule
Evaluator & Bernhard & Guang & Justin & Luping & Tolga & Yixuan \\
\midrule
Bernhard & - & 0.04 & - & -0.08 & -0.00 & - \\
Guang & 0.04 & - & 0.06 & 0.01 & - & 0.22 \\
Justin & - & 0.06 & - & - & 0.05 & -0.14 \\
Luping & -0.08 & 0.01 & - & - & - & 0.16 \\
Tolga & -0.00 & - & 0.05 & - & - & 0.01 \\
Yixuan & - & 0.22 & -0.14 & 0.16 & 0.01 & - \\
\bottomrule
\end{tabular}

\end{table}

\subsection{Gwet's AC2}
Gwet's AC2 is often more robust to marginal imbalance and ordinal data.

\begin{table}[H]
\centering
\caption{Pairwise Gwet's AC2 Agreement}
\label{tab:ac2}
\begin{tabular}{lllllll}
\toprule
Evaluator & Bernhard & Guang & Justin & Luping & Tolga & Yixuan \\
\midrule
Bernhard & - & -0.00 & - & -0.11 & -0.04 & - \\
Guang & -0.00 & - & 0.05 & -0.02 & - & 0.22 \\
Justin & - & 0.05 & - & - & 0.01 & -0.17 \\
Luping & -0.11 & -0.02 & - & - & - & 0.15 \\
Tolga & -0.04 & - & 0.01 & - & - & -0.16 \\
Yixuan & - & 0.22 & -0.17 & 0.15 & -0.16 & - \\
\bottomrule
\end{tabular}

\end{table}

\section{Breakdown wrt Accepted versus Rejected Papers}
Analysis of review characteristics based on the final decision (Accept vs Reject).

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_human_scores.png}
        \caption{Human Scores (Accept/Reject)}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_ai_scores.png}
        \caption{AI Scores (Accept/Reject)}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/decision_turing_combined.png}
    \caption{Turing Test Confusion Matrices (Accept/Reject)}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_detection_metrics.png}
        \caption{AI Detection Metrics}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_distribution.png}
        \caption{Dataset Distribution}
    \end{minipage}
\end{figure}

\appendix
\newpage
\section{Appendix: Guide to Interpretations}

\subsection{Interpreting Box Plots}
The box plots in this report visualize the distribution of review scores.
\begin{itemize}
    \item \textbf{Box}: Represents the Interquartile Range (IQR), spanning from the 25th percentile ($Q1$) to the 75th percentile ($Q3$). It contains the middle 50\% of the data.
    \item \textbf{Median}: The line inside the box marks the median score (50th percentile).
    \item \textbf{Whiskers}: Extend from the box to the most extreme data points that are not considered outliers. Typically, this is $1.5 \times IQR$.
    \item \textbf{Empty Circles (Outliers)}: Points lying beyond the whiskers are plotted individually as empty circles. These represent outlier scores that are unusually high or low compared to the rest of the distribution.
\end{itemize}

\subsection{Statistical Methodology Details}
This section explains the intuition and computation behind the statistical tests used.

\subsubsection{Mann-Whitney U Test}
\textbf{Intuition}: A non-parametric test for independent samples (e.g., Human vs AI scores across different papers). It assesses whether one group's values are stochastically larger than the other's. It does not assume a normal distribution.
\textbf{Computation}: All observations are ranked together. The sum of ranks for each group is calculated. The $U$ statistic is derived from these rank sums, comparing the number of times a value from one group precedes a value from the other.

\subsubsection{Wilcoxon Signed-Rank Test}
\textbf{Intuition}: A non-parametric paired test used for per-model comparisons where we have matched scores (Human and AI reviewing the \textit{same} paper). It tests if the distribution of differences is symmetric about zero.
\textbf{Computation}: Differences between paired scores ($d_i = x_{human} - x_{ai}$) are calculated. Absolute differences $|d_i|$ are ranked. Ranks are signed according to the sign of $d_i$. The test statistic $W$ is the sum of positive ranks.

\subsubsection{Levene's Test}
\textbf{Intuition}: Tests the null hypothesis that the variances (spread) of the two groups are equal (Homogeneity of Variance).
\textbf{Computation}: It performs an Analysis of Variance (ANOVA) on the absolute deviations of scores from their group means (or medians). A significant $p$-value suggests the groups have different consistency levels.

\subsubsection{Cliff's Delta ($\delta$)}
\textbf{Intuition}: An effect size measure quantifying the magnitude of difference between two groups. It represents the probability that a randomly selected value from one group is greater than one from the other, minus the reverse probability. values range from -1 to +1.
\textbf{Computation}:
\[ \delta = \frac{\#(x_H > x_A) - \#(x_H < x_A)}{n_H \times n_A} \]
where $x_H$ and $x_A$ are scores from Human and AI groups respectively. 
Interpretation: $|\delta| < 0.147$ (Negligible), $< 0.33$ (Small), $< 0.474$ (Medium), else (Large).

\subsubsection{Cohen's Kappa ($\kappa$)}
\textbf{Intuition}: Measures inter-rater agreement for categorical items, correcting for agreement occurring by chance.
\textbf{Computation}:
\[ \kappa = \frac{p_o - p_e}{1 - p_e} \]
where $p_o$ is the relative observed agreement, and $p_e$ is the hypothetical probability of chance agreement based on marginal frequencies.

\end{document}
