\documentclass{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{float}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, linktoc=all}

\title{AI Reviewer Evaluation Report}
\author{Automated Analysis}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
This report presents a comprehensive evaluation of AI reviewers compared to human performance.

\section{Score Statistics}

\subsection{Score Distributions}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/human_vs_ai_boxplot.png}
    \caption{Distribution of Review Scores (Human vs AI).}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/radar_human_vs_ai.png}
    \caption{Average Score Profile: Human vs AI.}
\end{figure}

\subsection{Per-Evaluator Statistics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/evaluator_boxplot.png}
    \caption{Score Distribution by Evaluator.}
\end{figure}

\subsection{Per-Evaluator per Metric Statistics}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/evaluator_per_metric_boxplot.png}
    \caption{Score Distribution: Evaluator per Metric.}
\end{figure}

\section{Statistical Significance Tests}

\subsection{Methodology}
We confirm performance differences using Mann-Whitney U (unpaired), Wilcoxon Signed-Rank (paired), and assess variance equality with Levene's Test. Effect size is measured by Cliff's Delta.

\subsection{Global Analysis (Human vs All AI)}

\begin{table}[H]
\centering
\caption{Statistical Significance (Overall)}
\label{tab:sig_global}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.4727$ & $0.2872$ & $0.5537$ & -0.037 \\
Specificity & $0.0148^{*}$ & $0.3177$ & $0.6603$ & +0.126 \\
Correctness & $0.6081$ & $0.6546$ & $0.1543$ & +0.027 \\
Constructiveness & $0.7601$ & $0.9317$ & $0.9859$ & +0.016 \\
Stance & $0.1758$ & $0.2412$ & $0.6084$ & -0.069 \\
\bottomrule
\end{tabular}

\end{table}

\subsection{Per-Model Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/radar_models.png}
    \caption{Performance Profile per AI Model.}
\end{figure}
\subsubsection{Model: claude-sonnet-4-20250514}

\begin{table}[H]
\centering
\caption{Significance: Human vs claude-sonnet-4-20250514}
\label{tab:sig_claude-sonnet-4-20250514}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.9780$ & $0.9204$ & $1.0000$ & -0.002 \\
Specificity & $0.1591$ & $0.5735$ & $0.7804$ & +0.119 \\
Correctness & $0.4857$ & $0.6120$ & $0.2798$ & +0.059 \\
Constructiveness & $0.3805$ & $0.5026$ & $0.9137$ & +0.074 \\
Stance & $0.4292$ & $0.2853$ & $0.4104$ & -0.066 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: fudan}

\begin{table}[H]
\centering
\caption{Significance: Human vs fudan}
\label{tab:sig_fudan}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.5627$ & $0.2158$ & $0.3076$ & -0.049 \\
Specificity & $0.1847$ & $0.4504$ & $0.4520$ & +0.112 \\
Correctness & $0.7863$ & $0.4491$ & $0.7560$ & -0.023 \\
Constructiveness & $0.9296$ & $0.5149$ & $0.9294$ & -0.007 \\
Stance & $0.2760$ & $0.0673$ & $0.4246$ & -0.091 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: gpt-5-chat-latest}

\begin{table}[H]
\centering
\caption{Significance: Human vs gpt-5-chat-latest}
\label{tab:sig_gpt-5-chat-latest}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.8837$ & $0.6992$ & $0.9562$ & -0.012 \\
Specificity & $0.1752$ & $0.5455$ & $0.9357$ & +0.115 \\
Correctness & $0.5446$ & $0.4443$ & $0.1362$ & +0.051 \\
Constructiveness & $0.8546$ & $0.9498$ & $0.4333$ & -0.015 \\
Stance & $0.6342$ & $0.6469$ & $0.9548$ & -0.040 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: hkust-reviewer}

\begin{table}[H]
\centering
\caption{Significance: Human vs hkust-reviewer}
\label{tab:sig_hkust-reviewer}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.0503$ & $0.0104^{*}$ & $0.8714$ & -0.165 \\
Specificity & $0.5538$ & $0.9490$ & $0.6102$ & +0.050 \\
Correctness & $0.9494$ & $0.6110$ & $0.4044$ & +0.005 \\
Constructiveness & $0.3369$ & $0.1553$ & $0.5075$ & -0.081 \\
Stance & $0.4284$ & $0.3116$ & $0.4613$ & -0.067 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: rpi}

\begin{table}[H]
\centering
\caption{Significance: Human vs rpi}
\label{tab:sig_rpi}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.1428$ & $0.5453$ & $0.5192$ & +0.123 \\
Specificity & $0.0224^{*}$ & $0.1919$ & $0.6972$ & +0.193 \\
Correctness & $0.9494$ & $0.7463$ & $0.8985$ & +0.005 \\
Constructiveness & $0.5533$ & $0.9270$ & $0.9648$ & +0.050 \\
Stance & $0.1941$ & $0.2312$ & $0.1507$ & -0.109 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: stanford}

\begin{table}[H]
\centering
\caption{Significance: Human vs stanford}
\label{tab:sig_stanford}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.1656$ & $0.1145$ & $0.4615$ & -0.117 \\
Specificity & $0.0500$ & $0.3699$ & $0.1712$ & +0.166 \\
Correctness & $0.4700$ & $0.5009$ & $0.1237$ & +0.061 \\
Constructiveness & $0.3763$ & $0.8481$ & $0.6760$ & +0.074 \\
Stance & $0.6072$ & $0.4528$ & $0.6338$ & -0.043 \\
\bottomrule
\end{tabular}

\end{table}


\section{Turing Test Analysis (AI Detection)}
Evaluators were asked to guess if the review was written by AI or Human. We present the confusion matrices below.

\begin{table}[H]
\centering
\caption{Turing Test Performance Metrics}
\label{tab:tur_metrics}
\begin{tabular}{lrrrr}
\toprule
Evaluator & Accuracy & Precision & Recall & F1 \\
\midrule
Bernhard & 0.667 & 0.667 & 1.000 & 0.800 \\
Guang & 0.378 & 0.536 & 0.500 & 0.517 \\
Justin & 0.600 & 0.658 & 0.833 & 0.735 \\
Luping & 0.478 & 0.741 & 0.333 & 0.460 \\
Tolga & 0.478 & 0.667 & 0.433 & 0.525 \\
Yixuan & 0.567 & 0.769 & 0.500 & 0.606 \\
Overall & 0.528 & 0.661 & 0.600 & 0.629 \\
\bottomrule
\end{tabular}

\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/turing_cm_overall.png}
    \caption{Overall Confusion Matrix (AI Detection).}
\end{figure}
\subsection{Per-Evaluator Confusion Matrices}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{plots/turing_cm_evaluators_combined.png}
    \caption{Confusion Matrices per Evaluator.}
\end{figure}

\section{Inter-Evaluator Agreement}
Cohen's Kappa and Gwet's AC2 agreement between evaluators on review scores (discretized).

\subsection{Cohen's Kappa}

\begin{table}[H]
\centering
\caption{Pairwise Cohen's Kappa Agreement}
\label{tab:kappa}
\begin{tabular}{lllllll}
\toprule
Evaluator & Bernhard & Guang & Justin & Luping & Tolga & Yixuan \\
\midrule
Bernhard & - & 0.03 & - & -0.01 & 0.05 & - \\
Guang & 0.03 & - & -0.08 & -0.01 & - & 0.26 \\
Justin & - & -0.08 & - & - & 0.13 & -0.06 \\
Luping & -0.01 & -0.01 & - & - & - & 0.24 \\
Tolga & 0.05 & - & 0.13 & - & - & 0.11 \\
Yixuan & - & 0.26 & -0.06 & 0.24 & 0.11 & - \\
\bottomrule
\end{tabular}

\end{table}

\subsection{Gwet's AC2}
Gwet's AC2 is often more robust to marginal imbalance and ordinal data.

\begin{table}[H]
\centering
\caption{Pairwise Gwet's AC2 Agreement}
\label{tab:ac2}
\begin{tabular}{lllllll}
\toprule
Evaluator & Bernhard & Guang & Justin & Luping & Tolga & Yixuan \\
\midrule
Bernhard & - & -0.16 & - & -0.37 & -0.05 & - \\
Guang & -0.16 & - & -0.09 & -0.30 & - & 0.26 \\
Justin & - & -0.09 & - & - & 0.07 & -0.15 \\
Luping & -0.37 & -0.30 & - & - & - & 0.18 \\
Tolga & -0.05 & - & 0.07 & - & - & 0.03 \\
Yixuan & - & 0.26 & -0.15 & 0.18 & 0.03 & - \\
\bottomrule
\end{tabular}

\end{table}

\section{Breakdown wrt Accepted versus Rejected Papers}
Analysis of review characteristics based on the final decision (Accept vs Reject).

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_human_scores.png}
        \caption{Human Scores (Accept/Reject)}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_ai_scores.png}
        \caption{AI Scores (Accept/Reject)}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/decision_turing_combined.png}
    \caption{Turing Test Confusion Matrices (Accept/Reject)}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_detection_metrics.png}
        \caption{AI Detection Metrics}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_distribution.png}
        \caption{Dataset Distribution}
    \end{minipage}
\end{figure}

\appendix
\newpage
\section{Appendix: Guide to Interpretations}

\subsection{Interpreting Box Plots}
The box plots in this report visualize the distribution of review scores.
\begin{itemize}
    \item \textbf{Box}: Represents the Interquartile Range (IQR), spanning from the 25th percentile ($Q1$) to the 75th percentile ($Q3$). It contains the middle 50\% of the data.
    \item \textbf{Median}: The line inside the box marks the median score (50th percentile).
    \item \textbf{Whiskers}: Extend from the box to the most extreme data points that are not considered outliers. Typically, this is $1.5 \times IQR$.
    \item \textbf{Empty Circles (Outliers)}: Points lying beyond the whiskers are plotted individually as empty circles. These represent outlier scores that are unusually high or low compared to the rest of the distribution.
\end{itemize}

\subsection{Statistical Methodology Details}
This section explains the intuition and computation behind the statistical tests used.

\subsubsection{Mann-Whitney U Test}
\textbf{Intuition}: A non-parametric test for independent samples (e.g., Human vs AI scores across different papers). It assesses whether one group's values are stochastically larger than the other's. It does not assume a normal distribution.
\textbf{Computation}: All observations are ranked together. The sum of ranks for each group is calculated. The $U$ statistic is derived from these rank sums, comparing the number of times a value from one group precedes a value from the other.

\subsubsection{Wilcoxon Signed-Rank Test}
\textbf{Intuition}: A non-parametric paired test used for per-model comparisons where we have matched scores (Human and AI reviewing the \textit{same} paper). It tests if the distribution of differences is symmetric about zero.
\textbf{Computation}: Differences between paired scores ($d_i = x_{human} - x_{ai}$) are calculated. Absolute differences $|d_i|$ are ranked. Ranks are signed according to the sign of $d_i$. The test statistic $W$ is the sum of positive ranks.

\subsubsection{Levene's Test}
\textbf{Intuition}: Tests the null hypothesis that the variances (spread) of the two groups are equal (Homogeneity of Variance).
\textbf{Computation}: It performs an Analysis of Variance (ANOVA) on the absolute deviations of scores from their group means (or medians). A significant $p$-value suggests the groups have different consistency levels.

\subsubsection{Cliff's Delta ($\delta$)}
\textbf{Intuition}: An effect size measure quantifying the magnitude of difference between two groups. It represents the probability that a randomly selected value from one group is greater than one from the other, minus the reverse probability. values range from -1 to +1.
\textbf{Computation}:
\[ \delta = \frac{\#(x_H > x_A) - \#(x_H < x_A)}{n_H \times n_A} \]
where $x_H$ and $x_A$ are scores from Human and AI groups respectively. 
Interpretation: $|\delta| < 0.147$ (Negligible), $< 0.33$ (Small), $< 0.474$ (Medium), else (Large).

\subsubsection{Cohen's Kappa ($\kappa$)}
\textbf{Intuition}: Measures inter-rater agreement for categorical items, correcting for agreement occurring by chance.
\textbf{Computation}:
\[ \kappa = \frac{p_o - p_e}{1 - p_e} \]
where $p_o$ is the relative observed agreement, and $p_e$ is the hypothetical probability of chance agreement based on marginal frequencies.

\end{document}
