\documentclass{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{float}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, linktoc=all}

\title{AI Reviewer Evaluation Report}
\author{Automated Analysis}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\newpage

\section{Introduction}
This report presents a comprehensive evaluation of AI reviewers compared to human performance.

\section{Score Statistics}

\subsection{Score Distributions}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{plots/human_vs_ai_boxplot.png}
    \caption{Distribution of Review Scores (Human vs AI).}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{plots/radar_human_vs_ai.png}
    \caption{Average Score Profile: Human vs AI.}
\end{figure}

\subsection{Per-Evaluator Statistics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/evaluator_boxplot.png}
    \caption{Score Distribution by Evaluator.}
\end{figure}

\subsection{Per-Evaluator per Metric Statistics}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/evaluator_per_metric_boxplot.png}
    \caption{Score Distribution: Evaluator per Metric.}
\end{figure}

\section{Statistical Significance Tests}

\subsection{Methodology}
We confirm performance differences using Mann-Whitney U (unpaired), Wilcoxon Signed-Rank (paired), and assess variance equality with Levene's Test. Effect size is measured by Cliff's Delta.

\subsection{Global Analysis (Human vs All AI)}

\begin{table}[H]
\centering
\caption{Statistical Significance (Overall)}
\label{tab:sig_global}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.3066$ & $0.1812$ & $0.3899$ & -0.053 \\
Specificity & $0.0177^{*}$ & $0.2872$ & $0.1653$ & +0.122 \\
Correctness & $0.8575$ & $0.3564$ & $0.8690$ & +0.009 \\
Constructiveness & $0.6904$ & $0.9165$ & $0.5780$ & +0.020 \\
Stance & $0.3405$ & $0.2990$ & $0.4229$ & -0.049 \\
\bottomrule
\end{tabular}

\end{table}

\subsection{Per-Model Analysis}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{plots/radar_models.png}
    \caption{Performance Profile per AI Model.}
\end{figure}
\subsubsection{Model: claude-sonnet-4-20250514}

\begin{table}[H]
\centering
\caption{Significance: Human vs claude-sonnet-4-20250514}
\label{tab:sig_claude-sonnet-4-20250514}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.8356$ & $0.5397$ & $0.7947$ & -0.018 \\
Specificity & $0.1441$ & $0.3737$ & $0.3821$ & +0.124 \\
Correctness & $0.7960$ & $0.9372$ & $0.9496$ & +0.022 \\
Constructiveness & $0.3895$ & $0.3755$ & $0.3187$ & +0.070 \\
Stance & $0.6479$ & $0.4789$ & $0.9268$ & -0.039 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: fudan}

\begin{table}[H]
\centering
\caption{Significance: Human vs fudan}
\label{tab:sig_fudan}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.3513$ & $0.2334$ & $0.4661$ & -0.078 \\
Specificity & $0.1944$ & $0.3720$ & $0.1419$ & +0.110 \\
Correctness & $0.6317$ & $0.4241$ & $0.9740$ & -0.041 \\
Constructiveness & $0.8000$ & $0.5965$ & $0.9004$ & -0.021 \\
Stance & $0.3754$ & $0.2081$ & $0.4644$ & -0.075 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: gpt-5-chat-latest}

\begin{table}[H]
\centering
\caption{Significance: Human vs gpt-5-chat-latest}
\label{tab:sig_gpt-5-chat-latest}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.9149$ & $0.5969$ & $0.8414$ & -0.009 \\
Specificity & $0.1935$ & $0.5592$ & $0.9736$ & +0.110 \\
Correctness & $0.4943$ & $0.6553$ & $0.2176$ & +0.058 \\
Constructiveness & $0.9122$ & $0.7193$ & $0.9754$ & +0.009 \\
Stance & $0.9264$ & $0.6093$ & $0.4107$ & +0.008 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: hkust-reviewer}

\begin{table}[H]
\centering
\caption{Significance: Human vs hkust-reviewer}
\label{tab:sig_hkust-reviewer}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.0279^{*}$ & $0.0226^{*}$ & $0.8780$ & -0.185 \\
Specificity & $0.5086$ & $0.8930$ & $0.8963$ & +0.056 \\
Correctness & $0.6658$ & $0.3593$ & $0.6902$ & -0.037 \\
Constructiveness & $0.2996$ & $0.2151$ & $0.3840$ & -0.085 \\
Stance & $0.4898$ & $0.2056$ & $0.0588$ & -0.058 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: rpi}

\begin{table}[H]
\centering
\caption{Significance: Human vs rpi}
\label{tab:sig_rpi}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.2350$ & $0.2830$ & $0.3523$ & +0.100 \\
Specificity & $0.0251^{*}$ & $0.1047$ & $0.2377$ & +0.190 \\
Correctness & $0.9285$ & $0.5547$ & $0.5892$ & -0.008 \\
Constructiveness & $0.2958$ & $0.3909$ & $0.6874$ & +0.085 \\
Stance & $0.2676$ & $0.2334$ & $0.8541$ & -0.093 \\
\bottomrule
\end{tabular}

\end{table}
\subsubsection{Model: stanford}

\begin{table}[H]
\centering
\caption{Significance: Human vs stanford}
\label{tab:sig_stanford}
\begin{tabular}{lllll}
\toprule
Metric & MW U ($p$) & Wilcoxon ($p$) & Levene ($p$) & Cliff's $\delta$ \\
\midrule
Coverage & $0.1361$ & $0.1118$ & $0.3978$ & -0.125 \\
Specificity & $0.0868$ & $0.1929$ & $0.0212^{*}$ & +0.145 \\
Correctness & $0.4724$ & $0.7546$ & $0.6364$ & +0.061 \\
Constructiveness & $0.4630$ & $0.6983$ & $0.1395$ & +0.060 \\
Stance & $0.6514$ & $0.7448$ & $0.3161$ & -0.038 \\
\bottomrule
\end{tabular}

\end{table}


\section{Turing Test Analysis (AI Detection)}
Evaluators were asked to guess if the review was written by AI or Human. We present the confusion matrices below.

\begin{table}[H]
\centering
\caption{Turing Test Performance Metrics}
\label{tab:tur_metrics}
\begin{tabular}{lrrrr}
\toprule
Evaluator & Accuracy & Precision & Recall & F1 \\
\midrule
Bernhard & 0.667 & 0.667 & 1.000 & 0.800 \\
Guang & 0.378 & 0.536 & 0.500 & 0.517 \\
Justin & 0.600 & 0.658 & 0.833 & 0.735 \\
Luping & 0.478 & 0.741 & 0.333 & 0.460 \\
Tolga & 0.478 & 0.667 & 0.433 & 0.525 \\
Yixuan & 0.567 & 0.769 & 0.500 & 0.606 \\
Overall & 0.528 & 0.661 & 0.600 & 0.629 \\
\bottomrule
\end{tabular}

\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/turing_cm_overall.png}
    \caption{Overall Confusion Matrix (AI Detection).}
\end{figure}
\subsection{Per-Evaluator Confusion Matrices}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{plots/turing_cm_evaluators_combined.png}
    \caption{Confusion Matrices per Evaluator.}
\end{figure}

\section{Inter-Evaluator Agreement}
Cohen's Kappa agreement between evaluators on review scores (discretized).

\begin{table}[H]
\centering
\caption{Pairwise Cohen's Kappa Agreement}
\label{tab:kappa}
\begin{tabular}{lllllll}
\toprule
Evaluator & Bernhard & Guang & Justin & Luping & Tolga & Yixuan \\
\midrule
Bernhard & - & 0.00 & - & -0.07 & 0.06 & - \\
Guang & 0.00 & - & -0.16 & -0.04 & - & 0.21 \\
Justin & - & -0.16 & - & - & 0.04 & -0.07 \\
Luping & -0.07 & -0.04 & - & - & - & 0.22 \\
Tolga & 0.06 & - & 0.04 & - & - & 0.07 \\
Yixuan & - & 0.21 & -0.07 & 0.22 & 0.07 & - \\
\bottomrule
\end{tabular}

\end{table}

\section{Breakdown wrt Accepted versus Rejected Papers}
Analysis of review characteristics based on the final decision (Accept vs Reject).

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_human_scores.png}
        \caption{Human Scores (Accept/Reject)}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_ai_scores.png}
        \caption{AI Scores (Accept/Reject)}
    \end{minipage}
\end{figure}



\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{plots/decision_turing_combined.png}
    \caption{Turing Test Confusion Matrices (Accept/Reject)}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_detection_metrics.png}
        \caption{AI Detection Metrics}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{plots/decision_distribution.png}
        \caption{Dataset Distribution}
    \end{minipage}
\end{figure}

\appendix
\newpage
\section{Appendix: Guide to Interpretations}

\subsection{Interpreting Box Plots}
The box plots in this report visualize the distribution of review scores.
\begin{itemize}
    \item \textbf{Box}: Represents the Interquartile Range (IQR), spanning from the 25th percentile ($Q1$) to the 75th percentile ($Q3$). It contains the middle 50\% of the data.
    \item \textbf{Median}: The line inside the box marks the median score (50th percentile).
    \item \textbf{Whiskers}: Extend from the box to the most extreme data points that are not considered outliers. Typically, this is $1.5 \times IQR$.
    \item \textbf{Empty Circles (Outliers)}: Points lying beyond the whiskers are plotted individually as empty circles. These represent outlier scores that are unusually high or low compared to the rest of the distribution.
\end{itemize}

\subsection{Statistical Methodology Details}
This section explains the intuition and computation behind the statistical tests used.

\subsubsection{Mann-Whitney U Test}
\textbf{Intuition}: A non-parametric test for independent samples (e.g., Human vs AI scores across different papers). It assesses whether one group's values are stochastically larger than the other's. It does not assume a normal distribution.
\textbf{Computation}: All observations are ranked together. The sum of ranks for each group is calculated. The $U$ statistic is derived from these rank sums, comparing the number of times a value from one group precedes a value from the other.

\subsubsection{Wilcoxon Signed-Rank Test}
\textbf{Intuition}: A non-parametric paired test used for per-model comparisons where we have matched scores (Human and AI reviewing the \textit{same} paper). It tests if the distribution of differences is symmetric about zero.
\textbf{Computation}: Differences between paired scores ($d_i = x_{human} - x_{ai}$) are calculated. Absolute differences $|d_i|$ are ranked. Ranks are signed according to the sign of $d_i$. The test statistic $W$ is the sum of positive ranks.

\subsubsection{Levene's Test}
\textbf{Intuition}: Tests the null hypothesis that the variances (spread) of the two groups are equal (Homogeneity of Variance).
\textbf{Computation}: It performs an Analysis of Variance (ANOVA) on the absolute deviations of scores from their group means (or medians). A significant $p$-value suggests the groups have different consistency levels.

\subsubsection{Cliff's Delta ($\delta$)}
\textbf{Intuition}: An effect size measure quantifying the magnitude of difference between two groups. It represents the probability that a randomly selected value from one group is greater than one from the other, minus the reverse probability. values range from -1 to +1.
\textbf{Computation}:
\[ \delta = \frac{\#(x_H > x_A) - \#(x_H < x_A)}{n_H \times n_A} \]
where $x_H$ and $x_A$ are scores from Human and AI groups respectively. 
Interpretation: $|\delta| < 0.147$ (Negligible), $< 0.33$ (Small), $< 0.474$ (Medium), else (Large).

\subsubsection{Cohen's Kappa ($\kappa$)}
\textbf{Intuition}: Measures inter-rater agreement for categorical items, correcting for agreement occurring by chance.
\textbf{Computation}:
\[ \kappa = \frac{p_o - p_e}{1 - p_e} \]
where $p_o$ is the relative observed agreement, and $p_e$ is the hypothetical probability of chance agreement based on marginal frequencies.

\end{document}
